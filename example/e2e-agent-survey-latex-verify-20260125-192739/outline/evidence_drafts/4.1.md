# Evidence draft: 4.1 Planning and reasoning loops

## Evidence snippets (with provenance)
- (E-P0089-771620f84f) Experimental evaluation on the complex task planning benchmark demonstrates that our 1.5B parameter model trained with single-turn GRPO achieves superior performance compared to larger baseline models up to 14B parameters, with success rates of 70% for long-horizon planning tasks. Hu2025Training (provenance: paper_notes | papers/paper_notes.jsonl:paper_id=P0089#key_results[0])
- (E-P0233-c7ba5be441) To this end, we introduce USTBench, the first benchmark to evaluate LLMs' spatiotemporal reasoning abilities as urban agents across four decomposed dimensions: spatiotemporal understanding, forecasting, planning, and reflection with feedback. Lai2025Ustbench (provenance: paper_notes | papers/paper_notes.jsonl:paper_id=P0233#method)
- (E-P0150-4a55873ef2) However, a detailed analysis of their testing processes reveals specific strengths and limitations; notably, LLM agents struggle with maintaining coherent long-horizon plans, performing complex reasoning, and effectively utilizing specialized tools. Wang2025Automated (provenance: paper_notes | papers/paper_notes.jsonl:paper_id=P0150#limitations[1])
- (E-P0109-c0a4d47a26) While a lot of recent research focuses on enhancing the textual reasoning capabilities of Large Language Models (LLMs) by optimizing the multi-agent framework or reasoning chains, several benchmark tasks can be solved with 100\% success through direct coding, which is more scalable and avoids the computational overhead associated with textual iterating and searching. Chen2024Steering (provenance: paper_notes | papers/paper_notes.jsonl:paper_id=P0109#key_results[0])
- (E-P0001-ca4a00b5cf) On two interactive decision making benchmarks (ALFWorld and WebShop), ReAct outperforms imitation and reinforcement learning methods by an absolute success rate of 34% and 10% respectively, while being prompted with only one or two in-context examples. Yao2022React (provenance: paper_notes | papers/paper_notes.jsonl:paper_id=P0001#key_results[0])
- (E-P0038-2a7ea60588) Experiments on three real-world multi-tabular EHR datasets show that EHRAgent outperforms the strongest baseline by up to 29.6% in success rate. Shi2024Ehragent (provenance: paper_notes | papers/paper_notes.jsonl:paper_id=P0038#key_results[0])
- (E-P0088-4ae722da57) We demonstrate particularly strong gains on Wikidata benchmarks (+16\% improvement over previous best methods) alongside consistent improvements on Freebase benchmarks. Sun2025Search (provenance: paper_notes | papers/paper_notes.jsonl:paper_id=P0088#key_results[0])
- (E-P0030-8517628bd0) While existing adversarial attacks primarily focus on content falsification or instruction injection, we identify a novel, process-oriented attack surface: the agent's reasoning style. Zhou2025Reasoning (provenance: paper_notes | papers/paper_notes.jsonl:paper_id=P0030#summary_bullets[1])
- (E-P0027-dac95cdbce) We introduce Agentic Reasoning, a framework that enhances large language model (LLM) reasoning by integrating external tool-using agents. Wu2025Agentic (provenance: paper_notes | papers/paper_notes.jsonl:paper_id=P0027#method)
- (E-P0200-1b99de5317) From this observation, we build memory retrieval as an autonomous, accurate, and compatible agent system, named MemR$^3$, which has two core mechanisms: 1) a router that selects among retrieve, reflect, and answer actions to optimize answer quality; 2) a global evidence-gap tracker that explicitly renders the answering process transparent and tracks the evidence collection process. Du2025Memr (provenance: paper_notes | papers/paper_notes.jsonl:paper_id=P0200#key_results[1])
- (E-P0062-0d56c76ce0) Recent benchmarks for Large Language Model (LLM) agents primarily focus on evaluating reasoning, planning, and execution capabilities, while another critical component-memory, encompassing how agents memorize, update, and retrieve long-term information-is under-evaluated due to the lack of benchmarks. Hu2025Evaluating (provenance: paper_notes | papers/paper_notes.jsonl:paper_id=P0062#key_results[0])
- (E-P0110-c0a98eb625) We evaluate PDoctor with three mainstream agent frameworks and two powerful LLMs (GPT-3.5 and GPT-4). Ji2024Testing (provenance: paper_notes | papers/paper_notes.jsonl:paper_id=P0110#key_results[0])
- (E-P0077-23a1945a58) Subsequently, we systematically summarize existing evaluation frameworks, including benchmark datasets, evaluation metrics and performance comparisons between representative planning methods. Cao2025Large (provenance: paper_notes | papers/paper_notes.jsonl:paper_id=P0077#key_results[1])
- (E-P0186-b4b0a2aed4) By analyzing metrics such as planning time, success rate, group debits, and planning-step reduction rate, we demonstrate the superior performance of LLM+MAP, while also providing insights into robotic reasoning. Chu2025Bimanual (provenance: paper_notes | papers/paper_notes.jsonl:paper_id=P0186#key_results[1])
- (E-P0042-44dce85d3e) We further review representative agentic reasoning frameworks across real-world applications and benchmarks, including science, robotics, healthcare, autonomous research, and mathematics. Wei2026Agentic (provenance: paper_notes | papers/paper_notes.jsonl:paper_id=P0042#key_results[0])
- (E-P0276-b0d63bedde) The results were evaluated on four metrics, surpassing human experts in satisfaction and inclusion, and rivaling state-of-the-art reinforcement learning methods in service and ecology. Zhou2024Large (provenance: paper_notes | papers/paper_notes.jsonl:paper_id=P0276#key_results[1])
- (E-P0059-b35b53de13) We systematically evaluate their performance using a suite of coordination-sensitive metrics, including task success rate, redundant actions, room conflicts, and urgency-weighted efficiency. Silva2025Agents (provenance: paper_notes | papers/paper_notes.jsonl:paper_id=P0059#key_results[0])
- (E-P0092-d1af7d9a55) To rigorously evaluate the agent's performance in this emerging research area, we introduce a comprehensive benchmark consisting of diverse construction tasks designed to test creativity, spatial reasoning, adherence to in-game rules, and the effective integration of multimodal instructions. Chen2024Architectural (provenance: paper_notes | papers/paper_notes.jsonl:paper_id=P0092#key_results[0])

## Definitions / setup

- Setup: Which design choices in Planning and reasoning loops drive the major trade-offs, and how are those trade-offs measured? Scope: in-scope: Core topics directly relevant to 'Planning and reasoning loops'.. Axes: evaluation protocol (datasets, metrics, human evaluation); compute and latency constraints; tool interface contract (schemas / protocols); tool selection / routing policy; sandboxing / permissions / observability. Wei2026Agentic Hatalis2025Review Wu2025Agentic

## Claim candidates

- Experimental evaluation on the complex task planning benchmark demonstrates that our 1.5B parameter model trained with single-turn GRPO achieves superior performance compared to larger baseline models up to 14B parameters, with success rates of 70% for long-horizon planning tasks. Hu2025Training
- To this end, we introduce USTBench, the first benchmark to evaluate LLMs' spatiotemporal reasoning abilities as urban agents across four decomposed dimensions: spatiotemporal understanding, forecasting, planning, and reflection with feedback. Lai2025Ustbench
- However, a detailed analysis of their testing processes reveals specific strengths and limitations; notably, LLM agents struggle with maintaining coherent long-horizon plans, performing complex reasoning, and effectively utilizing specialized tools. Wang2025Automated
- While a lot of recent research focuses on enhancing the textual reasoning capabilities of Large Language Models (LLMs) by optimizing the multi-agent framework or reasoning chains, several benchmark tasks can be solved with 100\% success through direct coding, which is more scalable and avoids the computational overhead associated with textual iterating and searching. Chen2024Steering
- On two interactive decision making benchmarks (ALFWorld and WebShop), ReAct outperforms imitation and reinforcement learning methods by an absolute success rate of 34% and 10% respectively, while being prompted with only one or two in-context examples. Yao2022React

## Concrete comparisons

- Axis: evaluation protocol (datasets, metrics, human evaluation); A: Planning / reasoning loops: `P0042`, `P0015`, `P0027`; B: Agent frameworks / architectures: `P0042`, `P0015`, `P0027`. Cao2025Large Wei2026Agentic Hu2025Evaluating Hu2025Training
  - A highlight: (E-P0077-23a1945a58) Subsequently, we systematically summarize existing evaluation frameworks, including benchmark datasets, evaluation metrics and performance comparisons between representative planning methods. Cao2025Large (pointer: papers/paper_notes.jsonl:paper_id=P0077#key_results[1])
  - A highlight: (E-P0042-44dce85d3e) We further review representative agentic reasoning frameworks across real-world applications and benchmarks, including science, robotics, healthcare, autonomous research, and mathematics. Wei2026Agentic (pointer: papers/paper_notes.jsonl:paper_id=P0042#key_results[0])
  - B highlight: (E-P0062-0d56c76ce0) Recent benchmarks for Large Language Model (LLM) agents primarily focus on evaluating reasoning, planning, and execution capabilities, while another critical component-memory, encompassing how agents memorize, update, and retrieve long-term information-is under-evaluated due to Hu2025Evaluating (pointer: papers/paper_notes.jsonl:paper_id=P0062#key_results[0])
  - B highlight: (E-P0089-771620f84f) Experimental evaluation on the complex task planning benchmark demonstrates that our 1.5B parameter model trained with single-turn GRPO achieves superior performance compared to larger baseline models up to 14B parameters, with success rates of 70% for long-horizon planning Hu2025Training (pointer: papers/paper_notes.jsonl:paper_id=P0089#key_results[0])
- Axis: evaluation protocol (datasets, metrics, human evaluation); A: Planning / reasoning loops: `P0042`, `P0015`, `P0027`; B: Memory / retrieval augmentation: `P0062`, `P0200`, `P0038`. Cao2025Large Wei2026Agentic Du2025Memr Hu2025Evaluating
  - A highlight: (E-P0077-23a1945a58) Subsequently, we systematically summarize existing evaluation frameworks, including benchmark datasets, evaluation metrics and performance comparisons between representative planning methods. Cao2025Large (pointer: papers/paper_notes.jsonl:paper_id=P0077#key_results[1])
  - A highlight: (E-P0042-44dce85d3e) We further review representative agentic reasoning frameworks across real-world applications and benchmarks, including science, robotics, healthcare, autonomous research, and mathematics. Wei2026Agentic (pointer: papers/paper_notes.jsonl:paper_id=P0042#key_results[0])
  - B highlight: (E-P0200-1b99de5317) From this observation, we build memory retrieval as an autonomous, accurate, and compatible agent system, named MemR$^3$, which has two core mechanisms: 1) a router that selects among retrieve, reflect, and answer actions to optimize answer quality; 2) a global evidence-gap Du2025Memr (pointer: papers/paper_notes.jsonl:paper_id=P0200#key_results[1])
  - B highlight: (E-P0062-0d56c76ce0) Recent benchmarks for Large Language Model (LLM) agents primarily focus on evaluating reasoning, planning, and execution capabilities, while another critical component-memory, encompassing how agents memorize, update, and retrieve long-term information-is under-evaluated due to Hu2025Evaluating (pointer: papers/paper_notes.jsonl:paper_id=P0062#key_results[0])
- Axis: evaluation protocol (datasets, metrics, human evaluation); A: Agent frameworks / architectures: `P0042`, `P0015`, `P0027`; B: Memory / retrieval augmentation: `P0062`, `P0200`, `P0038`. Hu2025Evaluating Hu2025Training Du2025Memr
  - A highlight: (E-P0062-0d56c76ce0) Recent benchmarks for Large Language Model (LLM) agents primarily focus on evaluating reasoning, planning, and execution capabilities, while another critical component-memory, encompassing how agents memorize, update, and retrieve long-term information-is under-evaluated due to Hu2025Evaluating (pointer: papers/paper_notes.jsonl:paper_id=P0062#key_results[0])
  - A highlight: (E-P0089-771620f84f) Experimental evaluation on the complex task planning benchmark demonstrates that our 1.5B parameter model trained with single-turn GRPO achieves superior performance compared to larger baseline models up to 14B parameters, with success rates of 70% for long-horizon planning Hu2025Training (pointer: papers/paper_notes.jsonl:paper_id=P0089#key_results[0])
  - B highlight: (E-P0200-1b99de5317) From this observation, we build memory retrieval as an autonomous, accurate, and compatible agent system, named MemR$^3$, which has two core mechanisms: 1) a router that selects among retrieve, reflect, and answer actions to optimize answer quality; 2) a global evidence-gap Du2025Memr (pointer: papers/paper_notes.jsonl:paper_id=P0200#key_results[1])
  - B highlight: (E-P0062-0d56c76ce0) Recent benchmarks for Large Language Model (LLM) agents primarily focus on evaluating reasoning, planning, and execution capabilities, while another critical component-memory, encompassing how agents memorize, update, and retrieve long-term information-is under-evaluated due to Hu2025Evaluating (pointer: papers/paper_notes.jsonl:paper_id=P0062#key_results[0])
- Axis: compute and latency constraints; A: Planning / reasoning loops: `P0042`, `P0015`, `P0027`; B: Agent frameworks / architectures: `P0042`, `P0015`, `P0027`. Cao2025Large Silva2025Agents Hu2025Training Hu2025Evaluating
  - A highlight: (E-P0077-23a1945a58) Subsequently, we systematically summarize existing evaluation frameworks, including benchmark datasets, evaluation metrics and performance comparisons between representative planning methods. Cao2025Large (pointer: papers/paper_notes.jsonl:paper_id=P0077#key_results[1])
  - A highlight: (E-P0059-b35b53de13) We systematically evaluate their performance using a suite of coordination-sensitive metrics, including task success rate, redundant actions, room conflicts, and urgency-weighted efficiency. Silva2025Agents (pointer: papers/paper_notes.jsonl:paper_id=P0059#key_results[0])
  - B highlight: (E-P0089-771620f84f) Experimental evaluation on the complex task planning benchmark demonstrates that our 1.5B parameter model trained with single-turn GRPO achieves superior performance compared to larger baseline models up to 14B parameters, with success rates of 70% for long-horizon planning Hu2025Training (pointer: papers/paper_notes.jsonl:paper_id=P0089#key_results[0])
  - B highlight: (E-P0062-0d56c76ce0) Recent benchmarks for Large Language Model (LLM) agents primarily focus on evaluating reasoning, planning, and execution capabilities, while another critical component-memory, encompassing how agents memorize, update, and retrieve long-term information-is under-evaluated due to Hu2025Evaluating (pointer: papers/paper_notes.jsonl:paper_id=P0062#key_results[0])
- Axis: compute and latency constraints; A: Planning / reasoning loops: `P0042`, `P0015`, `P0027`; B: Memory / retrieval augmentation: `P0062`, `P0200`, `P0038`. Cao2025Large Silva2025Agents Du2025Memr Hu2025Evaluating
  - A highlight: (E-P0077-23a1945a58) Subsequently, we systematically summarize existing evaluation frameworks, including benchmark datasets, evaluation metrics and performance comparisons between representative planning methods. Cao2025Large (pointer: papers/paper_notes.jsonl:paper_id=P0077#key_results[1])
  - A highlight: (E-P0059-b35b53de13) We systematically evaluate their performance using a suite of coordination-sensitive metrics, including task success rate, redundant actions, room conflicts, and urgency-weighted efficiency. Silva2025Agents (pointer: papers/paper_notes.jsonl:paper_id=P0059#key_results[0])
  - B highlight: (E-P0200-1b99de5317) From this observation, we build memory retrieval as an autonomous, accurate, and compatible agent system, named MemR$^3$, which has two core mechanisms: 1) a router that selects among retrieve, reflect, and answer actions to optimize answer quality; 2) a global evidence-gap Du2025Memr (pointer: papers/paper_notes.jsonl:paper_id=P0200#key_results[1])
  - B highlight: (E-P0062-0d56c76ce0) Recent benchmarks for Large Language Model (LLM) agents primarily focus on evaluating reasoning, planning, and execution capabilities, while another critical component-memory, encompassing how agents memorize, update, and retrieve long-term information-is under-evaluated due to Hu2025Evaluating (pointer: papers/paper_notes.jsonl:paper_id=P0062#key_results[0])
- Axis: compute and latency constraints; A: Agent frameworks / architectures: `P0042`, `P0015`, `P0027`; B: Memory / retrieval augmentation: `P0062`, `P0200`, `P0038`. Hu2025Training Hu2025Evaluating Du2025Memr
  - A highlight: (E-P0089-771620f84f) Experimental evaluation on the complex task planning benchmark demonstrates that our 1.5B parameter model trained with single-turn GRPO achieves superior performance compared to larger baseline models up to 14B parameters, with success rates of 70% for long-horizon planning Hu2025Training (pointer: papers/paper_notes.jsonl:paper_id=P0089#key_results[0])
  - A highlight: (E-P0062-0d56c76ce0) Recent benchmarks for Large Language Model (LLM) agents primarily focus on evaluating reasoning, planning, and execution capabilities, while another critical component-memory, encompassing how agents memorize, update, and retrieve long-term information-is under-evaluated due to Hu2025Evaluating (pointer: papers/paper_notes.jsonl:paper_id=P0062#key_results[0])
  - B highlight: (E-P0200-1b99de5317) From this observation, we build memory retrieval as an autonomous, accurate, and compatible agent system, named MemR$^3$, which has two core mechanisms: 1) a router that selects among retrieve, reflect, and answer actions to optimize answer quality; 2) a global evidence-gap Du2025Memr (pointer: papers/paper_notes.jsonl:paper_id=P0200#key_results[1])
  - B highlight: (E-P0062-0d56c76ce0) Recent benchmarks for Large Language Model (LLM) agents primarily focus on evaluating reasoning, planning, and execution capabilities, while another critical component-memory, encompassing how agents memorize, update, and retrieve long-term information-is under-evaluated due to Hu2025Evaluating (pointer: papers/paper_notes.jsonl:paper_id=P0062#key_results[0])
- Axis: tool interface contract (schemas / protocols); A: Planning / reasoning loops: `P0042`, `P0015`, `P0027`; B: Agent frameworks / architectures: `P0042`, `P0015`, `P0027`. Wu2025Agentic Cao2025Large Hu2025Evaluating
  - A highlight: (E-P0027-dac95cdbce) We introduce Agentic Reasoning, a framework that enhances large language model (LLM) reasoning by integrating external tool-using agents. Wu2025Agentic (pointer: papers/paper_notes.jsonl:paper_id=P0027#method)
  - A highlight: (E-P0077-23a1945a58) Subsequently, we systematically summarize existing evaluation frameworks, including benchmark datasets, evaluation metrics and performance comparisons between representative planning methods. Cao2025Large (pointer: papers/paper_notes.jsonl:paper_id=P0077#key_results[1])
  - B highlight: (E-P0027-dac95cdbce) We introduce Agentic Reasoning, a framework that enhances large language model (LLM) reasoning by integrating external tool-using agents. Wu2025Agentic (pointer: papers/paper_notes.jsonl:paper_id=P0027#method)
  - B highlight: (E-P0062-0d56c76ce0) Recent benchmarks for Large Language Model (LLM) agents primarily focus on evaluating reasoning, planning, and execution capabilities, while another critical component-memory, encompassing how agents memorize, update, and retrieve long-term information-is under-evaluated due to Hu2025Evaluating (pointer: papers/paper_notes.jsonl:paper_id=P0062#key_results[0])
- Axis: tool interface contract (schemas / protocols); A: Planning / reasoning loops: `P0042`, `P0015`, `P0027`; B: Memory / retrieval augmentation: `P0062`, `P0200`, `P0038`. Wu2025Agentic Cao2025Large Du2025Memr Hu2025Evaluating
  - A highlight: (E-P0027-dac95cdbce) We introduce Agentic Reasoning, a framework that enhances large language model (LLM) reasoning by integrating external tool-using agents. Wu2025Agentic (pointer: papers/paper_notes.jsonl:paper_id=P0027#method)
  - A highlight: (E-P0077-23a1945a58) Subsequently, we systematically summarize existing evaluation frameworks, including benchmark datasets, evaluation metrics and performance comparisons between representative planning methods. Cao2025Large (pointer: papers/paper_notes.jsonl:paper_id=P0077#key_results[1])
  - B highlight: (E-P0200-1b99de5317) From this observation, we build memory retrieval as an autonomous, accurate, and compatible agent system, named MemR$^3$, which has two core mechanisms: 1) a router that selects among retrieve, reflect, and answer actions to optimize answer quality; 2) a global evidence-gap Du2025Memr (pointer: papers/paper_notes.jsonl:paper_id=P0200#key_results[1])
  - B highlight: (E-P0062-0d56c76ce0) Recent benchmarks for Large Language Model (LLM) agents primarily focus on evaluating reasoning, planning, and execution capabilities, while another critical component-memory, encompassing how agents memorize, update, and retrieve long-term information-is under-evaluated due to Hu2025Evaluating (pointer: papers/paper_notes.jsonl:paper_id=P0062#key_results[0])
- Axis: tool interface contract (schemas / protocols); A: Agent frameworks / architectures: `P0042`, `P0015`, `P0027`; B: Memory / retrieval augmentation: `P0062`, `P0200`, `P0038`. Wu2025Agentic Hu2025Evaluating Du2025Memr
  - A highlight: (E-P0027-dac95cdbce) We introduce Agentic Reasoning, a framework that enhances large language model (LLM) reasoning by integrating external tool-using agents. Wu2025Agentic (pointer: papers/paper_notes.jsonl:paper_id=P0027#method)
  - A highlight: (E-P0062-0d56c76ce0) Recent benchmarks for Large Language Model (LLM) agents primarily focus on evaluating reasoning, planning, and execution capabilities, while another critical component-memory, encompassing how agents memorize, update, and retrieve long-term information-is under-evaluated due to Hu2025Evaluating (pointer: papers/paper_notes.jsonl:paper_id=P0062#key_results[0])
  - B highlight: (E-P0200-1b99de5317) From this observation, we build memory retrieval as an autonomous, accurate, and compatible agent system, named MemR$^3$, which has two core mechanisms: 1) a router that selects among retrieve, reflect, and answer actions to optimize answer quality; 2) a global evidence-gap Du2025Memr (pointer: papers/paper_notes.jsonl:paper_id=P0200#key_results[1])
  - B highlight: (E-P0062-0d56c76ce0) Recent benchmarks for Large Language Model (LLM) agents primarily focus on evaluating reasoning, planning, and execution capabilities, while another critical component-memory, encompassing how agents memorize, update, and retrieve long-term information-is under-evaluated due to Hu2025Evaluating (pointer: papers/paper_notes.jsonl:paper_id=P0062#key_results[0])
- Axis: tool selection / routing policy; A: Planning / reasoning loops: `P0042`, `P0015`, `P0027`; B: Agent frameworks / architectures: `P0042`, `P0015`, `P0027`. Wu2025Agentic Cao2025Large Hu2025Evaluating
  - A highlight: (E-P0027-dac95cdbce) We introduce Agentic Reasoning, a framework that enhances large language model (LLM) reasoning by integrating external tool-using agents. Wu2025Agentic (pointer: papers/paper_notes.jsonl:paper_id=P0027#method)
  - A highlight: (E-P0077-23a1945a58) Subsequently, we systematically summarize existing evaluation frameworks, including benchmark datasets, evaluation metrics and performance comparisons between representative planning methods. Cao2025Large (pointer: papers/paper_notes.jsonl:paper_id=P0077#key_results[1])
  - B highlight: (E-P0027-dac95cdbce) We introduce Agentic Reasoning, a framework that enhances large language model (LLM) reasoning by integrating external tool-using agents. Wu2025Agentic (pointer: papers/paper_notes.jsonl:paper_id=P0027#method)
  - B highlight: (E-P0062-0d56c76ce0) Recent benchmarks for Large Language Model (LLM) agents primarily focus on evaluating reasoning, planning, and execution capabilities, while another critical component-memory, encompassing how agents memorize, update, and retrieve long-term information-is under-evaluated due to Hu2025Evaluating (pointer: papers/paper_notes.jsonl:paper_id=P0062#key_results[0])

## Evaluation protocol

- Evaluation mentions include: LLMs, CBR, CBR-enhanced, SOTA, DeepSeek-R1, OpenAI, RSP, GSI, RSV, FEVER. Wei2026Agentic Hatalis2025Review Wu2025Agentic Zhou2025Reasoning
- When comparing results, anchor the paragraph with: task type + metric + constraint (budget, tool access, horizon, or threat model) when stated. Wei2026Agentic Hatalis2025Review
- Prefer head-to-head comparisons only when benchmark/metric are shared; otherwise frame differences as protocol-driven rather than method superiority. Wei2026Agentic Hatalis2025Review
- Avoid underspecified model/baseline naming; if abstracts omit details, state that the baseline is reported but underspecified instead of guessing. Wei2026Agentic Hatalis2025Review
- If a claim relies on a single reported number, pair it with a limitation/caveat from the same evidence so the draft remains conservative. Wei2026Agentic Hatalis2025Review
- If budgets or environments differ across papers, treat cross-paper numeric comparison as fragile and prefer qualitative contrasts aligned to the subsection axes. Wei2026Agentic Hatalis2025Review

## Failures / limitations

- Still, they face limitations in tasks requiring specific, structured knowledge, flexibility, or accountable decision-making. Hatalis2025Review
- While existing adversarial attacks primarily focus on content falsification or instruction injection, we identify a novel, process-oriented attack surface: the agent's reasoning style. Zhou2025Reasoning
- We introduce Generative Style Injection (GSI), an attack method that rewrites retrieved documents into pathological tones--specifically "analysis paralysis" or "cognitive haste"--without altering underlying facts or using explicit triggers. Zhou2025Reasoning
- This study offers new insights into the strengths and failure modes of LLMs in physically grounded multi-agent collaboration tasks, contributing to future benchmarks and architectural improvements. Silva2025Agents
- To address these limitations, we propose Search-on-Graph (SoG), a simple yet effective framework that enables LLMs to perform iterative informed graph navigation using a single, carefully designed \textsc{Search} function. Sun2025Search
- Large language models (LLMs) have demonstrated impressive reasoning abilities yet remain unreliable on knowledge-intensive, multi-hop questions -- they miss long-tail facts, hallucinate when uncertain, and their internal knowledge lags behind real-world change. Sun2025Search

## Verify fields (non-blocking)

- named benchmarks/datasets used
- metrics/human-eval protocol
- compute/training/inference cost
- training data and supervision signal
- baseline choices and ablation evidence
