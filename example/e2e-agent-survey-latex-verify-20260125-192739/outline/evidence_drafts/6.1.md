# Evidence draft: 6.1 Benchmarks and evaluation protocols

## Evidence snippets (with provenance)
- (E-P0113-152d1ce5af) Existing evaluation methods suffer from following shortcomings: (1) constrained evaluation abilities, (2) vulnerable benchmarks, (3) unobjective metrics. Lin2023Agentsims (provenance: paper_notes | papers/paper_notes.jsonl:paper_id=P0113#key_results[0])
- (E-P0122-ef4cf62416) We introduce WildAGTEval, a benchmark designed to evaluate large language model (LLM) agents' function-calling capabilities under realistic API complexity. Kim2026Beyond (provenance: paper_notes | papers/paper_notes.jsonl:paper_id=P0122#method)
- (E-P0216-e2e3b7fa97) Our evaluation of 16 agents under a unified ReAct framework reveals distinct capabilities and limitations across models. Seo2025Simuhome (provenance: paper_notes | papers/paper_notes.jsonl:paper_id=P0216#limitations[1])
- (E-P0086-68db58914f) Our extensive evaluation across various agent use cases, using benchmarks like AgentDojo, ASB, and AgentPoison, demonstrates that Progent reduces attack success rates to 0%, while preserving agent utility and speed. Shi2025Progent (provenance: paper_notes | papers/paper_notes.jsonl:paper_id=P0086#key_results[0])
- (E-P0037-38a26e4777) Extensive experiments across six benchmarks, covering the diverse scenarios of web, embodied, tool use and game applications, show that AgentSquare substantially outperforms hand-crafted agents, achieving an average performance gain of 17.2% against best-known human designs. Shang2024Agentsquare (provenance: paper_notes | papers/paper_notes.jsonl:paper_id=P0037#key_results[0])
- (E-P0244-0bcc4031a6) This paper presents a systematic and comprehensive review of MLLM evaluation methods, covering the following key aspects: (1) the background of MLLMs and their evaluation; (2) "what to evaluate" that reviews and categorizes existing MLLM evaluation tasks based on the capabilities assessed, including general multimodal recognition, perception, reasoning and trustworthiness, and domain-specific applications such as socioeconomic, natural sciences and engineering, medical usage, AI agent, remote sensing, video and audio processing, 3D point cloud analysis, and others; (3) "where to evaluate" that summarizes MLLM evaluation benchmarks into general and specific benchmarks; (4) "how to evaluate" that reviews and illustrates MLLM evaluation steps and metrics; Our overarching goal is to provide valuable insights for researchers in the field of MLLM evaluation, thereby facilitating the development of more capable and reliable MLLMs. Huang2024Survey (provenance: paper_notes | papers/paper_notes.jsonl:paper_id=P0244#key_results[0])
- (E-P0124-60cc0d458f) Experiments on challenging agentic benchmarks such as GAIA and BrowseComp+ demonstrate that EvoRoute, when integrated into off-the-shelf agentic systems, not only sustains or enhances system performance but also reduces execution cost by up to $80\%$ and latency by over $70\%$. Zhang2026Evoroute (provenance: paper_notes | papers/paper_notes.jsonl:paper_id=P0124#key_results[0])
- (E-P0224-5607dc887c) Based on these findings, we design three novel adaptive attacks that significantly improve attack success rates targeting specific frameworks, demonstrating the severity of the flaws in these defenses. Ji2025Taxonomy (provenance: paper_notes | papers/paper_notes.jsonl:paper_id=P0224#key_results[1])
- (E-P0033-46914a4804) Yet their sophisticated architectures amplify vulnerability to cascading failures, where a single root-cause error propagates through subsequent decisions, leading to task failure. Zhu2025Where (provenance: paper_notes | papers/paper_notes.jsonl:paper_id=P0033#summary_bullets[1])
- (E-P0133-a68f39bc04) This survey provides a comprehensive overview of foundation models, agentic systems, datasets, and computational tools supporting this growing field. Van2025Survey (provenance: paper_notes | papers/paper_notes.jsonl:paper_id=P0133#key_results[0])
- (E-P0231-4fc221fdea) This paper proposes an evidence-based, actionable, and generalizable evaluation design guideline for LLM-based RPA by systematically reviewing 1,676 papers published between Jan. Chen2025Towards (provenance: paper_notes | papers/paper_notes.jsonl:paper_id=P0231#key_results[0])
- (E-P0162-39281e5083) Experimental results demonstrate that API-based models outperform open-sourced models on all metrics and Deepseek-Coder-33B-Instruct achieves the highest score among open-sourced models. Zhang2025Datascibench (provenance: paper_notes | papers/paper_notes.jsonl:paper_id=P0162#key_results[1])
- (E-P0001-ca4a00b5cf) On two interactive decision making benchmarks (ALFWorld and WebShop), ReAct outperforms imitation and reinforcement learning methods by an absolute success rate of 34% and 10% respectively, while being prompted with only one or two in-context examples. Yao2022React (provenance: paper_notes | papers/paper_notes.jsonl:paper_id=P0001#key_results[0])
- (E-P0149-42512d3cac) We evaluate AutoSCORE on four benchmark datasets from the ASAP benchmark, using both proprietary and open-source LLMs (GPT-4o, LLaMA-3.1-8B, and LLaMA-3.1-70B). Wang2025Autoscore (provenance: paper_notes | papers/paper_notes.jsonl:paper_id=P0149#key_results[0])
- (E-P0177-acf4a27e30) The system employs tool-augmented reasoning, hierarchical retrieval-augmented generation, forensic-tuned LLMs, and human-in-the-loop feedback to ensure legal and medical validity. Shen2025Feat (provenance: paper_notes | papers/paper_notes.jsonl:paper_id=P0177#key_results[0])
- (E-P0181-29c690fb6c) It proposes five research questions that guide the analysis of the recent literature from 2020 to 2025, focusing on evaluation methods and mitigation techniques. Rahman2025Hallucination (provenance: paper_notes | papers/paper_notes.jsonl:paper_id=P0181#key_results[0])
- (E-P0028-08c43ee74b) ACBench spans (1) 12 tasks across 4 capabilities (e.g., WorfBench for workflow generation, Needle-in-Haystack for long-context retrieval), (2) quantization (GPTQ, AWQ) and pruning (Wanda, SparseGPT), and (3) 15 models, including small (Gemma-2B), standard (Qwen2.5 7B-32B), and distilled reasoning LLMs (DeepSeek-R1-Distill). Dong2025Compressed (provenance: paper_notes | papers/paper_notes.jsonl:paper_id=P0028#key_results[1])
- (E-P0274-52fea1d199) We address research questions such as existing GUI agent frameworks, the collection and utilization of data for training specialized GUI agents, the development of large action models tailored for GUI tasks, and the evaluation metrics and benchmarks necessary to assess their effectiveness. Zhang2024Large (provenance: paper_notes | papers/paper_notes.jsonl:paper_id=P0274#key_results[1])

## Definitions / setup

- Setup: Which design choices in Benchmarks and evaluation protocols drive the major trade-offs, and how are those trade-offs measured? Scope: in-scope: Core topics directly relevant to 'Benchmarks and evaluation protocols'.. Axes: evaluation protocol (datasets, metrics, human evaluation); compute and latency constraints; tool interface contract (schemas / protocols); tool selection / routing policy; sandboxing / permissions / observability. Kim2026Beyond Zhang2026Evoroute Tang2025Empowering

## Claim candidates

- Existing evaluation methods suffer from following shortcomings: (1) constrained evaluation abilities, (2) vulnerable benchmarks, (3) unobjective metrics. Lin2023Agentsims
- We introduce WildAGTEval, a benchmark designed to evaluate large language model (LLM) agents' function-calling capabilities under realistic API complexity. Kim2026Beyond
- Our evaluation of 16 agents under a unified ReAct framework reveals distinct capabilities and limitations across models. Seo2025Simuhome
- Our extensive evaluation across various agent use cases, using benchmarks like AgentDojo, ASB, and AgentPoison, demonstrates that Progent reduces attack success rates to 0%, while preserving agent utility and speed. Shi2025Progent
- Extensive experiments across six benchmarks, covering the diverse scenarios of web, embodied, tool use and game applications, show that AgentSquare substantially outperforms hand-crafted agents, achieving an average performance gain of 17.2% against best-known human designs. Shang2024Agentsquare

## Concrete comparisons

- Axis: evaluation protocol (datasets, metrics, human evaluation); A: Agent frameworks / architectures: `P0122`, `P0124`, `P0006`; B: Evaluation / benchmark-focused works: `P0122`, `P0006`, `P0028`. Kim2026Beyond Van2025Survey Zhang2025Datascibench
  - A highlight: (E-P0122-ef4cf62416) We introduce WildAGTEval, a benchmark designed to evaluate large language model (LLM) agents' function-calling capabilities under realistic API complexity. Kim2026Beyond (pointer: papers/paper_notes.jsonl:paper_id=P0122#method)
  - A highlight: (E-P0133-a68f39bc04) This survey provides a comprehensive overview of foundation models, agentic systems, datasets, and computational tools supporting this growing field. Van2025Survey (pointer: papers/paper_notes.jsonl:paper_id=P0133#key_results[0])
  - B highlight: (E-P0122-ef4cf62416) We introduce WildAGTEval, a benchmark designed to evaluate large language model (LLM) agents' function-calling capabilities under realistic API complexity. Kim2026Beyond (pointer: papers/paper_notes.jsonl:paper_id=P0122#method)
  - B highlight: (E-P0162-39281e5083) Experimental results demonstrate that API-based models outperform open-sourced models on all metrics and Deepseek-Coder-33B-Instruct achieves the highest score among open-sourced models. Zhang2025Datascibench (pointer: papers/paper_notes.jsonl:paper_id=P0162#key_results[1])
- Axis: evaluation protocol (datasets, metrics, human evaluation); A: Agent frameworks / architectures: `P0122`, `P0124`, `P0006`; B: Multi-agent coordination: `P0149`, `P0177`, `P0193`. Kim2026Beyond Van2025Survey Wang2025Autoscore Shen2025Feat
  - A highlight: (E-P0122-ef4cf62416) We introduce WildAGTEval, a benchmark designed to evaluate large language model (LLM) agents' function-calling capabilities under realistic API complexity. Kim2026Beyond (pointer: papers/paper_notes.jsonl:paper_id=P0122#method)
  - A highlight: (E-P0133-a68f39bc04) This survey provides a comprehensive overview of foundation models, agentic systems, datasets, and computational tools supporting this growing field. Van2025Survey (pointer: papers/paper_notes.jsonl:paper_id=P0133#key_results[0])
  - B highlight: (E-P0149-42512d3cac) We evaluate AutoSCORE on four benchmark datasets from the ASAP benchmark, using both proprietary and open-source LLMs (GPT-4o, LLaMA-3.1-8B, and LLaMA-3.1-70B). Wang2025Autoscore (pointer: papers/paper_notes.jsonl:paper_id=P0149#key_results[0])
  - B highlight: (E-P0177-acf4a27e30) The system employs tool-augmented reasoning, hierarchical retrieval-augmented generation, forensic-tuned LLMs, and human-in-the-loop feedback to ensure legal and medical validity. Shen2025Feat (pointer: papers/paper_notes.jsonl:paper_id=P0177#key_results[0])
- Axis: evaluation protocol (datasets, metrics, human evaluation); A: Evaluation / benchmark-focused works: `P0122`, `P0006`, `P0028`; B: Multi-agent coordination: `P0149`, `P0177`, `P0193`. Kim2026Beyond Zhang2025Datascibench Wang2025Autoscore Shen2025Feat
  - A highlight: (E-P0122-ef4cf62416) We introduce WildAGTEval, a benchmark designed to evaluate large language model (LLM) agents' function-calling capabilities under realistic API complexity. Kim2026Beyond (pointer: papers/paper_notes.jsonl:paper_id=P0122#method)
  - A highlight: (E-P0162-39281e5083) Experimental results demonstrate that API-based models outperform open-sourced models on all metrics and Deepseek-Coder-33B-Instruct achieves the highest score among open-sourced models. Zhang2025Datascibench (pointer: papers/paper_notes.jsonl:paper_id=P0162#key_results[1])
  - B highlight: (E-P0149-42512d3cac) We evaluate AutoSCORE on four benchmark datasets from the ASAP benchmark, using both proprietary and open-source LLMs (GPT-4o, LLaMA-3.1-8B, and LLaMA-3.1-70B). Wang2025Autoscore (pointer: papers/paper_notes.jsonl:paper_id=P0149#key_results[0])
  - B highlight: (E-P0177-acf4a27e30) The system employs tool-augmented reasoning, hierarchical retrieval-augmented generation, forensic-tuned LLMs, and human-in-the-loop feedback to ensure legal and medical validity. Shen2025Feat (pointer: papers/paper_notes.jsonl:paper_id=P0177#key_results[0])
- Axis: compute and latency constraints; A: Agent frameworks / architectures: `P0122`, `P0124`, `P0006`; B: Evaluation / benchmark-focused works: `P0122`, `P0006`, `P0028`. Zhang2026Evoroute Shi2025Progent Dong2025Compressed Zhang2025Datascibench
  - A highlight: (E-P0124-60cc0d458f) Experiments on challenging agentic benchmarks such as GAIA and BrowseComp+ demonstrate that EvoRoute, when integrated into off-the-shelf agentic systems, not only sustains or enhances system performance but also reduces execution cost by up to $80\%$ and latency by over $70\%$. Zhang2026Evoroute (pointer: papers/paper_notes.jsonl:paper_id=P0124#key_results[0])
  - A highlight: (E-P0086-68db58914f) Our extensive evaluation across various agent use cases, using benchmarks like AgentDojo, ASB, and AgentPoison, demonstrates that Progent reduces attack success rates to 0%, while preserving agent utility and speed. Shi2025Progent (pointer: papers/paper_notes.jsonl:paper_id=P0086#key_results[0])
  - B highlight: (E-P0028-08c43ee74b) ACBench spans (1) 12 tasks across 4 capabilities (e.g., WorfBench for workflow generation, Needle-in-Haystack for long-context retrieval), (2) quantization (GPTQ, AWQ) and pruning (Wanda, SparseGPT), and (3) 15 models, including small (Gemma-2B), standard (Qwen2.5 7B-32B), and Dong2025Compressed (pointer: papers/paper_notes.jsonl:paper_id=P0028#key_results[1])
  - B highlight: (E-P0162-39281e5083) Experimental results demonstrate that API-based models outperform open-sourced models on all metrics and Deepseek-Coder-33B-Instruct achieves the highest score among open-sourced models. Zhang2025Datascibench (pointer: papers/paper_notes.jsonl:paper_id=P0162#key_results[1])
- Axis: compute and latency constraints; A: Agent frameworks / architectures: `P0122`, `P0124`, `P0006`; B: Multi-agent coordination: `P0149`, `P0177`, `P0193`. Zhang2026Evoroute Shi2025Progent Shen2025Feat Wang2025Autoscore
  - A highlight: (E-P0124-60cc0d458f) Experiments on challenging agentic benchmarks such as GAIA and BrowseComp+ demonstrate that EvoRoute, when integrated into off-the-shelf agentic systems, not only sustains or enhances system performance but also reduces execution cost by up to $80\%$ and latency by over $70\%$. Zhang2026Evoroute (pointer: papers/paper_notes.jsonl:paper_id=P0124#key_results[0])
  - A highlight: (E-P0086-68db58914f) Our extensive evaluation across various agent use cases, using benchmarks like AgentDojo, ASB, and AgentPoison, demonstrates that Progent reduces attack success rates to 0%, while preserving agent utility and speed. Shi2025Progent (pointer: papers/paper_notes.jsonl:paper_id=P0086#key_results[0])
  - B highlight: (E-P0177-acf4a27e30) The system employs tool-augmented reasoning, hierarchical retrieval-augmented generation, forensic-tuned LLMs, and human-in-the-loop feedback to ensure legal and medical validity. Shen2025Feat (pointer: papers/paper_notes.jsonl:paper_id=P0177#key_results[0])
  - B highlight: (E-P0149-42512d3cac) We evaluate AutoSCORE on four benchmark datasets from the ASAP benchmark, using both proprietary and open-source LLMs (GPT-4o, LLaMA-3.1-8B, and LLaMA-3.1-70B). Wang2025Autoscore (pointer: papers/paper_notes.jsonl:paper_id=P0149#key_results[0])
- Axis: compute and latency constraints; A: Evaluation / benchmark-focused works: `P0122`, `P0006`, `P0028`; B: Multi-agent coordination: `P0149`, `P0177`, `P0193`. Dong2025Compressed Zhang2025Datascibench Shen2025Feat Wang2025Autoscore
  - A highlight: (E-P0028-08c43ee74b) ACBench spans (1) 12 tasks across 4 capabilities (e.g., WorfBench for workflow generation, Needle-in-Haystack for long-context retrieval), (2) quantization (GPTQ, AWQ) and pruning (Wanda, SparseGPT), and (3) 15 models, including small (Gemma-2B), standard (Qwen2.5 7B-32B), and Dong2025Compressed (pointer: papers/paper_notes.jsonl:paper_id=P0028#key_results[1])
  - A highlight: (E-P0162-39281e5083) Experimental results demonstrate that API-based models outperform open-sourced models on all metrics and Deepseek-Coder-33B-Instruct achieves the highest score among open-sourced models. Zhang2025Datascibench (pointer: papers/paper_notes.jsonl:paper_id=P0162#key_results[1])
  - B highlight: (E-P0177-acf4a27e30) The system employs tool-augmented reasoning, hierarchical retrieval-augmented generation, forensic-tuned LLMs, and human-in-the-loop feedback to ensure legal and medical validity. Shen2025Feat (pointer: papers/paper_notes.jsonl:paper_id=P0177#key_results[0])
  - B highlight: (E-P0149-42512d3cac) We evaluate AutoSCORE on four benchmark datasets from the ASAP benchmark, using both proprietary and open-source LLMs (GPT-4o, LLaMA-3.1-8B, and LLaMA-3.1-70B). Wang2025Autoscore (pointer: papers/paper_notes.jsonl:paper_id=P0149#key_results[0])
- Axis: tool interface contract (schemas / protocols); A: Agent frameworks / architectures: `P0122`, `P0124`, `P0006`; B: Evaluation / benchmark-focused works: `P0122`, `P0006`, `P0028`. Kim2026Beyond Van2025Survey Zhang2025Datascibench
  - A highlight: (E-P0122-ef4cf62416) We introduce WildAGTEval, a benchmark designed to evaluate large language model (LLM) agents' function-calling capabilities under realistic API complexity. Kim2026Beyond (pointer: papers/paper_notes.jsonl:paper_id=P0122#method)
  - A highlight: (E-P0133-a68f39bc04) This survey provides a comprehensive overview of foundation models, agentic systems, datasets, and computational tools supporting this growing field. Van2025Survey (pointer: papers/paper_notes.jsonl:paper_id=P0133#key_results[0])
  - B highlight: (E-P0122-ef4cf62416) We introduce WildAGTEval, a benchmark designed to evaluate large language model (LLM) agents' function-calling capabilities under realistic API complexity. Kim2026Beyond (pointer: papers/paper_notes.jsonl:paper_id=P0122#method)
  - B highlight: (E-P0162-39281e5083) Experimental results demonstrate that API-based models outperform open-sourced models on all metrics and Deepseek-Coder-33B-Instruct achieves the highest score among open-sourced models. Zhang2025Datascibench (pointer: papers/paper_notes.jsonl:paper_id=P0162#key_results[1])
- Axis: tool interface contract (schemas / protocols); A: Agent frameworks / architectures: `P0122`, `P0124`, `P0006`; B: Multi-agent coordination: `P0149`, `P0177`, `P0193`. Kim2026Beyond Van2025Survey Shen2025Feat Wang2025Autoscore
  - A highlight: (E-P0122-ef4cf62416) We introduce WildAGTEval, a benchmark designed to evaluate large language model (LLM) agents' function-calling capabilities under realistic API complexity. Kim2026Beyond (pointer: papers/paper_notes.jsonl:paper_id=P0122#method)
  - A highlight: (E-P0133-a68f39bc04) This survey provides a comprehensive overview of foundation models, agentic systems, datasets, and computational tools supporting this growing field. Van2025Survey (pointer: papers/paper_notes.jsonl:paper_id=P0133#key_results[0])
  - B highlight: (E-P0177-acf4a27e30) The system employs tool-augmented reasoning, hierarchical retrieval-augmented generation, forensic-tuned LLMs, and human-in-the-loop feedback to ensure legal and medical validity. Shen2025Feat (pointer: papers/paper_notes.jsonl:paper_id=P0177#key_results[0])
  - B highlight: (E-P0149-42512d3cac) We evaluate AutoSCORE on four benchmark datasets from the ASAP benchmark, using both proprietary and open-source LLMs (GPT-4o, LLaMA-3.1-8B, and LLaMA-3.1-70B). Wang2025Autoscore (pointer: papers/paper_notes.jsonl:paper_id=P0149#key_results[0])
- Axis: tool interface contract (schemas / protocols); A: Evaluation / benchmark-focused works: `P0122`, `P0006`, `P0028`; B: Multi-agent coordination: `P0149`, `P0177`, `P0193`. Kim2026Beyond Zhang2025Datascibench Shen2025Feat Wang2025Autoscore
  - A highlight: (E-P0122-ef4cf62416) We introduce WildAGTEval, a benchmark designed to evaluate large language model (LLM) agents' function-calling capabilities under realistic API complexity. Kim2026Beyond (pointer: papers/paper_notes.jsonl:paper_id=P0122#method)
  - A highlight: (E-P0162-39281e5083) Experimental results demonstrate that API-based models outperform open-sourced models on all metrics and Deepseek-Coder-33B-Instruct achieves the highest score among open-sourced models. Zhang2025Datascibench (pointer: papers/paper_notes.jsonl:paper_id=P0162#key_results[1])
  - B highlight: (E-P0177-acf4a27e30) The system employs tool-augmented reasoning, hierarchical retrieval-augmented generation, forensic-tuned LLMs, and human-in-the-loop feedback to ensure legal and medical validity. Shen2025Feat (pointer: papers/paper_notes.jsonl:paper_id=P0177#key_results[0])
  - B highlight: (E-P0149-42512d3cac) We evaluate AutoSCORE on four benchmark datasets from the ASAP benchmark, using both proprietary and open-source LLMs (GPT-4o, LLaMA-3.1-8B, and LLaMA-3.1-70B). Wang2025Autoscore (pointer: papers/paper_notes.jsonl:paper_id=P0149#key_results[0])
- Axis: tool selection / routing policy; A: Agent frameworks / architectures: `P0122`, `P0124`, `P0006`; B: Evaluation / benchmark-focused works: `P0122`, `P0006`, `P0028`. Kim2026Beyond Van2025Survey Zhang2025Datascibench
  - A highlight: (E-P0122-ef4cf62416) We introduce WildAGTEval, a benchmark designed to evaluate large language model (LLM) agents' function-calling capabilities under realistic API complexity. Kim2026Beyond (pointer: papers/paper_notes.jsonl:paper_id=P0122#method)
  - A highlight: (E-P0133-a68f39bc04) This survey provides a comprehensive overview of foundation models, agentic systems, datasets, and computational tools supporting this growing field. Van2025Survey (pointer: papers/paper_notes.jsonl:paper_id=P0133#key_results[0])
  - B highlight: (E-P0122-ef4cf62416) We introduce WildAGTEval, a benchmark designed to evaluate large language model (LLM) agents' function-calling capabilities under realistic API complexity. Kim2026Beyond (pointer: papers/paper_notes.jsonl:paper_id=P0122#method)
  - B highlight: (E-P0162-39281e5083) Experimental results demonstrate that API-based models outperform open-sourced models on all metrics and Deepseek-Coder-33B-Instruct achieves the highest score among open-sourced models. Zhang2025Datascibench (pointer: papers/paper_notes.jsonl:paper_id=P0162#key_results[1])

## Evaluation protocol

- Evaluation mentions include: API, LLMs, WildAGTEval, GAIA, EvoRoute, BrowseComp, LLM-based, GLUE, ACBench, GPTQ. Kim2026Beyond Zhang2026Evoroute Tang2025Empowering Zhang2025Generalizability
- When comparing results, anchor the paragraph with: task type + metric + constraint (budget, tool access, horizon, or threat model) when stated. Kim2026Beyond Zhang2026Evoroute
- Prefer head-to-head comparisons only when benchmark/metric are shared; otherwise frame differences as protocol-driven rather than method superiority. Kim2026Beyond Zhang2026Evoroute
- Avoid underspecified model/baseline naming; if abstracts omit details, state that the baseline is reported but underspecified instead of guessing. Kim2026Beyond Zhang2026Evoroute
- If a claim relies on a single reported number, pair it with a limitation/caveat from the same evidence so the draft remains conservative. Kim2026Beyond Zhang2026Evoroute
- If budgets or environments differ across papers, treat cross-paper numeric comparison as fragile and prefer qualitative contrasts aligned to the subsection axes. Kim2026Beyond Zhang2026Evoroute

## Failures / limitations

- We formalize this challenge as the \textbf{Agent System Trilemma}: the inherent tension among achieving state-of-the-art performance, minimizing monetary cost, and ensuring rapid task completion. Zhang2026Evoroute
- However, how to translate the research on general agents into productivity that drives industry transformations remains a significant challenge. Tang2025Empowering
- We then review datasets, evaluation dimensions, and metrics, highlighting their limitations. Zhang2025Generalizability
- A critical challenge, however, lies in ensuring agent generalizability - the ability to maintain consistent performance across varied instructions, tasks, environments, and domains, especially those beyond agents' fine-tuning data. Zhang2025Generalizability
- Yet their sophisticated architectures amplify vulnerability to cascading failures, where a single root-cause error propagates through subsequent decisions, leading to task failure. Zhu2025Where
- First, we introduce the AgentErrorTaxonomy, a modular classification of failure modes spanning memory, reflection, planning, action, and system-level operations. Zhu2025Where
- Second, we construct AgentErrorBench, the first dataset of systematically annotated failure trajectories from ALFWorld, GAIA, and WebShop, grounding error analysis in real-world agent rollouts. Zhu2025Where
- Thanks to our modular design, integrating Progent does not alter agent internals and only requires minimal changes to the existing agent implementation, enhancing its practicality and potential for widespread adoption. Shi2025Progent

## Verify fields (non-blocking)

- named benchmarks/datasets used
- metrics/human-eval protocol
- compute/training/inference cost
- training data and supervision signal
- baseline choices and ablation evidence
