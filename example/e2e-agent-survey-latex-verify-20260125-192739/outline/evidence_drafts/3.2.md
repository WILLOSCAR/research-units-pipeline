# Evidence draft: 3.2 Tool interfaces and orchestration

## Evidence snippets (with provenance)
- (E-P0192-fae121f81b) Our evaluation of 66 real-world tools from the repositories of two major LLM agent development frameworks, LangChain and LlamaIndex, revealed a significant security concern: 75% are vulnerable to XTHP attacks, highlighting the prevalence of this threat. Li2025Dissonances (provenance: paper_notes | papers/paper_notes.jsonl:paper_id=P0192#key_results[0])
- (E-P0229-92b9d774cc) To evaluate different autonomy levels, we propose four LLM paradigms: (1) centralized cooperation, where a single LLM allocates tools to all agents; (2) centralized self-organization, where a central LLM autonomously activates agents while keeping others inactive; (3) decentralized cooperation, where each agent has its own LLM and calls tools based on local information; and (4) self-organization, where a randomly chosen initial agent can request collaboration, activating additional agents via tool calls. Zhang2025Tool (provenance: paper_notes | papers/paper_notes.jsonl:paper_id=P0229#method)
- (E-P0196-3c65d38a2a) Through extensive evaluation of leading LLMs, we find that even SOTA models such as GPT-5 (43.72%), Grok-4 (33.33%) and Claude-4.0-Sonnet (29.44%) exhibit significant performance limitations. Luo2025Universe (provenance: paper_notes | papers/paper_notes.jsonl:paper_id=P0196#limitations[1])
- (E-P0022-b6b7af5a81) Across six LLMs on the ToolBench and BFCL benchmarks, our attack expands tasks into trajectories exceeding 60,000 tokens, inflates costs by up to 658x, and raises energy by 100-560x. Zhou2026Beyond (provenance: paper_notes | papers/paper_notes.jsonl:paper_id=P0022#key_results[0])
- (E-P0214-2e6956a116) Evaluation on two existing multi-turn tool-use agent benchmarks, M3ToolEval and TauBench, shows the Self-Challenging framework achieves over a two-fold improvement in Llama-3.1-8B-Instruct, despite using only self-generated training data. Zhou2025Self (provenance: paper_notes | papers/paper_notes.jsonl:paper_id=P0214#key_results[0])
- (E-P0083-35271418ac) Evaluating each MemTool mode across 13+ LLMs on the ScaleMCP benchmark, we conducted experiments over 100 consecutive user interactions, measuring tool removal ratios (short-term memory efficiency) and task completion accuracy. Lumer2025Memtool (provenance: paper_notes | papers/paper_notes.jsonl:paper_id=P0083#key_results[0])
- (E-P0081-419e1464da) MCP-RADAR features a challenging dataset of 507 tasks spanning six domains: mathematical reasoning, web search, email, calendar, file management, and terminal operations. Gao2025Radar (provenance: paper_notes | papers/paper_notes.jsonl:paper_id=P0081#key_results[0])
- (E-P0108-9640816b42) Evaluation across various tool-use benchmarks illustrates that our proposed multi-LLM framework surpasses the traditional single-LLM approach, highlighting its efficacy and advantages in tool learning. Shen2024Small (provenance: paper_notes | papers/paper_notes.jsonl:paper_id=P0108#key_results[1])
- (E-P0009-e0bcca0d9e) Furthermore, we evaluated current benchmarks and assessment protocols and have provided an analysis of 68 publicly available datasets to assess the performance of LLM-based agents in various tasks. Chowa2025From (provenance: paper_notes | papers/paper_notes.jsonl:paper_id=P0009#key_results[0])
- (E-P0282-cd97392159) To facilitate the evaluation of the model's capability, we collect a dataset featuring multi-modal input tools from HuggingFace. Wang2024Mllm (provenance: paper_notes | papers/paper_notes.jsonl:paper_id=P0282#key_results[0])
- (E-P0197-3a4792de2b) Current MCP evaluation sets suffer from issues such as reliance on external MCP services and a lack of difficulty awareness. Liu2025Mcpagentbench (provenance: paper_notes | papers/paper_notes.jsonl:paper_id=P0197#key_results[0])
- (E-P0001-ca4a00b5cf) On two interactive decision making benchmarks (ALFWorld and WebShop), ReAct outperforms imitation and reinforcement learning methods by an absolute success rate of 34% and 10% respectively, while being prompted with only one or two in-context examples. Yao2022React (provenance: paper_notes | papers/paper_notes.jsonl:paper_id=P0001#key_results[0])
- (E-P0016-dc2266b72d) Specifically, 1) the taxonomy defines environments/tasks, common LLM-profiled roles or LMPRs (policy models, evaluators, and dynamic models), and universally applicable workflows found in prior work, and 2) it enables a comparison of key perspectives on the implementations of LMPRs and workflow designs across different agent paradigms and frameworks. Li2024Review (provenance: paper_notes | papers/paper_notes.jsonl:paper_id=P0016#key_results[0])
- (E-P0096-3575a7c673) We find AvaTaR consistently outperforms state-of-the-art approaches across all seven tasks, exhibiting strong generalization ability when applied to novel cases and achieving an average relative improvement of 14% on the Hit@1 metric for the retrieval datasets and 13% for the QA datasets. Wu2024Avatar (provenance: paper_notes | papers/paper_notes.jsonl:paper_id=P0096#key_results[0])
- (E-P0242-7cd49652d3) To study this challenge, we constructed TicToc, a diverse dataset of multi-turn user-agent message trajectories across 76 scenarios, spanning dynamic environments with high, medium, and low time sensitivity. Cheng2025Your (provenance: paper_notes | papers/paper_notes.jsonl:paper_id=P0242#key_results[0])
- (E-P0026-f0ea009256) Finally, we summarize the datasets and benchmarks used for evaluation and tuning, review key applications of LLM-based agents, and discuss major challenges and promising future directions. Du2025Survey (provenance: paper_notes | papers/paper_notes.jsonl:paper_id=P0026#key_results[0])
- (E-P0010-793979aed4) We then review datasets, evaluation dimensions, and metrics, highlighting their limitations. Zhang2025Generalizability (provenance: paper_notes | papers/paper_notes.jsonl:paper_id=P0010#key_results[0])
- (E-P0230-15e523063d) Evaluation metrics include standard classification metrics, quality assessment of reasoning processes, and robustness testing against rewritten content. Cui2025Toward (provenance: paper_notes | papers/paper_notes.jsonl:paper_id=P0230#key_results[1])

## Definitions / setup

- Setup: Which design choices in Tool interfaces and orchestration drive the major trade-offs, and how are those trade-offs measured? Scope: in-scope: Core topics directly relevant to 'Tool interfaces and orchestration'.. Axes: evaluation protocol (datasets, metrics, human evaluation); compute and latency constraints; tool interface contract (schemas / protocols); tool selection / routing policy; sandboxing / permissions / observability. Zhou2026Beyond Chowa2025From Zhang2025Generalizability

## Claim candidates

- Our evaluation of 66 real-world tools from the repositories of two major LLM agent development frameworks, LangChain and LlamaIndex, revealed a significant security concern: 75% are vulnerable to XTHP attacks, highlighting the prevalence of this threat. Li2025Dissonances
- To evaluate different autonomy levels, we propose four LLM paradigms: (1) centralized cooperation, where a single LLM allocates tools to all agents; (2) centralized self-organization, where a central LLM autonomously activates agents while keeping others inactive; (3) decentralized cooperation, where each agent has its own LLM and calls tools based on local information; and (4) Zhang2025Tool
- Through extensive evaluation of leading LLMs, we find that even SOTA models such as GPT-5 (43.72%), Grok-4 (33.33%) and Claude-4.0-Sonnet (29.44%) exhibit significant performance limitations. Luo2025Universe
- Across six LLMs on the ToolBench and BFCL benchmarks, our attack expands tasks into trajectories exceeding 60,000 tokens, inflates costs by up to 658x, and raises energy by 100-560x. Zhou2026Beyond
- Evaluation on two existing multi-turn tool-use agent benchmarks, M3ToolEval and TauBench, shows the Self-Challenging framework achieves over a two-fold improvement in Llama-3.1-8B-Instruct, despite using only self-generated training data. Zhou2025Self

## Concrete comparisons

- Axis: evaluation protocol (datasets, metrics, human evaluation); A: Agent frameworks / architectures: `P0022`, `P0009`, `P0010`; B: Tool-use and function calling: `P0022`, `P0009`, `P0054`. Lumer2025Memtool Chowa2025From
  - A highlight: (E-P0083-35271418ac) Evaluating each MemTool mode across 13+ LLMs on the ScaleMCP benchmark, we conducted experiments over 100 consecutive user interactions, measuring tool removal ratios (short-term memory efficiency) and task completion accuracy. Lumer2025Memtool (pointer: papers/paper_notes.jsonl:paper_id=P0083#key_results[0])
  - A highlight: (E-P0009-e0bcca0d9e) Furthermore, we evaluated current benchmarks and assessment protocols and have provided an analysis of 68 publicly available datasets to assess the performance of LLM-based agents in various tasks. Chowa2025From (pointer: papers/paper_notes.jsonl:paper_id=P0009#key_results[0])
  - B highlight: (E-P0083-35271418ac) Evaluating each MemTool mode across 13+ LLMs on the ScaleMCP benchmark, we conducted experiments over 100 consecutive user interactions, measuring tool removal ratios (short-term memory efficiency) and task completion accuracy. Lumer2025Memtool (pointer: papers/paper_notes.jsonl:paper_id=P0083#key_results[0])
  - B highlight: (E-P0009-e0bcca0d9e) Furthermore, we evaluated current benchmarks and assessment protocols and have provided an analysis of 68 publicly available datasets to assess the performance of LLM-based agents in various tasks. Chowa2025From (pointer: papers/paper_notes.jsonl:paper_id=P0009#key_results[0])
- Axis: evaluation protocol (datasets, metrics, human evaluation); A: Agent frameworks / architectures: `P0022`, `P0009`, `P0010`; B: Planning / reasoning loops: `P0013`, `P0054`, `P0016`. Lumer2025Memtool Chowa2025From Wu2024Avatar Yao2022React
  - A highlight: (E-P0083-35271418ac) Evaluating each MemTool mode across 13+ LLMs on the ScaleMCP benchmark, we conducted experiments over 100 consecutive user interactions, measuring tool removal ratios (short-term memory efficiency) and task completion accuracy. Lumer2025Memtool (pointer: papers/paper_notes.jsonl:paper_id=P0083#key_results[0])
  - A highlight: (E-P0009-e0bcca0d9e) Furthermore, we evaluated current benchmarks and assessment protocols and have provided an analysis of 68 publicly available datasets to assess the performance of LLM-based agents in various tasks. Chowa2025From (pointer: papers/paper_notes.jsonl:paper_id=P0009#key_results[0])
  - B highlight: (E-P0096-3575a7c673) We find AvaTaR consistently outperforms state-of-the-art approaches across all seven tasks, exhibiting strong generalization ability when applied to novel cases and achieving an average relative improvement of 14% on the Hit@1 metric for the retrieval datasets and 13% for the Wu2024Avatar (pointer: papers/paper_notes.jsonl:paper_id=P0096#key_results[0])
  - B highlight: (E-P0001-ca4a00b5cf) On two interactive decision making benchmarks (ALFWorld and WebShop), ReAct outperforms imitation and reinforcement learning methods by an absolute success rate of 34% and 10% respectively, while being prompted with only one or two in-context examples. Yao2022React (pointer: papers/paper_notes.jsonl:paper_id=P0001#key_results[0])
- Axis: evaluation protocol (datasets, metrics, human evaluation); A: Tool-use and function calling: `P0022`, `P0009`, `P0054`; B: Planning / reasoning loops: `P0013`, `P0054`, `P0016`. Lumer2025Memtool Chowa2025From Wu2024Avatar Yao2022React
  - A highlight: (E-P0083-35271418ac) Evaluating each MemTool mode across 13+ LLMs on the ScaleMCP benchmark, we conducted experiments over 100 consecutive user interactions, measuring tool removal ratios (short-term memory efficiency) and task completion accuracy. Lumer2025Memtool (pointer: papers/paper_notes.jsonl:paper_id=P0083#key_results[0])
  - A highlight: (E-P0009-e0bcca0d9e) Furthermore, we evaluated current benchmarks and assessment protocols and have provided an analysis of 68 publicly available datasets to assess the performance of LLM-based agents in various tasks. Chowa2025From (pointer: papers/paper_notes.jsonl:paper_id=P0009#key_results[0])
  - B highlight: (E-P0096-3575a7c673) We find AvaTaR consistently outperforms state-of-the-art approaches across all seven tasks, exhibiting strong generalization ability when applied to novel cases and achieving an average relative improvement of 14% on the Hit@1 metric for the retrieval datasets and 13% for the Wu2024Avatar (pointer: papers/paper_notes.jsonl:paper_id=P0096#key_results[0])
  - B highlight: (E-P0001-ca4a00b5cf) On two interactive decision making benchmarks (ALFWorld and WebShop), ReAct outperforms imitation and reinforcement learning methods by an absolute success rate of 34% and 10% respectively, while being prompted with only one or two in-context examples. Yao2022React (pointer: papers/paper_notes.jsonl:paper_id=P0001#key_results[0])
- Axis: compute and latency constraints; A: Agent frameworks / architectures: `P0022`, `P0009`, `P0010`; B: Tool-use and function calling: `P0022`, `P0009`, `P0054`. Zhou2026Beyond Lumer2025Memtool Li2025Dissonances
  - A highlight: (E-P0022-b6b7af5a81) Across six LLMs on the ToolBench and BFCL benchmarks, our attack expands tasks into trajectories exceeding 60,000 tokens, inflates costs by up to 658x, and raises energy by 100-560x. Zhou2026Beyond (pointer: papers/paper_notes.jsonl:paper_id=P0022#key_results[0])
  - A highlight: (E-P0083-35271418ac) Evaluating each MemTool mode across 13+ LLMs on the ScaleMCP benchmark, we conducted experiments over 100 consecutive user interactions, measuring tool removal ratios (short-term memory efficiency) and task completion accuracy. Lumer2025Memtool (pointer: papers/paper_notes.jsonl:paper_id=P0083#key_results[0])
  - B highlight: (E-P0022-b6b7af5a81) Across six LLMs on the ToolBench and BFCL benchmarks, our attack expands tasks into trajectories exceeding 60,000 tokens, inflates costs by up to 658x, and raises energy by 100-560x. Zhou2026Beyond (pointer: papers/paper_notes.jsonl:paper_id=P0022#key_results[0])
  - B highlight: (E-P0192-fae121f81b) Our evaluation of 66 real-world tools from the repositories of two major LLM agent development frameworks, LangChain and LlamaIndex, revealed a significant security concern: 75% are vulnerable to XTHP attacks, highlighting the prevalence of this threat. Li2025Dissonances (pointer: papers/paper_notes.jsonl:paper_id=P0192#key_results[0])
- Axis: compute and latency constraints; A: Agent frameworks / architectures: `P0022`, `P0009`, `P0010`; B: Planning / reasoning loops: `P0013`, `P0054`, `P0016`. Zhou2026Beyond Lumer2025Memtool Li2024Review Wu2024Avatar
  - A highlight: (E-P0022-b6b7af5a81) Across six LLMs on the ToolBench and BFCL benchmarks, our attack expands tasks into trajectories exceeding 60,000 tokens, inflates costs by up to 658x, and raises energy by 100-560x. Zhou2026Beyond (pointer: papers/paper_notes.jsonl:paper_id=P0022#key_results[0])
  - A highlight: (E-P0083-35271418ac) Evaluating each MemTool mode across 13+ LLMs on the ScaleMCP benchmark, we conducted experiments over 100 consecutive user interactions, measuring tool removal ratios (short-term memory efficiency) and task completion accuracy. Lumer2025Memtool (pointer: papers/paper_notes.jsonl:paper_id=P0083#key_results[0])
  - B highlight: (E-P0016-dc2266b72d) Specifically, 1) the taxonomy defines environments/tasks, common LLM-profiled roles or LMPRs (policy models, evaluators, and dynamic models), and universally applicable workflows found in prior work, and 2) it enables a comparison of key perspectives on the implementations of Li2024Review (pointer: papers/paper_notes.jsonl:paper_id=P0016#key_results[0])
  - B highlight: (E-P0096-3575a7c673) We find AvaTaR consistently outperforms state-of-the-art approaches across all seven tasks, exhibiting strong generalization ability when applied to novel cases and achieving an average relative improvement of 14% on the Hit@1 metric for the retrieval datasets and 13% for the Wu2024Avatar (pointer: papers/paper_notes.jsonl:paper_id=P0096#key_results[0])
- Axis: compute and latency constraints; A: Tool-use and function calling: `P0022`, `P0009`, `P0054`; B: Planning / reasoning loops: `P0013`, `P0054`, `P0016`. Zhou2026Beyond Li2025Dissonances Li2024Review Wu2024Avatar
  - A highlight: (E-P0022-b6b7af5a81) Across six LLMs on the ToolBench and BFCL benchmarks, our attack expands tasks into trajectories exceeding 60,000 tokens, inflates costs by up to 658x, and raises energy by 100-560x. Zhou2026Beyond (pointer: papers/paper_notes.jsonl:paper_id=P0022#key_results[0])
  - A highlight: (E-P0192-fae121f81b) Our evaluation of 66 real-world tools from the repositories of two major LLM agent development frameworks, LangChain and LlamaIndex, revealed a significant security concern: 75% are vulnerable to XTHP attacks, highlighting the prevalence of this threat. Li2025Dissonances (pointer: papers/paper_notes.jsonl:paper_id=P0192#key_results[0])
  - B highlight: (E-P0016-dc2266b72d) Specifically, 1) the taxonomy defines environments/tasks, common LLM-profiled roles or LMPRs (policy models, evaluators, and dynamic models), and universally applicable workflows found in prior work, and 2) it enables a comparison of key perspectives on the implementations of Li2024Review (pointer: papers/paper_notes.jsonl:paper_id=P0016#key_results[0])
  - B highlight: (E-P0096-3575a7c673) We find AvaTaR consistently outperforms state-of-the-art approaches across all seven tasks, exhibiting strong generalization ability when applied to novel cases and achieving an average relative improvement of 14% on the Hit@1 metric for the retrieval datasets and 13% for the Wu2024Avatar (pointer: papers/paper_notes.jsonl:paper_id=P0096#key_results[0])
- Axis: tool interface contract (schemas / protocols); A: Agent frameworks / architectures: `P0022`, `P0009`, `P0010`; B: Tool-use and function calling: `P0022`, `P0009`, `P0054`. Lumer2025Memtool Chowa2025From Li2025Dissonances
  - A highlight: (E-P0083-35271418ac) Evaluating each MemTool mode across 13+ LLMs on the ScaleMCP benchmark, we conducted experiments over 100 consecutive user interactions, measuring tool removal ratios (short-term memory efficiency) and task completion accuracy. Lumer2025Memtool (pointer: papers/paper_notes.jsonl:paper_id=P0083#key_results[0])
  - A highlight: (E-P0009-e0bcca0d9e) Furthermore, we evaluated current benchmarks and assessment protocols and have provided an analysis of 68 publicly available datasets to assess the performance of LLM-based agents in various tasks. Chowa2025From (pointer: papers/paper_notes.jsonl:paper_id=P0009#key_results[0])
  - B highlight: (E-P0083-35271418ac) Evaluating each MemTool mode across 13+ LLMs on the ScaleMCP benchmark, we conducted experiments over 100 consecutive user interactions, measuring tool removal ratios (short-term memory efficiency) and task completion accuracy. Lumer2025Memtool (pointer: papers/paper_notes.jsonl:paper_id=P0083#key_results[0])
  - B highlight: (E-P0192-fae121f81b) Our evaluation of 66 real-world tools from the repositories of two major LLM agent development frameworks, LangChain and LlamaIndex, revealed a significant security concern: 75% are vulnerable to XTHP attacks, highlighting the prevalence of this threat. Li2025Dissonances (pointer: papers/paper_notes.jsonl:paper_id=P0192#key_results[0])
- Axis: tool interface contract (schemas / protocols); A: Agent frameworks / architectures: `P0022`, `P0009`, `P0010`; B: Planning / reasoning loops: `P0013`, `P0054`, `P0016`. Lumer2025Memtool Chowa2025From Li2024Review Wu2024Avatar
  - A highlight: (E-P0083-35271418ac) Evaluating each MemTool mode across 13+ LLMs on the ScaleMCP benchmark, we conducted experiments over 100 consecutive user interactions, measuring tool removal ratios (short-term memory efficiency) and task completion accuracy. Lumer2025Memtool (pointer: papers/paper_notes.jsonl:paper_id=P0083#key_results[0])
  - A highlight: (E-P0009-e0bcca0d9e) Furthermore, we evaluated current benchmarks and assessment protocols and have provided an analysis of 68 publicly available datasets to assess the performance of LLM-based agents in various tasks. Chowa2025From (pointer: papers/paper_notes.jsonl:paper_id=P0009#key_results[0])
  - B highlight: (E-P0016-dc2266b72d) Specifically, 1) the taxonomy defines environments/tasks, common LLM-profiled roles or LMPRs (policy models, evaluators, and dynamic models), and universally applicable workflows found in prior work, and 2) it enables a comparison of key perspectives on the implementations of Li2024Review (pointer: papers/paper_notes.jsonl:paper_id=P0016#key_results[0])
  - B highlight: (E-P0096-3575a7c673) We find AvaTaR consistently outperforms state-of-the-art approaches across all seven tasks, exhibiting strong generalization ability when applied to novel cases and achieving an average relative improvement of 14% on the Hit@1 metric for the retrieval datasets and 13% for the Wu2024Avatar (pointer: papers/paper_notes.jsonl:paper_id=P0096#key_results[0])
- Axis: tool interface contract (schemas / protocols); A: Tool-use and function calling: `P0022`, `P0009`, `P0054`; B: Planning / reasoning loops: `P0013`, `P0054`, `P0016`. Lumer2025Memtool Li2025Dissonances Li2024Review Wu2024Avatar
  - A highlight: (E-P0083-35271418ac) Evaluating each MemTool mode across 13+ LLMs on the ScaleMCP benchmark, we conducted experiments over 100 consecutive user interactions, measuring tool removal ratios (short-term memory efficiency) and task completion accuracy. Lumer2025Memtool (pointer: papers/paper_notes.jsonl:paper_id=P0083#key_results[0])
  - A highlight: (E-P0192-fae121f81b) Our evaluation of 66 real-world tools from the repositories of two major LLM agent development frameworks, LangChain and LlamaIndex, revealed a significant security concern: 75% are vulnerable to XTHP attacks, highlighting the prevalence of this threat. Li2025Dissonances (pointer: papers/paper_notes.jsonl:paper_id=P0192#key_results[0])
  - B highlight: (E-P0016-dc2266b72d) Specifically, 1) the taxonomy defines environments/tasks, common LLM-profiled roles or LMPRs (policy models, evaluators, and dynamic models), and universally applicable workflows found in prior work, and 2) it enables a comparison of key perspectives on the implementations of Li2024Review (pointer: papers/paper_notes.jsonl:paper_id=P0016#key_results[0])
  - B highlight: (E-P0096-3575a7c673) We find AvaTaR consistently outperforms state-of-the-art approaches across all seven tasks, exhibiting strong generalization ability when applied to novel cases and achieving an average relative improvement of 14% on the Hit@1 metric for the retrieval datasets and 13% for the Wu2024Avatar (pointer: papers/paper_notes.jsonl:paper_id=P0096#key_results[0])
- Axis: tool selection / routing policy; A: Agent frameworks / architectures: `P0022`, `P0009`, `P0010`; B: Tool-use and function calling: `P0022`, `P0009`, `P0054`. Lumer2025Memtool Chowa2025From Li2025Dissonances
  - A highlight: (E-P0083-35271418ac) Evaluating each MemTool mode across 13+ LLMs on the ScaleMCP benchmark, we conducted experiments over 100 consecutive user interactions, measuring tool removal ratios (short-term memory efficiency) and task completion accuracy. Lumer2025Memtool (pointer: papers/paper_notes.jsonl:paper_id=P0083#key_results[0])
  - A highlight: (E-P0009-e0bcca0d9e) Furthermore, we evaluated current benchmarks and assessment protocols and have provided an analysis of 68 publicly available datasets to assess the performance of LLM-based agents in various tasks. Chowa2025From (pointer: papers/paper_notes.jsonl:paper_id=P0009#key_results[0])
  - B highlight: (E-P0083-35271418ac) Evaluating each MemTool mode across 13+ LLMs on the ScaleMCP benchmark, we conducted experiments over 100 consecutive user interactions, measuring tool removal ratios (short-term memory efficiency) and task completion accuracy. Lumer2025Memtool (pointer: papers/paper_notes.jsonl:paper_id=P0083#key_results[0])
  - B highlight: (E-P0192-fae121f81b) Our evaluation of 66 real-world tools from the repositories of two major LLM agent development frameworks, LangChain and LlamaIndex, revealed a significant security concern: 75% are vulnerable to XTHP attacks, highlighting the prevalence of this threat. Li2025Dissonances (pointer: papers/paper_notes.jsonl:paper_id=P0192#key_results[0])

## Evaluation protocol

- Evaluation mentions include: RAG, MCP, MCTS, LLMs, BFCL, GPU, ToolBench, LLM-based, SCL, CCAM. Zhou2026Beyond Chowa2025From Zhang2025Generalizability Kim2025Bridging
- When comparing results, anchor the paragraph with: task type + metric + constraint (budget, tool access, horizon, or threat model) when stated. Zhou2026Beyond Chowa2025From
- Prefer head-to-head comparisons only when benchmark/metric are shared; otherwise frame differences as protocol-driven rather than method superiority. Zhou2026Beyond Chowa2025From
- Avoid underspecified model/baseline naming; if abstracts omit details, state that the baseline is reported but underspecified instead of guessing. Zhou2026Beyond Chowa2025From
- If a claim relies on a single reported number, pair it with a limitation/caveat from the same evidence so the draft remains conservative. Zhou2026Beyond Chowa2025From
- If budgets or environments differ across papers, treat cross-paper numeric comparison as fragile and prefer qualitative contrasts aligned to the subsection axes. Zhou2026Beyond Chowa2025From

## Failures / limitations

- The agent-tool communication loop is a critical attack surface in modern Large Language Model (LLM) agents. Zhou2026Beyond
- We introduce a stealthy, multi-turn economic DoS attack that operates at the tool layer under the guise of a correctly completed task. Zhou2026Beyond
- Across six LLMs on the ToolBench and BFCL benchmarks, our attack expands tasks into trajectories exceeding 60,000 tokens, inflates costs by up to 658x, and raises energy by 100-560x. Zhou2026Beyond
- These results elevate the agent-tool interface to a first-class security frontier, demanding a paradigm shift from validating final answers to monitoring the economic and computational cost of the entire agentic process. Zhou2026Beyond
- We then review datasets, evaluation dimensions, and metrics, highlighting their limitations. Zhang2025Generalizability
- A critical challenge, however, lies in ensuring agent generalizability - the ability to maintain consistent performance across varied instructions, tasks, environments, and domains, especially those beyond agents' fine-tuning data. Zhang2025Generalizability
- Through InfoBid, we hope to foster the use of LLMs as proxies for human economic and social agents in empirical studies, enhancing our understanding of their capabilities and limitations. Yin2025Infobid
- In this paper, we present the first systematic security analysis of task control flows in multi-tool-enabled LLM agents. Li2025Dissonances

## Verify fields (non-blocking)

- named benchmarks/datasets used
- metrics/human-eval protocol
- compute/training/inference cost
- training data and supervision signal
- baseline choices and ablation evidence
