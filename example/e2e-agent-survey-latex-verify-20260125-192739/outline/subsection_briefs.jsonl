{"sub_id": "3.1", "title": "Agent loop and action spaces", "section_id": "3", "section_title": "Foundations & Interfaces", "rq": "Which design choices in Agent loop and action spaces drive the major trade-offs, and how are those trade-offs measured?", "thesis": "In Agent loop and action spaces, differences in evaluation protocol (datasets, metrics, human evaluation) and compute and latency constraints frequently imply different evaluation setups, so the key is to compare under consistent protocols where possible.", "scope_rule": {"include": ["Core topics directly relevant to 'Agent loop and action spaces'."], "exclude": [], "notes": "If you include an out-of-scope paper as a bridge, state the reason in 1 sentence and keep it secondary."}, "axes": ["evaluation protocol (datasets, metrics, human evaluation)", "compute and latency constraints", "tool interface contract (schemas / protocols)", "tool selection / routing policy", "sandboxing / permissions / observability"], "bridge_terms": ["benchmarks/metrics", "compute"], "contrast_hook": "evaluation", "tension_statement": "A central tension in Agent loop and action spaces is the trade-off between evaluation protocol (datasets, metrics, human evaluation), compute and latency constraints and what can be evaluated reliably under realistic constraints.", "evaluation_anchor_minimal": {"task": "attack/defense evaluation", "metric": "attack success rate", "constraint": "policy/sandbox setting"}, "required_evidence_fields": ["benchmarks/datasets", "metrics / human-eval protocol", "compute / cost (train/infer)", "training signal / supervision", "threat model", "defense surface"], "clusters": [{"label": "Agent frameworks / architectures", "rationale": "Grouped by keyword tag `agents` from titles (bootstrap).", "paper_ids": ["P0124", "P0013", "P0029", "P0045", "P0047", "P0049", "P0052", "P0055"], "bibkeys": ["Zhang2026Evoroute", "Kim2025Bridging", "Feng2025Group", "Hu2025Survey", "Wei2025Memguard", "Li2025Agentswift", "Liu2025Aligning", "Nusrat2025Automated"]}, {"label": "Planning / reasoning loops", "rationale": "Grouped by keyword tag `planning` from titles (bootstrap).", "paper_ids": ["P0013", "P0049", "P0055", "P0175", "P0037", "P0102", "P0255", "P0001"], "bibkeys": ["Kim2025Bridging", "Li2025Agentswift", "Nusrat2025Automated", "Xu2025Exemplar", "Shang2024Agentsquare", "Jiang2024Agent", "Inoue2024Drugagent", "Yao2022React"]}, {"label": "Memory / retrieval augmentation", "rationale": "Grouped by keyword tag `memory` from titles (bootstrap).", "paper_ids": ["P0047", "P0201", "P0203", "P0300"], "bibkeys": ["Wei2025Memguard", "Wu2025Meta", "Abbineni2025Muallm", "Yu2023Finmem"]}], "paragraph_plan": [{"para": 1, "argument_role": "setup_thesis", "intent": "Define scope, setup, and the subsection thesis (no pipeline jargon).", "focus": ["scope boundary", "key definitions", "thesis vs neighboring subsections"], "connector_to_prev": "", "connector_phrase": "", "use_clusters": ["Agent frameworks / architectures"], "rq": "Which design choices in Agent loop and action spaces drive the major trade-offs, and how are those trade-offs measured?"}, {"para": 2, "argument_role": "mechanism_cluster_A", "intent": "Explain cluster A: core mechanism and system architecture and what decision it makes in the agent loop.", "focus": ["cluster: Agent frameworks / architectures", "core mechanism and system architecture", "assumptions"], "connector_to_prev": "grounding", "connector_phrase": "baseline route (Agent frameworks / architectures)", "use_clusters": ["Agent frameworks / architectures"]}, {"para": 3, "argument_role": "implementation_cluster_A", "intent": "Cluster A implementation details: training and data signals and interface contract (tools/memory) that constrain behavior.", "focus": ["cluster: Agent frameworks / architectures", "training and data setup", "interface contract", "axes: evaluation protocol (datasets, metrics, human evaluation), compute and latency constraints, tool interface contract (schemas / protocols), tool selection / routing policy, sandboxing / permissions / observability"], "connector_to_prev": "elaboration", "connector_phrase": "implementation assumptions (interface + training)", "use_clusters": ["Agent frameworks / architectures"]}, {"para": 4, "argument_role": "evaluation_cluster_A", "intent": "Cluster A evaluation/trade-offs: where it works, costs (compute/latency), and typical failure modes.", "focus": ["cluster: Agent frameworks / architectures", "evaluation anchor", "efficiency", "failure modes"], "connector_to_prev": "evaluation", "connector_phrase": "evaluation anchor (task/metric/constraint) + failure modes", "use_clusters": ["Agent frameworks / architectures"]}, {"para": 5, "argument_role": "contrast_cluster_B", "intent": "Explain cluster B (contrast with A): core mechanism and system architecture and what it optimizes for.", "focus": ["cluster: Planning / reasoning loops", "contrast with Agent frameworks / architectures", "core mechanism and system architecture"], "connector_to_prev": "contrast", "connector_phrase": "contrast route (Planning / reasoning loops vs Agent frameworks / architectures)", "use_clusters": ["Planning / reasoning loops"]}, {"para": 6, "argument_role": "implementation_cluster_B", "intent": "Cluster B implementation details: training and data and interface assumptions (mirror A for comparability).", "focus": ["cluster: Planning / reasoning loops", "training and data setup", "interface contract", "axes: evaluation protocol (datasets, metrics, human evaluation), compute and latency constraints, tool interface contract (schemas / protocols), tool selection / routing policy, sandboxing / permissions / observability"], "connector_to_prev": "elaboration", "connector_phrase": "contrast implementation assumptions (B)", "use_clusters": ["Planning / reasoning loops"]}, {"para": 7, "argument_role": "evaluation_cluster_B", "intent": "Cluster B evaluation/trade-offs: where it works, costs, and failure modes (mirror A).", "focus": ["cluster: Planning / reasoning loops", "evaluation anchor", "efficiency", "failure modes"], "connector_to_prev": "evaluation", "connector_phrase": "contrast evaluation anchor + trade-offs (B)", "use_clusters": ["Planning / reasoning loops"]}, {"para": 8, "argument_role": "cross_paper_synthesis", "intent": "Cross-paper synthesis: compare clusters along the main axes (include >=2 citations in one paragraph).", "focus": ["compare Agent frameworks / architectures vs Planning / reasoning loops", "multiple citations in one paragraph", "axes: evaluation protocol (datasets, metrics, human evaluation), compute and latency constraints, tool interface contract (schemas / protocols), tool selection / routing policy, sandboxing / permissions / observability"], "connector_to_prev": "synthesis", "connector_phrase": "cross-paper synthesis (Agent frameworks / architectures vs Planning / reasoning loops)", "use_clusters": ["Agent frameworks / architectures", "Planning / reasoning loops", "Memory / retrieval augmentation"]}, {"para": 9, "argument_role": "decision_guidance", "intent": "Decision guidance: when to choose which route (criteria + evaluation signals + engineering constraints).", "focus": ["decision checklist", "evaluation protocol", "practical constraints"], "connector_to_prev": "consequence", "connector_phrase": "decision guidance / criteria", "use_clusters": ["Agent frameworks / architectures", "Planning / reasoning loops", "Memory / retrieval augmentation"]}, {"para": 10, "argument_role": "limitations_open_questions", "intent": "Limitations + verification targets; end with a concrete open question to hand off.", "focus": ["limitations", "evidence mode: provisional", "what needs verification", "open question"], "connector_to_prev": "limitations", "connector_phrase": "limitations + verification targets", "use_clusters": ["Agent frameworks / architectures", "Planning / reasoning loops", "Memory / retrieval augmentation"], "policy": "Use conservative language; avoid strong conclusions; prefer questions-to-answer + explicit evidence gaps list."}], "evidence_level_summary": {"fulltext": 0, "abstract": 28, "title": 0}, "generated_at": "2026-01-25T17:05:38"}
{"sub_id": "3.2", "title": "Tool interfaces and orchestration", "section_id": "3", "section_title": "Foundations & Interfaces", "rq": "Which design choices in Tool interfaces and orchestration drive the major trade-offs, and how are those trade-offs measured?", "thesis": "Tool interfaces and orchestration methods emphasize evaluation protocol (datasets, metrics, human evaluation) and compute and latency constraints trade-offs, but synthesis is clearest when claims are tied to explicit evaluation settings and reporting conventions.", "scope_rule": {"include": ["Core topics directly relevant to 'Tool interfaces and orchestration'."], "exclude": [], "notes": "If you include an out-of-scope paper as a bridge, state the reason in 1 sentence and keep it secondary."}, "axes": ["evaluation protocol (datasets, metrics, human evaluation)", "compute and latency constraints", "tool interface contract (schemas / protocols)", "tool selection / routing policy", "sandboxing / permissions / observability"], "bridge_terms": ["function calling", "tool schema", "routing", "sandbox", "observability", "benchmarks/metrics"], "contrast_hook": "tool interfaces", "tension_statement": "In Tool interfaces and orchestration, a practical tension is expressivity versus control: richer interfaces expand capability but make behavior harder to constrain and verify.", "evaluation_anchor_minimal": {"task": "attack/defense evaluation", "metric": "attack success rate", "constraint": "policy/sandbox setting"}, "required_evidence_fields": ["benchmarks/datasets", "metrics / human-eval protocol", "compute / cost (train/infer)", "training signal / supervision", "threat model", "defense surface"], "clusters": [{"label": "Agent frameworks / architectures", "rationale": "Grouped by keyword tag `agents` from titles (bootstrap).", "paper_ids": ["P0022", "P0009", "P0010", "P0013", "P0026", "P0054", "P0066", "P0083"], "bibkeys": ["Zhou2026Beyond", "Chowa2025From", "Zhang2025Generalizability", "Kim2025Bridging", "Du2025Survey", "Jia2025Autotool", "Yin2025Infobid", "Lumer2025Memtool"]}, {"label": "Tool-use and function calling", "rationale": "Grouped by keyword tag `tool-use` from titles (bootstrap).", "paper_ids": ["P0022", "P0009", "P0054", "P0081", "P0083", "P0138", "P0192", "P0196"], "bibkeys": ["Zhou2026Beyond", "Chowa2025From", "Jia2025Autotool", "Gao2025Radar", "Lumer2025Memtool", "Samplawski2025Agent", "Li2025Dissonances", "Luo2025Universe"]}, {"label": "Planning / reasoning loops", "rationale": "Grouped by keyword tag `planning` from titles (bootstrap).", "paper_ids": ["P0013", "P0054", "P0016", "P0096", "P0001"], "bibkeys": ["Kim2025Bridging", "Jia2025Autotool", "Li2024Review", "Wu2024Avatar", "Yao2022React"]}], "paragraph_plan": [{"para": 1, "argument_role": "setup_thesis", "intent": "Define scope, setup, and the subsection thesis (no pipeline jargon).", "focus": ["scope boundary", "key definitions", "thesis vs neighboring subsections"], "connector_to_prev": "", "connector_phrase": "", "use_clusters": ["Agent frameworks / architectures"], "rq": "Which design choices in Tool interfaces and orchestration drive the major trade-offs, and how are those trade-offs measured?"}, {"para": 2, "argument_role": "mechanism_cluster_A", "intent": "Explain cluster A: core mechanism and system architecture and what decision it makes in the agent loop.", "focus": ["cluster: Agent frameworks / architectures", "core mechanism and system architecture", "assumptions"], "connector_to_prev": "grounding", "connector_phrase": "baseline route (Agent frameworks / architectures)", "use_clusters": ["Agent frameworks / architectures"]}, {"para": 3, "argument_role": "implementation_cluster_A", "intent": "Cluster A implementation details: training and data signals and interface contract (tools/memory) that constrain behavior.", "focus": ["cluster: Agent frameworks / architectures", "training and data setup", "interface contract", "axes: evaluation protocol (datasets, metrics, human evaluation), compute and latency constraints, tool interface contract (schemas / protocols), tool selection / routing policy, sandboxing / permissions / observability"], "connector_to_prev": "elaboration", "connector_phrase": "implementation assumptions (interface + training)", "use_clusters": ["Agent frameworks / architectures"]}, {"para": 4, "argument_role": "evaluation_cluster_A", "intent": "Cluster A evaluation/trade-offs: where it works, costs (compute/latency), and typical failure modes.", "focus": ["cluster: Agent frameworks / architectures", "evaluation anchor", "efficiency", "failure modes"], "connector_to_prev": "evaluation", "connector_phrase": "evaluation anchor (task/metric/constraint) + failure modes", "use_clusters": ["Agent frameworks / architectures"]}, {"para": 5, "argument_role": "contrast_cluster_B", "intent": "Explain cluster B (contrast with A): core mechanism and system architecture and what it optimizes for.", "focus": ["cluster: Tool-use and function calling", "contrast with Agent frameworks / architectures", "core mechanism and system architecture"], "connector_to_prev": "contrast", "connector_phrase": "contrast route (Tool-use and function calling vs Agent frameworks / architectures)", "use_clusters": ["Tool-use and function calling"]}, {"para": 6, "argument_role": "implementation_cluster_B", "intent": "Cluster B implementation details: training and data and interface assumptions (mirror A for comparability).", "focus": ["cluster: Tool-use and function calling", "training and data setup", "interface contract", "axes: evaluation protocol (datasets, metrics, human evaluation), compute and latency constraints, tool interface contract (schemas / protocols), tool selection / routing policy, sandboxing / permissions / observability"], "connector_to_prev": "elaboration", "connector_phrase": "contrast implementation assumptions (B)", "use_clusters": ["Tool-use and function calling"]}, {"para": 7, "argument_role": "evaluation_cluster_B", "intent": "Cluster B evaluation/trade-offs: where it works, costs, and failure modes (mirror A).", "focus": ["cluster: Tool-use and function calling", "evaluation anchor", "efficiency", "failure modes"], "connector_to_prev": "evaluation", "connector_phrase": "contrast evaluation anchor + trade-offs (B)", "use_clusters": ["Tool-use and function calling"]}, {"para": 8, "argument_role": "cross_paper_synthesis", "intent": "Cross-paper synthesis: compare clusters along the main axes (include >=2 citations in one paragraph).", "focus": ["compare Agent frameworks / architectures vs Tool-use and function calling", "multiple citations in one paragraph", "axes: evaluation protocol (datasets, metrics, human evaluation), compute and latency constraints, tool interface contract (schemas / protocols), tool selection / routing policy, sandboxing / permissions / observability"], "connector_to_prev": "synthesis", "connector_phrase": "cross-paper synthesis (Agent frameworks / architectures vs Tool-use and function calling)", "use_clusters": ["Agent frameworks / architectures", "Tool-use and function calling", "Planning / reasoning loops"]}, {"para": 9, "argument_role": "decision_guidance", "intent": "Decision guidance: when to choose which route (criteria + evaluation signals + engineering constraints).", "focus": ["decision checklist", "evaluation protocol", "practical constraints"], "connector_to_prev": "consequence", "connector_phrase": "decision guidance / criteria", "use_clusters": ["Agent frameworks / architectures", "Tool-use and function calling", "Planning / reasoning loops"]}, {"para": 10, "argument_role": "limitations_open_questions", "intent": "Limitations + verification targets; end with a concrete open question to hand off.", "focus": ["limitations", "evidence mode: provisional", "what needs verification", "open question"], "connector_to_prev": "limitations", "connector_phrase": "limitations + verification targets", "use_clusters": ["Agent frameworks / architectures", "Tool-use and function calling", "Planning / reasoning loops"], "policy": "Use conservative language; avoid strong conclusions; prefer questions-to-answer + explicit evidence gaps list."}], "evidence_level_summary": {"fulltext": 0, "abstract": 28, "title": 0}, "generated_at": "2026-01-25T17:05:38"}
{"sub_id": "4.1", "title": "Planning and reasoning loops", "section_id": "4", "section_title": "Core Components (Planning + Memory)", "rq": "Which design choices in Planning and reasoning loops drive the major trade-offs, and how are those trade-offs measured?", "thesis": "In Planning and reasoning loops, differences in evaluation protocol (datasets, metrics, human evaluation) and compute and latency constraints frequently imply different evaluation setups, so the key is to compare under consistent protocols where possible.", "scope_rule": {"include": ["Core topics directly relevant to 'Planning and reasoning loops'."], "exclude": [], "notes": "If you include an out-of-scope paper as a bridge, state the reason in 1 sentence and keep it secondary."}, "axes": ["evaluation protocol (datasets, metrics, human evaluation)", "compute and latency constraints", "tool interface contract (schemas / protocols)", "tool selection / routing policy", "sandboxing / permissions / observability"], "bridge_terms": ["planner/executor", "search", "deliberation", "action grounding", "benchmarks/metrics", "compute"], "contrast_hook": "planning/control loop", "tension_statement": "In Planning and reasoning loops, a recurring tension is deliberation depth versus cost: more planning can improve reliability but increases latency and budget sensitivity.", "evaluation_anchor_minimal": {"task": "attack/defense evaluation", "metric": "attack success rate", "constraint": "policy/sandbox setting"}, "required_evidence_fields": ["benchmarks/datasets", "metrics / human-eval protocol", "compute / cost (train/infer)", "training signal / supervision", "threat model", "defense surface"], "clusters": [{"label": "Planning / reasoning loops", "rationale": "Grouped by keyword tag `planning` from titles (bootstrap).", "paper_ids": ["P0042", "P0015", "P0027", "P0030", "P0055", "P0059", "P0077", "P0088"], "bibkeys": ["Wei2026Agentic", "Hatalis2025Review", "Wu2025Agentic", "Zhou2025Reasoning", "Nusrat2025Automated", "Silva2025Agents", "Cao2025Large", "Sun2025Search"]}, {"label": "Agent frameworks / architectures", "rationale": "Grouped by keyword tag `agents` from titles (bootstrap).", "paper_ids": ["P0042", "P0015", "P0027", "P0030", "P0055", "P0059", "P0062", "P0089"], "bibkeys": ["Wei2026Agentic", "Hatalis2025Review", "Wu2025Agentic", "Zhou2025Reasoning", "Nusrat2025Automated", "Silva2025Agents", "Hu2025Evaluating", "Hu2025Training"]}, {"label": "Memory / retrieval augmentation", "rationale": "Grouped by keyword tag `memory` from titles (bootstrap).", "paper_ids": ["P0062", "P0200", "P0038"], "bibkeys": ["Hu2025Evaluating", "Du2025Memr", "Shi2024Ehragent"]}], "paragraph_plan": [{"para": 1, "argument_role": "setup_thesis", "intent": "Define scope, setup, and the subsection thesis (no pipeline jargon).", "focus": ["scope boundary", "key definitions", "thesis vs neighboring subsections"], "connector_to_prev": "", "connector_phrase": "", "use_clusters": ["Planning / reasoning loops"], "rq": "Which design choices in Planning and reasoning loops drive the major trade-offs, and how are those trade-offs measured?"}, {"para": 2, "argument_role": "mechanism_cluster_A", "intent": "Explain cluster A: core mechanism and system architecture and what decision it makes in the agent loop.", "focus": ["cluster: Planning / reasoning loops", "core mechanism and system architecture", "assumptions"], "connector_to_prev": "grounding", "connector_phrase": "baseline route (Planning / reasoning loops)", "use_clusters": ["Planning / reasoning loops"]}, {"para": 3, "argument_role": "implementation_cluster_A", "intent": "Cluster A implementation details: training and data signals and interface contract (tools/memory) that constrain behavior.", "focus": ["cluster: Planning / reasoning loops", "training and data setup", "interface contract", "axes: evaluation protocol (datasets, metrics, human evaluation), compute and latency constraints, tool interface contract (schemas / protocols), tool selection / routing policy, sandboxing / permissions / observability"], "connector_to_prev": "elaboration", "connector_phrase": "implementation assumptions (interface + training)", "use_clusters": ["Planning / reasoning loops"]}, {"para": 4, "argument_role": "evaluation_cluster_A", "intent": "Cluster A evaluation/trade-offs: where it works, costs (compute/latency), and typical failure modes.", "focus": ["cluster: Planning / reasoning loops", "evaluation anchor", "efficiency", "failure modes"], "connector_to_prev": "evaluation", "connector_phrase": "evaluation anchor (task/metric/constraint) + failure modes", "use_clusters": ["Planning / reasoning loops"]}, {"para": 5, "argument_role": "contrast_cluster_B", "intent": "Explain cluster B (contrast with A): core mechanism and system architecture and what it optimizes for.", "focus": ["cluster: Agent frameworks / architectures", "contrast with Planning / reasoning loops", "core mechanism and system architecture"], "connector_to_prev": "contrast", "connector_phrase": "contrast route (Agent frameworks / architectures vs Planning / reasoning loops)", "use_clusters": ["Agent frameworks / architectures"]}, {"para": 6, "argument_role": "implementation_cluster_B", "intent": "Cluster B implementation details: training and data and interface assumptions (mirror A for comparability).", "focus": ["cluster: Agent frameworks / architectures", "training and data setup", "interface contract", "axes: evaluation protocol (datasets, metrics, human evaluation), compute and latency constraints, tool interface contract (schemas / protocols), tool selection / routing policy, sandboxing / permissions / observability"], "connector_to_prev": "elaboration", "connector_phrase": "contrast implementation assumptions (B)", "use_clusters": ["Agent frameworks / architectures"]}, {"para": 7, "argument_role": "evaluation_cluster_B", "intent": "Cluster B evaluation/trade-offs: where it works, costs, and failure modes (mirror A).", "focus": ["cluster: Agent frameworks / architectures", "evaluation anchor", "efficiency", "failure modes"], "connector_to_prev": "evaluation", "connector_phrase": "contrast evaluation anchor + trade-offs (B)", "use_clusters": ["Agent frameworks / architectures"]}, {"para": 8, "argument_role": "cross_paper_synthesis", "intent": "Cross-paper synthesis: compare clusters along the main axes (include >=2 citations in one paragraph).", "focus": ["compare Planning / reasoning loops vs Agent frameworks / architectures", "multiple citations in one paragraph", "axes: evaluation protocol (datasets, metrics, human evaluation), compute and latency constraints, tool interface contract (schemas / protocols), tool selection / routing policy, sandboxing / permissions / observability"], "connector_to_prev": "synthesis", "connector_phrase": "cross-paper synthesis (Planning / reasoning loops vs Agent frameworks / architectures)", "use_clusters": ["Planning / reasoning loops", "Agent frameworks / architectures", "Memory / retrieval augmentation"]}, {"para": 9, "argument_role": "decision_guidance", "intent": "Decision guidance: when to choose which route (criteria + evaluation signals + engineering constraints).", "focus": ["decision checklist", "evaluation protocol", "practical constraints"], "connector_to_prev": "consequence", "connector_phrase": "decision guidance / criteria", "use_clusters": ["Planning / reasoning loops", "Agent frameworks / architectures", "Memory / retrieval augmentation"]}, {"para": 10, "argument_role": "limitations_open_questions", "intent": "Limitations + verification targets; end with a concrete open question to hand off.", "focus": ["limitations", "evidence mode: provisional", "what needs verification", "open question"], "connector_to_prev": "limitations", "connector_phrase": "limitations + verification targets", "use_clusters": ["Planning / reasoning loops", "Agent frameworks / architectures", "Memory / retrieval augmentation"], "policy": "Use conservative language; avoid strong conclusions; prefer questions-to-answer + explicit evidence gaps list."}], "evidence_level_summary": {"fulltext": 0, "abstract": 28, "title": 0}, "generated_at": "2026-01-25T17:05:38"}
{"sub_id": "4.2", "title": "Memory and retrieval (RAG)", "section_id": "4", "section_title": "Core Components (Planning + Memory)", "rq": "Which design choices in Memory and retrieval (RAG) drive the major trade-offs, and how are those trade-offs measured?", "thesis": "In Memory and retrieval (RAG), differences in evaluation protocol (datasets, metrics, human evaluation) and compute and latency constraints frequently imply different evaluation setups, so the key is to compare under consistent protocols where possible.", "scope_rule": {"include": ["Core topics directly relevant to 'Memory and retrieval (RAG)'."], "exclude": [], "notes": "If you include an out-of-scope paper as a bridge, state the reason in 1 sentence and keep it secondary."}, "axes": ["evaluation protocol (datasets, metrics, human evaluation)", "compute and latency constraints", "tool interface contract (schemas / protocols)", "tool selection / routing policy", "sandboxing / permissions / observability"], "bridge_terms": ["retrieval", "index", "write policy", "long-term memory", "benchmarks/metrics", "compute"], "contrast_hook": "memory/retrieval", "tension_statement": "In Memory and retrieval (RAG), the core tension is persistence versus freshness: retaining more context helps long-horizon tasks but raises staleness, contamination, and verification challenges.", "evaluation_anchor_minimal": {"task": "attack/defense evaluation", "metric": "attack success rate", "constraint": "policy/sandbox setting"}, "required_evidence_fields": ["benchmarks/datasets", "metrics / human-eval protocol", "compute / cost (train/infer)", "training signal / supervision", "threat model", "defense surface"], "clusters": [{"label": "Agent frameworks / architectures", "rationale": "Grouped by keyword tag `agents` from titles (bootstrap).", "paper_ids": ["P0120", "P0121", "P0128", "P0011", "P0026", "P0032", "P0033", "P0034"], "bibkeys": ["Verma2026Active", "Yu2026Agentic", "Tao2026Membox", "Yao2025Survey", "Du2025Survey", "Ye2025Task", "Zhu2025Where", "Lu2025Youtu"]}, {"label": "Memory / retrieval augmentation", "rationale": "Grouped by keyword tag `memory` from titles (bootstrap).", "paper_ids": ["P0120", "P0121", "P0128", "P0032", "P0047", "P0062", "P0064", "P0166"], "bibkeys": ["Verma2026Active", "Yu2026Agentic", "Tao2026Membox", "Ye2025Task", "Wei2025Memguard", "Hu2025Evaluating", "Li2025Graphcodeagent", "Kang2025Distilling"]}, {"label": "Planning / reasoning loops", "rationale": "Grouped by keyword tag `planning` from titles (bootstrap).", "paper_ids": ["P0200", "P0016", "P0118", "P0001"], "bibkeys": ["Du2025Memr", "Li2024Review", "Liu2023Reason", "Yao2022React"]}], "paragraph_plan": [{"para": 1, "argument_role": "setup_thesis", "intent": "Define scope, setup, and the subsection thesis (no pipeline jargon).", "focus": ["scope boundary", "key definitions", "thesis vs neighboring subsections"], "connector_to_prev": "", "connector_phrase": "", "use_clusters": ["Agent frameworks / architectures"], "rq": "Which design choices in Memory and retrieval (RAG) drive the major trade-offs, and how are those trade-offs measured?"}, {"para": 2, "argument_role": "mechanism_cluster_A", "intent": "Explain cluster A: core mechanism and system architecture and what decision it makes in the agent loop.", "focus": ["cluster: Agent frameworks / architectures", "core mechanism and system architecture", "assumptions"], "connector_to_prev": "grounding", "connector_phrase": "baseline route (Agent frameworks / architectures)", "use_clusters": ["Agent frameworks / architectures"]}, {"para": 3, "argument_role": "implementation_cluster_A", "intent": "Cluster A implementation details: training and data signals and interface contract (tools/memory) that constrain behavior.", "focus": ["cluster: Agent frameworks / architectures", "training and data setup", "interface contract", "axes: evaluation protocol (datasets, metrics, human evaluation), compute and latency constraints, tool interface contract (schemas / protocols), tool selection / routing policy, sandboxing / permissions / observability"], "connector_to_prev": "elaboration", "connector_phrase": "implementation assumptions (interface + training)", "use_clusters": ["Agent frameworks / architectures"]}, {"para": 4, "argument_role": "evaluation_cluster_A", "intent": "Cluster A evaluation/trade-offs: where it works, costs (compute/latency), and typical failure modes.", "focus": ["cluster: Agent frameworks / architectures", "evaluation anchor", "efficiency", "failure modes"], "connector_to_prev": "evaluation", "connector_phrase": "evaluation anchor (task/metric/constraint) + failure modes", "use_clusters": ["Agent frameworks / architectures"]}, {"para": 5, "argument_role": "contrast_cluster_B", "intent": "Explain cluster B (contrast with A): core mechanism and system architecture and what it optimizes for.", "focus": ["cluster: Memory / retrieval augmentation", "contrast with Agent frameworks / architectures", "core mechanism and system architecture"], "connector_to_prev": "contrast", "connector_phrase": "contrast route (Memory / retrieval augmentation vs Agent frameworks / architectures)", "use_clusters": ["Memory / retrieval augmentation"]}, {"para": 6, "argument_role": "implementation_cluster_B", "intent": "Cluster B implementation details: training and data and interface assumptions (mirror A for comparability).", "focus": ["cluster: Memory / retrieval augmentation", "training and data setup", "interface contract", "axes: evaluation protocol (datasets, metrics, human evaluation), compute and latency constraints, tool interface contract (schemas / protocols), tool selection / routing policy, sandboxing / permissions / observability"], "connector_to_prev": "elaboration", "connector_phrase": "contrast implementation assumptions (B)", "use_clusters": ["Memory / retrieval augmentation"]}, {"para": 7, "argument_role": "evaluation_cluster_B", "intent": "Cluster B evaluation/trade-offs: where it works, costs, and failure modes (mirror A).", "focus": ["cluster: Memory / retrieval augmentation", "evaluation anchor", "efficiency", "failure modes"], "connector_to_prev": "evaluation", "connector_phrase": "contrast evaluation anchor + trade-offs (B)", "use_clusters": ["Memory / retrieval augmentation"]}, {"para": 8, "argument_role": "cross_paper_synthesis", "intent": "Cross-paper synthesis: compare clusters along the main axes (include >=2 citations in one paragraph).", "focus": ["compare Agent frameworks / architectures vs Memory / retrieval augmentation", "multiple citations in one paragraph", "axes: evaluation protocol (datasets, metrics, human evaluation), compute and latency constraints, tool interface contract (schemas / protocols), tool selection / routing policy, sandboxing / permissions / observability"], "connector_to_prev": "synthesis", "connector_phrase": "cross-paper synthesis (Agent frameworks / architectures vs Memory / retrieval augmentation)", "use_clusters": ["Agent frameworks / architectures", "Memory / retrieval augmentation", "Planning / reasoning loops"]}, {"para": 9, "argument_role": "decision_guidance", "intent": "Decision guidance: when to choose which route (criteria + evaluation signals + engineering constraints).", "focus": ["decision checklist", "evaluation protocol", "practical constraints"], "connector_to_prev": "consequence", "connector_phrase": "decision guidance / criteria", "use_clusters": ["Agent frameworks / architectures", "Memory / retrieval augmentation", "Planning / reasoning loops"]}, {"para": 10, "argument_role": "limitations_open_questions", "intent": "Limitations + verification targets; end with a concrete open question to hand off.", "focus": ["limitations", "evidence mode: provisional", "what needs verification", "open question"], "connector_to_prev": "limitations", "connector_phrase": "limitations + verification targets", "use_clusters": ["Agent frameworks / architectures", "Memory / retrieval augmentation", "Planning / reasoning loops"], "policy": "Use conservative language; avoid strong conclusions; prefer questions-to-answer + explicit evidence gaps list."}], "evidence_level_summary": {"fulltext": 0, "abstract": 28, "title": 0}, "generated_at": "2026-01-25T17:05:38"}
{"sub_id": "5.1", "title": "Self-improvement and adaptation", "section_id": "5", "section_title": "Learning, Adaptation & Coordination", "rq": "Which design choices in Self-improvement and adaptation drive the major trade-offs, and how are those trade-offs measured?", "thesis": "Self-improvement and adaptation highlights a tension around evaluation protocol (datasets, metrics, human evaluation) and compute and latency constraints, motivating a protocol-aware synthesis rather than per-paper summaries.", "scope_rule": {"include": ["Core topics directly relevant to 'Self-improvement and adaptation'."], "exclude": [], "notes": "If you include an out-of-scope paper as a bridge, state the reason in 1 sentence and keep it secondary."}, "axes": ["evaluation protocol (datasets, metrics, human evaluation)", "compute and latency constraints", "communication protocol / roles", "aggregation (vote / debate / referee)", "stability / robustness"], "bridge_terms": ["preference", "reward", "feedback", "self-improvement", "benchmarks/metrics", "compute"], "contrast_hook": "learning/feedback", "tension_statement": "In Self-improvement and adaptation, the core trade-off is adaptability versus stability: systems that change themselves can improve over time but risk drifting, overfitting, or becoming harder to evaluate and control.", "evaluation_anchor_minimal": {"task": "agent benchmark tasks", "metric": "success rate", "constraint": "budget/cost model"}, "required_evidence_fields": ["benchmarks/datasets", "metrics / human-eval protocol", "compute / cost (train/infer)", "training signal / supervision", "failure modes and limitations"], "clusters": [{"label": "Agent frameworks / architectures", "rationale": "Grouped by keyword tag `agents` from titles (bootstrap).", "paper_ids": ["P0042", "P0050", "P0051", "P0079", "P0084", "P0087", "P0133", "P0144"], "bibkeys": ["Wei2026Agentic", "Jiang2025Agentic", "Belle2025Agents", "Li2025Learn", "Bilal2025Meta", "Xia2025Sand", "Van2025Survey", "Zhang2025Agentic"]}, {"label": "Planning / reasoning loops", "rationale": "Grouped by keyword tag `planning` from titles (bootstrap).", "paper_ids": ["P0042", "P0051", "P0136", "P0208"], "bibkeys": ["Wei2026Agentic", "Belle2025Agents", "Chen2025Survey", "Lu2025Pilotrl"]}, {"label": "Code agents / software tasks", "rationale": "Grouped by keyword tag `code` from titles (bootstrap).", "paper_ids": ["P0050", "P0221", "P0099"], "bibkeys": ["Jiang2025Agentic", "Sarkar2025Survey", "Jin2024From"]}], "paragraph_plan": [{"para": 1, "argument_role": "setup_thesis", "intent": "Define scope, setup, and the subsection thesis (no pipeline jargon).", "focus": ["scope boundary", "key definitions", "thesis vs neighboring subsections"], "connector_to_prev": "", "connector_phrase": "", "use_clusters": ["Agent frameworks / architectures"], "rq": "Which design choices in Self-improvement and adaptation drive the major trade-offs, and how are those trade-offs measured?"}, {"para": 2, "argument_role": "mechanism_cluster_A", "intent": "Explain cluster A: core mechanism and system architecture and what decision it makes in the agent loop.", "focus": ["cluster: Agent frameworks / architectures", "core mechanism and system architecture", "assumptions"], "connector_to_prev": "grounding", "connector_phrase": "baseline route (Agent frameworks / architectures)", "use_clusters": ["Agent frameworks / architectures"]}, {"para": 3, "argument_role": "implementation_cluster_A", "intent": "Cluster A implementation details: training and data signals and interface contract (tools/memory) that constrain behavior.", "focus": ["cluster: Agent frameworks / architectures", "training and data setup", "interface contract", "axes: evaluation protocol (datasets, metrics, human evaluation), compute and latency constraints, communication protocol / roles, aggregation (vote / debate / referee), stability / robustness"], "connector_to_prev": "elaboration", "connector_phrase": "implementation assumptions (interface + training)", "use_clusters": ["Agent frameworks / architectures"]}, {"para": 4, "argument_role": "evaluation_cluster_A", "intent": "Cluster A evaluation/trade-offs: where it works, costs (compute/latency), and typical failure modes.", "focus": ["cluster: Agent frameworks / architectures", "evaluation anchor", "efficiency", "failure modes"], "connector_to_prev": "evaluation", "connector_phrase": "evaluation anchor (task/metric/constraint) + failure modes", "use_clusters": ["Agent frameworks / architectures"]}, {"para": 5, "argument_role": "contrast_cluster_B", "intent": "Explain cluster B (contrast with A): core mechanism and system architecture and what it optimizes for.", "focus": ["cluster: Planning / reasoning loops", "contrast with Agent frameworks / architectures", "core mechanism and system architecture"], "connector_to_prev": "contrast", "connector_phrase": "contrast route (Planning / reasoning loops vs Agent frameworks / architectures)", "use_clusters": ["Planning / reasoning loops"]}, {"para": 6, "argument_role": "implementation_cluster_B", "intent": "Cluster B implementation details: training and data and interface assumptions (mirror A for comparability).", "focus": ["cluster: Planning / reasoning loops", "training and data setup", "interface contract", "axes: evaluation protocol (datasets, metrics, human evaluation), compute and latency constraints, communication protocol / roles, aggregation (vote / debate / referee), stability / robustness"], "connector_to_prev": "elaboration", "connector_phrase": "contrast implementation assumptions (B)", "use_clusters": ["Planning / reasoning loops"]}, {"para": 7, "argument_role": "evaluation_cluster_B", "intent": "Cluster B evaluation/trade-offs: where it works, costs, and failure modes (mirror A).", "focus": ["cluster: Planning / reasoning loops", "evaluation anchor", "efficiency", "failure modes"], "connector_to_prev": "evaluation", "connector_phrase": "contrast evaluation anchor + trade-offs (B)", "use_clusters": ["Planning / reasoning loops"]}, {"para": 8, "argument_role": "cross_paper_synthesis", "intent": "Cross-paper synthesis: compare clusters along the main axes (include >=2 citations in one paragraph).", "focus": ["compare Agent frameworks / architectures vs Planning / reasoning loops", "multiple citations in one paragraph", "axes: evaluation protocol (datasets, metrics, human evaluation), compute and latency constraints, communication protocol / roles, aggregation (vote / debate / referee), stability / robustness"], "connector_to_prev": "synthesis", "connector_phrase": "cross-paper synthesis (Agent frameworks / architectures vs Planning / reasoning loops)", "use_clusters": ["Agent frameworks / architectures", "Planning / reasoning loops", "Code agents / software tasks"]}, {"para": 9, "argument_role": "decision_guidance", "intent": "Decision guidance: when to choose which route (criteria + evaluation signals + engineering constraints).", "focus": ["decision checklist", "evaluation protocol", "practical constraints"], "connector_to_prev": "consequence", "connector_phrase": "decision guidance / criteria", "use_clusters": ["Agent frameworks / architectures", "Planning / reasoning loops", "Code agents / software tasks"]}, {"para": 10, "argument_role": "limitations_open_questions", "intent": "Limitations + verification targets; end with a concrete open question to hand off.", "focus": ["limitations", "evidence mode: provisional", "what needs verification", "open question"], "connector_to_prev": "limitations", "connector_phrase": "limitations + verification targets", "use_clusters": ["Agent frameworks / architectures", "Planning / reasoning loops", "Code agents / software tasks"], "policy": "Use conservative language; avoid strong conclusions; prefer questions-to-answer + explicit evidence gaps list."}], "evidence_level_summary": {"fulltext": 0, "abstract": 28, "title": 0}, "generated_at": "2026-01-25T17:05:38"}
{"sub_id": "5.2", "title": "Multi-agent coordination", "section_id": "5", "section_title": "Learning, Adaptation & Coordination", "rq": "Which design choices in Multi-agent coordination drive the major trade-offs, and how are those trade-offs measured?", "thesis": "Multi-agent coordination highlights a tension around evaluation protocol (datasets, metrics, human evaluation) and compute and latency constraints, motivating a protocol-aware synthesis rather than per-paper summaries.", "scope_rule": {"include": ["Core topics directly relevant to 'Multi-agent coordination'."], "exclude": [], "notes": "If you include an out-of-scope paper as a bridge, state the reason in 1 sentence and keep it secondary."}, "axes": ["evaluation protocol (datasets, metrics, human evaluation)", "compute and latency constraints", "tool interface contract (schemas / protocols)", "tool selection / routing policy", "sandboxing / permissions / observability"], "bridge_terms": ["roles", "communication", "debate", "aggregation", "stability", "benchmarks/metrics"], "contrast_hook": "coordination", "tension_statement": "In Multi-agent coordination, the central trade-off is specialization versus coordination: dividing labor can boost performance but adds communication overhead and stability risks.", "evaluation_anchor_minimal": {"task": "attack/defense evaluation", "metric": "attack success rate", "constraint": "policy/sandbox setting"}, "required_evidence_fields": ["benchmarks/datasets", "metrics / human-eval protocol", "compute / cost (train/infer)", "training signal / supervision", "threat model", "defense surface"], "clusters": [{"label": "Agent frameworks / architectures", "rationale": "Grouped by keyword tag `agents` from titles (bootstrap).", "paper_ids": ["P0012", "P0070", "P0078", "P0079", "P0080", "P0084", "P0085", "P0139"], "bibkeys": ["Aratchige2025Llms", "Zahedifar2025Agent", "Rouzrokh2025Lattereview", "Li2025Learn", "Collini2025Marvel", "Bilal2025Meta", "Wu2025Multi", "Masters2025Arcane"]}, {"label": "Multi-agent coordination", "rationale": "Grouped by keyword tag `multi-agent` from titles (bootstrap).", "paper_ids": ["P0070", "P0078", "P0079", "P0080", "P0084", "P0085", "P0139", "P0149"], "bibkeys": ["Zahedifar2025Agent", "Rouzrokh2025Lattereview", "Li2025Learn", "Collini2025Marvel", "Bilal2025Meta", "Wu2025Multi", "Masters2025Arcane", "Wang2025Autoscore"]}, {"label": "Planning / reasoning loops", "rationale": "Grouped by keyword tag `planning` from titles (bootstrap).", "paper_ids": ["P0183", "P0212", "P0255", "P0280", "P0119"], "bibkeys": ["Xu2025Autonomous", "Chen2025Schema", "Inoue2024Drugagent", "Motwani2024Malt", "Xu2023Towards"]}], "paragraph_plan": [{"para": 1, "argument_role": "setup_thesis", "intent": "Define scope, setup, and the subsection thesis (no pipeline jargon).", "focus": ["scope boundary", "key definitions", "thesis vs neighboring subsections"], "connector_to_prev": "", "connector_phrase": "", "use_clusters": ["Agent frameworks / architectures"], "rq": "Which design choices in Multi-agent coordination drive the major trade-offs, and how are those trade-offs measured?"}, {"para": 2, "argument_role": "mechanism_cluster_A", "intent": "Explain cluster A: core mechanism and system architecture and what decision it makes in the agent loop.", "focus": ["cluster: Agent frameworks / architectures", "core mechanism and system architecture", "assumptions"], "connector_to_prev": "grounding", "connector_phrase": "baseline route (Agent frameworks / architectures)", "use_clusters": ["Agent frameworks / architectures"]}, {"para": 3, "argument_role": "implementation_cluster_A", "intent": "Cluster A implementation details: training and data signals and interface contract (tools/memory) that constrain behavior.", "focus": ["cluster: Agent frameworks / architectures", "training and data setup", "interface contract", "axes: evaluation protocol (datasets, metrics, human evaluation), compute and latency constraints, tool interface contract (schemas / protocols), tool selection / routing policy, sandboxing / permissions / observability"], "connector_to_prev": "elaboration", "connector_phrase": "implementation assumptions (interface + training)", "use_clusters": ["Agent frameworks / architectures"]}, {"para": 4, "argument_role": "evaluation_cluster_A", "intent": "Cluster A evaluation/trade-offs: where it works, costs (compute/latency), and typical failure modes.", "focus": ["cluster: Agent frameworks / architectures", "evaluation anchor", "efficiency", "failure modes"], "connector_to_prev": "evaluation", "connector_phrase": "evaluation anchor (task/metric/constraint) + failure modes", "use_clusters": ["Agent frameworks / architectures"]}, {"para": 5, "argument_role": "contrast_cluster_B", "intent": "Explain cluster B (contrast with A): core mechanism and system architecture and what it optimizes for.", "focus": ["cluster: Multi-agent coordination", "contrast with Agent frameworks / architectures", "core mechanism and system architecture"], "connector_to_prev": "contrast", "connector_phrase": "contrast route (Multi-agent coordination vs Agent frameworks / architectures)", "use_clusters": ["Multi-agent coordination"]}, {"para": 6, "argument_role": "implementation_cluster_B", "intent": "Cluster B implementation details: training and data and interface assumptions (mirror A for comparability).", "focus": ["cluster: Multi-agent coordination", "training and data setup", "interface contract", "axes: evaluation protocol (datasets, metrics, human evaluation), compute and latency constraints, tool interface contract (schemas / protocols), tool selection / routing policy, sandboxing / permissions / observability"], "connector_to_prev": "elaboration", "connector_phrase": "contrast implementation assumptions (B)", "use_clusters": ["Multi-agent coordination"]}, {"para": 7, "argument_role": "evaluation_cluster_B", "intent": "Cluster B evaluation/trade-offs: where it works, costs, and failure modes (mirror A).", "focus": ["cluster: Multi-agent coordination", "evaluation anchor", "efficiency", "failure modes"], "connector_to_prev": "evaluation", "connector_phrase": "contrast evaluation anchor + trade-offs (B)", "use_clusters": ["Multi-agent coordination"]}, {"para": 8, "argument_role": "cross_paper_synthesis", "intent": "Cross-paper synthesis: compare clusters along the main axes (include >=2 citations in one paragraph).", "focus": ["compare Agent frameworks / architectures vs Multi-agent coordination", "multiple citations in one paragraph", "axes: evaluation protocol (datasets, metrics, human evaluation), compute and latency constraints, tool interface contract (schemas / protocols), tool selection / routing policy, sandboxing / permissions / observability"], "connector_to_prev": "synthesis", "connector_phrase": "cross-paper synthesis (Agent frameworks / architectures vs Multi-agent coordination)", "use_clusters": ["Agent frameworks / architectures", "Multi-agent coordination", "Planning / reasoning loops"]}, {"para": 9, "argument_role": "decision_guidance", "intent": "Decision guidance: when to choose which route (criteria + evaluation signals + engineering constraints).", "focus": ["decision checklist", "evaluation protocol", "practical constraints"], "connector_to_prev": "consequence", "connector_phrase": "decision guidance / criteria", "use_clusters": ["Agent frameworks / architectures", "Multi-agent coordination", "Planning / reasoning loops"]}, {"para": 10, "argument_role": "limitations_open_questions", "intent": "Limitations + verification targets; end with a concrete open question to hand off.", "focus": ["limitations", "evidence mode: provisional", "what needs verification", "open question"], "connector_to_prev": "limitations", "connector_phrase": "limitations + verification targets", "use_clusters": ["Agent frameworks / architectures", "Multi-agent coordination", "Planning / reasoning loops"], "policy": "Use conservative language; avoid strong conclusions; prefer questions-to-answer + explicit evidence gaps list."}], "evidence_level_summary": {"fulltext": 0, "abstract": 28, "title": 0}, "generated_at": "2026-01-25T17:05:38"}
{"sub_id": "6.1", "title": "Benchmarks and evaluation protocols", "section_id": "6", "section_title": "Evaluation & Risks", "rq": "Which design choices in Benchmarks and evaluation protocols drive the major trade-offs, and how are those trade-offs measured?", "thesis": "Benchmarks and evaluation protocols methods emphasize evaluation protocol (datasets, metrics, human evaluation) and compute and latency constraints trade-offs, but synthesis is clearest when claims are tied to explicit evaluation settings and reporting conventions.", "scope_rule": {"include": ["Core topics directly relevant to 'Benchmarks and evaluation protocols'."], "exclude": [], "notes": "If you include an out-of-scope paper as a bridge, state the reason in 1 sentence and keep it secondary."}, "axes": ["evaluation protocol (datasets, metrics, human evaluation)", "compute and latency constraints", "tool interface contract (schemas / protocols)", "tool selection / routing policy", "sandboxing / permissions / observability"], "bridge_terms": ["function calling", "tool schema", "routing", "sandbox", "observability", "benchmarks"], "contrast_hook": "tool interfaces", "tension_statement": "In Benchmarks and evaluation protocols, a recurring tension is coverage versus comparability: broader suites capture more behaviors but make head-to-head comparison fragile when protocols and constraints differ.", "evaluation_anchor_minimal": {"task": "attack/defense evaluation", "metric": "attack success rate", "constraint": "policy/sandbox setting"}, "required_evidence_fields": ["benchmarks/datasets", "metrics / human-eval protocol", "compute / cost (train/infer)", "training signal / supervision", "threat model", "defense surface"], "clusters": [{"label": "Agent frameworks / architectures", "rationale": "Grouped by keyword tag `agents` from titles (bootstrap).", "paper_ids": ["P0122", "P0124", "P0006", "P0010", "P0028", "P0033", "P0086", "P0133"], "bibkeys": ["Kim2026Beyond", "Zhang2026Evoroute", "Tang2025Empowering", "Zhang2025Generalizability", "Dong2025Compressed", "Zhu2025Where", "Shi2025Progent", "Van2025Survey"]}, {"label": "Evaluation / benchmark-focused works", "rationale": "Grouped by keyword tag `evaluation` from titles (bootstrap).", "paper_ids": ["P0122", "P0006", "P0028", "P0162", "P0174", "P0181", "P0193", "P0216"], "bibkeys": ["Kim2026Beyond", "Tang2025Empowering", "Dong2025Compressed", "Zhang2025Datascibench", "Zhu2025Evolutionary", "Rahman2025Hallucination", "Wu2025Lessons", "Seo2025Simuhome"]}, {"label": "Multi-agent coordination", "rationale": "Grouped by keyword tag `multi-agent` from titles (bootstrap).", "paper_ids": ["P0149", "P0177", "P0193", "P0248"], "bibkeys": ["Wang2025Autoscore", "Shen2025Feat", "Wu2025Lessons", "Trirat2024Automl"]}], "paragraph_plan": [{"para": 1, "argument_role": "setup_thesis", "intent": "Define scope, setup, and the subsection thesis (no pipeline jargon).", "focus": ["scope boundary", "key definitions", "thesis vs neighboring subsections"], "connector_to_prev": "", "connector_phrase": "", "use_clusters": ["Agent frameworks / architectures"], "rq": "Which design choices in Benchmarks and evaluation protocols drive the major trade-offs, and how are those trade-offs measured?"}, {"para": 2, "argument_role": "mechanism_cluster_A", "intent": "Explain cluster A: core mechanism and system architecture and what decision it makes in the agent loop.", "focus": ["cluster: Agent frameworks / architectures", "core mechanism and system architecture", "assumptions"], "connector_to_prev": "grounding", "connector_phrase": "baseline route (Agent frameworks / architectures)", "use_clusters": ["Agent frameworks / architectures"]}, {"para": 3, "argument_role": "implementation_cluster_A", "intent": "Cluster A implementation details: training and data signals and interface contract (tools/memory) that constrain behavior.", "focus": ["cluster: Agent frameworks / architectures", "training and data setup", "interface contract", "axes: evaluation protocol (datasets, metrics, human evaluation), compute and latency constraints, tool interface contract (schemas / protocols), tool selection / routing policy, sandboxing / permissions / observability"], "connector_to_prev": "elaboration", "connector_phrase": "implementation assumptions (interface + training)", "use_clusters": ["Agent frameworks / architectures"]}, {"para": 4, "argument_role": "evaluation_cluster_A", "intent": "Cluster A evaluation/trade-offs: where it works, costs (compute/latency), and typical failure modes.", "focus": ["cluster: Agent frameworks / architectures", "evaluation anchor", "efficiency", "failure modes"], "connector_to_prev": "evaluation", "connector_phrase": "evaluation anchor (task/metric/constraint) + failure modes", "use_clusters": ["Agent frameworks / architectures"]}, {"para": 5, "argument_role": "contrast_cluster_B", "intent": "Explain cluster B (contrast with A): core mechanism and system architecture and what it optimizes for.", "focus": ["cluster: Evaluation / benchmark-focused works", "contrast with Agent frameworks / architectures", "core mechanism and system architecture"], "connector_to_prev": "contrast", "connector_phrase": "contrast route (Evaluation / benchmark-focused works vs Agent frameworks / architectures)", "use_clusters": ["Evaluation / benchmark-focused works"]}, {"para": 6, "argument_role": "implementation_cluster_B", "intent": "Cluster B implementation details: training and data and interface assumptions (mirror A for comparability).", "focus": ["cluster: Evaluation / benchmark-focused works", "training and data setup", "interface contract", "axes: evaluation protocol (datasets, metrics, human evaluation), compute and latency constraints, tool interface contract (schemas / protocols), tool selection / routing policy, sandboxing / permissions / observability"], "connector_to_prev": "elaboration", "connector_phrase": "contrast implementation assumptions (B)", "use_clusters": ["Evaluation / benchmark-focused works"]}, {"para": 7, "argument_role": "evaluation_cluster_B", "intent": "Cluster B evaluation/trade-offs: where it works, costs, and failure modes (mirror A).", "focus": ["cluster: Evaluation / benchmark-focused works", "evaluation anchor", "efficiency", "failure modes"], "connector_to_prev": "evaluation", "connector_phrase": "contrast evaluation anchor + trade-offs (B)", "use_clusters": ["Evaluation / benchmark-focused works"]}, {"para": 8, "argument_role": "cross_paper_synthesis", "intent": "Cross-paper synthesis: compare clusters along the main axes (include >=2 citations in one paragraph).", "focus": ["compare Agent frameworks / architectures vs Evaluation / benchmark-focused works", "multiple citations in one paragraph", "axes: evaluation protocol (datasets, metrics, human evaluation), compute and latency constraints, tool interface contract (schemas / protocols), tool selection / routing policy, sandboxing / permissions / observability"], "connector_to_prev": "synthesis", "connector_phrase": "cross-paper synthesis (Agent frameworks / architectures vs Evaluation / benchmark-focused works)", "use_clusters": ["Agent frameworks / architectures", "Evaluation / benchmark-focused works", "Multi-agent coordination"]}, {"para": 9, "argument_role": "decision_guidance", "intent": "Decision guidance: when to choose which route (criteria + evaluation signals + engineering constraints).", "focus": ["decision checklist", "evaluation protocol", "practical constraints"], "connector_to_prev": "consequence", "connector_phrase": "decision guidance / criteria", "use_clusters": ["Agent frameworks / architectures", "Evaluation / benchmark-focused works", "Multi-agent coordination"]}, {"para": 10, "argument_role": "limitations_open_questions", "intent": "Limitations + verification targets; end with a concrete open question to hand off.", "focus": ["limitations", "evidence mode: provisional", "what needs verification", "open question"], "connector_to_prev": "limitations", "connector_phrase": "limitations + verification targets", "use_clusters": ["Agent frameworks / architectures", "Evaluation / benchmark-focused works", "Multi-agent coordination"], "policy": "Use conservative language; avoid strong conclusions; prefer questions-to-answer + explicit evidence gaps list."}], "evidence_level_summary": {"fulltext": 0, "abstract": 28, "title": 0}, "generated_at": "2026-01-25T17:05:38"}
{"sub_id": "6.2", "title": "Safety, security, and governance", "section_id": "6", "section_title": "Evaluation & Risks", "rq": "Which design choices in Safety, security, and governance drive the major trade-offs, and how are those trade-offs measured?", "thesis": "Safety, security, and governance highlights a tension around evaluation protocol (datasets, metrics, human evaluation) and compute and latency constraints, motivating a protocol-aware synthesis rather than per-paper summaries.", "scope_rule": {"include": ["Core topics directly relevant to 'Safety, security, and governance'."], "exclude": [], "notes": "If you include an out-of-scope paper as a bridge, state the reason in 1 sentence and keep it secondary."}, "axes": ["evaluation protocol (datasets, metrics, human evaluation)", "compute and latency constraints", "tool interface contract (schemas / protocols)", "tool selection / routing policy", "sandboxing / permissions / observability"], "bridge_terms": ["threat model", "prompt/tool injection", "monitoring", "guardrails", "benchmarks/metrics", "compute"], "contrast_hook": "security", "tension_statement": "In Safety, security, and governance, a key tension is capability versus safety: stronger agent actions increase utility but widen the attack surface and raise containment requirements.", "evaluation_anchor_minimal": {"task": "attack/defense evaluation", "metric": "attack success rate", "constraint": "policy/sandbox setting"}, "required_evidence_fields": ["benchmarks/datasets", "metrics / human-eval protocol", "compute / cost (train/infer)", "training signal / supervision", "threat model", "defense surface"], "clusters": [{"label": "Agent frameworks / architectures", "rationale": "Grouped by keyword tag `agents` from titles (bootstrap).", "paper_ids": ["P0126", "P0008", "P0030", "P0048", "P0058", "P0075", "P0086", "P0129"], "bibkeys": ["Liu2026Agents", "Plaat2025Agentic", "Zhou2025Reasoning", "Kang2025Acon", "Gasmi2025Bridging", "Han2025Large", "Shi2025Progent", "Wang2025Comprehensive"]}, {"label": "Safety / security / guardrails", "rationale": "Grouped by keyword tag `security` from titles (bootstrap).", "paper_ids": ["P0058", "P0129", "P0142", "P0145", "P0158", "P0195", "P0213", "P0239"], "bibkeys": ["Gasmi2025Bridging", "Wang2025Comprehensive", "Wang2025Agentvigil", "Rosario2025Architecting", "Bonagiri2025Check", "Zhang2025Security", "Liu2025Secure", "Hadeliya2025When"]}, {"label": "Tool-use and function calling", "rationale": "Grouped by keyword tag `tool-use` from titles (bootstrap).", "paper_ids": ["P0081", "P0146", "P0155", "P0195", "P0196"], "bibkeys": ["Gao2025Radar", "Mo2025Attractive", "Weng2025Bridgescope", "Zhang2025Security", "Luo2025Universe"]}], "paragraph_plan": [{"para": 1, "argument_role": "setup_thesis", "intent": "Define scope, setup, and the subsection thesis (no pipeline jargon).", "focus": ["scope boundary", "key definitions", "thesis vs neighboring subsections"], "connector_to_prev": "", "connector_phrase": "", "use_clusters": ["Agent frameworks / architectures"], "rq": "Which design choices in Safety, security, and governance drive the major trade-offs, and how are those trade-offs measured?"}, {"para": 2, "argument_role": "mechanism_cluster_A", "intent": "Explain cluster A: core mechanism and system architecture and what decision it makes in the agent loop.", "focus": ["cluster: Agent frameworks / architectures", "core mechanism and system architecture", "assumptions"], "connector_to_prev": "grounding", "connector_phrase": "baseline route (Agent frameworks / architectures)", "use_clusters": ["Agent frameworks / architectures"]}, {"para": 3, "argument_role": "implementation_cluster_A", "intent": "Cluster A implementation details: training and data signals and interface contract (tools/memory) that constrain behavior.", "focus": ["cluster: Agent frameworks / architectures", "training and data setup", "interface contract", "axes: evaluation protocol (datasets, metrics, human evaluation), compute and latency constraints, tool interface contract (schemas / protocols), tool selection / routing policy, sandboxing / permissions / observability"], "connector_to_prev": "elaboration", "connector_phrase": "implementation assumptions (interface + training)", "use_clusters": ["Agent frameworks / architectures"]}, {"para": 4, "argument_role": "evaluation_cluster_A", "intent": "Cluster A evaluation/trade-offs: where it works, costs (compute/latency), and typical failure modes.", "focus": ["cluster: Agent frameworks / architectures", "evaluation anchor", "efficiency", "failure modes"], "connector_to_prev": "evaluation", "connector_phrase": "evaluation anchor (task/metric/constraint) + failure modes", "use_clusters": ["Agent frameworks / architectures"]}, {"para": 5, "argument_role": "contrast_cluster_B", "intent": "Explain cluster B (contrast with A): core mechanism and system architecture and what it optimizes for.", "focus": ["cluster: Safety / security / guardrails", "contrast with Agent frameworks / architectures", "core mechanism and system architecture"], "connector_to_prev": "contrast", "connector_phrase": "contrast route (Safety / security / guardrails vs Agent frameworks / architectures)", "use_clusters": ["Safety / security / guardrails"]}, {"para": 6, "argument_role": "implementation_cluster_B", "intent": "Cluster B implementation details: training and data and interface assumptions (mirror A for comparability).", "focus": ["cluster: Safety / security / guardrails", "training and data setup", "interface contract", "axes: evaluation protocol (datasets, metrics, human evaluation), compute and latency constraints, tool interface contract (schemas / protocols), tool selection / routing policy, sandboxing / permissions / observability"], "connector_to_prev": "elaboration", "connector_phrase": "contrast implementation assumptions (B)", "use_clusters": ["Safety / security / guardrails"]}, {"para": 7, "argument_role": "evaluation_cluster_B", "intent": "Cluster B evaluation/trade-offs: where it works, costs, and failure modes (mirror A).", "focus": ["cluster: Safety / security / guardrails", "evaluation anchor", "efficiency", "failure modes"], "connector_to_prev": "evaluation", "connector_phrase": "contrast evaluation anchor + trade-offs (B)", "use_clusters": ["Safety / security / guardrails"]}, {"para": 8, "argument_role": "cross_paper_synthesis", "intent": "Cross-paper synthesis: compare clusters along the main axes (include >=2 citations in one paragraph).", "focus": ["compare Agent frameworks / architectures vs Safety / security / guardrails", "multiple citations in one paragraph", "axes: evaluation protocol (datasets, metrics, human evaluation), compute and latency constraints, tool interface contract (schemas / protocols), tool selection / routing policy, sandboxing / permissions / observability"], "connector_to_prev": "synthesis", "connector_phrase": "cross-paper synthesis (Agent frameworks / architectures vs Safety / security / guardrails)", "use_clusters": ["Agent frameworks / architectures", "Safety / security / guardrails", "Tool-use and function calling"]}, {"para": 9, "argument_role": "decision_guidance", "intent": "Decision guidance: when to choose which route (criteria + evaluation signals + engineering constraints).", "focus": ["decision checklist", "evaluation protocol", "practical constraints"], "connector_to_prev": "consequence", "connector_phrase": "decision guidance / criteria", "use_clusters": ["Agent frameworks / architectures", "Safety / security / guardrails", "Tool-use and function calling"]}, {"para": 10, "argument_role": "limitations_open_questions", "intent": "Limitations + verification targets; end with a concrete open question to hand off.", "focus": ["limitations", "evidence mode: provisional", "what needs verification", "open question"], "connector_to_prev": "limitations", "connector_phrase": "limitations + verification targets", "use_clusters": ["Agent frameworks / architectures", "Safety / security / guardrails", "Tool-use and function calling"], "policy": "Use conservative language; avoid strong conclusions; prefer questions-to-answer + explicit evidence gaps list."}], "evidence_level_summary": {"fulltext": 0, "abstract": 28, "title": 0}, "generated_at": "2026-01-25T17:05:38"}
