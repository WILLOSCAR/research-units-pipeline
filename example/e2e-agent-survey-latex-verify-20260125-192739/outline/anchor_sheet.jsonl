{"sub_id": "3.1", "title": "Agent loop and action spaces", "anchors": [{"hook_type": "quant", "text": "On evaluation, we examine over 190 benchmark datasets and trace a shift from static exams toward process- and discovery-oriented assessments with advanced evaluation protocols.", "citations": ["Hu2025Survey"], "paper_id": "P0045", "evidence_id": "E-P0045-7219ca15f4", "pointer": "papers/paper_notes.jsonl:paper_id=P0045#key_results[1]"}, {"hook_type": "quant", "text": "Evaluated across a comprehensive set of seven benchmarks spanning embodied, math, web, tool, and game domains, AgentSwift discovers agents that achieve an average performance gain of 8.34\\% over both existing automated agent search methods and manually designed agents.", "citations": ["Li2025Agentswift"], "paper_id": "P0049", "evidence_id": "E-P0049-904ba35500", "pointer": "papers/paper_notes.jsonl:paper_id=P0049#key_results[0]"}, {"hook_type": "quant", "text": "Extensive experiments across six benchmarks, covering the diverse scenarios of web, embodied, tool use and game applications, show that AgentSquare substantially outperforms hand-crafted agents, achieving an average performance gain of 17.2% against best-known human designs.", "citations": ["Shang2024Agentsquare"], "paper_id": "P0037", "evidence_id": "E-P0037-38a26e4777", "pointer": "papers/paper_notes.jsonl:paper_id=P0037#key_results[0]"}, {"hook_type": "quant", "text": "To evaluate MuaLLM, we introduce two custom datasets: RAG-250, targeting retrieval and citation performance, and Reasoning-100 (Reas-100), focused on multistep reasoning in circuit design.", "citations": ["Abbineni2025Muallm"], "paper_id": "P0203", "evidence_id": "E-P0203-e294aeefb5", "pointer": "papers/paper_notes.jsonl:paper_id=P0203#key_results[1]"}, {"hook_type": "quant", "text": "Comprehensive evaluations on multiple benchmarks show that A-MemGuard effectively cuts attack success rates by over 95% while incurring a minimal utility cost.", "citations": ["Wei2025Memguard"], "paper_id": "P0047", "evidence_id": "E-P0047-cb9370ac71", "pointer": "papers/paper_notes.jsonl:paper_id=P0047#key_results[0]"}, {"hook_type": "quant", "text": "Experiments on challenging agentic benchmarks such as GAIA and BrowseComp+ demonstrate that EvoRoute, when integrated into off-the-shelf agentic systems, not only sustains or enhances system performance but also reduces execution cost by up to $80\\%$ and latency by over $70\\%$.", "citations": ["Zhang2026Evoroute"], "paper_id": "P0124", "evidence_id": "E-P0124-60cc0d458f", "pointer": "papers/paper_notes.jsonl:paper_id=P0124#key_results[0]"}, {"hook_type": "quant", "text": "We evaluate GiGPO on challenging agent benchmarks, including ALFWorld and WebShop, as well as tool-integrated reasoning on search-augmented QA tasks, using Qwen2.5-1.5B/3B/7B-Instruct.", "citations": ["Feng2025Group"], "paper_id": "P0029", "evidence_id": "E-P0029-4b027dfb27", "pointer": "papers/paper_notes.jsonl:paper_id=P0029#key_results[0]"}, {"hook_type": "quant", "text": "Function Calling showed higher overall attack success rates (73.5% vs 62.59% for MCP), with greater system-centric vulnerability while MCP exhibited increased LLM-centric exposure.", "citations": ["Gasmi2025Bridging"], "paper_id": "P0058", "evidence_id": "E-P0058-8e34a29629", "pointer": "papers/paper_notes.jsonl:paper_id=P0058#key_results[0]"}, {"hook_type": "quant", "text": "On two interactive decision making benchmarks (ALFWorld and WebShop), ReAct outperforms imitation and reinforcement learning methods by an absolute success rate of 34% and 10% respectively, while being prompted with only one or two in-context examples.", "citations": ["Yao2022React"], "paper_id": "P0001", "evidence_id": "E-P0001-ca4a00b5cf", "pointer": "papers/paper_notes.jsonl:paper_id=P0001#key_results[0]"}, {"hook_type": "quant", "text": "We conducted comprehensive experiments using a kinase inhibitor dataset, where our multi-agent LLM method outperformed the non-reasoning multi-agent model (GPT-4o mini) by 45% in F1 score (0.514 vs 0.355).", "citations": ["Inoue2024Drugagent"], "paper_id": "P0255", "evidence_id": "E-P0255-5e37132da5", "pointer": "papers/paper_notes.jsonl:paper_id=P0255#key_results[0]"}, {"hook_type": "quant", "text": "Extensive experiments demonstrate that only using 10K samples for tuning LLaMA-7B can outperform state-of-the-art methods using larger LLMs or more data, on both in-domain and out-domain datasets.", "citations": ["Jiang2024Agent"], "paper_id": "P0102", "evidence_id": "E-P0102-0cc318c2c4", "pointer": "papers/paper_notes.jsonl:paper_id=P0102#key_results[0]"}, {"hook_type": "eval", "text": "We use the recently released AircraftVerse dataset, which is especially suited for developing and evaluating large language models for designs.", "citations": ["Samplawski2025Agent"], "paper_id": "P0138", "evidence_id": "E-P0138-da6dd163cf", "pointer": "papers/paper_notes.jsonl:paper_id=P0138#key_results[0]"}], "generated_at": "2026-01-25T17:07:00", "section_id": "3", "section_title": "Foundations & Interfaces"}
{"sub_id": "3.2", "title": "Tool interfaces and orchestration", "anchors": [{"hook_type": "quant", "text": "Evaluating each MemTool mode across 13+ LLMs on the ScaleMCP benchmark, we conducted experiments over 100 consecutive user interactions, measuring tool removal ratios (short-term memory efficiency) and task completion accuracy.", "citations": ["Lumer2025Memtool"], "paper_id": "P0083", "evidence_id": "E-P0083-35271418ac", "pointer": "papers/paper_notes.jsonl:paper_id=P0083#key_results[0]"}, {"hook_type": "quant", "text": "Furthermore, we evaluated current benchmarks and assessment protocols and have provided an analysis of 68 publicly available datasets to assess the performance of LLM-based agents in various tasks.", "citations": ["Chowa2025From"], "paper_id": "P0009", "evidence_id": "E-P0009-e0bcca0d9e", "pointer": "papers/paper_notes.jsonl:paper_id=P0009#key_results[0]"}, {"hook_type": "quant", "text": "We find AvaTaR consistently outperforms state-of-the-art approaches across all seven tasks, exhibiting strong generalization ability when applied to novel cases and achieving an average relative improvement of 14% on the Hit@1 metric for the retrieval datasets and 13% for the", "citations": ["Wu2024Avatar"], "paper_id": "P0096", "evidence_id": "E-P0096-3575a7c673", "pointer": "papers/paper_notes.jsonl:paper_id=P0096#key_results[0]"}, {"hook_type": "quant", "text": "On two interactive decision making benchmarks (ALFWorld and WebShop), ReAct outperforms imitation and reinforcement learning methods by an absolute success rate of 34% and 10% respectively, while being prompted with only one or two in-context examples.", "citations": ["Yao2022React"], "paper_id": "P0001", "evidence_id": "E-P0001-ca4a00b5cf", "pointer": "papers/paper_notes.jsonl:paper_id=P0001#key_results[0]"}, {"hook_type": "quant", "text": "Across six LLMs on the ToolBench and BFCL benchmarks, our attack expands tasks into trajectories exceeding 60,000 tokens, inflates costs by up to 658x, and raises energy by 100-560x.", "citations": ["Zhou2026Beyond"], "paper_id": "P0022", "evidence_id": "E-P0022-b6b7af5a81", "pointer": "papers/paper_notes.jsonl:paper_id=P0022#key_results[0]"}, {"hook_type": "quant", "text": "Our evaluation of 66 real-world tools from the repositories of two major LLM agent development frameworks, LangChain and LlamaIndex, revealed a significant security concern: 75% are vulnerable to XTHP attacks, highlighting the prevalence of this threat.", "citations": ["Li2025Dissonances"], "paper_id": "P0192", "evidence_id": "E-P0192-fae121f81b", "pointer": "papers/paper_notes.jsonl:paper_id=P0192#key_results[0]"}, {"hook_type": "quant", "text": "Specifically, 1) the taxonomy defines environments/tasks, common LLM-profiled roles or LMPRs (policy models, evaluators, and dynamic models), and universally applicable workflows found in prior work, and 2) it enables a comparison of key perspectives on the implementations of", "citations": ["Li2024Review"], "paper_id": "P0016", "evidence_id": "E-P0016-dc2266b72d", "pointer": "papers/paper_notes.jsonl:paper_id=P0016#key_results[0]"}, {"hook_type": "quant", "text": "To evaluate different autonomy levels, we propose four LLM paradigms: (1) centralized cooperation, where a single LLM allocates tools to all agents; (2) centralized self-organization, where a central LLM autonomously activates agents while keeping others inactive; (3) decentraliz", "citations": ["Zhang2025Tool"], "paper_id": "P0229", "evidence_id": "E-P0229-92b9d774cc", "pointer": "papers/paper_notes.jsonl:paper_id=P0229#method"}, {"hook_type": "quant", "text": "Through extensive evaluation of leading LLMs, we find that even SOTA models such as GPT-5 (43.72%), Grok-4 (33.33%) and Claude-4.0-Sonnet (29.44%) exhibit significant performance limitations.", "citations": ["Luo2025Universe"], "paper_id": "P0196", "evidence_id": "E-P0196-3c65d38a2a", "pointer": "papers/paper_notes.jsonl:paper_id=P0196#limitations[1]"}, {"hook_type": "quant", "text": "Evaluation on two existing multi-turn tool-use agent benchmarks, M3ToolEval and TauBench, shows the Self-Challenging framework achieves over a two-fold improvement in Llama-3.1-8B-Instruct, despite using only self-generated training data.", "citations": ["Zhou2025Self"], "paper_id": "P0214", "evidence_id": "E-P0214-2e6956a116", "pointer": "papers/paper_notes.jsonl:paper_id=P0214#key_results[0]"}, {"hook_type": "quant", "text": "MCP-RADAR features a challenging dataset of 507 tasks spanning six domains: mathematical reasoning, web search, email, calendar, file management, and terminal operations.", "citations": ["Gao2025Radar"], "paper_id": "P0081", "evidence_id": "E-P0081-419e1464da", "pointer": "papers/paper_notes.jsonl:paper_id=P0081#key_results[0]"}, {"hook_type": "eval", "text": "Evaluation across various tool-use benchmarks illustrates that our proposed multi-LLM framework surpasses the traditional single-LLM approach, highlighting its efficacy and advantages in tool learning.", "citations": ["Shen2024Small"], "paper_id": "P0108", "evidence_id": "E-P0108-9640816b42", "pointer": "papers/paper_notes.jsonl:paper_id=P0108#key_results[1]"}], "generated_at": "2026-01-25T17:07:00", "section_id": "3", "section_title": "Foundations & Interfaces"}
{"sub_id": "4.1", "title": "Planning and reasoning loops", "anchors": [{"hook_type": "quant", "text": "Experimental evaluation on the complex task planning benchmark demonstrates that our 1.5B parameter model trained with single-turn GRPO achieves superior performance compared to larger baseline models up to 14B parameters, with success rates of 70% for long-horizon planning", "citations": ["Hu2025Training"], "paper_id": "P0089", "evidence_id": "E-P0089-771620f84f", "pointer": "papers/paper_notes.jsonl:paper_id=P0089#key_results[0]"}, {"hook_type": "quant", "text": "From this observation, we build memory retrieval as an autonomous, accurate, and compatible agent system, named MemR$^3$, which has two core mechanisms: 1) a router that selects among retrieve, reflect, and answer actions to optimize answer quality; 2) a global evidence-gap", "citations": ["Du2025Memr"], "paper_id": "P0200", "evidence_id": "E-P0200-1b99de5317", "pointer": "papers/paper_notes.jsonl:paper_id=P0200#key_results[1]"}, {"hook_type": "quant", "text": "Experimental evaluation on the complex task planning benchmark demonstrates that our 1.5B parameter model trained with single-turn GRPO achieves superior performance compared to larger baseline models up to 14B parameters, with success rates of 70% for long-horizon planning tasks", "citations": ["Hu2025Training"], "paper_id": "P0089", "evidence_id": "E-P0089-771620f84f", "pointer": "papers/paper_notes.jsonl:paper_id=P0089#key_results[0]"}, {"hook_type": "eval", "text": "To this end, we introduce USTBench, the first benchmark to evaluate LLMs' spatiotemporal reasoning abilities as urban agents across four decomposed dimensions: spatiotemporal understanding, forecasting, planning, and reflection with feedback.", "citations": ["Lai2025Ustbench"], "paper_id": "P0233", "evidence_id": "E-P0233-c7ba5be441", "pointer": "papers/paper_notes.jsonl:paper_id=P0233#method"}, {"hook_type": "quant", "text": "While a lot of recent research focuses on enhancing the textual reasoning capabilities of Large Language Models (LLMs) by optimizing the multi-agent framework or reasoning chains, several benchmark tasks can be solved with 100\\% success through direct coding, which is more scalab", "citations": ["Chen2024Steering"], "paper_id": "P0109", "evidence_id": "E-P0109-c0a4d47a26", "pointer": "papers/paper_notes.jsonl:paper_id=P0109#key_results[0]"}, {"hook_type": "quant", "text": "On two interactive decision making benchmarks (ALFWorld and WebShop), ReAct outperforms imitation and reinforcement learning methods by an absolute success rate of 34% and 10% respectively, while being prompted with only one or two in-context examples.", "citations": ["Yao2022React"], "paper_id": "P0001", "evidence_id": "E-P0001-ca4a00b5cf", "pointer": "papers/paper_notes.jsonl:paper_id=P0001#key_results[0]"}, {"hook_type": "quant", "text": "Experiments on three real-world multi-tabular EHR datasets show that EHRAgent outperforms the strongest baseline by up to 29.6% in success rate.", "citations": ["Shi2024Ehragent"], "paper_id": "P0038", "evidence_id": "E-P0038-2a7ea60588", "pointer": "papers/paper_notes.jsonl:paper_id=P0038#key_results[0]"}, {"hook_type": "quant", "text": "We demonstrate particularly strong gains on Wikidata benchmarks (+16\\% improvement over previous best methods) alongside consistent improvements on Freebase benchmarks.", "citations": ["Sun2025Search"], "paper_id": "P0088", "evidence_id": "E-P0088-4ae722da57", "pointer": "papers/paper_notes.jsonl:paper_id=P0088#key_results[0]"}, {"hook_type": "quant", "text": "From this observation, we build memory retrieval as an autonomous, accurate, and compatible agent system, named MemR$^3$, which has two core mechanisms: 1) a router that selects among retrieve, reflect, and answer actions to optimize answer quality; 2) a global evidence-gap track", "citations": ["Du2025Memr"], "paper_id": "P0200", "evidence_id": "E-P0200-1b99de5317", "pointer": "papers/paper_notes.jsonl:paper_id=P0200#key_results[1]"}, {"hook_type": "quant", "text": "We evaluate PDoctor with three mainstream agent frameworks and two powerful LLMs (GPT-3.5 and GPT-4).", "citations": ["Ji2024Testing"], "paper_id": "P0110", "evidence_id": "E-P0110-c0a98eb625", "pointer": "papers/paper_notes.jsonl:paper_id=P0110#key_results[0]"}, {"hook_type": "eval", "text": "Subsequently, we systematically summarize existing evaluation frameworks, including benchmark datasets, evaluation metrics and performance comparisons between representative planning methods.", "citations": ["Cao2025Large"], "paper_id": "P0077", "evidence_id": "E-P0077-23a1945a58", "pointer": "papers/paper_notes.jsonl:paper_id=P0077#key_results[1]"}, {"hook_type": "eval", "text": "To rigorously evaluate the agent's performance in this emerging research area, we introduce a comprehensive benchmark consisting of diverse construction tasks designed to test creativity, spatial reasoning, adherence to in-game rules, and the effective integration of multimodal i", "citations": ["Chen2024Architectural"], "paper_id": "P0092", "evidence_id": "E-P0092-d1af7d9a55", "pointer": "papers/paper_notes.jsonl:paper_id=P0092#key_results[0]"}], "generated_at": "2026-01-25T17:07:00", "section_id": "4", "section_title": "Core Components (Planning + Memory)"}
{"sub_id": "4.2", "title": "Memory and retrieval (RAG)", "anchors": [{"hook_type": "quant", "text": "From this observation, we build memory retrieval as an autonomous, accurate, and compatible agent system, named MemR$^3$, which has two core mechanisms: 1) a router that selects among retrieve, reflect, and answer actions to optimize answer quality; 2) a global evidence-gap", "citations": ["Du2025Memr"], "paper_id": "P0200", "evidence_id": "E-P0200-1b99de5317", "pointer": "papers/paper_notes.jsonl:paper_id=P0200#key_results[1]"}, {"hook_type": "quant", "text": "On two interactive decision making benchmarks (ALFWorld and WebShop), ReAct outperforms imitation and reinforcement learning methods by an absolute success rate of 34% and 10% respectively, while being prompted with only one or two in-context examples.", "citations": ["Yao2022React"], "paper_id": "P0001", "evidence_id": "E-P0001-ca4a00b5cf", "pointer": "papers/paper_notes.jsonl:paper_id=P0001#key_results[0]"}, {"hook_type": "quant", "text": "Comprehensive evaluations on multiple benchmarks show that A-MemGuard effectively cuts attack success rates by over 95% while incurring a minimal utility cost.", "citations": ["Wei2025Memguard"], "paper_id": "P0047", "evidence_id": "E-P0047-cb9370ac71", "pointer": "papers/paper_notes.jsonl:paper_id=P0047#key_results[0]"}, {"hook_type": "quant", "text": "Using an optimized scaffold matching industry best practices (persistent bash + string-replacement editor), we evaluated Focus on N=5 context-intensive instances from SWE-bench Lite using Claude Haiku 4.5.", "citations": ["Verma2026Active"], "paper_id": "P0120", "evidence_id": "E-P0120-9abcf1bf8a", "pointer": "papers/paper_notes.jsonl:paper_id=P0120#key_results[1]"}, {"hook_type": "eval", "text": "Structural drawings are widely used in many fields, e.g., mechanical engineering, civil engineering, etc. In civil engineering, structural drawings serve as the main communication tool between architects, engineers, and builders to avoid conflicts, act as legal documentation, and", "citations": ["Zhang2025Largea"], "paper_id": "P0189", "evidence_id": "E-P0189-897bcc2f50", "pointer": "papers/paper_notes.jsonl:paper_id=P0189#key_results[0]"}, {"hook_type": "quant", "text": "From this observation, we build memory retrieval as an autonomous, accurate, and compatible agent system, named MemR$^3$, which has two core mechanisms: 1) a router that selects among retrieve, reflect, and answer actions to optimize answer quality; 2) a global evidence-gap track", "citations": ["Du2025Memr"], "paper_id": "P0200", "evidence_id": "E-P0200-1b99de5317", "pointer": "papers/paper_notes.jsonl:paper_id=P0200#key_results[1]"}, {"hook_type": "quant", "text": "We unify and implement over ten representative memory modules and evaluate them across 10 diverse multi-turn goal-oriented and single-turn reasoning and QA datasets.", "citations": ["Wei2025Memory"], "paper_id": "P0173", "evidence_id": "E-P0173-06e45507a7", "pointer": "papers/paper_notes.jsonl:paper_id=P0173#key_results[0]"}, {"hook_type": "quant", "text": "This study presents SAFE (system for accurate fact extraction and evaluation), an agent system that combines large language models with retrieval-augmented generation (RAG) to improve automated fact-checking of long-form COVID-19 misinformation.", "citations": ["Huang2025Retrieval"], "paper_id": "P0235", "evidence_id": "E-P0235-4af0cf3c02", "pointer": "papers/paper_notes.jsonl:paper_id=P0235#key_results[1]"}, {"hook_type": "limitation", "text": "Yet their sophisticated architectures amplify vulnerability to cascading failures, where a single root-cause error propagates through subsequent decisions, leading to task failure.", "citations": ["Zhu2025Where"], "paper_id": "P0033", "evidence_id": "E-P0033-46914a4804", "pointer": "papers/paper_notes.jsonl:paper_id=P0033#summary_bullets[1]"}, {"hook_type": "eval", "text": "Finally, we summarize the datasets and benchmarks used for evaluation and tuning, review key applications of LLM-based agents, and discuss major challenges and promising future directions.", "citations": ["Du2025Survey"], "paper_id": "P0026", "evidence_id": "E-P0026-f0ea009256", "pointer": "papers/paper_notes.jsonl:paper_id=P0026#key_results[0]"}, {"hook_type": "limitation", "text": "Existing benchmarks either rely on limited context lengths or are tailored for static, long-context settings like book-based QA, which do not reflect the interactive, multi-turn nature of memory agents that incrementally accumulate information.", "citations": ["Hu2025Evaluating"], "paper_id": "P0062", "evidence_id": "E-P0062-86184d0d44", "pointer": "papers/paper_notes.jsonl:paper_id=P0062#key_results[1]"}, {"hook_type": "eval", "text": "To further accelerate research in this area for the community, we compile open-source training frameworks, training and evaluation datasets for developing agentic MLLMs.", "citations": ["Yao2025Survey"], "paper_id": "P0011", "evidence_id": "E-P0011-1b6fe3407a", "pointer": "papers/paper_notes.jsonl:paper_id=P0011#key_results[0]"}], "generated_at": "2026-01-25T17:07:00", "section_id": "4", "section_title": "Core Components (Planning + Memory)"}
{"sub_id": "5.1", "title": "Self-improvement and adaptation", "anchors": [{"hook_type": "quant", "text": "Across agent and domain-specific benchmarks, ACE optimizes contexts both offline (e.g., system prompts) and online (e.g., agent memory), consistently outperforming strong baselines: +10.6% on agents and +8.6% on finance, while significantly reducing adaptation latency and", "citations": ["Zhang2025Agentic"], "paper_id": "P0144", "evidence_id": "E-P0144-7d85a7241d", "pointer": "papers/paper_notes.jsonl:paper_id=P0144#key_results[0]"}, {"hook_type": "quant", "text": "Across agent and domain-specific benchmarks, ACE optimizes contexts both offline (e.g., system prompts) and online (e.g., agent memory), consistently outperforming strong baselines: +10.6% on agents and +8.6% on finance, while significantly reducing adaptation latency and rollout", "citations": ["Zhang2025Agentic"], "paper_id": "P0144", "evidence_id": "E-P0144-7d85a7241d", "pointer": "papers/paper_notes.jsonl:paper_id=P0144#key_results[0]"}, {"hook_type": "eval", "text": "We present MSB (MCP Security Benchmark), the first end-to-end evaluation suite that systematically measures how well LLM agents resist MCP-specific attacks throughout the full tool-use pipeline: task planning, tool invocation, and response handling.", "citations": ["Zhang2025Security"], "paper_id": "P0195", "evidence_id": "E-P0195-8bcb673a7d", "pointer": "papers/paper_notes.jsonl:paper_id=P0195#method"}, {"hook_type": "limitation", "text": "We address this limitation by introducing a framework that enables LLM agents to learn and evolve both before and during test time, equipping them with environment-relevant knowledge for better planning and enhanced communication for improved cooperation.", "citations": ["Li2025Learn"], "paper_id": "P0079", "evidence_id": "E-P0079-01f439b837", "pointer": "papers/paper_notes.jsonl:paper_id=P0079#limitations[1]"}, {"hook_type": "quant", "text": "Evaluation on two existing multi-turn tool-use agent benchmarks, M3ToolEval and TauBench, shows the Self-Challenging framework achieves over a two-fold improvement in Llama-3.1-8B-Instruct, despite using only self-generated training data.", "citations": ["Zhou2025Self"], "paper_id": "P0214", "evidence_id": "E-P0214-2e6956a116", "pointer": "papers/paper_notes.jsonl:paper_id=P0214#key_results[0]"}, {"hook_type": "quant", "text": "From the data science perspective, we identify key processes for LLM-based agents, including data preprocessing, model development, evaluation, visualization, etc. Our work offers two key contributions: (1) a comprehensive review of recent developments in applying LLMbased agents", "citations": ["Chen2025Largea"], "paper_id": "P0190", "evidence_id": "E-P0190-3620ae9178", "pointer": "papers/paper_notes.jsonl:paper_id=P0190#key_results[0]"}, {"hook_type": "quant", "text": "Evaluating on two representative interactive agent tasks, SAND achieves an average 20% improvement over initial supervised finetuning and also outperforms state-of-the-art agent tuning approaches.", "citations": ["Xia2025Sand"], "paper_id": "P0087", "evidence_id": "E-P0087-6273763a98", "pointer": "papers/paper_notes.jsonl:paper_id=P0087#key_results[0]"}, {"hook_type": "quant", "text": "The results indicate that 1) all tested LLMs spontaneously misrepresent their actions in at least some conditions, 2) they are generally more likely to do so in situations in which deception would benefit them, and 3) models exhibiting better reasoning capacity overall tend to de", "citations": ["Taylor2025Large"], "paper_id": "P0167", "evidence_id": "E-P0167-047d14c804", "pointer": "papers/paper_notes.jsonl:paper_id=P0167#key_results[0]"}, {"hook_type": "eval", "text": "We review and differentiate the work of LLMs and LLM-based agents from these six topics, examining their differences and similarities in tasks, benchmarks, and evaluation metrics.", "citations": ["Jin2024From"], "paper_id": "P0099", "evidence_id": "E-P0099-33c7e5174b", "pointer": "papers/paper_notes.jsonl:paper_id=P0099#key_results[1]"}, {"hook_type": "eval", "text": "Settlers of Catan provides a challenging benchmark: success depends on balancing short- and long-term goals amid randomness, trading, expansion, and blocking.", "citations": ["Belle2025Agents"], "paper_id": "P0051", "evidence_id": "E-P0051-eae2720c71", "pointer": "papers/paper_notes.jsonl:paper_id=P0051#key_results[1]"}, {"hook_type": "quant", "text": "We conducted extensive experiments and the results show that FICAL has competitive performance compared to other SOTA baselines with a significant communication cost decrease of $\\mathbf{3.33\\times10^5}$ times.", "citations": ["Wu2024Federated"], "paper_id": "P0260", "evidence_id": "E-P0260-76aa5df0e2", "pointer": "papers/paper_notes.jsonl:paper_id=P0260#key_results[0]"}, {"hook_type": "quant", "text": "Empirical results averaged across models show major improvements in task success, with absolute gains of +10.3% on ALFWorld, +23.8% on BabyAI, and +8.3% on PDDL in the Self-sustaining mode.", "citations": ["Bharadwaj2025Omnireflect"], "paper_id": "P0205", "evidence_id": "E-P0205-85b5f3734b", "pointer": "papers/paper_notes.jsonl:paper_id=P0205#key_results[0]"}], "generated_at": "2026-01-25T17:07:00", "section_id": "5", "section_title": "Learning, Adaptation & Coordination"}
{"sub_id": "5.2", "title": "Multi-agent coordination", "anchors": [{"hook_type": "quant", "text": "Using a corpus of 219 labeled rubrics derived from the GDPVal benchmark, we evaluate ARCANE on challenging tasks requiring multi-step reasoning and tool use.", "citations": ["Masters2025Arcane"], "paper_id": "P0139", "evidence_id": "E-P0139-8eb9ff221b", "pointer": "papers/paper_notes.jsonl:paper_id=P0139#key_results[0]"}, {"hook_type": "quant", "text": "Our experiments on Communicative Watch-And-Help and ThreeD-World Multi-Agent Transport benchmarks demonstrate that LIET, instantiated with both LLaMA and GPT-4o, outperforms existing baselines and exhibits strong cooperative planning abilities.", "citations": ["Li2025Learn"], "paper_id": "P0079", "evidence_id": "E-P0079-1dd544863c", "pointer": "papers/paper_notes.jsonl:paper_id=P0079#key_results[1]"}, {"hook_type": "quant", "text": "We evaluate AutoSCORE on four benchmark datasets from the ASAP benchmark, using both proprietary and open-source LLMs (GPT-4o, LLaMA-3.1-8B, and LLaMA-3.1-70B).", "citations": ["Wang2025Autoscore"], "paper_id": "P0149", "evidence_id": "E-P0149-42512d3cac", "pointer": "papers/paper_notes.jsonl:paper_id=P0149#key_results[0]"}, {"hook_type": "quant", "text": "We conducted comprehensive experiments using a kinase inhibitor dataset, where our multi-agent LLM method outperformed the non-reasoning multi-agent model (GPT-4o mini) by 45% in F1 score (0.514 vs 0.355).", "citations": ["Inoue2024Drugagent"], "paper_id": "P0255", "evidence_id": "E-P0255-5e37132da5", "pointer": "papers/paper_notes.jsonl:paper_id=P0255#key_results[0]"}, {"hook_type": "quant", "text": "In this work, we propose SG^2, an iterative Schema-Guided Scene-Graph reasoning framework based on multi-agent LLMs.", "citations": ["Chen2025Schema"], "paper_id": "P0212", "evidence_id": "E-P0212-1167f52f16", "pointer": "papers/paper_notes.jsonl:paper_id=P0212#key_results[0]"}, {"hook_type": "quant", "text": "To this end, we propose a multi-agent system with customized communication knowledge and tools for solving communication related tasks using natural language, comprising three components: (1) Multi-agent Data Retrieval (MDR), which employs the condensate and inference agents to r", "citations": ["Jiang2023Large"], "paper_id": "P0040", "evidence_id": "E-P0040-96b056b80b", "pointer": "papers/paper_notes.jsonl:paper_id=P0040#key_results[0]"}, {"hook_type": "eval", "text": "Current frameworks for multi-agent debate are often designed towards tool use, lack integrated evaluation, or provide limited configurability of agent personas, response generators, discussion paradigms, and decision protocols.", "citations": ["Becker2025Mallm"], "paper_id": "P0194", "evidence_id": "E-P0194-f2e78c673e", "pointer": "papers/paper_notes.jsonl:paper_id=P0194#key_results[1]"}, {"hook_type": "quant", "text": "We find that 20 of the 48 issues reported by MARVEL pose security vulnerabilities.", "citations": ["Collini2025Marvel"], "paper_id": "P0080", "evidence_id": "E-P0080-0e82f75090", "pointer": "papers/paper_notes.jsonl:paper_id=P0080#key_results[0]"}, {"hook_type": "eval", "text": "Evaluation across various tool-use benchmarks illustrates that our proposed multi-LLM framework surpasses the traditional single-LLM approach, highlighting its efficacy and advantages in tool learning.", "citations": ["Shen2024Small"], "paper_id": "P0108", "evidence_id": "E-P0108-9640816b42", "pointer": "papers/paper_notes.jsonl:paper_id=P0108#key_results[1]"}, {"hook_type": "quant", "text": "Validation using 551 GNWT-Agents and Columbia University Speed Dating dataset demonstrates 72% correlation with human attraction patterns, 77.8% match prediction accuracy, and 74% agreement in human validation studies.", "citations": ["Ye2025Cognipair"], "paper_id": "P0159", "evidence_id": "E-P0159-1ea69fbec3", "pointer": "papers/paper_notes.jsonl:paper_id=P0159#key_results[0]"}, {"hook_type": "quant", "text": "We evaluate seven LLMs, quantitatively highlighting a significant capability gap of over threefold between the strongest, GPT o1, and the weakest, Llama-2-70B.", "citations": ["Xu2023Magic"], "paper_id": "P0115", "evidence_id": "E-P0115-af857798be", "pointer": "papers/paper_notes.jsonl:paper_id=P0115#key_results[0]"}, {"hook_type": "eval", "text": "Systematic literature reviews and meta-analyses are essential for synthesizing research insights, but they remain time-intensive and labor-intensive due to the iterative processes of screening, evaluation, and data extraction.", "citations": ["Rouzrokh2025Lattereview"], "paper_id": "P0078", "evidence_id": "E-P0078-fa4336d046", "pointer": "papers/paper_notes.jsonl:paper_id=P0078#key_results[0]"}], "generated_at": "2026-01-25T17:07:00", "section_id": "5", "section_title": "Learning, Adaptation & Coordination"}
{"sub_id": "6.1", "title": "Benchmarks and evaluation protocols", "anchors": [{"hook_type": "quant", "text": "Experimental results demonstrate that API-based models outperform open-sourced models on all metrics and Deepseek-Coder-33B-Instruct achieves the highest score among open-sourced models.", "citations": ["Zhang2025Datascibench"], "paper_id": "P0162", "evidence_id": "E-P0162-39281e5083", "pointer": "papers/paper_notes.jsonl:paper_id=P0162#key_results[1]"}, {"hook_type": "quant", "text": "We evaluate AutoSCORE on four benchmark datasets from the ASAP benchmark, using both proprietary and open-source LLMs (GPT-4o, LLaMA-3.1-8B, and LLaMA-3.1-70B).", "citations": ["Wang2025Autoscore"], "paper_id": "P0149", "evidence_id": "E-P0149-42512d3cac", "pointer": "papers/paper_notes.jsonl:paper_id=P0149#key_results[0]"}, {"hook_type": "quant", "text": "Experiments on challenging agentic benchmarks such as GAIA and BrowseComp+ demonstrate that EvoRoute, when integrated into off-the-shelf agentic systems, not only sustains or enhances system performance but also reduces execution cost by up to $80\\%$ and latency by over $70\\%$.", "citations": ["Zhang2026Evoroute"], "paper_id": "P0124", "evidence_id": "E-P0124-60cc0d458f", "pointer": "papers/paper_notes.jsonl:paper_id=P0124#key_results[0]"}, {"hook_type": "quant", "text": "Our extensive evaluation across various agent use cases, using benchmarks like AgentDojo, ASB, and AgentPoison, demonstrates that Progent reduces attack success rates to 0%, while preserving agent utility and speed.", "citations": ["Shi2025Progent"], "paper_id": "P0086", "evidence_id": "E-P0086-68db58914f", "pointer": "papers/paper_notes.jsonl:paper_id=P0086#key_results[0]"}, {"hook_type": "quant", "text": "ACBench spans (1) 12 tasks across 4 capabilities (e.g., WorfBench for workflow generation, Needle-in-Haystack for long-context retrieval), (2) quantization (GPTQ, AWQ) and pruning (Wanda, SparseGPT), and (3) 15 models, including small (Gemma-2B), standard (Qwen2.5 7B-32B), and", "citations": ["Dong2025Compressed"], "paper_id": "P0028", "evidence_id": "E-P0028-08c43ee74b", "pointer": "papers/paper_notes.jsonl:paper_id=P0028#key_results[1]"}, {"hook_type": "quant", "text": "Existing evaluation methods suffer from following shortcomings: (1) constrained evaluation abilities, (2) vulnerable benchmarks, (3) unobjective metrics.", "citations": ["Lin2023Agentsims"], "paper_id": "P0113", "evidence_id": "E-P0113-152d1ce5af", "pointer": "papers/paper_notes.jsonl:paper_id=P0113#key_results[0]"}, {"hook_type": "eval", "text": "We introduce WildAGTEval, a benchmark designed to evaluate large language model (LLM) agents' function-calling capabilities under realistic API complexity.", "citations": ["Kim2026Beyond"], "paper_id": "P0122", "evidence_id": "E-P0122-ef4cf62416", "pointer": "papers/paper_notes.jsonl:paper_id=P0122#method"}, {"hook_type": "quant", "text": "Our evaluation of 16 agents under a unified ReAct framework reveals distinct capabilities and limitations across models.", "citations": ["Seo2025Simuhome"], "paper_id": "P0216", "evidence_id": "E-P0216-e2e3b7fa97", "pointer": "papers/paper_notes.jsonl:paper_id=P0216#limitations[1]"}, {"hook_type": "quant", "text": "Extensive experiments across six benchmarks, covering the diverse scenarios of web, embodied, tool use and game applications, show that AgentSquare substantially outperforms hand-crafted agents, achieving an average performance gain of 17.2% against best-known human designs.", "citations": ["Shang2024Agentsquare"], "paper_id": "P0037", "evidence_id": "E-P0037-38a26e4777", "pointer": "papers/paper_notes.jsonl:paper_id=P0037#key_results[0]"}, {"hook_type": "quant", "text": "This paper presents a systematic and comprehensive review of MLLM evaluation methods, covering the following key aspects: (1) the background of MLLMs and their evaluation; (2) \"what to evaluate\" that reviews and categorizes existing MLLM evaluation tasks based on the capabilities", "citations": ["Huang2024Survey"], "paper_id": "P0244", "evidence_id": "E-P0244-0bcc4031a6", "pointer": "papers/paper_notes.jsonl:paper_id=P0244#key_results[0]"}, {"hook_type": "limitation", "text": "Yet their sophisticated architectures amplify vulnerability to cascading failures, where a single root-cause error propagates through subsequent decisions, leading to task failure.", "citations": ["Zhu2025Where"], "paper_id": "P0033", "evidence_id": "E-P0033-46914a4804", "pointer": "papers/paper_notes.jsonl:paper_id=P0033#summary_bullets[1]"}, {"hook_type": "quant", "text": "This paper proposes an evidence-based, actionable, and generalizable evaluation design guideline for LLM-based RPA by systematically reviewing 1,676 papers published between Jan.", "citations": ["Chen2025Towards"], "paper_id": "P0231", "evidence_id": "E-P0231-4fc221fdea", "pointer": "papers/paper_notes.jsonl:paper_id=P0231#key_results[0]"}], "generated_at": "2026-01-25T17:07:00", "section_id": "6", "section_title": "Evaluation & Risks"}
{"sub_id": "6.2", "title": "Safety, security, and governance", "anchors": [{"hook_type": "quant", "text": "Function Calling showed higher overall attack success rates (73.5% vs 62.59% for MCP), with greater system-centric vulnerability while MCP exhibited increased LLM-centric exposure.", "citations": ["Gasmi2025Bridging"], "paper_id": "P0058", "evidence_id": "E-P0058-8e34a29629", "pointer": "papers/paper_notes.jsonl:paper_id=P0058#key_results[0]"}, {"hook_type": "quant", "text": "MSB contributes: (1) a taxonomy of 12 attacks including name-collision, preference manipulation, prompt injections embedded in tool descriptions, out-of-scope parameter requests, user-impersonating responses, false-error escalation, tool-transfer, retrieval injection, and mixed", "citations": ["Zhang2025Security"], "paper_id": "P0195", "evidence_id": "E-P0195-d6095e10e9", "pointer": "papers/paper_notes.jsonl:paper_id=P0195#key_results[0]"}, {"hook_type": "quant", "text": "Evaluations on two novel benchmarks demonstrate that BridgeScope enables LLM agents to operate databases more effectively, reduces token usage by up to 80% through improved security awareness, and uniquely supports data-intensive workflows beyond existing toolkits, establishing", "citations": ["Weng2025Bridgescope"], "paper_id": "P0155", "evidence_id": "E-P0155-e4be1f69f1", "pointer": "papers/paper_notes.jsonl:paper_id=P0155#key_results[0]"}, {"hook_type": "quant", "text": "Our extensive evaluation across various agent use cases, using benchmarks like AgentDojo, ASB, and AgentPoison, demonstrates that Progent reduces attack success rates to 0%, while preserving agent utility and speed.", "citations": ["Shi2025Progent"], "paper_id": "P0086", "evidence_id": "E-P0086-68db58914f", "pointer": "papers/paper_notes.jsonl:paper_id=P0086#key_results[0]"}, {"hook_type": "quant", "text": "We evaluate AgentVigil on two public benchmarks, AgentDojo and VWA-adv, where it achieves 71% and 70% success rates against agents based on o3-mini and GPT-4o, respectively, nearly doubling the performance of baseline attacks.", "citations": ["Wang2025Agentvigil"], "paper_id": "P0142", "evidence_id": "E-P0142-03ed8e82d6", "pointer": "papers/paper_notes.jsonl:paper_id=P0142#key_results[0]"}, {"hook_type": "quant", "text": "MSB contributes: (1) a taxonomy of 12 attacks including name-collision, preference manipulation, prompt injections embedded in tool descriptions, out-of-scope parameter requests, user-impersonating responses, false-error escalation, tool-transfer, retrieval injection, and mixed a", "citations": ["Zhang2025Security"], "paper_id": "P0195", "evidence_id": "E-P0195-d6095e10e9", "pointer": "papers/paper_notes.jsonl:paper_id=P0195#key_results[0]"}, {"hook_type": "quant", "text": "Through extensive evaluation of leading LLMs, we find that even SOTA models such as GPT-5 (43.72%), Grok-4 (33.33%) and Claude-4.0-Sonnet (29.44%) exhibit significant performance limitations.", "citations": ["Luo2025Universe"], "paper_id": "P0196", "evidence_id": "E-P0196-3c65d38a2a", "pointer": "papers/paper_notes.jsonl:paper_id=P0196#limitations[1]"}, {"hook_type": "quant", "text": "Evaluations on two novel benchmarks demonstrate that BridgeScope enables LLM agents to operate databases more effectively, reduces token usage by up to 80% through improved security awareness, and uniquely supports data-intensive workflows beyond existing toolkits, establishing B", "citations": ["Weng2025Bridgescope"], "paper_id": "P0155", "evidence_id": "E-P0155-e4be1f69f1", "pointer": "papers/paper_notes.jsonl:paper_id=P0155#key_results[0]"}, {"hook_type": "quant", "text": "Leveraging the ToolEmu framework, we conduct a systematic evaluation of quitting behavior across 12 state-of-the-art LLMs.", "citations": ["Bonagiri2025Check"], "paper_id": "P0158", "evidence_id": "E-P0158-7edb91824f", "pointer": "papers/paper_notes.jsonl:paper_id=P0158#key_results[0]"}, {"hook_type": "quant", "text": "Extensive experiments across ten realistic, simulated tool-use scenarios and a range of popular LLM agents demonstrate consistently high attack success rates (81\\%-95\\%) and significant privacy leakage, with negligible impact on primary task execution.", "citations": ["Mo2025Attractive"], "paper_id": "P0146", "evidence_id": "E-P0146-a0b404d928", "pointer": "papers/paper_notes.jsonl:paper_id=P0146#key_results[0]"}, {"hook_type": "quant", "text": "MCP-RADAR features a challenging dataset of 507 tasks spanning six domains: mathematical reasoning, web search, email, calendar, file management, and terminal operations.", "citations": ["Gao2025Radar"], "paper_id": "P0081", "evidence_id": "E-P0081-419e1464da", "pointer": "papers/paper_notes.jsonl:paper_id=P0081#key_results[0]"}, {"hook_type": "quant", "text": "Our major contributions include: (1) systematically analyzing the technical transition from standard legal LLMs to legal agents; (2) presenting a structured taxonomy of current agent applications across distinct legal practice areas; (3) discussing evaluation methodologies specif", "citations": ["Liu2026Agents"], "paper_id": "P0126", "evidence_id": "E-P0126-8b56718f74", "pointer": "papers/paper_notes.jsonl:paper_id=P0126#key_results[0]"}], "generated_at": "2026-01-25T17:07:00", "section_id": "6", "section_title": "Evaluation & Risks"}
