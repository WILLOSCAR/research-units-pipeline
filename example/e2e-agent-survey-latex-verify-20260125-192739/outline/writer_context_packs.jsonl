{"sub_id": "3.1", "title": "Agent loop and action spaces", "section_id": "3", "section_title": "Foundations & Interfaces", "rq": "Which design choices in Agent loop and action spaces drive the major trade-offs, and how are those trade-offs measured?", "thesis": "In Agent loop and action spaces, differences in evaluation protocol (datasets, metrics, human evaluation) and compute and latency constraints frequently imply different evaluation setups, so the key is to compare under consistent protocols where possible.", "tension_statement": "A central tension in Agent loop and action spaces is the trade-off between evaluation protocol (datasets, metrics, human evaluation), compute and latency constraints and what can be evaluated reliably under realistic constraints.", "evaluation_anchor_minimal": {"task": "attack/defense evaluation", "metric": "attack success rate", "constraint": "policy/sandbox setting"}, "paper_voice_palette": {"forbidden_pipeline_voice": ["this run", "this run is", "pipeline", "workspace", "unit", "quality gate", "evidence pack", "evidence packs", "writer context pack", "Method note (evidence policy):", "Methodology note:", "Methodology note (evidence policy):", "Methodology note (evidence-policy):", "Method note:"], "high_risk_templates": ["This subsection surveys", "This subsection argues", "In this subsection", "This section surveys", "This section reviews", "This section discusses", "This section covers", "This section presents", "This section introduces", "provides an overview", "This survey provides an overview", "In this section, we provide an overview", "This section provides an overview", "Next, we move from", "We now turn to", "A few representative references include", "Notable lines of work include", "Concrete examples include", "Work in this area includes", "Recent systems include", "Examples include", "Representative systems include", "Therefore, survey synthesis should", "Therefore, survey comparisons should", "As a result, survey comparisons should", "survey synthesis should", "survey comparisons should", "Taken together,", "The key point is that", "Three limitations", "Two limitations", "claims remain provisional under abstract-only evidence", "abstract-only evidence", "Key takeaway:", "Main takeaway:", "protocol token", "protocol tokens", "constraint token", "constraint tokens", "metric token", "metric tokens", "evaluation token", "evaluation tokens", "In this survey", "In our survey", "Our survey", "This survey"], "opener_archetypes": {"tension-first": ["A key tension is", "The central trade-off is", "A recurring constraint is"], "decision-first": ["For system builders, the crux is", "A practical decision is", "One design choice is"], "lens-first": ["Seen through the lens of", "From the perspective of", "Under an interface contract,"], "contrast-first": ["A useful contrast is between", "One sharp contrast is between", "A recurring split is between"], "protocol-first": ["Comparisons hinge on", "Results are only comparable when", "Evaluation claims depend on"]}, "synthesis_stems": ["Across these studies,", "Collectively,", "In summary,", "The evidence suggests that", "A consistent theme is that"], "rewrite_rules": [{"avoid_stem": "This subsection surveys", "prefer_stem": "A key tension is"}, {"avoid_stem": "In this subsection", "prefer_stem": "We focus on"}, {"avoid_stem": "provides an overview", "prefer_stem": "We focus on"}, {"avoid_stem": "Next, we move", "prefer_stem": "Having established"}, {"avoid_stem": "We now turn", "prefer_stem": "We then examine"}, {"avoid_stem": "survey comparisons should", "prefer_stem": "Across protocols, we observe"}, {"avoid_stem": "Two limitations", "prefer_stem": "A caveat is that"}, {"avoid_stem": "Three limitations", "prefer_stem": "One limitation is that"}, {"avoid_stem": "In this survey", "prefer_stem": "We examine"}, {"avoid_stem": "In our survey", "prefer_stem": "We examine"}, {"avoid_stem": "Our survey", "prefer_stem": "This work"}, {"avoid_stem": "This survey", "prefer_stem": "This work"}], "discourse_stem_watchlist": ["Additionally,", "Moreover,", "Furthermore,", "This suggests", "Taken together,", "The key point is that", "Overall,", "In summary,", "In practice,", "More broadly,", "Importantly,", "Notably,", "Crucially,", "In general,", "At a high level,", "In other words,", "Therefore,", "In addition,", "As a result,"], "discourse_stem_rewrites": {"Additionally,": ["More importantly,", "At the same time,", "A second consideration is"], "This suggests": ["This pattern indicates", "These results point to", "A plausible explanation is"], "Taken together,": ["Across these studies,", "Collectively,", "The evidence suggests"], "The key point is that": ["A practical implication is that", "A useful way to read these results is that", "One takeaway is that"], "Overall,": ["In practice,", "More broadly,", "A practical implication is that", "One way to read this evidence is that"], "In summary,": ["Across these studies,", "Collectively,", "A consistent theme is that", "The evidence suggests that"], "Importantly,": ["More importantly,", "A key constraint is that", "A practical implication is that"], "Notably,": ["In particular,", "A concrete example is that", "One implication is that"], "Crucially,": ["More importantly,", "A central constraint is that", "A key takeaway is that"], "At a high level,": ["Concretely,", "Under this protocol,", "In practice,"], "In other words,": ["Put differently,", "Equivalently,", "More concretely,"], "Therefore,": ["As a result,", "This in turn means that", "A practical implication is that"], "In addition,": ["At the same time,", "A second consideration is", "More importantly,"], "As a result,": ["This in turn means that", "Consequently,", "A practical implication is that"]}, "role_cards": {"section_author": {"mission": "Write one subsection as an argument (not a topic list), using in-scope citations as evidence.", "do": ["State a concrete tension/trade-off early and commit to a thesis.", "Make at least two explicit A-vs-B contrasts (mechanism -> outcome) instead of per-paper summaries.", "When citing numbers, add minimal context (task + metric + constraint) in the same paragraph.", "End with a limitation that changes interpretation (protocol mismatch, unclear threat model, missing ablations)."], "avoid": ["Outline narration (This subsection..., In this subsection...).", "Slide navigation (Next, we move..., We now turn...).", "Meta survey advice (survey comparisons should...).", "Cite dumps (a trailing [@a; @b; @c] without a claim)."]}, "evidence_steward": {"mission": "Keep claims auditable: citations support the sentence that needs them, and scope stays local.", "do": ["Embed citations inside claim sentences; name concrete nouns (system/benchmark/protocol) near each cite.", "Prefer subsection-scoped citations; use chapter/global only when truly cross-cutting.", "Downgrade overconfident claims when evidence is thin; convert unknowns into verification targets (once, not spam)."], "avoid": ["Out-of-scope citations to make a paragraph sound stronger.", "Repeating evidence-policy disclaimers in every subsection.", "Ambiguous model naming (e.g., GPT-5) unless the cited work uses it."]}, "style_harmonizer": {"mission": "Make the prose read like a paper: calm, specific, and varied in rhythm without sounding templated.", "do": ["Vary opener cadence across sections (tension-first / decision-first / lens-first) without reusing the same stem.", "Prefer argument bridges over navigation; keep signposting light and content-bearing.", "Reduce slash-enumerations (A/B/C axis labels) by rewriting into natural prose."], "avoid": ["Count-based openers used as a slot (e.g., Two limitations..., Three takeaways...).", "Repeated discourse stems (Additionally, This suggests, Taken together) across many paragraphs.", "Pipeline words (workspace/unit/quality gate/evidence pack) in reader-facing text.", "PPT speaker-note tone (Now we..., The remainder of...).", "Internal shorthand that reads like planning notes (e.g., protocol/metric/constraint tokens)."]}}, "version": "0.4"}, "opener_mode": "tension-first", "opener_hint": "Start with the subsectionâ€™s central tension/trade-off; end paragraph 1 with the thesis.", "axes": ["evaluation protocol (datasets, metrics, human evaluation)", "compute and latency constraints", "tool interface contract (schemas / protocols)", "tool selection / routing policy", "sandboxing / permissions / observability"], "bridge_terms": ["benchmarks/metrics", "compute"], "contrast_hook": "evaluation", "required_evidence_fields": ["benchmarks/datasets", "metrics / human-eval protocol", "compute / cost (train/infer)", "training signal / supervision", "threat model", "defense surface"], "paragraph_plan": [{"para": 1, "argument_role": "setup_thesis", "intent": "Define scope, setup, and the subsection thesis (no pipeline jargon).", "focus": ["scope boundary", "key definitions", "thesis vs neighboring subsections"], "connector_to_prev": "", "connector_phrase": "", "use_clusters": ["Agent frameworks / architectures"], "rq": "Which design choices in Agent loop and action spaces drive the major trade-offs, and how are those trade-offs measured?"}, {"para": 2, "argument_role": "mechanism_cluster_A", "intent": "Explain cluster A: core mechanism and system architecture and what decision it makes in the agent loop.", "focus": ["cluster: Agent frameworks / architectures", "core mechanism and system architecture", "assumptions"], "connector_to_prev": "grounding", "connector_phrase": "baseline route (Agent frameworks / architectures)", "use_clusters": ["Agent frameworks / architectures"]}, {"para": 3, "argument_role": "implementation_cluster_A", "intent": "Cluster A implementation details: training and data signals and interface contract (tools/memory) that constrain behavior.", "focus": ["cluster: Agent frameworks / architectures", "training and data setup", "interface contract", "axes: evaluation protocol (datasets, metrics, human evaluation), compute and latency constraints, tool interface contract (schemas / protocols), tool selection / routing policy, sandboxing / permissions / observability"], "connector_to_prev": "elaboration", "connector_phrase": "implementation assumptions (interface + training)", "use_clusters": ["Agent frameworks / architectures"]}, {"para": 4, "argument_role": "evaluation_cluster_A", "intent": "Cluster A evaluation/trade-offs: where it works, costs (compute/latency), and typical failure modes.", "focus": ["cluster: Agent frameworks / architectures", "evaluation anchor", "efficiency", "failure modes"], "connector_to_prev": "evaluation", "connector_phrase": "evaluation anchor (task/metric/constraint) + failure modes", "use_clusters": ["Agent frameworks / architectures"]}, {"para": 5, "argument_role": "contrast_cluster_B", "intent": "Explain cluster B (contrast with A): core mechanism and system architecture and what it optimizes for.", "focus": ["cluster: Planning / reasoning loops", "contrast with Agent frameworks / architectures", "core mechanism and system architecture"], "connector_to_prev": "contrast", "connector_phrase": "contrast route (Planning / reasoning loops vs Agent frameworks / architectures)", "use_clusters": ["Planning / reasoning loops"]}, {"para": 6, "argument_role": "implementation_cluster_B", "intent": "Cluster B implementation details: training and data and interface assumptions (mirror A for comparability).", "focus": ["cluster: Planning / reasoning loops", "training and data setup", "interface contract", "axes: evaluation protocol (datasets, metrics, human evaluation), compute and latency constraints, tool interface contract (schemas / protocols), tool selection / routing policy, sandboxing / permissions / observability"], "connector_to_prev": "elaboration", "connector_phrase": "contrast implementation assumptions (B)", "use_clusters": ["Planning / reasoning loops"]}, {"para": 7, "argument_role": "evaluation_cluster_B", "intent": "Cluster B evaluation/trade-offs: where it works, costs, and failure modes (mirror A).", "focus": ["cluster: Planning / reasoning loops", "evaluation anchor", "efficiency", "failure modes"], "connector_to_prev": "evaluation", "connector_phrase": "contrast evaluation anchor + trade-offs (B)", "use_clusters": ["Planning / reasoning loops"]}, {"para": 8, "argument_role": "cross_paper_synthesis", "intent": "Cross-paper synthesis: compare clusters along the main axes (include >=2 citations in one paragraph).", "focus": ["compare Agent frameworks / architectures vs Planning / reasoning loops", "multiple citations in one paragraph", "axes: evaluation protocol (datasets, metrics, human evaluation), compute and latency constraints, tool interface contract (schemas / protocols), tool selection / routing policy, sandboxing / permissions / observability"], "connector_to_prev": "synthesis", "connector_phrase": "cross-paper synthesis (Agent frameworks / architectures vs Planning / reasoning loops)", "use_clusters": ["Agent frameworks / architectures", "Planning / reasoning loops", "Memory / retrieval augmentation"]}, {"para": 9, "argument_role": "decision_guidance", "intent": "Decision guidance: when to choose which route (criteria + evaluation signals + engineering constraints).", "focus": ["decision checklist", "evaluation protocol", "practical constraints"], "connector_to_prev": "consequence", "connector_phrase": "decision guidance / criteria", "use_clusters": ["Agent frameworks / architectures", "Planning / reasoning loops", "Memory / retrieval augmentation"]}, {"para": 10, "argument_role": "limitations_open_questions", "intent": "Limitations + verification targets; end with a concrete open question to hand off.", "focus": ["limitations", "evidence mode: provisional", "what needs verification", "open question"], "connector_to_prev": "limitations", "connector_phrase": "limitations + verification targets", "use_clusters": ["Agent frameworks / architectures", "Planning / reasoning loops", "Memory / retrieval augmentation"], "policy": "Use conservative language; avoid strong conclusions; prefer questions-to-answer + explicit evidence gaps list."}], "chapter_throughline": ["Pin scope to goal: agent survey latex.", "Compare approaches along: evaluation protocol (datasets, metrics, human evaluation).", "Compare approaches along: compute and latency constraints.", "Compare approaches along: tool interface contract (schemas / protocols).", "Compare approaches along: tool selection / routing policy.", "Compare approaches along: sandboxing / permissions / observability."], "chapter_key_contrasts": ["evaluation", "tool interfaces"], "chapter_synthesis_mode": "tradeoff_matrix", "allowed_bibkeys_selected": ["Wei2025Memguard", "Kim2025Bridging", "Zou2025Based", "Shang2024Agentsquare", "Li2025Agentswift", "Feng2025Group", "Gasmi2025Bridging", "Shen2025Feat", "Hu2025Survey", "Zhang2026Evoroute", "Abbineni2025Muallm", "Yao2022React", "Inoue2024Drugagent", "Chen2024Llmarena", "Jiang2024Agent", "Samplawski2025Agent", "Xu2025Exemplar", "Yang2025Proagent", "Hu2023Avis", "Nusrat2025Automated", "Li2023Modelscope", "Li2025What"], "allowed_bibkeys_mapped": ["Kim2025Bridging", "Hu2025Survey", "Feng2025Group", "Kulkarni2025Agent", "Maranto2024Llmsat", "Zhang2026Evoroute", "Li2025Agentswift", "Nusrat2025Automated", "Inoue2024Drugagent", "Yu2023Finmem", "Wu2025Meta", "Wei2025Memguard", "Liu2025Aligning", "Gasmi2025Bridging", "Zou2025Based", "Xu2025Exemplar", "Abbineni2025Muallm", "Yang2025Proagent", "Lindenbauer2025Complexity", "Li2025What", "Jiang2024Agent", "Chen2024Llmarena", "Samplawski2025Agent", "Shen2025Feat", "Shang2024Agentsquare", "Li2023Modelscope", "Hu2023Avis", "Yao2022React"], "allowed_bibkeys_chapter": ["Abbineni2025Muallm", "Chen2024Llmarena", "Cheng2025Your", "Chowa2025From", "Cui2025Toward", "Du2025Survey", "Feng2025Group", "Gao2025Radar", "Gasmi2025Bridging", "Hu2023Avis", "Hu2025Survey", "Inoue2024Drugagent", "Jia2025Autotool", "Jiang2024Agent", "Kim2025Bridging", "Kulkarni2025Agent", "Li2023Modelscope", "Li2024Personal", "Li2024Review", "Li2024Stride", "Li2025Agentswift", "Li2025Dissonances", "Li2025What", "Lindenbauer2025Complexity", "Liu2025Aligning", "Liu2025Mcpagentbench", "Lumer2025Memtool", "Luo2025Universe", "Maranto2024Llmsat", "Nusrat2025Automated", "Samplawski2025Agent", "Shang2024Agentsquare", "Shen2024Small", "Shen2025Feat", "Taylor2025Large", "Wang2024Learning", "Wang2024Mllm", "Wei2025Memguard", "Wu2024Avatar", "Wu2025Meta", "Xu2025Exemplar", "Yang2025Proagent", "Yao2022React", "Yin2025Infobid", "Yu2023Finmem", "Zhang2025Generalizability", "Zhang2025Tool", "Zhang2026Evoroute", "Zhou2024Archer", "Zhou2025Self", "Zhou2026Beyond", "Zhu2024Menti", "Zou2025Based"], "allowed_bibkeys_global": ["Li2023Modelscope", "Yao2022React"], "evidence_ids": ["E-P0047-cb9370ac71", "E-P0013-9d9d60644a", "E-P0071-9a8a480e9c", "E-P0037-38a26e4777", "E-P0049-904ba35500", "E-P0029-4b027dfb27", "E-P0058-8e34a29629", "E-P0177-acf4a27e30", "E-P0045-7219ca15f4", "E-P0124-60cc0d458f", "E-P0203-e294aeefb5", "E-P0001-ca4a00b5cf", "E-P0255-5e37132da5", "E-P0270-764758958d", "E-P0102-0cc318c2c4", "E-P0138-da6dd163cf", "E-P0175-be70dd09e9", "E-P0209-aa504d163c", "E-P0297-1c25e10ffc", "E-P0055-c6c63e4ab5", "E-P0116-7a2e32f4bc", "E-P0238-ccc7d0cc50", "E-P0055-71f2629f1b", "E-P0175-3a47ba911a"], "anchor_facts": [{"hook_type": "quant", "text": "On evaluation, we examine over 190 benchmark datasets and trace a shift from static exams toward process- and discovery-oriented assessments with advanced evaluation protocols.", "citations": ["Hu2025Survey"], "paper_id": "P0045", "evidence_id": "E-P0045-7219ca15f4", "pointer": "papers/paper_notes.jsonl:paper_id=P0045#key_results[1]"}, {"hook_type": "quant", "text": "Evaluated across a comprehensive set of seven benchmarks spanning embodied, math, web, tool, and game domains, AgentSwift discovers agents that achieve an average performance gain of 8.34\\% over both existing automated agent search methods and manually designed agents.", "citations": ["Li2025Agentswift"], "paper_id": "P0049", "evidence_id": "E-P0049-904ba35500", "pointer": "papers/paper_notes.jsonl:paper_id=P0049#key_results[0]"}, {"hook_type": "quant", "text": "Extensive experiments across six benchmarks, covering the diverse scenarios of web, embodied, tool use and game applications, show that AgentSquare substantially outperforms hand-crafted agents, achieving an average performance gain of 17.2% against best-known human designs.", "citations": ["Shang2024Agentsquare"], "paper_id": "P0037", "evidence_id": "E-P0037-38a26e4777", "pointer": "papers/paper_notes.jsonl:paper_id=P0037#key_results[0]"}, {"hook_type": "quant", "text": "To evaluate MuaLLM, we introduce two custom datasets: RAG-250, targeting retrieval and citation performance, and Reasoning-100 (Reas-100), focused on multistep reasoning in circuit design.", "citations": ["Abbineni2025Muallm"], "paper_id": "P0203", "evidence_id": "E-P0203-e294aeefb5", "pointer": "papers/paper_notes.jsonl:paper_id=P0203#key_results[1]"}, {"hook_type": "quant", "text": "Comprehensive evaluations on multiple benchmarks show that A-MemGuard effectively cuts attack success rates by over 95% while incurring a minimal utility cost.", "citations": ["Wei2025Memguard"], "paper_id": "P0047", "evidence_id": "E-P0047-cb9370ac71", "pointer": "papers/paper_notes.jsonl:paper_id=P0047#key_results[0]"}, {"hook_type": "quant", "text": "Experiments on challenging agentic benchmarks such as GAIA and BrowseComp+ demonstrate that EvoRoute, when integrated into off-the-shelf agentic systems, not only sustains or enhances system performance but also reduces execution cost by up to $80\\%$ and latency by over $70\\%$.", "citations": ["Zhang2026Evoroute"], "paper_id": "P0124", "evidence_id": "E-P0124-60cc0d458f", "pointer": "papers/paper_notes.jsonl:paper_id=P0124#key_results[0]"}, {"hook_type": "quant", "text": "We evaluate GiGPO on challenging agent benchmarks, including ALFWorld and WebShop, as well as tool-integrated reasoning on search-augmented QA tasks, using Qwen2.5-1.5B/3B/7B-Instruct.", "citations": ["Feng2025Group"], "paper_id": "P0029", "evidence_id": "E-P0029-4b027dfb27", "pointer": "papers/paper_notes.jsonl:paper_id=P0029#key_results[0]"}, {"hook_type": "quant", "text": "Function Calling showed higher overall attack success rates (73.5% vs 62.59% for MCP), with greater system-centric vulnerability while MCP exhibited increased LLM-centric exposure.", "citations": ["Gasmi2025Bridging"], "paper_id": "P0058", "evidence_id": "E-P0058-8e34a29629", "pointer": "papers/paper_notes.jsonl:paper_id=P0058#key_results[0]"}, {"hook_type": "quant", "text": "On two interactive decision making benchmarks (ALFWorld and WebShop), ReAct outperforms imitation and reinforcement learning methods by an absolute success rate of 34% and 10% respectively, while being prompted with only one or two in-context examples.", "citations": ["Yao2022React"], "paper_id": "P0001", "evidence_id": "E-P0001-ca4a00b5cf", "pointer": "papers/paper_notes.jsonl:paper_id=P0001#key_results[0]"}, {"hook_type": "quant", "text": "We conducted comprehensive experiments using a kinase inhibitor dataset, where our multi-agent LLM method outperformed the non-reasoning multi-agent model (GPT-4o mini) by 45% in F1 score (0.514 vs 0.355).", "citations": ["Inoue2024Drugagent"], "paper_id": "P0255", "evidence_id": "E-P0255-5e37132da5", "pointer": "papers/paper_notes.jsonl:paper_id=P0255#key_results[0]"}], "comparison_cards": [{"axis": "evaluation protocol (datasets, metrics, human evaluation)", "A_label": "Agent frameworks / architectures", "B_label": "Planning / reasoning loops", "citations": ["Hu2025Survey", "Li2025Agentswift", "Shang2024Agentsquare"], "A_highlights": [{"paper_id": "P0045", "evidence_id": "E-P0045-7219ca15f4", "excerpt": "On evaluation, we examine over 190 benchmark datasets and trace a shift from static exams toward process- and discovery-oriented assessments with advanced evaluation protocols.", "citations": ["Hu2025Survey"], "pointer": "papers/paper_notes.jsonl:paper_id=P0045#key_results[1]"}, {"paper_id": "P0049", "evidence_id": "E-P0049-904ba35500", "excerpt": "Evaluated across a comprehensive set of seven benchmarks spanning embodied, math, web, tool, and game domains, AgentSwift discovers agents that achieve an average performance gain of 8.34\\% over both existing automated agent search methods and manually designed agents.", "citations": ["Li2025Agentswift"], "pointer": "papers/paper_notes.jsonl:paper_id=P0049#key_results[0]"}], "B_highlights": [{"paper_id": "P0037", "evidence_id": "E-P0037-38a26e4777", "excerpt": "Extensive experiments across six benchmarks, covering the diverse scenarios of web, embodied, tool use and game applications, show that AgentSquare substantially outperforms hand-crafted agents, achieving an average performance gain of 17.2% against best-known human designs.", "citations": ["Shang2024Agentsquare"], "pointer": "papers/paper_notes.jsonl:paper_id=P0037#key_results[0]"}, {"paper_id": "P0049", "evidence_id": "E-P0049-904ba35500", "excerpt": "Evaluated across a comprehensive set of seven benchmarks spanning embodied, math, web, tool, and game domains, AgentSwift discovers agents that achieve an average performance gain of 8.34\\% over both existing automated agent search methods and manually designed agents.", "citations": ["Li2025Agentswift"], "pointer": "papers/paper_notes.jsonl:paper_id=P0049#key_results[0]"}], "write_prompt": "Contrast Agent frameworks / architectures vs Planning / reasoning loops along \"evaluation protocol (datasets, metrics, human evaluation)\". Ground A and B using the highlight snippets; do not introduce new claims beyond the cited evidence."}, {"axis": "evaluation protocol (datasets, metrics, human evaluation)", "A_label": "Agent frameworks / architectures", "B_label": "Memory / retrieval augmentation", "citations": ["Hu2025Survey", "Li2025Agentswift", "Abbineni2025Muallm", "Wei2025Memguard"], "A_highlights": [{"paper_id": "P0045", "evidence_id": "E-P0045-7219ca15f4", "excerpt": "On evaluation, we examine over 190 benchmark datasets and trace a shift from static exams toward process- and discovery-oriented assessments with advanced evaluation protocols.", "citations": ["Hu2025Survey"], "pointer": "papers/paper_notes.jsonl:paper_id=P0045#key_results[1]"}, {"paper_id": "P0049", "evidence_id": "E-P0049-904ba35500", "excerpt": "Evaluated across a comprehensive set of seven benchmarks spanning embodied, math, web, tool, and game domains, AgentSwift discovers agents that achieve an average performance gain of 8.34\\% over both existing automated agent search methods and manually designed agents.", "citations": ["Li2025Agentswift"], "pointer": "papers/paper_notes.jsonl:paper_id=P0049#key_results[0]"}], "B_highlights": [{"paper_id": "P0203", "evidence_id": "E-P0203-e294aeefb5", "excerpt": "To evaluate MuaLLM, we introduce two custom datasets: RAG-250, targeting retrieval and citation performance, and Reasoning-100 (Reas-100), focused on multistep reasoning in circuit design.", "citations": ["Abbineni2025Muallm"], "pointer": "papers/paper_notes.jsonl:paper_id=P0203#key_results[1]"}, {"paper_id": "P0047", "evidence_id": "E-P0047-cb9370ac71", "excerpt": "Comprehensive evaluations on multiple benchmarks show that A-MemGuard effectively cuts attack success rates by over 95% while incurring a minimal utility cost.", "citations": ["Wei2025Memguard"], "pointer": "papers/paper_notes.jsonl:paper_id=P0047#key_results[0]"}], "write_prompt": "Contrast Agent frameworks / architectures vs Memory / retrieval augmentation along \"evaluation protocol (datasets, metrics, human evaluation)\". Ground A and B using the highlight snippets; do not introduce new claims beyond the cited evidence."}, {"axis": "evaluation protocol (datasets, metrics, human evaluation)", "A_label": "Planning / reasoning loops", "B_label": "Memory / retrieval augmentation", "citations": ["Shang2024Agentsquare", "Li2025Agentswift", "Abbineni2025Muallm", "Wei2025Memguard"], "A_highlights": [{"paper_id": "P0037", "evidence_id": "E-P0037-38a26e4777", "excerpt": "Extensive experiments across six benchmarks, covering the diverse scenarios of web, embodied, tool use and game applications, show that AgentSquare substantially outperforms hand-crafted agents, achieving an average performance gain of 17.2% against best-known human designs.", "citations": ["Shang2024Agentsquare"], "pointer": "papers/paper_notes.jsonl:paper_id=P0037#key_results[0]"}, {"paper_id": "P0049", "evidence_id": "E-P0049-904ba35500", "excerpt": "Evaluated across a comprehensive set of seven benchmarks spanning embodied, math, web, tool, and game domains, AgentSwift discovers agents that achieve an average performance gain of 8.34\\% over both existing automated agent search methods and manually designed agents.", "citations": ["Li2025Agentswift"], "pointer": "papers/paper_notes.jsonl:paper_id=P0049#key_results[0]"}], "B_highlights": [{"paper_id": "P0203", "evidence_id": "E-P0203-e294aeefb5", "excerpt": "To evaluate MuaLLM, we introduce two custom datasets: RAG-250, targeting retrieval and citation performance, and Reasoning-100 (Reas-100), focused on multistep reasoning in circuit design.", "citations": ["Abbineni2025Muallm"], "pointer": "papers/paper_notes.jsonl:paper_id=P0203#key_results[1]"}, {"paper_id": "P0047", "evidence_id": "E-P0047-cb9370ac71", "excerpt": "Comprehensive evaluations on multiple benchmarks show that A-MemGuard effectively cuts attack success rates by over 95% while incurring a minimal utility cost.", "citations": ["Wei2025Memguard"], "pointer": "papers/paper_notes.jsonl:paper_id=P0047#key_results[0]"}], "write_prompt": "Contrast Planning / reasoning loops vs Memory / retrieval augmentation along \"evaluation protocol (datasets, metrics, human evaluation)\". Ground A and B using the highlight snippets; do not introduce new claims beyond the cited evidence."}, {"axis": "compute and latency constraints", "A_label": "Agent frameworks / architectures", "B_label": "Planning / reasoning loops", "citations": ["Zhang2026Evoroute", "Wei2025Memguard", "Shang2024Agentsquare", "Li2025Agentswift"], "A_highlights": [{"paper_id": "P0124", "evidence_id": "E-P0124-60cc0d458f", "excerpt": "Experiments on challenging agentic benchmarks such as GAIA and BrowseComp+ demonstrate that EvoRoute, when integrated into off-the-shelf agentic systems, not only sustains or enhances system performance but also reduces execution cost by up to $80\\%$ and latency by over $70\\%$.", "citations": ["Zhang2026Evoroute"], "pointer": "papers/paper_notes.jsonl:paper_id=P0124#key_results[0]"}, {"paper_id": "P0047", "evidence_id": "E-P0047-cb9370ac71", "excerpt": "Comprehensive evaluations on multiple benchmarks show that A-MemGuard effectively cuts attack success rates by over 95% while incurring a minimal utility cost.", "citations": ["Wei2025Memguard"], "pointer": "papers/paper_notes.jsonl:paper_id=P0047#key_results[0]"}], "B_highlights": [{"paper_id": "P0037", "evidence_id": "E-P0037-38a26e4777", "excerpt": "Extensive experiments across six benchmarks, covering the diverse scenarios of web, embodied, tool use and game applications, show that AgentSquare substantially outperforms hand-crafted agents, achieving an average performance gain of 17.2% against best-known human designs.", "citations": ["Shang2024Agentsquare"], "pointer": "papers/paper_notes.jsonl:paper_id=P0037#key_results[0]"}, {"paper_id": "P0049", "evidence_id": "E-P0049-904ba35500", "excerpt": "Evaluated across a comprehensive set of seven benchmarks spanning embodied, math, web, tool, and game domains, AgentSwift discovers agents that achieve an average performance gain of 8.34\\% over both existing automated agent search methods and manually designed agents.", "citations": ["Li2025Agentswift"], "pointer": "papers/paper_notes.jsonl:paper_id=P0049#key_results[0]"}], "write_prompt": "Contrast Agent frameworks / architectures vs Planning / reasoning loops along \"compute and latency constraints\". Ground A and B using the highlight snippets; do not introduce new claims beyond the cited evidence."}, {"axis": "compute and latency constraints", "A_label": "Agent frameworks / architectures", "B_label": "Memory / retrieval augmentation", "citations": ["Zhang2026Evoroute", "Wei2025Memguard", "Abbineni2025Muallm"], "A_highlights": [{"paper_id": "P0124", "evidence_id": "E-P0124-60cc0d458f", "excerpt": "Experiments on challenging agentic benchmarks such as GAIA and BrowseComp+ demonstrate that EvoRoute, when integrated into off-the-shelf agentic systems, not only sustains or enhances system performance but also reduces execution cost by up to $80\\%$ and latency by over $70\\%$.", "citations": ["Zhang2026Evoroute"], "pointer": "papers/paper_notes.jsonl:paper_id=P0124#key_results[0]"}, {"paper_id": "P0047", "evidence_id": "E-P0047-cb9370ac71", "excerpt": "Comprehensive evaluations on multiple benchmarks show that A-MemGuard effectively cuts attack success rates by over 95% while incurring a minimal utility cost.", "citations": ["Wei2025Memguard"], "pointer": "papers/paper_notes.jsonl:paper_id=P0047#key_results[0]"}], "B_highlights": [{"paper_id": "P0047", "evidence_id": "E-P0047-cb9370ac71", "excerpt": "Comprehensive evaluations on multiple benchmarks show that A-MemGuard effectively cuts attack success rates by over 95% while incurring a minimal utility cost.", "citations": ["Wei2025Memguard"], "pointer": "papers/paper_notes.jsonl:paper_id=P0047#key_results[0]"}, {"paper_id": "P0203", "evidence_id": "E-P0203-e294aeefb5", "excerpt": "To evaluate MuaLLM, we introduce two custom datasets: RAG-250, targeting retrieval and citation performance, and Reasoning-100 (Reas-100), focused on multistep reasoning in circuit design.", "citations": ["Abbineni2025Muallm"], "pointer": "papers/paper_notes.jsonl:paper_id=P0203#key_results[1]"}], "write_prompt": "Contrast Agent frameworks / architectures vs Memory / retrieval augmentation along \"compute and latency constraints\". Ground A and B using the highlight snippets; do not introduce new claims beyond the cited evidence."}, {"axis": "compute and latency constraints", "A_label": "Planning / reasoning loops", "B_label": "Memory / retrieval augmentation", "citations": ["Shang2024Agentsquare", "Li2025Agentswift", "Wei2025Memguard", "Abbineni2025Muallm"], "A_highlights": [{"paper_id": "P0037", "evidence_id": "E-P0037-38a26e4777", "excerpt": "Extensive experiments across six benchmarks, covering the diverse scenarios of web, embodied, tool use and game applications, show that AgentSquare substantially outperforms hand-crafted agents, achieving an average performance gain of 17.2% against best-known human designs.", "citations": ["Shang2024Agentsquare"], "pointer": "papers/paper_notes.jsonl:paper_id=P0037#key_results[0]"}, {"paper_id": "P0049", "evidence_id": "E-P0049-904ba35500", "excerpt": "Evaluated across a comprehensive set of seven benchmarks spanning embodied, math, web, tool, and game domains, AgentSwift discovers agents that achieve an average performance gain of 8.34\\% over both existing automated agent search methods and manually designed agents.", "citations": ["Li2025Agentswift"], "pointer": "papers/paper_notes.jsonl:paper_id=P0049#key_results[0]"}], "B_highlights": [{"paper_id": "P0047", "evidence_id": "E-P0047-cb9370ac71", "excerpt": "Comprehensive evaluations on multiple benchmarks show that A-MemGuard effectively cuts attack success rates by over 95% while incurring a minimal utility cost.", "citations": ["Wei2025Memguard"], "pointer": "papers/paper_notes.jsonl:paper_id=P0047#key_results[0]"}, {"paper_id": "P0203", "evidence_id": "E-P0203-e294aeefb5", "excerpt": "To evaluate MuaLLM, we introduce two custom datasets: RAG-250, targeting retrieval and citation performance, and Reasoning-100 (Reas-100), focused on multistep reasoning in circuit design.", "citations": ["Abbineni2025Muallm"], "pointer": "papers/paper_notes.jsonl:paper_id=P0203#key_results[1]"}], "write_prompt": "Contrast Planning / reasoning loops vs Memory / retrieval augmentation along \"compute and latency constraints\". Ground A and B using the highlight snippets; do not introduce new claims beyond the cited evidence."}, {"axis": "tool interface contract (schemas / protocols)", "A_label": "Agent frameworks / architectures", "B_label": "Planning / reasoning loops", "citations": ["Li2025Agentswift", "Feng2025Group", "Shang2024Agentsquare"], "A_highlights": [{"paper_id": "P0049", "evidence_id": "E-P0049-904ba35500", "excerpt": "Evaluated across a comprehensive set of seven benchmarks spanning embodied, math, web, tool, and game domains, AgentSwift discovers agents that achieve an average performance gain of 8.34\\% over both existing automated agent search methods and manually designed agents.", "citations": ["Li2025Agentswift"], "pointer": "papers/paper_notes.jsonl:paper_id=P0049#key_results[0]"}, {"paper_id": "P0029", "evidence_id": "E-P0029-4b027dfb27", "excerpt": "We evaluate GiGPO on challenging agent benchmarks, including ALFWorld and WebShop, as well as tool-integrated reasoning on search-augmented QA tasks, using Qwen2.5-1.5B/3B/7B-Instruct.", "citations": ["Feng2025Group"], "pointer": "papers/paper_notes.jsonl:paper_id=P0029#key_results[0]"}], "B_highlights": [{"paper_id": "P0037", "evidence_id": "E-P0037-38a26e4777", "excerpt": "Extensive experiments across six benchmarks, covering the diverse scenarios of web, embodied, tool use and game applications, show that AgentSquare substantially outperforms hand-crafted agents, achieving an average performance gain of 17.2% against best-known human designs.", "citations": ["Shang2024Agentsquare"], "pointer": "papers/paper_notes.jsonl:paper_id=P0037#key_results[0]"}, {"paper_id": "P0049", "evidence_id": "E-P0049-904ba35500", "excerpt": "Evaluated across a comprehensive set of seven benchmarks spanning embodied, math, web, tool, and game domains, AgentSwift discovers agents that achieve an average performance gain of 8.34\\% over both existing automated agent search methods and manually designed agents.", "citations": ["Li2025Agentswift"], "pointer": "papers/paper_notes.jsonl:paper_id=P0049#key_results[0]"}], "write_prompt": "Contrast Agent frameworks / architectures vs Planning / reasoning loops along \"tool interface contract (schemas / protocols)\". Ground A and B using the highlight snippets; do not introduce new claims beyond the cited evidence."}], "evaluation_protocol": [{"bullet": "Evaluation mentions include: LLMs, GAIA, EvoRoute, BrowseComp, SCL, CCAM, GPT-4o-powered, ReAct, AutoGPT, ALFWorld.", "citations": ["Zhang2026Evoroute", "Kim2025Bridging", "Feng2025Group", "Hu2025Survey"]}, {"bullet": "When comparing results, anchor the paragraph with: task type + metric + constraint (budget, tool access, horizon, or threat model) when stated.", "citations": ["Zhang2026Evoroute", "Kim2025Bridging"]}, {"bullet": "Prefer head-to-head comparisons only when benchmark/metric are shared; otherwise frame differences as protocol-driven rather than method superiority.", "citations": ["Zhang2026Evoroute", "Kim2025Bridging"]}, {"bullet": "Avoid underspecified model/baseline naming; if abstracts omit details, state that the baseline is reported but underspecified instead of guessing.", "citations": ["Zhang2026Evoroute", "Kim2025Bridging"]}, {"bullet": "If a claim relies on a single reported number, pair it with a limitation/caveat from the same evidence so the draft remains conservative.", "citations": ["Zhang2026Evoroute", "Kim2025Bridging"]}, {"bullet": "If budgets or environments differ across papers, treat cross-paper numeric comparison as fragile and prefer qualitative contrasts aligned to the subsection axes.", "citations": ["Zhang2026Evoroute", "Kim2025Bridging"]}], "limitation_hooks": [{"excerpt": "We formalize this challenge as the \\textbf{Agent System Trilemma}: the inherent tension among achieving state-of-the-art performance, minimizing monetary cost, and ensuring rapid task completion.", "citations": ["Zhang2026Evoroute"], "pointer": ""}, {"excerpt": "However, this reliance on memory introduces a critical security risk: an adversary can inject seemingly harmless records into an agent's memory to manipulate its future behavior.", "citations": ["Wei2025Memguard"], "pointer": ""}, {"excerpt": "Comprehensive evaluations on multiple benchmarks show that A-MemGuard effectively cuts attack success rates by over 95% while incurring a minimal utility cost.", "citations": ["Wei2025Memguard"], "pointer": ""}, {"excerpt": "This work shifts LLM memory security from static filtering to a proactive, experience-driven model where defenses strengthen over time.", "citations": ["Wei2025Memguard"], "pointer": ""}, {"excerpt": "Large language model (LLM) agents have demonstrated strong capabilities across diverse domains, yet automated agent design remains a significant challenge.", "citations": ["Li2025Agentswift"], "pointer": ""}, {"excerpt": "To address these limitations, we propose a novel framework, Exemplar-Guided Planning (EGP), which enhances the planning capabilities of LLM agents for KGQA.", "citations": ["Xu2025Exemplar"], "pointer": ""}, {"excerpt": "Still, limitations in their training prevent them from answering accurately in scenarios that could benefit from multiple perspectives.", "citations": ["Inoue2024Drugagent"], "pointer": ""}, {"excerpt": "MPR (i) externalizes reusable corrective knowledge without model weight updates, (ii) enforces domain constraints to reduce unsafe or invalid actions, and (iii) retains the adaptability of language-based reflection.", "citations": ["Wu2025Meta"], "pointer": ""}], "must_use": {"min_anchor_facts": 4, "min_comparison_cards": 4, "min_limitation_hooks": 2, "require_cited_numeric_if_available": true, "require_multi_cite_synthesis_paragraph": true, "thesis_required": true}, "do_not_repeat_phrases": ["this run", "this run is", "pipeline", "workspace", "unit", "quality gate", "evidence pack", "evidence packs", "writer context pack", "Method note (evidence policy):", "Methodology note:", "Methodology note (evidence policy):", "Methodology note (evidence-policy):", "Method note:", "This subsection surveys", "This subsection argues", "In this subsection", "This section surveys", "This section reviews", "This section discusses", "This section covers", "This section presents", "This section introduces", "provides an overview", "This survey provides an overview", "In this section, we provide an overview", "This section provides an overview", "Next, we move from", "We now turn to", "A few representative references include", "Notable lines of work include", "Concrete examples include", "Work in this area includes", "Recent systems include", "Examples include", "Representative systems include", "Therefore, survey synthesis should", "Therefore, survey comparisons should", "As a result, survey comparisons should", "survey synthesis should", "survey comparisons should", "Taken together,", "The key point is that", "Three limitations", "Two limitations", "claims remain provisional under abstract-only evidence", "abstract-only evidence", "Key takeaway:", "Main takeaway:", "protocol token", "protocol tokens", "constraint token", "constraint tokens", "metric token", "metric tokens", "evaluation token", "evaluation tokens", "In this survey", "In our survey", "Our survey", "This survey"], "pack_warnings": [], "pack_stats": {"anchors": {"raw": 12, "considered": 10, "kept": 10, "dropped_no_cites": 0}, "comparisons": {"raw": 10, "considered": 7, "kept": 7, "dropped_no_highlights": 0, "highlights_dropped_no_cites": 0}, "evaluation_protocol": {"raw": 6, "considered": 6, "kept": 6, "dropped_no_cites": 0}, "limitation_hooks": {"raw": 8, "considered": 8, "kept": 8, "dropped_no_cites": 0}, "trim_policy": {"default": 400, "anchor_fact": 420, "highlight_excerpt": 280, "comparison_write_prompt": 420, "eval_bullet": 320, "limitation_excerpt": 320}}, "generated_at": "2026-01-25T17:09:25"}
{"sub_id": "3.2", "title": "Tool interfaces and orchestration", "section_id": "3", "section_title": "Foundations & Interfaces", "rq": "Which design choices in Tool interfaces and orchestration drive the major trade-offs, and how are those trade-offs measured?", "thesis": "Tool interfaces and orchestration methods emphasize evaluation protocol (datasets, metrics, human evaluation) and compute and latency constraints trade-offs, but synthesis is clearest when claims are tied to explicit evaluation settings and reporting conventions.", "tension_statement": "In Tool interfaces and orchestration, a practical tension is expressivity versus control: richer interfaces expand capability but make behavior harder to constrain and verify.", "evaluation_anchor_minimal": {"task": "attack/defense evaluation", "metric": "attack success rate", "constraint": "policy/sandbox setting"}, "paper_voice_palette": {"forbidden_pipeline_voice": ["this run", "this run is", "pipeline", "workspace", "unit", "quality gate", "evidence pack", "evidence packs", "writer context pack", "Method note (evidence policy):", "Methodology note:", "Methodology note (evidence policy):", "Methodology note (evidence-policy):", "Method note:"], "high_risk_templates": ["This subsection surveys", "This subsection argues", "In this subsection", "This section surveys", "This section reviews", "This section discusses", "This section covers", "This section presents", "This section introduces", "provides an overview", "This survey provides an overview", "In this section, we provide an overview", "This section provides an overview", "Next, we move from", "We now turn to", "A few representative references include", "Notable lines of work include", "Concrete examples include", "Work in this area includes", "Recent systems include", "Examples include", "Representative systems include", "Therefore, survey synthesis should", "Therefore, survey comparisons should", "As a result, survey comparisons should", "survey synthesis should", "survey comparisons should", "Taken together,", "The key point is that", "Three limitations", "Two limitations", "claims remain provisional under abstract-only evidence", "abstract-only evidence", "Key takeaway:", "Main takeaway:", "protocol token", "protocol tokens", "constraint token", "constraint tokens", "metric token", "metric tokens", "evaluation token", "evaluation tokens", "In this survey", "In our survey", "Our survey", "This survey"], "opener_archetypes": {"tension-first": ["A key tension is", "The central trade-off is", "A recurring constraint is"], "decision-first": ["For system builders, the crux is", "A practical decision is", "One design choice is"], "lens-first": ["Seen through the lens of", "From the perspective of", "Under an interface contract,"], "contrast-first": ["A useful contrast is between", "One sharp contrast is between", "A recurring split is between"], "protocol-first": ["Comparisons hinge on", "Results are only comparable when", "Evaluation claims depend on"]}, "synthesis_stems": ["Across these studies,", "Collectively,", "In summary,", "The evidence suggests that", "A consistent theme is that"], "rewrite_rules": [{"avoid_stem": "This subsection surveys", "prefer_stem": "A key tension is"}, {"avoid_stem": "In this subsection", "prefer_stem": "We focus on"}, {"avoid_stem": "provides an overview", "prefer_stem": "We focus on"}, {"avoid_stem": "Next, we move", "prefer_stem": "Having established"}, {"avoid_stem": "We now turn", "prefer_stem": "We then examine"}, {"avoid_stem": "survey comparisons should", "prefer_stem": "Across protocols, we observe"}, {"avoid_stem": "Two limitations", "prefer_stem": "A caveat is that"}, {"avoid_stem": "Three limitations", "prefer_stem": "One limitation is that"}, {"avoid_stem": "In this survey", "prefer_stem": "We examine"}, {"avoid_stem": "In our survey", "prefer_stem": "We examine"}, {"avoid_stem": "Our survey", "prefer_stem": "This work"}, {"avoid_stem": "This survey", "prefer_stem": "This work"}], "discourse_stem_watchlist": ["Additionally,", "Moreover,", "Furthermore,", "This suggests", "Taken together,", "The key point is that", "Overall,", "In summary,", "In practice,", "More broadly,", "Importantly,", "Notably,", "Crucially,", "In general,", "At a high level,", "In other words,", "Therefore,", "In addition,", "As a result,"], "discourse_stem_rewrites": {"Additionally,": ["More importantly,", "At the same time,", "A second consideration is"], "This suggests": ["This pattern indicates", "These results point to", "A plausible explanation is"], "Taken together,": ["Across these studies,", "Collectively,", "The evidence suggests"], "The key point is that": ["A practical implication is that", "A useful way to read these results is that", "One takeaway is that"], "Overall,": ["In practice,", "More broadly,", "A practical implication is that", "One way to read this evidence is that"], "In summary,": ["Across these studies,", "Collectively,", "A consistent theme is that", "The evidence suggests that"], "Importantly,": ["More importantly,", "A key constraint is that", "A practical implication is that"], "Notably,": ["In particular,", "A concrete example is that", "One implication is that"], "Crucially,": ["More importantly,", "A central constraint is that", "A key takeaway is that"], "At a high level,": ["Concretely,", "Under this protocol,", "In practice,"], "In other words,": ["Put differently,", "Equivalently,", "More concretely,"], "Therefore,": ["As a result,", "This in turn means that", "A practical implication is that"], "In addition,": ["At the same time,", "A second consideration is", "More importantly,"], "As a result,": ["This in turn means that", "Consequently,", "A practical implication is that"]}, "role_cards": {"section_author": {"mission": "Write one subsection as an argument (not a topic list), using in-scope citations as evidence.", "do": ["State a concrete tension/trade-off early and commit to a thesis.", "Make at least two explicit A-vs-B contrasts (mechanism -> outcome) instead of per-paper summaries.", "When citing numbers, add minimal context (task + metric + constraint) in the same paragraph.", "End with a limitation that changes interpretation (protocol mismatch, unclear threat model, missing ablations)."], "avoid": ["Outline narration (This subsection..., In this subsection...).", "Slide navigation (Next, we move..., We now turn...).", "Meta survey advice (survey comparisons should...).", "Cite dumps (a trailing [@a; @b; @c] without a claim)."]}, "evidence_steward": {"mission": "Keep claims auditable: citations support the sentence that needs them, and scope stays local.", "do": ["Embed citations inside claim sentences; name concrete nouns (system/benchmark/protocol) near each cite.", "Prefer subsection-scoped citations; use chapter/global only when truly cross-cutting.", "Downgrade overconfident claims when evidence is thin; convert unknowns into verification targets (once, not spam)."], "avoid": ["Out-of-scope citations to make a paragraph sound stronger.", "Repeating evidence-policy disclaimers in every subsection.", "Ambiguous model naming (e.g., GPT-5) unless the cited work uses it."]}, "style_harmonizer": {"mission": "Make the prose read like a paper: calm, specific, and varied in rhythm without sounding templated.", "do": ["Vary opener cadence across sections (tension-first / decision-first / lens-first) without reusing the same stem.", "Prefer argument bridges over navigation; keep signposting light and content-bearing.", "Reduce slash-enumerations (A/B/C axis labels) by rewriting into natural prose."], "avoid": ["Count-based openers used as a slot (e.g., Two limitations..., Three takeaways...).", "Repeated discourse stems (Additionally, This suggests, Taken together) across many paragraphs.", "Pipeline words (workspace/unit/quality gate/evidence pack) in reader-facing text.", "PPT speaker-note tone (Now we..., The remainder of...).", "Internal shorthand that reads like planning notes (e.g., protocol/metric/constraint tokens)."]}}, "version": "0.4"}, "opener_mode": "decision-first", "opener_hint": "Start with a builder/research decision; state what it depends on; end paragraph 1 with the thesis.", "axes": ["evaluation protocol (datasets, metrics, human evaluation)", "compute and latency constraints", "tool interface contract (schemas / protocols)", "tool selection / routing policy", "sandboxing / permissions / observability"], "bridge_terms": ["function calling", "tool schema", "routing", "sandbox", "observability", "benchmarks/metrics"], "contrast_hook": "tool interfaces", "required_evidence_fields": ["benchmarks/datasets", "metrics / human-eval protocol", "compute / cost (train/infer)", "training signal / supervision", "threat model", "defense surface"], "paragraph_plan": [{"para": 1, "argument_role": "setup_thesis", "intent": "Define scope, setup, and the subsection thesis (no pipeline jargon).", "focus": ["scope boundary", "key definitions", "thesis vs neighboring subsections"], "connector_to_prev": "", "connector_phrase": "", "use_clusters": ["Agent frameworks / architectures"], "rq": "Which design choices in Tool interfaces and orchestration drive the major trade-offs, and how are those trade-offs measured?"}, {"para": 2, "argument_role": "mechanism_cluster_A", "intent": "Explain cluster A: core mechanism and system architecture and what decision it makes in the agent loop.", "focus": ["cluster: Agent frameworks / architectures", "core mechanism and system architecture", "assumptions"], "connector_to_prev": "grounding", "connector_phrase": "baseline route (Agent frameworks / architectures)", "use_clusters": ["Agent frameworks / architectures"]}, {"para": 3, "argument_role": "implementation_cluster_A", "intent": "Cluster A implementation details: training and data signals and interface contract (tools/memory) that constrain behavior.", "focus": ["cluster: Agent frameworks / architectures", "training and data setup", "interface contract", "axes: evaluation protocol (datasets, metrics, human evaluation), compute and latency constraints, tool interface contract (schemas / protocols), tool selection / routing policy, sandboxing / permissions / observability"], "connector_to_prev": "elaboration", "connector_phrase": "implementation assumptions (interface + training)", "use_clusters": ["Agent frameworks / architectures"]}, {"para": 4, "argument_role": "evaluation_cluster_A", "intent": "Cluster A evaluation/trade-offs: where it works, costs (compute/latency), and typical failure modes.", "focus": ["cluster: Agent frameworks / architectures", "evaluation anchor", "efficiency", "failure modes"], "connector_to_prev": "evaluation", "connector_phrase": "evaluation anchor (task/metric/constraint) + failure modes", "use_clusters": ["Agent frameworks / architectures"]}, {"para": 5, "argument_role": "contrast_cluster_B", "intent": "Explain cluster B (contrast with A): core mechanism and system architecture and what it optimizes for.", "focus": ["cluster: Tool-use and function calling", "contrast with Agent frameworks / architectures", "core mechanism and system architecture"], "connector_to_prev": "contrast", "connector_phrase": "contrast route (Tool-use and function calling vs Agent frameworks / architectures)", "use_clusters": ["Tool-use and function calling"]}, {"para": 6, "argument_role": "implementation_cluster_B", "intent": "Cluster B implementation details: training and data and interface assumptions (mirror A for comparability).", "focus": ["cluster: Tool-use and function calling", "training and data setup", "interface contract", "axes: evaluation protocol (datasets, metrics, human evaluation), compute and latency constraints, tool interface contract (schemas / protocols), tool selection / routing policy, sandboxing / permissions / observability"], "connector_to_prev": "elaboration", "connector_phrase": "contrast implementation assumptions (B)", "use_clusters": ["Tool-use and function calling"]}, {"para": 7, "argument_role": "evaluation_cluster_B", "intent": "Cluster B evaluation/trade-offs: where it works, costs, and failure modes (mirror A).", "focus": ["cluster: Tool-use and function calling", "evaluation anchor", "efficiency", "failure modes"], "connector_to_prev": "evaluation", "connector_phrase": "contrast evaluation anchor + trade-offs (B)", "use_clusters": ["Tool-use and function calling"]}, {"para": 8, "argument_role": "cross_paper_synthesis", "intent": "Cross-paper synthesis: compare clusters along the main axes (include >=2 citations in one paragraph).", "focus": ["compare Agent frameworks / architectures vs Tool-use and function calling", "multiple citations in one paragraph", "axes: evaluation protocol (datasets, metrics, human evaluation), compute and latency constraints, tool interface contract (schemas / protocols), tool selection / routing policy, sandboxing / permissions / observability"], "connector_to_prev": "synthesis", "connector_phrase": "cross-paper synthesis (Agent frameworks / architectures vs Tool-use and function calling)", "use_clusters": ["Agent frameworks / architectures", "Tool-use and function calling", "Planning / reasoning loops"]}, {"para": 9, "argument_role": "decision_guidance", "intent": "Decision guidance: when to choose which route (criteria + evaluation signals + engineering constraints).", "focus": ["decision checklist", "evaluation protocol", "practical constraints"], "connector_to_prev": "consequence", "connector_phrase": "decision guidance / criteria", "use_clusters": ["Agent frameworks / architectures", "Tool-use and function calling", "Planning / reasoning loops"]}, {"para": 10, "argument_role": "limitations_open_questions", "intent": "Limitations + verification targets; end with a concrete open question to hand off.", "focus": ["limitations", "evidence mode: provisional", "what needs verification", "open question"], "connector_to_prev": "limitations", "connector_phrase": "limitations + verification targets", "use_clusters": ["Agent frameworks / architectures", "Tool-use and function calling", "Planning / reasoning loops"], "policy": "Use conservative language; avoid strong conclusions; prefer questions-to-answer + explicit evidence gaps list."}], "chapter_throughline": ["Pin scope to goal: agent survey latex.", "Compare approaches along: evaluation protocol (datasets, metrics, human evaluation).", "Compare approaches along: compute and latency constraints.", "Compare approaches along: tool interface contract (schemas / protocols).", "Compare approaches along: tool selection / routing policy.", "Compare approaches along: sandboxing / permissions / observability."], "chapter_key_contrasts": ["evaluation", "tool interfaces"], "chapter_synthesis_mode": "tradeoff_matrix", "allowed_bibkeys_selected": ["Li2025Dissonances", "Zhang2025Tool", "Luo2025Universe", "Zhou2026Beyond", "Zhou2025Self", "Lumer2025Memtool", "Gao2025Radar", "Shen2024Small", "Chowa2025From", "Wang2024Mllm", "Liu2025Mcpagentbench", "Yao2022React", "Li2024Review", "Wu2024Avatar", "Cheng2025Your", "Du2025Survey", "Zhang2025Generalizability", "Cui2025Toward", "Jia2025Autotool", "Li2024Stride", "Samplawski2025Agent"], "allowed_bibkeys_mapped": ["Gao2025Radar", "Wang2024Mllm", "Chowa2025From", "Jia2025Autotool", "Cui2025Toward", "Cheng2025Your", "Shen2024Small", "Zhou2026Beyond", "Samplawski2025Agent", "Li2025Dissonances", "Liu2025Mcpagentbench", "Wu2024Avatar", "Zhang2025Generalizability", "Yin2025Infobid", "Lumer2025Memtool", "Taylor2025Large", "Zhou2025Self", "Li2024Review", "Li2024Stride", "Zhu2024Menti", "Kim2025Bridging", "Du2025Survey", "Luo2025Universe", "Zhang2025Tool", "Li2024Personal", "Zhou2024Archer", "Wang2024Learning", "Yao2022React"], "allowed_bibkeys_chapter": ["Abbineni2025Muallm", "Chen2024Llmarena", "Cheng2025Your", "Chowa2025From", "Cui2025Toward", "Du2025Survey", "Feng2025Group", "Gao2025Radar", "Gasmi2025Bridging", "Hu2023Avis", "Hu2025Survey", "Inoue2024Drugagent", "Jia2025Autotool", "Jiang2024Agent", "Kim2025Bridging", "Kulkarni2025Agent", "Li2023Modelscope", "Li2024Personal", "Li2024Review", "Li2024Stride", "Li2025Agentswift", "Li2025Dissonances", "Li2025What", "Lindenbauer2025Complexity", "Liu2025Aligning", "Liu2025Mcpagentbench", "Lumer2025Memtool", "Luo2025Universe", "Maranto2024Llmsat", "Nusrat2025Automated", "Samplawski2025Agent", "Shang2024Agentsquare", "Shen2024Small", "Shen2025Feat", "Taylor2025Large", "Wang2024Learning", "Wang2024Mllm", "Wei2025Memguard", "Wu2024Avatar", "Wu2025Meta", "Xu2025Exemplar", "Yang2025Proagent", "Yao2022React", "Yin2025Infobid", "Yu2023Finmem", "Zhang2025Generalizability", "Zhang2025Tool", "Zhang2026Evoroute", "Zhou2024Archer", "Zhou2025Self", "Zhou2026Beyond", "Zhu2024Menti", "Zou2025Based"], "allowed_bibkeys_global": ["Li2023Modelscope", "Yao2022React"], "evidence_ids": ["E-P0192-fae121f81b", "E-P0229-92b9d774cc", "E-P0196-3c65d38a2a", "E-P0022-b6b7af5a81", "E-P0214-2e6956a116", "E-P0083-35271418ac", "E-P0081-419e1464da", "E-P0108-9640816b42", "E-P0009-e0bcca0d9e", "E-P0282-cd97392159", "E-P0197-3a4792de2b", "E-P0001-ca4a00b5cf", "E-P0016-dc2266b72d", "E-P0096-3575a7c673", "E-P0242-7cd49652d3", "E-P0026-f0ea009256", "E-P0010-793979aed4", "E-P0230-15e523063d", "E-P0054-d3344c79dc", "E-P0107-e483f51daa", "E-P0108-8158d9909c", "E-P0138-da6dd163cf", "E-P0229-877da0da9f", "E-P0230-171b93237a"], "anchor_facts": [{"hook_type": "quant", "text": "Evaluating each MemTool mode across 13+ LLMs on the ScaleMCP benchmark, we conducted experiments over 100 consecutive user interactions, measuring tool removal ratios (short-term memory efficiency) and task completion accuracy.", "citations": ["Lumer2025Memtool"], "paper_id": "P0083", "evidence_id": "E-P0083-35271418ac", "pointer": "papers/paper_notes.jsonl:paper_id=P0083#key_results[0]"}, {"hook_type": "quant", "text": "Furthermore, we evaluated current benchmarks and assessment protocols and have provided an analysis of 68 publicly available datasets to assess the performance of LLM-based agents in various tasks.", "citations": ["Chowa2025From"], "paper_id": "P0009", "evidence_id": "E-P0009-e0bcca0d9e", "pointer": "papers/paper_notes.jsonl:paper_id=P0009#key_results[0]"}, {"hook_type": "quant", "text": "We find AvaTaR consistently outperforms state-of-the-art approaches across all seven tasks, exhibiting strong generalization ability when applied to novel cases and achieving an average relative improvement of 14% on the Hit@1 metric for the retrieval datasets and 13% for the", "citations": ["Wu2024Avatar"], "paper_id": "P0096", "evidence_id": "E-P0096-3575a7c673", "pointer": "papers/paper_notes.jsonl:paper_id=P0096#key_results[0]"}, {"hook_type": "quant", "text": "On two interactive decision making benchmarks (ALFWorld and WebShop), ReAct outperforms imitation and reinforcement learning methods by an absolute success rate of 34% and 10% respectively, while being prompted with only one or two in-context examples.", "citations": ["Yao2022React"], "paper_id": "P0001", "evidence_id": "E-P0001-ca4a00b5cf", "pointer": "papers/paper_notes.jsonl:paper_id=P0001#key_results[0]"}, {"hook_type": "quant", "text": "Across six LLMs on the ToolBench and BFCL benchmarks, our attack expands tasks into trajectories exceeding 60,000 tokens, inflates costs by up to 658x, and raises energy by 100-560x.", "citations": ["Zhou2026Beyond"], "paper_id": "P0022", "evidence_id": "E-P0022-b6b7af5a81", "pointer": "papers/paper_notes.jsonl:paper_id=P0022#key_results[0]"}, {"hook_type": "quant", "text": "Our evaluation of 66 real-world tools from the repositories of two major LLM agent development frameworks, LangChain and LlamaIndex, revealed a significant security concern: 75% are vulnerable to XTHP attacks, highlighting the prevalence of this threat.", "citations": ["Li2025Dissonances"], "paper_id": "P0192", "evidence_id": "E-P0192-fae121f81b", "pointer": "papers/paper_notes.jsonl:paper_id=P0192#key_results[0]"}, {"hook_type": "quant", "text": "Specifically, 1) the taxonomy defines environments/tasks, common LLM-profiled roles or LMPRs (policy models, evaluators, and dynamic models), and universally applicable workflows found in prior work, and 2) it enables a comparison of key perspectives on the implementations of", "citations": ["Li2024Review"], "paper_id": "P0016", "evidence_id": "E-P0016-dc2266b72d", "pointer": "papers/paper_notes.jsonl:paper_id=P0016#key_results[0]"}, {"hook_type": "quant", "text": "To evaluate different autonomy levels, we propose four LLM paradigms: (1) centralized cooperation, where a single LLM allocates tools to all agents; (2) centralized self-organization, where a central LLM autonomously activates agents while keeping others inactive; (3) decentraliz", "citations": ["Zhang2025Tool"], "paper_id": "P0229", "evidence_id": "E-P0229-92b9d774cc", "pointer": "papers/paper_notes.jsonl:paper_id=P0229#method"}, {"hook_type": "quant", "text": "Through extensive evaluation of leading LLMs, we find that even SOTA models such as GPT-5 (43.72%), Grok-4 (33.33%) and Claude-4.0-Sonnet (29.44%) exhibit significant performance limitations.", "citations": ["Luo2025Universe"], "paper_id": "P0196", "evidence_id": "E-P0196-3c65d38a2a", "pointer": "papers/paper_notes.jsonl:paper_id=P0196#limitations[1]"}, {"hook_type": "quant", "text": "Evaluation on two existing multi-turn tool-use agent benchmarks, M3ToolEval and TauBench, shows the Self-Challenging framework achieves over a two-fold improvement in Llama-3.1-8B-Instruct, despite using only self-generated training data.", "citations": ["Zhou2025Self"], "paper_id": "P0214", "evidence_id": "E-P0214-2e6956a116", "pointer": "papers/paper_notes.jsonl:paper_id=P0214#key_results[0]"}], "comparison_cards": [{"axis": "evaluation protocol (datasets, metrics, human evaluation)", "A_label": "Agent frameworks / architectures", "B_label": "Tool-use and function calling", "citations": ["Lumer2025Memtool", "Chowa2025From"], "A_highlights": [{"paper_id": "P0083", "evidence_id": "E-P0083-35271418ac", "excerpt": "Evaluating each MemTool mode across 13+ LLMs on the ScaleMCP benchmark, we conducted experiments over 100 consecutive user interactions, measuring tool removal ratios (short-term memory efficiency) and task completion accuracy.", "citations": ["Lumer2025Memtool"], "pointer": "papers/paper_notes.jsonl:paper_id=P0083#key_results[0]"}, {"paper_id": "P0009", "evidence_id": "E-P0009-e0bcca0d9e", "excerpt": "Furthermore, we evaluated current benchmarks and assessment protocols and have provided an analysis of 68 publicly available datasets to assess the performance of LLM-based agents in various tasks.", "citations": ["Chowa2025From"], "pointer": "papers/paper_notes.jsonl:paper_id=P0009#key_results[0]"}], "B_highlights": [{"paper_id": "P0083", "evidence_id": "E-P0083-35271418ac", "excerpt": "Evaluating each MemTool mode across 13+ LLMs on the ScaleMCP benchmark, we conducted experiments over 100 consecutive user interactions, measuring tool removal ratios (short-term memory efficiency) and task completion accuracy.", "citations": ["Lumer2025Memtool"], "pointer": "papers/paper_notes.jsonl:paper_id=P0083#key_results[0]"}, {"paper_id": "P0009", "evidence_id": "E-P0009-e0bcca0d9e", "excerpt": "Furthermore, we evaluated current benchmarks and assessment protocols and have provided an analysis of 68 publicly available datasets to assess the performance of LLM-based agents in various tasks.", "citations": ["Chowa2025From"], "pointer": "papers/paper_notes.jsonl:paper_id=P0009#key_results[0]"}], "write_prompt": "Contrast Agent frameworks / architectures vs Tool-use and function calling along \"evaluation protocol (datasets, metrics, human evaluation)\". Ground A and B using the highlight snippets; do not introduce new claims beyond the cited evidence."}, {"axis": "evaluation protocol (datasets, metrics, human evaluation)", "A_label": "Agent frameworks / architectures", "B_label": "Planning / reasoning loops", "citations": ["Lumer2025Memtool", "Chowa2025From", "Wu2024Avatar", "Yao2022React"], "A_highlights": [{"paper_id": "P0083", "evidence_id": "E-P0083-35271418ac", "excerpt": "Evaluating each MemTool mode across 13+ LLMs on the ScaleMCP benchmark, we conducted experiments over 100 consecutive user interactions, measuring tool removal ratios (short-term memory efficiency) and task completion accuracy.", "citations": ["Lumer2025Memtool"], "pointer": "papers/paper_notes.jsonl:paper_id=P0083#key_results[0]"}, {"paper_id": "P0009", "evidence_id": "E-P0009-e0bcca0d9e", "excerpt": "Furthermore, we evaluated current benchmarks and assessment protocols and have provided an analysis of 68 publicly available datasets to assess the performance of LLM-based agents in various tasks.", "citations": ["Chowa2025From"], "pointer": "papers/paper_notes.jsonl:paper_id=P0009#key_results[0]"}], "B_highlights": [{"paper_id": "P0096", "evidence_id": "E-P0096-3575a7c673", "excerpt": "We find AvaTaR consistently outperforms state-of-the-art approaches across all seven tasks, exhibiting strong generalization ability when applied to novel cases and achieving an average relative improvement of 14% on the Hit@1 metric for the retrieval datasets and 13% for the", "citations": ["Wu2024Avatar"], "pointer": "papers/paper_notes.jsonl:paper_id=P0096#key_results[0]"}, {"paper_id": "P0001", "evidence_id": "E-P0001-ca4a00b5cf", "excerpt": "On two interactive decision making benchmarks (ALFWorld and WebShop), ReAct outperforms imitation and reinforcement learning methods by an absolute success rate of 34% and 10% respectively, while being prompted with only one or two in-context examples.", "citations": ["Yao2022React"], "pointer": "papers/paper_notes.jsonl:paper_id=P0001#key_results[0]"}], "write_prompt": "Contrast Agent frameworks / architectures vs Planning / reasoning loops along \"evaluation protocol (datasets, metrics, human evaluation)\". Ground A and B using the highlight snippets; do not introduce new claims beyond the cited evidence."}, {"axis": "evaluation protocol (datasets, metrics, human evaluation)", "A_label": "Tool-use and function calling", "B_label": "Planning / reasoning loops", "citations": ["Lumer2025Memtool", "Chowa2025From", "Wu2024Avatar", "Yao2022React"], "A_highlights": [{"paper_id": "P0083", "evidence_id": "E-P0083-35271418ac", "excerpt": "Evaluating each MemTool mode across 13+ LLMs on the ScaleMCP benchmark, we conducted experiments over 100 consecutive user interactions, measuring tool removal ratios (short-term memory efficiency) and task completion accuracy.", "citations": ["Lumer2025Memtool"], "pointer": "papers/paper_notes.jsonl:paper_id=P0083#key_results[0]"}, {"paper_id": "P0009", "evidence_id": "E-P0009-e0bcca0d9e", "excerpt": "Furthermore, we evaluated current benchmarks and assessment protocols and have provided an analysis of 68 publicly available datasets to assess the performance of LLM-based agents in various tasks.", "citations": ["Chowa2025From"], "pointer": "papers/paper_notes.jsonl:paper_id=P0009#key_results[0]"}], "B_highlights": [{"paper_id": "P0096", "evidence_id": "E-P0096-3575a7c673", "excerpt": "We find AvaTaR consistently outperforms state-of-the-art approaches across all seven tasks, exhibiting strong generalization ability when applied to novel cases and achieving an average relative improvement of 14% on the Hit@1 metric for the retrieval datasets and 13% for the", "citations": ["Wu2024Avatar"], "pointer": "papers/paper_notes.jsonl:paper_id=P0096#key_results[0]"}, {"paper_id": "P0001", "evidence_id": "E-P0001-ca4a00b5cf", "excerpt": "On two interactive decision making benchmarks (ALFWorld and WebShop), ReAct outperforms imitation and reinforcement learning methods by an absolute success rate of 34% and 10% respectively, while being prompted with only one or two in-context examples.", "citations": ["Yao2022React"], "pointer": "papers/paper_notes.jsonl:paper_id=P0001#key_results[0]"}], "write_prompt": "Contrast Tool-use and function calling vs Planning / reasoning loops along \"evaluation protocol (datasets, metrics, human evaluation)\". Ground A and B using the highlight snippets; do not introduce new claims beyond the cited evidence."}, {"axis": "compute and latency constraints", "A_label": "Agent frameworks / architectures", "B_label": "Tool-use and function calling", "citations": ["Zhou2026Beyond", "Lumer2025Memtool", "Li2025Dissonances"], "A_highlights": [{"paper_id": "P0022", "evidence_id": "E-P0022-b6b7af5a81", "excerpt": "Across six LLMs on the ToolBench and BFCL benchmarks, our attack expands tasks into trajectories exceeding 60,000 tokens, inflates costs by up to 658x, and raises energy by 100-560x.", "citations": ["Zhou2026Beyond"], "pointer": "papers/paper_notes.jsonl:paper_id=P0022#key_results[0]"}, {"paper_id": "P0083", "evidence_id": "E-P0083-35271418ac", "excerpt": "Evaluating each MemTool mode across 13+ LLMs on the ScaleMCP benchmark, we conducted experiments over 100 consecutive user interactions, measuring tool removal ratios (short-term memory efficiency) and task completion accuracy.", "citations": ["Lumer2025Memtool"], "pointer": "papers/paper_notes.jsonl:paper_id=P0083#key_results[0]"}], "B_highlights": [{"paper_id": "P0022", "evidence_id": "E-P0022-b6b7af5a81", "excerpt": "Across six LLMs on the ToolBench and BFCL benchmarks, our attack expands tasks into trajectories exceeding 60,000 tokens, inflates costs by up to 658x, and raises energy by 100-560x.", "citations": ["Zhou2026Beyond"], "pointer": "papers/paper_notes.jsonl:paper_id=P0022#key_results[0]"}, {"paper_id": "P0192", "evidence_id": "E-P0192-fae121f81b", "excerpt": "Our evaluation of 66 real-world tools from the repositories of two major LLM agent development frameworks, LangChain and LlamaIndex, revealed a significant security concern: 75% are vulnerable to XTHP attacks, highlighting the prevalence of this threat.", "citations": ["Li2025Dissonances"], "pointer": "papers/paper_notes.jsonl:paper_id=P0192#key_results[0]"}], "write_prompt": "Contrast Agent frameworks / architectures vs Tool-use and function calling along \"compute and latency constraints\". Ground A and B using the highlight snippets; do not introduce new claims beyond the cited evidence."}, {"axis": "compute and latency constraints", "A_label": "Agent frameworks / architectures", "B_label": "Planning / reasoning loops", "citations": ["Zhou2026Beyond", "Lumer2025Memtool", "Li2024Review", "Wu2024Avatar"], "A_highlights": [{"paper_id": "P0022", "evidence_id": "E-P0022-b6b7af5a81", "excerpt": "Across six LLMs on the ToolBench and BFCL benchmarks, our attack expands tasks into trajectories exceeding 60,000 tokens, inflates costs by up to 658x, and raises energy by 100-560x.", "citations": ["Zhou2026Beyond"], "pointer": "papers/paper_notes.jsonl:paper_id=P0022#key_results[0]"}, {"paper_id": "P0083", "evidence_id": "E-P0083-35271418ac", "excerpt": "Evaluating each MemTool mode across 13+ LLMs on the ScaleMCP benchmark, we conducted experiments over 100 consecutive user interactions, measuring tool removal ratios (short-term memory efficiency) and task completion accuracy.", "citations": ["Lumer2025Memtool"], "pointer": "papers/paper_notes.jsonl:paper_id=P0083#key_results[0]"}], "B_highlights": [{"paper_id": "P0016", "evidence_id": "E-P0016-dc2266b72d", "excerpt": "Specifically, 1) the taxonomy defines environments/tasks, common LLM-profiled roles or LMPRs (policy models, evaluators, and dynamic models), and universally applicable workflows found in prior work, and 2) it enables a comparison of key perspectives on the implementations of", "citations": ["Li2024Review"], "pointer": "papers/paper_notes.jsonl:paper_id=P0016#key_results[0]"}, {"paper_id": "P0096", "evidence_id": "E-P0096-3575a7c673", "excerpt": "We find AvaTaR consistently outperforms state-of-the-art approaches across all seven tasks, exhibiting strong generalization ability when applied to novel cases and achieving an average relative improvement of 14% on the Hit@1 metric for the retrieval datasets and 13% for the", "citations": ["Wu2024Avatar"], "pointer": "papers/paper_notes.jsonl:paper_id=P0096#key_results[0]"}], "write_prompt": "Contrast Agent frameworks / architectures vs Planning / reasoning loops along \"compute and latency constraints\". Ground A and B using the highlight snippets; do not introduce new claims beyond the cited evidence."}, {"axis": "compute and latency constraints", "A_label": "Tool-use and function calling", "B_label": "Planning / reasoning loops", "citations": ["Zhou2026Beyond", "Li2025Dissonances", "Li2024Review", "Wu2024Avatar"], "A_highlights": [{"paper_id": "P0022", "evidence_id": "E-P0022-b6b7af5a81", "excerpt": "Across six LLMs on the ToolBench and BFCL benchmarks, our attack expands tasks into trajectories exceeding 60,000 tokens, inflates costs by up to 658x, and raises energy by 100-560x.", "citations": ["Zhou2026Beyond"], "pointer": "papers/paper_notes.jsonl:paper_id=P0022#key_results[0]"}, {"paper_id": "P0192", "evidence_id": "E-P0192-fae121f81b", "excerpt": "Our evaluation of 66 real-world tools from the repositories of two major LLM agent development frameworks, LangChain and LlamaIndex, revealed a significant security concern: 75% are vulnerable to XTHP attacks, highlighting the prevalence of this threat.", "citations": ["Li2025Dissonances"], "pointer": "papers/paper_notes.jsonl:paper_id=P0192#key_results[0]"}], "B_highlights": [{"paper_id": "P0016", "evidence_id": "E-P0016-dc2266b72d", "excerpt": "Specifically, 1) the taxonomy defines environments/tasks, common LLM-profiled roles or LMPRs (policy models, evaluators, and dynamic models), and universally applicable workflows found in prior work, and 2) it enables a comparison of key perspectives on the implementations of", "citations": ["Li2024Review"], "pointer": "papers/paper_notes.jsonl:paper_id=P0016#key_results[0]"}, {"paper_id": "P0096", "evidence_id": "E-P0096-3575a7c673", "excerpt": "We find AvaTaR consistently outperforms state-of-the-art approaches across all seven tasks, exhibiting strong generalization ability when applied to novel cases and achieving an average relative improvement of 14% on the Hit@1 metric for the retrieval datasets and 13% for the", "citations": ["Wu2024Avatar"], "pointer": "papers/paper_notes.jsonl:paper_id=P0096#key_results[0]"}], "write_prompt": "Contrast Tool-use and function calling vs Planning / reasoning loops along \"compute and latency constraints\". Ground A and B using the highlight snippets; do not introduce new claims beyond the cited evidence."}, {"axis": "tool interface contract (schemas / protocols)", "A_label": "Agent frameworks / architectures", "B_label": "Tool-use and function calling", "citations": ["Lumer2025Memtool", "Chowa2025From", "Li2025Dissonances"], "A_highlights": [{"paper_id": "P0083", "evidence_id": "E-P0083-35271418ac", "excerpt": "Evaluating each MemTool mode across 13+ LLMs on the ScaleMCP benchmark, we conducted experiments over 100 consecutive user interactions, measuring tool removal ratios (short-term memory efficiency) and task completion accuracy.", "citations": ["Lumer2025Memtool"], "pointer": "papers/paper_notes.jsonl:paper_id=P0083#key_results[0]"}, {"paper_id": "P0009", "evidence_id": "E-P0009-e0bcca0d9e", "excerpt": "Furthermore, we evaluated current benchmarks and assessment protocols and have provided an analysis of 68 publicly available datasets to assess the performance of LLM-based agents in various tasks.", "citations": ["Chowa2025From"], "pointer": "papers/paper_notes.jsonl:paper_id=P0009#key_results[0]"}], "B_highlights": [{"paper_id": "P0083", "evidence_id": "E-P0083-35271418ac", "excerpt": "Evaluating each MemTool mode across 13+ LLMs on the ScaleMCP benchmark, we conducted experiments over 100 consecutive user interactions, measuring tool removal ratios (short-term memory efficiency) and task completion accuracy.", "citations": ["Lumer2025Memtool"], "pointer": "papers/paper_notes.jsonl:paper_id=P0083#key_results[0]"}, {"paper_id": "P0192", "evidence_id": "E-P0192-fae121f81b", "excerpt": "Our evaluation of 66 real-world tools from the repositories of two major LLM agent development frameworks, LangChain and LlamaIndex, revealed a significant security concern: 75% are vulnerable to XTHP attacks, highlighting the prevalence of this threat.", "citations": ["Li2025Dissonances"], "pointer": "papers/paper_notes.jsonl:paper_id=P0192#key_results[0]"}], "write_prompt": "Contrast Agent frameworks / architectures vs Tool-use and function calling along \"tool interface contract (schemas / protocols)\". Ground A and B using the highlight snippets; do not introduce new claims beyond the cited evidence."}], "evaluation_protocol": [{"bullet": "Evaluation mentions include: RAG, MCP, MCTS, LLMs, BFCL, GPU, ToolBench, LLM-based, SCL, CCAM.", "citations": ["Zhou2026Beyond", "Chowa2025From", "Zhang2025Generalizability", "Kim2025Bridging"]}, {"bullet": "When comparing results, anchor the paragraph with: task type + metric + constraint (budget, tool access, horizon, or threat model) when stated.", "citations": ["Zhou2026Beyond", "Chowa2025From"]}, {"bullet": "Prefer head-to-head comparisons only when benchmark/metric are shared; otherwise frame differences as protocol-driven rather than method superiority.", "citations": ["Zhou2026Beyond", "Chowa2025From"]}, {"bullet": "Avoid underspecified model/baseline naming; if abstracts omit details, state that the baseline is reported but underspecified instead of guessing.", "citations": ["Zhou2026Beyond", "Chowa2025From"]}, {"bullet": "If a claim relies on a single reported number, pair it with a limitation/caveat from the same evidence so the draft remains conservative.", "citations": ["Zhou2026Beyond", "Chowa2025From"]}, {"bullet": "If budgets or environments differ across papers, treat cross-paper numeric comparison as fragile and prefer qualitative contrasts aligned to the subsection axes.", "citations": ["Zhou2026Beyond", "Chowa2025From"]}], "limitation_hooks": [{"excerpt": "The agent-tool communication loop is a critical attack surface in modern Large Language Model (LLM) agents.", "citations": ["Zhou2026Beyond"], "pointer": ""}, {"excerpt": "We introduce a stealthy, multi-turn economic DoS attack that operates at the tool layer under the guise of a correctly completed task.", "citations": ["Zhou2026Beyond"], "pointer": ""}, {"excerpt": "Across six LLMs on the ToolBench and BFCL benchmarks, our attack expands tasks into trajectories exceeding 60,000 tokens, inflates costs by up to 658x, and raises energy by 100-560x.", "citations": ["Zhou2026Beyond"], "pointer": ""}, {"excerpt": "These results elevate the agent-tool interface to a first-class security frontier, demanding a paradigm shift from validating final answers to monitoring the economic and computational cost of the entire agentic process.", "citations": ["Zhou2026Beyond"], "pointer": ""}, {"excerpt": "We then review datasets, evaluation dimensions, and metrics, highlighting their limitations.", "citations": ["Zhang2025Generalizability"], "pointer": ""}, {"excerpt": "A critical challenge, however, lies in ensuring agent generalizability - the ability to maintain consistent performance across varied instructions, tasks, environments, and domains, especially those beyond agents' fine-tuning data.", "citations": ["Zhang2025Generalizability"], "pointer": ""}, {"excerpt": "Through InfoBid, we hope to foster the use of LLMs as proxies for human economic and social agents in empirical studies, enhancing our understanding of their capabilities and limitations.", "citations": ["Yin2025Infobid"], "pointer": ""}, {"excerpt": "In this paper, we present the first systematic security analysis of task control flows in multi-tool-enabled LLM agents.", "citations": ["Li2025Dissonances"], "pointer": ""}], "must_use": {"min_anchor_facts": 4, "min_comparison_cards": 4, "min_limitation_hooks": 2, "require_cited_numeric_if_available": true, "require_multi_cite_synthesis_paragraph": true, "thesis_required": true}, "do_not_repeat_phrases": ["this run", "this run is", "pipeline", "workspace", "unit", "quality gate", "evidence pack", "evidence packs", "writer context pack", "Method note (evidence policy):", "Methodology note:", "Methodology note (evidence policy):", "Methodology note (evidence-policy):", "Method note:", "This subsection surveys", "This subsection argues", "In this subsection", "This section surveys", "This section reviews", "This section discusses", "This section covers", "This section presents", "This section introduces", "provides an overview", "This survey provides an overview", "In this section, we provide an overview", "This section provides an overview", "Next, we move from", "We now turn to", "A few representative references include", "Notable lines of work include", "Concrete examples include", "Work in this area includes", "Recent systems include", "Examples include", "Representative systems include", "Therefore, survey synthesis should", "Therefore, survey comparisons should", "As a result, survey comparisons should", "survey synthesis should", "survey comparisons should", "Taken together,", "The key point is that", "Three limitations", "Two limitations", "claims remain provisional under abstract-only evidence", "abstract-only evidence", "Key takeaway:", "Main takeaway:", "protocol token", "protocol tokens", "constraint token", "constraint tokens", "metric token", "metric tokens", "evaluation token", "evaluation tokens", "In this survey", "In our survey", "Our survey", "This survey"], "pack_warnings": [], "pack_stats": {"anchors": {"raw": 12, "considered": 10, "kept": 10, "dropped_no_cites": 0}, "comparisons": {"raw": 10, "considered": 7, "kept": 7, "dropped_no_highlights": 0, "highlights_dropped_no_cites": 0}, "evaluation_protocol": {"raw": 6, "considered": 6, "kept": 6, "dropped_no_cites": 0}, "limitation_hooks": {"raw": 8, "considered": 8, "kept": 8, "dropped_no_cites": 0}, "trim_policy": {"default": 400, "anchor_fact": 420, "highlight_excerpt": 280, "comparison_write_prompt": 420, "eval_bullet": 320, "limitation_excerpt": 320}}, "generated_at": "2026-01-25T17:09:25"}
{"sub_id": "4.1", "title": "Planning and reasoning loops", "section_id": "4", "section_title": "Core Components (Planning + Memory)", "rq": "Which design choices in Planning and reasoning loops drive the major trade-offs, and how are those trade-offs measured?", "thesis": "In Planning and reasoning loops, differences in evaluation protocol (datasets, metrics, human evaluation) and compute and latency constraints frequently imply different evaluation setups, so the key is to compare under consistent protocols where possible.", "tension_statement": "In Planning and reasoning loops, a recurring tension is deliberation depth versus cost: more planning can improve reliability but increases latency and budget sensitivity.", "evaluation_anchor_minimal": {"task": "attack/defense evaluation", "metric": "attack success rate", "constraint": "policy/sandbox setting"}, "paper_voice_palette": {"forbidden_pipeline_voice": ["this run", "this run is", "pipeline", "workspace", "unit", "quality gate", "evidence pack", "evidence packs", "writer context pack", "Method note (evidence policy):", "Methodology note:", "Methodology note (evidence policy):", "Methodology note (evidence-policy):", "Method note:"], "high_risk_templates": ["This subsection surveys", "This subsection argues", "In this subsection", "This section surveys", "This section reviews", "This section discusses", "This section covers", "This section presents", "This section introduces", "provides an overview", "This survey provides an overview", "In this section, we provide an overview", "This section provides an overview", "Next, we move from", "We now turn to", "A few representative references include", "Notable lines of work include", "Concrete examples include", "Work in this area includes", "Recent systems include", "Examples include", "Representative systems include", "Therefore, survey synthesis should", "Therefore, survey comparisons should", "As a result, survey comparisons should", "survey synthesis should", "survey comparisons should", "Taken together,", "The key point is that", "Three limitations", "Two limitations", "claims remain provisional under abstract-only evidence", "abstract-only evidence", "Key takeaway:", "Main takeaway:", "protocol token", "protocol tokens", "constraint token", "constraint tokens", "metric token", "metric tokens", "evaluation token", "evaluation tokens", "In this survey", "In our survey", "Our survey", "This survey"], "opener_archetypes": {"tension-first": ["A key tension is", "The central trade-off is", "A recurring constraint is"], "decision-first": ["For system builders, the crux is", "A practical decision is", "One design choice is"], "lens-first": ["Seen through the lens of", "From the perspective of", "Under an interface contract,"], "contrast-first": ["A useful contrast is between", "One sharp contrast is between", "A recurring split is between"], "protocol-first": ["Comparisons hinge on", "Results are only comparable when", "Evaluation claims depend on"]}, "synthesis_stems": ["Across these studies,", "Collectively,", "In summary,", "The evidence suggests that", "A consistent theme is that"], "rewrite_rules": [{"avoid_stem": "This subsection surveys", "prefer_stem": "A key tension is"}, {"avoid_stem": "In this subsection", "prefer_stem": "We focus on"}, {"avoid_stem": "provides an overview", "prefer_stem": "We focus on"}, {"avoid_stem": "Next, we move", "prefer_stem": "Having established"}, {"avoid_stem": "We now turn", "prefer_stem": "We then examine"}, {"avoid_stem": "survey comparisons should", "prefer_stem": "Across protocols, we observe"}, {"avoid_stem": "Two limitations", "prefer_stem": "A caveat is that"}, {"avoid_stem": "Three limitations", "prefer_stem": "One limitation is that"}, {"avoid_stem": "In this survey", "prefer_stem": "We examine"}, {"avoid_stem": "In our survey", "prefer_stem": "We examine"}, {"avoid_stem": "Our survey", "prefer_stem": "This work"}, {"avoid_stem": "This survey", "prefer_stem": "This work"}], "discourse_stem_watchlist": ["Additionally,", "Moreover,", "Furthermore,", "This suggests", "Taken together,", "The key point is that", "Overall,", "In summary,", "In practice,", "More broadly,", "Importantly,", "Notably,", "Crucially,", "In general,", "At a high level,", "In other words,", "Therefore,", "In addition,", "As a result,"], "discourse_stem_rewrites": {"Additionally,": ["More importantly,", "At the same time,", "A second consideration is"], "This suggests": ["This pattern indicates", "These results point to", "A plausible explanation is"], "Taken together,": ["Across these studies,", "Collectively,", "The evidence suggests"], "The key point is that": ["A practical implication is that", "A useful way to read these results is that", "One takeaway is that"], "Overall,": ["In practice,", "More broadly,", "A practical implication is that", "One way to read this evidence is that"], "In summary,": ["Across these studies,", "Collectively,", "A consistent theme is that", "The evidence suggests that"], "Importantly,": ["More importantly,", "A key constraint is that", "A practical implication is that"], "Notably,": ["In particular,", "A concrete example is that", "One implication is that"], "Crucially,": ["More importantly,", "A central constraint is that", "A key takeaway is that"], "At a high level,": ["Concretely,", "Under this protocol,", "In practice,"], "In other words,": ["Put differently,", "Equivalently,", "More concretely,"], "Therefore,": ["As a result,", "This in turn means that", "A practical implication is that"], "In addition,": ["At the same time,", "A second consideration is", "More importantly,"], "As a result,": ["This in turn means that", "Consequently,", "A practical implication is that"]}, "role_cards": {"section_author": {"mission": "Write one subsection as an argument (not a topic list), using in-scope citations as evidence.", "do": ["State a concrete tension/trade-off early and commit to a thesis.", "Make at least two explicit A-vs-B contrasts (mechanism -> outcome) instead of per-paper summaries.", "When citing numbers, add minimal context (task + metric + constraint) in the same paragraph.", "End with a limitation that changes interpretation (protocol mismatch, unclear threat model, missing ablations)."], "avoid": ["Outline narration (This subsection..., In this subsection...).", "Slide navigation (Next, we move..., We now turn...).", "Meta survey advice (survey comparisons should...).", "Cite dumps (a trailing [@a; @b; @c] without a claim)."]}, "evidence_steward": {"mission": "Keep claims auditable: citations support the sentence that needs them, and scope stays local.", "do": ["Embed citations inside claim sentences; name concrete nouns (system/benchmark/protocol) near each cite.", "Prefer subsection-scoped citations; use chapter/global only when truly cross-cutting.", "Downgrade overconfident claims when evidence is thin; convert unknowns into verification targets (once, not spam)."], "avoid": ["Out-of-scope citations to make a paragraph sound stronger.", "Repeating evidence-policy disclaimers in every subsection.", "Ambiguous model naming (e.g., GPT-5) unless the cited work uses it."]}, "style_harmonizer": {"mission": "Make the prose read like a paper: calm, specific, and varied in rhythm without sounding templated.", "do": ["Vary opener cadence across sections (tension-first / decision-first / lens-first) without reusing the same stem.", "Prefer argument bridges over navigation; keep signposting light and content-bearing.", "Reduce slash-enumerations (A/B/C axis labels) by rewriting into natural prose."], "avoid": ["Count-based openers used as a slot (e.g., Two limitations..., Three takeaways...).", "Repeated discourse stems (Additionally, This suggests, Taken together) across many paragraphs.", "Pipeline words (workspace/unit/quality gate/evidence pack) in reader-facing text.", "PPT speaker-note tone (Now we..., The remainder of...).", "Internal shorthand that reads like planning notes (e.g., protocol/metric/constraint tokens)."]}}, "version": "0.4"}, "opener_mode": "protocol-first", "opener_hint": "Start with comparability constraints (protocol/budget/tool access); state what they make meaningful; end paragraph 1 with the thesis.", "axes": ["evaluation protocol (datasets, metrics, human evaluation)", "compute and latency constraints", "tool interface contract (schemas / protocols)", "tool selection / routing policy", "sandboxing / permissions / observability"], "bridge_terms": ["planner/executor", "search", "deliberation", "action grounding", "benchmarks/metrics", "compute"], "contrast_hook": "planning/control loop", "required_evidence_fields": ["benchmarks/datasets", "metrics / human-eval protocol", "compute / cost (train/infer)", "training signal / supervision", "threat model", "defense surface"], "paragraph_plan": [{"para": 1, "argument_role": "setup_thesis", "intent": "Define scope, setup, and the subsection thesis (no pipeline jargon).", "focus": ["scope boundary", "key definitions", "thesis vs neighboring subsections"], "connector_to_prev": "", "connector_phrase": "", "use_clusters": ["Planning / reasoning loops"], "rq": "Which design choices in Planning and reasoning loops drive the major trade-offs, and how are those trade-offs measured?"}, {"para": 2, "argument_role": "mechanism_cluster_A", "intent": "Explain cluster A: core mechanism and system architecture and what decision it makes in the agent loop.", "focus": ["cluster: Planning / reasoning loops", "core mechanism and system architecture", "assumptions"], "connector_to_prev": "grounding", "connector_phrase": "baseline route (Planning / reasoning loops)", "use_clusters": ["Planning / reasoning loops"]}, {"para": 3, "argument_role": "implementation_cluster_A", "intent": "Cluster A implementation details: training and data signals and interface contract (tools/memory) that constrain behavior.", "focus": ["cluster: Planning / reasoning loops", "training and data setup", "interface contract", "axes: evaluation protocol (datasets, metrics, human evaluation), compute and latency constraints, tool interface contract (schemas / protocols), tool selection / routing policy, sandboxing / permissions / observability"], "connector_to_prev": "elaboration", "connector_phrase": "implementation assumptions (interface + training)", "use_clusters": ["Planning / reasoning loops"]}, {"para": 4, "argument_role": "evaluation_cluster_A", "intent": "Cluster A evaluation/trade-offs: where it works, costs (compute/latency), and typical failure modes.", "focus": ["cluster: Planning / reasoning loops", "evaluation anchor", "efficiency", "failure modes"], "connector_to_prev": "evaluation", "connector_phrase": "evaluation anchor (task/metric/constraint) + failure modes", "use_clusters": ["Planning / reasoning loops"]}, {"para": 5, "argument_role": "contrast_cluster_B", "intent": "Explain cluster B (contrast with A): core mechanism and system architecture and what it optimizes for.", "focus": ["cluster: Agent frameworks / architectures", "contrast with Planning / reasoning loops", "core mechanism and system architecture"], "connector_to_prev": "contrast", "connector_phrase": "contrast route (Agent frameworks / architectures vs Planning / reasoning loops)", "use_clusters": ["Agent frameworks / architectures"]}, {"para": 6, "argument_role": "implementation_cluster_B", "intent": "Cluster B implementation details: training and data and interface assumptions (mirror A for comparability).", "focus": ["cluster: Agent frameworks / architectures", "training and data setup", "interface contract", "axes: evaluation protocol (datasets, metrics, human evaluation), compute and latency constraints, tool interface contract (schemas / protocols), tool selection / routing policy, sandboxing / permissions / observability"], "connector_to_prev": "elaboration", "connector_phrase": "contrast implementation assumptions (B)", "use_clusters": ["Agent frameworks / architectures"]}, {"para": 7, "argument_role": "evaluation_cluster_B", "intent": "Cluster B evaluation/trade-offs: where it works, costs, and failure modes (mirror A).", "focus": ["cluster: Agent frameworks / architectures", "evaluation anchor", "efficiency", "failure modes"], "connector_to_prev": "evaluation", "connector_phrase": "contrast evaluation anchor + trade-offs (B)", "use_clusters": ["Agent frameworks / architectures"]}, {"para": 8, "argument_role": "cross_paper_synthesis", "intent": "Cross-paper synthesis: compare clusters along the main axes (include >=2 citations in one paragraph).", "focus": ["compare Planning / reasoning loops vs Agent frameworks / architectures", "multiple citations in one paragraph", "axes: evaluation protocol (datasets, metrics, human evaluation), compute and latency constraints, tool interface contract (schemas / protocols), tool selection / routing policy, sandboxing / permissions / observability"], "connector_to_prev": "synthesis", "connector_phrase": "cross-paper synthesis (Planning / reasoning loops vs Agent frameworks / architectures)", "use_clusters": ["Planning / reasoning loops", "Agent frameworks / architectures", "Memory / retrieval augmentation"]}, {"para": 9, "argument_role": "decision_guidance", "intent": "Decision guidance: when to choose which route (criteria + evaluation signals + engineering constraints).", "focus": ["decision checklist", "evaluation protocol", "practical constraints"], "connector_to_prev": "consequence", "connector_phrase": "decision guidance / criteria", "use_clusters": ["Planning / reasoning loops", "Agent frameworks / architectures", "Memory / retrieval augmentation"]}, {"para": 10, "argument_role": "limitations_open_questions", "intent": "Limitations + verification targets; end with a concrete open question to hand off.", "focus": ["limitations", "evidence mode: provisional", "what needs verification", "open question"], "connector_to_prev": "limitations", "connector_phrase": "limitations + verification targets", "use_clusters": ["Planning / reasoning loops", "Agent frameworks / architectures", "Memory / retrieval augmentation"], "policy": "Use conservative language; avoid strong conclusions; prefer questions-to-answer + explicit evidence gaps list."}], "chapter_throughline": ["Pin scope to goal: agent survey latex.", "Compare approaches along: evaluation protocol (datasets, metrics, human evaluation).", "Compare approaches along: compute and latency constraints.", "Compare approaches along: tool interface contract (schemas / protocols).", "Compare approaches along: tool selection / routing policy.", "Compare approaches along: sandboxing / permissions / observability."], "chapter_key_contrasts": ["planning/control loop", "memory/retrieval"], "chapter_synthesis_mode": "tradeoff_matrix", "allowed_bibkeys_selected": ["Hu2025Training", "Lai2025Ustbench", "Wang2025Automated", "Chen2024Steering", "Yao2022React", "Shi2024Ehragent", "Sun2025Search", "Zhou2025Reasoning", "Wu2025Agentic", "Du2025Memr", "Hu2025Evaluating", "Ji2024Testing", "Cao2025Large", "Chu2025Bimanual", "Wei2026Agentic", "Zhou2024Large", "Silva2025Agents", "Chen2024Architectural", "Webb2023Improving", "Huang2025Surgical", "Zhou2023Navgpt", "Nusrat2025Automated"], "allowed_bibkeys_mapped": ["Hu2025Training", "Wei2026Agentic", "Cao2025Large", "Webb2023Improving", "Hatalis2025Review", "Silva2025Agents", "Sun2025Search", "Lu2025Pilotrl", "Huang2025Surgical", "Nusrat2025Automated", "Zhou2025Reasoning", "Chu2025Bimanual", "Du2025Memr", "Lai2025Ustbench", "Shi2024Ehragent", "Motwani2024Malt", "Bai2024Twostep", "Wu2025Agentic", "Hu2025Evaluating", "Kiruluta2025Novel", "Wang2025Automated", "Chen2024Architectural", "Chen2024Steering", "Ji2024Testing", "Zhou2024Large", "Zhao2024Lightva", "Zhou2023Navgpt", "Yao2022React"], "allowed_bibkeys_chapter": ["Anokhin2024Arigraph", "Bai2024Twostep", "Cao2025Large", "Chen2024Architectural", "Chen2024Steering", "Chiang2024Llamp", "Chu2025Bimanual", "Du2025Memr", "Du2025Survey", "Ge2025Surveya", "Hatalis2025Review", "Hu2025Evaluating", "Hu2025Training", "Huang2025Retrieval", "Huang2025Surgical", "Ji2024Testing", "Kang2025Distilling", "Kiruluta2025Novel", "Lai2025Ustbench", "Li2024Review", "Li2025Graphcodeagent", "Liu2023Reason", "Liu2024From", "Liu2025Echo", "Lu2025Pilotrl", "Lu2025Youtu", "Motwani2024Malt", "Nusrat2025Automated", "Shi2024Ehragent", "Silva2025Agents", "Sun2025Search", "Tao2026Membox", "Verma2026Active", "Wang2025Automated", "Webb2023Improving", "Wei2025Memguard", "Wei2025Memory", "Wei2026Agentic", "Wu2025Agentic", "Yao2022React", "Yao2025Survey", "Ye2025Task", "Ye2025Taska", "Yin2025Infobid", "Yu2023Finmem", "Yu2026Agentic", "Zhang2024Survey", "Zhang2025Largea", "Zhao2024Lightva", "Zhou2023Navgpt", "Zhou2024Large", "Zhou2025Reasoning", "Zhu2025Where"], "allowed_bibkeys_global": ["Li2023Modelscope", "Yao2022React"], "evidence_ids": ["E-P0089-771620f84f", "E-P0233-c7ba5be441", "E-P0150-4a55873ef2", "E-P0109-c0a4d47a26", "E-P0001-ca4a00b5cf", "E-P0038-2a7ea60588", "E-P0088-4ae722da57", "E-P0030-8517628bd0", "E-P0027-dac95cdbce", "E-P0200-1b99de5317", "E-P0062-0d56c76ce0", "E-P0110-c0a98eb625", "E-P0077-23a1945a58", "E-P0186-b4b0a2aed4", "E-P0042-44dce85d3e", "E-P0276-b0d63bedde", "E-P0059-b35b53de13", "E-P0092-d1af7d9a55", "E-P0114-373af18bcc", "E-P0219-327d2749fc", "E-P0117-2c12f2ff60", "E-P0055-71f2629f1b", "E-P0055-c6c63e4ab5", "E-P0117-ebbc83e2c7"], "anchor_facts": [{"hook_type": "quant", "text": "Experimental evaluation on the complex task planning benchmark demonstrates that our 1.5B parameter model trained with single-turn GRPO achieves superior performance compared to larger baseline models up to 14B parameters, with success rates of 70% for long-horizon planning", "citations": ["Hu2025Training"], "paper_id": "P0089", "evidence_id": "E-P0089-771620f84f", "pointer": "papers/paper_notes.jsonl:paper_id=P0089#key_results[0]"}, {"hook_type": "quant", "text": "From this observation, we build memory retrieval as an autonomous, accurate, and compatible agent system, named MemR$^3$, which has two core mechanisms: 1) a router that selects among retrieve, reflect, and answer actions to optimize answer quality; 2) a global evidence-gap", "citations": ["Du2025Memr"], "paper_id": "P0200", "evidence_id": "E-P0200-1b99de5317", "pointer": "papers/paper_notes.jsonl:paper_id=P0200#key_results[1]"}, {"hook_type": "quant", "text": "Experimental evaluation on the complex task planning benchmark demonstrates that our 1.5B parameter model trained with single-turn GRPO achieves superior performance compared to larger baseline models up to 14B parameters, with success rates of 70% for long-horizon planning tasks", "citations": ["Hu2025Training"], "paper_id": "P0089", "evidence_id": "E-P0089-771620f84f", "pointer": "papers/paper_notes.jsonl:paper_id=P0089#key_results[0]"}, {"hook_type": "eval", "text": "To this end, we introduce USTBench, the first benchmark to evaluate LLMs' spatiotemporal reasoning abilities as urban agents across four decomposed dimensions: spatiotemporal understanding, forecasting, planning, and reflection with feedback.", "citations": ["Lai2025Ustbench"], "paper_id": "P0233", "evidence_id": "E-P0233-c7ba5be441", "pointer": "papers/paper_notes.jsonl:paper_id=P0233#method"}, {"hook_type": "quant", "text": "While a lot of recent research focuses on enhancing the textual reasoning capabilities of Large Language Models (LLMs) by optimizing the multi-agent framework or reasoning chains, several benchmark tasks can be solved with 100\\% success through direct coding, which is more scalab", "citations": ["Chen2024Steering"], "paper_id": "P0109", "evidence_id": "E-P0109-c0a4d47a26", "pointer": "papers/paper_notes.jsonl:paper_id=P0109#key_results[0]"}, {"hook_type": "quant", "text": "On two interactive decision making benchmarks (ALFWorld and WebShop), ReAct outperforms imitation and reinforcement learning methods by an absolute success rate of 34% and 10% respectively, while being prompted with only one or two in-context examples.", "citations": ["Yao2022React"], "paper_id": "P0001", "evidence_id": "E-P0001-ca4a00b5cf", "pointer": "papers/paper_notes.jsonl:paper_id=P0001#key_results[0]"}, {"hook_type": "quant", "text": "Experiments on three real-world multi-tabular EHR datasets show that EHRAgent outperforms the strongest baseline by up to 29.6% in success rate.", "citations": ["Shi2024Ehragent"], "paper_id": "P0038", "evidence_id": "E-P0038-2a7ea60588", "pointer": "papers/paper_notes.jsonl:paper_id=P0038#key_results[0]"}, {"hook_type": "quant", "text": "We demonstrate particularly strong gains on Wikidata benchmarks (+16\\% improvement over previous best methods) alongside consistent improvements on Freebase benchmarks.", "citations": ["Sun2025Search"], "paper_id": "P0088", "evidence_id": "E-P0088-4ae722da57", "pointer": "papers/paper_notes.jsonl:paper_id=P0088#key_results[0]"}, {"hook_type": "quant", "text": "From this observation, we build memory retrieval as an autonomous, accurate, and compatible agent system, named MemR$^3$, which has two core mechanisms: 1) a router that selects among retrieve, reflect, and answer actions to optimize answer quality; 2) a global evidence-gap track", "citations": ["Du2025Memr"], "paper_id": "P0200", "evidence_id": "E-P0200-1b99de5317", "pointer": "papers/paper_notes.jsonl:paper_id=P0200#key_results[1]"}, {"hook_type": "quant", "text": "We evaluate PDoctor with three mainstream agent frameworks and two powerful LLMs (GPT-3.5 and GPT-4).", "citations": ["Ji2024Testing"], "paper_id": "P0110", "evidence_id": "E-P0110-c0a98eb625", "pointer": "papers/paper_notes.jsonl:paper_id=P0110#key_results[0]"}], "comparison_cards": [{"axis": "evaluation protocol (datasets, metrics, human evaluation)", "A_label": "Planning / reasoning loops", "B_label": "Agent frameworks / architectures", "citations": ["Cao2025Large", "Wei2026Agentic", "Hu2025Evaluating", "Hu2025Training"], "A_highlights": [{"paper_id": "P0077", "evidence_id": "E-P0077-23a1945a58", "excerpt": "Subsequently, we systematically summarize existing evaluation frameworks, including benchmark datasets, evaluation metrics and performance comparisons between representative planning methods.", "citations": ["Cao2025Large"], "pointer": "papers/paper_notes.jsonl:paper_id=P0077#key_results[1]"}, {"paper_id": "P0042", "evidence_id": "E-P0042-44dce85d3e", "excerpt": "We further review representative agentic reasoning frameworks across real-world applications and benchmarks, including science, robotics, healthcare, autonomous research, and mathematics.", "citations": ["Wei2026Agentic"], "pointer": "papers/paper_notes.jsonl:paper_id=P0042#key_results[0]"}], "B_highlights": [{"paper_id": "P0062", "evidence_id": "E-P0062-0d56c76ce0", "excerpt": "Recent benchmarks for Large Language Model (LLM) agents primarily focus on evaluating reasoning, planning, and execution capabilities, while another critical component-memory, encompassing how agents memorize, update, and retrieve long-term information-is under-evaluated due to", "citations": ["Hu2025Evaluating"], "pointer": "papers/paper_notes.jsonl:paper_id=P0062#key_results[0]"}, {"paper_id": "P0089", "evidence_id": "E-P0089-771620f84f", "excerpt": "Experimental evaluation on the complex task planning benchmark demonstrates that our 1.5B parameter model trained with single-turn GRPO achieves superior performance compared to larger baseline models up to 14B parameters, with success rates of 70% for long-horizon planning", "citations": ["Hu2025Training"], "pointer": "papers/paper_notes.jsonl:paper_id=P0089#key_results[0]"}], "write_prompt": "Contrast Planning / reasoning loops vs Agent frameworks / architectures along \"evaluation protocol (datasets, metrics, human evaluation)\". Ground A and B using the highlight snippets; do not introduce new claims beyond the cited evidence."}, {"axis": "evaluation protocol (datasets, metrics, human evaluation)", "A_label": "Planning / reasoning loops", "B_label": "Memory / retrieval augmentation", "citations": ["Cao2025Large", "Wei2026Agentic", "Du2025Memr", "Hu2025Evaluating"], "A_highlights": [{"paper_id": "P0077", "evidence_id": "E-P0077-23a1945a58", "excerpt": "Subsequently, we systematically summarize existing evaluation frameworks, including benchmark datasets, evaluation metrics and performance comparisons between representative planning methods.", "citations": ["Cao2025Large"], "pointer": "papers/paper_notes.jsonl:paper_id=P0077#key_results[1]"}, {"paper_id": "P0042", "evidence_id": "E-P0042-44dce85d3e", "excerpt": "We further review representative agentic reasoning frameworks across real-world applications and benchmarks, including science, robotics, healthcare, autonomous research, and mathematics.", "citations": ["Wei2026Agentic"], "pointer": "papers/paper_notes.jsonl:paper_id=P0042#key_results[0]"}], "B_highlights": [{"paper_id": "P0200", "evidence_id": "E-P0200-1b99de5317", "excerpt": "From this observation, we build memory retrieval as an autonomous, accurate, and compatible agent system, named MemR$^3$, which has two core mechanisms: 1) a router that selects among retrieve, reflect, and answer actions to optimize answer quality; 2) a global evidence-gap", "citations": ["Du2025Memr"], "pointer": "papers/paper_notes.jsonl:paper_id=P0200#key_results[1]"}, {"paper_id": "P0062", "evidence_id": "E-P0062-0d56c76ce0", "excerpt": "Recent benchmarks for Large Language Model (LLM) agents primarily focus on evaluating reasoning, planning, and execution capabilities, while another critical component-memory, encompassing how agents memorize, update, and retrieve long-term information-is under-evaluated due to", "citations": ["Hu2025Evaluating"], "pointer": "papers/paper_notes.jsonl:paper_id=P0062#key_results[0]"}], "write_prompt": "Contrast Planning / reasoning loops vs Memory / retrieval augmentation along \"evaluation protocol (datasets, metrics, human evaluation)\". Ground A and B using the highlight snippets; do not introduce new claims beyond the cited evidence."}, {"axis": "evaluation protocol (datasets, metrics, human evaluation)", "A_label": "Agent frameworks / architectures", "B_label": "Memory / retrieval augmentation", "citations": ["Hu2025Evaluating", "Hu2025Training", "Du2025Memr"], "A_highlights": [{"paper_id": "P0062", "evidence_id": "E-P0062-0d56c76ce0", "excerpt": "Recent benchmarks for Large Language Model (LLM) agents primarily focus on evaluating reasoning, planning, and execution capabilities, while another critical component-memory, encompassing how agents memorize, update, and retrieve long-term information-is under-evaluated due to", "citations": ["Hu2025Evaluating"], "pointer": "papers/paper_notes.jsonl:paper_id=P0062#key_results[0]"}, {"paper_id": "P0089", "evidence_id": "E-P0089-771620f84f", "excerpt": "Experimental evaluation on the complex task planning benchmark demonstrates that our 1.5B parameter model trained with single-turn GRPO achieves superior performance compared to larger baseline models up to 14B parameters, with success rates of 70% for long-horizon planning", "citations": ["Hu2025Training"], "pointer": "papers/paper_notes.jsonl:paper_id=P0089#key_results[0]"}], "B_highlights": [{"paper_id": "P0200", "evidence_id": "E-P0200-1b99de5317", "excerpt": "From this observation, we build memory retrieval as an autonomous, accurate, and compatible agent system, named MemR$^3$, which has two core mechanisms: 1) a router that selects among retrieve, reflect, and answer actions to optimize answer quality; 2) a global evidence-gap", "citations": ["Du2025Memr"], "pointer": "papers/paper_notes.jsonl:paper_id=P0200#key_results[1]"}, {"paper_id": "P0062", "evidence_id": "E-P0062-0d56c76ce0", "excerpt": "Recent benchmarks for Large Language Model (LLM) agents primarily focus on evaluating reasoning, planning, and execution capabilities, while another critical component-memory, encompassing how agents memorize, update, and retrieve long-term information-is under-evaluated due to", "citations": ["Hu2025Evaluating"], "pointer": "papers/paper_notes.jsonl:paper_id=P0062#key_results[0]"}], "write_prompt": "Contrast Agent frameworks / architectures vs Memory / retrieval augmentation along \"evaluation protocol (datasets, metrics, human evaluation)\". Ground A and B using the highlight snippets; do not introduce new claims beyond the cited evidence."}, {"axis": "compute and latency constraints", "A_label": "Planning / reasoning loops", "B_label": "Agent frameworks / architectures", "citations": ["Cao2025Large", "Silva2025Agents", "Hu2025Training", "Hu2025Evaluating"], "A_highlights": [{"paper_id": "P0077", "evidence_id": "E-P0077-23a1945a58", "excerpt": "Subsequently, we systematically summarize existing evaluation frameworks, including benchmark datasets, evaluation metrics and performance comparisons between representative planning methods.", "citations": ["Cao2025Large"], "pointer": "papers/paper_notes.jsonl:paper_id=P0077#key_results[1]"}, {"paper_id": "P0059", "evidence_id": "E-P0059-b35b53de13", "excerpt": "We systematically evaluate their performance using a suite of coordination-sensitive metrics, including task success rate, redundant actions, room conflicts, and urgency-weighted efficiency.", "citations": ["Silva2025Agents"], "pointer": "papers/paper_notes.jsonl:paper_id=P0059#key_results[0]"}], "B_highlights": [{"paper_id": "P0089", "evidence_id": "E-P0089-771620f84f", "excerpt": "Experimental evaluation on the complex task planning benchmark demonstrates that our 1.5B parameter model trained with single-turn GRPO achieves superior performance compared to larger baseline models up to 14B parameters, with success rates of 70% for long-horizon planning", "citations": ["Hu2025Training"], "pointer": "papers/paper_notes.jsonl:paper_id=P0089#key_results[0]"}, {"paper_id": "P0062", "evidence_id": "E-P0062-0d56c76ce0", "excerpt": "Recent benchmarks for Large Language Model (LLM) agents primarily focus on evaluating reasoning, planning, and execution capabilities, while another critical component-memory, encompassing how agents memorize, update, and retrieve long-term information-is under-evaluated due to", "citations": ["Hu2025Evaluating"], "pointer": "papers/paper_notes.jsonl:paper_id=P0062#key_results[0]"}], "write_prompt": "Contrast Planning / reasoning loops vs Agent frameworks / architectures along \"compute and latency constraints\". Ground A and B using the highlight snippets; do not introduce new claims beyond the cited evidence."}, {"axis": "compute and latency constraints", "A_label": "Planning / reasoning loops", "B_label": "Memory / retrieval augmentation", "citations": ["Cao2025Large", "Silva2025Agents", "Du2025Memr", "Hu2025Evaluating"], "A_highlights": [{"paper_id": "P0077", "evidence_id": "E-P0077-23a1945a58", "excerpt": "Subsequently, we systematically summarize existing evaluation frameworks, including benchmark datasets, evaluation metrics and performance comparisons between representative planning methods.", "citations": ["Cao2025Large"], "pointer": "papers/paper_notes.jsonl:paper_id=P0077#key_results[1]"}, {"paper_id": "P0059", "evidence_id": "E-P0059-b35b53de13", "excerpt": "We systematically evaluate their performance using a suite of coordination-sensitive metrics, including task success rate, redundant actions, room conflicts, and urgency-weighted efficiency.", "citations": ["Silva2025Agents"], "pointer": "papers/paper_notes.jsonl:paper_id=P0059#key_results[0]"}], "B_highlights": [{"paper_id": "P0200", "evidence_id": "E-P0200-1b99de5317", "excerpt": "From this observation, we build memory retrieval as an autonomous, accurate, and compatible agent system, named MemR$^3$, which has two core mechanisms: 1) a router that selects among retrieve, reflect, and answer actions to optimize answer quality; 2) a global evidence-gap", "citations": ["Du2025Memr"], "pointer": "papers/paper_notes.jsonl:paper_id=P0200#key_results[1]"}, {"paper_id": "P0062", "evidence_id": "E-P0062-0d56c76ce0", "excerpt": "Recent benchmarks for Large Language Model (LLM) agents primarily focus on evaluating reasoning, planning, and execution capabilities, while another critical component-memory, encompassing how agents memorize, update, and retrieve long-term information-is under-evaluated due to", "citations": ["Hu2025Evaluating"], "pointer": "papers/paper_notes.jsonl:paper_id=P0062#key_results[0]"}], "write_prompt": "Contrast Planning / reasoning loops vs Memory / retrieval augmentation along \"compute and latency constraints\". Ground A and B using the highlight snippets; do not introduce new claims beyond the cited evidence."}, {"axis": "compute and latency constraints", "A_label": "Agent frameworks / architectures", "B_label": "Memory / retrieval augmentation", "citations": ["Hu2025Training", "Hu2025Evaluating", "Du2025Memr"], "A_highlights": [{"paper_id": "P0089", "evidence_id": "E-P0089-771620f84f", "excerpt": "Experimental evaluation on the complex task planning benchmark demonstrates that our 1.5B parameter model trained with single-turn GRPO achieves superior performance compared to larger baseline models up to 14B parameters, with success rates of 70% for long-horizon planning", "citations": ["Hu2025Training"], "pointer": "papers/paper_notes.jsonl:paper_id=P0089#key_results[0]"}, {"paper_id": "P0062", "evidence_id": "E-P0062-0d56c76ce0", "excerpt": "Recent benchmarks for Large Language Model (LLM) agents primarily focus on evaluating reasoning, planning, and execution capabilities, while another critical component-memory, encompassing how agents memorize, update, and retrieve long-term information-is under-evaluated due to", "citations": ["Hu2025Evaluating"], "pointer": "papers/paper_notes.jsonl:paper_id=P0062#key_results[0]"}], "B_highlights": [{"paper_id": "P0200", "evidence_id": "E-P0200-1b99de5317", "excerpt": "From this observation, we build memory retrieval as an autonomous, accurate, and compatible agent system, named MemR$^3$, which has two core mechanisms: 1) a router that selects among retrieve, reflect, and answer actions to optimize answer quality; 2) a global evidence-gap", "citations": ["Du2025Memr"], "pointer": "papers/paper_notes.jsonl:paper_id=P0200#key_results[1]"}, {"paper_id": "P0062", "evidence_id": "E-P0062-0d56c76ce0", "excerpt": "Recent benchmarks for Large Language Model (LLM) agents primarily focus on evaluating reasoning, planning, and execution capabilities, while another critical component-memory, encompassing how agents memorize, update, and retrieve long-term information-is under-evaluated due to", "citations": ["Hu2025Evaluating"], "pointer": "papers/paper_notes.jsonl:paper_id=P0062#key_results[0]"}], "write_prompt": "Contrast Agent frameworks / architectures vs Memory / retrieval augmentation along \"compute and latency constraints\". Ground A and B using the highlight snippets; do not introduce new claims beyond the cited evidence."}, {"axis": "tool interface contract (schemas / protocols)", "A_label": "Planning / reasoning loops", "B_label": "Agent frameworks / architectures", "citations": ["Wu2025Agentic", "Cao2025Large", "Hu2025Evaluating"], "A_highlights": [{"paper_id": "P0027", "evidence_id": "E-P0027-dac95cdbce", "excerpt": "We introduce Agentic Reasoning, a framework that enhances large language model (LLM) reasoning by integrating external tool-using agents.", "citations": ["Wu2025Agentic"], "pointer": "papers/paper_notes.jsonl:paper_id=P0027#method"}, {"paper_id": "P0077", "evidence_id": "E-P0077-23a1945a58", "excerpt": "Subsequently, we systematically summarize existing evaluation frameworks, including benchmark datasets, evaluation metrics and performance comparisons between representative planning methods.", "citations": ["Cao2025Large"], "pointer": "papers/paper_notes.jsonl:paper_id=P0077#key_results[1]"}], "B_highlights": [{"paper_id": "P0027", "evidence_id": "E-P0027-dac95cdbce", "excerpt": "We introduce Agentic Reasoning, a framework that enhances large language model (LLM) reasoning by integrating external tool-using agents.", "citations": ["Wu2025Agentic"], "pointer": "papers/paper_notes.jsonl:paper_id=P0027#method"}, {"paper_id": "P0062", "evidence_id": "E-P0062-0d56c76ce0", "excerpt": "Recent benchmarks for Large Language Model (LLM) agents primarily focus on evaluating reasoning, planning, and execution capabilities, while another critical component-memory, encompassing how agents memorize, update, and retrieve long-term information-is under-evaluated due to", "citations": ["Hu2025Evaluating"], "pointer": "papers/paper_notes.jsonl:paper_id=P0062#key_results[0]"}], "write_prompt": "Contrast Planning / reasoning loops vs Agent frameworks / architectures along \"tool interface contract (schemas / protocols)\". Ground A and B using the highlight snippets; do not introduce new claims beyond the cited evidence."}], "evaluation_protocol": [{"bullet": "Evaluation mentions include: LLMs, CBR, CBR-enhanced, SOTA, DeepSeek-R1, OpenAI, RSP, GSI, RSV, FEVER.", "citations": ["Wei2026Agentic", "Hatalis2025Review", "Wu2025Agentic", "Zhou2025Reasoning"]}, {"bullet": "When comparing results, anchor the paragraph with: task type + metric + constraint (budget, tool access, horizon, or threat model) when stated.", "citations": ["Wei2026Agentic", "Hatalis2025Review"]}, {"bullet": "Prefer head-to-head comparisons only when benchmark/metric are shared; otherwise frame differences as protocol-driven rather than method superiority.", "citations": ["Wei2026Agentic", "Hatalis2025Review"]}, {"bullet": "Avoid underspecified model/baseline naming; if abstracts omit details, state that the baseline is reported but underspecified instead of guessing.", "citations": ["Wei2026Agentic", "Hatalis2025Review"]}, {"bullet": "If a claim relies on a single reported number, pair it with a limitation/caveat from the same evidence so the draft remains conservative.", "citations": ["Wei2026Agentic", "Hatalis2025Review"]}, {"bullet": "If budgets or environments differ across papers, treat cross-paper numeric comparison as fragile and prefer qualitative contrasts aligned to the subsection axes.", "citations": ["Wei2026Agentic", "Hatalis2025Review"]}], "limitation_hooks": [{"excerpt": "Still, they face limitations in tasks requiring specific, structured knowledge, flexibility, or accountable decision-making.", "citations": ["Hatalis2025Review"], "pointer": ""}, {"excerpt": "While existing adversarial attacks primarily focus on content falsification or instruction injection, we identify a novel, process-oriented attack surface: the agent's reasoning style.", "citations": ["Zhou2025Reasoning"], "pointer": ""}, {"excerpt": "We introduce Generative Style Injection (GSI), an attack method that rewrites retrieved documents into pathological tones--specifically \"analysis paralysis\" or \"cognitive haste\"--without altering underlying facts or using explicit triggers.", "citations": ["Zhou2025Reasoning"], "pointer": ""}, {"excerpt": "This study offers new insights into the strengths and failure modes of LLMs in physically grounded multi-agent collaboration tasks, contributing to future benchmarks and architectural improvements.", "citations": ["Silva2025Agents"], "pointer": ""}, {"excerpt": "To address these limitations, we propose Search-on-Graph (SoG), a simple yet effective framework that enables LLMs to perform iterative informed graph navigation using a single, carefully designed \\textsc{Search} function.", "citations": ["Sun2025Search"], "pointer": ""}, {"excerpt": "Large language models (LLMs) have demonstrated impressive reasoning abilities yet remain unreliable on knowledge-intensive, multi-hop questions -- they miss long-tail facts, hallucinate when uncertain, and their internal knowledge lags behind real-world change.", "citations": ["Sun2025Search"], "pointer": ""}], "must_use": {"min_anchor_facts": 4, "min_comparison_cards": 4, "min_limitation_hooks": 2, "require_cited_numeric_if_available": true, "require_multi_cite_synthesis_paragraph": true, "thesis_required": true}, "do_not_repeat_phrases": ["this run", "this run is", "pipeline", "workspace", "unit", "quality gate", "evidence pack", "evidence packs", "writer context pack", "Method note (evidence policy):", "Methodology note:", "Methodology note (evidence policy):", "Methodology note (evidence-policy):", "Method note:", "This subsection surveys", "This subsection argues", "In this subsection", "This section surveys", "This section reviews", "This section discusses", "This section covers", "This section presents", "This section introduces", "provides an overview", "This survey provides an overview", "In this section, we provide an overview", "This section provides an overview", "Next, we move from", "We now turn to", "A few representative references include", "Notable lines of work include", "Concrete examples include", "Work in this area includes", "Recent systems include", "Examples include", "Representative systems include", "Therefore, survey synthesis should", "Therefore, survey comparisons should", "As a result, survey comparisons should", "survey synthesis should", "survey comparisons should", "Taken together,", "The key point is that", "Three limitations", "Two limitations", "claims remain provisional under abstract-only evidence", "abstract-only evidence", "Key takeaway:", "Main takeaway:", "protocol token", "protocol tokens", "constraint token", "constraint tokens", "metric token", "metric tokens", "evaluation token", "evaluation tokens", "In this survey", "In our survey", "Our survey", "This survey"], "pack_warnings": [], "pack_stats": {"anchors": {"raw": 12, "considered": 10, "kept": 10, "dropped_no_cites": 0}, "comparisons": {"raw": 10, "considered": 7, "kept": 7, "dropped_no_highlights": 0, "highlights_dropped_no_cites": 0}, "evaluation_protocol": {"raw": 6, "considered": 6, "kept": 6, "dropped_no_cites": 0}, "limitation_hooks": {"raw": 6, "considered": 6, "kept": 6, "dropped_no_cites": 0}, "trim_policy": {"default": 400, "anchor_fact": 420, "highlight_excerpt": 280, "comparison_write_prompt": 420, "eval_bullet": 320, "limitation_excerpt": 320}}, "generated_at": "2026-01-25T17:09:25"}
{"sub_id": "4.2", "title": "Memory and retrieval (RAG)", "section_id": "4", "section_title": "Core Components (Planning + Memory)", "rq": "Which design choices in Memory and retrieval (RAG) drive the major trade-offs, and how are those trade-offs measured?", "thesis": "In Memory and retrieval (RAG), differences in evaluation protocol (datasets, metrics, human evaluation) and compute and latency constraints frequently imply different evaluation setups, so the key is to compare under consistent protocols where possible.", "tension_statement": "In Memory and retrieval (RAG), the core tension is persistence versus freshness: retaining more context helps long-horizon tasks but raises staleness, contamination, and verification challenges.", "evaluation_anchor_minimal": {"task": "attack/defense evaluation", "metric": "attack success rate", "constraint": "policy/sandbox setting"}, "paper_voice_palette": {"forbidden_pipeline_voice": ["this run", "this run is", "pipeline", "workspace", "unit", "quality gate", "evidence pack", "evidence packs", "writer context pack", "Method note (evidence policy):", "Methodology note:", "Methodology note (evidence policy):", "Methodology note (evidence-policy):", "Method note:"], "high_risk_templates": ["This subsection surveys", "This subsection argues", "In this subsection", "This section surveys", "This section reviews", "This section discusses", "This section covers", "This section presents", "This section introduces", "provides an overview", "This survey provides an overview", "In this section, we provide an overview", "This section provides an overview", "Next, we move from", "We now turn to", "A few representative references include", "Notable lines of work include", "Concrete examples include", "Work in this area includes", "Recent systems include", "Examples include", "Representative systems include", "Therefore, survey synthesis should", "Therefore, survey comparisons should", "As a result, survey comparisons should", "survey synthesis should", "survey comparisons should", "Taken together,", "The key point is that", "Three limitations", "Two limitations", "claims remain provisional under abstract-only evidence", "abstract-only evidence", "Key takeaway:", "Main takeaway:", "protocol token", "protocol tokens", "constraint token", "constraint tokens", "metric token", "metric tokens", "evaluation token", "evaluation tokens", "In this survey", "In our survey", "Our survey", "This survey"], "opener_archetypes": {"tension-first": ["A key tension is", "The central trade-off is", "A recurring constraint is"], "decision-first": ["For system builders, the crux is", "A practical decision is", "One design choice is"], "lens-first": ["Seen through the lens of", "From the perspective of", "Under an interface contract,"], "contrast-first": ["A useful contrast is between", "One sharp contrast is between", "A recurring split is between"], "protocol-first": ["Comparisons hinge on", "Results are only comparable when", "Evaluation claims depend on"]}, "synthesis_stems": ["Across these studies,", "Collectively,", "In summary,", "The evidence suggests that", "A consistent theme is that"], "rewrite_rules": [{"avoid_stem": "This subsection surveys", "prefer_stem": "A key tension is"}, {"avoid_stem": "In this subsection", "prefer_stem": "We focus on"}, {"avoid_stem": "provides an overview", "prefer_stem": "We focus on"}, {"avoid_stem": "Next, we move", "prefer_stem": "Having established"}, {"avoid_stem": "We now turn", "prefer_stem": "We then examine"}, {"avoid_stem": "survey comparisons should", "prefer_stem": "Across protocols, we observe"}, {"avoid_stem": "Two limitations", "prefer_stem": "A caveat is that"}, {"avoid_stem": "Three limitations", "prefer_stem": "One limitation is that"}, {"avoid_stem": "In this survey", "prefer_stem": "We examine"}, {"avoid_stem": "In our survey", "prefer_stem": "We examine"}, {"avoid_stem": "Our survey", "prefer_stem": "This work"}, {"avoid_stem": "This survey", "prefer_stem": "This work"}], "discourse_stem_watchlist": ["Additionally,", "Moreover,", "Furthermore,", "This suggests", "Taken together,", "The key point is that", "Overall,", "In summary,", "In practice,", "More broadly,", "Importantly,", "Notably,", "Crucially,", "In general,", "At a high level,", "In other words,", "Therefore,", "In addition,", "As a result,"], "discourse_stem_rewrites": {"Additionally,": ["More importantly,", "At the same time,", "A second consideration is"], "This suggests": ["This pattern indicates", "These results point to", "A plausible explanation is"], "Taken together,": ["Across these studies,", "Collectively,", "The evidence suggests"], "The key point is that": ["A practical implication is that", "A useful way to read these results is that", "One takeaway is that"], "Overall,": ["In practice,", "More broadly,", "A practical implication is that", "One way to read this evidence is that"], "In summary,": ["Across these studies,", "Collectively,", "A consistent theme is that", "The evidence suggests that"], "Importantly,": ["More importantly,", "A key constraint is that", "A practical implication is that"], "Notably,": ["In particular,", "A concrete example is that", "One implication is that"], "Crucially,": ["More importantly,", "A central constraint is that", "A key takeaway is that"], "At a high level,": ["Concretely,", "Under this protocol,", "In practice,"], "In other words,": ["Put differently,", "Equivalently,", "More concretely,"], "Therefore,": ["As a result,", "This in turn means that", "A practical implication is that"], "In addition,": ["At the same time,", "A second consideration is", "More importantly,"], "As a result,": ["This in turn means that", "Consequently,", "A practical implication is that"]}, "role_cards": {"section_author": {"mission": "Write one subsection as an argument (not a topic list), using in-scope citations as evidence.", "do": ["State a concrete tension/trade-off early and commit to a thesis.", "Make at least two explicit A-vs-B contrasts (mechanism -> outcome) instead of per-paper summaries.", "When citing numbers, add minimal context (task + metric + constraint) in the same paragraph.", "End with a limitation that changes interpretation (protocol mismatch, unclear threat model, missing ablations)."], "avoid": ["Outline narration (This subsection..., In this subsection...).", "Slide navigation (Next, we move..., We now turn...).", "Meta survey advice (survey comparisons should...).", "Cite dumps (a trailing [@a; @b; @c] without a claim)."]}, "evidence_steward": {"mission": "Keep claims auditable: citations support the sentence that needs them, and scope stays local.", "do": ["Embed citations inside claim sentences; name concrete nouns (system/benchmark/protocol) near each cite.", "Prefer subsection-scoped citations; use chapter/global only when truly cross-cutting.", "Downgrade overconfident claims when evidence is thin; convert unknowns into verification targets (once, not spam)."], "avoid": ["Out-of-scope citations to make a paragraph sound stronger.", "Repeating evidence-policy disclaimers in every subsection.", "Ambiguous model naming (e.g., GPT-5) unless the cited work uses it."]}, "style_harmonizer": {"mission": "Make the prose read like a paper: calm, specific, and varied in rhythm without sounding templated.", "do": ["Vary opener cadence across sections (tension-first / decision-first / lens-first) without reusing the same stem.", "Prefer argument bridges over navigation; keep signposting light and content-bearing.", "Reduce slash-enumerations (A/B/C axis labels) by rewriting into natural prose."], "avoid": ["Count-based openers used as a slot (e.g., Two limitations..., Three takeaways...).", "Repeated discourse stems (Additionally, This suggests, Taken together) across many paragraphs.", "Pipeline words (workspace/unit/quality gate/evidence pack) in reader-facing text.", "PPT speaker-note tone (Now we..., The remainder of...).", "Internal shorthand that reads like planning notes (e.g., protocol/metric/constraint tokens)."]}}, "version": "0.4"}, "opener_mode": "lens-first", "opener_hint": "Start by naming the lens (interface/protocol/threat model); state what it reveals; end paragraph 1 with the thesis.", "axes": ["evaluation protocol (datasets, metrics, human evaluation)", "compute and latency constraints", "tool interface contract (schemas / protocols)", "tool selection / routing policy", "sandboxing / permissions / observability"], "bridge_terms": ["retrieval", "index", "write policy", "long-term memory", "benchmarks/metrics", "compute"], "contrast_hook": "memory/retrieval", "required_evidence_fields": ["benchmarks/datasets", "metrics / human-eval protocol", "compute / cost (train/infer)", "training signal / supervision", "threat model", "defense surface"], "paragraph_plan": [{"para": 1, "argument_role": "setup_thesis", "intent": "Define scope, setup, and the subsection thesis (no pipeline jargon).", "focus": ["scope boundary", "key definitions", "thesis vs neighboring subsections"], "connector_to_prev": "", "connector_phrase": "", "use_clusters": ["Agent frameworks / architectures"], "rq": "Which design choices in Memory and retrieval (RAG) drive the major trade-offs, and how are those trade-offs measured?"}, {"para": 2, "argument_role": "mechanism_cluster_A", "intent": "Explain cluster A: core mechanism and system architecture and what decision it makes in the agent loop.", "focus": ["cluster: Agent frameworks / architectures", "core mechanism and system architecture", "assumptions"], "connector_to_prev": "grounding", "connector_phrase": "baseline route (Agent frameworks / architectures)", "use_clusters": ["Agent frameworks / architectures"]}, {"para": 3, "argument_role": "implementation_cluster_A", "intent": "Cluster A implementation details: training and data signals and interface contract (tools/memory) that constrain behavior.", "focus": ["cluster: Agent frameworks / architectures", "training and data setup", "interface contract", "axes: evaluation protocol (datasets, metrics, human evaluation), compute and latency constraints, tool interface contract (schemas / protocols), tool selection / routing policy, sandboxing / permissions / observability"], "connector_to_prev": "elaboration", "connector_phrase": "implementation assumptions (interface + training)", "use_clusters": ["Agent frameworks / architectures"]}, {"para": 4, "argument_role": "evaluation_cluster_A", "intent": "Cluster A evaluation/trade-offs: where it works, costs (compute/latency), and typical failure modes.", "focus": ["cluster: Agent frameworks / architectures", "evaluation anchor", "efficiency", "failure modes"], "connector_to_prev": "evaluation", "connector_phrase": "evaluation anchor (task/metric/constraint) + failure modes", "use_clusters": ["Agent frameworks / architectures"]}, {"para": 5, "argument_role": "contrast_cluster_B", "intent": "Explain cluster B (contrast with A): core mechanism and system architecture and what it optimizes for.", "focus": ["cluster: Memory / retrieval augmentation", "contrast with Agent frameworks / architectures", "core mechanism and system architecture"], "connector_to_prev": "contrast", "connector_phrase": "contrast route (Memory / retrieval augmentation vs Agent frameworks / architectures)", "use_clusters": ["Memory / retrieval augmentation"]}, {"para": 6, "argument_role": "implementation_cluster_B", "intent": "Cluster B implementation details: training and data and interface assumptions (mirror A for comparability).", "focus": ["cluster: Memory / retrieval augmentation", "training and data setup", "interface contract", "axes: evaluation protocol (datasets, metrics, human evaluation), compute and latency constraints, tool interface contract (schemas / protocols), tool selection / routing policy, sandboxing / permissions / observability"], "connector_to_prev": "elaboration", "connector_phrase": "contrast implementation assumptions (B)", "use_clusters": ["Memory / retrieval augmentation"]}, {"para": 7, "argument_role": "evaluation_cluster_B", "intent": "Cluster B evaluation/trade-offs: where it works, costs, and failure modes (mirror A).", "focus": ["cluster: Memory / retrieval augmentation", "evaluation anchor", "efficiency", "failure modes"], "connector_to_prev": "evaluation", "connector_phrase": "contrast evaluation anchor + trade-offs (B)", "use_clusters": ["Memory / retrieval augmentation"]}, {"para": 8, "argument_role": "cross_paper_synthesis", "intent": "Cross-paper synthesis: compare clusters along the main axes (include >=2 citations in one paragraph).", "focus": ["compare Agent frameworks / architectures vs Memory / retrieval augmentation", "multiple citations in one paragraph", "axes: evaluation protocol (datasets, metrics, human evaluation), compute and latency constraints, tool interface contract (schemas / protocols), tool selection / routing policy, sandboxing / permissions / observability"], "connector_to_prev": "synthesis", "connector_phrase": "cross-paper synthesis (Agent frameworks / architectures vs Memory / retrieval augmentation)", "use_clusters": ["Agent frameworks / architectures", "Memory / retrieval augmentation", "Planning / reasoning loops"]}, {"para": 9, "argument_role": "decision_guidance", "intent": "Decision guidance: when to choose which route (criteria + evaluation signals + engineering constraints).", "focus": ["decision checklist", "evaluation protocol", "practical constraints"], "connector_to_prev": "consequence", "connector_phrase": "decision guidance / criteria", "use_clusters": ["Agent frameworks / architectures", "Memory / retrieval augmentation", "Planning / reasoning loops"]}, {"para": 10, "argument_role": "limitations_open_questions", "intent": "Limitations + verification targets; end with a concrete open question to hand off.", "focus": ["limitations", "evidence mode: provisional", "what needs verification", "open question"], "connector_to_prev": "limitations", "connector_phrase": "limitations + verification targets", "use_clusters": ["Agent frameworks / architectures", "Memory / retrieval augmentation", "Planning / reasoning loops"], "policy": "Use conservative language; avoid strong conclusions; prefer questions-to-answer + explicit evidence gaps list."}], "chapter_throughline": ["Pin scope to goal: agent survey latex.", "Compare approaches along: evaluation protocol (datasets, metrics, human evaluation).", "Compare approaches along: compute and latency constraints.", "Compare approaches along: tool interface contract (schemas / protocols).", "Compare approaches along: tool selection / routing policy.", "Compare approaches along: sandboxing / permissions / observability."], "chapter_key_contrasts": ["planning/control loop", "memory/retrieval"], "chapter_synthesis_mode": "tradeoff_matrix", "allowed_bibkeys_selected": ["Wei2025Memguard", "Kang2025Distilling", "Ge2025Surveya", "Zhang2025Largea", "Du2025Memr", "Wei2025Memory", "Huang2025Retrieval", "Yao2022React", "Zhu2025Where", "Li2024Review", "Verma2026Active", "Du2025Survey", "Hu2025Evaluating", "Yao2025Survey", "Yu2026Agentic", "Liu2025Echo", "Ye2025Task", "Li2025Graphcodeagent", "Liu2023Reason", "Chiang2024Llamp", "Lu2025Youtu"], "allowed_bibkeys_mapped": ["Hu2025Evaluating", "Du2025Memr", "Yu2026Agentic", "Huang2025Retrieval", "Zhang2024Survey", "Chiang2024Llamp", "Liu2024From", "Wei2025Memguard", "Verma2026Active", "Yao2025Survey", "Ye2025Task", "Lu2025Youtu", "Li2025Graphcodeagent", "Liu2025Echo", "Zhang2025Largea", "Ge2025Surveya", "Ye2025Taska", "Anokhin2024Arigraph", "Du2025Survey", "Li2024Review", "Tao2026Membox", "Yin2025Infobid", "Yu2023Finmem", "Kang2025Distilling", "Wei2025Memory", "Liu2023Reason", "Zhu2025Where", "Yao2022React"], "allowed_bibkeys_chapter": ["Anokhin2024Arigraph", "Bai2024Twostep", "Cao2025Large", "Chen2024Architectural", "Chen2024Steering", "Chiang2024Llamp", "Chu2025Bimanual", "Du2025Memr", "Du2025Survey", "Ge2025Surveya", "Hatalis2025Review", "Hu2025Evaluating", "Hu2025Training", "Huang2025Retrieval", "Huang2025Surgical", "Ji2024Testing", "Kang2025Distilling", "Kiruluta2025Novel", "Lai2025Ustbench", "Li2024Review", "Li2025Graphcodeagent", "Liu2023Reason", "Liu2024From", "Liu2025Echo", "Lu2025Pilotrl", "Lu2025Youtu", "Motwani2024Malt", "Nusrat2025Automated", "Shi2024Ehragent", "Silva2025Agents", "Sun2025Search", "Tao2026Membox", "Verma2026Active", "Wang2025Automated", "Webb2023Improving", "Wei2025Memguard", "Wei2025Memory", "Wei2026Agentic", "Wu2025Agentic", "Yao2022React", "Yao2025Survey", "Ye2025Task", "Ye2025Taska", "Yin2025Infobid", "Yu2023Finmem", "Yu2026Agentic", "Zhang2024Survey", "Zhang2025Largea", "Zhao2024Lightva", "Zhou2023Navgpt", "Zhou2024Large", "Zhou2025Reasoning", "Zhu2025Where"], "allowed_bibkeys_global": ["Li2023Modelscope", "Yao2022React"], "evidence_ids": ["E-P0047-cb9370ac71", "E-P0166-17fb12f5b7", "E-P0220-eedb53e99a", "E-P0189-897bcc2f50", "E-P0200-1b99de5317", "E-P0173-06e45507a7", "E-P0235-4af0cf3c02", "E-P0001-ca4a00b5cf", "E-P0033-46914a4804", "E-P0016-62b8101d4e", "E-P0120-9abcf1bf8a", "E-P0026-f0ea009256", "E-P0062-86184d0d44", "E-P0011-1b6fe3407a", "E-P0121-f0f0faaada", "E-P0169-0a0006ef38", "E-P0032-53536132a8", "E-P0064-d568c53e1d", "E-P0118-fa466c449b", "E-P0104-7b7a296576", "E-P0104-c8b3e662a6", "E-P0220-50b7d357a4", "E-P0047-a4dcbe4147", "E-P0034-0f3e2d76f4"], "anchor_facts": [{"hook_type": "quant", "text": "From this observation, we build memory retrieval as an autonomous, accurate, and compatible agent system, named MemR$^3$, which has two core mechanisms: 1) a router that selects among retrieve, reflect, and answer actions to optimize answer quality; 2) a global evidence-gap", "citations": ["Du2025Memr"], "paper_id": "P0200", "evidence_id": "E-P0200-1b99de5317", "pointer": "papers/paper_notes.jsonl:paper_id=P0200#key_results[1]"}, {"hook_type": "quant", "text": "On two interactive decision making benchmarks (ALFWorld and WebShop), ReAct outperforms imitation and reinforcement learning methods by an absolute success rate of 34% and 10% respectively, while being prompted with only one or two in-context examples.", "citations": ["Yao2022React"], "paper_id": "P0001", "evidence_id": "E-P0001-ca4a00b5cf", "pointer": "papers/paper_notes.jsonl:paper_id=P0001#key_results[0]"}, {"hook_type": "quant", "text": "Comprehensive evaluations on multiple benchmarks show that A-MemGuard effectively cuts attack success rates by over 95% while incurring a minimal utility cost.", "citations": ["Wei2025Memguard"], "paper_id": "P0047", "evidence_id": "E-P0047-cb9370ac71", "pointer": "papers/paper_notes.jsonl:paper_id=P0047#key_results[0]"}, {"hook_type": "quant", "text": "Using an optimized scaffold matching industry best practices (persistent bash + string-replacement editor), we evaluated Focus on N=5 context-intensive instances from SWE-bench Lite using Claude Haiku 4.5.", "citations": ["Verma2026Active"], "paper_id": "P0120", "evidence_id": "E-P0120-9abcf1bf8a", "pointer": "papers/paper_notes.jsonl:paper_id=P0120#key_results[1]"}, {"hook_type": "eval", "text": "Structural drawings are widely used in many fields, e.g., mechanical engineering, civil engineering, etc. In civil engineering, structural drawings serve as the main communication tool between architects, engineers, and builders to avoid conflicts, act as legal documentation, and", "citations": ["Zhang2025Largea"], "paper_id": "P0189", "evidence_id": "E-P0189-897bcc2f50", "pointer": "papers/paper_notes.jsonl:paper_id=P0189#key_results[0]"}, {"hook_type": "quant", "text": "From this observation, we build memory retrieval as an autonomous, accurate, and compatible agent system, named MemR$^3$, which has two core mechanisms: 1) a router that selects among retrieve, reflect, and answer actions to optimize answer quality; 2) a global evidence-gap track", "citations": ["Du2025Memr"], "paper_id": "P0200", "evidence_id": "E-P0200-1b99de5317", "pointer": "papers/paper_notes.jsonl:paper_id=P0200#key_results[1]"}, {"hook_type": "quant", "text": "We unify and implement over ten representative memory modules and evaluate them across 10 diverse multi-turn goal-oriented and single-turn reasoning and QA datasets.", "citations": ["Wei2025Memory"], "paper_id": "P0173", "evidence_id": "E-P0173-06e45507a7", "pointer": "papers/paper_notes.jsonl:paper_id=P0173#key_results[0]"}, {"hook_type": "quant", "text": "This study presents SAFE (system for accurate fact extraction and evaluation), an agent system that combines large language models with retrieval-augmented generation (RAG) to improve automated fact-checking of long-form COVID-19 misinformation.", "citations": ["Huang2025Retrieval"], "paper_id": "P0235", "evidence_id": "E-P0235-4af0cf3c02", "pointer": "papers/paper_notes.jsonl:paper_id=P0235#key_results[1]"}, {"hook_type": "limitation", "text": "Yet their sophisticated architectures amplify vulnerability to cascading failures, where a single root-cause error propagates through subsequent decisions, leading to task failure.", "citations": ["Zhu2025Where"], "paper_id": "P0033", "evidence_id": "E-P0033-46914a4804", "pointer": "papers/paper_notes.jsonl:paper_id=P0033#summary_bullets[1]"}, {"hook_type": "eval", "text": "Finally, we summarize the datasets and benchmarks used for evaluation and tuning, review key applications of LLM-based agents, and discuss major challenges and promising future directions.", "citations": ["Du2025Survey"], "paper_id": "P0026", "evidence_id": "E-P0026-f0ea009256", "pointer": "papers/paper_notes.jsonl:paper_id=P0026#key_results[0]"}], "comparison_cards": [{"axis": "evaluation protocol (datasets, metrics, human evaluation)", "A_label": "Agent frameworks / architectures", "B_label": "Memory / retrieval augmentation", "citations": ["Du2025Survey", "Yu2026Agentic", "Hu2025Evaluating"], "A_highlights": [{"paper_id": "P0026", "evidence_id": "E-P0026-f0ea009256", "excerpt": "Finally, we summarize the datasets and benchmarks used for evaluation and tuning, review key applications of LLM-based agents, and discuss major challenges and promising future directions.", "citations": ["Du2025Survey"], "pointer": "papers/paper_notes.jsonl:paper_id=P0026#key_results[0]"}, {"paper_id": "P0121", "evidence_id": "E-P0121-f0f0faaada", "excerpt": "Experiments on five long-horizon benchmarks demonstrate that AgeMem consistently outperforms strong memory-augmented baselines across multiple LLM backbones, achieving improved task performance, higher-quality long-term memory, and more efficient context usage.", "citations": ["Yu2026Agentic"], "pointer": "papers/paper_notes.jsonl:paper_id=P0121#key_results[0]"}], "B_highlights": [{"paper_id": "P0121", "evidence_id": "E-P0121-f0f0faaada", "excerpt": "Experiments on five long-horizon benchmarks demonstrate that AgeMem consistently outperforms strong memory-augmented baselines across multiple LLM backbones, achieving improved task performance, higher-quality long-term memory, and more efficient context usage.", "citations": ["Yu2026Agentic"], "pointer": "papers/paper_notes.jsonl:paper_id=P0121#key_results[0]"}, {"paper_id": "P0062", "evidence_id": "E-P0062-86184d0d44", "excerpt": "Existing benchmarks either rely on limited context lengths or are tailored for static, long-context settings like book-based QA, which do not reflect the interactive, multi-turn nature of memory agents that incrementally accumulate information.", "citations": ["Hu2025Evaluating"], "pointer": "papers/paper_notes.jsonl:paper_id=P0062#key_results[1]"}], "write_prompt": "Contrast Agent frameworks / architectures vs Memory / retrieval augmentation along \"evaluation protocol (datasets, metrics, human evaluation)\". Ground A and B using the highlight snippets; do not introduce new claims beyond the cited evidence."}, {"axis": "evaluation protocol (datasets, metrics, human evaluation)", "A_label": "Agent frameworks / architectures", "B_label": "Planning / reasoning loops", "citations": ["Du2025Survey", "Yu2026Agentic", "Du2025Memr", "Yao2022React"], "A_highlights": [{"paper_id": "P0026", "evidence_id": "E-P0026-f0ea009256", "excerpt": "Finally, we summarize the datasets and benchmarks used for evaluation and tuning, review key applications of LLM-based agents, and discuss major challenges and promising future directions.", "citations": ["Du2025Survey"], "pointer": "papers/paper_notes.jsonl:paper_id=P0026#key_results[0]"}, {"paper_id": "P0121", "evidence_id": "E-P0121-f0f0faaada", "excerpt": "Experiments on five long-horizon benchmarks demonstrate that AgeMem consistently outperforms strong memory-augmented baselines across multiple LLM backbones, achieving improved task performance, higher-quality long-term memory, and more efficient context usage.", "citations": ["Yu2026Agentic"], "pointer": "papers/paper_notes.jsonl:paper_id=P0121#key_results[0]"}], "B_highlights": [{"paper_id": "P0200", "evidence_id": "E-P0200-1b99de5317", "excerpt": "From this observation, we build memory retrieval as an autonomous, accurate, and compatible agent system, named MemR$^3$, which has two core mechanisms: 1) a router that selects among retrieve, reflect, and answer actions to optimize answer quality; 2) a global evidence-gap", "citations": ["Du2025Memr"], "pointer": "papers/paper_notes.jsonl:paper_id=P0200#key_results[1]"}, {"paper_id": "P0001", "evidence_id": "E-P0001-ca4a00b5cf", "excerpt": "On two interactive decision making benchmarks (ALFWorld and WebShop), ReAct outperforms imitation and reinforcement learning methods by an absolute success rate of 34% and 10% respectively, while being prompted with only one or two in-context examples.", "citations": ["Yao2022React"], "pointer": "papers/paper_notes.jsonl:paper_id=P0001#key_results[0]"}], "write_prompt": "Contrast Agent frameworks / architectures vs Planning / reasoning loops along \"evaluation protocol (datasets, metrics, human evaluation)\". Ground A and B using the highlight snippets; do not introduce new claims beyond the cited evidence."}, {"axis": "evaluation protocol (datasets, metrics, human evaluation)", "A_label": "Memory / retrieval augmentation", "B_label": "Planning / reasoning loops", "citations": ["Yu2026Agentic", "Hu2025Evaluating", "Du2025Memr", "Yao2022React"], "A_highlights": [{"paper_id": "P0121", "evidence_id": "E-P0121-f0f0faaada", "excerpt": "Experiments on five long-horizon benchmarks demonstrate that AgeMem consistently outperforms strong memory-augmented baselines across multiple LLM backbones, achieving improved task performance, higher-quality long-term memory, and more efficient context usage.", "citations": ["Yu2026Agentic"], "pointer": "papers/paper_notes.jsonl:paper_id=P0121#key_results[0]"}, {"paper_id": "P0062", "evidence_id": "E-P0062-86184d0d44", "excerpt": "Existing benchmarks either rely on limited context lengths or are tailored for static, long-context settings like book-based QA, which do not reflect the interactive, multi-turn nature of memory agents that incrementally accumulate information.", "citations": ["Hu2025Evaluating"], "pointer": "papers/paper_notes.jsonl:paper_id=P0062#key_results[1]"}], "B_highlights": [{"paper_id": "P0200", "evidence_id": "E-P0200-1b99de5317", "excerpt": "From this observation, we build memory retrieval as an autonomous, accurate, and compatible agent system, named MemR$^3$, which has two core mechanisms: 1) a router that selects among retrieve, reflect, and answer actions to optimize answer quality; 2) a global evidence-gap", "citations": ["Du2025Memr"], "pointer": "papers/paper_notes.jsonl:paper_id=P0200#key_results[1]"}, {"paper_id": "P0001", "evidence_id": "E-P0001-ca4a00b5cf", "excerpt": "On two interactive decision making benchmarks (ALFWorld and WebShop), ReAct outperforms imitation and reinforcement learning methods by an absolute success rate of 34% and 10% respectively, while being prompted with only one or two in-context examples.", "citations": ["Yao2022React"], "pointer": "papers/paper_notes.jsonl:paper_id=P0001#key_results[0]"}], "write_prompt": "Contrast Memory / retrieval augmentation vs Planning / reasoning loops along \"evaluation protocol (datasets, metrics, human evaluation)\". Ground A and B using the highlight snippets; do not introduce new claims beyond the cited evidence."}, {"axis": "compute and latency constraints", "A_label": "Agent frameworks / architectures", "B_label": "Memory / retrieval augmentation", "citations": ["Yao2025Survey", "Yu2026Agentic", "Wei2025Memguard"], "A_highlights": [{"paper_id": "P0011", "evidence_id": "E-P0011-1b6fe3407a", "excerpt": "To further accelerate research in this area for the community, we compile open-source training frameworks, training and evaluation datasets for developing agentic MLLMs.", "citations": ["Yao2025Survey"], "pointer": "papers/paper_notes.jsonl:paper_id=P0011#key_results[0]"}, {"paper_id": "P0121", "evidence_id": "E-P0121-f0f0faaada", "excerpt": "Experiments on five long-horizon benchmarks demonstrate that AgeMem consistently outperforms strong memory-augmented baselines across multiple LLM backbones, achieving improved task performance, higher-quality long-term memory, and more efficient context usage.", "citations": ["Yu2026Agentic"], "pointer": "papers/paper_notes.jsonl:paper_id=P0121#key_results[0]"}], "B_highlights": [{"paper_id": "P0121", "evidence_id": "E-P0121-f0f0faaada", "excerpt": "Experiments on five long-horizon benchmarks demonstrate that AgeMem consistently outperforms strong memory-augmented baselines across multiple LLM backbones, achieving improved task performance, higher-quality long-term memory, and more efficient context usage.", "citations": ["Yu2026Agentic"], "pointer": "papers/paper_notes.jsonl:paper_id=P0121#key_results[0]"}, {"paper_id": "P0047", "evidence_id": "E-P0047-cb9370ac71", "excerpt": "Comprehensive evaluations on multiple benchmarks show that A-MemGuard effectively cuts attack success rates by over 95% while incurring a minimal utility cost.", "citations": ["Wei2025Memguard"], "pointer": "papers/paper_notes.jsonl:paper_id=P0047#key_results[0]"}], "write_prompt": "Contrast Agent frameworks / architectures vs Memory / retrieval augmentation along \"compute and latency constraints\". Ground A and B using the highlight snippets; do not introduce new claims beyond the cited evidence."}, {"axis": "compute and latency constraints", "A_label": "Agent frameworks / architectures", "B_label": "Planning / reasoning loops", "citations": ["Yao2025Survey", "Yu2026Agentic", "Du2025Memr", "Yao2022React"], "A_highlights": [{"paper_id": "P0011", "evidence_id": "E-P0011-1b6fe3407a", "excerpt": "To further accelerate research in this area for the community, we compile open-source training frameworks, training and evaluation datasets for developing agentic MLLMs.", "citations": ["Yao2025Survey"], "pointer": "papers/paper_notes.jsonl:paper_id=P0011#key_results[0]"}, {"paper_id": "P0121", "evidence_id": "E-P0121-f0f0faaada", "excerpt": "Experiments on five long-horizon benchmarks demonstrate that AgeMem consistently outperforms strong memory-augmented baselines across multiple LLM backbones, achieving improved task performance, higher-quality long-term memory, and more efficient context usage.", "citations": ["Yu2026Agentic"], "pointer": "papers/paper_notes.jsonl:paper_id=P0121#key_results[0]"}], "B_highlights": [{"paper_id": "P0200", "evidence_id": "E-P0200-1b99de5317", "excerpt": "From this observation, we build memory retrieval as an autonomous, accurate, and compatible agent system, named MemR$^3$, which has two core mechanisms: 1) a router that selects among retrieve, reflect, and answer actions to optimize answer quality; 2) a global evidence-gap", "citations": ["Du2025Memr"], "pointer": "papers/paper_notes.jsonl:paper_id=P0200#key_results[1]"}, {"paper_id": "P0001", "evidence_id": "E-P0001-ca4a00b5cf", "excerpt": "On two interactive decision making benchmarks (ALFWorld and WebShop), ReAct outperforms imitation and reinforcement learning methods by an absolute success rate of 34% and 10% respectively, while being prompted with only one or two in-context examples.", "citations": ["Yao2022React"], "pointer": "papers/paper_notes.jsonl:paper_id=P0001#key_results[0]"}], "write_prompt": "Contrast Agent frameworks / architectures vs Planning / reasoning loops along \"compute and latency constraints\". Ground A and B using the highlight snippets; do not introduce new claims beyond the cited evidence."}, {"axis": "compute and latency constraints", "A_label": "Memory / retrieval augmentation", "B_label": "Planning / reasoning loops", "citations": ["Yu2026Agentic", "Wei2025Memguard", "Du2025Memr", "Yao2022React"], "A_highlights": [{"paper_id": "P0121", "evidence_id": "E-P0121-f0f0faaada", "excerpt": "Experiments on five long-horizon benchmarks demonstrate that AgeMem consistently outperforms strong memory-augmented baselines across multiple LLM backbones, achieving improved task performance, higher-quality long-term memory, and more efficient context usage.", "citations": ["Yu2026Agentic"], "pointer": "papers/paper_notes.jsonl:paper_id=P0121#key_results[0]"}, {"paper_id": "P0047", "evidence_id": "E-P0047-cb9370ac71", "excerpt": "Comprehensive evaluations on multiple benchmarks show that A-MemGuard effectively cuts attack success rates by over 95% while incurring a minimal utility cost.", "citations": ["Wei2025Memguard"], "pointer": "papers/paper_notes.jsonl:paper_id=P0047#key_results[0]"}], "B_highlights": [{"paper_id": "P0200", "evidence_id": "E-P0200-1b99de5317", "excerpt": "From this observation, we build memory retrieval as an autonomous, accurate, and compatible agent system, named MemR$^3$, which has two core mechanisms: 1) a router that selects among retrieve, reflect, and answer actions to optimize answer quality; 2) a global evidence-gap", "citations": ["Du2025Memr"], "pointer": "papers/paper_notes.jsonl:paper_id=P0200#key_results[1]"}, {"paper_id": "P0001", "evidence_id": "E-P0001-ca4a00b5cf", "excerpt": "On two interactive decision making benchmarks (ALFWorld and WebShop), ReAct outperforms imitation and reinforcement learning methods by an absolute success rate of 34% and 10% respectively, while being prompted with only one or two in-context examples.", "citations": ["Yao2022React"], "pointer": "papers/paper_notes.jsonl:paper_id=P0001#key_results[0]"}], "write_prompt": "Contrast Memory / retrieval augmentation vs Planning / reasoning loops along \"compute and latency constraints\". Ground A and B using the highlight snippets; do not introduce new claims beyond the cited evidence."}, {"axis": "tool interface contract (schemas / protocols)", "A_label": "Agent frameworks / architectures", "B_label": "Memory / retrieval augmentation", "citations": ["Yu2026Agentic", "Verma2026Active", "Kang2025Distilling"], "A_highlights": [{"paper_id": "P0121", "evidence_id": "E-P0121-f0f0faaada", "excerpt": "Experiments on five long-horizon benchmarks demonstrate that AgeMem consistently outperforms strong memory-augmented baselines across multiple LLM backbones, achieving improved task performance, higher-quality long-term memory, and more efficient context usage.", "citations": ["Yu2026Agentic"], "pointer": "papers/paper_notes.jsonl:paper_id=P0121#key_results[0]"}, {"paper_id": "P0120", "evidence_id": "E-P0120-9abcf1bf8a", "excerpt": "Using an optimized scaffold matching industry best practices (persistent bash + string-replacement editor), we evaluated Focus on N=5 context-intensive instances from SWE-bench Lite using Claude Haiku 4.5.", "citations": ["Verma2026Active"], "pointer": "papers/paper_notes.jsonl:paper_id=P0120#key_results[1]"}], "B_highlights": [{"paper_id": "P0166", "evidence_id": "E-P0166-17fb12f5b7", "excerpt": "In this work, we propose Agent Distillation, a framework for transferring not only reasoning capability but full task-solving behavior from LLM-based agents into sLMs with retrieval and code tools.", "citations": ["Kang2025Distilling"], "pointer": "papers/paper_notes.jsonl:paper_id=P0166#method"}, {"paper_id": "P0121", "evidence_id": "E-P0121-f0f0faaada", "excerpt": "Experiments on five long-horizon benchmarks demonstrate that AgeMem consistently outperforms strong memory-augmented baselines across multiple LLM backbones, achieving improved task performance, higher-quality long-term memory, and more efficient context usage.", "citations": ["Yu2026Agentic"], "pointer": "papers/paper_notes.jsonl:paper_id=P0121#key_results[0]"}], "write_prompt": "Contrast Agent frameworks / architectures vs Memory / retrieval augmentation along \"tool interface contract (schemas / protocols)\". Ground A and B using the highlight snippets; do not introduce new claims beyond the cited evidence."}], "evaluation_protocol": [{"bullet": "Evaluation mentions include: SWE-bench, LTM, STM, GRPO, AgeMem, MEM, LoCoMo, AGI, MLLMs, MLLM-based.", "citations": ["Verma2026Active", "Yu2026Agentic", "Tao2026Membox", "Yao2025Survey"]}, {"bullet": "When comparing results, anchor the paragraph with: task type + metric + constraint (budget, tool access, horizon, or threat model) when stated.", "citations": ["Verma2026Active", "Yu2026Agentic"]}, {"bullet": "Prefer head-to-head comparisons only when benchmark/metric are shared; otherwise frame differences as protocol-driven rather than method superiority.", "citations": ["Verma2026Active", "Yu2026Agentic"]}, {"bullet": "Avoid underspecified model/baseline naming; if abstracts omit details, state that the baseline is reported but underspecified instead of guessing.", "citations": ["Verma2026Active", "Yu2026Agentic"]}, {"bullet": "If a claim relies on a single reported number, pair it with a limitation/caveat from the same evidence so the draft remains conservative.", "citations": ["Verma2026Active", "Yu2026Agentic"]}, {"bullet": "If budgets or environments differ across papers, treat cross-paper numeric comparison as fragile and prefer qualitative contrasts aligned to the subsection axes.", "citations": ["Verma2026Active", "Yu2026Agentic"]}], "limitation_hooks": [{"excerpt": "Large language model (LLM) agents face fundamental limitations in long-horizon reasoning due to finite context windows, making effective memory management critical.", "citations": ["Yu2026Agentic"], "pointer": ""}, {"excerpt": "Yet their sophisticated architectures amplify vulnerability to cascading failures, where a single root-cause error propagates through subsequent decisions, leading to task failure.", "citations": ["Zhu2025Where"], "pointer": ""}, {"excerpt": "First, we introduce the AgentErrorTaxonomy, a modular classification of failure modes spanning memory, reflection, planning, action, and system-level operations.", "citations": ["Zhu2025Where"], "pointer": ""}, {"excerpt": "Second, we construct AgentErrorBench, the first dataset of systematically annotated failure trajectories from ALFWorld, GAIA, and WebShop, grounding error analysis in real-world agent rollouts.", "citations": ["Zhu2025Where"], "pointer": ""}, {"excerpt": "However, this reliance on memory introduces a critical security risk: an adversary can inject seemingly harmless records into an agent's memory to manipulate its future behavior.", "citations": ["Wei2025Memguard"], "pointer": ""}, {"excerpt": "Comprehensive evaluations on multiple benchmarks show that A-MemGuard effectively cuts attack success rates by over 95% while incurring a minimal utility cost.", "citations": ["Wei2025Memguard"], "pointer": ""}, {"excerpt": "This work shifts LLM memory security from static filtering to a proactive, experience-driven model where defenses strengthen over time.", "citations": ["Wei2025Memguard"], "pointer": ""}, {"excerpt": "This results in the failure to retrieve the relevant code of these fine-grained subtasks.", "citations": ["Li2025Graphcodeagent"], "pointer": ""}], "must_use": {"min_anchor_facts": 4, "min_comparison_cards": 4, "min_limitation_hooks": 2, "require_cited_numeric_if_available": true, "require_multi_cite_synthesis_paragraph": true, "thesis_required": true}, "do_not_repeat_phrases": ["this run", "this run is", "pipeline", "workspace", "unit", "quality gate", "evidence pack", "evidence packs", "writer context pack", "Method note (evidence policy):", "Methodology note:", "Methodology note (evidence policy):", "Methodology note (evidence-policy):", "Method note:", "This subsection surveys", "This subsection argues", "In this subsection", "This section surveys", "This section reviews", "This section discusses", "This section covers", "This section presents", "This section introduces", "provides an overview", "This survey provides an overview", "In this section, we provide an overview", "This section provides an overview", "Next, we move from", "We now turn to", "A few representative references include", "Notable lines of work include", "Concrete examples include", "Work in this area includes", "Recent systems include", "Examples include", "Representative systems include", "Therefore, survey synthesis should", "Therefore, survey comparisons should", "As a result, survey comparisons should", "survey synthesis should", "survey comparisons should", "Taken together,", "The key point is that", "Three limitations", "Two limitations", "claims remain provisional under abstract-only evidence", "abstract-only evidence", "Key takeaway:", "Main takeaway:", "protocol token", "protocol tokens", "constraint token", "constraint tokens", "metric token", "metric tokens", "evaluation token", "evaluation tokens", "In this survey", "In our survey", "Our survey", "This survey"], "pack_warnings": [], "pack_stats": {"anchors": {"raw": 12, "considered": 10, "kept": 10, "dropped_no_cites": 0}, "comparisons": {"raw": 10, "considered": 7, "kept": 7, "dropped_no_highlights": 0, "highlights_dropped_no_cites": 0}, "evaluation_protocol": {"raw": 6, "considered": 6, "kept": 6, "dropped_no_cites": 0}, "limitation_hooks": {"raw": 8, "considered": 8, "kept": 8, "dropped_no_cites": 0}, "trim_policy": {"default": 400, "anchor_fact": 420, "highlight_excerpt": 280, "comparison_write_prompt": 420, "eval_bullet": 320, "limitation_excerpt": 320}}, "generated_at": "2026-01-25T17:09:25"}
{"sub_id": "5.1", "title": "Self-improvement and adaptation", "section_id": "5", "section_title": "Learning, Adaptation & Coordination", "rq": "Which design choices in Self-improvement and adaptation drive the major trade-offs, and how are those trade-offs measured?", "thesis": "Self-improvement and adaptation highlights a tension around evaluation protocol (datasets, metrics, human evaluation) and compute and latency constraints, motivating a protocol-aware synthesis rather than per-paper summaries.", "tension_statement": "In Self-improvement and adaptation, the core trade-off is adaptability versus stability: systems that change themselves can improve over time but risk drifting, overfitting, or becoming harder to evaluate and control.", "evaluation_anchor_minimal": {"task": "agent benchmark tasks", "metric": "success rate", "constraint": "budget/cost model"}, "paper_voice_palette": {"forbidden_pipeline_voice": ["this run", "this run is", "pipeline", "workspace", "unit", "quality gate", "evidence pack", "evidence packs", "writer context pack", "Method note (evidence policy):", "Methodology note:", "Methodology note (evidence policy):", "Methodology note (evidence-policy):", "Method note:"], "high_risk_templates": ["This subsection surveys", "This subsection argues", "In this subsection", "This section surveys", "This section reviews", "This section discusses", "This section covers", "This section presents", "This section introduces", "provides an overview", "This survey provides an overview", "In this section, we provide an overview", "This section provides an overview", "Next, we move from", "We now turn to", "A few representative references include", "Notable lines of work include", "Concrete examples include", "Work in this area includes", "Recent systems include", "Examples include", "Representative systems include", "Therefore, survey synthesis should", "Therefore, survey comparisons should", "As a result, survey comparisons should", "survey synthesis should", "survey comparisons should", "Taken together,", "The key point is that", "Three limitations", "Two limitations", "claims remain provisional under abstract-only evidence", "abstract-only evidence", "Key takeaway:", "Main takeaway:", "protocol token", "protocol tokens", "constraint token", "constraint tokens", "metric token", "metric tokens", "evaluation token", "evaluation tokens", "In this survey", "In our survey", "Our survey", "This survey"], "opener_archetypes": {"tension-first": ["A key tension is", "The central trade-off is", "A recurring constraint is"], "decision-first": ["For system builders, the crux is", "A practical decision is", "One design choice is"], "lens-first": ["Seen through the lens of", "From the perspective of", "Under an interface contract,"], "contrast-first": ["A useful contrast is between", "One sharp contrast is between", "A recurring split is between"], "protocol-first": ["Comparisons hinge on", "Results are only comparable when", "Evaluation claims depend on"]}, "synthesis_stems": ["Across these studies,", "Collectively,", "In summary,", "The evidence suggests that", "A consistent theme is that"], "rewrite_rules": [{"avoid_stem": "This subsection surveys", "prefer_stem": "A key tension is"}, {"avoid_stem": "In this subsection", "prefer_stem": "We focus on"}, {"avoid_stem": "provides an overview", "prefer_stem": "We focus on"}, {"avoid_stem": "Next, we move", "prefer_stem": "Having established"}, {"avoid_stem": "We now turn", "prefer_stem": "We then examine"}, {"avoid_stem": "survey comparisons should", "prefer_stem": "Across protocols, we observe"}, {"avoid_stem": "Two limitations", "prefer_stem": "A caveat is that"}, {"avoid_stem": "Three limitations", "prefer_stem": "One limitation is that"}, {"avoid_stem": "In this survey", "prefer_stem": "We examine"}, {"avoid_stem": "In our survey", "prefer_stem": "We examine"}, {"avoid_stem": "Our survey", "prefer_stem": "This work"}, {"avoid_stem": "This survey", "prefer_stem": "This work"}], "discourse_stem_watchlist": ["Additionally,", "Moreover,", "Furthermore,", "This suggests", "Taken together,", "The key point is that", "Overall,", "In summary,", "In practice,", "More broadly,", "Importantly,", "Notably,", "Crucially,", "In general,", "At a high level,", "In other words,", "Therefore,", "In addition,", "As a result,"], "discourse_stem_rewrites": {"Additionally,": ["More importantly,", "At the same time,", "A second consideration is"], "This suggests": ["This pattern indicates", "These results point to", "A plausible explanation is"], "Taken together,": ["Across these studies,", "Collectively,", "The evidence suggests"], "The key point is that": ["A practical implication is that", "A useful way to read these results is that", "One takeaway is that"], "Overall,": ["In practice,", "More broadly,", "A practical implication is that", "One way to read this evidence is that"], "In summary,": ["Across these studies,", "Collectively,", "A consistent theme is that", "The evidence suggests that"], "Importantly,": ["More importantly,", "A key constraint is that", "A practical implication is that"], "Notably,": ["In particular,", "A concrete example is that", "One implication is that"], "Crucially,": ["More importantly,", "A central constraint is that", "A key takeaway is that"], "At a high level,": ["Concretely,", "Under this protocol,", "In practice,"], "In other words,": ["Put differently,", "Equivalently,", "More concretely,"], "Therefore,": ["As a result,", "This in turn means that", "A practical implication is that"], "In addition,": ["At the same time,", "A second consideration is", "More importantly,"], "As a result,": ["This in turn means that", "Consequently,", "A practical implication is that"]}, "role_cards": {"section_author": {"mission": "Write one subsection as an argument (not a topic list), using in-scope citations as evidence.", "do": ["State a concrete tension/trade-off early and commit to a thesis.", "Make at least two explicit A-vs-B contrasts (mechanism -> outcome) instead of per-paper summaries.", "When citing numbers, add minimal context (task + metric + constraint) in the same paragraph.", "End with a limitation that changes interpretation (protocol mismatch, unclear threat model, missing ablations)."], "avoid": ["Outline narration (This subsection..., In this subsection...).", "Slide navigation (Next, we move..., We now turn...).", "Meta survey advice (survey comparisons should...).", "Cite dumps (a trailing [@a; @b; @c] without a claim)."]}, "evidence_steward": {"mission": "Keep claims auditable: citations support the sentence that needs them, and scope stays local.", "do": ["Embed citations inside claim sentences; name concrete nouns (system/benchmark/protocol) near each cite.", "Prefer subsection-scoped citations; use chapter/global only when truly cross-cutting.", "Downgrade overconfident claims when evidence is thin; convert unknowns into verification targets (once, not spam)."], "avoid": ["Out-of-scope citations to make a paragraph sound stronger.", "Repeating evidence-policy disclaimers in every subsection.", "Ambiguous model naming (e.g., GPT-5) unless the cited work uses it."]}, "style_harmonizer": {"mission": "Make the prose read like a paper: calm, specific, and varied in rhythm without sounding templated.", "do": ["Vary opener cadence across sections (tension-first / decision-first / lens-first) without reusing the same stem.", "Prefer argument bridges over navigation; keep signposting light and content-bearing.", "Reduce slash-enumerations (A/B/C axis labels) by rewriting into natural prose."], "avoid": ["Count-based openers used as a slot (e.g., Two limitations..., Three takeaways...).", "Repeated discourse stems (Additionally, This suggests, Taken together) across many paragraphs.", "Pipeline words (workspace/unit/quality gate/evidence pack) in reader-facing text.", "PPT speaker-note tone (Now we..., The remainder of...).", "Internal shorthand that reads like planning notes (e.g., protocol/metric/constraint tokens)."]}}, "version": "0.4"}, "opener_mode": "decision-first", "opener_hint": "Start with a builder/research decision; state what it depends on; end paragraph 1 with the thesis.", "axes": ["evaluation protocol (datasets, metrics, human evaluation)", "compute and latency constraints", "communication protocol / roles", "aggregation (vote / debate / referee)", "stability / robustness"], "bridge_terms": ["preference", "reward", "feedback", "self-improvement", "benchmarks/metrics", "compute"], "contrast_hook": "learning/feedback", "required_evidence_fields": ["benchmarks/datasets", "metrics / human-eval protocol", "compute / cost (train/infer)", "training signal / supervision", "failure modes and limitations"], "paragraph_plan": [{"para": 1, "argument_role": "setup_thesis", "intent": "Define scope, setup, and the subsection thesis (no pipeline jargon).", "focus": ["scope boundary", "key definitions", "thesis vs neighboring subsections"], "connector_to_prev": "", "connector_phrase": "", "use_clusters": ["Agent frameworks / architectures"], "rq": "Which design choices in Self-improvement and adaptation drive the major trade-offs, and how are those trade-offs measured?"}, {"para": 2, "argument_role": "mechanism_cluster_A", "intent": "Explain cluster A: core mechanism and system architecture and what decision it makes in the agent loop.", "focus": ["cluster: Agent frameworks / architectures", "core mechanism and system architecture", "assumptions"], "connector_to_prev": "grounding", "connector_phrase": "baseline route (Agent frameworks / architectures)", "use_clusters": ["Agent frameworks / architectures"]}, {"para": 3, "argument_role": "implementation_cluster_A", "intent": "Cluster A implementation details: training and data signals and interface contract (tools/memory) that constrain behavior.", "focus": ["cluster: Agent frameworks / architectures", "training and data setup", "interface contract", "axes: evaluation protocol (datasets, metrics, human evaluation), compute and latency constraints, communication protocol / roles, aggregation (vote / debate / referee), stability / robustness"], "connector_to_prev": "elaboration", "connector_phrase": "implementation assumptions (interface + training)", "use_clusters": ["Agent frameworks / architectures"]}, {"para": 4, "argument_role": "evaluation_cluster_A", "intent": "Cluster A evaluation/trade-offs: where it works, costs (compute/latency), and typical failure modes.", "focus": ["cluster: Agent frameworks / architectures", "evaluation anchor", "efficiency", "failure modes"], "connector_to_prev": "evaluation", "connector_phrase": "evaluation anchor (task/metric/constraint) + failure modes", "use_clusters": ["Agent frameworks / architectures"]}, {"para": 5, "argument_role": "contrast_cluster_B", "intent": "Explain cluster B (contrast with A): core mechanism and system architecture and what it optimizes for.", "focus": ["cluster: Planning / reasoning loops", "contrast with Agent frameworks / architectures", "core mechanism and system architecture"], "connector_to_prev": "contrast", "connector_phrase": "contrast route (Planning / reasoning loops vs Agent frameworks / architectures)", "use_clusters": ["Planning / reasoning loops"]}, {"para": 6, "argument_role": "implementation_cluster_B", "intent": "Cluster B implementation details: training and data and interface assumptions (mirror A for comparability).", "focus": ["cluster: Planning / reasoning loops", "training and data setup", "interface contract", "axes: evaluation protocol (datasets, metrics, human evaluation), compute and latency constraints, communication protocol / roles, aggregation (vote / debate / referee), stability / robustness"], "connector_to_prev": "elaboration", "connector_phrase": "contrast implementation assumptions (B)", "use_clusters": ["Planning / reasoning loops"]}, {"para": 7, "argument_role": "evaluation_cluster_B", "intent": "Cluster B evaluation/trade-offs: where it works, costs, and failure modes (mirror A).", "focus": ["cluster: Planning / reasoning loops", "evaluation anchor", "efficiency", "failure modes"], "connector_to_prev": "evaluation", "connector_phrase": "contrast evaluation anchor + trade-offs (B)", "use_clusters": ["Planning / reasoning loops"]}, {"para": 8, "argument_role": "cross_paper_synthesis", "intent": "Cross-paper synthesis: compare clusters along the main axes (include >=2 citations in one paragraph).", "focus": ["compare Agent frameworks / architectures vs Planning / reasoning loops", "multiple citations in one paragraph", "axes: evaluation protocol (datasets, metrics, human evaluation), compute and latency constraints, communication protocol / roles, aggregation (vote / debate / referee), stability / robustness"], "connector_to_prev": "synthesis", "connector_phrase": "cross-paper synthesis (Agent frameworks / architectures vs Planning / reasoning loops)", "use_clusters": ["Agent frameworks / architectures", "Planning / reasoning loops", "Code agents / software tasks"]}, {"para": 9, "argument_role": "decision_guidance", "intent": "Decision guidance: when to choose which route (criteria + evaluation signals + engineering constraints).", "focus": ["decision checklist", "evaluation protocol", "practical constraints"], "connector_to_prev": "consequence", "connector_phrase": "decision guidance / criteria", "use_clusters": ["Agent frameworks / architectures", "Planning / reasoning loops", "Code agents / software tasks"]}, {"para": 10, "argument_role": "limitations_open_questions", "intent": "Limitations + verification targets; end with a concrete open question to hand off.", "focus": ["limitations", "evidence mode: provisional", "what needs verification", "open question"], "connector_to_prev": "limitations", "connector_phrase": "limitations + verification targets", "use_clusters": ["Agent frameworks / architectures", "Planning / reasoning loops", "Code agents / software tasks"], "policy": "Use conservative language; avoid strong conclusions; prefer questions-to-answer + explicit evidence gaps list."}], "chapter_throughline": ["Pin scope to goal: agent survey latex.", "Compare approaches along: evaluation protocol (datasets, metrics, human evaluation).", "Compare approaches along: compute and latency constraints.", "Compare approaches along: communication protocol / roles.", "Compare approaches along: aggregation (vote / debate / referee).", "Compare approaches along: stability / robustness."], "chapter_key_contrasts": ["learning/feedback", "coordination"], "chapter_synthesis_mode": "tradeoff_matrix", "allowed_bibkeys_selected": ["Zhang2025Agentic", "Zhang2025Security", "Li2025Learn", "Zhou2025Self", "Chen2025Largea", "Xia2025Sand", "Taylor2025Large", "Jin2024From", "Samaei2025Epidemiqs", "Liu2025Powered", "Wei2026Agentic", "Jiang2025Agentic", "Van2025Survey", "Chen2025Grounded", "Belle2025Agents", "Wu2024Federated", "Bharadwaj2025Omnireflect", "Mou2024From", "Zhou2024Archer", "Lu2025Pilotrl", "Tao2024Survey", "Bilal2025Meta"], "allowed_bibkeys_mapped": ["Chen2025Grounded", "Belle2025Agents", "Xia2025Sand", "Zhou2025Self", "Zhang2025Agentic", "Li2025Learn", "Bilal2025Meta", "Samaei2025Epidemiqs", "Liu2025Powered", "Jin2024From", "Tao2024Survey", "Zhang2024Affective", "Jiang2025Agentic", "Wu2024Federated", "Zhou2024Archer", "Wang2024Learning", "Chen2025Survey", "Chen2025Largea", "Zhang2025Security", "Bharadwaj2025Omnireflect", "Sarkar2025Survey", "Hu2024Survey", "Mou2024From", "Wei2026Agentic", "Van2025Survey", "Taylor2025Large", "Lu2025Pilotrl", "Yu2023Finmem"], "allowed_bibkeys_chapter": ["Aratchige2025Llms", "Becker2025Mallm", "Belle2025Agents", "Bharadwaj2025Omnireflect", "Bilal2025Meta", "Chen2024Llmarena", "Chen2024Solving", "Chen2025Grounded", "Chen2025Largea", "Chen2025Schema", "Chen2025Survey", "Collini2025Marvel", "Cui2025Toward", "Hu2024Survey", "Inoue2024Drugagent", "Jiang2023Large", "Jiang2025Agentic", "Jin2024From", "Le2024Multi", "Li2023Modelscope", "Li2025Learn", "Liu2025Medchat", "Liu2025Powered", "Lu2025Pilotrl", "Masters2025Arcane", "Motwani2024Malt", "Mou2024From", "Mushtaq2025Harnessing", "Rouzrokh2025Lattereview", "Samaei2025Epidemiqs", "Sarkar2025Survey", "Shen2024Small", "Tao2024Survey", "Taylor2025Large", "Trirat2024Automl", "Van2025Survey", "Wang2024Learning", "Wang2025Autoscore", "Wei2026Agentic", "Wu2024Federated", "Wu2025Multi", "Xia2025Sand", "Xu2023Magic", "Xu2023Towards", "Xu2025Autonomous", "Ye2025Cognipair", "Yu2023Finmem", "Zahedifar2025Agent", "Zhang2024Affective", "Zhang2024Towards", "Zhang2025Agentic", "Zhang2025Security", "Zhou2024Archer", "Zhou2025Self"], "allowed_bibkeys_global": ["Li2023Modelscope", "Yao2022React"], "evidence_ids": ["E-P0144-7d85a7241d", "E-P0195-8bcb673a7d", "E-P0079-01f439b837", "E-P0214-2e6956a116", "E-P0190-3620ae9178", "E-P0087-6273763a98", "E-P0167-047d14c804", "E-P0099-33c7e5174b", "E-P0171-ca050ed55f", "E-P0187-ec46292a2b", "E-P0042-44dce85d3e", "E-P0050-f38be5e04b", "E-P0133-a68f39bc04", "E-P0180-af945eb2fa", "E-P0051-eae2720c71", "E-P0260-76aa5df0e2", "E-P0205-85b5f3734b", "E-P0261-3e23cb8c51", "E-P0093-faa3d4c9ee", "E-P0208-07f6a1adfe", "E-P0245-ebe044543a", "E-P0084-61917beed9", "E-P0084-ffbc3301be", "E-P0245-3df3fdc7a7"], "anchor_facts": [{"hook_type": "quant", "text": "Across agent and domain-specific benchmarks, ACE optimizes contexts both offline (e.g., system prompts) and online (e.g., agent memory), consistently outperforming strong baselines: +10.6% on agents and +8.6% on finance, while significantly reducing adaptation latency and", "citations": ["Zhang2025Agentic"], "paper_id": "P0144", "evidence_id": "E-P0144-7d85a7241d", "pointer": "papers/paper_notes.jsonl:paper_id=P0144#key_results[0]"}, {"hook_type": "quant", "text": "Across agent and domain-specific benchmarks, ACE optimizes contexts both offline (e.g., system prompts) and online (e.g., agent memory), consistently outperforming strong baselines: +10.6% on agents and +8.6% on finance, while significantly reducing adaptation latency and rollout", "citations": ["Zhang2025Agentic"], "paper_id": "P0144", "evidence_id": "E-P0144-7d85a7241d", "pointer": "papers/paper_notes.jsonl:paper_id=P0144#key_results[0]"}, {"hook_type": "eval", "text": "We present MSB (MCP Security Benchmark), the first end-to-end evaluation suite that systematically measures how well LLM agents resist MCP-specific attacks throughout the full tool-use pipeline: task planning, tool invocation, and response handling.", "citations": ["Zhang2025Security"], "paper_id": "P0195", "evidence_id": "E-P0195-8bcb673a7d", "pointer": "papers/paper_notes.jsonl:paper_id=P0195#method"}, {"hook_type": "limitation", "text": "We address this limitation by introducing a framework that enables LLM agents to learn and evolve both before and during test time, equipping them with environment-relevant knowledge for better planning and enhanced communication for improved cooperation.", "citations": ["Li2025Learn"], "paper_id": "P0079", "evidence_id": "E-P0079-01f439b837", "pointer": "papers/paper_notes.jsonl:paper_id=P0079#limitations[1]"}, {"hook_type": "quant", "text": "Evaluation on two existing multi-turn tool-use agent benchmarks, M3ToolEval and TauBench, shows the Self-Challenging framework achieves over a two-fold improvement in Llama-3.1-8B-Instruct, despite using only self-generated training data.", "citations": ["Zhou2025Self"], "paper_id": "P0214", "evidence_id": "E-P0214-2e6956a116", "pointer": "papers/paper_notes.jsonl:paper_id=P0214#key_results[0]"}, {"hook_type": "quant", "text": "From the data science perspective, we identify key processes for LLM-based agents, including data preprocessing, model development, evaluation, visualization, etc. Our work offers two key contributions: (1) a comprehensive review of recent developments in applying LLMbased agents", "citations": ["Chen2025Largea"], "paper_id": "P0190", "evidence_id": "E-P0190-3620ae9178", "pointer": "papers/paper_notes.jsonl:paper_id=P0190#key_results[0]"}, {"hook_type": "quant", "text": "Evaluating on two representative interactive agent tasks, SAND achieves an average 20% improvement over initial supervised finetuning and also outperforms state-of-the-art agent tuning approaches.", "citations": ["Xia2025Sand"], "paper_id": "P0087", "evidence_id": "E-P0087-6273763a98", "pointer": "papers/paper_notes.jsonl:paper_id=P0087#key_results[0]"}, {"hook_type": "quant", "text": "The results indicate that 1) all tested LLMs spontaneously misrepresent their actions in at least some conditions, 2) they are generally more likely to do so in situations in which deception would benefit them, and 3) models exhibiting better reasoning capacity overall tend to de", "citations": ["Taylor2025Large"], "paper_id": "P0167", "evidence_id": "E-P0167-047d14c804", "pointer": "papers/paper_notes.jsonl:paper_id=P0167#key_results[0]"}, {"hook_type": "eval", "text": "We review and differentiate the work of LLMs and LLM-based agents from these six topics, examining their differences and similarities in tasks, benchmarks, and evaluation metrics.", "citations": ["Jin2024From"], "paper_id": "P0099", "evidence_id": "E-P0099-33c7e5174b", "pointer": "papers/paper_notes.jsonl:paper_id=P0099#key_results[1]"}, {"hook_type": "eval", "text": "Settlers of Catan provides a challenging benchmark: success depends on balancing short- and long-term goals amid randomness, trading, expansion, and blocking.", "citations": ["Belle2025Agents"], "paper_id": "P0051", "evidence_id": "E-P0051-eae2720c71", "pointer": "papers/paper_notes.jsonl:paper_id=P0051#key_results[1]"}], "comparison_cards": [{"axis": "evaluation protocol (datasets, metrics, human evaluation)", "A_label": "Agent frameworks / architectures", "B_label": "Planning / reasoning loops", "citations": ["Van2025Survey", "Zhang2025Agentic", "Wei2026Agentic", "Belle2025Agents"], "A_highlights": [{"paper_id": "P0133", "evidence_id": "E-P0133-a68f39bc04", "excerpt": "This survey provides a comprehensive overview of foundation models, agentic systems, datasets, and computational tools supporting this growing field.", "citations": ["Van2025Survey"], "pointer": "papers/paper_notes.jsonl:paper_id=P0133#key_results[0]"}, {"paper_id": "P0144", "evidence_id": "E-P0144-7d85a7241d", "excerpt": "Across agent and domain-specific benchmarks, ACE optimizes contexts both offline (e.g., system prompts) and online (e.g., agent memory), consistently outperforming strong baselines: +10.6% on agents and +8.6% on finance, while significantly reducing adaptation latency and", "citations": ["Zhang2025Agentic"], "pointer": "papers/paper_notes.jsonl:paper_id=P0144#key_results[0]"}], "B_highlights": [{"paper_id": "P0042", "evidence_id": "E-P0042-44dce85d3e", "excerpt": "We further review representative agentic reasoning frameworks across real-world applications and benchmarks, including science, robotics, healthcare, autonomous research, and mathematics.", "citations": ["Wei2026Agentic"], "pointer": "papers/paper_notes.jsonl:paper_id=P0042#key_results[0]"}, {"paper_id": "P0051", "evidence_id": "E-P0051-eae2720c71", "excerpt": "Settlers of Catan provides a challenging benchmark: success depends on balancing short- and long-term goals amid randomness, trading, expansion, and blocking.", "citations": ["Belle2025Agents"], "pointer": "papers/paper_notes.jsonl:paper_id=P0051#key_results[1]"}], "write_prompt": "Contrast Agent frameworks / architectures vs Planning / reasoning loops along \"evaluation protocol (datasets, metrics, human evaluation)\". Ground A and B using the highlight snippets; do not introduce new claims beyond the cited evidence."}, {"axis": "evaluation protocol (datasets, metrics, human evaluation)", "A_label": "Agent frameworks / architectures", "B_label": "Code agents / software tasks", "citations": ["Van2025Survey", "Zhang2025Agentic", "Jin2024From", "Jiang2025Agentic"], "A_highlights": [{"paper_id": "P0133", "evidence_id": "E-P0133-a68f39bc04", "excerpt": "This survey provides a comprehensive overview of foundation models, agentic systems, datasets, and computational tools supporting this growing field.", "citations": ["Van2025Survey"], "pointer": "papers/paper_notes.jsonl:paper_id=P0133#key_results[0]"}, {"paper_id": "P0144", "evidence_id": "E-P0144-7d85a7241d", "excerpt": "Across agent and domain-specific benchmarks, ACE optimizes contexts both offline (e.g., system prompts) and online (e.g., agent memory), consistently outperforming strong baselines: +10.6% on agents and +8.6% on finance, while significantly reducing adaptation latency and", "citations": ["Zhang2025Agentic"], "pointer": "papers/paper_notes.jsonl:paper_id=P0144#key_results[0]"}], "B_highlights": [{"paper_id": "P0099", "evidence_id": "E-P0099-33c7e5174b", "excerpt": "We review and differentiate the work of LLMs and LLM-based agents from these six topics, examining their differences and similarities in tasks, benchmarks, and evaluation metrics.", "citations": ["Jin2024From"], "pointer": "papers/paper_notes.jsonl:paper_id=P0099#key_results[1]"}, {"paper_id": "P0050", "evidence_id": "E-P0050-f38be5e04b", "excerpt": "It outlines the general workflow of the task and establishes a taxonomy across three dimensions: benchmarks, techniques, and empirical studies.", "citations": ["Jiang2025Agentic"], "pointer": "papers/paper_notes.jsonl:paper_id=P0050#key_results[1]"}], "write_prompt": "Contrast Agent frameworks / architectures vs Code agents / software tasks along \"evaluation protocol (datasets, metrics, human evaluation)\". Ground A and B using the highlight snippets; do not introduce new claims beyond the cited evidence."}, {"axis": "evaluation protocol (datasets, metrics, human evaluation)", "A_label": "Planning / reasoning loops", "B_label": "Code agents / software tasks", "citations": ["Wei2026Agentic", "Belle2025Agents", "Jin2024From", "Jiang2025Agentic"], "A_highlights": [{"paper_id": "P0042", "evidence_id": "E-P0042-44dce85d3e", "excerpt": "We further review representative agentic reasoning frameworks across real-world applications and benchmarks, including science, robotics, healthcare, autonomous research, and mathematics.", "citations": ["Wei2026Agentic"], "pointer": "papers/paper_notes.jsonl:paper_id=P0042#key_results[0]"}, {"paper_id": "P0051", "evidence_id": "E-P0051-eae2720c71", "excerpt": "Settlers of Catan provides a challenging benchmark: success depends on balancing short- and long-term goals amid randomness, trading, expansion, and blocking.", "citations": ["Belle2025Agents"], "pointer": "papers/paper_notes.jsonl:paper_id=P0051#key_results[1]"}], "B_highlights": [{"paper_id": "P0099", "evidence_id": "E-P0099-33c7e5174b", "excerpt": "We review and differentiate the work of LLMs and LLM-based agents from these six topics, examining their differences and similarities in tasks, benchmarks, and evaluation metrics.", "citations": ["Jin2024From"], "pointer": "papers/paper_notes.jsonl:paper_id=P0099#key_results[1]"}, {"paper_id": "P0050", "evidence_id": "E-P0050-f38be5e04b", "excerpt": "It outlines the general workflow of the task and establishes a taxonomy across three dimensions: benchmarks, techniques, and empirical studies.", "citations": ["Jiang2025Agentic"], "pointer": "papers/paper_notes.jsonl:paper_id=P0050#key_results[1]"}], "write_prompt": "Contrast Planning / reasoning loops vs Code agents / software tasks along \"evaluation protocol (datasets, metrics, human evaluation)\". Ground A and B using the highlight snippets; do not introduce new claims beyond the cited evidence."}, {"axis": "compute and latency constraints", "A_label": "Agent frameworks / architectures", "B_label": "Planning / reasoning loops", "citations": ["Zhang2025Agentic", "Li2025Learn", "Wei2026Agentic", "Belle2025Agents"], "A_highlights": [{"paper_id": "P0144", "evidence_id": "E-P0144-7d85a7241d", "excerpt": "Across agent and domain-specific benchmarks, ACE optimizes contexts both offline (e.g., system prompts) and online (e.g., agent memory), consistently outperforming strong baselines: +10.6% on agents and +8.6% on finance, while significantly reducing adaptation latency and", "citations": ["Zhang2025Agentic"], "pointer": "papers/paper_notes.jsonl:paper_id=P0144#key_results[0]"}, {"paper_id": "P0079", "evidence_id": "E-P0079-01f439b837", "excerpt": "We address this limitation by introducing a framework that enables LLM agents to learn and evolve both before and during test time, equipping them with environment-relevant knowledge for better planning and enhanced communication for improved cooperation.", "citations": ["Li2025Learn"], "pointer": "papers/paper_notes.jsonl:paper_id=P0079#limitations[1]"}], "B_highlights": [{"paper_id": "P0042", "evidence_id": "E-P0042-44dce85d3e", "excerpt": "We further review representative agentic reasoning frameworks across real-world applications and benchmarks, including science, robotics, healthcare, autonomous research, and mathematics.", "citations": ["Wei2026Agentic"], "pointer": "papers/paper_notes.jsonl:paper_id=P0042#key_results[0]"}, {"paper_id": "P0051", "evidence_id": "E-P0051-eae2720c71", "excerpt": "Settlers of Catan provides a challenging benchmark: success depends on balancing short- and long-term goals amid randomness, trading, expansion, and blocking.", "citations": ["Belle2025Agents"], "pointer": "papers/paper_notes.jsonl:paper_id=P0051#key_results[1]"}], "write_prompt": "Contrast Agent frameworks / architectures vs Planning / reasoning loops along \"compute and latency constraints\". Ground A and B using the highlight snippets; do not introduce new claims beyond the cited evidence."}, {"axis": "compute and latency constraints", "A_label": "Agent frameworks / architectures", "B_label": "Code agents / software tasks", "citations": ["Zhang2025Agentic", "Li2025Learn", "Jin2024From", "Jiang2025Agentic"], "A_highlights": [{"paper_id": "P0144", "evidence_id": "E-P0144-7d85a7241d", "excerpt": "Across agent and domain-specific benchmarks, ACE optimizes contexts both offline (e.g., system prompts) and online (e.g., agent memory), consistently outperforming strong baselines: +10.6% on agents and +8.6% on finance, while significantly reducing adaptation latency and", "citations": ["Zhang2025Agentic"], "pointer": "papers/paper_notes.jsonl:paper_id=P0144#key_results[0]"}, {"paper_id": "P0079", "evidence_id": "E-P0079-01f439b837", "excerpt": "We address this limitation by introducing a framework that enables LLM agents to learn and evolve both before and during test time, equipping them with environment-relevant knowledge for better planning and enhanced communication for improved cooperation.", "citations": ["Li2025Learn"], "pointer": "papers/paper_notes.jsonl:paper_id=P0079#limitations[1]"}], "B_highlights": [{"paper_id": "P0099", "evidence_id": "E-P0099-33c7e5174b", "excerpt": "We review and differentiate the work of LLMs and LLM-based agents from these six topics, examining their differences and similarities in tasks, benchmarks, and evaluation metrics.", "citations": ["Jin2024From"], "pointer": "papers/paper_notes.jsonl:paper_id=P0099#key_results[1]"}, {"paper_id": "P0050", "evidence_id": "E-P0050-f38be5e04b", "excerpt": "It outlines the general workflow of the task and establishes a taxonomy across three dimensions: benchmarks, techniques, and empirical studies.", "citations": ["Jiang2025Agentic"], "pointer": "papers/paper_notes.jsonl:paper_id=P0050#key_results[1]"}], "write_prompt": "Contrast Agent frameworks / architectures vs Code agents / software tasks along \"compute and latency constraints\". Ground A and B using the highlight snippets; do not introduce new claims beyond the cited evidence."}, {"axis": "compute and latency constraints", "A_label": "Planning / reasoning loops", "B_label": "Code agents / software tasks", "citations": ["Wei2026Agentic", "Belle2025Agents", "Jin2024From", "Jiang2025Agentic"], "A_highlights": [{"paper_id": "P0042", "evidence_id": "E-P0042-44dce85d3e", "excerpt": "We further review representative agentic reasoning frameworks across real-world applications and benchmarks, including science, robotics, healthcare, autonomous research, and mathematics.", "citations": ["Wei2026Agentic"], "pointer": "papers/paper_notes.jsonl:paper_id=P0042#key_results[0]"}, {"paper_id": "P0051", "evidence_id": "E-P0051-eae2720c71", "excerpt": "Settlers of Catan provides a challenging benchmark: success depends on balancing short- and long-term goals amid randomness, trading, expansion, and blocking.", "citations": ["Belle2025Agents"], "pointer": "papers/paper_notes.jsonl:paper_id=P0051#key_results[1]"}], "B_highlights": [{"paper_id": "P0099", "evidence_id": "E-P0099-33c7e5174b", "excerpt": "We review and differentiate the work of LLMs and LLM-based agents from these six topics, examining their differences and similarities in tasks, benchmarks, and evaluation metrics.", "citations": ["Jin2024From"], "pointer": "papers/paper_notes.jsonl:paper_id=P0099#key_results[1]"}, {"paper_id": "P0050", "evidence_id": "E-P0050-f38be5e04b", "excerpt": "It outlines the general workflow of the task and establishes a taxonomy across three dimensions: benchmarks, techniques, and empirical studies.", "citations": ["Jiang2025Agentic"], "pointer": "papers/paper_notes.jsonl:paper_id=P0050#key_results[1]"}], "write_prompt": "Contrast Planning / reasoning loops vs Code agents / software tasks along \"compute and latency constraints\". Ground A and B using the highlight snippets; do not introduce new claims beyond the cited evidence."}, {"axis": "communication protocol / roles", "A_label": "Agent frameworks / architectures", "B_label": "Planning / reasoning loops", "citations": ["Van2025Survey", "Zhang2025Agentic", "Wei2026Agentic", "Belle2025Agents"], "A_highlights": [{"paper_id": "P0133", "evidence_id": "E-P0133-a68f39bc04", "excerpt": "This survey provides a comprehensive overview of foundation models, agentic systems, datasets, and computational tools supporting this growing field.", "citations": ["Van2025Survey"], "pointer": "papers/paper_notes.jsonl:paper_id=P0133#key_results[0]"}, {"paper_id": "P0144", "evidence_id": "E-P0144-7d85a7241d", "excerpt": "Across agent and domain-specific benchmarks, ACE optimizes contexts both offline (e.g., system prompts) and online (e.g., agent memory), consistently outperforming strong baselines: +10.6% on agents and +8.6% on finance, while significantly reducing adaptation latency and", "citations": ["Zhang2025Agentic"], "pointer": "papers/paper_notes.jsonl:paper_id=P0144#key_results[0]"}], "B_highlights": [{"paper_id": "P0042", "evidence_id": "E-P0042-44dce85d3e", "excerpt": "We further review representative agentic reasoning frameworks across real-world applications and benchmarks, including science, robotics, healthcare, autonomous research, and mathematics.", "citations": ["Wei2026Agentic"], "pointer": "papers/paper_notes.jsonl:paper_id=P0042#key_results[0]"}, {"paper_id": "P0051", "evidence_id": "E-P0051-eae2720c71", "excerpt": "Settlers of Catan provides a challenging benchmark: success depends on balancing short- and long-term goals amid randomness, trading, expansion, and blocking.", "citations": ["Belle2025Agents"], "pointer": "papers/paper_notes.jsonl:paper_id=P0051#key_results[1]"}], "write_prompt": "Contrast Agent frameworks / architectures vs Planning / reasoning loops along \"communication protocol / roles\". Ground A and B using the highlight snippets; do not introduce new claims beyond the cited evidence."}], "evaluation_protocol": [{"bullet": "Evaluation mentions include: LLMs, LLM-based, ReAct, HexMachina, AlphaBeta, LIET, LLaMA, GPT-4o, ThreeD-World, MARL.", "citations": ["Wei2026Agentic", "Jiang2025Agentic", "Belle2025Agents", "Li2025Learn"]}, {"bullet": "When comparing results, anchor the paragraph with: task type + metric + constraint (budget, tool access, horizon, or threat model) when stated.", "citations": ["Wei2026Agentic", "Jiang2025Agentic"]}, {"bullet": "Prefer head-to-head comparisons only when benchmark/metric are shared; otherwise frame differences as protocol-driven rather than method superiority.", "citations": ["Wei2026Agentic", "Jiang2025Agentic"]}, {"bullet": "Avoid underspecified model/baseline naming; if abstracts omit details, state that the baseline is reported but underspecified instead of guessing.", "citations": ["Wei2026Agentic", "Jiang2025Agentic"]}, {"bullet": "If a claim relies on a single reported number, pair it with a limitation/caveat from the same evidence so the draft remains conservative.", "citations": ["Wei2026Agentic", "Jiang2025Agentic"]}, {"bullet": "If budgets or environments differ across papers, treat cross-paper numeric comparison as fragile and prefer qualitative contrasts aligned to the subsection axes.", "citations": ["Wei2026Agentic", "Jiang2025Agentic"]}], "limitation_hooks": [{"excerpt": "We address this limitation by introducing a framework that enables LLM agents to learn and evolve both before and during test time, equipping them with environment-relevant knowledge for better planning and enhanced communication for improved cooperation.", "citations": ["Li2025Learn"], "pointer": ""}, {"excerpt": "The survey begins by analyzing current LLM limitations, such as hallucinations and the lack of internal self-assessment mechanisms.", "citations": ["Bilal2025Meta"], "pointer": ""}, {"excerpt": "It then talks about newer methods, including RL from human feedback (RLHF), self-distillation, and chain-of-thought prompting, and each of their limitations.", "citations": ["Bilal2025Meta"], "pointer": ""}, {"excerpt": "We assess the early successes of foundation models and identify persistent limitations, including challenges in generalizability, interpretability, data imbalance, safety concerns, and limited multimodal fusion.", "citations": ["Van2025Survey"], "pointer": ""}, {"excerpt": "On the AppWorld leaderboard, ACE matches the top-ranked production-level agent on the overall average and surpasses it on the harder test-challenge split, despite using a smaller open-source model.", "citations": ["Zhang2025Agentic"], "pointer": ""}, {"excerpt": "The article concludes by outlining open challenges, potential security risks, and promising directions for advancing robust, interoperable, and scalable multi-agent LLM ecosystems.", "citations": ["Sarkar2025Survey"], "pointer": ""}, {"excerpt": "However, they also exhibit numerous limitations and shortcomings.", "citations": ["Jin2024From"], "pointer": ""}, {"excerpt": "LLM-based agents, a novel tech nology with the potential for Artificial General Intelligence (AGI), combine LLMs as the core for decision-making and action-taking, addressing some of the inherent limitations of LLMs such as lack of autonomy and self-improvement.", "citations": ["Jin2024From"], "pointer": ""}], "must_use": {"min_anchor_facts": 4, "min_comparison_cards": 4, "min_limitation_hooks": 2, "require_cited_numeric_if_available": true, "require_multi_cite_synthesis_paragraph": true, "thesis_required": true}, "do_not_repeat_phrases": ["this run", "this run is", "pipeline", "workspace", "unit", "quality gate", "evidence pack", "evidence packs", "writer context pack", "Method note (evidence policy):", "Methodology note:", "Methodology note (evidence policy):", "Methodology note (evidence-policy):", "Method note:", "This subsection surveys", "This subsection argues", "In this subsection", "This section surveys", "This section reviews", "This section discusses", "This section covers", "This section presents", "This section introduces", "provides an overview", "This survey provides an overview", "In this section, we provide an overview", "This section provides an overview", "Next, we move from", "We now turn to", "A few representative references include", "Notable lines of work include", "Concrete examples include", "Work in this area includes", "Recent systems include", "Examples include", "Representative systems include", "Therefore, survey synthesis should", "Therefore, survey comparisons should", "As a result, survey comparisons should", "survey synthesis should", "survey comparisons should", "Taken together,", "The key point is that", "Three limitations", "Two limitations", "claims remain provisional under abstract-only evidence", "abstract-only evidence", "Key takeaway:", "Main takeaway:", "protocol token", "protocol tokens", "constraint token", "constraint tokens", "metric token", "metric tokens", "evaluation token", "evaluation tokens", "In this survey", "In our survey", "Our survey", "This survey"], "pack_warnings": [], "pack_stats": {"anchors": {"raw": 12, "considered": 10, "kept": 10, "dropped_no_cites": 0}, "comparisons": {"raw": 10, "considered": 7, "kept": 7, "dropped_no_highlights": 0, "highlights_dropped_no_cites": 0}, "evaluation_protocol": {"raw": 6, "considered": 6, "kept": 6, "dropped_no_cites": 0}, "limitation_hooks": {"raw": 8, "considered": 8, "kept": 8, "dropped_no_cites": 0}, "trim_policy": {"default": 400, "anchor_fact": 420, "highlight_excerpt": 280, "comparison_write_prompt": 420, "eval_bullet": 320, "limitation_excerpt": 320}}, "generated_at": "2026-01-25T17:09:25"}
{"sub_id": "5.2", "title": "Multi-agent coordination", "section_id": "5", "section_title": "Learning, Adaptation & Coordination", "rq": "Which design choices in Multi-agent coordination drive the major trade-offs, and how are those trade-offs measured?", "thesis": "Multi-agent coordination highlights a tension around evaluation protocol (datasets, metrics, human evaluation) and compute and latency constraints, motivating a protocol-aware synthesis rather than per-paper summaries.", "tension_statement": "In Multi-agent coordination, the central trade-off is specialization versus coordination: dividing labor can boost performance but adds communication overhead and stability risks.", "evaluation_anchor_minimal": {"task": "attack/defense evaluation", "metric": "attack success rate", "constraint": "policy/sandbox setting"}, "paper_voice_palette": {"forbidden_pipeline_voice": ["this run", "this run is", "pipeline", "workspace", "unit", "quality gate", "evidence pack", "evidence packs", "writer context pack", "Method note (evidence policy):", "Methodology note:", "Methodology note (evidence policy):", "Methodology note (evidence-policy):", "Method note:"], "high_risk_templates": ["This subsection surveys", "This subsection argues", "In this subsection", "This section surveys", "This section reviews", "This section discusses", "This section covers", "This section presents", "This section introduces", "provides an overview", "This survey provides an overview", "In this section, we provide an overview", "This section provides an overview", "Next, we move from", "We now turn to", "A few representative references include", "Notable lines of work include", "Concrete examples include", "Work in this area includes", "Recent systems include", "Examples include", "Representative systems include", "Therefore, survey synthesis should", "Therefore, survey comparisons should", "As a result, survey comparisons should", "survey synthesis should", "survey comparisons should", "Taken together,", "The key point is that", "Three limitations", "Two limitations", "claims remain provisional under abstract-only evidence", "abstract-only evidence", "Key takeaway:", "Main takeaway:", "protocol token", "protocol tokens", "constraint token", "constraint tokens", "metric token", "metric tokens", "evaluation token", "evaluation tokens", "In this survey", "In our survey", "Our survey", "This survey"], "opener_archetypes": {"tension-first": ["A key tension is", "The central trade-off is", "A recurring constraint is"], "decision-first": ["For system builders, the crux is", "A practical decision is", "One design choice is"], "lens-first": ["Seen through the lens of", "From the perspective of", "Under an interface contract,"], "contrast-first": ["A useful contrast is between", "One sharp contrast is between", "A recurring split is between"], "protocol-first": ["Comparisons hinge on", "Results are only comparable when", "Evaluation claims depend on"]}, "synthesis_stems": ["Across these studies,", "Collectively,", "In summary,", "The evidence suggests that", "A consistent theme is that"], "rewrite_rules": [{"avoid_stem": "This subsection surveys", "prefer_stem": "A key tension is"}, {"avoid_stem": "In this subsection", "prefer_stem": "We focus on"}, {"avoid_stem": "provides an overview", "prefer_stem": "We focus on"}, {"avoid_stem": "Next, we move", "prefer_stem": "Having established"}, {"avoid_stem": "We now turn", "prefer_stem": "We then examine"}, {"avoid_stem": "survey comparisons should", "prefer_stem": "Across protocols, we observe"}, {"avoid_stem": "Two limitations", "prefer_stem": "A caveat is that"}, {"avoid_stem": "Three limitations", "prefer_stem": "One limitation is that"}, {"avoid_stem": "In this survey", "prefer_stem": "We examine"}, {"avoid_stem": "In our survey", "prefer_stem": "We examine"}, {"avoid_stem": "Our survey", "prefer_stem": "This work"}, {"avoid_stem": "This survey", "prefer_stem": "This work"}], "discourse_stem_watchlist": ["Additionally,", "Moreover,", "Furthermore,", "This suggests", "Taken together,", "The key point is that", "Overall,", "In summary,", "In practice,", "More broadly,", "Importantly,", "Notably,", "Crucially,", "In general,", "At a high level,", "In other words,", "Therefore,", "In addition,", "As a result,"], "discourse_stem_rewrites": {"Additionally,": ["More importantly,", "At the same time,", "A second consideration is"], "This suggests": ["This pattern indicates", "These results point to", "A plausible explanation is"], "Taken together,": ["Across these studies,", "Collectively,", "The evidence suggests"], "The key point is that": ["A practical implication is that", "A useful way to read these results is that", "One takeaway is that"], "Overall,": ["In practice,", "More broadly,", "A practical implication is that", "One way to read this evidence is that"], "In summary,": ["Across these studies,", "Collectively,", "A consistent theme is that", "The evidence suggests that"], "Importantly,": ["More importantly,", "A key constraint is that", "A practical implication is that"], "Notably,": ["In particular,", "A concrete example is that", "One implication is that"], "Crucially,": ["More importantly,", "A central constraint is that", "A key takeaway is that"], "At a high level,": ["Concretely,", "Under this protocol,", "In practice,"], "In other words,": ["Put differently,", "Equivalently,", "More concretely,"], "Therefore,": ["As a result,", "This in turn means that", "A practical implication is that"], "In addition,": ["At the same time,", "A second consideration is", "More importantly,"], "As a result,": ["This in turn means that", "Consequently,", "A practical implication is that"]}, "role_cards": {"section_author": {"mission": "Write one subsection as an argument (not a topic list), using in-scope citations as evidence.", "do": ["State a concrete tension/trade-off early and commit to a thesis.", "Make at least two explicit A-vs-B contrasts (mechanism -> outcome) instead of per-paper summaries.", "When citing numbers, add minimal context (task + metric + constraint) in the same paragraph.", "End with a limitation that changes interpretation (protocol mismatch, unclear threat model, missing ablations)."], "avoid": ["Outline narration (This subsection..., In this subsection...).", "Slide navigation (Next, we move..., We now turn...).", "Meta survey advice (survey comparisons should...).", "Cite dumps (a trailing [@a; @b; @c] without a claim)."]}, "evidence_steward": {"mission": "Keep claims auditable: citations support the sentence that needs them, and scope stays local.", "do": ["Embed citations inside claim sentences; name concrete nouns (system/benchmark/protocol) near each cite.", "Prefer subsection-scoped citations; use chapter/global only when truly cross-cutting.", "Downgrade overconfident claims when evidence is thin; convert unknowns into verification targets (once, not spam)."], "avoid": ["Out-of-scope citations to make a paragraph sound stronger.", "Repeating evidence-policy disclaimers in every subsection.", "Ambiguous model naming (e.g., GPT-5) unless the cited work uses it."]}, "style_harmonizer": {"mission": "Make the prose read like a paper: calm, specific, and varied in rhythm without sounding templated.", "do": ["Vary opener cadence across sections (tension-first / decision-first / lens-first) without reusing the same stem.", "Prefer argument bridges over navigation; keep signposting light and content-bearing.", "Reduce slash-enumerations (A/B/C axis labels) by rewriting into natural prose."], "avoid": ["Count-based openers used as a slot (e.g., Two limitations..., Three takeaways...).", "Repeated discourse stems (Additionally, This suggests, Taken together) across many paragraphs.", "Pipeline words (workspace/unit/quality gate/evidence pack) in reader-facing text.", "PPT speaker-note tone (Now we..., The remainder of...).", "Internal shorthand that reads like planning notes (e.g., protocol/metric/constraint tokens)."]}}, "version": "0.4"}, "opener_mode": "contrast-first", "opener_hint": "Start with an explicit A-vs-B contrast; state why it matters; end paragraph 1 with the thesis.", "axes": ["evaluation protocol (datasets, metrics, human evaluation)", "compute and latency constraints", "tool interface contract (schemas / protocols)", "tool selection / routing policy", "sandboxing / permissions / observability"], "bridge_terms": ["roles", "communication", "debate", "aggregation", "stability", "benchmarks/metrics"], "contrast_hook": "coordination", "required_evidence_fields": ["benchmarks/datasets", "metrics / human-eval protocol", "compute / cost (train/infer)", "training signal / supervision", "threat model", "defense surface"], "paragraph_plan": [{"para": 1, "argument_role": "setup_thesis", "intent": "Define scope, setup, and the subsection thesis (no pipeline jargon).", "focus": ["scope boundary", "key definitions", "thesis vs neighboring subsections"], "connector_to_prev": "", "connector_phrase": "", "use_clusters": ["Agent frameworks / architectures"], "rq": "Which design choices in Multi-agent coordination drive the major trade-offs, and how are those trade-offs measured?"}, {"para": 2, "argument_role": "mechanism_cluster_A", "intent": "Explain cluster A: core mechanism and system architecture and what decision it makes in the agent loop.", "focus": ["cluster: Agent frameworks / architectures", "core mechanism and system architecture", "assumptions"], "connector_to_prev": "grounding", "connector_phrase": "baseline route (Agent frameworks / architectures)", "use_clusters": ["Agent frameworks / architectures"]}, {"para": 3, "argument_role": "implementation_cluster_A", "intent": "Cluster A implementation details: training and data signals and interface contract (tools/memory) that constrain behavior.", "focus": ["cluster: Agent frameworks / architectures", "training and data setup", "interface contract", "axes: evaluation protocol (datasets, metrics, human evaluation), compute and latency constraints, tool interface contract (schemas / protocols), tool selection / routing policy, sandboxing / permissions / observability"], "connector_to_prev": "elaboration", "connector_phrase": "implementation assumptions (interface + training)", "use_clusters": ["Agent frameworks / architectures"]}, {"para": 4, "argument_role": "evaluation_cluster_A", "intent": "Cluster A evaluation/trade-offs: where it works, costs (compute/latency), and typical failure modes.", "focus": ["cluster: Agent frameworks / architectures", "evaluation anchor", "efficiency", "failure modes"], "connector_to_prev": "evaluation", "connector_phrase": "evaluation anchor (task/metric/constraint) + failure modes", "use_clusters": ["Agent frameworks / architectures"]}, {"para": 5, "argument_role": "contrast_cluster_B", "intent": "Explain cluster B (contrast with A): core mechanism and system architecture and what it optimizes for.", "focus": ["cluster: Multi-agent coordination", "contrast with Agent frameworks / architectures", "core mechanism and system architecture"], "connector_to_prev": "contrast", "connector_phrase": "contrast route (Multi-agent coordination vs Agent frameworks / architectures)", "use_clusters": ["Multi-agent coordination"]}, {"para": 6, "argument_role": "implementation_cluster_B", "intent": "Cluster B implementation details: training and data and interface assumptions (mirror A for comparability).", "focus": ["cluster: Multi-agent coordination", "training and data setup", "interface contract", "axes: evaluation protocol (datasets, metrics, human evaluation), compute and latency constraints, tool interface contract (schemas / protocols), tool selection / routing policy, sandboxing / permissions / observability"], "connector_to_prev": "elaboration", "connector_phrase": "contrast implementation assumptions (B)", "use_clusters": ["Multi-agent coordination"]}, {"para": 7, "argument_role": "evaluation_cluster_B", "intent": "Cluster B evaluation/trade-offs: where it works, costs, and failure modes (mirror A).", "focus": ["cluster: Multi-agent coordination", "evaluation anchor", "efficiency", "failure modes"], "connector_to_prev": "evaluation", "connector_phrase": "contrast evaluation anchor + trade-offs (B)", "use_clusters": ["Multi-agent coordination"]}, {"para": 8, "argument_role": "cross_paper_synthesis", "intent": "Cross-paper synthesis: compare clusters along the main axes (include >=2 citations in one paragraph).", "focus": ["compare Agent frameworks / architectures vs Multi-agent coordination", "multiple citations in one paragraph", "axes: evaluation protocol (datasets, metrics, human evaluation), compute and latency constraints, tool interface contract (schemas / protocols), tool selection / routing policy, sandboxing / permissions / observability"], "connector_to_prev": "synthesis", "connector_phrase": "cross-paper synthesis (Agent frameworks / architectures vs Multi-agent coordination)", "use_clusters": ["Agent frameworks / architectures", "Multi-agent coordination", "Planning / reasoning loops"]}, {"para": 9, "argument_role": "decision_guidance", "intent": "Decision guidance: when to choose which route (criteria + evaluation signals + engineering constraints).", "focus": ["decision checklist", "evaluation protocol", "practical constraints"], "connector_to_prev": "consequence", "connector_phrase": "decision guidance / criteria", "use_clusters": ["Agent frameworks / architectures", "Multi-agent coordination", "Planning / reasoning loops"]}, {"para": 10, "argument_role": "limitations_open_questions", "intent": "Limitations + verification targets; end with a concrete open question to hand off.", "focus": ["limitations", "evidence mode: provisional", "what needs verification", "open question"], "connector_to_prev": "limitations", "connector_phrase": "limitations + verification targets", "use_clusters": ["Agent frameworks / architectures", "Multi-agent coordination", "Planning / reasoning loops"], "policy": "Use conservative language; avoid strong conclusions; prefer questions-to-answer + explicit evidence gaps list."}], "chapter_throughline": ["Pin scope to goal: agent survey latex.", "Compare approaches along: evaluation protocol (datasets, metrics, human evaluation).", "Compare approaches along: compute and latency constraints.", "Compare approaches along: communication protocol / roles.", "Compare approaches along: aggregation (vote / debate / referee).", "Compare approaches along: stability / robustness."], "chapter_key_contrasts": ["learning/feedback", "coordination"], "chapter_synthesis_mode": "tradeoff_matrix", "allowed_bibkeys_selected": ["Jiang2023Large", "Aratchige2025Llms", "Masters2025Arcane", "Becker2025Mallm", "Inoue2024Drugagent", "Wang2025Autoscore", "Collini2025Marvel", "Xu2025Autonomous", "Shen2024Small", "Ye2025Cognipair", "Xu2023Magic", "Chen2024Llmarena", "Rouzrokh2025Lattereview", "Li2025Learn", "Cui2025Toward", "Chen2025Schema", "Zahedifar2025Agent", "Xu2023Towards", "Trirat2024Automl", "Le2024Multi", "Chen2024Solving"], "allowed_bibkeys_mapped": ["Trirat2024Automl", "Aratchige2025Llms", "Wang2025Autoscore", "Cui2025Toward", "Collini2025Marvel", "Wu2025Multi", "Masters2025Arcane", "Ye2025Cognipair", "Liu2025Medchat", "Le2024Multi", "Chen2024Solving", "Xu2023Magic", "Motwani2024Malt", "Zahedifar2025Agent", "Rouzrokh2025Lattereview", "Mushtaq2025Harnessing", "Becker2025Mallm", "Zhang2024Towards", "Jiang2023Large", "Xu2023Towards", "Li2025Learn", "Bilal2025Meta", "Shen2024Small", "Chen2024Llmarena", "Xu2025Autonomous", "Chen2025Schema", "Inoue2024Drugagent", "Li2023Modelscope"], "allowed_bibkeys_chapter": ["Aratchige2025Llms", "Becker2025Mallm", "Belle2025Agents", "Bharadwaj2025Omnireflect", "Bilal2025Meta", "Chen2024Llmarena", "Chen2024Solving", "Chen2025Grounded", "Chen2025Largea", "Chen2025Schema", "Chen2025Survey", "Collini2025Marvel", "Cui2025Toward", "Hu2024Survey", "Inoue2024Drugagent", "Jiang2023Large", "Jiang2025Agentic", "Jin2024From", "Le2024Multi", "Li2023Modelscope", "Li2025Learn", "Liu2025Medchat", "Liu2025Powered", "Lu2025Pilotrl", "Masters2025Arcane", "Motwani2024Malt", "Mou2024From", "Mushtaq2025Harnessing", "Rouzrokh2025Lattereview", "Samaei2025Epidemiqs", "Sarkar2025Survey", "Shen2024Small", "Tao2024Survey", "Taylor2025Large", "Trirat2024Automl", "Van2025Survey", "Wang2024Learning", "Wang2025Autoscore", "Wei2026Agentic", "Wu2024Federated", "Wu2025Multi", "Xia2025Sand", "Xu2023Magic", "Xu2023Towards", "Xu2025Autonomous", "Ye2025Cognipair", "Yu2023Finmem", "Zahedifar2025Agent", "Zhang2024Affective", "Zhang2024Towards", "Zhang2025Agentic", "Zhang2025Security", "Zhou2024Archer", "Zhou2025Self"], "allowed_bibkeys_global": ["Li2023Modelscope", "Yao2022React"], "evidence_ids": ["E-P0040-96b056b80b", "E-P0012-7cac5db9a9", "E-P0139-8eb9ff221b", "E-P0194-f2e78c673e", "E-P0255-5e37132da5", "E-P0149-42512d3cac", "E-P0080-0e82f75090", "E-P0183-6770e6174b", "E-P0108-9640816b42", "E-P0159-1ea69fbec3", "E-P0115-af857798be", "E-P0270-764758958d", "E-P0078-fa4336d046", "E-P0079-1dd544863c", "E-P0230-15e523063d", "E-P0212-1167f52f16", "E-P0070-0f34c44fa9", "E-P0119-02f16b54ff", "E-P0248-c44176def5", "E-P0286-73bcad694d", "E-P0183-1c544ba66e", "E-P0294-cefe4d686d", "E-P0080-6701c90e15", "E-P0248-3fe4bbe4f2"], "anchor_facts": [{"hook_type": "quant", "text": "Using a corpus of 219 labeled rubrics derived from the GDPVal benchmark, we evaluate ARCANE on challenging tasks requiring multi-step reasoning and tool use.", "citations": ["Masters2025Arcane"], "paper_id": "P0139", "evidence_id": "E-P0139-8eb9ff221b", "pointer": "papers/paper_notes.jsonl:paper_id=P0139#key_results[0]"}, {"hook_type": "quant", "text": "Our experiments on Communicative Watch-And-Help and ThreeD-World Multi-Agent Transport benchmarks demonstrate that LIET, instantiated with both LLaMA and GPT-4o, outperforms existing baselines and exhibits strong cooperative planning abilities.", "citations": ["Li2025Learn"], "paper_id": "P0079", "evidence_id": "E-P0079-1dd544863c", "pointer": "papers/paper_notes.jsonl:paper_id=P0079#key_results[1]"}, {"hook_type": "quant", "text": "We evaluate AutoSCORE on four benchmark datasets from the ASAP benchmark, using both proprietary and open-source LLMs (GPT-4o, LLaMA-3.1-8B, and LLaMA-3.1-70B).", "citations": ["Wang2025Autoscore"], "paper_id": "P0149", "evidence_id": "E-P0149-42512d3cac", "pointer": "papers/paper_notes.jsonl:paper_id=P0149#key_results[0]"}, {"hook_type": "quant", "text": "We conducted comprehensive experiments using a kinase inhibitor dataset, where our multi-agent LLM method outperformed the non-reasoning multi-agent model (GPT-4o mini) by 45% in F1 score (0.514 vs 0.355).", "citations": ["Inoue2024Drugagent"], "paper_id": "P0255", "evidence_id": "E-P0255-5e37132da5", "pointer": "papers/paper_notes.jsonl:paper_id=P0255#key_results[0]"}, {"hook_type": "quant", "text": "In this work, we propose SG^2, an iterative Schema-Guided Scene-Graph reasoning framework based on multi-agent LLMs.", "citations": ["Chen2025Schema"], "paper_id": "P0212", "evidence_id": "E-P0212-1167f52f16", "pointer": "papers/paper_notes.jsonl:paper_id=P0212#key_results[0]"}, {"hook_type": "quant", "text": "To this end, we propose a multi-agent system with customized communication knowledge and tools for solving communication related tasks using natural language, comprising three components: (1) Multi-agent Data Retrieval (MDR), which employs the condensate and inference agents to r", "citations": ["Jiang2023Large"], "paper_id": "P0040", "evidence_id": "E-P0040-96b056b80b", "pointer": "papers/paper_notes.jsonl:paper_id=P0040#key_results[0]"}, {"hook_type": "eval", "text": "Current frameworks for multi-agent debate are often designed towards tool use, lack integrated evaluation, or provide limited configurability of agent personas, response generators, discussion paradigms, and decision protocols.", "citations": ["Becker2025Mallm"], "paper_id": "P0194", "evidence_id": "E-P0194-f2e78c673e", "pointer": "papers/paper_notes.jsonl:paper_id=P0194#key_results[1]"}, {"hook_type": "quant", "text": "We find that 20 of the 48 issues reported by MARVEL pose security vulnerabilities.", "citations": ["Collini2025Marvel"], "paper_id": "P0080", "evidence_id": "E-P0080-0e82f75090", "pointer": "papers/paper_notes.jsonl:paper_id=P0080#key_results[0]"}, {"hook_type": "eval", "text": "Evaluation across various tool-use benchmarks illustrates that our proposed multi-LLM framework surpasses the traditional single-LLM approach, highlighting its efficacy and advantages in tool learning.", "citations": ["Shen2024Small"], "paper_id": "P0108", "evidence_id": "E-P0108-9640816b42", "pointer": "papers/paper_notes.jsonl:paper_id=P0108#key_results[1]"}, {"hook_type": "quant", "text": "Validation using 551 GNWT-Agents and Columbia University Speed Dating dataset demonstrates 72% correlation with human attraction patterns, 77.8% match prediction accuracy, and 74% agreement in human validation studies.", "citations": ["Ye2025Cognipair"], "paper_id": "P0159", "evidence_id": "E-P0159-1ea69fbec3", "pointer": "papers/paper_notes.jsonl:paper_id=P0159#key_results[0]"}], "comparison_cards": [{"axis": "evaluation protocol (datasets, metrics, human evaluation)", "A_label": "Agent frameworks / architectures", "B_label": "Multi-agent coordination", "citations": ["Masters2025Arcane", "Li2025Learn", "Wang2025Autoscore"], "A_highlights": [{"paper_id": "P0139", "evidence_id": "E-P0139-8eb9ff221b", "excerpt": "Using a corpus of 219 labeled rubrics derived from the GDPVal benchmark, we evaluate ARCANE on challenging tasks requiring multi-step reasoning and tool use.", "citations": ["Masters2025Arcane"], "pointer": "papers/paper_notes.jsonl:paper_id=P0139#key_results[0]"}, {"paper_id": "P0079", "evidence_id": "E-P0079-1dd544863c", "excerpt": "Our experiments on Communicative Watch-And-Help and ThreeD-World Multi-Agent Transport benchmarks demonstrate that LIET, instantiated with both LLaMA and GPT-4o, outperforms existing baselines and exhibits strong cooperative planning abilities.", "citations": ["Li2025Learn"], "pointer": "papers/paper_notes.jsonl:paper_id=P0079#key_results[1]"}], "B_highlights": [{"paper_id": "P0149", "evidence_id": "E-P0149-42512d3cac", "excerpt": "We evaluate AutoSCORE on four benchmark datasets from the ASAP benchmark, using both proprietary and open-source LLMs (GPT-4o, LLaMA-3.1-8B, and LLaMA-3.1-70B).", "citations": ["Wang2025Autoscore"], "pointer": "papers/paper_notes.jsonl:paper_id=P0149#key_results[0]"}, {"paper_id": "P0139", "evidence_id": "E-P0139-8eb9ff221b", "excerpt": "Using a corpus of 219 labeled rubrics derived from the GDPVal benchmark, we evaluate ARCANE on challenging tasks requiring multi-step reasoning and tool use.", "citations": ["Masters2025Arcane"], "pointer": "papers/paper_notes.jsonl:paper_id=P0139#key_results[0]"}], "write_prompt": "Contrast Agent frameworks / architectures vs Multi-agent coordination along \"evaluation protocol (datasets, metrics, human evaluation)\". Ground A and B using the highlight snippets; do not introduce new claims beyond the cited evidence."}, {"axis": "evaluation protocol (datasets, metrics, human evaluation)", "A_label": "Agent frameworks / architectures", "B_label": "Planning / reasoning loops", "citations": ["Masters2025Arcane", "Li2025Learn", "Inoue2024Drugagent", "Xu2023Towards"], "A_highlights": [{"paper_id": "P0139", "evidence_id": "E-P0139-8eb9ff221b", "excerpt": "Using a corpus of 219 labeled rubrics derived from the GDPVal benchmark, we evaluate ARCANE on challenging tasks requiring multi-step reasoning and tool use.", "citations": ["Masters2025Arcane"], "pointer": "papers/paper_notes.jsonl:paper_id=P0139#key_results[0]"}, {"paper_id": "P0079", "evidence_id": "E-P0079-1dd544863c", "excerpt": "Our experiments on Communicative Watch-And-Help and ThreeD-World Multi-Agent Transport benchmarks demonstrate that LIET, instantiated with both LLaMA and GPT-4o, outperforms existing baselines and exhibits strong cooperative planning abilities.", "citations": ["Li2025Learn"], "pointer": "papers/paper_notes.jsonl:paper_id=P0079#key_results[1]"}], "B_highlights": [{"paper_id": "P0255", "evidence_id": "E-P0255-5e37132da5", "excerpt": "We conducted comprehensive experiments using a kinase inhibitor dataset, where our multi-agent LLM method outperformed the non-reasoning multi-agent model (GPT-4o mini) by 45% in F1 score (0.514 vs 0.355).", "citations": ["Inoue2024Drugagent"], "pointer": "papers/paper_notes.jsonl:paper_id=P0255#key_results[0]"}, {"paper_id": "P0119", "evidence_id": "E-P0119-02f16b54ff", "excerpt": "Extensive experiments on three different types of reasoning tasks show that our collaboration approach delivers superior accuracy across all ten datasets compared to existing methods.", "citations": ["Xu2023Towards"], "pointer": "papers/paper_notes.jsonl:paper_id=P0119#key_results[1]"}], "write_prompt": "Contrast Agent frameworks / architectures vs Planning / reasoning loops along \"evaluation protocol (datasets, metrics, human evaluation)\". Ground A and B using the highlight snippets; do not introduce new claims beyond the cited evidence."}, {"axis": "evaluation protocol (datasets, metrics, human evaluation)", "A_label": "Multi-agent coordination", "B_label": "Planning / reasoning loops", "citations": ["Wang2025Autoscore", "Masters2025Arcane", "Inoue2024Drugagent", "Xu2023Towards"], "A_highlights": [{"paper_id": "P0149", "evidence_id": "E-P0149-42512d3cac", "excerpt": "We evaluate AutoSCORE on four benchmark datasets from the ASAP benchmark, using both proprietary and open-source LLMs (GPT-4o, LLaMA-3.1-8B, and LLaMA-3.1-70B).", "citations": ["Wang2025Autoscore"], "pointer": "papers/paper_notes.jsonl:paper_id=P0149#key_results[0]"}, {"paper_id": "P0139", "evidence_id": "E-P0139-8eb9ff221b", "excerpt": "Using a corpus of 219 labeled rubrics derived from the GDPVal benchmark, we evaluate ARCANE on challenging tasks requiring multi-step reasoning and tool use.", "citations": ["Masters2025Arcane"], "pointer": "papers/paper_notes.jsonl:paper_id=P0139#key_results[0]"}], "B_highlights": [{"paper_id": "P0255", "evidence_id": "E-P0255-5e37132da5", "excerpt": "We conducted comprehensive experiments using a kinase inhibitor dataset, where our multi-agent LLM method outperformed the non-reasoning multi-agent model (GPT-4o mini) by 45% in F1 score (0.514 vs 0.355).", "citations": ["Inoue2024Drugagent"], "pointer": "papers/paper_notes.jsonl:paper_id=P0255#key_results[0]"}, {"paper_id": "P0119", "evidence_id": "E-P0119-02f16b54ff", "excerpt": "Extensive experiments on three different types of reasoning tasks show that our collaboration approach delivers superior accuracy across all ten datasets compared to existing methods.", "citations": ["Xu2023Towards"], "pointer": "papers/paper_notes.jsonl:paper_id=P0119#key_results[1]"}], "write_prompt": "Contrast Multi-agent coordination vs Planning / reasoning loops along \"evaluation protocol (datasets, metrics, human evaluation)\". Ground A and B using the highlight snippets; do not introduce new claims beyond the cited evidence."}, {"axis": "compute and latency constraints", "A_label": "Agent frameworks / architectures", "B_label": "Multi-agent coordination", "citations": ["Aratchige2025Llms", "Li2025Learn", "Rouzrokh2025Lattereview"], "A_highlights": [{"paper_id": "P0012", "evidence_id": "E-P0012-7cac5db9a9", "excerpt": "By analyzing recent advancements and their limitations - such as scalability, real-time response challenges, and agent coordination constraints, we provide a detailed view of the technological landscape.", "citations": ["Aratchige2025Llms"], "pointer": "papers/paper_notes.jsonl:paper_id=P0012#limitations[1]"}, {"paper_id": "P0079", "evidence_id": "E-P0079-1dd544863c", "excerpt": "Our experiments on Communicative Watch-And-Help and ThreeD-World Multi-Agent Transport benchmarks demonstrate that LIET, instantiated with both LLaMA and GPT-4o, outperforms existing baselines and exhibits strong cooperative planning abilities.", "citations": ["Li2025Learn"], "pointer": "papers/paper_notes.jsonl:paper_id=P0079#key_results[1]"}], "B_highlights": [{"paper_id": "P0079", "evidence_id": "E-P0079-1dd544863c", "excerpt": "Our experiments on Communicative Watch-And-Help and ThreeD-World Multi-Agent Transport benchmarks demonstrate that LIET, instantiated with both LLaMA and GPT-4o, outperforms existing baselines and exhibits strong cooperative planning abilities.", "citations": ["Li2025Learn"], "pointer": "papers/paper_notes.jsonl:paper_id=P0079#key_results[1]"}, {"paper_id": "P0078", "evidence_id": "E-P0078-fa4336d046", "excerpt": "Systematic literature reviews and meta-analyses are essential for synthesizing research insights, but they remain time-intensive and labor-intensive due to the iterative processes of screening, evaluation, and data extraction.", "citations": ["Rouzrokh2025Lattereview"], "pointer": "papers/paper_notes.jsonl:paper_id=P0078#key_results[0]"}], "write_prompt": "Contrast Agent frameworks / architectures vs Multi-agent coordination along \"compute and latency constraints\". Ground A and B using the highlight snippets; do not introduce new claims beyond the cited evidence."}, {"axis": "compute and latency constraints", "A_label": "Agent frameworks / architectures", "B_label": "Planning / reasoning loops", "citations": ["Aratchige2025Llms", "Li2025Learn", "Inoue2024Drugagent", "Xu2025Autonomous"], "A_highlights": [{"paper_id": "P0012", "evidence_id": "E-P0012-7cac5db9a9", "excerpt": "By analyzing recent advancements and their limitations - such as scalability, real-time response challenges, and agent coordination constraints, we provide a detailed view of the technological landscape.", "citations": ["Aratchige2025Llms"], "pointer": "papers/paper_notes.jsonl:paper_id=P0012#limitations[1]"}, {"paper_id": "P0079", "evidence_id": "E-P0079-1dd544863c", "excerpt": "Our experiments on Communicative Watch-And-Help and ThreeD-World Multi-Agent Transport benchmarks demonstrate that LIET, instantiated with both LLaMA and GPT-4o, outperforms existing baselines and exhibits strong cooperative planning abilities.", "citations": ["Li2025Learn"], "pointer": "papers/paper_notes.jsonl:paper_id=P0079#key_results[1]"}], "B_highlights": [{"paper_id": "P0255", "evidence_id": "E-P0255-5e37132da5", "excerpt": "We conducted comprehensive experiments using a kinase inhibitor dataset, where our multi-agent LLM method outperformed the non-reasoning multi-agent model (GPT-4o mini) by 45% in F1 score (0.514 vs 0.355).", "citations": ["Inoue2024Drugagent"], "pointer": "papers/paper_notes.jsonl:paper_id=P0255#key_results[0]"}, {"paper_id": "P0183", "evidence_id": "E-P0183-6770e6174b", "excerpt": "The increasing integration of Industrial IoT (IIoT) exposes critical cyber-physical systems to sophisticated, multi-stage attacks that elude traditional defenses lacking contextual awareness.", "citations": ["Xu2025Autonomous"], "pointer": "papers/paper_notes.jsonl:paper_id=P0183#summary_bullets[0]"}], "write_prompt": "Contrast Agent frameworks / architectures vs Planning / reasoning loops along \"compute and latency constraints\". Ground A and B using the highlight snippets; do not introduce new claims beyond the cited evidence."}, {"axis": "compute and latency constraints", "A_label": "Multi-agent coordination", "B_label": "Planning / reasoning loops", "citations": ["Li2025Learn", "Rouzrokh2025Lattereview", "Inoue2024Drugagent", "Xu2025Autonomous"], "A_highlights": [{"paper_id": "P0079", "evidence_id": "E-P0079-1dd544863c", "excerpt": "Our experiments on Communicative Watch-And-Help and ThreeD-World Multi-Agent Transport benchmarks demonstrate that LIET, instantiated with both LLaMA and GPT-4o, outperforms existing baselines and exhibits strong cooperative planning abilities.", "citations": ["Li2025Learn"], "pointer": "papers/paper_notes.jsonl:paper_id=P0079#key_results[1]"}, {"paper_id": "P0078", "evidence_id": "E-P0078-fa4336d046", "excerpt": "Systematic literature reviews and meta-analyses are essential for synthesizing research insights, but they remain time-intensive and labor-intensive due to the iterative processes of screening, evaluation, and data extraction.", "citations": ["Rouzrokh2025Lattereview"], "pointer": "papers/paper_notes.jsonl:paper_id=P0078#key_results[0]"}], "B_highlights": [{"paper_id": "P0255", "evidence_id": "E-P0255-5e37132da5", "excerpt": "We conducted comprehensive experiments using a kinase inhibitor dataset, where our multi-agent LLM method outperformed the non-reasoning multi-agent model (GPT-4o mini) by 45% in F1 score (0.514 vs 0.355).", "citations": ["Inoue2024Drugagent"], "pointer": "papers/paper_notes.jsonl:paper_id=P0255#key_results[0]"}, {"paper_id": "P0183", "evidence_id": "E-P0183-6770e6174b", "excerpt": "The increasing integration of Industrial IoT (IIoT) exposes critical cyber-physical systems to sophisticated, multi-stage attacks that elude traditional defenses lacking contextual awareness.", "citations": ["Xu2025Autonomous"], "pointer": "papers/paper_notes.jsonl:paper_id=P0183#summary_bullets[0]"}], "write_prompt": "Contrast Multi-agent coordination vs Planning / reasoning loops along \"compute and latency constraints\". Ground A and B using the highlight snippets; do not introduce new claims beyond the cited evidence."}, {"axis": "tool interface contract (schemas / protocols)", "A_label": "Agent frameworks / architectures", "B_label": "Multi-agent coordination", "citations": ["Masters2025Arcane", "Li2025Learn"], "A_highlights": [{"paper_id": "P0139", "evidence_id": "E-P0139-8eb9ff221b", "excerpt": "Using a corpus of 219 labeled rubrics derived from the GDPVal benchmark, we evaluate ARCANE on challenging tasks requiring multi-step reasoning and tool use.", "citations": ["Masters2025Arcane"], "pointer": "papers/paper_notes.jsonl:paper_id=P0139#key_results[0]"}, {"paper_id": "P0079", "evidence_id": "E-P0079-1dd544863c", "excerpt": "Our experiments on Communicative Watch-And-Help and ThreeD-World Multi-Agent Transport benchmarks demonstrate that LIET, instantiated with both LLaMA and GPT-4o, outperforms existing baselines and exhibits strong cooperative planning abilities.", "citations": ["Li2025Learn"], "pointer": "papers/paper_notes.jsonl:paper_id=P0079#key_results[1]"}], "B_highlights": [{"paper_id": "P0139", "evidence_id": "E-P0139-8eb9ff221b", "excerpt": "Using a corpus of 219 labeled rubrics derived from the GDPVal benchmark, we evaluate ARCANE on challenging tasks requiring multi-step reasoning and tool use.", "citations": ["Masters2025Arcane"], "pointer": "papers/paper_notes.jsonl:paper_id=P0139#key_results[0]"}, {"paper_id": "P0079", "evidence_id": "E-P0079-1dd544863c", "excerpt": "Our experiments on Communicative Watch-And-Help and ThreeD-World Multi-Agent Transport benchmarks demonstrate that LIET, instantiated with both LLaMA and GPT-4o, outperforms existing baselines and exhibits strong cooperative planning abilities.", "citations": ["Li2025Learn"], "pointer": "papers/paper_notes.jsonl:paper_id=P0079#key_results[1]"}], "write_prompt": "Contrast Agent frameworks / architectures vs Multi-agent coordination along \"tool interface contract (schemas / protocols)\". Ground A and B using the highlight snippets; do not introduce new claims beyond the cited evidence."}], "evaluation_protocol": [{"bullet": "Evaluation mentions include: ReAct, LLM-Agent-Controller, RAG, LLMs, LatteReview, GitHub, LLM-based, LIET, LLaMA, GPT-4o.", "citations": ["Aratchige2025Llms", "Zahedifar2025Agent", "Rouzrokh2025Lattereview", "Li2025Learn"]}, {"bullet": "When comparing results, anchor the paragraph with: task type + metric + constraint (budget, tool access, horizon, or threat model) when stated.", "citations": ["Aratchige2025Llms", "Zahedifar2025Agent"]}, {"bullet": "Prefer head-to-head comparisons only when benchmark/metric are shared; otherwise frame differences as protocol-driven rather than method superiority.", "citations": ["Aratchige2025Llms", "Zahedifar2025Agent"]}, {"bullet": "Avoid underspecified model/baseline naming; if abstracts omit details, state that the baseline is reported but underspecified instead of guessing.", "citations": ["Aratchige2025Llms", "Zahedifar2025Agent"]}, {"bullet": "If a claim relies on a single reported number, pair it with a limitation/caveat from the same evidence so the draft remains conservative.", "citations": ["Aratchige2025Llms", "Zahedifar2025Agent"]}, {"bullet": "If budgets or environments differ across papers, treat cross-paper numeric comparison as fragile and prefer qualitative contrasts aligned to the subsection axes.", "citations": ["Aratchige2025Llms", "Zahedifar2025Agent"]}], "limitation_hooks": [{"excerpt": "By analyzing recent advancements and their limitations - such as scalability, real-time response challenges, and agent coordination constraints, we provide a detailed view of the technological landscape.", "citations": ["Aratchige2025Llms"], "pointer": ""}, {"excerpt": "We address this limitation by introducing a framework that enables LLM agents to learn and evolve both before and during test time, equipping them with environment-relevant knowledge for better planning and enhanced communication for improved cooperation.", "citations": ["Li2025Learn"], "pointer": ""}, {"excerpt": "Hardware security verification is a challenging and time-consuming task.", "citations": ["Collini2025Marvel"], "pointer": ""}, {"excerpt": "MARVEL mimics the cognitive process of a designer looking for security vulnerabilities in RTL code.", "citations": ["Collini2025Marvel"], "pointer": ""}, {"excerpt": "It consists of a supervisor agent that devises the security policy of the system-on-chips (SoCs) using its security documentation.", "citations": ["Collini2025Marvel"], "pointer": ""}, {"excerpt": "It delegates tasks to validate the security policy to individual executor agents.", "citations": ["Collini2025Marvel"], "pointer": ""}, {"excerpt": "Each executor agent may use one or more tools to identify potential security bugs in the design and send the results back to the supervisor agent for further analysis and confirmation.", "citations": ["Collini2025Marvel"], "pointer": ""}, {"excerpt": "We find that 20 of the 48 issues reported by MARVEL pose security vulnerabilities.", "citations": ["Collini2025Marvel"], "pointer": ""}], "must_use": {"min_anchor_facts": 4, "min_comparison_cards": 4, "min_limitation_hooks": 2, "require_cited_numeric_if_available": true, "require_multi_cite_synthesis_paragraph": true, "thesis_required": true}, "do_not_repeat_phrases": ["this run", "this run is", "pipeline", "workspace", "unit", "quality gate", "evidence pack", "evidence packs", "writer context pack", "Method note (evidence policy):", "Methodology note:", "Methodology note (evidence policy):", "Methodology note (evidence-policy):", "Method note:", "This subsection surveys", "This subsection argues", "In this subsection", "This section surveys", "This section reviews", "This section discusses", "This section covers", "This section presents", "This section introduces", "provides an overview", "This survey provides an overview", "In this section, we provide an overview", "This section provides an overview", "Next, we move from", "We now turn to", "A few representative references include", "Notable lines of work include", "Concrete examples include", "Work in this area includes", "Recent systems include", "Examples include", "Representative systems include", "Therefore, survey synthesis should", "Therefore, survey comparisons should", "As a result, survey comparisons should", "survey synthesis should", "survey comparisons should", "Taken together,", "The key point is that", "Three limitations", "Two limitations", "claims remain provisional under abstract-only evidence", "abstract-only evidence", "Key takeaway:", "Main takeaway:", "protocol token", "protocol tokens", "constraint token", "constraint tokens", "metric token", "metric tokens", "evaluation token", "evaluation tokens", "In this survey", "In our survey", "Our survey", "This survey"], "pack_warnings": [], "pack_stats": {"anchors": {"raw": 12, "considered": 10, "kept": 10, "dropped_no_cites": 0}, "comparisons": {"raw": 10, "considered": 7, "kept": 7, "dropped_no_highlights": 0, "highlights_dropped_no_cites": 0}, "evaluation_protocol": {"raw": 6, "considered": 6, "kept": 6, "dropped_no_cites": 0}, "limitation_hooks": {"raw": 8, "considered": 8, "kept": 8, "dropped_no_cites": 0}, "trim_policy": {"default": 400, "anchor_fact": 420, "highlight_excerpt": 280, "comparison_write_prompt": 420, "eval_bullet": 320, "limitation_excerpt": 320}}, "generated_at": "2026-01-25T17:09:25"}
{"sub_id": "6.1", "title": "Benchmarks and evaluation protocols", "section_id": "6", "section_title": "Evaluation & Risks", "rq": "Which design choices in Benchmarks and evaluation protocols drive the major trade-offs, and how are those trade-offs measured?", "thesis": "Benchmarks and evaluation protocols methods emphasize evaluation protocol (datasets, metrics, human evaluation) and compute and latency constraints trade-offs, but synthesis is clearest when claims are tied to explicit evaluation settings and reporting conventions.", "tension_statement": "In Benchmarks and evaluation protocols, a recurring tension is coverage versus comparability: broader suites capture more behaviors but make head-to-head comparison fragile when protocols and constraints differ.", "evaluation_anchor_minimal": {"task": "attack/defense evaluation", "metric": "attack success rate", "constraint": "policy/sandbox setting"}, "paper_voice_palette": {"forbidden_pipeline_voice": ["this run", "this run is", "pipeline", "workspace", "unit", "quality gate", "evidence pack", "evidence packs", "writer context pack", "Method note (evidence policy):", "Methodology note:", "Methodology note (evidence policy):", "Methodology note (evidence-policy):", "Method note:"], "high_risk_templates": ["This subsection surveys", "This subsection argues", "In this subsection", "This section surveys", "This section reviews", "This section discusses", "This section covers", "This section presents", "This section introduces", "provides an overview", "This survey provides an overview", "In this section, we provide an overview", "This section provides an overview", "Next, we move from", "We now turn to", "A few representative references include", "Notable lines of work include", "Concrete examples include", "Work in this area includes", "Recent systems include", "Examples include", "Representative systems include", "Therefore, survey synthesis should", "Therefore, survey comparisons should", "As a result, survey comparisons should", "survey synthesis should", "survey comparisons should", "Taken together,", "The key point is that", "Three limitations", "Two limitations", "claims remain provisional under abstract-only evidence", "abstract-only evidence", "Key takeaway:", "Main takeaway:", "protocol token", "protocol tokens", "constraint token", "constraint tokens", "metric token", "metric tokens", "evaluation token", "evaluation tokens", "In this survey", "In our survey", "Our survey", "This survey"], "opener_archetypes": {"tension-first": ["A key tension is", "The central trade-off is", "A recurring constraint is"], "decision-first": ["For system builders, the crux is", "A practical decision is", "One design choice is"], "lens-first": ["Seen through the lens of", "From the perspective of", "Under an interface contract,"], "contrast-first": ["A useful contrast is between", "One sharp contrast is between", "A recurring split is between"], "protocol-first": ["Comparisons hinge on", "Results are only comparable when", "Evaluation claims depend on"]}, "synthesis_stems": ["Across these studies,", "Collectively,", "In summary,", "The evidence suggests that", "A consistent theme is that"], "rewrite_rules": [{"avoid_stem": "This subsection surveys", "prefer_stem": "A key tension is"}, {"avoid_stem": "In this subsection", "prefer_stem": "We focus on"}, {"avoid_stem": "provides an overview", "prefer_stem": "We focus on"}, {"avoid_stem": "Next, we move", "prefer_stem": "Having established"}, {"avoid_stem": "We now turn", "prefer_stem": "We then examine"}, {"avoid_stem": "survey comparisons should", "prefer_stem": "Across protocols, we observe"}, {"avoid_stem": "Two limitations", "prefer_stem": "A caveat is that"}, {"avoid_stem": "Three limitations", "prefer_stem": "One limitation is that"}, {"avoid_stem": "In this survey", "prefer_stem": "We examine"}, {"avoid_stem": "In our survey", "prefer_stem": "We examine"}, {"avoid_stem": "Our survey", "prefer_stem": "This work"}, {"avoid_stem": "This survey", "prefer_stem": "This work"}], "discourse_stem_watchlist": ["Additionally,", "Moreover,", "Furthermore,", "This suggests", "Taken together,", "The key point is that", "Overall,", "In summary,", "In practice,", "More broadly,", "Importantly,", "Notably,", "Crucially,", "In general,", "At a high level,", "In other words,", "Therefore,", "In addition,", "As a result,"], "discourse_stem_rewrites": {"Additionally,": ["More importantly,", "At the same time,", "A second consideration is"], "This suggests": ["This pattern indicates", "These results point to", "A plausible explanation is"], "Taken together,": ["Across these studies,", "Collectively,", "The evidence suggests"], "The key point is that": ["A practical implication is that", "A useful way to read these results is that", "One takeaway is that"], "Overall,": ["In practice,", "More broadly,", "A practical implication is that", "One way to read this evidence is that"], "In summary,": ["Across these studies,", "Collectively,", "A consistent theme is that", "The evidence suggests that"], "Importantly,": ["More importantly,", "A key constraint is that", "A practical implication is that"], "Notably,": ["In particular,", "A concrete example is that", "One implication is that"], "Crucially,": ["More importantly,", "A central constraint is that", "A key takeaway is that"], "At a high level,": ["Concretely,", "Under this protocol,", "In practice,"], "In other words,": ["Put differently,", "Equivalently,", "More concretely,"], "Therefore,": ["As a result,", "This in turn means that", "A practical implication is that"], "In addition,": ["At the same time,", "A second consideration is", "More importantly,"], "As a result,": ["This in turn means that", "Consequently,", "A practical implication is that"]}, "role_cards": {"section_author": {"mission": "Write one subsection as an argument (not a topic list), using in-scope citations as evidence.", "do": ["State a concrete tension/trade-off early and commit to a thesis.", "Make at least two explicit A-vs-B contrasts (mechanism -> outcome) instead of per-paper summaries.", "When citing numbers, add minimal context (task + metric + constraint) in the same paragraph.", "End with a limitation that changes interpretation (protocol mismatch, unclear threat model, missing ablations)."], "avoid": ["Outline narration (This subsection..., In this subsection...).", "Slide navigation (Next, we move..., We now turn...).", "Meta survey advice (survey comparisons should...).", "Cite dumps (a trailing [@a; @b; @c] without a claim)."]}, "evidence_steward": {"mission": "Keep claims auditable: citations support the sentence that needs them, and scope stays local.", "do": ["Embed citations inside claim sentences; name concrete nouns (system/benchmark/protocol) near each cite.", "Prefer subsection-scoped citations; use chapter/global only when truly cross-cutting.", "Downgrade overconfident claims when evidence is thin; convert unknowns into verification targets (once, not spam)."], "avoid": ["Out-of-scope citations to make a paragraph sound stronger.", "Repeating evidence-policy disclaimers in every subsection.", "Ambiguous model naming (e.g., GPT-5) unless the cited work uses it."]}, "style_harmonizer": {"mission": "Make the prose read like a paper: calm, specific, and varied in rhythm without sounding templated.", "do": ["Vary opener cadence across sections (tension-first / decision-first / lens-first) without reusing the same stem.", "Prefer argument bridges over navigation; keep signposting light and content-bearing.", "Reduce slash-enumerations (A/B/C axis labels) by rewriting into natural prose."], "avoid": ["Count-based openers used as a slot (e.g., Two limitations..., Three takeaways...).", "Repeated discourse stems (Additionally, This suggests, Taken together) across many paragraphs.", "Pipeline words (workspace/unit/quality gate/evidence pack) in reader-facing text.", "PPT speaker-note tone (Now we..., The remainder of...).", "Internal shorthand that reads like planning notes (e.g., protocol/metric/constraint tokens)."]}}, "version": "0.4"}, "opener_mode": "decision-first", "opener_hint": "Start with a builder/research decision; state what it depends on; end paragraph 1 with the thesis.", "axes": ["evaluation protocol (datasets, metrics, human evaluation)", "compute and latency constraints", "tool interface contract (schemas / protocols)", "tool selection / routing policy", "sandboxing / permissions / observability"], "bridge_terms": ["function calling", "tool schema", "routing", "sandbox", "observability", "benchmarks"], "contrast_hook": "tool interfaces", "required_evidence_fields": ["benchmarks/datasets", "metrics / human-eval protocol", "compute / cost (train/infer)", "training signal / supervision", "threat model", "defense surface"], "paragraph_plan": [{"para": 1, "argument_role": "setup_thesis", "intent": "Define scope, setup, and the subsection thesis (no pipeline jargon).", "focus": ["scope boundary", "key definitions", "thesis vs neighboring subsections"], "connector_to_prev": "", "connector_phrase": "", "use_clusters": ["Agent frameworks / architectures"], "rq": "Which design choices in Benchmarks and evaluation protocols drive the major trade-offs, and how are those trade-offs measured?"}, {"para": 2, "argument_role": "mechanism_cluster_A", "intent": "Explain cluster A: core mechanism and system architecture and what decision it makes in the agent loop.", "focus": ["cluster: Agent frameworks / architectures", "core mechanism and system architecture", "assumptions"], "connector_to_prev": "grounding", "connector_phrase": "baseline route (Agent frameworks / architectures)", "use_clusters": ["Agent frameworks / architectures"]}, {"para": 3, "argument_role": "implementation_cluster_A", "intent": "Cluster A implementation details: training and data signals and interface contract (tools/memory) that constrain behavior.", "focus": ["cluster: Agent frameworks / architectures", "training and data setup", "interface contract", "axes: evaluation protocol (datasets, metrics, human evaluation), compute and latency constraints, tool interface contract (schemas / protocols), tool selection / routing policy, sandboxing / permissions / observability"], "connector_to_prev": "elaboration", "connector_phrase": "implementation assumptions (interface + training)", "use_clusters": ["Agent frameworks / architectures"]}, {"para": 4, "argument_role": "evaluation_cluster_A", "intent": "Cluster A evaluation/trade-offs: where it works, costs (compute/latency), and typical failure modes.", "focus": ["cluster: Agent frameworks / architectures", "evaluation anchor", "efficiency", "failure modes"], "connector_to_prev": "evaluation", "connector_phrase": "evaluation anchor (task/metric/constraint) + failure modes", "use_clusters": ["Agent frameworks / architectures"]}, {"para": 5, "argument_role": "contrast_cluster_B", "intent": "Explain cluster B (contrast with A): core mechanism and system architecture and what it optimizes for.", "focus": ["cluster: Evaluation / benchmark-focused works", "contrast with Agent frameworks / architectures", "core mechanism and system architecture"], "connector_to_prev": "contrast", "connector_phrase": "contrast route (Evaluation / benchmark-focused works vs Agent frameworks / architectures)", "use_clusters": ["Evaluation / benchmark-focused works"]}, {"para": 6, "argument_role": "implementation_cluster_B", "intent": "Cluster B implementation details: training and data and interface assumptions (mirror A for comparability).", "focus": ["cluster: Evaluation / benchmark-focused works", "training and data setup", "interface contract", "axes: evaluation protocol (datasets, metrics, human evaluation), compute and latency constraints, tool interface contract (schemas / protocols), tool selection / routing policy, sandboxing / permissions / observability"], "connector_to_prev": "elaboration", "connector_phrase": "contrast implementation assumptions (B)", "use_clusters": ["Evaluation / benchmark-focused works"]}, {"para": 7, "argument_role": "evaluation_cluster_B", "intent": "Cluster B evaluation/trade-offs: where it works, costs, and failure modes (mirror A).", "focus": ["cluster: Evaluation / benchmark-focused works", "evaluation anchor", "efficiency", "failure modes"], "connector_to_prev": "evaluation", "connector_phrase": "contrast evaluation anchor + trade-offs (B)", "use_clusters": ["Evaluation / benchmark-focused works"]}, {"para": 8, "argument_role": "cross_paper_synthesis", "intent": "Cross-paper synthesis: compare clusters along the main axes (include >=2 citations in one paragraph).", "focus": ["compare Agent frameworks / architectures vs Evaluation / benchmark-focused works", "multiple citations in one paragraph", "axes: evaluation protocol (datasets, metrics, human evaluation), compute and latency constraints, tool interface contract (schemas / protocols), tool selection / routing policy, sandboxing / permissions / observability"], "connector_to_prev": "synthesis", "connector_phrase": "cross-paper synthesis (Agent frameworks / architectures vs Evaluation / benchmark-focused works)", "use_clusters": ["Agent frameworks / architectures", "Evaluation / benchmark-focused works", "Multi-agent coordination"]}, {"para": 9, "argument_role": "decision_guidance", "intent": "Decision guidance: when to choose which route (criteria + evaluation signals + engineering constraints).", "focus": ["decision checklist", "evaluation protocol", "practical constraints"], "connector_to_prev": "consequence", "connector_phrase": "decision guidance / criteria", "use_clusters": ["Agent frameworks / architectures", "Evaluation / benchmark-focused works", "Multi-agent coordination"]}, {"para": 10, "argument_role": "limitations_open_questions", "intent": "Limitations + verification targets; end with a concrete open question to hand off.", "focus": ["limitations", "evidence mode: provisional", "what needs verification", "open question"], "connector_to_prev": "limitations", "connector_phrase": "limitations + verification targets", "use_clusters": ["Agent frameworks / architectures", "Evaluation / benchmark-focused works", "Multi-agent coordination"], "policy": "Use conservative language; avoid strong conclusions; prefer questions-to-answer + explicit evidence gaps list."}], "chapter_throughline": ["Pin scope to goal: agent survey latex.", "Compare approaches along: evaluation protocol (datasets, metrics, human evaluation).", "Compare approaches along: compute and latency constraints.", "Compare approaches along: tool interface contract (schemas / protocols).", "Compare approaches along: tool selection / routing policy.", "Compare approaches along: sandboxing / permissions / observability."], "chapter_key_contrasts": ["tool interfaces", "security"], "chapter_synthesis_mode": "tradeoff_matrix", "allowed_bibkeys_selected": ["Lin2023Agentsims", "Kim2026Beyond", "Seo2025Simuhome", "Shi2025Progent", "Shang2024Agentsquare", "Huang2024Survey", "Zhang2026Evoroute", "Ji2025Taxonomy", "Zhu2025Where", "Van2025Survey", "Chen2025Towards", "Zhang2025Datascibench", "Yao2022React", "Wang2025Autoscore", "Shen2025Feat", "Rahman2025Hallucination", "Dong2025Compressed", "Zhang2024Large", "Zhang2025Generalizability", "Qi2024Large", "Tang2025Empowering", "Chang2023Survey", "Zhu2025Evolutionary"], "allowed_bibkeys_mapped": ["Chang2023Survey", "Wu2025Lessons", "Huang2024Survey", "Qi2024Large", "Kim2026Beyond", "Rahman2025Hallucination", "Chen2025Towards", "Lin2023Agentsims", "Zhang2026Evoroute", "Tang2025Empowering", "Dong2025Compressed", "Shi2025Progent", "Wu2025Meta", "Seo2025Simuhome", "Shang2024Agentsquare", "Zhang2025Generalizability", "Zhu2025Where", "Van2025Survey", "Wang2025Autoscore", "Zhang2025Datascibench", "Zhu2025Evolutionary", "Shen2025Feat", "Ji2025Taxonomy", "Trirat2024Automl", "Zhang2024Large", "Li2023Modelscope", "Hu2023Avis", "Yao2022React"], "allowed_bibkeys_chapter": ["Agrawal2025Language", "Bonagiri2025Check", "Chang2023Survey", "Chen2025Towards", "Dong2025Compressed", "Gao2025Radar", "Gasmi2025Bridging", "Hadeliya2025When", "Han2025Large", "He2024Emerged", "Henke2025Autopentest", "Hu2023Avis", "Huang2024Survey", "Ji2025Taxonomy", "Kang2025Acon", "Kim2026Beyond", "Li2023Modelscope", "Li2024Personal", "Lin2023Agentsims", "Liu2025Secure", "Liu2026Agents", "Luo2025Universe", "Marandi2025Complex", "Mo2025Attractive", "Plaat2025Agentic", "Qi2024Large", "Rahman2025Hallucination", "Rosario2025Architecting", "Seo2025Simuhome", "Shang2024Agentsquare", "Shen2025Feat", "Shi2025Progent", "Sun2024Survey", "Tang2025Empowering", "Trirat2024Automl", "Van2025Survey", "VarangotReille2025Doing", "Wang2025Agentvigil", "Wang2025Autoscore", "Wang2025Comprehensive", "Weng2025Bridgescope", "Wu2025Lessons", "Wu2025Meta", "Yao2022React", "Zhang2024Large", "Zhang2025Datascibench", "Zhang2025Generalizability", "Zhang2025Security", "Zhang2026Evoroute", "Zhou2025Reasoning", "Zhu2025Evolutionary", "Zhu2025Where"], "allowed_bibkeys_global": ["Li2023Modelscope", "Yao2022React"], "evidence_ids": ["E-P0113-152d1ce5af", "E-P0122-ef4cf62416", "E-P0216-e2e3b7fa97", "E-P0086-68db58914f", "E-P0037-38a26e4777", "E-P0244-0bcc4031a6", "E-P0124-60cc0d458f", "E-P0224-5607dc887c", "E-P0033-46914a4804", "E-P0133-a68f39bc04", "E-P0231-4fc221fdea", "E-P0162-39281e5083", "E-P0001-ca4a00b5cf", "E-P0149-42512d3cac", "E-P0177-acf4a27e30", "E-P0181-29c690fb6c", "E-P0028-08c43ee74b", "E-P0274-52fea1d199", "E-P0010-793979aed4", "E-P0275-389f65b30d", "E-P0006-3f9a43a03b", "E-P0112-8f668d83bf", "E-P0149-d956dcaf4c", "E-P0174-96f876973b"], "anchor_facts": [{"hook_type": "quant", "text": "Experimental results demonstrate that API-based models outperform open-sourced models on all metrics and Deepseek-Coder-33B-Instruct achieves the highest score among open-sourced models.", "citations": ["Zhang2025Datascibench"], "paper_id": "P0162", "evidence_id": "E-P0162-39281e5083", "pointer": "papers/paper_notes.jsonl:paper_id=P0162#key_results[1]"}, {"hook_type": "quant", "text": "We evaluate AutoSCORE on four benchmark datasets from the ASAP benchmark, using both proprietary and open-source LLMs (GPT-4o, LLaMA-3.1-8B, and LLaMA-3.1-70B).", "citations": ["Wang2025Autoscore"], "paper_id": "P0149", "evidence_id": "E-P0149-42512d3cac", "pointer": "papers/paper_notes.jsonl:paper_id=P0149#key_results[0]"}, {"hook_type": "quant", "text": "Experiments on challenging agentic benchmarks such as GAIA and BrowseComp+ demonstrate that EvoRoute, when integrated into off-the-shelf agentic systems, not only sustains or enhances system performance but also reduces execution cost by up to $80\\%$ and latency by over $70\\%$.", "citations": ["Zhang2026Evoroute"], "paper_id": "P0124", "evidence_id": "E-P0124-60cc0d458f", "pointer": "papers/paper_notes.jsonl:paper_id=P0124#key_results[0]"}, {"hook_type": "quant", "text": "Our extensive evaluation across various agent use cases, using benchmarks like AgentDojo, ASB, and AgentPoison, demonstrates that Progent reduces attack success rates to 0%, while preserving agent utility and speed.", "citations": ["Shi2025Progent"], "paper_id": "P0086", "evidence_id": "E-P0086-68db58914f", "pointer": "papers/paper_notes.jsonl:paper_id=P0086#key_results[0]"}, {"hook_type": "quant", "text": "ACBench spans (1) 12 tasks across 4 capabilities (e.g., WorfBench for workflow generation, Needle-in-Haystack for long-context retrieval), (2) quantization (GPTQ, AWQ) and pruning (Wanda, SparseGPT), and (3) 15 models, including small (Gemma-2B), standard (Qwen2.5 7B-32B), and", "citations": ["Dong2025Compressed"], "paper_id": "P0028", "evidence_id": "E-P0028-08c43ee74b", "pointer": "papers/paper_notes.jsonl:paper_id=P0028#key_results[1]"}, {"hook_type": "quant", "text": "Existing evaluation methods suffer from following shortcomings: (1) constrained evaluation abilities, (2) vulnerable benchmarks, (3) unobjective metrics.", "citations": ["Lin2023Agentsims"], "paper_id": "P0113", "evidence_id": "E-P0113-152d1ce5af", "pointer": "papers/paper_notes.jsonl:paper_id=P0113#key_results[0]"}, {"hook_type": "eval", "text": "We introduce WildAGTEval, a benchmark designed to evaluate large language model (LLM) agents' function-calling capabilities under realistic API complexity.", "citations": ["Kim2026Beyond"], "paper_id": "P0122", "evidence_id": "E-P0122-ef4cf62416", "pointer": "papers/paper_notes.jsonl:paper_id=P0122#method"}, {"hook_type": "quant", "text": "Our evaluation of 16 agents under a unified ReAct framework reveals distinct capabilities and limitations across models.", "citations": ["Seo2025Simuhome"], "paper_id": "P0216", "evidence_id": "E-P0216-e2e3b7fa97", "pointer": "papers/paper_notes.jsonl:paper_id=P0216#limitations[1]"}, {"hook_type": "quant", "text": "Extensive experiments across six benchmarks, covering the diverse scenarios of web, embodied, tool use and game applications, show that AgentSquare substantially outperforms hand-crafted agents, achieving an average performance gain of 17.2% against best-known human designs.", "citations": ["Shang2024Agentsquare"], "paper_id": "P0037", "evidence_id": "E-P0037-38a26e4777", "pointer": "papers/paper_notes.jsonl:paper_id=P0037#key_results[0]"}, {"hook_type": "quant", "text": "This paper presents a systematic and comprehensive review of MLLM evaluation methods, covering the following key aspects: (1) the background of MLLMs and their evaluation; (2) \"what to evaluate\" that reviews and categorizes existing MLLM evaluation tasks based on the capabilities", "citations": ["Huang2024Survey"], "paper_id": "P0244", "evidence_id": "E-P0244-0bcc4031a6", "pointer": "papers/paper_notes.jsonl:paper_id=P0244#key_results[0]"}], "comparison_cards": [{"axis": "evaluation protocol (datasets, metrics, human evaluation)", "A_label": "Agent frameworks / architectures", "B_label": "Evaluation / benchmark-focused works", "citations": ["Kim2026Beyond", "Van2025Survey", "Zhang2025Datascibench"], "A_highlights": [{"paper_id": "P0122", "evidence_id": "E-P0122-ef4cf62416", "excerpt": "We introduce WildAGTEval, a benchmark designed to evaluate large language model (LLM) agents' function-calling capabilities under realistic API complexity.", "citations": ["Kim2026Beyond"], "pointer": "papers/paper_notes.jsonl:paper_id=P0122#method"}, {"paper_id": "P0133", "evidence_id": "E-P0133-a68f39bc04", "excerpt": "This survey provides a comprehensive overview of foundation models, agentic systems, datasets, and computational tools supporting this growing field.", "citations": ["Van2025Survey"], "pointer": "papers/paper_notes.jsonl:paper_id=P0133#key_results[0]"}], "B_highlights": [{"paper_id": "P0122", "evidence_id": "E-P0122-ef4cf62416", "excerpt": "We introduce WildAGTEval, a benchmark designed to evaluate large language model (LLM) agents' function-calling capabilities under realistic API complexity.", "citations": ["Kim2026Beyond"], "pointer": "papers/paper_notes.jsonl:paper_id=P0122#method"}, {"paper_id": "P0162", "evidence_id": "E-P0162-39281e5083", "excerpt": "Experimental results demonstrate that API-based models outperform open-sourced models on all metrics and Deepseek-Coder-33B-Instruct achieves the highest score among open-sourced models.", "citations": ["Zhang2025Datascibench"], "pointer": "papers/paper_notes.jsonl:paper_id=P0162#key_results[1]"}], "write_prompt": "Contrast Agent frameworks / architectures vs Evaluation / benchmark-focused works along \"evaluation protocol (datasets, metrics, human evaluation)\". Ground A and B using the highlight snippets; do not introduce new claims beyond the cited evidence."}, {"axis": "evaluation protocol (datasets, metrics, human evaluation)", "A_label": "Agent frameworks / architectures", "B_label": "Multi-agent coordination", "citations": ["Kim2026Beyond", "Van2025Survey", "Wang2025Autoscore", "Shen2025Feat"], "A_highlights": [{"paper_id": "P0122", "evidence_id": "E-P0122-ef4cf62416", "excerpt": "We introduce WildAGTEval, a benchmark designed to evaluate large language model (LLM) agents' function-calling capabilities under realistic API complexity.", "citations": ["Kim2026Beyond"], "pointer": "papers/paper_notes.jsonl:paper_id=P0122#method"}, {"paper_id": "P0133", "evidence_id": "E-P0133-a68f39bc04", "excerpt": "This survey provides a comprehensive overview of foundation models, agentic systems, datasets, and computational tools supporting this growing field.", "citations": ["Van2025Survey"], "pointer": "papers/paper_notes.jsonl:paper_id=P0133#key_results[0]"}], "B_highlights": [{"paper_id": "P0149", "evidence_id": "E-P0149-42512d3cac", "excerpt": "We evaluate AutoSCORE on four benchmark datasets from the ASAP benchmark, using both proprietary and open-source LLMs (GPT-4o, LLaMA-3.1-8B, and LLaMA-3.1-70B).", "citations": ["Wang2025Autoscore"], "pointer": "papers/paper_notes.jsonl:paper_id=P0149#key_results[0]"}, {"paper_id": "P0177", "evidence_id": "E-P0177-acf4a27e30", "excerpt": "The system employs tool-augmented reasoning, hierarchical retrieval-augmented generation, forensic-tuned LLMs, and human-in-the-loop feedback to ensure legal and medical validity.", "citations": ["Shen2025Feat"], "pointer": "papers/paper_notes.jsonl:paper_id=P0177#key_results[0]"}], "write_prompt": "Contrast Agent frameworks / architectures vs Multi-agent coordination along \"evaluation protocol (datasets, metrics, human evaluation)\". Ground A and B using the highlight snippets; do not introduce new claims beyond the cited evidence."}, {"axis": "evaluation protocol (datasets, metrics, human evaluation)", "A_label": "Evaluation / benchmark-focused works", "B_label": "Multi-agent coordination", "citations": ["Kim2026Beyond", "Zhang2025Datascibench", "Wang2025Autoscore", "Shen2025Feat"], "A_highlights": [{"paper_id": "P0122", "evidence_id": "E-P0122-ef4cf62416", "excerpt": "We introduce WildAGTEval, a benchmark designed to evaluate large language model (LLM) agents' function-calling capabilities under realistic API complexity.", "citations": ["Kim2026Beyond"], "pointer": "papers/paper_notes.jsonl:paper_id=P0122#method"}, {"paper_id": "P0162", "evidence_id": "E-P0162-39281e5083", "excerpt": "Experimental results demonstrate that API-based models outperform open-sourced models on all metrics and Deepseek-Coder-33B-Instruct achieves the highest score among open-sourced models.", "citations": ["Zhang2025Datascibench"], "pointer": "papers/paper_notes.jsonl:paper_id=P0162#key_results[1]"}], "B_highlights": [{"paper_id": "P0149", "evidence_id": "E-P0149-42512d3cac", "excerpt": "We evaluate AutoSCORE on four benchmark datasets from the ASAP benchmark, using both proprietary and open-source LLMs (GPT-4o, LLaMA-3.1-8B, and LLaMA-3.1-70B).", "citations": ["Wang2025Autoscore"], "pointer": "papers/paper_notes.jsonl:paper_id=P0149#key_results[0]"}, {"paper_id": "P0177", "evidence_id": "E-P0177-acf4a27e30", "excerpt": "The system employs tool-augmented reasoning, hierarchical retrieval-augmented generation, forensic-tuned LLMs, and human-in-the-loop feedback to ensure legal and medical validity.", "citations": ["Shen2025Feat"], "pointer": "papers/paper_notes.jsonl:paper_id=P0177#key_results[0]"}], "write_prompt": "Contrast Evaluation / benchmark-focused works vs Multi-agent coordination along \"evaluation protocol (datasets, metrics, human evaluation)\". Ground A and B using the highlight snippets; do not introduce new claims beyond the cited evidence."}, {"axis": "compute and latency constraints", "A_label": "Agent frameworks / architectures", "B_label": "Evaluation / benchmark-focused works", "citations": ["Zhang2026Evoroute", "Shi2025Progent", "Dong2025Compressed", "Zhang2025Datascibench"], "A_highlights": [{"paper_id": "P0124", "evidence_id": "E-P0124-60cc0d458f", "excerpt": "Experiments on challenging agentic benchmarks such as GAIA and BrowseComp+ demonstrate that EvoRoute, when integrated into off-the-shelf agentic systems, not only sustains or enhances system performance but also reduces execution cost by up to $80\\%$ and latency by over $70\\%$.", "citations": ["Zhang2026Evoroute"], "pointer": "papers/paper_notes.jsonl:paper_id=P0124#key_results[0]"}, {"paper_id": "P0086", "evidence_id": "E-P0086-68db58914f", "excerpt": "Our extensive evaluation across various agent use cases, using benchmarks like AgentDojo, ASB, and AgentPoison, demonstrates that Progent reduces attack success rates to 0%, while preserving agent utility and speed.", "citations": ["Shi2025Progent"], "pointer": "papers/paper_notes.jsonl:paper_id=P0086#key_results[0]"}], "B_highlights": [{"paper_id": "P0028", "evidence_id": "E-P0028-08c43ee74b", "excerpt": "ACBench spans (1) 12 tasks across 4 capabilities (e.g., WorfBench for workflow generation, Needle-in-Haystack for long-context retrieval), (2) quantization (GPTQ, AWQ) and pruning (Wanda, SparseGPT), and (3) 15 models, including small (Gemma-2B), standard (Qwen2.5 7B-32B), and", "citations": ["Dong2025Compressed"], "pointer": "papers/paper_notes.jsonl:paper_id=P0028#key_results[1]"}, {"paper_id": "P0162", "evidence_id": "E-P0162-39281e5083", "excerpt": "Experimental results demonstrate that API-based models outperform open-sourced models on all metrics and Deepseek-Coder-33B-Instruct achieves the highest score among open-sourced models.", "citations": ["Zhang2025Datascibench"], "pointer": "papers/paper_notes.jsonl:paper_id=P0162#key_results[1]"}], "write_prompt": "Contrast Agent frameworks / architectures vs Evaluation / benchmark-focused works along \"compute and latency constraints\". Ground A and B using the highlight snippets; do not introduce new claims beyond the cited evidence."}, {"axis": "compute and latency constraints", "A_label": "Agent frameworks / architectures", "B_label": "Multi-agent coordination", "citations": ["Zhang2026Evoroute", "Shi2025Progent", "Shen2025Feat", "Wang2025Autoscore"], "A_highlights": [{"paper_id": "P0124", "evidence_id": "E-P0124-60cc0d458f", "excerpt": "Experiments on challenging agentic benchmarks such as GAIA and BrowseComp+ demonstrate that EvoRoute, when integrated into off-the-shelf agentic systems, not only sustains or enhances system performance but also reduces execution cost by up to $80\\%$ and latency by over $70\\%$.", "citations": ["Zhang2026Evoroute"], "pointer": "papers/paper_notes.jsonl:paper_id=P0124#key_results[0]"}, {"paper_id": "P0086", "evidence_id": "E-P0086-68db58914f", "excerpt": "Our extensive evaluation across various agent use cases, using benchmarks like AgentDojo, ASB, and AgentPoison, demonstrates that Progent reduces attack success rates to 0%, while preserving agent utility and speed.", "citations": ["Shi2025Progent"], "pointer": "papers/paper_notes.jsonl:paper_id=P0086#key_results[0]"}], "B_highlights": [{"paper_id": "P0177", "evidence_id": "E-P0177-acf4a27e30", "excerpt": "The system employs tool-augmented reasoning, hierarchical retrieval-augmented generation, forensic-tuned LLMs, and human-in-the-loop feedback to ensure legal and medical validity.", "citations": ["Shen2025Feat"], "pointer": "papers/paper_notes.jsonl:paper_id=P0177#key_results[0]"}, {"paper_id": "P0149", "evidence_id": "E-P0149-42512d3cac", "excerpt": "We evaluate AutoSCORE on four benchmark datasets from the ASAP benchmark, using both proprietary and open-source LLMs (GPT-4o, LLaMA-3.1-8B, and LLaMA-3.1-70B).", "citations": ["Wang2025Autoscore"], "pointer": "papers/paper_notes.jsonl:paper_id=P0149#key_results[0]"}], "write_prompt": "Contrast Agent frameworks / architectures vs Multi-agent coordination along \"compute and latency constraints\". Ground A and B using the highlight snippets; do not introduce new claims beyond the cited evidence."}, {"axis": "compute and latency constraints", "A_label": "Evaluation / benchmark-focused works", "B_label": "Multi-agent coordination", "citations": ["Dong2025Compressed", "Zhang2025Datascibench", "Shen2025Feat", "Wang2025Autoscore"], "A_highlights": [{"paper_id": "P0028", "evidence_id": "E-P0028-08c43ee74b", "excerpt": "ACBench spans (1) 12 tasks across 4 capabilities (e.g., WorfBench for workflow generation, Needle-in-Haystack for long-context retrieval), (2) quantization (GPTQ, AWQ) and pruning (Wanda, SparseGPT), and (3) 15 models, including small (Gemma-2B), standard (Qwen2.5 7B-32B), and", "citations": ["Dong2025Compressed"], "pointer": "papers/paper_notes.jsonl:paper_id=P0028#key_results[1]"}, {"paper_id": "P0162", "evidence_id": "E-P0162-39281e5083", "excerpt": "Experimental results demonstrate that API-based models outperform open-sourced models on all metrics and Deepseek-Coder-33B-Instruct achieves the highest score among open-sourced models.", "citations": ["Zhang2025Datascibench"], "pointer": "papers/paper_notes.jsonl:paper_id=P0162#key_results[1]"}], "B_highlights": [{"paper_id": "P0177", "evidence_id": "E-P0177-acf4a27e30", "excerpt": "The system employs tool-augmented reasoning, hierarchical retrieval-augmented generation, forensic-tuned LLMs, and human-in-the-loop feedback to ensure legal and medical validity.", "citations": ["Shen2025Feat"], "pointer": "papers/paper_notes.jsonl:paper_id=P0177#key_results[0]"}, {"paper_id": "P0149", "evidence_id": "E-P0149-42512d3cac", "excerpt": "We evaluate AutoSCORE on four benchmark datasets from the ASAP benchmark, using both proprietary and open-source LLMs (GPT-4o, LLaMA-3.1-8B, and LLaMA-3.1-70B).", "citations": ["Wang2025Autoscore"], "pointer": "papers/paper_notes.jsonl:paper_id=P0149#key_results[0]"}], "write_prompt": "Contrast Evaluation / benchmark-focused works vs Multi-agent coordination along \"compute and latency constraints\". Ground A and B using the highlight snippets; do not introduce new claims beyond the cited evidence."}, {"axis": "tool interface contract (schemas / protocols)", "A_label": "Agent frameworks / architectures", "B_label": "Evaluation / benchmark-focused works", "citations": ["Kim2026Beyond", "Van2025Survey", "Zhang2025Datascibench"], "A_highlights": [{"paper_id": "P0122", "evidence_id": "E-P0122-ef4cf62416", "excerpt": "We introduce WildAGTEval, a benchmark designed to evaluate large language model (LLM) agents' function-calling capabilities under realistic API complexity.", "citations": ["Kim2026Beyond"], "pointer": "papers/paper_notes.jsonl:paper_id=P0122#method"}, {"paper_id": "P0133", "evidence_id": "E-P0133-a68f39bc04", "excerpt": "This survey provides a comprehensive overview of foundation models, agentic systems, datasets, and computational tools supporting this growing field.", "citations": ["Van2025Survey"], "pointer": "papers/paper_notes.jsonl:paper_id=P0133#key_results[0]"}], "B_highlights": [{"paper_id": "P0122", "evidence_id": "E-P0122-ef4cf62416", "excerpt": "We introduce WildAGTEval, a benchmark designed to evaluate large language model (LLM) agents' function-calling capabilities under realistic API complexity.", "citations": ["Kim2026Beyond"], "pointer": "papers/paper_notes.jsonl:paper_id=P0122#method"}, {"paper_id": "P0162", "evidence_id": "E-P0162-39281e5083", "excerpt": "Experimental results demonstrate that API-based models outperform open-sourced models on all metrics and Deepseek-Coder-33B-Instruct achieves the highest score among open-sourced models.", "citations": ["Zhang2025Datascibench"], "pointer": "papers/paper_notes.jsonl:paper_id=P0162#key_results[1]"}], "write_prompt": "Contrast Agent frameworks / architectures vs Evaluation / benchmark-focused works along \"tool interface contract (schemas / protocols)\". Ground A and B using the highlight snippets; do not introduce new claims beyond the cited evidence."}], "evaluation_protocol": [{"bullet": "Evaluation mentions include: API, LLMs, WildAGTEval, GAIA, EvoRoute, BrowseComp, LLM-based, GLUE, ACBench, GPTQ.", "citations": ["Kim2026Beyond", "Zhang2026Evoroute", "Tang2025Empowering", "Zhang2025Generalizability"]}, {"bullet": "When comparing results, anchor the paragraph with: task type + metric + constraint (budget, tool access, horizon, or threat model) when stated.", "citations": ["Kim2026Beyond", "Zhang2026Evoroute"]}, {"bullet": "Prefer head-to-head comparisons only when benchmark/metric are shared; otherwise frame differences as protocol-driven rather than method superiority.", "citations": ["Kim2026Beyond", "Zhang2026Evoroute"]}, {"bullet": "Avoid underspecified model/baseline naming; if abstracts omit details, state that the baseline is reported but underspecified instead of guessing.", "citations": ["Kim2026Beyond", "Zhang2026Evoroute"]}, {"bullet": "If a claim relies on a single reported number, pair it with a limitation/caveat from the same evidence so the draft remains conservative.", "citations": ["Kim2026Beyond", "Zhang2026Evoroute"]}, {"bullet": "If budgets or environments differ across papers, treat cross-paper numeric comparison as fragile and prefer qualitative contrasts aligned to the subsection axes.", "citations": ["Kim2026Beyond", "Zhang2026Evoroute"]}], "limitation_hooks": [{"excerpt": "We formalize this challenge as the \\textbf{Agent System Trilemma}: the inherent tension among achieving state-of-the-art performance, minimizing monetary cost, and ensuring rapid task completion.", "citations": ["Zhang2026Evoroute"], "pointer": ""}, {"excerpt": "However, how to translate the research on general agents into productivity that drives industry transformations remains a significant challenge.", "citations": ["Tang2025Empowering"], "pointer": ""}, {"excerpt": "We then review datasets, evaluation dimensions, and metrics, highlighting their limitations.", "citations": ["Zhang2025Generalizability"], "pointer": ""}, {"excerpt": "A critical challenge, however, lies in ensuring agent generalizability - the ability to maintain consistent performance across varied instructions, tasks, environments, and domains, especially those beyond agents' fine-tuning data.", "citations": ["Zhang2025Generalizability"], "pointer": ""}, {"excerpt": "Yet their sophisticated architectures amplify vulnerability to cascading failures, where a single root-cause error propagates through subsequent decisions, leading to task failure.", "citations": ["Zhu2025Where"], "pointer": ""}, {"excerpt": "First, we introduce the AgentErrorTaxonomy, a modular classification of failure modes spanning memory, reflection, planning, action, and system-level operations.", "citations": ["Zhu2025Where"], "pointer": ""}, {"excerpt": "Second, we construct AgentErrorBench, the first dataset of systematically annotated failure trajectories from ALFWorld, GAIA, and WebShop, grounding error analysis in real-world agent rollouts.", "citations": ["Zhu2025Where"], "pointer": ""}, {"excerpt": "Thanks to our modular design, integrating Progent does not alter agent internals and only requires minimal changes to the existing agent implementation, enhancing its practicality and potential for widespread adoption.", "citations": ["Shi2025Progent"], "pointer": ""}], "must_use": {"min_anchor_facts": 4, "min_comparison_cards": 4, "min_limitation_hooks": 2, "require_cited_numeric_if_available": true, "require_multi_cite_synthesis_paragraph": true, "thesis_required": true}, "do_not_repeat_phrases": ["this run", "this run is", "pipeline", "workspace", "unit", "quality gate", "evidence pack", "evidence packs", "writer context pack", "Method note (evidence policy):", "Methodology note:", "Methodology note (evidence policy):", "Methodology note (evidence-policy):", "Method note:", "This subsection surveys", "This subsection argues", "In this subsection", "This section surveys", "This section reviews", "This section discusses", "This section covers", "This section presents", "This section introduces", "provides an overview", "This survey provides an overview", "In this section, we provide an overview", "This section provides an overview", "Next, we move from", "We now turn to", "A few representative references include", "Notable lines of work include", "Concrete examples include", "Work in this area includes", "Recent systems include", "Examples include", "Representative systems include", "Therefore, survey synthesis should", "Therefore, survey comparisons should", "As a result, survey comparisons should", "survey synthesis should", "survey comparisons should", "Taken together,", "The key point is that", "Three limitations", "Two limitations", "claims remain provisional under abstract-only evidence", "abstract-only evidence", "Key takeaway:", "Main takeaway:", "protocol token", "protocol tokens", "constraint token", "constraint tokens", "metric token", "metric tokens", "evaluation token", "evaluation tokens", "In this survey", "In our survey", "Our survey", "This survey"], "pack_warnings": [], "pack_stats": {"anchors": {"raw": 12, "considered": 10, "kept": 10, "dropped_no_cites": 0}, "comparisons": {"raw": 10, "considered": 7, "kept": 7, "dropped_no_highlights": 0, "highlights_dropped_no_cites": 0}, "evaluation_protocol": {"raw": 6, "considered": 6, "kept": 6, "dropped_no_cites": 0}, "limitation_hooks": {"raw": 8, "considered": 8, "kept": 8, "dropped_no_cites": 0}, "trim_policy": {"default": 400, "anchor_fact": 420, "highlight_excerpt": 280, "comparison_write_prompt": 420, "eval_bullet": 320, "limitation_excerpt": 320}}, "generated_at": "2026-01-25T17:09:25"}
{"sub_id": "6.2", "title": "Safety, security, and governance", "section_id": "6", "section_title": "Evaluation & Risks", "rq": "Which design choices in Safety, security, and governance drive the major trade-offs, and how are those trade-offs measured?", "thesis": "Safety, security, and governance highlights a tension around evaluation protocol (datasets, metrics, human evaluation) and compute and latency constraints, motivating a protocol-aware synthesis rather than per-paper summaries.", "tension_statement": "In Safety, security, and governance, a key tension is capability versus safety: stronger agent actions increase utility but widen the attack surface and raise containment requirements.", "evaluation_anchor_minimal": {"task": "attack/defense evaluation", "metric": "attack success rate", "constraint": "policy/sandbox setting"}, "paper_voice_palette": {"forbidden_pipeline_voice": ["this run", "this run is", "pipeline", "workspace", "unit", "quality gate", "evidence pack", "evidence packs", "writer context pack", "Method note (evidence policy):", "Methodology note:", "Methodology note (evidence policy):", "Methodology note (evidence-policy):", "Method note:"], "high_risk_templates": ["This subsection surveys", "This subsection argues", "In this subsection", "This section surveys", "This section reviews", "This section discusses", "This section covers", "This section presents", "This section introduces", "provides an overview", "This survey provides an overview", "In this section, we provide an overview", "This section provides an overview", "Next, we move from", "We now turn to", "A few representative references include", "Notable lines of work include", "Concrete examples include", "Work in this area includes", "Recent systems include", "Examples include", "Representative systems include", "Therefore, survey synthesis should", "Therefore, survey comparisons should", "As a result, survey comparisons should", "survey synthesis should", "survey comparisons should", "Taken together,", "The key point is that", "Three limitations", "Two limitations", "claims remain provisional under abstract-only evidence", "abstract-only evidence", "Key takeaway:", "Main takeaway:", "protocol token", "protocol tokens", "constraint token", "constraint tokens", "metric token", "metric tokens", "evaluation token", "evaluation tokens", "In this survey", "In our survey", "Our survey", "This survey"], "opener_archetypes": {"tension-first": ["A key tension is", "The central trade-off is", "A recurring constraint is"], "decision-first": ["For system builders, the crux is", "A practical decision is", "One design choice is"], "lens-first": ["Seen through the lens of", "From the perspective of", "Under an interface contract,"], "contrast-first": ["A useful contrast is between", "One sharp contrast is between", "A recurring split is between"], "protocol-first": ["Comparisons hinge on", "Results are only comparable when", "Evaluation claims depend on"]}, "synthesis_stems": ["Across these studies,", "Collectively,", "In summary,", "The evidence suggests that", "A consistent theme is that"], "rewrite_rules": [{"avoid_stem": "This subsection surveys", "prefer_stem": "A key tension is"}, {"avoid_stem": "In this subsection", "prefer_stem": "We focus on"}, {"avoid_stem": "provides an overview", "prefer_stem": "We focus on"}, {"avoid_stem": "Next, we move", "prefer_stem": "Having established"}, {"avoid_stem": "We now turn", "prefer_stem": "We then examine"}, {"avoid_stem": "survey comparisons should", "prefer_stem": "Across protocols, we observe"}, {"avoid_stem": "Two limitations", "prefer_stem": "A caveat is that"}, {"avoid_stem": "Three limitations", "prefer_stem": "One limitation is that"}, {"avoid_stem": "In this survey", "prefer_stem": "We examine"}, {"avoid_stem": "In our survey", "prefer_stem": "We examine"}, {"avoid_stem": "Our survey", "prefer_stem": "This work"}, {"avoid_stem": "This survey", "prefer_stem": "This work"}], "discourse_stem_watchlist": ["Additionally,", "Moreover,", "Furthermore,", "This suggests", "Taken together,", "The key point is that", "Overall,", "In summary,", "In practice,", "More broadly,", "Importantly,", "Notably,", "Crucially,", "In general,", "At a high level,", "In other words,", "Therefore,", "In addition,", "As a result,"], "discourse_stem_rewrites": {"Additionally,": ["More importantly,", "At the same time,", "A second consideration is"], "This suggests": ["This pattern indicates", "These results point to", "A plausible explanation is"], "Taken together,": ["Across these studies,", "Collectively,", "The evidence suggests"], "The key point is that": ["A practical implication is that", "A useful way to read these results is that", "One takeaway is that"], "Overall,": ["In practice,", "More broadly,", "A practical implication is that", "One way to read this evidence is that"], "In summary,": ["Across these studies,", "Collectively,", "A consistent theme is that", "The evidence suggests that"], "Importantly,": ["More importantly,", "A key constraint is that", "A practical implication is that"], "Notably,": ["In particular,", "A concrete example is that", "One implication is that"], "Crucially,": ["More importantly,", "A central constraint is that", "A key takeaway is that"], "At a high level,": ["Concretely,", "Under this protocol,", "In practice,"], "In other words,": ["Put differently,", "Equivalently,", "More concretely,"], "Therefore,": ["As a result,", "This in turn means that", "A practical implication is that"], "In addition,": ["At the same time,", "A second consideration is", "More importantly,"], "As a result,": ["This in turn means that", "Consequently,", "A practical implication is that"]}, "role_cards": {"section_author": {"mission": "Write one subsection as an argument (not a topic list), using in-scope citations as evidence.", "do": ["State a concrete tension/trade-off early and commit to a thesis.", "Make at least two explicit A-vs-B contrasts (mechanism -> outcome) instead of per-paper summaries.", "When citing numbers, add minimal context (task + metric + constraint) in the same paragraph.", "End with a limitation that changes interpretation (protocol mismatch, unclear threat model, missing ablations)."], "avoid": ["Outline narration (This subsection..., In this subsection...).", "Slide navigation (Next, we move..., We now turn...).", "Meta survey advice (survey comparisons should...).", "Cite dumps (a trailing [@a; @b; @c] without a claim)."]}, "evidence_steward": {"mission": "Keep claims auditable: citations support the sentence that needs them, and scope stays local.", "do": ["Embed citations inside claim sentences; name concrete nouns (system/benchmark/protocol) near each cite.", "Prefer subsection-scoped citations; use chapter/global only when truly cross-cutting.", "Downgrade overconfident claims when evidence is thin; convert unknowns into verification targets (once, not spam)."], "avoid": ["Out-of-scope citations to make a paragraph sound stronger.", "Repeating evidence-policy disclaimers in every subsection.", "Ambiguous model naming (e.g., GPT-5) unless the cited work uses it."]}, "style_harmonizer": {"mission": "Make the prose read like a paper: calm, specific, and varied in rhythm without sounding templated.", "do": ["Vary opener cadence across sections (tension-first / decision-first / lens-first) without reusing the same stem.", "Prefer argument bridges over navigation; keep signposting light and content-bearing.", "Reduce slash-enumerations (A/B/C axis labels) by rewriting into natural prose."], "avoid": ["Count-based openers used as a slot (e.g., Two limitations..., Three takeaways...).", "Repeated discourse stems (Additionally, This suggests, Taken together) across many paragraphs.", "Pipeline words (workspace/unit/quality gate/evidence pack) in reader-facing text.", "PPT speaker-note tone (Now we..., The remainder of...).", "Internal shorthand that reads like planning notes (e.g., protocol/metric/constraint tokens)."]}}, "version": "0.4"}, "opener_mode": "lens-first", "opener_hint": "Start by naming the lens (interface/protocol/threat model); state what it reveals; end paragraph 1 with the thesis.", "axes": ["evaluation protocol (datasets, metrics, human evaluation)", "compute and latency constraints", "tool interface contract (schemas / protocols)", "tool selection / routing policy", "sandboxing / permissions / observability"], "bridge_terms": ["threat model", "prompt/tool injection", "monitoring", "guardrails", "benchmarks/metrics", "compute"], "contrast_hook": "security", "required_evidence_fields": ["benchmarks/datasets", "metrics / human-eval protocol", "compute / cost (train/infer)", "training signal / supervision", "threat model", "defense surface"], "paragraph_plan": [{"para": 1, "argument_role": "setup_thesis", "intent": "Define scope, setup, and the subsection thesis (no pipeline jargon).", "focus": ["scope boundary", "key definitions", "thesis vs neighboring subsections"], "connector_to_prev": "", "connector_phrase": "", "use_clusters": ["Agent frameworks / architectures"], "rq": "Which design choices in Safety, security, and governance drive the major trade-offs, and how are those trade-offs measured?"}, {"para": 2, "argument_role": "mechanism_cluster_A", "intent": "Explain cluster A: core mechanism and system architecture and what decision it makes in the agent loop.", "focus": ["cluster: Agent frameworks / architectures", "core mechanism and system architecture", "assumptions"], "connector_to_prev": "grounding", "connector_phrase": "baseline route (Agent frameworks / architectures)", "use_clusters": ["Agent frameworks / architectures"]}, {"para": 3, "argument_role": "implementation_cluster_A", "intent": "Cluster A implementation details: training and data signals and interface contract (tools/memory) that constrain behavior.", "focus": ["cluster: Agent frameworks / architectures", "training and data setup", "interface contract", "axes: evaluation protocol (datasets, metrics, human evaluation), compute and latency constraints, tool interface contract (schemas / protocols), tool selection / routing policy, sandboxing / permissions / observability"], "connector_to_prev": "elaboration", "connector_phrase": "implementation assumptions (interface + training)", "use_clusters": ["Agent frameworks / architectures"]}, {"para": 4, "argument_role": "evaluation_cluster_A", "intent": "Cluster A evaluation/trade-offs: where it works, costs (compute/latency), and typical failure modes.", "focus": ["cluster: Agent frameworks / architectures", "evaluation anchor", "efficiency", "failure modes"], "connector_to_prev": "evaluation", "connector_phrase": "evaluation anchor (task/metric/constraint) + failure modes", "use_clusters": ["Agent frameworks / architectures"]}, {"para": 5, "argument_role": "contrast_cluster_B", "intent": "Explain cluster B (contrast with A): core mechanism and system architecture and what it optimizes for.", "focus": ["cluster: Safety / security / guardrails", "contrast with Agent frameworks / architectures", "core mechanism and system architecture"], "connector_to_prev": "contrast", "connector_phrase": "contrast route (Safety / security / guardrails vs Agent frameworks / architectures)", "use_clusters": ["Safety / security / guardrails"]}, {"para": 6, "argument_role": "implementation_cluster_B", "intent": "Cluster B implementation details: training and data and interface assumptions (mirror A for comparability).", "focus": ["cluster: Safety / security / guardrails", "training and data setup", "interface contract", "axes: evaluation protocol (datasets, metrics, human evaluation), compute and latency constraints, tool interface contract (schemas / protocols), tool selection / routing policy, sandboxing / permissions / observability"], "connector_to_prev": "elaboration", "connector_phrase": "contrast implementation assumptions (B)", "use_clusters": ["Safety / security / guardrails"]}, {"para": 7, "argument_role": "evaluation_cluster_B", "intent": "Cluster B evaluation/trade-offs: where it works, costs, and failure modes (mirror A).", "focus": ["cluster: Safety / security / guardrails", "evaluation anchor", "efficiency", "failure modes"], "connector_to_prev": "evaluation", "connector_phrase": "contrast evaluation anchor + trade-offs (B)", "use_clusters": ["Safety / security / guardrails"]}, {"para": 8, "argument_role": "cross_paper_synthesis", "intent": "Cross-paper synthesis: compare clusters along the main axes (include >=2 citations in one paragraph).", "focus": ["compare Agent frameworks / architectures vs Safety / security / guardrails", "multiple citations in one paragraph", "axes: evaluation protocol (datasets, metrics, human evaluation), compute and latency constraints, tool interface contract (schemas / protocols), tool selection / routing policy, sandboxing / permissions / observability"], "connector_to_prev": "synthesis", "connector_phrase": "cross-paper synthesis (Agent frameworks / architectures vs Safety / security / guardrails)", "use_clusters": ["Agent frameworks / architectures", "Safety / security / guardrails", "Tool-use and function calling"]}, {"para": 9, "argument_role": "decision_guidance", "intent": "Decision guidance: when to choose which route (criteria + evaluation signals + engineering constraints).", "focus": ["decision checklist", "evaluation protocol", "practical constraints"], "connector_to_prev": "consequence", "connector_phrase": "decision guidance / criteria", "use_clusters": ["Agent frameworks / architectures", "Safety / security / guardrails", "Tool-use and function calling"]}, {"para": 10, "argument_role": "limitations_open_questions", "intent": "Limitations + verification targets; end with a concrete open question to hand off.", "focus": ["limitations", "evidence mode: provisional", "what needs verification", "open question"], "connector_to_prev": "limitations", "connector_phrase": "limitations + verification targets", "use_clusters": ["Agent frameworks / architectures", "Safety / security / guardrails", "Tool-use and function calling"], "policy": "Use conservative language; avoid strong conclusions; prefer questions-to-answer + explicit evidence gaps list."}], "chapter_throughline": ["Pin scope to goal: agent survey latex.", "Compare approaches along: evaluation protocol (datasets, metrics, human evaluation).", "Compare approaches along: compute and latency constraints.", "Compare approaches along: tool interface contract (schemas / protocols).", "Compare approaches along: tool selection / routing policy.", "Compare approaches along: sandboxing / permissions / observability."], "chapter_key_contrasts": ["tool interfaces", "security"], "chapter_synthesis_mode": "tradeoff_matrix", "allowed_bibkeys_selected": ["Zhang2025Security", "Luo2025Universe", "Shi2025Progent", "Wang2025Agentvigil", "Weng2025Bridgescope", "Bonagiri2025Check", "Gasmi2025Bridging", "Mo2025Attractive", "Gao2025Radar", "Liu2026Agents", "Seo2025Simuhome", "Zhang2024Large", "Plaat2025Agentic", "Han2025Large", "Marandi2025Complex", "Agrawal2025Language", "Zhou2025Reasoning", "Kang2025Acon", "VarangotReille2025Doing", "Li2023Modelscope", "Wang2025Comprehensive", "Henke2025Autopentest"], "allowed_bibkeys_mapped": ["Li2024Personal", "Bonagiri2025Check", "Gasmi2025Bridging", "Hadeliya2025When", "Zhang2025Security", "Shi2025Progent", "Wang2025Comprehensive", "Weng2025Bridgescope", "Han2025Large", "Rosario2025Architecting", "Sun2024Survey", "Seo2025Simuhome", "Liu2026Agents", "Wang2025Agentvigil", "Mo2025Attractive", "VarangotReille2025Doing", "Liu2025Secure", "Agrawal2025Language", "He2024Emerged", "Zhou2025Reasoning", "Gao2025Radar", "Luo2025Universe", "Zhang2024Large", "Plaat2025Agentic", "Kang2025Acon", "Marandi2025Complex", "Henke2025Autopentest", "Li2023Modelscope"], "allowed_bibkeys_chapter": ["Agrawal2025Language", "Bonagiri2025Check", "Chang2023Survey", "Chen2025Towards", "Dong2025Compressed", "Gao2025Radar", "Gasmi2025Bridging", "Hadeliya2025When", "Han2025Large", "He2024Emerged", "Henke2025Autopentest", "Hu2023Avis", "Huang2024Survey", "Ji2025Taxonomy", "Kang2025Acon", "Kim2026Beyond", "Li2023Modelscope", "Li2024Personal", "Lin2023Agentsims", "Liu2025Secure", "Liu2026Agents", "Luo2025Universe", "Marandi2025Complex", "Mo2025Attractive", "Plaat2025Agentic", "Qi2024Large", "Rahman2025Hallucination", "Rosario2025Architecting", "Seo2025Simuhome", "Shang2024Agentsquare", "Shen2025Feat", "Shi2025Progent", "Sun2024Survey", "Tang2025Empowering", "Trirat2024Automl", "Van2025Survey", "VarangotReille2025Doing", "Wang2025Agentvigil", "Wang2025Autoscore", "Wang2025Comprehensive", "Weng2025Bridgescope", "Wu2025Lessons", "Wu2025Meta", "Yao2022React", "Zhang2024Large", "Zhang2025Datascibench", "Zhang2025Generalizability", "Zhang2025Security", "Zhang2026Evoroute", "Zhou2025Reasoning", "Zhu2025Evolutionary", "Zhu2025Where"], "allowed_bibkeys_global": ["Li2023Modelscope", "Yao2022React"], "evidence_ids": ["E-P0195-d6095e10e9", "E-P0196-3c65d38a2a", "E-P0086-68db58914f", "E-P0142-03ed8e82d6", "E-P0155-e4be1f69f1", "E-P0158-7edb91824f", "E-P0058-8e34a29629", "E-P0146-a0b404d928", "E-P0081-419e1464da", "E-P0126-8b56718f74", "E-P0216-469a70bb44", "E-P0274-52fea1d199", "E-P0008-a1fb101bda", "E-P0075-fedc33d121", "E-P0060-f10376c14d", "E-P0241-5603d51445", "E-P0030-e38b4bdff3", "E-P0048-108346ac22", "E-P0168-7320460853", "E-P0116-7a2e32f4bc", "E-P0129-d541ee15e3", "E-P0148-7e9d7616ab", "E-P0148-89865e4c80", "E-P0158-0550cab83d"], "anchor_facts": [{"hook_type": "quant", "text": "Function Calling showed higher overall attack success rates (73.5% vs 62.59% for MCP), with greater system-centric vulnerability while MCP exhibited increased LLM-centric exposure.", "citations": ["Gasmi2025Bridging"], "paper_id": "P0058", "evidence_id": "E-P0058-8e34a29629", "pointer": "papers/paper_notes.jsonl:paper_id=P0058#key_results[0]"}, {"hook_type": "quant", "text": "MSB contributes: (1) a taxonomy of 12 attacks including name-collision, preference manipulation, prompt injections embedded in tool descriptions, out-of-scope parameter requests, user-impersonating responses, false-error escalation, tool-transfer, retrieval injection, and mixed", "citations": ["Zhang2025Security"], "paper_id": "P0195", "evidence_id": "E-P0195-d6095e10e9", "pointer": "papers/paper_notes.jsonl:paper_id=P0195#key_results[0]"}, {"hook_type": "quant", "text": "Evaluations on two novel benchmarks demonstrate that BridgeScope enables LLM agents to operate databases more effectively, reduces token usage by up to 80% through improved security awareness, and uniquely supports data-intensive workflows beyond existing toolkits, establishing", "citations": ["Weng2025Bridgescope"], "paper_id": "P0155", "evidence_id": "E-P0155-e4be1f69f1", "pointer": "papers/paper_notes.jsonl:paper_id=P0155#key_results[0]"}, {"hook_type": "quant", "text": "Our extensive evaluation across various agent use cases, using benchmarks like AgentDojo, ASB, and AgentPoison, demonstrates that Progent reduces attack success rates to 0%, while preserving agent utility and speed.", "citations": ["Shi2025Progent"], "paper_id": "P0086", "evidence_id": "E-P0086-68db58914f", "pointer": "papers/paper_notes.jsonl:paper_id=P0086#key_results[0]"}, {"hook_type": "quant", "text": "We evaluate AgentVigil on two public benchmarks, AgentDojo and VWA-adv, where it achieves 71% and 70% success rates against agents based on o3-mini and GPT-4o, respectively, nearly doubling the performance of baseline attacks.", "citations": ["Wang2025Agentvigil"], "paper_id": "P0142", "evidence_id": "E-P0142-03ed8e82d6", "pointer": "papers/paper_notes.jsonl:paper_id=P0142#key_results[0]"}, {"hook_type": "quant", "text": "MSB contributes: (1) a taxonomy of 12 attacks including name-collision, preference manipulation, prompt injections embedded in tool descriptions, out-of-scope parameter requests, user-impersonating responses, false-error escalation, tool-transfer, retrieval injection, and mixed a", "citations": ["Zhang2025Security"], "paper_id": "P0195", "evidence_id": "E-P0195-d6095e10e9", "pointer": "papers/paper_notes.jsonl:paper_id=P0195#key_results[0]"}, {"hook_type": "quant", "text": "Through extensive evaluation of leading LLMs, we find that even SOTA models such as GPT-5 (43.72%), Grok-4 (33.33%) and Claude-4.0-Sonnet (29.44%) exhibit significant performance limitations.", "citations": ["Luo2025Universe"], "paper_id": "P0196", "evidence_id": "E-P0196-3c65d38a2a", "pointer": "papers/paper_notes.jsonl:paper_id=P0196#limitations[1]"}, {"hook_type": "quant", "text": "Evaluations on two novel benchmarks demonstrate that BridgeScope enables LLM agents to operate databases more effectively, reduces token usage by up to 80% through improved security awareness, and uniquely supports data-intensive workflows beyond existing toolkits, establishing B", "citations": ["Weng2025Bridgescope"], "paper_id": "P0155", "evidence_id": "E-P0155-e4be1f69f1", "pointer": "papers/paper_notes.jsonl:paper_id=P0155#key_results[0]"}, {"hook_type": "quant", "text": "Leveraging the ToolEmu framework, we conduct a systematic evaluation of quitting behavior across 12 state-of-the-art LLMs.", "citations": ["Bonagiri2025Check"], "paper_id": "P0158", "evidence_id": "E-P0158-7edb91824f", "pointer": "papers/paper_notes.jsonl:paper_id=P0158#key_results[0]"}, {"hook_type": "quant", "text": "Extensive experiments across ten realistic, simulated tool-use scenarios and a range of popular LLM agents demonstrate consistently high attack success rates (81\\%-95\\%) and significant privacy leakage, with negligible impact on primary task execution.", "citations": ["Mo2025Attractive"], "paper_id": "P0146", "evidence_id": "E-P0146-a0b404d928", "pointer": "papers/paper_notes.jsonl:paper_id=P0146#key_results[0]"}], "comparison_cards": [{"axis": "evaluation protocol (datasets, metrics, human evaluation)", "A_label": "Agent frameworks / architectures", "B_label": "Safety / security / guardrails", "citations": ["Gasmi2025Bridging", "Plaat2025Agentic", "Zhang2025Security"], "A_highlights": [{"paper_id": "P0058", "evidence_id": "E-P0058-8e34a29629", "excerpt": "Function Calling showed higher overall attack success rates (73.5% vs 62.59% for MCP), with greater system-centric vulnerability while MCP exhibited increased LLM-centric exposure.", "citations": ["Gasmi2025Bridging"], "pointer": "papers/paper_notes.jsonl:paper_id=P0058#key_results[0]"}, {"paper_id": "P0008", "evidence_id": "E-P0008-a1fb101bda", "excerpt": "Further, agentic LLMs provide a solution for the problem of LLMs running out of training data: inference-time behavior generates new training states, such that LLMs can keep learning without needing ever larger datasets.", "citations": ["Plaat2025Agentic"], "pointer": "papers/paper_notes.jsonl:paper_id=P0008#key_results[1]"}], "B_highlights": [{"paper_id": "P0195", "evidence_id": "E-P0195-d6095e10e9", "excerpt": "MSB contributes: (1) a taxonomy of 12 attacks including name-collision, preference manipulation, prompt injections embedded in tool descriptions, out-of-scope parameter requests, user-impersonating responses, false-error escalation, tool-transfer, retrieval injection, and mixed", "citations": ["Zhang2025Security"], "pointer": "papers/paper_notes.jsonl:paper_id=P0195#key_results[0]"}, {"paper_id": "P0058", "evidence_id": "E-P0058-8e34a29629", "excerpt": "Function Calling showed higher overall attack success rates (73.5% vs 62.59% for MCP), with greater system-centric vulnerability while MCP exhibited increased LLM-centric exposure.", "citations": ["Gasmi2025Bridging"], "pointer": "papers/paper_notes.jsonl:paper_id=P0058#key_results[0]"}], "write_prompt": "Contrast Agent frameworks / architectures vs Safety / security / guardrails along \"evaluation protocol (datasets, metrics, human evaluation)\". Ground A and B using the highlight snippets; do not introduce new claims beyond the cited evidence."}, {"axis": "evaluation protocol (datasets, metrics, human evaluation)", "A_label": "Agent frameworks / architectures", "B_label": "Tool-use and function calling", "citations": ["Gasmi2025Bridging", "Plaat2025Agentic", "Zhang2025Security", "Weng2025Bridgescope"], "A_highlights": [{"paper_id": "P0058", "evidence_id": "E-P0058-8e34a29629", "excerpt": "Function Calling showed higher overall attack success rates (73.5% vs 62.59% for MCP), with greater system-centric vulnerability while MCP exhibited increased LLM-centric exposure.", "citations": ["Gasmi2025Bridging"], "pointer": "papers/paper_notes.jsonl:paper_id=P0058#key_results[0]"}, {"paper_id": "P0008", "evidence_id": "E-P0008-a1fb101bda", "excerpt": "Further, agentic LLMs provide a solution for the problem of LLMs running out of training data: inference-time behavior generates new training states, such that LLMs can keep learning without needing ever larger datasets.", "citations": ["Plaat2025Agentic"], "pointer": "papers/paper_notes.jsonl:paper_id=P0008#key_results[1]"}], "B_highlights": [{"paper_id": "P0195", "evidence_id": "E-P0195-d6095e10e9", "excerpt": "MSB contributes: (1) a taxonomy of 12 attacks including name-collision, preference manipulation, prompt injections embedded in tool descriptions, out-of-scope parameter requests, user-impersonating responses, false-error escalation, tool-transfer, retrieval injection, and mixed", "citations": ["Zhang2025Security"], "pointer": "papers/paper_notes.jsonl:paper_id=P0195#key_results[0]"}, {"paper_id": "P0155", "evidence_id": "E-P0155-e4be1f69f1", "excerpt": "Evaluations on two novel benchmarks demonstrate that BridgeScope enables LLM agents to operate databases more effectively, reduces token usage by up to 80% through improved security awareness, and uniquely supports data-intensive workflows beyond existing toolkits, establishing", "citations": ["Weng2025Bridgescope"], "pointer": "papers/paper_notes.jsonl:paper_id=P0155#key_results[0]"}], "write_prompt": "Contrast Agent frameworks / architectures vs Tool-use and function calling along \"evaluation protocol (datasets, metrics, human evaluation)\". Ground A and B using the highlight snippets; do not introduce new claims beyond the cited evidence."}, {"axis": "evaluation protocol (datasets, metrics, human evaluation)", "A_label": "Safety / security / guardrails", "B_label": "Tool-use and function calling", "citations": ["Zhang2025Security", "Gasmi2025Bridging", "Weng2025Bridgescope"], "A_highlights": [{"paper_id": "P0195", "evidence_id": "E-P0195-d6095e10e9", "excerpt": "MSB contributes: (1) a taxonomy of 12 attacks including name-collision, preference manipulation, prompt injections embedded in tool descriptions, out-of-scope parameter requests, user-impersonating responses, false-error escalation, tool-transfer, retrieval injection, and mixed", "citations": ["Zhang2025Security"], "pointer": "papers/paper_notes.jsonl:paper_id=P0195#key_results[0]"}, {"paper_id": "P0058", "evidence_id": "E-P0058-8e34a29629", "excerpt": "Function Calling showed higher overall attack success rates (73.5% vs 62.59% for MCP), with greater system-centric vulnerability while MCP exhibited increased LLM-centric exposure.", "citations": ["Gasmi2025Bridging"], "pointer": "papers/paper_notes.jsonl:paper_id=P0058#key_results[0]"}], "B_highlights": [{"paper_id": "P0195", "evidence_id": "E-P0195-d6095e10e9", "excerpt": "MSB contributes: (1) a taxonomy of 12 attacks including name-collision, preference manipulation, prompt injections embedded in tool descriptions, out-of-scope parameter requests, user-impersonating responses, false-error escalation, tool-transfer, retrieval injection, and mixed", "citations": ["Zhang2025Security"], "pointer": "papers/paper_notes.jsonl:paper_id=P0195#key_results[0]"}, {"paper_id": "P0155", "evidence_id": "E-P0155-e4be1f69f1", "excerpt": "Evaluations on two novel benchmarks demonstrate that BridgeScope enables LLM agents to operate databases more effectively, reduces token usage by up to 80% through improved security awareness, and uniquely supports data-intensive workflows beyond existing toolkits, establishing", "citations": ["Weng2025Bridgescope"], "pointer": "papers/paper_notes.jsonl:paper_id=P0155#key_results[0]"}], "write_prompt": "Contrast Safety / security / guardrails vs Tool-use and function calling along \"evaluation protocol (datasets, metrics, human evaluation)\". Ground A and B using the highlight snippets; do not introduce new claims beyond the cited evidence."}, {"axis": "compute and latency constraints", "A_label": "Agent frameworks / architectures", "B_label": "Safety / security / guardrails", "citations": ["Plaat2025Agentic", "Shi2025Progent", "Zhang2025Security", "Wang2025Agentvigil"], "A_highlights": [{"paper_id": "P0008", "evidence_id": "E-P0008-a1fb101bda", "excerpt": "Further, agentic LLMs provide a solution for the problem of LLMs running out of training data: inference-time behavior generates new training states, such that LLMs can keep learning without needing ever larger datasets.", "citations": ["Plaat2025Agentic"], "pointer": "papers/paper_notes.jsonl:paper_id=P0008#key_results[1]"}, {"paper_id": "P0086", "evidence_id": "E-P0086-68db58914f", "excerpt": "Our extensive evaluation across various agent use cases, using benchmarks like AgentDojo, ASB, and AgentPoison, demonstrates that Progent reduces attack success rates to 0%, while preserving agent utility and speed.", "citations": ["Shi2025Progent"], "pointer": "papers/paper_notes.jsonl:paper_id=P0086#key_results[0]"}], "B_highlights": [{"paper_id": "P0195", "evidence_id": "E-P0195-d6095e10e9", "excerpt": "MSB contributes: (1) a taxonomy of 12 attacks including name-collision, preference manipulation, prompt injections embedded in tool descriptions, out-of-scope parameter requests, user-impersonating responses, false-error escalation, tool-transfer, retrieval injection, and mixed", "citations": ["Zhang2025Security"], "pointer": "papers/paper_notes.jsonl:paper_id=P0195#key_results[0]"}, {"paper_id": "P0142", "evidence_id": "E-P0142-03ed8e82d6", "excerpt": "We evaluate AgentVigil on two public benchmarks, AgentDojo and VWA-adv, where it achieves 71% and 70% success rates against agents based on o3-mini and GPT-4o, respectively, nearly doubling the performance of baseline attacks.", "citations": ["Wang2025Agentvigil"], "pointer": "papers/paper_notes.jsonl:paper_id=P0142#key_results[0]"}], "write_prompt": "Contrast Agent frameworks / architectures vs Safety / security / guardrails along \"compute and latency constraints\". Ground A and B using the highlight snippets; do not introduce new claims beyond the cited evidence."}, {"axis": "compute and latency constraints", "A_label": "Agent frameworks / architectures", "B_label": "Tool-use and function calling", "citations": ["Plaat2025Agentic", "Shi2025Progent", "Zhang2025Security", "Weng2025Bridgescope"], "A_highlights": [{"paper_id": "P0008", "evidence_id": "E-P0008-a1fb101bda", "excerpt": "Further, agentic LLMs provide a solution for the problem of LLMs running out of training data: inference-time behavior generates new training states, such that LLMs can keep learning without needing ever larger datasets.", "citations": ["Plaat2025Agentic"], "pointer": "papers/paper_notes.jsonl:paper_id=P0008#key_results[1]"}, {"paper_id": "P0086", "evidence_id": "E-P0086-68db58914f", "excerpt": "Our extensive evaluation across various agent use cases, using benchmarks like AgentDojo, ASB, and AgentPoison, demonstrates that Progent reduces attack success rates to 0%, while preserving agent utility and speed.", "citations": ["Shi2025Progent"], "pointer": "papers/paper_notes.jsonl:paper_id=P0086#key_results[0]"}], "B_highlights": [{"paper_id": "P0195", "evidence_id": "E-P0195-d6095e10e9", "excerpt": "MSB contributes: (1) a taxonomy of 12 attacks including name-collision, preference manipulation, prompt injections embedded in tool descriptions, out-of-scope parameter requests, user-impersonating responses, false-error escalation, tool-transfer, retrieval injection, and mixed", "citations": ["Zhang2025Security"], "pointer": "papers/paper_notes.jsonl:paper_id=P0195#key_results[0]"}, {"paper_id": "P0155", "evidence_id": "E-P0155-e4be1f69f1", "excerpt": "Evaluations on two novel benchmarks demonstrate that BridgeScope enables LLM agents to operate databases more effectively, reduces token usage by up to 80% through improved security awareness, and uniquely supports data-intensive workflows beyond existing toolkits, establishing", "citations": ["Weng2025Bridgescope"], "pointer": "papers/paper_notes.jsonl:paper_id=P0155#key_results[0]"}], "write_prompt": "Contrast Agent frameworks / architectures vs Tool-use and function calling along \"compute and latency constraints\". Ground A and B using the highlight snippets; do not introduce new claims beyond the cited evidence."}, {"axis": "compute and latency constraints", "A_label": "Safety / security / guardrails", "B_label": "Tool-use and function calling", "citations": ["Zhang2025Security", "Wang2025Agentvigil", "Weng2025Bridgescope"], "A_highlights": [{"paper_id": "P0195", "evidence_id": "E-P0195-d6095e10e9", "excerpt": "MSB contributes: (1) a taxonomy of 12 attacks including name-collision, preference manipulation, prompt injections embedded in tool descriptions, out-of-scope parameter requests, user-impersonating responses, false-error escalation, tool-transfer, retrieval injection, and mixed", "citations": ["Zhang2025Security"], "pointer": "papers/paper_notes.jsonl:paper_id=P0195#key_results[0]"}, {"paper_id": "P0142", "evidence_id": "E-P0142-03ed8e82d6", "excerpt": "We evaluate AgentVigil on two public benchmarks, AgentDojo and VWA-adv, where it achieves 71% and 70% success rates against agents based on o3-mini and GPT-4o, respectively, nearly doubling the performance of baseline attacks.", "citations": ["Wang2025Agentvigil"], "pointer": "papers/paper_notes.jsonl:paper_id=P0142#key_results[0]"}], "B_highlights": [{"paper_id": "P0195", "evidence_id": "E-P0195-d6095e10e9", "excerpt": "MSB contributes: (1) a taxonomy of 12 attacks including name-collision, preference manipulation, prompt injections embedded in tool descriptions, out-of-scope parameter requests, user-impersonating responses, false-error escalation, tool-transfer, retrieval injection, and mixed", "citations": ["Zhang2025Security"], "pointer": "papers/paper_notes.jsonl:paper_id=P0195#key_results[0]"}, {"paper_id": "P0155", "evidence_id": "E-P0155-e4be1f69f1", "excerpt": "Evaluations on two novel benchmarks demonstrate that BridgeScope enables LLM agents to operate databases more effectively, reduces token usage by up to 80% through improved security awareness, and uniquely supports data-intensive workflows beyond existing toolkits, establishing", "citations": ["Weng2025Bridgescope"], "pointer": "papers/paper_notes.jsonl:paper_id=P0155#key_results[0]"}], "write_prompt": "Contrast Safety / security / guardrails vs Tool-use and function calling along \"compute and latency constraints\". Ground A and B using the highlight snippets; do not introduce new claims beyond the cited evidence."}, {"axis": "tool interface contract (schemas / protocols)", "A_label": "Agent frameworks / architectures", "B_label": "Safety / security / guardrails", "citations": ["Gasmi2025Bridging", "Kang2025Acon", "Zhang2025Security"], "A_highlights": [{"paper_id": "P0058", "evidence_id": "E-P0058-8e34a29629", "excerpt": "Function Calling showed higher overall attack success rates (73.5% vs 62.59% for MCP), with greater system-centric vulnerability while MCP exhibited increased LLM-centric exposure.", "citations": ["Gasmi2025Bridging"], "pointer": "papers/paper_notes.jsonl:paper_id=P0058#key_results[0]"}, {"paper_id": "P0048", "evidence_id": "E-P0048-108346ac22", "excerpt": "Large language models (LLMs) are increasingly deployed as agents in dynamic, real-world environments, where success requires both reasoning and effective tool use.", "citations": ["Kang2025Acon"], "pointer": "papers/paper_notes.jsonl:paper_id=P0048#key_results[1]"}], "B_highlights": [{"paper_id": "P0195", "evidence_id": "E-P0195-d6095e10e9", "excerpt": "MSB contributes: (1) a taxonomy of 12 attacks including name-collision, preference manipulation, prompt injections embedded in tool descriptions, out-of-scope parameter requests, user-impersonating responses, false-error escalation, tool-transfer, retrieval injection, and mixed", "citations": ["Zhang2025Security"], "pointer": "papers/paper_notes.jsonl:paper_id=P0195#key_results[0]"}, {"paper_id": "P0058", "evidence_id": "E-P0058-8e34a29629", "excerpt": "Function Calling showed higher overall attack success rates (73.5% vs 62.59% for MCP), with greater system-centric vulnerability while MCP exhibited increased LLM-centric exposure.", "citations": ["Gasmi2025Bridging"], "pointer": "papers/paper_notes.jsonl:paper_id=P0058#key_results[0]"}], "write_prompt": "Contrast Agent frameworks / architectures vs Safety / security / guardrails along \"tool interface contract (schemas / protocols)\". Ground A and B using the highlight snippets; do not introduce new claims beyond the cited evidence."}], "evaluation_protocol": [{"bullet": "Evaluation mentions include: LLMs, RSP, GSI, RSV, FEVER, RSP-M, HotpotQA, ReAct, ACON, LMs.", "citations": ["Liu2026Agents", "Plaat2025Agentic", "Zhou2025Reasoning", "Kang2025Acon"]}, {"bullet": "When comparing results, anchor the paragraph with: task type + metric + constraint (budget, tool access, horizon, or threat model) when stated.", "citations": ["Liu2026Agents", "Plaat2025Agentic"]}, {"bullet": "Prefer head-to-head comparisons only when benchmark/metric are shared; otherwise frame differences as protocol-driven rather than method superiority.", "citations": ["Liu2026Agents", "Plaat2025Agentic"]}, {"bullet": "Avoid underspecified model/baseline naming; if abstracts omit details, state that the baseline is reported but underspecified instead of guessing.", "citations": ["Liu2026Agents", "Plaat2025Agentic"]}, {"bullet": "If a claim relies on a single reported number, pair it with a limitation/caveat from the same evidence so the draft remains conservative.", "citations": ["Liu2026Agents", "Plaat2025Agentic"]}, {"bullet": "If budgets or environments differ across papers, treat cross-paper numeric comparison as fragile and prefer qualitative contrasts aligned to the subsection axes.", "citations": ["Liu2026Agents", "Plaat2025Agentic"]}], "limitation_hooks": [{"excerpt": "Large language models (LLMs) have precipitated a dramatic improvement in the legal domain, yet the deployment of standalone models faces significant limitations regarding hallucination, outdated information, and verifiability.", "citations": ["Liu2026Agents"], "pointer": ""}, {"excerpt": "We note that there is risk associated with LLM assistants taking action in the real world-safety, liability and security are open problems-while agentic LLMs are also likely to benefit society.", "citations": ["Plaat2025Agentic"], "pointer": ""}, {"excerpt": "While existing adversarial attacks primarily focus on content falsification or instruction injection, we identify a novel, process-oriented attack surface: the agent's reasoning style.", "citations": ["Zhou2025Reasoning"], "pointer": ""}, {"excerpt": "We introduce Generative Style Injection (GSI), an attack method that rewrites retrieved documents into pathological tones--specifically \"analysis paralysis\" or \"cognitive haste\"--without altering underlying facts or using explicit triggers.", "citations": ["Zhou2025Reasoning"], "pointer": ""}, {"excerpt": "A central challenge for agentic tasks is the growing context length, as agents must accumulate long histories of actions and observations.", "citations": ["Kang2025Acon"], "pointer": ""}, {"excerpt": "ACON leverages compression guideline optimization in natural language space: given paired trajectories where full context succeeds but compressed context fails, capable LLMs analyze the causes of failure, and the compression guideline is updated accordingly.", "citations": ["Kang2025Acon"], "pointer": ""}, {"excerpt": "Large Language Model (LLM) agents face security vulnerabilities spanning AI-specific and traditional software domains, yet current research addresses these separately.", "citations": ["Gasmi2025Bridging"], "pointer": ""}, {"excerpt": "This study bridges this gap through comparative evaluation of Function Calling architecture and Model Context Protocol (MCP) deployment paradigms using a unified threat classification framework.", "citations": ["Gasmi2025Bridging"], "pointer": ""}], "must_use": {"min_anchor_facts": 4, "min_comparison_cards": 4, "min_limitation_hooks": 2, "require_cited_numeric_if_available": true, "require_multi_cite_synthesis_paragraph": true, "thesis_required": true}, "do_not_repeat_phrases": ["this run", "this run is", "pipeline", "workspace", "unit", "quality gate", "evidence pack", "evidence packs", "writer context pack", "Method note (evidence policy):", "Methodology note:", "Methodology note (evidence policy):", "Methodology note (evidence-policy):", "Method note:", "This subsection surveys", "This subsection argues", "In this subsection", "This section surveys", "This section reviews", "This section discusses", "This section covers", "This section presents", "This section introduces", "provides an overview", "This survey provides an overview", "In this section, we provide an overview", "This section provides an overview", "Next, we move from", "We now turn to", "A few representative references include", "Notable lines of work include", "Concrete examples include", "Work in this area includes", "Recent systems include", "Examples include", "Representative systems include", "Therefore, survey synthesis should", "Therefore, survey comparisons should", "As a result, survey comparisons should", "survey synthesis should", "survey comparisons should", "Taken together,", "The key point is that", "Three limitations", "Two limitations", "claims remain provisional under abstract-only evidence", "abstract-only evidence", "Key takeaway:", "Main takeaway:", "protocol token", "protocol tokens", "constraint token", "constraint tokens", "metric token", "metric tokens", "evaluation token", "evaluation tokens", "In this survey", "In our survey", "Our survey", "This survey"], "pack_warnings": [], "pack_stats": {"anchors": {"raw": 12, "considered": 10, "kept": 10, "dropped_no_cites": 0}, "comparisons": {"raw": 10, "considered": 7, "kept": 7, "dropped_no_highlights": 0, "highlights_dropped_no_cites": 0}, "evaluation_protocol": {"raw": 6, "considered": 6, "kept": 6, "dropped_no_cites": 0}, "limitation_hooks": {"raw": 8, "considered": 8, "kept": 8, "dropped_no_cites": 0}, "trim_policy": {"default": 400, "anchor_fact": 420, "highlight_excerpt": 280, "comparison_write_prompt": 420, "eval_bullet": 320, "limitation_excerpt": 320}}, "generated_at": "2026-01-25T17:09:25"}
