{"sub_id": "3.1", "title": "Agent loop and action spaces", "evidence_ids": ["E-P0047-cb9370ac71", "E-P0013-9d9d60644a", "E-P0071-9a8a480e9c", "E-P0037-38a26e4777", "E-P0049-904ba35500", "E-P0029-4b027dfb27", "E-P0058-8e34a29629", "E-P0177-acf4a27e30", "E-P0045-7219ca15f4", "E-P0124-60cc0d458f", "E-P0203-e294aeefb5", "E-P0001-ca4a00b5cf", "E-P0255-5e37132da5", "E-P0270-764758958d", "E-P0102-0cc318c2c4", "E-P0138-da6dd163cf", "E-P0175-be70dd09e9", "E-P0209-aa504d163c", "E-P0297-1c25e10ffc", "E-P0055-c6c63e4ab5", "E-P0116-7a2e32f4bc", "E-P0238-ccc7d0cc50", "E-P0055-71f2629f1b", "E-P0175-3a47ba911a"], "evidence_level_summary": {"fulltext": 0, "abstract": 28, "title": 0}, "evidence_snippets": [{"evidence_id": "E-P0047-cb9370ac71", "text": "Comprehensive evaluations on multiple benchmarks show that A-MemGuard effectively cuts attack success rates by over 95% while incurring a minimal utility cost.", "paper_id": "P0047", "citations": ["Wei2025Memguard"], "provenance": {"evidence_level": "abstract", "source": "paper_notes", "pointer": "papers/paper_notes.jsonl:paper_id=P0047#key_results[0]"}}, {"evidence_id": "E-P0013-9d9d60644a", "text": "We introduce Structured Cognitive Loop (SCL), a modular architecture that explicitly separates agent cognition into five phases: Retrieval, Cognition, Control, Action, and Memory (R-CCAM).", "paper_id": "P0013", "citations": ["Kim2025Bridging"], "provenance": {"evidence_level": "abstract", "source": "paper_notes", "pointer": "papers/paper_notes.jsonl:paper_id=P0013#method"}}, {"evidence_id": "E-P0071-9a8a480e9c", "text": "To overcome these limitations, LLM-based human-agent systems (LLM-HAS) incorporate human-provided information, feedback, or control into the agent system to enhance system performance, reliability and safety.", "paper_id": "P0071", "citations": ["Zou2025Based"], "provenance": {"evidence_level": "abstract", "source": "paper_notes", "pointer": "papers/paper_notes.jsonl:paper_id=P0071#limitations[1]"}}, {"evidence_id": "E-P0037-38a26e4777", "text": "Extensive experiments across six benchmarks, covering the diverse scenarios of web, embodied, tool use and game applications, show that AgentSquare substantially outperforms hand-crafted agents, achieving an average performance gain of 17.2% against best-known human designs.", "paper_id": "P0037", "citations": ["Shang2024Agentsquare"], "provenance": {"evidence_level": "abstract", "source": "paper_notes", "pointer": "papers/paper_notes.jsonl:paper_id=P0037#key_results[0]"}}, {"evidence_id": "E-P0049-904ba35500", "text": "Evaluated across a comprehensive set of seven benchmarks spanning embodied, math, web, tool, and game domains, AgentSwift discovers agents that achieve an average performance gain of 8.34\\% over both existing automated agent search methods and manually designed agents.", "paper_id": "P0049", "citations": ["Li2025Agentswift"], "provenance": {"evidence_level": "abstract", "source": "paper_notes", "pointer": "papers/paper_notes.jsonl:paper_id=P0049#key_results[0]"}}, {"evidence_id": "E-P0029-4b027dfb27", "text": "We evaluate GiGPO on challenging agent benchmarks, including ALFWorld and WebShop, as well as tool-integrated reasoning on search-augmented QA tasks, using Qwen2.5-1.5B/3B/7B-Instruct.", "paper_id": "P0029", "citations": ["Feng2025Group"], "provenance": {"evidence_level": "abstract", "source": "paper_notes", "pointer": "papers/paper_notes.jsonl:paper_id=P0029#key_results[0]"}}, {"evidence_id": "E-P0058-8e34a29629", "text": "Function Calling showed higher overall attack success rates (73.5% vs 62.59% for MCP), with greater system-centric vulnerability while MCP exhibited increased LLM-centric exposure.", "paper_id": "P0058", "citations": ["Gasmi2025Bridging"], "provenance": {"evidence_level": "abstract", "source": "paper_notes", "pointer": "papers/paper_notes.jsonl:paper_id=P0058#key_results[0]"}}, {"evidence_id": "E-P0177-acf4a27e30", "text": "The system employs tool-augmented reasoning, hierarchical retrieval-augmented generation, forensic-tuned LLMs, and human-in-the-loop feedback to ensure legal and medical validity.", "paper_id": "P0177", "citations": ["Shen2025Feat"], "provenance": {"evidence_level": "abstract", "source": "paper_notes", "pointer": "papers/paper_notes.jsonl:paper_id=P0177#key_results[0]"}}, {"evidence_id": "E-P0045-7219ca15f4", "text": "On evaluation, we examine over 190 benchmark datasets and trace a shift from static exams toward process- and discovery-oriented assessments with advanced evaluation protocols.", "paper_id": "P0045", "citations": ["Hu2025Survey"], "provenance": {"evidence_level": "abstract", "source": "paper_notes", "pointer": "papers/paper_notes.jsonl:paper_id=P0045#key_results[1]"}}, {"evidence_id": "E-P0124-60cc0d458f", "text": "Experiments on challenging agentic benchmarks such as GAIA and BrowseComp+ demonstrate that EvoRoute, when integrated into off-the-shelf agentic systems, not only sustains or enhances system performance but also reduces execution cost by up to $80\\%$ and latency by over $70\\%$.", "paper_id": "P0124", "citations": ["Zhang2026Evoroute"], "provenance": {"evidence_level": "abstract", "source": "paper_notes", "pointer": "papers/paper_notes.jsonl:paper_id=P0124#key_results[0]"}}, {"evidence_id": "E-P0203-e294aeefb5", "text": "To evaluate MuaLLM, we introduce two custom datasets: RAG-250, targeting retrieval and citation performance, and Reasoning-100 (Reas-100), focused on multistep reasoning in circuit design.", "paper_id": "P0203", "citations": ["Abbineni2025Muallm"], "provenance": {"evidence_level": "abstract", "source": "paper_notes", "pointer": "papers/paper_notes.jsonl:paper_id=P0203#key_results[1]"}}, {"evidence_id": "E-P0001-ca4a00b5cf", "text": "On two interactive decision making benchmarks (ALFWorld and WebShop), ReAct outperforms imitation and reinforcement learning methods by an absolute success rate of 34% and 10% respectively, while being prompted with only one or two in-context examples.", "paper_id": "P0001", "citations": ["Yao2022React"], "provenance": {"evidence_level": "abstract", "source": "paper_notes", "pointer": "papers/paper_notes.jsonl:paper_id=P0001#key_results[0]"}}, {"evidence_id": "E-P0255-5e37132da5", "text": "We conducted comprehensive experiments using a kinase inhibitor dataset, where our multi-agent LLM method outperformed the non-reasoning multi-agent model (GPT-4o mini) by 45% in F1 score (0.514 vs 0.355).", "paper_id": "P0255", "citations": ["Inoue2024Drugagent"], "provenance": {"evidence_level": "abstract", "source": "paper_notes", "pointer": "papers/paper_notes.jsonl:paper_id=P0255#key_results[0]"}}, {"evidence_id": "E-P0270-764758958d", "text": "However, existing benchmarks for evaluating LLM Agents either use static datasets, potentially leading to data leakage or focus only on single-agent scenarios, overlooking the complexities of multi-agent interactions.", "paper_id": "P0270", "citations": ["Chen2024Llmarena"], "provenance": {"evidence_level": "abstract", "source": "paper_notes", "pointer": "papers/paper_notes.jsonl:paper_id=P0270#key_results[1]"}}, {"evidence_id": "E-P0102-0cc318c2c4", "text": "Extensive experiments demonstrate that only using 10K samples for tuning LLaMA-7B can outperform state-of-the-art methods using larger LLMs or more data, on both in-domain and out-domain datasets.", "paper_id": "P0102", "citations": ["Jiang2024Agent"], "provenance": {"evidence_level": "abstract", "source": "paper_notes", "pointer": "papers/paper_notes.jsonl:paper_id=P0102#key_results[0]"}}, {"evidence_id": "E-P0138-da6dd163cf", "text": "We use the recently released AircraftVerse dataset, which is especially suited for developing and evaluating large language models for designs.", "paper_id": "P0138", "citations": ["Samplawski2025Agent"], "provenance": {"evidence_level": "abstract", "source": "paper_notes", "pointer": "papers/paper_notes.jsonl:paper_id=P0138#key_results[0]"}}, {"evidence_id": "E-P0175-be70dd09e9", "text": "Extensive experiments on two real-world KGQA datasets, WebQSP and CWQ, demonstrate that PoG-EGP significantly improves over the baseline PoG system and other compared methods.", "paper_id": "P0175", "citations": ["Xu2025Exemplar"], "provenance": {"evidence_level": "abstract", "source": "paper_notes", "pointer": "papers/paper_notes.jsonl:paper_id=P0175#key_results[1]"}}, {"evidence_id": "E-P0209-aa504d163c", "text": "Results show that ProAgent achieves up to 33.4% higher proactive prediction accuracy, 16.8% higher tool-calling F1 score, and notable improvements in user satisfaction over state-of-the-art baselines, marking a significant step toward proactive assistants.", "paper_id": "P0209", "citations": ["Yang2025Proagent"], "provenance": {"evidence_level": "abstract", "source": "paper_notes", "pointer": "papers/paper_notes.jsonl:paper_id=P0209#key_results[0]"}}], "definitions_setup": [{"bullet": "Setup: Which design choices in Agent loop and action spaces drive the major trade-offs, and how are those trade-offs measured? Scope: in-scope: Core topics directly relevant to 'Agent loop and action spaces'.. Axes: evaluation protocol (datasets, metrics, human evaluation); compute and latency constraints; tool interface contract (schemas / protocols); tool selection / routing policy; sandboxing / permissions / observability.", "citations": ["Zhang2026Evoroute", "Kim2025Bridging", "Feng2025Group"]}], "claim_candidates": [{"claim": "Comprehensive evaluations on multiple benchmarks show that A-MemGuard effectively cuts attack success rates by over 95% while incurring a minimal utility cost.", "evidence_field": "evidence_snippet", "citations": ["Wei2025Memguard"]}, {"claim": "We introduce Structured Cognitive Loop (SCL), a modular architecture that explicitly separates agent cognition into five phases: Retrieval, Cognition, Control, Action, and Memory (R-CCAM).", "evidence_field": "evidence_snippet", "citations": ["Kim2025Bridging"]}, {"claim": "To overcome these limitations, LLM-based human-agent systems (LLM-HAS) incorporate human-provided information, feedback, or control into the agent system to enhance system performance, reliability and safety.", "evidence_field": "evidence_snippet", "citations": ["Zou2025Based"]}, {"claim": "Extensive experiments across six benchmarks, covering the diverse scenarios of web, embodied, tool use and game applications, show that AgentSquare substantially outperforms hand-crafted agents, achieving an average performance gain of 17.2% against best-known human designs.", "evidence_field": "evidence_snippet", "citations": ["Shang2024Agentsquare"]}, {"claim": "Evaluated across a comprehensive set of seven benchmarks spanning embodied, math, web, tool, and game domains, AgentSwift discovers agents that achieve an average performance gain of 8.34\\% over both existing automated agent search methods and manually designed agents.", "evidence_field": "evidence_snippet", "citations": ["Li2025Agentswift"]}], "concrete_comparisons": [{"axis": "evaluation protocol (datasets, metrics, human evaluation)", "A_label": "Agent frameworks / architectures", "B_label": "Planning / reasoning loops", "A_papers": "Agent frameworks / architectures: `P0124`, `P0013`, `P0029`", "B_papers": "Planning / reasoning loops: `P0013`, `P0049`, `P0055`", "A_highlights": [{"paper_id": "P0045", "evidence_id": "E-P0045-7219ca15f4", "excerpt": "On evaluation, we examine over 190 benchmark datasets and trace a shift from static exams toward process- and discovery-oriented assessments with advanced evaluation protocols.", "citations": ["Hu2025Survey"], "pointer": "papers/paper_notes.jsonl:paper_id=P0045#key_results[1]", "score": 3}, {"paper_id": "P0049", "evidence_id": "E-P0049-904ba35500", "excerpt": "Evaluated across a comprehensive set of seven benchmarks spanning embodied, math, web, tool, and game domains, AgentSwift discovers agents that achieve an average performance gain of 8.34\\% over both existing automated agent search methods and manually designed agents.", "citations": ["Li2025Agentswift"], "pointer": "papers/paper_notes.jsonl:paper_id=P0049#key_results[0]", "score": 2}], "B_highlights": [{"paper_id": "P0037", "evidence_id": "E-P0037-38a26e4777", "excerpt": "Extensive experiments across six benchmarks, covering the diverse scenarios of web, embodied, tool use and game applications, show that AgentSquare substantially outperforms hand-crafted agents, achieving an average performance gain of 17.2% against best-known human designs.", "citations": ["Shang2024Agentsquare"], "pointer": "papers/paper_notes.jsonl:paper_id=P0037#key_results[0]", "score": 2}, {"paper_id": "P0049", "evidence_id": "E-P0049-904ba35500", "excerpt": "Evaluated across a comprehensive set of seven benchmarks spanning embodied, math, web, tool, and game domains, AgentSwift discovers agents that achieve an average performance gain of 8.34\\% over both existing automated agent search methods and manually designed agents.", "citations": ["Li2025Agentswift"], "pointer": "papers/paper_notes.jsonl:paper_id=P0049#key_results[0]", "score": 2}], "write_prompt": "Contrast Agent frameworks / architectures vs Planning / reasoning loops along \"evaluation protocol (datasets, metrics, human evaluation)\". Ground A and B using the highlight snippets; do not introduce new claims beyond the cited evidence.", "evidence_field": "evaluation protocol (datasets, metrics, human evaluation)", "citations": ["Hu2025Survey", "Li2025Agentswift", "Shang2024Agentsquare"]}, {"axis": "evaluation protocol (datasets, metrics, human evaluation)", "A_label": "Agent frameworks / architectures", "B_label": "Memory / retrieval augmentation", "A_papers": "Agent frameworks / architectures: `P0124`, `P0013`, `P0029`", "B_papers": "Memory / retrieval augmentation: `P0047`, `P0201`, `P0203`", "A_highlights": [{"paper_id": "P0045", "evidence_id": "E-P0045-7219ca15f4", "excerpt": "On evaluation, we examine over 190 benchmark datasets and trace a shift from static exams toward process- and discovery-oriented assessments with advanced evaluation protocols.", "citations": ["Hu2025Survey"], "pointer": "papers/paper_notes.jsonl:paper_id=P0045#key_results[1]", "score": 3}, {"paper_id": "P0049", "evidence_id": "E-P0049-904ba35500", "excerpt": "Evaluated across a comprehensive set of seven benchmarks spanning embodied, math, web, tool, and game domains, AgentSwift discovers agents that achieve an average performance gain of 8.34\\% over both existing automated agent search methods and manually designed agents.", "citations": ["Li2025Agentswift"], "pointer": "papers/paper_notes.jsonl:paper_id=P0049#key_results[0]", "score": 2}], "B_highlights": [{"paper_id": "P0203", "evidence_id": "E-P0203-e294aeefb5", "excerpt": "To evaluate MuaLLM, we introduce two custom datasets: RAG-250, targeting retrieval and citation performance, and Reasoning-100 (Reas-100), focused on multistep reasoning in circuit design.", "citations": ["Abbineni2025Muallm"], "pointer": "papers/paper_notes.jsonl:paper_id=P0203#key_results[1]", "score": 1}, {"paper_id": "P0047", "evidence_id": "E-P0047-cb9370ac71", "excerpt": "Comprehensive evaluations on multiple benchmarks show that A-MemGuard effectively cuts attack success rates by over 95% while incurring a minimal utility cost.", "citations": ["Wei2025Memguard"], "pointer": "papers/paper_notes.jsonl:paper_id=P0047#key_results[0]", "score": 1}], "write_prompt": "Contrast Agent frameworks / architectures vs Memory / retrieval augmentation along \"evaluation protocol (datasets, metrics, human evaluation)\". Ground A and B using the highlight snippets; do not introduce new claims beyond the cited evidence.", "evidence_field": "evaluation protocol (datasets, metrics, human evaluation)", "citations": ["Hu2025Survey", "Li2025Agentswift", "Abbineni2025Muallm", "Wei2025Memguard"]}, {"axis": "evaluation protocol (datasets, metrics, human evaluation)", "A_label": "Planning / reasoning loops", "B_label": "Memory / retrieval augmentation", "A_papers": "Planning / reasoning loops: `P0013`, `P0049`, `P0055`", "B_papers": "Memory / retrieval augmentation: `P0047`, `P0201`, `P0203`", "A_highlights": [{"paper_id": "P0037", "evidence_id": "E-P0037-38a26e4777", "excerpt": "Extensive experiments across six benchmarks, covering the diverse scenarios of web, embodied, tool use and game applications, show that AgentSquare substantially outperforms hand-crafted agents, achieving an average performance gain of 17.2% against best-known human designs.", "citations": ["Shang2024Agentsquare"], "pointer": "papers/paper_notes.jsonl:paper_id=P0037#key_results[0]", "score": 2}, {"paper_id": "P0049", "evidence_id": "E-P0049-904ba35500", "excerpt": "Evaluated across a comprehensive set of seven benchmarks spanning embodied, math, web, tool, and game domains, AgentSwift discovers agents that achieve an average performance gain of 8.34\\% over both existing automated agent search methods and manually designed agents.", "citations": ["Li2025Agentswift"], "pointer": "papers/paper_notes.jsonl:paper_id=P0049#key_results[0]", "score": 2}], "B_highlights": [{"paper_id": "P0203", "evidence_id": "E-P0203-e294aeefb5", "excerpt": "To evaluate MuaLLM, we introduce two custom datasets: RAG-250, targeting retrieval and citation performance, and Reasoning-100 (Reas-100), focused on multistep reasoning in circuit design.", "citations": ["Abbineni2025Muallm"], "pointer": "papers/paper_notes.jsonl:paper_id=P0203#key_results[1]", "score": 1}, {"paper_id": "P0047", "evidence_id": "E-P0047-cb9370ac71", "excerpt": "Comprehensive evaluations on multiple benchmarks show that A-MemGuard effectively cuts attack success rates by over 95% while incurring a minimal utility cost.", "citations": ["Wei2025Memguard"], "pointer": "papers/paper_notes.jsonl:paper_id=P0047#key_results[0]", "score": 1}], "write_prompt": "Contrast Planning / reasoning loops vs Memory / retrieval augmentation along \"evaluation protocol (datasets, metrics, human evaluation)\". Ground A and B using the highlight snippets; do not introduce new claims beyond the cited evidence.", "evidence_field": "evaluation protocol (datasets, metrics, human evaluation)", "citations": ["Shang2024Agentsquare", "Li2025Agentswift", "Abbineni2025Muallm", "Wei2025Memguard"]}, {"axis": "compute and latency constraints", "A_label": "Agent frameworks / architectures", "B_label": "Planning / reasoning loops", "A_papers": "Agent frameworks / architectures: `P0124`, `P0013`, `P0029`", "B_papers": "Planning / reasoning loops: `P0013`, `P0049`, `P0055`", "A_highlights": [{"paper_id": "P0124", "evidence_id": "E-P0124-60cc0d458f", "excerpt": "Experiments on challenging agentic benchmarks such as GAIA and BrowseComp+ demonstrate that EvoRoute, when integrated into off-the-shelf agentic systems, not only sustains or enhances system performance but also reduces execution cost by up to $80\\%$ and latency by over $70\\%$.", "citations": ["Zhang2026Evoroute"], "pointer": "papers/paper_notes.jsonl:paper_id=P0124#key_results[0]", "score": 2}, {"paper_id": "P0047", "evidence_id": "E-P0047-cb9370ac71", "excerpt": "Comprehensive evaluations on multiple benchmarks show that A-MemGuard effectively cuts attack success rates by over 95% while incurring a minimal utility cost.", "citations": ["Wei2025Memguard"], "pointer": "papers/paper_notes.jsonl:paper_id=P0047#key_results[0]", "score": 1}], "B_highlights": [{"paper_id": "P0037", "evidence_id": "E-P0037-38a26e4777", "excerpt": "Extensive experiments across six benchmarks, covering the diverse scenarios of web, embodied, tool use and game applications, show that AgentSquare substantially outperforms hand-crafted agents, achieving an average performance gain of 17.2% against best-known human designs.", "citations": ["Shang2024Agentsquare"], "pointer": "papers/paper_notes.jsonl:paper_id=P0037#key_results[0]", "score": 0}, {"paper_id": "P0049", "evidence_id": "E-P0049-904ba35500", "excerpt": "Evaluated across a comprehensive set of seven benchmarks spanning embodied, math, web, tool, and game domains, AgentSwift discovers agents that achieve an average performance gain of 8.34\\% over both existing automated agent search methods and manually designed agents.", "citations": ["Li2025Agentswift"], "pointer": "papers/paper_notes.jsonl:paper_id=P0049#key_results[0]", "score": 0}], "write_prompt": "Contrast Agent frameworks / architectures vs Planning / reasoning loops along \"compute and latency constraints\". Ground A and B using the highlight snippets; do not introduce new claims beyond the cited evidence.", "evidence_field": "compute and latency constraints", "citations": ["Zhang2026Evoroute", "Wei2025Memguard", "Shang2024Agentsquare", "Li2025Agentswift"]}, {"axis": "compute and latency constraints", "A_label": "Agent frameworks / architectures", "B_label": "Memory / retrieval augmentation", "A_papers": "Agent frameworks / architectures: `P0124`, `P0013`, `P0029`", "B_papers": "Memory / retrieval augmentation: `P0047`, `P0201`, `P0203`", "A_highlights": [{"paper_id": "P0124", "evidence_id": "E-P0124-60cc0d458f", "excerpt": "Experiments on challenging agentic benchmarks such as GAIA and BrowseComp+ demonstrate that EvoRoute, when integrated into off-the-shelf agentic systems, not only sustains or enhances system performance but also reduces execution cost by up to $80\\%$ and latency by over $70\\%$.", "citations": ["Zhang2026Evoroute"], "pointer": "papers/paper_notes.jsonl:paper_id=P0124#key_results[0]", "score": 2}, {"paper_id": "P0047", "evidence_id": "E-P0047-cb9370ac71", "excerpt": "Comprehensive evaluations on multiple benchmarks show that A-MemGuard effectively cuts attack success rates by over 95% while incurring a minimal utility cost.", "citations": ["Wei2025Memguard"], "pointer": "papers/paper_notes.jsonl:paper_id=P0047#key_results[0]", "score": 1}], "B_highlights": [{"paper_id": "P0047", "evidence_id": "E-P0047-cb9370ac71", "excerpt": "Comprehensive evaluations on multiple benchmarks show that A-MemGuard effectively cuts attack success rates by over 95% while incurring a minimal utility cost.", "citations": ["Wei2025Memguard"], "pointer": "papers/paper_notes.jsonl:paper_id=P0047#key_results[0]", "score": 1}, {"paper_id": "P0203", "evidence_id": "E-P0203-e294aeefb5", "excerpt": "To evaluate MuaLLM, we introduce two custom datasets: RAG-250, targeting retrieval and citation performance, and Reasoning-100 (Reas-100), focused on multistep reasoning in circuit design.", "citations": ["Abbineni2025Muallm"], "pointer": "papers/paper_notes.jsonl:paper_id=P0203#key_results[1]", "score": 0}], "write_prompt": "Contrast Agent frameworks / architectures vs Memory / retrieval augmentation along \"compute and latency constraints\". Ground A and B using the highlight snippets; do not introduce new claims beyond the cited evidence.", "evidence_field": "compute and latency constraints", "citations": ["Zhang2026Evoroute", "Wei2025Memguard", "Abbineni2025Muallm"]}, {"axis": "compute and latency constraints", "A_label": "Planning / reasoning loops", "B_label": "Memory / retrieval augmentation", "A_papers": "Planning / reasoning loops: `P0013`, `P0049`, `P0055`", "B_papers": "Memory / retrieval augmentation: `P0047`, `P0201`, `P0203`", "A_highlights": [{"paper_id": "P0037", "evidence_id": "E-P0037-38a26e4777", "excerpt": "Extensive experiments across six benchmarks, covering the diverse scenarios of web, embodied, tool use and game applications, show that AgentSquare substantially outperforms hand-crafted agents, achieving an average performance gain of 17.2% against best-known human designs.", "citations": ["Shang2024Agentsquare"], "pointer": "papers/paper_notes.jsonl:paper_id=P0037#key_results[0]", "score": 0}, {"paper_id": "P0049", "evidence_id": "E-P0049-904ba35500", "excerpt": "Evaluated across a comprehensive set of seven benchmarks spanning embodied, math, web, tool, and game domains, AgentSwift discovers agents that achieve an average performance gain of 8.34\\% over both existing automated agent search methods and manually designed agents.", "citations": ["Li2025Agentswift"], "pointer": "papers/paper_notes.jsonl:paper_id=P0049#key_results[0]", "score": 0}], "B_highlights": [{"paper_id": "P0047", "evidence_id": "E-P0047-cb9370ac71", "excerpt": "Comprehensive evaluations on multiple benchmarks show that A-MemGuard effectively cuts attack success rates by over 95% while incurring a minimal utility cost.", "citations": ["Wei2025Memguard"], "pointer": "papers/paper_notes.jsonl:paper_id=P0047#key_results[0]", "score": 1}, {"paper_id": "P0203", "evidence_id": "E-P0203-e294aeefb5", "excerpt": "To evaluate MuaLLM, we introduce two custom datasets: RAG-250, targeting retrieval and citation performance, and Reasoning-100 (Reas-100), focused on multistep reasoning in circuit design.", "citations": ["Abbineni2025Muallm"], "pointer": "papers/paper_notes.jsonl:paper_id=P0203#key_results[1]", "score": 0}], "write_prompt": "Contrast Planning / reasoning loops vs Memory / retrieval augmentation along \"compute and latency constraints\". Ground A and B using the highlight snippets; do not introduce new claims beyond the cited evidence.", "evidence_field": "compute and latency constraints", "citations": ["Shang2024Agentsquare", "Li2025Agentswift", "Wei2025Memguard", "Abbineni2025Muallm"]}, {"axis": "tool interface contract (schemas / protocols)", "A_label": "Agent frameworks / architectures", "B_label": "Planning / reasoning loops", "A_papers": "Agent frameworks / architectures: `P0124`, `P0013`, `P0029`", "B_papers": "Planning / reasoning loops: `P0013`, `P0049`, `P0055`", "A_highlights": [{"paper_id": "P0049", "evidence_id": "E-P0049-904ba35500", "excerpt": "Evaluated across a comprehensive set of seven benchmarks spanning embodied, math, web, tool, and game domains, AgentSwift discovers agents that achieve an average performance gain of 8.34\\% over both existing automated agent search methods and manually designed agents.", "citations": ["Li2025Agentswift"], "pointer": "papers/paper_notes.jsonl:paper_id=P0049#key_results[0]", "score": 1}, {"paper_id": "P0029", "evidence_id": "E-P0029-4b027dfb27", "excerpt": "We evaluate GiGPO on challenging agent benchmarks, including ALFWorld and WebShop, as well as tool-integrated reasoning on search-augmented QA tasks, using Qwen2.5-1.5B/3B/7B-Instruct.", "citations": ["Feng2025Group"], "pointer": "papers/paper_notes.jsonl:paper_id=P0029#key_results[0]", "score": 1}], "B_highlights": [{"paper_id": "P0037", "evidence_id": "E-P0037-38a26e4777", "excerpt": "Extensive experiments across six benchmarks, covering the diverse scenarios of web, embodied, tool use and game applications, show that AgentSquare substantially outperforms hand-crafted agents, achieving an average performance gain of 17.2% against best-known human designs.", "citations": ["Shang2024Agentsquare"], "pointer": "papers/paper_notes.jsonl:paper_id=P0037#key_results[0]", "score": 1}, {"paper_id": "P0049", "evidence_id": "E-P0049-904ba35500", "excerpt": "Evaluated across a comprehensive set of seven benchmarks spanning embodied, math, web, tool, and game domains, AgentSwift discovers agents that achieve an average performance gain of 8.34\\% over both existing automated agent search methods and manually designed agents.", "citations": ["Li2025Agentswift"], "pointer": "papers/paper_notes.jsonl:paper_id=P0049#key_results[0]", "score": 1}], "write_prompt": "Contrast Agent frameworks / architectures vs Planning / reasoning loops along \"tool interface contract (schemas / protocols)\". Ground A and B using the highlight snippets; do not introduce new claims beyond the cited evidence.", "evidence_field": "tool interface contract (schemas / protocols)", "citations": ["Li2025Agentswift", "Feng2025Group", "Shang2024Agentsquare"]}, {"axis": "tool interface contract (schemas / protocols)", "A_label": "Agent frameworks / architectures", "B_label": "Memory / retrieval augmentation", "A_papers": "Agent frameworks / architectures: `P0124`, `P0013`, `P0029`", "B_papers": "Memory / retrieval augmentation: `P0047`, `P0201`, `P0203`", "A_highlights": [{"paper_id": "P0049", "evidence_id": "E-P0049-904ba35500", "excerpt": "Evaluated across a comprehensive set of seven benchmarks spanning embodied, math, web, tool, and game domains, AgentSwift discovers agents that achieve an average performance gain of 8.34\\% over both existing automated agent search methods and manually designed agents.", "citations": ["Li2025Agentswift"], "pointer": "papers/paper_notes.jsonl:paper_id=P0049#key_results[0]", "score": 1}, {"paper_id": "P0029", "evidence_id": "E-P0029-4b027dfb27", "excerpt": "We evaluate GiGPO on challenging agent benchmarks, including ALFWorld and WebShop, as well as tool-integrated reasoning on search-augmented QA tasks, using Qwen2.5-1.5B/3B/7B-Instruct.", "citations": ["Feng2025Group"], "pointer": "papers/paper_notes.jsonl:paper_id=P0029#key_results[0]", "score": 1}], "B_highlights": [{"paper_id": "P0203", "evidence_id": "E-P0203-e294aeefb5", "excerpt": "To evaluate MuaLLM, we introduce two custom datasets: RAG-250, targeting retrieval and citation performance, and Reasoning-100 (Reas-100), focused on multistep reasoning in circuit design.", "citations": ["Abbineni2025Muallm"], "pointer": "papers/paper_notes.jsonl:paper_id=P0203#key_results[1]", "score": 0}, {"paper_id": "P0047", "evidence_id": "E-P0047-cb9370ac71", "excerpt": "Comprehensive evaluations on multiple benchmarks show that A-MemGuard effectively cuts attack success rates by over 95% while incurring a minimal utility cost.", "citations": ["Wei2025Memguard"], "pointer": "papers/paper_notes.jsonl:paper_id=P0047#key_results[0]", "score": 0}], "write_prompt": "Contrast Agent frameworks / architectures vs Memory / retrieval augmentation along \"tool interface contract (schemas / protocols)\". Ground A and B using the highlight snippets; do not introduce new claims beyond the cited evidence.", "evidence_field": "tool interface contract (schemas / protocols)", "citations": ["Li2025Agentswift", "Feng2025Group", "Abbineni2025Muallm", "Wei2025Memguard"]}, {"axis": "tool interface contract (schemas / protocols)", "A_label": "Planning / reasoning loops", "B_label": "Memory / retrieval augmentation", "A_papers": "Planning / reasoning loops: `P0013`, `P0049`, `P0055`", "B_papers": "Memory / retrieval augmentation: `P0047`, `P0201`, `P0203`", "A_highlights": [{"paper_id": "P0037", "evidence_id": "E-P0037-38a26e4777", "excerpt": "Extensive experiments across six benchmarks, covering the diverse scenarios of web, embodied, tool use and game applications, show that AgentSquare substantially outperforms hand-crafted agents, achieving an average performance gain of 17.2% against best-known human designs.", "citations": ["Shang2024Agentsquare"], "pointer": "papers/paper_notes.jsonl:paper_id=P0037#key_results[0]", "score": 1}, {"paper_id": "P0049", "evidence_id": "E-P0049-904ba35500", "excerpt": "Evaluated across a comprehensive set of seven benchmarks spanning embodied, math, web, tool, and game domains, AgentSwift discovers agents that achieve an average performance gain of 8.34\\% over both existing automated agent search methods and manually designed agents.", "citations": ["Li2025Agentswift"], "pointer": "papers/paper_notes.jsonl:paper_id=P0049#key_results[0]", "score": 1}], "B_highlights": [{"paper_id": "P0203", "evidence_id": "E-P0203-e294aeefb5", "excerpt": "To evaluate MuaLLM, we introduce two custom datasets: RAG-250, targeting retrieval and citation performance, and Reasoning-100 (Reas-100), focused on multistep reasoning in circuit design.", "citations": ["Abbineni2025Muallm"], "pointer": "papers/paper_notes.jsonl:paper_id=P0203#key_results[1]", "score": 0}, {"paper_id": "P0047", "evidence_id": "E-P0047-cb9370ac71", "excerpt": "Comprehensive evaluations on multiple benchmarks show that A-MemGuard effectively cuts attack success rates by over 95% while incurring a minimal utility cost.", "citations": ["Wei2025Memguard"], "pointer": "papers/paper_notes.jsonl:paper_id=P0047#key_results[0]", "score": 0}], "write_prompt": "Contrast Planning / reasoning loops vs Memory / retrieval augmentation along \"tool interface contract (schemas / protocols)\". Ground A and B using the highlight snippets; do not introduce new claims beyond the cited evidence.", "evidence_field": "tool interface contract (schemas / protocols)", "citations": ["Shang2024Agentsquare", "Li2025Agentswift", "Abbineni2025Muallm", "Wei2025Memguard"]}, {"axis": "tool selection / routing policy", "A_label": "Agent frameworks / architectures", "B_label": "Planning / reasoning loops", "A_papers": "Agent frameworks / architectures: `P0124`, `P0013`, `P0029`", "B_papers": "Planning / reasoning loops: `P0013`, `P0049`, `P0055`", "A_highlights": [{"paper_id": "P0049", "evidence_id": "E-P0049-904ba35500", "excerpt": "Evaluated across a comprehensive set of seven benchmarks spanning embodied, math, web, tool, and game domains, AgentSwift discovers agents that achieve an average performance gain of 8.34\\% over both existing automated agent search methods and manually designed agents.", "citations": ["Li2025Agentswift"], "pointer": "papers/paper_notes.jsonl:paper_id=P0049#key_results[0]", "score": 1}, {"paper_id": "P0029", "evidence_id": "E-P0029-4b027dfb27", "excerpt": "We evaluate GiGPO on challenging agent benchmarks, including ALFWorld and WebShop, as well as tool-integrated reasoning on search-augmented QA tasks, using Qwen2.5-1.5B/3B/7B-Instruct.", "citations": ["Feng2025Group"], "pointer": "papers/paper_notes.jsonl:paper_id=P0029#key_results[0]", "score": 1}], "B_highlights": [{"paper_id": "P0037", "evidence_id": "E-P0037-38a26e4777", "excerpt": "Extensive experiments across six benchmarks, covering the diverse scenarios of web, embodied, tool use and game applications, show that AgentSquare substantially outperforms hand-crafted agents, achieving an average performance gain of 17.2% against best-known human designs.", "citations": ["Shang2024Agentsquare"], "pointer": "papers/paper_notes.jsonl:paper_id=P0037#key_results[0]", "score": 1}, {"paper_id": "P0049", "evidence_id": "E-P0049-904ba35500", "excerpt": "Evaluated across a comprehensive set of seven benchmarks spanning embodied, math, web, tool, and game domains, AgentSwift discovers agents that achieve an average performance gain of 8.34\\% over both existing automated agent search methods and manually designed agents.", "citations": ["Li2025Agentswift"], "pointer": "papers/paper_notes.jsonl:paper_id=P0049#key_results[0]", "score": 1}], "write_prompt": "Contrast Agent frameworks / architectures vs Planning / reasoning loops along \"tool selection / routing policy\". Ground A and B using the highlight snippets; do not introduce new claims beyond the cited evidence.", "evidence_field": "tool selection / routing policy", "citations": ["Li2025Agentswift", "Feng2025Group", "Shang2024Agentsquare"]}], "evaluation_protocol": [{"bullet": "Evaluation mentions include: LLMs, GAIA, EvoRoute, BrowseComp, SCL, CCAM, GPT-4o-powered, ReAct, AutoGPT, ALFWorld.", "citations": ["Zhang2026Evoroute", "Kim2025Bridging", "Feng2025Group", "Hu2025Survey"]}, {"bullet": "When comparing results, anchor the paragraph with: task type + metric + constraint (budget, tool access, horizon, or threat model) when stated.", "citations": ["Zhang2026Evoroute", "Kim2025Bridging"]}, {"bullet": "Prefer head-to-head comparisons only when benchmark/metric are shared; otherwise frame differences as protocol-driven rather than method superiority.", "citations": ["Zhang2026Evoroute", "Kim2025Bridging"]}, {"bullet": "Avoid underspecified model/baseline naming; if abstracts omit details, state that the baseline is reported but underspecified instead of guessing.", "citations": ["Zhang2026Evoroute", "Kim2025Bridging"]}, {"bullet": "If a claim relies on a single reported number, pair it with a limitation/caveat from the same evidence so the draft remains conservative.", "citations": ["Zhang2026Evoroute", "Kim2025Bridging"]}, {"bullet": "If budgets or environments differ across papers, treat cross-paper numeric comparison as fragile and prefer qualitative contrasts aligned to the subsection axes.", "citations": ["Zhang2026Evoroute", "Kim2025Bridging"]}], "failures_limitations": [{"bullet": "We formalize this challenge as the \\textbf{Agent System Trilemma}: the inherent tension among achieving state-of-the-art performance, minimizing monetary cost, and ensuring rapid task completion.", "citations": ["Zhang2026Evoroute"]}, {"bullet": "However, this reliance on memory introduces a critical security risk: an adversary can inject seemingly harmless records into an agent's memory to manipulate its future behavior.", "citations": ["Wei2025Memguard"]}, {"bullet": "Comprehensive evaluations on multiple benchmarks show that A-MemGuard effectively cuts attack success rates by over 95% while incurring a minimal utility cost.", "citations": ["Wei2025Memguard"]}, {"bullet": "This work shifts LLM memory security from static filtering to a proactive, experience-driven model where defenses strengthen over time.", "citations": ["Wei2025Memguard"]}, {"bullet": "Large language model (LLM) agents have demonstrated strong capabilities across diverse domains, yet automated agent design remains a significant challenge.", "citations": ["Li2025Agentswift"]}, {"bullet": "To address these limitations, we propose a novel framework, Exemplar-Guided Planning (EGP), which enhances the planning capabilities of LLM agents for KGQA.", "citations": ["Xu2025Exemplar"]}, {"bullet": "Still, limitations in their training prevent them from answering accurately in scenarios that could benefit from multiple perspectives.", "citations": ["Inoue2024Drugagent"]}, {"bullet": "MPR (i) externalizes reusable corrective knowledge without model weight updates, (ii) enforces domain constraints to reduce unsafe or invalid actions, and (iii) retains the adaptability of language-based reflection.", "citations": ["Wu2025Meta"]}], "blocking_missing": [], "verify_fields": ["named benchmarks/datasets used", "metrics/human-eval protocol", "compute/training/inference cost", "training data and supervision signal", "baseline choices and ablation evidence"], "generated_at": "2026-01-25T17:06:54", "section_id": "3", "section_title": "Foundations & Interfaces"}
{"sub_id": "3.2", "title": "Tool interfaces and orchestration", "evidence_ids": ["E-P0192-fae121f81b", "E-P0229-92b9d774cc", "E-P0196-3c65d38a2a", "E-P0022-b6b7af5a81", "E-P0214-2e6956a116", "E-P0083-35271418ac", "E-P0081-419e1464da", "E-P0108-9640816b42", "E-P0009-e0bcca0d9e", "E-P0282-cd97392159", "E-P0197-3a4792de2b", "E-P0001-ca4a00b5cf", "E-P0016-dc2266b72d", "E-P0096-3575a7c673", "E-P0242-7cd49652d3", "E-P0026-f0ea009256", "E-P0010-793979aed4", "E-P0230-15e523063d", "E-P0054-d3344c79dc", "E-P0107-e483f51daa", "E-P0108-8158d9909c", "E-P0138-da6dd163cf", "E-P0229-877da0da9f", "E-P0230-171b93237a"], "evidence_level_summary": {"fulltext": 0, "abstract": 28, "title": 0}, "evidence_snippets": [{"evidence_id": "E-P0192-fae121f81b", "text": "Our evaluation of 66 real-world tools from the repositories of two major LLM agent development frameworks, LangChain and LlamaIndex, revealed a significant security concern: 75% are vulnerable to XTHP attacks, highlighting the prevalence of this threat.", "paper_id": "P0192", "citations": ["Li2025Dissonances"], "provenance": {"evidence_level": "abstract", "source": "paper_notes", "pointer": "papers/paper_notes.jsonl:paper_id=P0192#key_results[0]"}}, {"evidence_id": "E-P0229-92b9d774cc", "text": "To evaluate different autonomy levels, we propose four LLM paradigms: (1) centralized cooperation, where a single LLM allocates tools to all agents; (2) centralized self-organization, where a central LLM autonomously activates agents while keeping others inactive; (3) decentralized cooperation, where each agent has its own LLM and calls tools based on local information; and (4) self-organization, where a randomly chosen initial agent can request collaboration, activating additional agents via tool calls.", "paper_id": "P0229", "citations": ["Zhang2025Tool"], "provenance": {"evidence_level": "abstract", "source": "paper_notes", "pointer": "papers/paper_notes.jsonl:paper_id=P0229#method"}}, {"evidence_id": "E-P0196-3c65d38a2a", "text": "Through extensive evaluation of leading LLMs, we find that even SOTA models such as GPT-5 (43.72%), Grok-4 (33.33%) and Claude-4.0-Sonnet (29.44%) exhibit significant performance limitations.", "paper_id": "P0196", "citations": ["Luo2025Universe"], "provenance": {"evidence_level": "abstract", "source": "paper_notes", "pointer": "papers/paper_notes.jsonl:paper_id=P0196#limitations[1]"}}, {"evidence_id": "E-P0022-b6b7af5a81", "text": "Across six LLMs on the ToolBench and BFCL benchmarks, our attack expands tasks into trajectories exceeding 60,000 tokens, inflates costs by up to 658x, and raises energy by 100-560x.", "paper_id": "P0022", "citations": ["Zhou2026Beyond"], "provenance": {"evidence_level": "abstract", "source": "paper_notes", "pointer": "papers/paper_notes.jsonl:paper_id=P0022#key_results[0]"}}, {"evidence_id": "E-P0214-2e6956a116", "text": "Evaluation on two existing multi-turn tool-use agent benchmarks, M3ToolEval and TauBench, shows the Self-Challenging framework achieves over a two-fold improvement in Llama-3.1-8B-Instruct, despite using only self-generated training data.", "paper_id": "P0214", "citations": ["Zhou2025Self"], "provenance": {"evidence_level": "abstract", "source": "paper_notes", "pointer": "papers/paper_notes.jsonl:paper_id=P0214#key_results[0]"}}, {"evidence_id": "E-P0083-35271418ac", "text": "Evaluating each MemTool mode across 13+ LLMs on the ScaleMCP benchmark, we conducted experiments over 100 consecutive user interactions, measuring tool removal ratios (short-term memory efficiency) and task completion accuracy.", "paper_id": "P0083", "citations": ["Lumer2025Memtool"], "provenance": {"evidence_level": "abstract", "source": "paper_notes", "pointer": "papers/paper_notes.jsonl:paper_id=P0083#key_results[0]"}}, {"evidence_id": "E-P0081-419e1464da", "text": "MCP-RADAR features a challenging dataset of 507 tasks spanning six domains: mathematical reasoning, web search, email, calendar, file management, and terminal operations.", "paper_id": "P0081", "citations": ["Gao2025Radar"], "provenance": {"evidence_level": "abstract", "source": "paper_notes", "pointer": "papers/paper_notes.jsonl:paper_id=P0081#key_results[0]"}}, {"evidence_id": "E-P0108-9640816b42", "text": "Evaluation across various tool-use benchmarks illustrates that our proposed multi-LLM framework surpasses the traditional single-LLM approach, highlighting its efficacy and advantages in tool learning.", "paper_id": "P0108", "citations": ["Shen2024Small"], "provenance": {"evidence_level": "abstract", "source": "paper_notes", "pointer": "papers/paper_notes.jsonl:paper_id=P0108#key_results[1]"}}, {"evidence_id": "E-P0009-e0bcca0d9e", "text": "Furthermore, we evaluated current benchmarks and assessment protocols and have provided an analysis of 68 publicly available datasets to assess the performance of LLM-based agents in various tasks.", "paper_id": "P0009", "citations": ["Chowa2025From"], "provenance": {"evidence_level": "abstract", "source": "paper_notes", "pointer": "papers/paper_notes.jsonl:paper_id=P0009#key_results[0]"}}, {"evidence_id": "E-P0282-cd97392159", "text": "To facilitate the evaluation of the model's capability, we collect a dataset featuring multi-modal input tools from HuggingFace.", "paper_id": "P0282", "citations": ["Wang2024Mllm"], "provenance": {"evidence_level": "abstract", "source": "paper_notes", "pointer": "papers/paper_notes.jsonl:paper_id=P0282#key_results[0]"}}, {"evidence_id": "E-P0197-3a4792de2b", "text": "Current MCP evaluation sets suffer from issues such as reliance on external MCP services and a lack of difficulty awareness.", "paper_id": "P0197", "citations": ["Liu2025Mcpagentbench"], "provenance": {"evidence_level": "abstract", "source": "paper_notes", "pointer": "papers/paper_notes.jsonl:paper_id=P0197#key_results[0]"}}, {"evidence_id": "E-P0001-ca4a00b5cf", "text": "On two interactive decision making benchmarks (ALFWorld and WebShop), ReAct outperforms imitation and reinforcement learning methods by an absolute success rate of 34% and 10% respectively, while being prompted with only one or two in-context examples.", "paper_id": "P0001", "citations": ["Yao2022React"], "provenance": {"evidence_level": "abstract", "source": "paper_notes", "pointer": "papers/paper_notes.jsonl:paper_id=P0001#key_results[0]"}}, {"evidence_id": "E-P0016-dc2266b72d", "text": "Specifically, 1) the taxonomy defines environments/tasks, common LLM-profiled roles or LMPRs (policy models, evaluators, and dynamic models), and universally applicable workflows found in prior work, and 2) it enables a comparison of key perspectives on the implementations of LMPRs and workflow designs across different agent paradigms and frameworks.", "paper_id": "P0016", "citations": ["Li2024Review"], "provenance": {"evidence_level": "abstract", "source": "paper_notes", "pointer": "papers/paper_notes.jsonl:paper_id=P0016#key_results[0]"}}, {"evidence_id": "E-P0096-3575a7c673", "text": "We find AvaTaR consistently outperforms state-of-the-art approaches across all seven tasks, exhibiting strong generalization ability when applied to novel cases and achieving an average relative improvement of 14% on the Hit@1 metric for the retrieval datasets and 13% for the QA datasets.", "paper_id": "P0096", "citations": ["Wu2024Avatar"], "provenance": {"evidence_level": "abstract", "source": "paper_notes", "pointer": "papers/paper_notes.jsonl:paper_id=P0096#key_results[0]"}}, {"evidence_id": "E-P0242-7cd49652d3", "text": "To study this challenge, we constructed TicToc, a diverse dataset of multi-turn user-agent message trajectories across 76 scenarios, spanning dynamic environments with high, medium, and low time sensitivity.", "paper_id": "P0242", "citations": ["Cheng2025Your"], "provenance": {"evidence_level": "abstract", "source": "paper_notes", "pointer": "papers/paper_notes.jsonl:paper_id=P0242#key_results[0]"}}, {"evidence_id": "E-P0026-f0ea009256", "text": "Finally, we summarize the datasets and benchmarks used for evaluation and tuning, review key applications of LLM-based agents, and discuss major challenges and promising future directions.", "paper_id": "P0026", "citations": ["Du2025Survey"], "provenance": {"evidence_level": "abstract", "source": "paper_notes", "pointer": "papers/paper_notes.jsonl:paper_id=P0026#key_results[0]"}}, {"evidence_id": "E-P0010-793979aed4", "text": "We then review datasets, evaluation dimensions, and metrics, highlighting their limitations.", "paper_id": "P0010", "citations": ["Zhang2025Generalizability"], "provenance": {"evidence_level": "abstract", "source": "paper_notes", "pointer": "papers/paper_notes.jsonl:paper_id=P0010#key_results[0]"}}, {"evidence_id": "E-P0230-15e523063d", "text": "Evaluation metrics include standard classification metrics, quality assessment of reasoning processes, and robustness testing against rewritten content.", "paper_id": "P0230", "citations": ["Cui2025Toward"], "provenance": {"evidence_level": "abstract", "source": "paper_notes", "pointer": "papers/paper_notes.jsonl:paper_id=P0230#key_results[1]"}}], "definitions_setup": [{"bullet": "Setup: Which design choices in Tool interfaces and orchestration drive the major trade-offs, and how are those trade-offs measured? Scope: in-scope: Core topics directly relevant to 'Tool interfaces and orchestration'.. Axes: evaluation protocol (datasets, metrics, human evaluation); compute and latency constraints; tool interface contract (schemas / protocols); tool selection / routing policy; sandboxing / permissions / observability.", "citations": ["Zhou2026Beyond", "Chowa2025From", "Zhang2025Generalizability"]}], "claim_candidates": [{"claim": "Our evaluation of 66 real-world tools from the repositories of two major LLM agent development frameworks, LangChain and LlamaIndex, revealed a significant security concern: 75% are vulnerable to XTHP attacks, highlighting the prevalence of this threat.", "evidence_field": "evidence_snippet", "citations": ["Li2025Dissonances"]}, {"claim": "To evaluate different autonomy levels, we propose four LLM paradigms: (1) centralized cooperation, where a single LLM allocates tools to all agents; (2) centralized self-organization, where a central LLM autonomously activates agents while keeping others inactive; (3) decentralized cooperation, where each agent has its own LLM and calls tools based on local information; and (4)", "evidence_field": "evidence_snippet", "citations": ["Zhang2025Tool"]}, {"claim": "Through extensive evaluation of leading LLMs, we find that even SOTA models such as GPT-5 (43.72%), Grok-4 (33.33%) and Claude-4.0-Sonnet (29.44%) exhibit significant performance limitations.", "evidence_field": "evidence_snippet", "citations": ["Luo2025Universe"]}, {"claim": "Across six LLMs on the ToolBench and BFCL benchmarks, our attack expands tasks into trajectories exceeding 60,000 tokens, inflates costs by up to 658x, and raises energy by 100-560x.", "evidence_field": "evidence_snippet", "citations": ["Zhou2026Beyond"]}, {"claim": "Evaluation on two existing multi-turn tool-use agent benchmarks, M3ToolEval and TauBench, shows the Self-Challenging framework achieves over a two-fold improvement in Llama-3.1-8B-Instruct, despite using only self-generated training data.", "evidence_field": "evidence_snippet", "citations": ["Zhou2025Self"]}], "concrete_comparisons": [{"axis": "evaluation protocol (datasets, metrics, human evaluation)", "A_label": "Agent frameworks / architectures", "B_label": "Tool-use and function calling", "A_papers": "Agent frameworks / architectures: `P0022`, `P0009`, `P0010`", "B_papers": "Tool-use and function calling: `P0022`, `P0009`, `P0054`", "A_highlights": [{"paper_id": "P0083", "evidence_id": "E-P0083-35271418ac", "excerpt": "Evaluating each MemTool mode across 13+ LLMs on the ScaleMCP benchmark, we conducted experiments over 100 consecutive user interactions, measuring tool removal ratios (short-term memory efficiency) and task completion accuracy.", "citations": ["Lumer2025Memtool"], "pointer": "papers/paper_notes.jsonl:paper_id=P0083#key_results[0]", "score": 3}, {"paper_id": "P0009", "evidence_id": "E-P0009-e0bcca0d9e", "excerpt": "Furthermore, we evaluated current benchmarks and assessment protocols and have provided an analysis of 68 publicly available datasets to assess the performance of LLM-based agents in various tasks.", "citations": ["Chowa2025From"], "pointer": "papers/paper_notes.jsonl:paper_id=P0009#key_results[0]", "score": 3}], "B_highlights": [{"paper_id": "P0083", "evidence_id": "E-P0083-35271418ac", "excerpt": "Evaluating each MemTool mode across 13+ LLMs on the ScaleMCP benchmark, we conducted experiments over 100 consecutive user interactions, measuring tool removal ratios (short-term memory efficiency) and task completion accuracy.", "citations": ["Lumer2025Memtool"], "pointer": "papers/paper_notes.jsonl:paper_id=P0083#key_results[0]", "score": 3}, {"paper_id": "P0009", "evidence_id": "E-P0009-e0bcca0d9e", "excerpt": "Furthermore, we evaluated current benchmarks and assessment protocols and have provided an analysis of 68 publicly available datasets to assess the performance of LLM-based agents in various tasks.", "citations": ["Chowa2025From"], "pointer": "papers/paper_notes.jsonl:paper_id=P0009#key_results[0]", "score": 3}], "write_prompt": "Contrast Agent frameworks / architectures vs Tool-use and function calling along \"evaluation protocol (datasets, metrics, human evaluation)\". Ground A and B using the highlight snippets; do not introduce new claims beyond the cited evidence.", "evidence_field": "evaluation protocol (datasets, metrics, human evaluation)", "citations": ["Lumer2025Memtool", "Chowa2025From"]}, {"axis": "evaluation protocol (datasets, metrics, human evaluation)", "A_label": "Agent frameworks / architectures", "B_label": "Planning / reasoning loops", "A_papers": "Agent frameworks / architectures: `P0022`, `P0009`, `P0010`", "B_papers": "Planning / reasoning loops: `P0013`, `P0054`, `P0016`", "A_highlights": [{"paper_id": "P0083", "evidence_id": "E-P0083-35271418ac", "excerpt": "Evaluating each MemTool mode across 13+ LLMs on the ScaleMCP benchmark, we conducted experiments over 100 consecutive user interactions, measuring tool removal ratios (short-term memory efficiency) and task completion accuracy.", "citations": ["Lumer2025Memtool"], "pointer": "papers/paper_notes.jsonl:paper_id=P0083#key_results[0]", "score": 3}, {"paper_id": "P0009", "evidence_id": "E-P0009-e0bcca0d9e", "excerpt": "Furthermore, we evaluated current benchmarks and assessment protocols and have provided an analysis of 68 publicly available datasets to assess the performance of LLM-based agents in various tasks.", "citations": ["Chowa2025From"], "pointer": "papers/paper_notes.jsonl:paper_id=P0009#key_results[0]", "score": 3}], "B_highlights": [{"paper_id": "P0096", "evidence_id": "E-P0096-3575a7c673", "excerpt": "We find AvaTaR consistently outperforms state-of-the-art approaches across all seven tasks, exhibiting strong generalization ability when applied to novel cases and achieving an average relative improvement of 14% on the Hit@1 metric for the retrieval datasets and 13% for the", "citations": ["Wu2024Avatar"], "pointer": "papers/paper_notes.jsonl:paper_id=P0096#key_results[0]", "score": 1}, {"paper_id": "P0001", "evidence_id": "E-P0001-ca4a00b5cf", "excerpt": "On two interactive decision making benchmarks (ALFWorld and WebShop), ReAct outperforms imitation and reinforcement learning methods by an absolute success rate of 34% and 10% respectively, while being prompted with only one or two in-context examples.", "citations": ["Yao2022React"], "pointer": "papers/paper_notes.jsonl:paper_id=P0001#key_results[0]", "score": 1}], "write_prompt": "Contrast Agent frameworks / architectures vs Planning / reasoning loops along \"evaluation protocol (datasets, metrics, human evaluation)\". Ground A and B using the highlight snippets; do not introduce new claims beyond the cited evidence.", "evidence_field": "evaluation protocol (datasets, metrics, human evaluation)", "citations": ["Lumer2025Memtool", "Chowa2025From", "Wu2024Avatar", "Yao2022React"]}, {"axis": "evaluation protocol (datasets, metrics, human evaluation)", "A_label": "Tool-use and function calling", "B_label": "Planning / reasoning loops", "A_papers": "Tool-use and function calling: `P0022`, `P0009`, `P0054`", "B_papers": "Planning / reasoning loops: `P0013`, `P0054`, `P0016`", "A_highlights": [{"paper_id": "P0083", "evidence_id": "E-P0083-35271418ac", "excerpt": "Evaluating each MemTool mode across 13+ LLMs on the ScaleMCP benchmark, we conducted experiments over 100 consecutive user interactions, measuring tool removal ratios (short-term memory efficiency) and task completion accuracy.", "citations": ["Lumer2025Memtool"], "pointer": "papers/paper_notes.jsonl:paper_id=P0083#key_results[0]", "score": 3}, {"paper_id": "P0009", "evidence_id": "E-P0009-e0bcca0d9e", "excerpt": "Furthermore, we evaluated current benchmarks and assessment protocols and have provided an analysis of 68 publicly available datasets to assess the performance of LLM-based agents in various tasks.", "citations": ["Chowa2025From"], "pointer": "papers/paper_notes.jsonl:paper_id=P0009#key_results[0]", "score": 3}], "B_highlights": [{"paper_id": "P0096", "evidence_id": "E-P0096-3575a7c673", "excerpt": "We find AvaTaR consistently outperforms state-of-the-art approaches across all seven tasks, exhibiting strong generalization ability when applied to novel cases and achieving an average relative improvement of 14% on the Hit@1 metric for the retrieval datasets and 13% for the", "citations": ["Wu2024Avatar"], "pointer": "papers/paper_notes.jsonl:paper_id=P0096#key_results[0]", "score": 1}, {"paper_id": "P0001", "evidence_id": "E-P0001-ca4a00b5cf", "excerpt": "On two interactive decision making benchmarks (ALFWorld and WebShop), ReAct outperforms imitation and reinforcement learning methods by an absolute success rate of 34% and 10% respectively, while being prompted with only one or two in-context examples.", "citations": ["Yao2022React"], "pointer": "papers/paper_notes.jsonl:paper_id=P0001#key_results[0]", "score": 1}], "write_prompt": "Contrast Tool-use and function calling vs Planning / reasoning loops along \"evaluation protocol (datasets, metrics, human evaluation)\". Ground A and B using the highlight snippets; do not introduce new claims beyond the cited evidence.", "evidence_field": "evaluation protocol (datasets, metrics, human evaluation)", "citations": ["Lumer2025Memtool", "Chowa2025From", "Wu2024Avatar", "Yao2022React"]}, {"axis": "compute and latency constraints", "A_label": "Agent frameworks / architectures", "B_label": "Tool-use and function calling", "A_papers": "Agent frameworks / architectures: `P0022`, `P0009`, `P0010`", "B_papers": "Tool-use and function calling: `P0022`, `P0009`, `P0054`", "A_highlights": [{"paper_id": "P0022", "evidence_id": "E-P0022-b6b7af5a81", "excerpt": "Across six LLMs on the ToolBench and BFCL benchmarks, our attack expands tasks into trajectories exceeding 60,000 tokens, inflates costs by up to 658x, and raises energy by 100-560x.", "citations": ["Zhou2026Beyond"], "pointer": "papers/paper_notes.jsonl:paper_id=P0022#key_results[0]", "score": 1}, {"paper_id": "P0083", "evidence_id": "E-P0083-35271418ac", "excerpt": "Evaluating each MemTool mode across 13+ LLMs on the ScaleMCP benchmark, we conducted experiments over 100 consecutive user interactions, measuring tool removal ratios (short-term memory efficiency) and task completion accuracy.", "citations": ["Lumer2025Memtool"], "pointer": "papers/paper_notes.jsonl:paper_id=P0083#key_results[0]", "score": 0}], "B_highlights": [{"paper_id": "P0022", "evidence_id": "E-P0022-b6b7af5a81", "excerpt": "Across six LLMs on the ToolBench and BFCL benchmarks, our attack expands tasks into trajectories exceeding 60,000 tokens, inflates costs by up to 658x, and raises energy by 100-560x.", "citations": ["Zhou2026Beyond"], "pointer": "papers/paper_notes.jsonl:paper_id=P0022#key_results[0]", "score": 1}, {"paper_id": "P0192", "evidence_id": "E-P0192-fae121f81b", "excerpt": "Our evaluation of 66 real-world tools from the repositories of two major LLM agent development frameworks, LangChain and LlamaIndex, revealed a significant security concern: 75% are vulnerable to XTHP attacks, highlighting the prevalence of this threat.", "citations": ["Li2025Dissonances"], "pointer": "papers/paper_notes.jsonl:paper_id=P0192#key_results[0]", "score": 0}], "write_prompt": "Contrast Agent frameworks / architectures vs Tool-use and function calling along \"compute and latency constraints\". Ground A and B using the highlight snippets; do not introduce new claims beyond the cited evidence.", "evidence_field": "compute and latency constraints", "citations": ["Zhou2026Beyond", "Lumer2025Memtool", "Li2025Dissonances"]}, {"axis": "compute and latency constraints", "A_label": "Agent frameworks / architectures", "B_label": "Planning / reasoning loops", "A_papers": "Agent frameworks / architectures: `P0022`, `P0009`, `P0010`", "B_papers": "Planning / reasoning loops: `P0013`, `P0054`, `P0016`", "A_highlights": [{"paper_id": "P0022", "evidence_id": "E-P0022-b6b7af5a81", "excerpt": "Across six LLMs on the ToolBench and BFCL benchmarks, our attack expands tasks into trajectories exceeding 60,000 tokens, inflates costs by up to 658x, and raises energy by 100-560x.", "citations": ["Zhou2026Beyond"], "pointer": "papers/paper_notes.jsonl:paper_id=P0022#key_results[0]", "score": 1}, {"paper_id": "P0083", "evidence_id": "E-P0083-35271418ac", "excerpt": "Evaluating each MemTool mode across 13+ LLMs on the ScaleMCP benchmark, we conducted experiments over 100 consecutive user interactions, measuring tool removal ratios (short-term memory efficiency) and task completion accuracy.", "citations": ["Lumer2025Memtool"], "pointer": "papers/paper_notes.jsonl:paper_id=P0083#key_results[0]", "score": 0}], "B_highlights": [{"paper_id": "P0016", "evidence_id": "E-P0016-dc2266b72d", "excerpt": "Specifically, 1) the taxonomy defines environments/tasks, common LLM-profiled roles or LMPRs (policy models, evaluators, and dynamic models), and universally applicable workflows found in prior work, and 2) it enables a comparison of key perspectives on the implementations of", "citations": ["Li2024Review"], "pointer": "papers/paper_notes.jsonl:paper_id=P0016#key_results[0]", "score": 0}, {"paper_id": "P0096", "evidence_id": "E-P0096-3575a7c673", "excerpt": "We find AvaTaR consistently outperforms state-of-the-art approaches across all seven tasks, exhibiting strong generalization ability when applied to novel cases and achieving an average relative improvement of 14% on the Hit@1 metric for the retrieval datasets and 13% for the", "citations": ["Wu2024Avatar"], "pointer": "papers/paper_notes.jsonl:paper_id=P0096#key_results[0]", "score": 0}], "write_prompt": "Contrast Agent frameworks / architectures vs Planning / reasoning loops along \"compute and latency constraints\". Ground A and B using the highlight snippets; do not introduce new claims beyond the cited evidence.", "evidence_field": "compute and latency constraints", "citations": ["Zhou2026Beyond", "Lumer2025Memtool", "Li2024Review", "Wu2024Avatar"]}, {"axis": "compute and latency constraints", "A_label": "Tool-use and function calling", "B_label": "Planning / reasoning loops", "A_papers": "Tool-use and function calling: `P0022`, `P0009`, `P0054`", "B_papers": "Planning / reasoning loops: `P0013`, `P0054`, `P0016`", "A_highlights": [{"paper_id": "P0022", "evidence_id": "E-P0022-b6b7af5a81", "excerpt": "Across six LLMs on the ToolBench and BFCL benchmarks, our attack expands tasks into trajectories exceeding 60,000 tokens, inflates costs by up to 658x, and raises energy by 100-560x.", "citations": ["Zhou2026Beyond"], "pointer": "papers/paper_notes.jsonl:paper_id=P0022#key_results[0]", "score": 1}, {"paper_id": "P0192", "evidence_id": "E-P0192-fae121f81b", "excerpt": "Our evaluation of 66 real-world tools from the repositories of two major LLM agent development frameworks, LangChain and LlamaIndex, revealed a significant security concern: 75% are vulnerable to XTHP attacks, highlighting the prevalence of this threat.", "citations": ["Li2025Dissonances"], "pointer": "papers/paper_notes.jsonl:paper_id=P0192#key_results[0]", "score": 0}], "B_highlights": [{"paper_id": "P0016", "evidence_id": "E-P0016-dc2266b72d", "excerpt": "Specifically, 1) the taxonomy defines environments/tasks, common LLM-profiled roles or LMPRs (policy models, evaluators, and dynamic models), and universally applicable workflows found in prior work, and 2) it enables a comparison of key perspectives on the implementations of", "citations": ["Li2024Review"], "pointer": "papers/paper_notes.jsonl:paper_id=P0016#key_results[0]", "score": 0}, {"paper_id": "P0096", "evidence_id": "E-P0096-3575a7c673", "excerpt": "We find AvaTaR consistently outperforms state-of-the-art approaches across all seven tasks, exhibiting strong generalization ability when applied to novel cases and achieving an average relative improvement of 14% on the Hit@1 metric for the retrieval datasets and 13% for the", "citations": ["Wu2024Avatar"], "pointer": "papers/paper_notes.jsonl:paper_id=P0096#key_results[0]", "score": 0}], "write_prompt": "Contrast Tool-use and function calling vs Planning / reasoning loops along \"compute and latency constraints\". Ground A and B using the highlight snippets; do not introduce new claims beyond the cited evidence.", "evidence_field": "compute and latency constraints", "citations": ["Zhou2026Beyond", "Li2025Dissonances", "Li2024Review", "Wu2024Avatar"]}, {"axis": "tool interface contract (schemas / protocols)", "A_label": "Agent frameworks / architectures", "B_label": "Tool-use and function calling", "A_papers": "Agent frameworks / architectures: `P0022`, `P0009`, `P0010`", "B_papers": "Tool-use and function calling: `P0022`, `P0009`, `P0054`", "A_highlights": [{"paper_id": "P0083", "evidence_id": "E-P0083-35271418ac", "excerpt": "Evaluating each MemTool mode across 13+ LLMs on the ScaleMCP benchmark, we conducted experiments over 100 consecutive user interactions, measuring tool removal ratios (short-term memory efficiency) and task completion accuracy.", "citations": ["Lumer2025Memtool"], "pointer": "papers/paper_notes.jsonl:paper_id=P0083#key_results[0]", "score": 2}, {"paper_id": "P0009", "evidence_id": "E-P0009-e0bcca0d9e", "excerpt": "Furthermore, we evaluated current benchmarks and assessment protocols and have provided an analysis of 68 publicly available datasets to assess the performance of LLM-based agents in various tasks.", "citations": ["Chowa2025From"], "pointer": "papers/paper_notes.jsonl:paper_id=P0009#key_results[0]", "score": 1}], "B_highlights": [{"paper_id": "P0083", "evidence_id": "E-P0083-35271418ac", "excerpt": "Evaluating each MemTool mode across 13+ LLMs on the ScaleMCP benchmark, we conducted experiments over 100 consecutive user interactions, measuring tool removal ratios (short-term memory efficiency) and task completion accuracy.", "citations": ["Lumer2025Memtool"], "pointer": "papers/paper_notes.jsonl:paper_id=P0083#key_results[0]", "score": 2}, {"paper_id": "P0192", "evidence_id": "E-P0192-fae121f81b", "excerpt": "Our evaluation of 66 real-world tools from the repositories of two major LLM agent development frameworks, LangChain and LlamaIndex, revealed a significant security concern: 75% are vulnerable to XTHP attacks, highlighting the prevalence of this threat.", "citations": ["Li2025Dissonances"], "pointer": "papers/paper_notes.jsonl:paper_id=P0192#key_results[0]", "score": 1}], "write_prompt": "Contrast Agent frameworks / architectures vs Tool-use and function calling along \"tool interface contract (schemas / protocols)\". Ground A and B using the highlight snippets; do not introduce new claims beyond the cited evidence.", "evidence_field": "tool interface contract (schemas / protocols)", "citations": ["Lumer2025Memtool", "Chowa2025From", "Li2025Dissonances"]}, {"axis": "tool interface contract (schemas / protocols)", "A_label": "Agent frameworks / architectures", "B_label": "Planning / reasoning loops", "A_papers": "Agent frameworks / architectures: `P0022`, `P0009`, `P0010`", "B_papers": "Planning / reasoning loops: `P0013`, `P0054`, `P0016`", "A_highlights": [{"paper_id": "P0083", "evidence_id": "E-P0083-35271418ac", "excerpt": "Evaluating each MemTool mode across 13+ LLMs on the ScaleMCP benchmark, we conducted experiments over 100 consecutive user interactions, measuring tool removal ratios (short-term memory efficiency) and task completion accuracy.", "citations": ["Lumer2025Memtool"], "pointer": "papers/paper_notes.jsonl:paper_id=P0083#key_results[0]", "score": 2}, {"paper_id": "P0009", "evidence_id": "E-P0009-e0bcca0d9e", "excerpt": "Furthermore, we evaluated current benchmarks and assessment protocols and have provided an analysis of 68 publicly available datasets to assess the performance of LLM-based agents in various tasks.", "citations": ["Chowa2025From"], "pointer": "papers/paper_notes.jsonl:paper_id=P0009#key_results[0]", "score": 1}], "B_highlights": [{"paper_id": "P0016", "evidence_id": "E-P0016-dc2266b72d", "excerpt": "Specifically, 1) the taxonomy defines environments/tasks, common LLM-profiled roles or LMPRs (policy models, evaluators, and dynamic models), and universally applicable workflows found in prior work, and 2) it enables a comparison of key perspectives on the implementations of", "citations": ["Li2024Review"], "pointer": "papers/paper_notes.jsonl:paper_id=P0016#key_results[0]", "score": 0}, {"paper_id": "P0096", "evidence_id": "E-P0096-3575a7c673", "excerpt": "We find AvaTaR consistently outperforms state-of-the-art approaches across all seven tasks, exhibiting strong generalization ability when applied to novel cases and achieving an average relative improvement of 14% on the Hit@1 metric for the retrieval datasets and 13% for the", "citations": ["Wu2024Avatar"], "pointer": "papers/paper_notes.jsonl:paper_id=P0096#key_results[0]", "score": 0}], "write_prompt": "Contrast Agent frameworks / architectures vs Planning / reasoning loops along \"tool interface contract (schemas / protocols)\". Ground A and B using the highlight snippets; do not introduce new claims beyond the cited evidence.", "evidence_field": "tool interface contract (schemas / protocols)", "citations": ["Lumer2025Memtool", "Chowa2025From", "Li2024Review", "Wu2024Avatar"]}, {"axis": "tool interface contract (schemas / protocols)", "A_label": "Tool-use and function calling", "B_label": "Planning / reasoning loops", "A_papers": "Tool-use and function calling: `P0022`, `P0009`, `P0054`", "B_papers": "Planning / reasoning loops: `P0013`, `P0054`, `P0016`", "A_highlights": [{"paper_id": "P0083", "evidence_id": "E-P0083-35271418ac", "excerpt": "Evaluating each MemTool mode across 13+ LLMs on the ScaleMCP benchmark, we conducted experiments over 100 consecutive user interactions, measuring tool removal ratios (short-term memory efficiency) and task completion accuracy.", "citations": ["Lumer2025Memtool"], "pointer": "papers/paper_notes.jsonl:paper_id=P0083#key_results[0]", "score": 2}, {"paper_id": "P0192", "evidence_id": "E-P0192-fae121f81b", "excerpt": "Our evaluation of 66 real-world tools from the repositories of two major LLM agent development frameworks, LangChain and LlamaIndex, revealed a significant security concern: 75% are vulnerable to XTHP attacks, highlighting the prevalence of this threat.", "citations": ["Li2025Dissonances"], "pointer": "papers/paper_notes.jsonl:paper_id=P0192#key_results[0]", "score": 1}], "B_highlights": [{"paper_id": "P0016", "evidence_id": "E-P0016-dc2266b72d", "excerpt": "Specifically, 1) the taxonomy defines environments/tasks, common LLM-profiled roles or LMPRs (policy models, evaluators, and dynamic models), and universally applicable workflows found in prior work, and 2) it enables a comparison of key perspectives on the implementations of", "citations": ["Li2024Review"], "pointer": "papers/paper_notes.jsonl:paper_id=P0016#key_results[0]", "score": 0}, {"paper_id": "P0096", "evidence_id": "E-P0096-3575a7c673", "excerpt": "We find AvaTaR consistently outperforms state-of-the-art approaches across all seven tasks, exhibiting strong generalization ability when applied to novel cases and achieving an average relative improvement of 14% on the Hit@1 metric for the retrieval datasets and 13% for the", "citations": ["Wu2024Avatar"], "pointer": "papers/paper_notes.jsonl:paper_id=P0096#key_results[0]", "score": 0}], "write_prompt": "Contrast Tool-use and function calling vs Planning / reasoning loops along \"tool interface contract (schemas / protocols)\". Ground A and B using the highlight snippets; do not introduce new claims beyond the cited evidence.", "evidence_field": "tool interface contract (schemas / protocols)", "citations": ["Lumer2025Memtool", "Li2025Dissonances", "Li2024Review", "Wu2024Avatar"]}, {"axis": "tool selection / routing policy", "A_label": "Agent frameworks / architectures", "B_label": "Tool-use and function calling", "A_papers": "Agent frameworks / architectures: `P0022`, `P0009`, `P0010`", "B_papers": "Tool-use and function calling: `P0022`, `P0009`, `P0054`", "A_highlights": [{"paper_id": "P0083", "evidence_id": "E-P0083-35271418ac", "excerpt": "Evaluating each MemTool mode across 13+ LLMs on the ScaleMCP benchmark, we conducted experiments over 100 consecutive user interactions, measuring tool removal ratios (short-term memory efficiency) and task completion accuracy.", "citations": ["Lumer2025Memtool"], "pointer": "papers/paper_notes.jsonl:paper_id=P0083#key_results[0]", "score": 2}, {"paper_id": "P0009", "evidence_id": "E-P0009-e0bcca0d9e", "excerpt": "Furthermore, we evaluated current benchmarks and assessment protocols and have provided an analysis of 68 publicly available datasets to assess the performance of LLM-based agents in various tasks.", "citations": ["Chowa2025From"], "pointer": "papers/paper_notes.jsonl:paper_id=P0009#key_results[0]", "score": 1}], "B_highlights": [{"paper_id": "P0083", "evidence_id": "E-P0083-35271418ac", "excerpt": "Evaluating each MemTool mode across 13+ LLMs on the ScaleMCP benchmark, we conducted experiments over 100 consecutive user interactions, measuring tool removal ratios (short-term memory efficiency) and task completion accuracy.", "citations": ["Lumer2025Memtool"], "pointer": "papers/paper_notes.jsonl:paper_id=P0083#key_results[0]", "score": 2}, {"paper_id": "P0192", "evidence_id": "E-P0192-fae121f81b", "excerpt": "Our evaluation of 66 real-world tools from the repositories of two major LLM agent development frameworks, LangChain and LlamaIndex, revealed a significant security concern: 75% are vulnerable to XTHP attacks, highlighting the prevalence of this threat.", "citations": ["Li2025Dissonances"], "pointer": "papers/paper_notes.jsonl:paper_id=P0192#key_results[0]", "score": 1}], "write_prompt": "Contrast Agent frameworks / architectures vs Tool-use and function calling along \"tool selection / routing policy\". Ground A and B using the highlight snippets; do not introduce new claims beyond the cited evidence.", "evidence_field": "tool selection / routing policy", "citations": ["Lumer2025Memtool", "Chowa2025From", "Li2025Dissonances"]}], "evaluation_protocol": [{"bullet": "Evaluation mentions include: RAG, MCP, MCTS, LLMs, BFCL, GPU, ToolBench, LLM-based, SCL, CCAM.", "citations": ["Zhou2026Beyond", "Chowa2025From", "Zhang2025Generalizability", "Kim2025Bridging"]}, {"bullet": "When comparing results, anchor the paragraph with: task type + metric + constraint (budget, tool access, horizon, or threat model) when stated.", "citations": ["Zhou2026Beyond", "Chowa2025From"]}, {"bullet": "Prefer head-to-head comparisons only when benchmark/metric are shared; otherwise frame differences as protocol-driven rather than method superiority.", "citations": ["Zhou2026Beyond", "Chowa2025From"]}, {"bullet": "Avoid underspecified model/baseline naming; if abstracts omit details, state that the baseline is reported but underspecified instead of guessing.", "citations": ["Zhou2026Beyond", "Chowa2025From"]}, {"bullet": "If a claim relies on a single reported number, pair it with a limitation/caveat from the same evidence so the draft remains conservative.", "citations": ["Zhou2026Beyond", "Chowa2025From"]}, {"bullet": "If budgets or environments differ across papers, treat cross-paper numeric comparison as fragile and prefer qualitative contrasts aligned to the subsection axes.", "citations": ["Zhou2026Beyond", "Chowa2025From"]}], "failures_limitations": [{"bullet": "The agent-tool communication loop is a critical attack surface in modern Large Language Model (LLM) agents.", "citations": ["Zhou2026Beyond"]}, {"bullet": "We introduce a stealthy, multi-turn economic DoS attack that operates at the tool layer under the guise of a correctly completed task.", "citations": ["Zhou2026Beyond"]}, {"bullet": "Across six LLMs on the ToolBench and BFCL benchmarks, our attack expands tasks into trajectories exceeding 60,000 tokens, inflates costs by up to 658x, and raises energy by 100-560x.", "citations": ["Zhou2026Beyond"]}, {"bullet": "These results elevate the agent-tool interface to a first-class security frontier, demanding a paradigm shift from validating final answers to monitoring the economic and computational cost of the entire agentic process.", "citations": ["Zhou2026Beyond"]}, {"bullet": "We then review datasets, evaluation dimensions, and metrics, highlighting their limitations.", "citations": ["Zhang2025Generalizability"]}, {"bullet": "A critical challenge, however, lies in ensuring agent generalizability - the ability to maintain consistent performance across varied instructions, tasks, environments, and domains, especially those beyond agents' fine-tuning data.", "citations": ["Zhang2025Generalizability"]}, {"bullet": "Through InfoBid, we hope to foster the use of LLMs as proxies for human economic and social agents in empirical studies, enhancing our understanding of their capabilities and limitations.", "citations": ["Yin2025Infobid"]}, {"bullet": "In this paper, we present the first systematic security analysis of task control flows in multi-tool-enabled LLM agents.", "citations": ["Li2025Dissonances"]}], "blocking_missing": [], "verify_fields": ["named benchmarks/datasets used", "metrics/human-eval protocol", "compute/training/inference cost", "training data and supervision signal", "baseline choices and ablation evidence"], "generated_at": "2026-01-25T17:06:54", "section_id": "3", "section_title": "Foundations & Interfaces"}
{"sub_id": "4.1", "title": "Planning and reasoning loops", "evidence_ids": ["E-P0089-771620f84f", "E-P0233-c7ba5be441", "E-P0150-4a55873ef2", "E-P0109-c0a4d47a26", "E-P0001-ca4a00b5cf", "E-P0038-2a7ea60588", "E-P0088-4ae722da57", "E-P0030-8517628bd0", "E-P0027-dac95cdbce", "E-P0200-1b99de5317", "E-P0062-0d56c76ce0", "E-P0110-c0a98eb625", "E-P0077-23a1945a58", "E-P0186-b4b0a2aed4", "E-P0042-44dce85d3e", "E-P0276-b0d63bedde", "E-P0059-b35b53de13", "E-P0092-d1af7d9a55", "E-P0114-373af18bcc", "E-P0219-327d2749fc", "E-P0117-2c12f2ff60", "E-P0055-71f2629f1b", "E-P0055-c6c63e4ab5", "E-P0117-ebbc83e2c7"], "evidence_level_summary": {"fulltext": 0, "abstract": 28, "title": 0}, "evidence_snippets": [{"evidence_id": "E-P0089-771620f84f", "text": "Experimental evaluation on the complex task planning benchmark demonstrates that our 1.5B parameter model trained with single-turn GRPO achieves superior performance compared to larger baseline models up to 14B parameters, with success rates of 70% for long-horizon planning tasks.", "paper_id": "P0089", "citations": ["Hu2025Training"], "provenance": {"evidence_level": "abstract", "source": "paper_notes", "pointer": "papers/paper_notes.jsonl:paper_id=P0089#key_results[0]"}}, {"evidence_id": "E-P0233-c7ba5be441", "text": "To this end, we introduce USTBench, the first benchmark to evaluate LLMs' spatiotemporal reasoning abilities as urban agents across four decomposed dimensions: spatiotemporal understanding, forecasting, planning, and reflection with feedback.", "paper_id": "P0233", "citations": ["Lai2025Ustbench"], "provenance": {"evidence_level": "abstract", "source": "paper_notes", "pointer": "papers/paper_notes.jsonl:paper_id=P0233#method"}}, {"evidence_id": "E-P0150-4a55873ef2", "text": "However, a detailed analysis of their testing processes reveals specific strengths and limitations; notably, LLM agents struggle with maintaining coherent long-horizon plans, performing complex reasoning, and effectively utilizing specialized tools.", "paper_id": "P0150", "citations": ["Wang2025Automated"], "provenance": {"evidence_level": "abstract", "source": "paper_notes", "pointer": "papers/paper_notes.jsonl:paper_id=P0150#limitations[1]"}}, {"evidence_id": "E-P0109-c0a4d47a26", "text": "While a lot of recent research focuses on enhancing the textual reasoning capabilities of Large Language Models (LLMs) by optimizing the multi-agent framework or reasoning chains, several benchmark tasks can be solved with 100\\% success through direct coding, which is more scalable and avoids the computational overhead associated with textual iterating and searching.", "paper_id": "P0109", "citations": ["Chen2024Steering"], "provenance": {"evidence_level": "abstract", "source": "paper_notes", "pointer": "papers/paper_notes.jsonl:paper_id=P0109#key_results[0]"}}, {"evidence_id": "E-P0001-ca4a00b5cf", "text": "On two interactive decision making benchmarks (ALFWorld and WebShop), ReAct outperforms imitation and reinforcement learning methods by an absolute success rate of 34% and 10% respectively, while being prompted with only one or two in-context examples.", "paper_id": "P0001", "citations": ["Yao2022React"], "provenance": {"evidence_level": "abstract", "source": "paper_notes", "pointer": "papers/paper_notes.jsonl:paper_id=P0001#key_results[0]"}}, {"evidence_id": "E-P0038-2a7ea60588", "text": "Experiments on three real-world multi-tabular EHR datasets show that EHRAgent outperforms the strongest baseline by up to 29.6% in success rate.", "paper_id": "P0038", "citations": ["Shi2024Ehragent"], "provenance": {"evidence_level": "abstract", "source": "paper_notes", "pointer": "papers/paper_notes.jsonl:paper_id=P0038#key_results[0]"}}, {"evidence_id": "E-P0088-4ae722da57", "text": "We demonstrate particularly strong gains on Wikidata benchmarks (+16\\% improvement over previous best methods) alongside consistent improvements on Freebase benchmarks.", "paper_id": "P0088", "citations": ["Sun2025Search"], "provenance": {"evidence_level": "abstract", "source": "paper_notes", "pointer": "papers/paper_notes.jsonl:paper_id=P0088#key_results[0]"}}, {"evidence_id": "E-P0030-8517628bd0", "text": "While existing adversarial attacks primarily focus on content falsification or instruction injection, we identify a novel, process-oriented attack surface: the agent's reasoning style.", "paper_id": "P0030", "citations": ["Zhou2025Reasoning"], "provenance": {"evidence_level": "abstract", "source": "paper_notes", "pointer": "papers/paper_notes.jsonl:paper_id=P0030#summary_bullets[1]"}}, {"evidence_id": "E-P0027-dac95cdbce", "text": "We introduce Agentic Reasoning, a framework that enhances large language model (LLM) reasoning by integrating external tool-using agents.", "paper_id": "P0027", "citations": ["Wu2025Agentic"], "provenance": {"evidence_level": "abstract", "source": "paper_notes", "pointer": "papers/paper_notes.jsonl:paper_id=P0027#method"}}, {"evidence_id": "E-P0200-1b99de5317", "text": "From this observation, we build memory retrieval as an autonomous, accurate, and compatible agent system, named MemR$^3$, which has two core mechanisms: 1) a router that selects among retrieve, reflect, and answer actions to optimize answer quality; 2) a global evidence-gap tracker that explicitly renders the answering process transparent and tracks the evidence collection process.", "paper_id": "P0200", "citations": ["Du2025Memr"], "provenance": {"evidence_level": "abstract", "source": "paper_notes", "pointer": "papers/paper_notes.jsonl:paper_id=P0200#key_results[1]"}}, {"evidence_id": "E-P0062-0d56c76ce0", "text": "Recent benchmarks for Large Language Model (LLM) agents primarily focus on evaluating reasoning, planning, and execution capabilities, while another critical component-memory, encompassing how agents memorize, update, and retrieve long-term information-is under-evaluated due to the lack of benchmarks.", "paper_id": "P0062", "citations": ["Hu2025Evaluating"], "provenance": {"evidence_level": "abstract", "source": "paper_notes", "pointer": "papers/paper_notes.jsonl:paper_id=P0062#key_results[0]"}}, {"evidence_id": "E-P0110-c0a98eb625", "text": "We evaluate PDoctor with three mainstream agent frameworks and two powerful LLMs (GPT-3.5 and GPT-4).", "paper_id": "P0110", "citations": ["Ji2024Testing"], "provenance": {"evidence_level": "abstract", "source": "paper_notes", "pointer": "papers/paper_notes.jsonl:paper_id=P0110#key_results[0]"}}, {"evidence_id": "E-P0077-23a1945a58", "text": "Subsequently, we systematically summarize existing evaluation frameworks, including benchmark datasets, evaluation metrics and performance comparisons between representative planning methods.", "paper_id": "P0077", "citations": ["Cao2025Large"], "provenance": {"evidence_level": "abstract", "source": "paper_notes", "pointer": "papers/paper_notes.jsonl:paper_id=P0077#key_results[1]"}}, {"evidence_id": "E-P0186-b4b0a2aed4", "text": "By analyzing metrics such as planning time, success rate, group debits, and planning-step reduction rate, we demonstrate the superior performance of LLM+MAP, while also providing insights into robotic reasoning.", "paper_id": "P0186", "citations": ["Chu2025Bimanual"], "provenance": {"evidence_level": "abstract", "source": "paper_notes", "pointer": "papers/paper_notes.jsonl:paper_id=P0186#key_results[1]"}}, {"evidence_id": "E-P0042-44dce85d3e", "text": "We further review representative agentic reasoning frameworks across real-world applications and benchmarks, including science, robotics, healthcare, autonomous research, and mathematics.", "paper_id": "P0042", "citations": ["Wei2026Agentic"], "provenance": {"evidence_level": "abstract", "source": "paper_notes", "pointer": "papers/paper_notes.jsonl:paper_id=P0042#key_results[0]"}}, {"evidence_id": "E-P0276-b0d63bedde", "text": "The results were evaluated on four metrics, surpassing human experts in satisfaction and inclusion, and rivaling state-of-the-art reinforcement learning methods in service and ecology.", "paper_id": "P0276", "citations": ["Zhou2024Large"], "provenance": {"evidence_level": "abstract", "source": "paper_notes", "pointer": "papers/paper_notes.jsonl:paper_id=P0276#key_results[1]"}}, {"evidence_id": "E-P0059-b35b53de13", "text": "We systematically evaluate their performance using a suite of coordination-sensitive metrics, including task success rate, redundant actions, room conflicts, and urgency-weighted efficiency.", "paper_id": "P0059", "citations": ["Silva2025Agents"], "provenance": {"evidence_level": "abstract", "source": "paper_notes", "pointer": "papers/paper_notes.jsonl:paper_id=P0059#key_results[0]"}}, {"evidence_id": "E-P0092-d1af7d9a55", "text": "To rigorously evaluate the agent's performance in this emerging research area, we introduce a comprehensive benchmark consisting of diverse construction tasks designed to test creativity, spatial reasoning, adherence to in-game rules, and the effective integration of multimodal instructions.", "paper_id": "P0092", "citations": ["Chen2024Architectural"], "provenance": {"evidence_level": "abstract", "source": "paper_notes", "pointer": "papers/paper_notes.jsonl:paper_id=P0092#key_results[0]"}}], "definitions_setup": [{"bullet": "Setup: Which design choices in Planning and reasoning loops drive the major trade-offs, and how are those trade-offs measured? Scope: in-scope: Core topics directly relevant to 'Planning and reasoning loops'.. Axes: evaluation protocol (datasets, metrics, human evaluation); compute and latency constraints; tool interface contract (schemas / protocols); tool selection / routing policy; sandboxing / permissions / observability.", "citations": ["Wei2026Agentic", "Hatalis2025Review", "Wu2025Agentic"]}], "claim_candidates": [{"claim": "Experimental evaluation on the complex task planning benchmark demonstrates that our 1.5B parameter model trained with single-turn GRPO achieves superior performance compared to larger baseline models up to 14B parameters, with success rates of 70% for long-horizon planning tasks.", "evidence_field": "evidence_snippet", "citations": ["Hu2025Training"]}, {"claim": "To this end, we introduce USTBench, the first benchmark to evaluate LLMs' spatiotemporal reasoning abilities as urban agents across four decomposed dimensions: spatiotemporal understanding, forecasting, planning, and reflection with feedback.", "evidence_field": "evidence_snippet", "citations": ["Lai2025Ustbench"]}, {"claim": "However, a detailed analysis of their testing processes reveals specific strengths and limitations; notably, LLM agents struggle with maintaining coherent long-horizon plans, performing complex reasoning, and effectively utilizing specialized tools.", "evidence_field": "evidence_snippet", "citations": ["Wang2025Automated"]}, {"claim": "While a lot of recent research focuses on enhancing the textual reasoning capabilities of Large Language Models (LLMs) by optimizing the multi-agent framework or reasoning chains, several benchmark tasks can be solved with 100\\% success through direct coding, which is more scalable and avoids the computational overhead associated with textual iterating and searching.", "evidence_field": "evidence_snippet", "citations": ["Chen2024Steering"]}, {"claim": "On two interactive decision making benchmarks (ALFWorld and WebShop), ReAct outperforms imitation and reinforcement learning methods by an absolute success rate of 34% and 10% respectively, while being prompted with only one or two in-context examples.", "evidence_field": "evidence_snippet", "citations": ["Yao2022React"]}], "concrete_comparisons": [{"axis": "evaluation protocol (datasets, metrics, human evaluation)", "A_label": "Planning / reasoning loops", "B_label": "Agent frameworks / architectures", "A_papers": "Planning / reasoning loops: `P0042`, `P0015`, `P0027`", "B_papers": "Agent frameworks / architectures: `P0042`, `P0015`, `P0027`", "A_highlights": [{"paper_id": "P0077", "evidence_id": "E-P0077-23a1945a58", "excerpt": "Subsequently, we systematically summarize existing evaluation frameworks, including benchmark datasets, evaluation metrics and performance comparisons between representative planning methods.", "citations": ["Cao2025Large"], "pointer": "papers/paper_notes.jsonl:paper_id=P0077#key_results[1]", "score": 2}, {"paper_id": "P0042", "evidence_id": "E-P0042-44dce85d3e", "excerpt": "We further review representative agentic reasoning frameworks across real-world applications and benchmarks, including science, robotics, healthcare, autonomous research, and mathematics.", "citations": ["Wei2026Agentic"], "pointer": "papers/paper_notes.jsonl:paper_id=P0042#key_results[0]", "score": 1}], "B_highlights": [{"paper_id": "P0062", "evidence_id": "E-P0062-0d56c76ce0", "excerpt": "Recent benchmarks for Large Language Model (LLM) agents primarily focus on evaluating reasoning, planning, and execution capabilities, while another critical component-memory, encompassing how agents memorize, update, and retrieve long-term information-is under-evaluated due to", "citations": ["Hu2025Evaluating"], "pointer": "papers/paper_notes.jsonl:paper_id=P0062#key_results[0]", "score": 1}, {"paper_id": "P0089", "evidence_id": "E-P0089-771620f84f", "excerpt": "Experimental evaluation on the complex task planning benchmark demonstrates that our 1.5B parameter model trained with single-turn GRPO achieves superior performance compared to larger baseline models up to 14B parameters, with success rates of 70% for long-horizon planning", "citations": ["Hu2025Training"], "pointer": "papers/paper_notes.jsonl:paper_id=P0089#key_results[0]", "score": 1}], "write_prompt": "Contrast Planning / reasoning loops vs Agent frameworks / architectures along \"evaluation protocol (datasets, metrics, human evaluation)\". Ground A and B using the highlight snippets; do not introduce new claims beyond the cited evidence.", "evidence_field": "evaluation protocol (datasets, metrics, human evaluation)", "citations": ["Cao2025Large", "Wei2026Agentic", "Hu2025Evaluating", "Hu2025Training"]}, {"axis": "evaluation protocol (datasets, metrics, human evaluation)", "A_label": "Planning / reasoning loops", "B_label": "Memory / retrieval augmentation", "A_papers": "Planning / reasoning loops: `P0042`, `P0015`, `P0027`", "B_papers": "Memory / retrieval augmentation: `P0062`, `P0200`, `P0038`", "A_highlights": [{"paper_id": "P0077", "evidence_id": "E-P0077-23a1945a58", "excerpt": "Subsequently, we systematically summarize existing evaluation frameworks, including benchmark datasets, evaluation metrics and performance comparisons between representative planning methods.", "citations": ["Cao2025Large"], "pointer": "papers/paper_notes.jsonl:paper_id=P0077#key_results[1]", "score": 2}, {"paper_id": "P0042", "evidence_id": "E-P0042-44dce85d3e", "excerpt": "We further review representative agentic reasoning frameworks across real-world applications and benchmarks, including science, robotics, healthcare, autonomous research, and mathematics.", "citations": ["Wei2026Agentic"], "pointer": "papers/paper_notes.jsonl:paper_id=P0042#key_results[0]", "score": 1}], "B_highlights": [{"paper_id": "P0200", "evidence_id": "E-P0200-1b99de5317", "excerpt": "From this observation, we build memory retrieval as an autonomous, accurate, and compatible agent system, named MemR$^3$, which has two core mechanisms: 1) a router that selects among retrieve, reflect, and answer actions to optimize answer quality; 2) a global evidence-gap", "citations": ["Du2025Memr"], "pointer": "papers/paper_notes.jsonl:paper_id=P0200#key_results[1]", "score": 1}, {"paper_id": "P0062", "evidence_id": "E-P0062-0d56c76ce0", "excerpt": "Recent benchmarks for Large Language Model (LLM) agents primarily focus on evaluating reasoning, planning, and execution capabilities, while another critical component-memory, encompassing how agents memorize, update, and retrieve long-term information-is under-evaluated due to", "citations": ["Hu2025Evaluating"], "pointer": "papers/paper_notes.jsonl:paper_id=P0062#key_results[0]", "score": 1}], "write_prompt": "Contrast Planning / reasoning loops vs Memory / retrieval augmentation along \"evaluation protocol (datasets, metrics, human evaluation)\". Ground A and B using the highlight snippets; do not introduce new claims beyond the cited evidence.", "evidence_field": "evaluation protocol (datasets, metrics, human evaluation)", "citations": ["Cao2025Large", "Wei2026Agentic", "Du2025Memr", "Hu2025Evaluating"]}, {"axis": "evaluation protocol (datasets, metrics, human evaluation)", "A_label": "Agent frameworks / architectures", "B_label": "Memory / retrieval augmentation", "A_papers": "Agent frameworks / architectures: `P0042`, `P0015`, `P0027`", "B_papers": "Memory / retrieval augmentation: `P0062`, `P0200`, `P0038`", "A_highlights": [{"paper_id": "P0062", "evidence_id": "E-P0062-0d56c76ce0", "excerpt": "Recent benchmarks for Large Language Model (LLM) agents primarily focus on evaluating reasoning, planning, and execution capabilities, while another critical component-memory, encompassing how agents memorize, update, and retrieve long-term information-is under-evaluated due to", "citations": ["Hu2025Evaluating"], "pointer": "papers/paper_notes.jsonl:paper_id=P0062#key_results[0]", "score": 1}, {"paper_id": "P0089", "evidence_id": "E-P0089-771620f84f", "excerpt": "Experimental evaluation on the complex task planning benchmark demonstrates that our 1.5B parameter model trained with single-turn GRPO achieves superior performance compared to larger baseline models up to 14B parameters, with success rates of 70% for long-horizon planning", "citations": ["Hu2025Training"], "pointer": "papers/paper_notes.jsonl:paper_id=P0089#key_results[0]", "score": 1}], "B_highlights": [{"paper_id": "P0200", "evidence_id": "E-P0200-1b99de5317", "excerpt": "From this observation, we build memory retrieval as an autonomous, accurate, and compatible agent system, named MemR$^3$, which has two core mechanisms: 1) a router that selects among retrieve, reflect, and answer actions to optimize answer quality; 2) a global evidence-gap", "citations": ["Du2025Memr"], "pointer": "papers/paper_notes.jsonl:paper_id=P0200#key_results[1]", "score": 1}, {"paper_id": "P0062", "evidence_id": "E-P0062-0d56c76ce0", "excerpt": "Recent benchmarks for Large Language Model (LLM) agents primarily focus on evaluating reasoning, planning, and execution capabilities, while another critical component-memory, encompassing how agents memorize, update, and retrieve long-term information-is under-evaluated due to", "citations": ["Hu2025Evaluating"], "pointer": "papers/paper_notes.jsonl:paper_id=P0062#key_results[0]", "score": 1}], "write_prompt": "Contrast Agent frameworks / architectures vs Memory / retrieval augmentation along \"evaluation protocol (datasets, metrics, human evaluation)\". Ground A and B using the highlight snippets; do not introduce new claims beyond the cited evidence.", "evidence_field": "evaluation protocol (datasets, metrics, human evaluation)", "citations": ["Hu2025Evaluating", "Hu2025Training", "Du2025Memr"]}, {"axis": "compute and latency constraints", "A_label": "Planning / reasoning loops", "B_label": "Agent frameworks / architectures", "A_papers": "Planning / reasoning loops: `P0042`, `P0015`, `P0027`", "B_papers": "Agent frameworks / architectures: `P0042`, `P0015`, `P0027`", "A_highlights": [{"paper_id": "P0077", "evidence_id": "E-P0077-23a1945a58", "excerpt": "Subsequently, we systematically summarize existing evaluation frameworks, including benchmark datasets, evaluation metrics and performance comparisons between representative planning methods.", "citations": ["Cao2025Large"], "pointer": "papers/paper_notes.jsonl:paper_id=P0077#key_results[1]", "score": 0}, {"paper_id": "P0059", "evidence_id": "E-P0059-b35b53de13", "excerpt": "We systematically evaluate their performance using a suite of coordination-sensitive metrics, including task success rate, redundant actions, room conflicts, and urgency-weighted efficiency.", "citations": ["Silva2025Agents"], "pointer": "papers/paper_notes.jsonl:paper_id=P0059#key_results[0]", "score": 0}], "B_highlights": [{"paper_id": "P0089", "evidence_id": "E-P0089-771620f84f", "excerpt": "Experimental evaluation on the complex task planning benchmark demonstrates that our 1.5B parameter model trained with single-turn GRPO achieves superior performance compared to larger baseline models up to 14B parameters, with success rates of 70% for long-horizon planning", "citations": ["Hu2025Training"], "pointer": "papers/paper_notes.jsonl:paper_id=P0089#key_results[0]", "score": 1}, {"paper_id": "P0062", "evidence_id": "E-P0062-0d56c76ce0", "excerpt": "Recent benchmarks for Large Language Model (LLM) agents primarily focus on evaluating reasoning, planning, and execution capabilities, while another critical component-memory, encompassing how agents memorize, update, and retrieve long-term information-is under-evaluated due to", "citations": ["Hu2025Evaluating"], "pointer": "papers/paper_notes.jsonl:paper_id=P0062#key_results[0]", "score": 0}], "write_prompt": "Contrast Planning / reasoning loops vs Agent frameworks / architectures along \"compute and latency constraints\". Ground A and B using the highlight snippets; do not introduce new claims beyond the cited evidence.", "evidence_field": "compute and latency constraints", "citations": ["Cao2025Large", "Silva2025Agents", "Hu2025Training", "Hu2025Evaluating"]}, {"axis": "compute and latency constraints", "A_label": "Planning / reasoning loops", "B_label": "Memory / retrieval augmentation", "A_papers": "Planning / reasoning loops: `P0042`, `P0015`, `P0027`", "B_papers": "Memory / retrieval augmentation: `P0062`, `P0200`, `P0038`", "A_highlights": [{"paper_id": "P0077", "evidence_id": "E-P0077-23a1945a58", "excerpt": "Subsequently, we systematically summarize existing evaluation frameworks, including benchmark datasets, evaluation metrics and performance comparisons between representative planning methods.", "citations": ["Cao2025Large"], "pointer": "papers/paper_notes.jsonl:paper_id=P0077#key_results[1]", "score": 0}, {"paper_id": "P0059", "evidence_id": "E-P0059-b35b53de13", "excerpt": "We systematically evaluate their performance using a suite of coordination-sensitive metrics, including task success rate, redundant actions, room conflicts, and urgency-weighted efficiency.", "citations": ["Silva2025Agents"], "pointer": "papers/paper_notes.jsonl:paper_id=P0059#key_results[0]", "score": 0}], "B_highlights": [{"paper_id": "P0200", "evidence_id": "E-P0200-1b99de5317", "excerpt": "From this observation, we build memory retrieval as an autonomous, accurate, and compatible agent system, named MemR$^3$, which has two core mechanisms: 1) a router that selects among retrieve, reflect, and answer actions to optimize answer quality; 2) a global evidence-gap", "citations": ["Du2025Memr"], "pointer": "papers/paper_notes.jsonl:paper_id=P0200#key_results[1]", "score": 0}, {"paper_id": "P0062", "evidence_id": "E-P0062-0d56c76ce0", "excerpt": "Recent benchmarks for Large Language Model (LLM) agents primarily focus on evaluating reasoning, planning, and execution capabilities, while another critical component-memory, encompassing how agents memorize, update, and retrieve long-term information-is under-evaluated due to", "citations": ["Hu2025Evaluating"], "pointer": "papers/paper_notes.jsonl:paper_id=P0062#key_results[0]", "score": 0}], "write_prompt": "Contrast Planning / reasoning loops vs Memory / retrieval augmentation along \"compute and latency constraints\". Ground A and B using the highlight snippets; do not introduce new claims beyond the cited evidence.", "evidence_field": "compute and latency constraints", "citations": ["Cao2025Large", "Silva2025Agents", "Du2025Memr", "Hu2025Evaluating"]}, {"axis": "compute and latency constraints", "A_label": "Agent frameworks / architectures", "B_label": "Memory / retrieval augmentation", "A_papers": "Agent frameworks / architectures: `P0042`, `P0015`, `P0027`", "B_papers": "Memory / retrieval augmentation: `P0062`, `P0200`, `P0038`", "A_highlights": [{"paper_id": "P0089", "evidence_id": "E-P0089-771620f84f", "excerpt": "Experimental evaluation on the complex task planning benchmark demonstrates that our 1.5B parameter model trained with single-turn GRPO achieves superior performance compared to larger baseline models up to 14B parameters, with success rates of 70% for long-horizon planning", "citations": ["Hu2025Training"], "pointer": "papers/paper_notes.jsonl:paper_id=P0089#key_results[0]", "score": 1}, {"paper_id": "P0062", "evidence_id": "E-P0062-0d56c76ce0", "excerpt": "Recent benchmarks for Large Language Model (LLM) agents primarily focus on evaluating reasoning, planning, and execution capabilities, while another critical component-memory, encompassing how agents memorize, update, and retrieve long-term information-is under-evaluated due to", "citations": ["Hu2025Evaluating"], "pointer": "papers/paper_notes.jsonl:paper_id=P0062#key_results[0]", "score": 0}], "B_highlights": [{"paper_id": "P0200", "evidence_id": "E-P0200-1b99de5317", "excerpt": "From this observation, we build memory retrieval as an autonomous, accurate, and compatible agent system, named MemR$^3$, which has two core mechanisms: 1) a router that selects among retrieve, reflect, and answer actions to optimize answer quality; 2) a global evidence-gap", "citations": ["Du2025Memr"], "pointer": "papers/paper_notes.jsonl:paper_id=P0200#key_results[1]", "score": 0}, {"paper_id": "P0062", "evidence_id": "E-P0062-0d56c76ce0", "excerpt": "Recent benchmarks for Large Language Model (LLM) agents primarily focus on evaluating reasoning, planning, and execution capabilities, while another critical component-memory, encompassing how agents memorize, update, and retrieve long-term information-is under-evaluated due to", "citations": ["Hu2025Evaluating"], "pointer": "papers/paper_notes.jsonl:paper_id=P0062#key_results[0]", "score": 0}], "write_prompt": "Contrast Agent frameworks / architectures vs Memory / retrieval augmentation along \"compute and latency constraints\". Ground A and B using the highlight snippets; do not introduce new claims beyond the cited evidence.", "evidence_field": "compute and latency constraints", "citations": ["Hu2025Training", "Hu2025Evaluating", "Du2025Memr"]}, {"axis": "tool interface contract (schemas / protocols)", "A_label": "Planning / reasoning loops", "B_label": "Agent frameworks / architectures", "A_papers": "Planning / reasoning loops: `P0042`, `P0015`, `P0027`", "B_papers": "Agent frameworks / architectures: `P0042`, `P0015`, `P0027`", "A_highlights": [{"paper_id": "P0027", "evidence_id": "E-P0027-dac95cdbce", "excerpt": "We introduce Agentic Reasoning, a framework that enhances large language model (LLM) reasoning by integrating external tool-using agents.", "citations": ["Wu2025Agentic"], "pointer": "papers/paper_notes.jsonl:paper_id=P0027#method", "score": 1}, {"paper_id": "P0077", "evidence_id": "E-P0077-23a1945a58", "excerpt": "Subsequently, we systematically summarize existing evaluation frameworks, including benchmark datasets, evaluation metrics and performance comparisons between representative planning methods.", "citations": ["Cao2025Large"], "pointer": "papers/paper_notes.jsonl:paper_id=P0077#key_results[1]", "score": 0}], "B_highlights": [{"paper_id": "P0027", "evidence_id": "E-P0027-dac95cdbce", "excerpt": "We introduce Agentic Reasoning, a framework that enhances large language model (LLM) reasoning by integrating external tool-using agents.", "citations": ["Wu2025Agentic"], "pointer": "papers/paper_notes.jsonl:paper_id=P0027#method", "score": 1}, {"paper_id": "P0062", "evidence_id": "E-P0062-0d56c76ce0", "excerpt": "Recent benchmarks for Large Language Model (LLM) agents primarily focus on evaluating reasoning, planning, and execution capabilities, while another critical component-memory, encompassing how agents memorize, update, and retrieve long-term information-is under-evaluated due to", "citations": ["Hu2025Evaluating"], "pointer": "papers/paper_notes.jsonl:paper_id=P0062#key_results[0]", "score": 0}], "write_prompt": "Contrast Planning / reasoning loops vs Agent frameworks / architectures along \"tool interface contract (schemas / protocols)\". Ground A and B using the highlight snippets; do not introduce new claims beyond the cited evidence.", "evidence_field": "tool interface contract (schemas / protocols)", "citations": ["Wu2025Agentic", "Cao2025Large", "Hu2025Evaluating"]}, {"axis": "tool interface contract (schemas / protocols)", "A_label": "Planning / reasoning loops", "B_label": "Memory / retrieval augmentation", "A_papers": "Planning / reasoning loops: `P0042`, `P0015`, `P0027`", "B_papers": "Memory / retrieval augmentation: `P0062`, `P0200`, `P0038`", "A_highlights": [{"paper_id": "P0027", "evidence_id": "E-P0027-dac95cdbce", "excerpt": "We introduce Agentic Reasoning, a framework that enhances large language model (LLM) reasoning by integrating external tool-using agents.", "citations": ["Wu2025Agentic"], "pointer": "papers/paper_notes.jsonl:paper_id=P0027#method", "score": 1}, {"paper_id": "P0077", "evidence_id": "E-P0077-23a1945a58", "excerpt": "Subsequently, we systematically summarize existing evaluation frameworks, including benchmark datasets, evaluation metrics and performance comparisons between representative planning methods.", "citations": ["Cao2025Large"], "pointer": "papers/paper_notes.jsonl:paper_id=P0077#key_results[1]", "score": 0}], "B_highlights": [{"paper_id": "P0200", "evidence_id": "E-P0200-1b99de5317", "excerpt": "From this observation, we build memory retrieval as an autonomous, accurate, and compatible agent system, named MemR$^3$, which has two core mechanisms: 1) a router that selects among retrieve, reflect, and answer actions to optimize answer quality; 2) a global evidence-gap", "citations": ["Du2025Memr"], "pointer": "papers/paper_notes.jsonl:paper_id=P0200#key_results[1]", "score": 1}, {"paper_id": "P0062", "evidence_id": "E-P0062-0d56c76ce0", "excerpt": "Recent benchmarks for Large Language Model (LLM) agents primarily focus on evaluating reasoning, planning, and execution capabilities, while another critical component-memory, encompassing how agents memorize, update, and retrieve long-term information-is under-evaluated due to", "citations": ["Hu2025Evaluating"], "pointer": "papers/paper_notes.jsonl:paper_id=P0062#key_results[0]", "score": 0}], "write_prompt": "Contrast Planning / reasoning loops vs Memory / retrieval augmentation along \"tool interface contract (schemas / protocols)\". Ground A and B using the highlight snippets; do not introduce new claims beyond the cited evidence.", "evidence_field": "tool interface contract (schemas / protocols)", "citations": ["Wu2025Agentic", "Cao2025Large", "Du2025Memr", "Hu2025Evaluating"]}, {"axis": "tool interface contract (schemas / protocols)", "A_label": "Agent frameworks / architectures", "B_label": "Memory / retrieval augmentation", "A_papers": "Agent frameworks / architectures: `P0042`, `P0015`, `P0027`", "B_papers": "Memory / retrieval augmentation: `P0062`, `P0200`, `P0038`", "A_highlights": [{"paper_id": "P0027", "evidence_id": "E-P0027-dac95cdbce", "excerpt": "We introduce Agentic Reasoning, a framework that enhances large language model (LLM) reasoning by integrating external tool-using agents.", "citations": ["Wu2025Agentic"], "pointer": "papers/paper_notes.jsonl:paper_id=P0027#method", "score": 1}, {"paper_id": "P0062", "evidence_id": "E-P0062-0d56c76ce0", "excerpt": "Recent benchmarks for Large Language Model (LLM) agents primarily focus on evaluating reasoning, planning, and execution capabilities, while another critical component-memory, encompassing how agents memorize, update, and retrieve long-term information-is under-evaluated due to", "citations": ["Hu2025Evaluating"], "pointer": "papers/paper_notes.jsonl:paper_id=P0062#key_results[0]", "score": 0}], "B_highlights": [{"paper_id": "P0200", "evidence_id": "E-P0200-1b99de5317", "excerpt": "From this observation, we build memory retrieval as an autonomous, accurate, and compatible agent system, named MemR$^3$, which has two core mechanisms: 1) a router that selects among retrieve, reflect, and answer actions to optimize answer quality; 2) a global evidence-gap", "citations": ["Du2025Memr"], "pointer": "papers/paper_notes.jsonl:paper_id=P0200#key_results[1]", "score": 1}, {"paper_id": "P0062", "evidence_id": "E-P0062-0d56c76ce0", "excerpt": "Recent benchmarks for Large Language Model (LLM) agents primarily focus on evaluating reasoning, planning, and execution capabilities, while another critical component-memory, encompassing how agents memorize, update, and retrieve long-term information-is under-evaluated due to", "citations": ["Hu2025Evaluating"], "pointer": "papers/paper_notes.jsonl:paper_id=P0062#key_results[0]", "score": 0}], "write_prompt": "Contrast Agent frameworks / architectures vs Memory / retrieval augmentation along \"tool interface contract (schemas / protocols)\". Ground A and B using the highlight snippets; do not introduce new claims beyond the cited evidence.", "evidence_field": "tool interface contract (schemas / protocols)", "citations": ["Wu2025Agentic", "Hu2025Evaluating", "Du2025Memr"]}, {"axis": "tool selection / routing policy", "A_label": "Planning / reasoning loops", "B_label": "Agent frameworks / architectures", "A_papers": "Planning / reasoning loops: `P0042`, `P0015`, `P0027`", "B_papers": "Agent frameworks / architectures: `P0042`, `P0015`, `P0027`", "A_highlights": [{"paper_id": "P0027", "evidence_id": "E-P0027-dac95cdbce", "excerpt": "We introduce Agentic Reasoning, a framework that enhances large language model (LLM) reasoning by integrating external tool-using agents.", "citations": ["Wu2025Agentic"], "pointer": "papers/paper_notes.jsonl:paper_id=P0027#method", "score": 1}, {"paper_id": "P0077", "evidence_id": "E-P0077-23a1945a58", "excerpt": "Subsequently, we systematically summarize existing evaluation frameworks, including benchmark datasets, evaluation metrics and performance comparisons between representative planning methods.", "citations": ["Cao2025Large"], "pointer": "papers/paper_notes.jsonl:paper_id=P0077#key_results[1]", "score": 0}], "B_highlights": [{"paper_id": "P0027", "evidence_id": "E-P0027-dac95cdbce", "excerpt": "We introduce Agentic Reasoning, a framework that enhances large language model (LLM) reasoning by integrating external tool-using agents.", "citations": ["Wu2025Agentic"], "pointer": "papers/paper_notes.jsonl:paper_id=P0027#method", "score": 1}, {"paper_id": "P0062", "evidence_id": "E-P0062-0d56c76ce0", "excerpt": "Recent benchmarks for Large Language Model (LLM) agents primarily focus on evaluating reasoning, planning, and execution capabilities, while another critical component-memory, encompassing how agents memorize, update, and retrieve long-term information-is under-evaluated due to", "citations": ["Hu2025Evaluating"], "pointer": "papers/paper_notes.jsonl:paper_id=P0062#key_results[0]", "score": 0}], "write_prompt": "Contrast Planning / reasoning loops vs Agent frameworks / architectures along \"tool selection / routing policy\". Ground A and B using the highlight snippets; do not introduce new claims beyond the cited evidence.", "evidence_field": "tool selection / routing policy", "citations": ["Wu2025Agentic", "Cao2025Large", "Hu2025Evaluating"]}], "evaluation_protocol": [{"bullet": "Evaluation mentions include: LLMs, CBR, CBR-enhanced, SOTA, DeepSeek-R1, OpenAI, RSP, GSI, RSV, FEVER.", "citations": ["Wei2026Agentic", "Hatalis2025Review", "Wu2025Agentic", "Zhou2025Reasoning"]}, {"bullet": "When comparing results, anchor the paragraph with: task type + metric + constraint (budget, tool access, horizon, or threat model) when stated.", "citations": ["Wei2026Agentic", "Hatalis2025Review"]}, {"bullet": "Prefer head-to-head comparisons only when benchmark/metric are shared; otherwise frame differences as protocol-driven rather than method superiority.", "citations": ["Wei2026Agentic", "Hatalis2025Review"]}, {"bullet": "Avoid underspecified model/baseline naming; if abstracts omit details, state that the baseline is reported but underspecified instead of guessing.", "citations": ["Wei2026Agentic", "Hatalis2025Review"]}, {"bullet": "If a claim relies on a single reported number, pair it with a limitation/caveat from the same evidence so the draft remains conservative.", "citations": ["Wei2026Agentic", "Hatalis2025Review"]}, {"bullet": "If budgets or environments differ across papers, treat cross-paper numeric comparison as fragile and prefer qualitative contrasts aligned to the subsection axes.", "citations": ["Wei2026Agentic", "Hatalis2025Review"]}], "failures_limitations": [{"bullet": "Still, they face limitations in tasks requiring specific, structured knowledge, flexibility, or accountable decision-making.", "citations": ["Hatalis2025Review"]}, {"bullet": "While existing adversarial attacks primarily focus on content falsification or instruction injection, we identify a novel, process-oriented attack surface: the agent's reasoning style.", "citations": ["Zhou2025Reasoning"]}, {"bullet": "We introduce Generative Style Injection (GSI), an attack method that rewrites retrieved documents into pathological tones--specifically \"analysis paralysis\" or \"cognitive haste\"--without altering underlying facts or using explicit triggers.", "citations": ["Zhou2025Reasoning"]}, {"bullet": "This study offers new insights into the strengths and failure modes of LLMs in physically grounded multi-agent collaboration tasks, contributing to future benchmarks and architectural improvements.", "citations": ["Silva2025Agents"]}, {"bullet": "To address these limitations, we propose Search-on-Graph (SoG), a simple yet effective framework that enables LLMs to perform iterative informed graph navigation using a single, carefully designed \\textsc{Search} function.", "citations": ["Sun2025Search"]}, {"bullet": "Large language models (LLMs) have demonstrated impressive reasoning abilities yet remain unreliable on knowledge-intensive, multi-hop questions -- they miss long-tail facts, hallucinate when uncertain, and their internal knowledge lags behind real-world change.", "citations": ["Sun2025Search"]}], "blocking_missing": [], "verify_fields": ["named benchmarks/datasets used", "metrics/human-eval protocol", "compute/training/inference cost", "training data and supervision signal", "baseline choices and ablation evidence"], "generated_at": "2026-01-25T17:06:54", "section_id": "4", "section_title": "Core Components (Planning + Memory)"}
{"sub_id": "4.2", "title": "Memory and retrieval (RAG)", "evidence_ids": ["E-P0047-cb9370ac71", "E-P0166-17fb12f5b7", "E-P0220-eedb53e99a", "E-P0189-897bcc2f50", "E-P0200-1b99de5317", "E-P0173-06e45507a7", "E-P0235-4af0cf3c02", "E-P0001-ca4a00b5cf", "E-P0033-46914a4804", "E-P0016-62b8101d4e", "E-P0120-9abcf1bf8a", "E-P0026-f0ea009256", "E-P0062-86184d0d44", "E-P0011-1b6fe3407a", "E-P0121-f0f0faaada", "E-P0169-0a0006ef38", "E-P0032-53536132a8", "E-P0064-d568c53e1d", "E-P0118-fa466c449b", "E-P0104-7b7a296576", "E-P0104-c8b3e662a6", "E-P0220-50b7d357a4", "E-P0047-a4dcbe4147", "E-P0034-0f3e2d76f4"], "evidence_level_summary": {"fulltext": 0, "abstract": 28, "title": 0}, "evidence_snippets": [{"evidence_id": "E-P0047-cb9370ac71", "text": "Comprehensive evaluations on multiple benchmarks show that A-MemGuard effectively cuts attack success rates by over 95% while incurring a minimal utility cost.", "paper_id": "P0047", "citations": ["Wei2025Memguard"], "provenance": {"evidence_level": "abstract", "source": "paper_notes", "pointer": "papers/paper_notes.jsonl:paper_id=P0047#key_results[0]"}}, {"evidence_id": "E-P0166-17fb12f5b7", "text": "In this work, we propose Agent Distillation, a framework for transferring not only reasoning capability but full task-solving behavior from LLM-based agents into sLMs with retrieval and code tools.", "paper_id": "P0166", "citations": ["Kang2025Distilling"], "provenance": {"evidence_level": "abstract", "source": "paper_notes", "pointer": "papers/paper_notes.jsonl:paper_id=P0166#method"}}, {"evidence_id": "E-P0220-eedb53e99a", "text": "Despite the growing prominence of LLMs in this field, there is a scarcity of scholarly works that systematically synthesize how advanced enhancement techniques, specifically Retrieval-Augmented Generation (RAG) and Agentic systems, can be utilized to address these reliability and reasoning limitations.", "paper_id": "P0220", "citations": ["Ge2025Surveya"], "provenance": {"evidence_level": "abstract", "source": "paper_notes", "pointer": "papers/paper_notes.jsonl:paper_id=P0220#limitations[1]"}}, {"evidence_id": "E-P0189-897bcc2f50", "text": "Structural drawings are widely used in many fields, e.g., mechanical engineering, civil engineering, etc. In civil engineering, structural drawings serve as the main communication tool between architects, engineers, and builders to avoid conflicts, act as legal documentation, and provide a reference for future maintenance or evaluation needs.", "paper_id": "P0189", "citations": ["Zhang2025Largea"], "provenance": {"evidence_level": "abstract", "source": "paper_notes", "pointer": "papers/paper_notes.jsonl:paper_id=P0189#key_results[0]"}}, {"evidence_id": "E-P0200-1b99de5317", "text": "From this observation, we build memory retrieval as an autonomous, accurate, and compatible agent system, named MemR$^3$, which has two core mechanisms: 1) a router that selects among retrieve, reflect, and answer actions to optimize answer quality; 2) a global evidence-gap tracker that explicitly renders the answering process transparent and tracks the evidence collection process.", "paper_id": "P0200", "citations": ["Du2025Memr"], "provenance": {"evidence_level": "abstract", "source": "paper_notes", "pointer": "papers/paper_notes.jsonl:paper_id=P0200#key_results[1]"}}, {"evidence_id": "E-P0173-06e45507a7", "text": "We unify and implement over ten representative memory modules and evaluate them across 10 diverse multi-turn goal-oriented and single-turn reasoning and QA datasets.", "paper_id": "P0173", "citations": ["Wei2025Memory"], "provenance": {"evidence_level": "abstract", "source": "paper_notes", "pointer": "papers/paper_notes.jsonl:paper_id=P0173#key_results[0]"}}, {"evidence_id": "E-P0235-4af0cf3c02", "text": "This study presents SAFE (system for accurate fact extraction and evaluation), an agent system that combines large language models with retrieval-augmented generation (RAG) to improve automated fact-checking of long-form COVID-19 misinformation.", "paper_id": "P0235", "citations": ["Huang2025Retrieval"], "provenance": {"evidence_level": "abstract", "source": "paper_notes", "pointer": "papers/paper_notes.jsonl:paper_id=P0235#key_results[1]"}}, {"evidence_id": "E-P0001-ca4a00b5cf", "text": "On two interactive decision making benchmarks (ALFWorld and WebShop), ReAct outperforms imitation and reinforcement learning methods by an absolute success rate of 34% and 10% respectively, while being prompted with only one or two in-context examples.", "paper_id": "P0001", "citations": ["Yao2022React"], "provenance": {"evidence_level": "abstract", "source": "paper_notes", "pointer": "papers/paper_notes.jsonl:paper_id=P0001#key_results[0]"}}, {"evidence_id": "E-P0033-46914a4804", "text": "Yet their sophisticated architectures amplify vulnerability to cascading failures, where a single root-cause error propagates through subsequent decisions, leading to task failure.", "paper_id": "P0033", "citations": ["Zhu2025Where"], "provenance": {"evidence_level": "abstract", "source": "paper_notes", "pointer": "papers/paper_notes.jsonl:paper_id=P0033#summary_bullets[1]"}}, {"evidence_id": "E-P0016-62b8101d4e", "text": "Tool use, planning, and feedback learning are currently three prominent paradigms for developing Large Language Model (LLM)-based agents across various tasks.", "paper_id": "P0016", "citations": ["Li2024Review"], "provenance": {"evidence_level": "abstract", "source": "paper_notes", "pointer": "papers/paper_notes.jsonl:paper_id=P0016#method"}}, {"evidence_id": "E-P0120-9abcf1bf8a", "text": "Using an optimized scaffold matching industry best practices (persistent bash + string-replacement editor), we evaluated Focus on N=5 context-intensive instances from SWE-bench Lite using Claude Haiku 4.5.", "paper_id": "P0120", "citations": ["Verma2026Active"], "provenance": {"evidence_level": "abstract", "source": "paper_notes", "pointer": "papers/paper_notes.jsonl:paper_id=P0120#key_results[1]"}}, {"evidence_id": "E-P0026-f0ea009256", "text": "Finally, we summarize the datasets and benchmarks used for evaluation and tuning, review key applications of LLM-based agents, and discuss major challenges and promising future directions.", "paper_id": "P0026", "citations": ["Du2025Survey"], "provenance": {"evidence_level": "abstract", "source": "paper_notes", "pointer": "papers/paper_notes.jsonl:paper_id=P0026#key_results[0]"}}, {"evidence_id": "E-P0062-86184d0d44", "text": "Existing benchmarks either rely on limited context lengths or are tailored for static, long-context settings like book-based QA, which do not reflect the interactive, multi-turn nature of memory agents that incrementally accumulate information.", "paper_id": "P0062", "citations": ["Hu2025Evaluating"], "provenance": {"evidence_level": "abstract", "source": "paper_notes", "pointer": "papers/paper_notes.jsonl:paper_id=P0062#key_results[1]"}}, {"evidence_id": "E-P0011-1b6fe3407a", "text": "To further accelerate research in this area for the community, we compile open-source training frameworks, training and evaluation datasets for developing agentic MLLMs.", "paper_id": "P0011", "citations": ["Yao2025Survey"], "provenance": {"evidence_level": "abstract", "source": "paper_notes", "pointer": "papers/paper_notes.jsonl:paper_id=P0011#key_results[0]"}}, {"evidence_id": "E-P0121-f0f0faaada", "text": "Experiments on five long-horizon benchmarks demonstrate that AgeMem consistently outperforms strong memory-augmented baselines across multiple LLM backbones, achieving improved task performance, higher-quality long-term memory, and more efficient context usage.", "paper_id": "P0121", "citations": ["Yu2026Agentic"], "provenance": {"evidence_level": "abstract", "source": "paper_notes", "pointer": "papers/paper_notes.jsonl:paper_id=P0121#key_results[0]"}}, {"evidence_id": "E-P0169-0a0006ef38", "text": "The EM-Test assesses performance across various time spans and difficulty levels, providing a comprehensive evaluation of multi-turn episodic memory dialogues.", "paper_id": "P0169", "citations": ["Liu2025Echo"], "provenance": {"evidence_level": "abstract", "source": "paper_notes", "pointer": "papers/paper_notes.jsonl:paper_id=P0169#key_results[1]"}}, {"evidence_id": "E-P0032-53536132a8", "text": "We release TME's codebase, benchmarks, and components as open-source resources, enabling researchers to develop reliable LLM agents.", "paper_id": "P0032", "citations": ["Ye2025Task"], "provenance": {"evidence_level": "abstract", "source": "paper_notes", "pointer": "papers/paper_notes.jsonl:paper_id=P0032#key_results[1]"}}, {"evidence_id": "E-P0064-d568c53e1d", "text": "We evaluated GraphCodeAgent on three advanced LLMs with the two widely-used repo-level code generation benchmarks DevEval and CoderEval.", "paper_id": "P0064", "citations": ["Li2025Graphcodeagent"], "provenance": {"evidence_level": "abstract", "source": "paper_notes", "pointer": "papers/paper_notes.jsonl:paper_id=P0064#key_results[0]"}}], "definitions_setup": [{"bullet": "Setup: Which design choices in Memory and retrieval (RAG) drive the major trade-offs, and how are those trade-offs measured? Scope: in-scope: Core topics directly relevant to 'Memory and retrieval (RAG)'.. Axes: evaluation protocol (datasets, metrics, human evaluation); compute and latency constraints; tool interface contract (schemas / protocols); tool selection / routing policy; sandboxing / permissions / observability.", "citations": ["Verma2026Active", "Yu2026Agentic", "Tao2026Membox"]}], "claim_candidates": [{"claim": "Comprehensive evaluations on multiple benchmarks show that A-MemGuard effectively cuts attack success rates by over 95% while incurring a minimal utility cost.", "evidence_field": "evidence_snippet", "citations": ["Wei2025Memguard"]}, {"claim": "In this work, we propose Agent Distillation, a framework for transferring not only reasoning capability but full task-solving behavior from LLM-based agents into sLMs with retrieval and code tools.", "evidence_field": "evidence_snippet", "citations": ["Kang2025Distilling"]}, {"claim": "Despite the growing prominence of LLMs in this field, there is a scarcity of scholarly works that systematically synthesize how advanced enhancement techniques, specifically Retrieval-Augmented Generation (RAG) and Agentic systems, can be utilized to address these reliability and reasoning limitations.", "evidence_field": "evidence_snippet", "citations": ["Ge2025Surveya"]}, {"claim": "Structural drawings are widely used in many fields, e.g., mechanical engineering, civil engineering, etc.", "evidence_field": "evidence_snippet", "citations": ["Zhang2025Largea"]}, {"claim": "From this observation, we build memory retrieval as an autonomous, accurate, and compatible agent system, named MemR$^3$, which has two core mechanisms: 1) a router that selects among retrieve, reflect, and answer actions to optimize answer quality; 2) a global evidence-gap tracker that explicitly renders the answering process transparent and tracks the evidence collection process.", "evidence_field": "evidence_snippet", "citations": ["Du2025Memr"]}], "concrete_comparisons": [{"axis": "evaluation protocol (datasets, metrics, human evaluation)", "A_label": "Agent frameworks / architectures", "B_label": "Memory / retrieval augmentation", "A_papers": "Agent frameworks / architectures: `P0120`, `P0121`, `P0128`", "B_papers": "Memory / retrieval augmentation: `P0120`, `P0121`, `P0128`", "A_highlights": [{"paper_id": "P0026", "evidence_id": "E-P0026-f0ea009256", "excerpt": "Finally, we summarize the datasets and benchmarks used for evaluation and tuning, review key applications of LLM-based agents, and discuss major challenges and promising future directions.", "citations": ["Du2025Survey"], "pointer": "papers/paper_notes.jsonl:paper_id=P0026#key_results[0]", "score": 2}, {"paper_id": "P0121", "evidence_id": "E-P0121-f0f0faaada", "excerpt": "Experiments on five long-horizon benchmarks demonstrate that AgeMem consistently outperforms strong memory-augmented baselines across multiple LLM backbones, achieving improved task performance, higher-quality long-term memory, and more efficient context usage.", "citations": ["Yu2026Agentic"], "pointer": "papers/paper_notes.jsonl:paper_id=P0121#key_results[0]", "score": 1}], "B_highlights": [{"paper_id": "P0121", "evidence_id": "E-P0121-f0f0faaada", "excerpt": "Experiments on five long-horizon benchmarks demonstrate that AgeMem consistently outperforms strong memory-augmented baselines across multiple LLM backbones, achieving improved task performance, higher-quality long-term memory, and more efficient context usage.", "citations": ["Yu2026Agentic"], "pointer": "papers/paper_notes.jsonl:paper_id=P0121#key_results[0]", "score": 1}, {"paper_id": "P0062", "evidence_id": "E-P0062-86184d0d44", "excerpt": "Existing benchmarks either rely on limited context lengths or are tailored for static, long-context settings like book-based QA, which do not reflect the interactive, multi-turn nature of memory agents that incrementally accumulate information.", "citations": ["Hu2025Evaluating"], "pointer": "papers/paper_notes.jsonl:paper_id=P0062#key_results[1]", "score": 1}], "write_prompt": "Contrast Agent frameworks / architectures vs Memory / retrieval augmentation along \"evaluation protocol (datasets, metrics, human evaluation)\". Ground A and B using the highlight snippets; do not introduce new claims beyond the cited evidence.", "evidence_field": "evaluation protocol (datasets, metrics, human evaluation)", "citations": ["Du2025Survey", "Yu2026Agentic", "Hu2025Evaluating"]}, {"axis": "evaluation protocol (datasets, metrics, human evaluation)", "A_label": "Agent frameworks / architectures", "B_label": "Planning / reasoning loops", "A_papers": "Agent frameworks / architectures: `P0120`, `P0121`, `P0128`", "B_papers": "Planning / reasoning loops: `P0200`, `P0016`, `P0118`", "A_highlights": [{"paper_id": "P0026", "evidence_id": "E-P0026-f0ea009256", "excerpt": "Finally, we summarize the datasets and benchmarks used for evaluation and tuning, review key applications of LLM-based agents, and discuss major challenges and promising future directions.", "citations": ["Du2025Survey"], "pointer": "papers/paper_notes.jsonl:paper_id=P0026#key_results[0]", "score": 2}, {"paper_id": "P0121", "evidence_id": "E-P0121-f0f0faaada", "excerpt": "Experiments on five long-horizon benchmarks demonstrate that AgeMem consistently outperforms strong memory-augmented baselines across multiple LLM backbones, achieving improved task performance, higher-quality long-term memory, and more efficient context usage.", "citations": ["Yu2026Agentic"], "pointer": "papers/paper_notes.jsonl:paper_id=P0121#key_results[0]", "score": 1}], "B_highlights": [{"paper_id": "P0200", "evidence_id": "E-P0200-1b99de5317", "excerpt": "From this observation, we build memory retrieval as an autonomous, accurate, and compatible agent system, named MemR$^3$, which has two core mechanisms: 1) a router that selects among retrieve, reflect, and answer actions to optimize answer quality; 2) a global evidence-gap", "citations": ["Du2025Memr"], "pointer": "papers/paper_notes.jsonl:paper_id=P0200#key_results[1]", "score": 1}, {"paper_id": "P0001", "evidence_id": "E-P0001-ca4a00b5cf", "excerpt": "On two interactive decision making benchmarks (ALFWorld and WebShop), ReAct outperforms imitation and reinforcement learning methods by an absolute success rate of 34% and 10% respectively, while being prompted with only one or two in-context examples.", "citations": ["Yao2022React"], "pointer": "papers/paper_notes.jsonl:paper_id=P0001#key_results[0]", "score": 1}], "write_prompt": "Contrast Agent frameworks / architectures vs Planning / reasoning loops along \"evaluation protocol (datasets, metrics, human evaluation)\". Ground A and B using the highlight snippets; do not introduce new claims beyond the cited evidence.", "evidence_field": "evaluation protocol (datasets, metrics, human evaluation)", "citations": ["Du2025Survey", "Yu2026Agentic", "Du2025Memr", "Yao2022React"]}, {"axis": "evaluation protocol (datasets, metrics, human evaluation)", "A_label": "Memory / retrieval augmentation", "B_label": "Planning / reasoning loops", "A_papers": "Memory / retrieval augmentation: `P0120`, `P0121`, `P0128`", "B_papers": "Planning / reasoning loops: `P0200`, `P0016`, `P0118`", "A_highlights": [{"paper_id": "P0121", "evidence_id": "E-P0121-f0f0faaada", "excerpt": "Experiments on five long-horizon benchmarks demonstrate that AgeMem consistently outperforms strong memory-augmented baselines across multiple LLM backbones, achieving improved task performance, higher-quality long-term memory, and more efficient context usage.", "citations": ["Yu2026Agentic"], "pointer": "papers/paper_notes.jsonl:paper_id=P0121#key_results[0]", "score": 1}, {"paper_id": "P0062", "evidence_id": "E-P0062-86184d0d44", "excerpt": "Existing benchmarks either rely on limited context lengths or are tailored for static, long-context settings like book-based QA, which do not reflect the interactive, multi-turn nature of memory agents that incrementally accumulate information.", "citations": ["Hu2025Evaluating"], "pointer": "papers/paper_notes.jsonl:paper_id=P0062#key_results[1]", "score": 1}], "B_highlights": [{"paper_id": "P0200", "evidence_id": "E-P0200-1b99de5317", "excerpt": "From this observation, we build memory retrieval as an autonomous, accurate, and compatible agent system, named MemR$^3$, which has two core mechanisms: 1) a router that selects among retrieve, reflect, and answer actions to optimize answer quality; 2) a global evidence-gap", "citations": ["Du2025Memr"], "pointer": "papers/paper_notes.jsonl:paper_id=P0200#key_results[1]", "score": 1}, {"paper_id": "P0001", "evidence_id": "E-P0001-ca4a00b5cf", "excerpt": "On two interactive decision making benchmarks (ALFWorld and WebShop), ReAct outperforms imitation and reinforcement learning methods by an absolute success rate of 34% and 10% respectively, while being prompted with only one or two in-context examples.", "citations": ["Yao2022React"], "pointer": "papers/paper_notes.jsonl:paper_id=P0001#key_results[0]", "score": 1}], "write_prompt": "Contrast Memory / retrieval augmentation vs Planning / reasoning loops along \"evaluation protocol (datasets, metrics, human evaluation)\". Ground A and B using the highlight snippets; do not introduce new claims beyond the cited evidence.", "evidence_field": "evaluation protocol (datasets, metrics, human evaluation)", "citations": ["Yu2026Agentic", "Hu2025Evaluating", "Du2025Memr", "Yao2022React"]}, {"axis": "compute and latency constraints", "A_label": "Agent frameworks / architectures", "B_label": "Memory / retrieval augmentation", "A_papers": "Agent frameworks / architectures: `P0120`, `P0121`, `P0128`", "B_papers": "Memory / retrieval augmentation: `P0120`, `P0121`, `P0128`", "A_highlights": [{"paper_id": "P0011", "evidence_id": "E-P0011-1b6fe3407a", "excerpt": "To further accelerate research in this area for the community, we compile open-source training frameworks, training and evaluation datasets for developing agentic MLLMs.", "citations": ["Yao2025Survey"], "pointer": "papers/paper_notes.jsonl:paper_id=P0011#key_results[0]", "score": 2}, {"paper_id": "P0121", "evidence_id": "E-P0121-f0f0faaada", "excerpt": "Experiments on five long-horizon benchmarks demonstrate that AgeMem consistently outperforms strong memory-augmented baselines across multiple LLM backbones, achieving improved task performance, higher-quality long-term memory, and more efficient context usage.", "citations": ["Yu2026Agentic"], "pointer": "papers/paper_notes.jsonl:paper_id=P0121#key_results[0]", "score": 1}], "B_highlights": [{"paper_id": "P0121", "evidence_id": "E-P0121-f0f0faaada", "excerpt": "Experiments on five long-horizon benchmarks demonstrate that AgeMem consistently outperforms strong memory-augmented baselines across multiple LLM backbones, achieving improved task performance, higher-quality long-term memory, and more efficient context usage.", "citations": ["Yu2026Agentic"], "pointer": "papers/paper_notes.jsonl:paper_id=P0121#key_results[0]", "score": 1}, {"paper_id": "P0047", "evidence_id": "E-P0047-cb9370ac71", "excerpt": "Comprehensive evaluations on multiple benchmarks show that A-MemGuard effectively cuts attack success rates by over 95% while incurring a minimal utility cost.", "citations": ["Wei2025Memguard"], "pointer": "papers/paper_notes.jsonl:paper_id=P0047#key_results[0]", "score": 1}], "write_prompt": "Contrast Agent frameworks / architectures vs Memory / retrieval augmentation along \"compute and latency constraints\". Ground A and B using the highlight snippets; do not introduce new claims beyond the cited evidence.", "evidence_field": "compute and latency constraints", "citations": ["Yao2025Survey", "Yu2026Agentic", "Wei2025Memguard"]}, {"axis": "compute and latency constraints", "A_label": "Agent frameworks / architectures", "B_label": "Planning / reasoning loops", "A_papers": "Agent frameworks / architectures: `P0120`, `P0121`, `P0128`", "B_papers": "Planning / reasoning loops: `P0200`, `P0016`, `P0118`", "A_highlights": [{"paper_id": "P0011", "evidence_id": "E-P0011-1b6fe3407a", "excerpt": "To further accelerate research in this area for the community, we compile open-source training frameworks, training and evaluation datasets for developing agentic MLLMs.", "citations": ["Yao2025Survey"], "pointer": "papers/paper_notes.jsonl:paper_id=P0011#key_results[0]", "score": 2}, {"paper_id": "P0121", "evidence_id": "E-P0121-f0f0faaada", "excerpt": "Experiments on five long-horizon benchmarks demonstrate that AgeMem consistently outperforms strong memory-augmented baselines across multiple LLM backbones, achieving improved task performance, higher-quality long-term memory, and more efficient context usage.", "citations": ["Yu2026Agentic"], "pointer": "papers/paper_notes.jsonl:paper_id=P0121#key_results[0]", "score": 1}], "B_highlights": [{"paper_id": "P0200", "evidence_id": "E-P0200-1b99de5317", "excerpt": "From this observation, we build memory retrieval as an autonomous, accurate, and compatible agent system, named MemR$^3$, which has two core mechanisms: 1) a router that selects among retrieve, reflect, and answer actions to optimize answer quality; 2) a global evidence-gap", "citations": ["Du2025Memr"], "pointer": "papers/paper_notes.jsonl:paper_id=P0200#key_results[1]", "score": 0}, {"paper_id": "P0001", "evidence_id": "E-P0001-ca4a00b5cf", "excerpt": "On two interactive decision making benchmarks (ALFWorld and WebShop), ReAct outperforms imitation and reinforcement learning methods by an absolute success rate of 34% and 10% respectively, while being prompted with only one or two in-context examples.", "citations": ["Yao2022React"], "pointer": "papers/paper_notes.jsonl:paper_id=P0001#key_results[0]", "score": 0}], "write_prompt": "Contrast Agent frameworks / architectures vs Planning / reasoning loops along \"compute and latency constraints\". Ground A and B using the highlight snippets; do not introduce new claims beyond the cited evidence.", "evidence_field": "compute and latency constraints", "citations": ["Yao2025Survey", "Yu2026Agentic", "Du2025Memr", "Yao2022React"]}, {"axis": "compute and latency constraints", "A_label": "Memory / retrieval augmentation", "B_label": "Planning / reasoning loops", "A_papers": "Memory / retrieval augmentation: `P0120`, `P0121`, `P0128`", "B_papers": "Planning / reasoning loops: `P0200`, `P0016`, `P0118`", "A_highlights": [{"paper_id": "P0121", "evidence_id": "E-P0121-f0f0faaada", "excerpt": "Experiments on five long-horizon benchmarks demonstrate that AgeMem consistently outperforms strong memory-augmented baselines across multiple LLM backbones, achieving improved task performance, higher-quality long-term memory, and more efficient context usage.", "citations": ["Yu2026Agentic"], "pointer": "papers/paper_notes.jsonl:paper_id=P0121#key_results[0]", "score": 1}, {"paper_id": "P0047", "evidence_id": "E-P0047-cb9370ac71", "excerpt": "Comprehensive evaluations on multiple benchmarks show that A-MemGuard effectively cuts attack success rates by over 95% while incurring a minimal utility cost.", "citations": ["Wei2025Memguard"], "pointer": "papers/paper_notes.jsonl:paper_id=P0047#key_results[0]", "score": 1}], "B_highlights": [{"paper_id": "P0200", "evidence_id": "E-P0200-1b99de5317", "excerpt": "From this observation, we build memory retrieval as an autonomous, accurate, and compatible agent system, named MemR$^3$, which has two core mechanisms: 1) a router that selects among retrieve, reflect, and answer actions to optimize answer quality; 2) a global evidence-gap", "citations": ["Du2025Memr"], "pointer": "papers/paper_notes.jsonl:paper_id=P0200#key_results[1]", "score": 0}, {"paper_id": "P0001", "evidence_id": "E-P0001-ca4a00b5cf", "excerpt": "On two interactive decision making benchmarks (ALFWorld and WebShop), ReAct outperforms imitation and reinforcement learning methods by an absolute success rate of 34% and 10% respectively, while being prompted with only one or two in-context examples.", "citations": ["Yao2022React"], "pointer": "papers/paper_notes.jsonl:paper_id=P0001#key_results[0]", "score": 0}], "write_prompt": "Contrast Memory / retrieval augmentation vs Planning / reasoning loops along \"compute and latency constraints\". Ground A and B using the highlight snippets; do not introduce new claims beyond the cited evidence.", "evidence_field": "compute and latency constraints", "citations": ["Yu2026Agentic", "Wei2025Memguard", "Du2025Memr", "Yao2022React"]}, {"axis": "tool interface contract (schemas / protocols)", "A_label": "Agent frameworks / architectures", "B_label": "Memory / retrieval augmentation", "A_papers": "Agent frameworks / architectures: `P0120`, `P0121`, `P0128`", "B_papers": "Memory / retrieval augmentation: `P0120`, `P0121`, `P0128`", "A_highlights": [{"paper_id": "P0121", "evidence_id": "E-P0121-f0f0faaada", "excerpt": "Experiments on five long-horizon benchmarks demonstrate that AgeMem consistently outperforms strong memory-augmented baselines across multiple LLM backbones, achieving improved task performance, higher-quality long-term memory, and more efficient context usage.", "citations": ["Yu2026Agentic"], "pointer": "papers/paper_notes.jsonl:paper_id=P0121#key_results[0]", "score": 0}, {"paper_id": "P0120", "evidence_id": "E-P0120-9abcf1bf8a", "excerpt": "Using an optimized scaffold matching industry best practices (persistent bash + string-replacement editor), we evaluated Focus on N=5 context-intensive instances from SWE-bench Lite using Claude Haiku 4.5.", "citations": ["Verma2026Active"], "pointer": "papers/paper_notes.jsonl:paper_id=P0120#key_results[1]", "score": 0}], "B_highlights": [{"paper_id": "P0166", "evidence_id": "E-P0166-17fb12f5b7", "excerpt": "In this work, we propose Agent Distillation, a framework for transferring not only reasoning capability but full task-solving behavior from LLM-based agents into sLMs with retrieval and code tools.", "citations": ["Kang2025Distilling"], "pointer": "papers/paper_notes.jsonl:paper_id=P0166#method", "score": 1}, {"paper_id": "P0121", "evidence_id": "E-P0121-f0f0faaada", "excerpt": "Experiments on five long-horizon benchmarks demonstrate that AgeMem consistently outperforms strong memory-augmented baselines across multiple LLM backbones, achieving improved task performance, higher-quality long-term memory, and more efficient context usage.", "citations": ["Yu2026Agentic"], "pointer": "papers/paper_notes.jsonl:paper_id=P0121#key_results[0]", "score": 0}], "write_prompt": "Contrast Agent frameworks / architectures vs Memory / retrieval augmentation along \"tool interface contract (schemas / protocols)\". Ground A and B using the highlight snippets; do not introduce new claims beyond the cited evidence.", "evidence_field": "tool interface contract (schemas / protocols)", "citations": ["Yu2026Agentic", "Verma2026Active", "Kang2025Distilling"]}, {"axis": "tool interface contract (schemas / protocols)", "A_label": "Agent frameworks / architectures", "B_label": "Planning / reasoning loops", "A_papers": "Agent frameworks / architectures: `P0120`, `P0121`, `P0128`", "B_papers": "Planning / reasoning loops: `P0200`, `P0016`, `P0118`", "A_highlights": [{"paper_id": "P0121", "evidence_id": "E-P0121-f0f0faaada", "excerpt": "Experiments on five long-horizon benchmarks demonstrate that AgeMem consistently outperforms strong memory-augmented baselines across multiple LLM backbones, achieving improved task performance, higher-quality long-term memory, and more efficient context usage.", "citations": ["Yu2026Agentic"], "pointer": "papers/paper_notes.jsonl:paper_id=P0121#key_results[0]", "score": 0}, {"paper_id": "P0120", "evidence_id": "E-P0120-9abcf1bf8a", "excerpt": "Using an optimized scaffold matching industry best practices (persistent bash + string-replacement editor), we evaluated Focus on N=5 context-intensive instances from SWE-bench Lite using Claude Haiku 4.5.", "citations": ["Verma2026Active"], "pointer": "papers/paper_notes.jsonl:paper_id=P0120#key_results[1]", "score": 0}], "B_highlights": [{"paper_id": "P0200", "evidence_id": "E-P0200-1b99de5317", "excerpt": "From this observation, we build memory retrieval as an autonomous, accurate, and compatible agent system, named MemR$^3$, which has two core mechanisms: 1) a router that selects among retrieve, reflect, and answer actions to optimize answer quality; 2) a global evidence-gap", "citations": ["Du2025Memr"], "pointer": "papers/paper_notes.jsonl:paper_id=P0200#key_results[1]", "score": 1}, {"paper_id": "P0016", "evidence_id": "E-P0016-62b8101d4e", "excerpt": "Tool use, planning, and feedback learning are currently three prominent paradigms for developing Large Language Model (LLM)-based agents across various tasks.", "citations": ["Li2024Review"], "pointer": "papers/paper_notes.jsonl:paper_id=P0016#method", "score": 1}], "write_prompt": "Contrast Agent frameworks / architectures vs Planning / reasoning loops along \"tool interface contract (schemas / protocols)\". Ground A and B using the highlight snippets; do not introduce new claims beyond the cited evidence.", "evidence_field": "tool interface contract (schemas / protocols)", "citations": ["Yu2026Agentic", "Verma2026Active", "Du2025Memr", "Li2024Review"]}, {"axis": "tool interface contract (schemas / protocols)", "A_label": "Memory / retrieval augmentation", "B_label": "Planning / reasoning loops", "A_papers": "Memory / retrieval augmentation: `P0120`, `P0121`, `P0128`", "B_papers": "Planning / reasoning loops: `P0200`, `P0016`, `P0118`", "A_highlights": [{"paper_id": "P0166", "evidence_id": "E-P0166-17fb12f5b7", "excerpt": "In this work, we propose Agent Distillation, a framework for transferring not only reasoning capability but full task-solving behavior from LLM-based agents into sLMs with retrieval and code tools.", "citations": ["Kang2025Distilling"], "pointer": "papers/paper_notes.jsonl:paper_id=P0166#method", "score": 1}, {"paper_id": "P0121", "evidence_id": "E-P0121-f0f0faaada", "excerpt": "Experiments on five long-horizon benchmarks demonstrate that AgeMem consistently outperforms strong memory-augmented baselines across multiple LLM backbones, achieving improved task performance, higher-quality long-term memory, and more efficient context usage.", "citations": ["Yu2026Agentic"], "pointer": "papers/paper_notes.jsonl:paper_id=P0121#key_results[0]", "score": 0}], "B_highlights": [{"paper_id": "P0200", "evidence_id": "E-P0200-1b99de5317", "excerpt": "From this observation, we build memory retrieval as an autonomous, accurate, and compatible agent system, named MemR$^3$, which has two core mechanisms: 1) a router that selects among retrieve, reflect, and answer actions to optimize answer quality; 2) a global evidence-gap", "citations": ["Du2025Memr"], "pointer": "papers/paper_notes.jsonl:paper_id=P0200#key_results[1]", "score": 1}, {"paper_id": "P0016", "evidence_id": "E-P0016-62b8101d4e", "excerpt": "Tool use, planning, and feedback learning are currently three prominent paradigms for developing Large Language Model (LLM)-based agents across various tasks.", "citations": ["Li2024Review"], "pointer": "papers/paper_notes.jsonl:paper_id=P0016#method", "score": 1}], "write_prompt": "Contrast Memory / retrieval augmentation vs Planning / reasoning loops along \"tool interface contract (schemas / protocols)\". Ground A and B using the highlight snippets; do not introduce new claims beyond the cited evidence.", "evidence_field": "tool interface contract (schemas / protocols)", "citations": ["Kang2025Distilling", "Yu2026Agentic", "Du2025Memr", "Li2024Review"]}, {"axis": "tool selection / routing policy", "A_label": "Agent frameworks / architectures", "B_label": "Memory / retrieval augmentation", "A_papers": "Agent frameworks / architectures: `P0120`, `P0121`, `P0128`", "B_papers": "Memory / retrieval augmentation: `P0120`, `P0121`, `P0128`", "A_highlights": [{"paper_id": "P0121", "evidence_id": "E-P0121-f0f0faaada", "excerpt": "Experiments on five long-horizon benchmarks demonstrate that AgeMem consistently outperforms strong memory-augmented baselines across multiple LLM backbones, achieving improved task performance, higher-quality long-term memory, and more efficient context usage.", "citations": ["Yu2026Agentic"], "pointer": "papers/paper_notes.jsonl:paper_id=P0121#key_results[0]", "score": 0}, {"paper_id": "P0120", "evidence_id": "E-P0120-9abcf1bf8a", "excerpt": "Using an optimized scaffold matching industry best practices (persistent bash + string-replacement editor), we evaluated Focus on N=5 context-intensive instances from SWE-bench Lite using Claude Haiku 4.5.", "citations": ["Verma2026Active"], "pointer": "papers/paper_notes.jsonl:paper_id=P0120#key_results[1]", "score": 0}], "B_highlights": [{"paper_id": "P0166", "evidence_id": "E-P0166-17fb12f5b7", "excerpt": "In this work, we propose Agent Distillation, a framework for transferring not only reasoning capability but full task-solving behavior from LLM-based agents into sLMs with retrieval and code tools.", "citations": ["Kang2025Distilling"], "pointer": "papers/paper_notes.jsonl:paper_id=P0166#method", "score": 1}, {"paper_id": "P0121", "evidence_id": "E-P0121-f0f0faaada", "excerpt": "Experiments on five long-horizon benchmarks demonstrate that AgeMem consistently outperforms strong memory-augmented baselines across multiple LLM backbones, achieving improved task performance, higher-quality long-term memory, and more efficient context usage.", "citations": ["Yu2026Agentic"], "pointer": "papers/paper_notes.jsonl:paper_id=P0121#key_results[0]", "score": 0}], "write_prompt": "Contrast Agent frameworks / architectures vs Memory / retrieval augmentation along \"tool selection / routing policy\". Ground A and B using the highlight snippets; do not introduce new claims beyond the cited evidence.", "evidence_field": "tool selection / routing policy", "citations": ["Yu2026Agentic", "Verma2026Active", "Kang2025Distilling"]}], "evaluation_protocol": [{"bullet": "Evaluation mentions include: SWE-bench, LTM, STM, GRPO, AgeMem, MEM, LoCoMo, AGI, MLLMs, MLLM-based.", "citations": ["Verma2026Active", "Yu2026Agentic", "Tao2026Membox", "Yao2025Survey"]}, {"bullet": "When comparing results, anchor the paragraph with: task type + metric + constraint (budget, tool access, horizon, or threat model) when stated.", "citations": ["Verma2026Active", "Yu2026Agentic"]}, {"bullet": "Prefer head-to-head comparisons only when benchmark/metric are shared; otherwise frame differences as protocol-driven rather than method superiority.", "citations": ["Verma2026Active", "Yu2026Agentic"]}, {"bullet": "Avoid underspecified model/baseline naming; if abstracts omit details, state that the baseline is reported but underspecified instead of guessing.", "citations": ["Verma2026Active", "Yu2026Agentic"]}, {"bullet": "If a claim relies on a single reported number, pair it with a limitation/caveat from the same evidence so the draft remains conservative.", "citations": ["Verma2026Active", "Yu2026Agentic"]}, {"bullet": "If budgets or environments differ across papers, treat cross-paper numeric comparison as fragile and prefer qualitative contrasts aligned to the subsection axes.", "citations": ["Verma2026Active", "Yu2026Agentic"]}], "failures_limitations": [{"bullet": "Large language model (LLM) agents face fundamental limitations in long-horizon reasoning due to finite context windows, making effective memory management critical.", "citations": ["Yu2026Agentic"]}, {"bullet": "Yet their sophisticated architectures amplify vulnerability to cascading failures, where a single root-cause error propagates through subsequent decisions, leading to task failure.", "citations": ["Zhu2025Where"]}, {"bullet": "First, we introduce the AgentErrorTaxonomy, a modular classification of failure modes spanning memory, reflection, planning, action, and system-level operations.", "citations": ["Zhu2025Where"]}, {"bullet": "Second, we construct AgentErrorBench, the first dataset of systematically annotated failure trajectories from ALFWorld, GAIA, and WebShop, grounding error analysis in real-world agent rollouts.", "citations": ["Zhu2025Where"]}, {"bullet": "However, this reliance on memory introduces a critical security risk: an adversary can inject seemingly harmless records into an agent's memory to manipulate its future behavior.", "citations": ["Wei2025Memguard"]}, {"bullet": "Comprehensive evaluations on multiple benchmarks show that A-MemGuard effectively cuts attack success rates by over 95% while incurring a minimal utility cost.", "citations": ["Wei2025Memguard"]}, {"bullet": "This work shifts LLM memory security from static filtering to a proactive, experience-driven model where defenses strengthen over time.", "citations": ["Wei2025Memguard"]}, {"bullet": "This results in the failure to retrieve the relevant code of these fine-grained subtasks.", "citations": ["Li2025Graphcodeagent"]}], "blocking_missing": [], "verify_fields": ["named benchmarks/datasets used", "metrics/human-eval protocol", "compute/training/inference cost", "training data and supervision signal", "baseline choices and ablation evidence"], "generated_at": "2026-01-25T17:06:54", "section_id": "4", "section_title": "Core Components (Planning + Memory)"}
{"sub_id": "5.1", "title": "Self-improvement and adaptation", "evidence_ids": ["E-P0144-7d85a7241d", "E-P0195-8bcb673a7d", "E-P0079-01f439b837", "E-P0214-2e6956a116", "E-P0190-3620ae9178", "E-P0087-6273763a98", "E-P0167-047d14c804", "E-P0099-33c7e5174b", "E-P0171-ca050ed55f", "E-P0187-ec46292a2b", "E-P0042-44dce85d3e", "E-P0050-f38be5e04b", "E-P0133-a68f39bc04", "E-P0180-af945eb2fa", "E-P0051-eae2720c71", "E-P0260-76aa5df0e2", "E-P0205-85b5f3734b", "E-P0261-3e23cb8c51", "E-P0093-faa3d4c9ee", "E-P0208-07f6a1adfe", "E-P0245-ebe044543a", "E-P0084-61917beed9", "E-P0084-ffbc3301be", "E-P0245-3df3fdc7a7"], "evidence_level_summary": {"fulltext": 0, "abstract": 28, "title": 0}, "evidence_snippets": [{"evidence_id": "E-P0144-7d85a7241d", "text": "Across agent and domain-specific benchmarks, ACE optimizes contexts both offline (e.g., system prompts) and online (e.g., agent memory), consistently outperforming strong baselines: +10.6% on agents and +8.6% on finance, while significantly reducing adaptation latency and rollout cost.", "paper_id": "P0144", "citations": ["Zhang2025Agentic"], "provenance": {"evidence_level": "abstract", "source": "paper_notes", "pointer": "papers/paper_notes.jsonl:paper_id=P0144#key_results[0]"}}, {"evidence_id": "E-P0195-8bcb673a7d", "text": "We present MSB (MCP Security Benchmark), the first end-to-end evaluation suite that systematically measures how well LLM agents resist MCP-specific attacks throughout the full tool-use pipeline: task planning, tool invocation, and response handling.", "paper_id": "P0195", "citations": ["Zhang2025Security"], "provenance": {"evidence_level": "abstract", "source": "paper_notes", "pointer": "papers/paper_notes.jsonl:paper_id=P0195#method"}}, {"evidence_id": "E-P0079-01f439b837", "text": "We address this limitation by introducing a framework that enables LLM agents to learn and evolve both before and during test time, equipping them with environment-relevant knowledge for better planning and enhanced communication for improved cooperation.", "paper_id": "P0079", "citations": ["Li2025Learn"], "provenance": {"evidence_level": "abstract", "source": "paper_notes", "pointer": "papers/paper_notes.jsonl:paper_id=P0079#limitations[1]"}}, {"evidence_id": "E-P0214-2e6956a116", "text": "Evaluation on two existing multi-turn tool-use agent benchmarks, M3ToolEval and TauBench, shows the Self-Challenging framework achieves over a two-fold improvement in Llama-3.1-8B-Instruct, despite using only self-generated training data.", "paper_id": "P0214", "citations": ["Zhou2025Self"], "provenance": {"evidence_level": "abstract", "source": "paper_notes", "pointer": "papers/paper_notes.jsonl:paper_id=P0214#key_results[0]"}}, {"evidence_id": "E-P0190-3620ae9178", "text": "From the data science perspective, we identify key processes for LLM-based agents, including data preprocessing, model development, evaluation, visualization, etc. Our work offers two key contributions: (1) a comprehensive review of recent developments in applying LLMbased agents to data science tasks; (2) a dual-perspective framework that connects general agent design principles with the practical workflows in data science.", "paper_id": "P0190", "citations": ["Chen2025Largea"], "provenance": {"evidence_level": "abstract", "source": "paper_notes", "pointer": "papers/paper_notes.jsonl:paper_id=P0190#key_results[0]"}}, {"evidence_id": "E-P0087-6273763a98", "text": "Evaluating on two representative interactive agent tasks, SAND achieves an average 20% improvement over initial supervised finetuning and also outperforms state-of-the-art agent tuning approaches.", "paper_id": "P0087", "citations": ["Xia2025Sand"], "provenance": {"evidence_level": "abstract", "source": "paper_notes", "pointer": "papers/paper_notes.jsonl:paper_id=P0087#key_results[0]"}}, {"evidence_id": "E-P0167-047d14c804", "text": "The results indicate that 1) all tested LLMs spontaneously misrepresent their actions in at least some conditions, 2) they are generally more likely to do so in situations in which deception would benefit them, and 3) models exhibiting better reasoning capacity overall tend to deceive at higher rates.", "paper_id": "P0167", "citations": ["Taylor2025Large"], "provenance": {"evidence_level": "abstract", "source": "paper_notes", "pointer": "papers/paper_notes.jsonl:paper_id=P0167#key_results[0]"}}, {"evidence_id": "E-P0099-33c7e5174b", "text": "We review and differentiate the work of LLMs and LLM-based agents from these six topics, examining their differences and similarities in tasks, benchmarks, and evaluation metrics.", "paper_id": "P0099", "citations": ["Jin2024From"], "provenance": {"evidence_level": "abstract", "source": "paper_notes", "pointer": "papers/paper_notes.jsonl:paper_id=P0099#key_results[1]"}}, {"evidence_id": "E-P0171-ca050ed55f", "text": "We evaluate EpidemIQs across different epidemic scenarios, measuring computational cost, completion success rate, and AI and human expert reviews of generated reports.", "paper_id": "P0171", "citations": ["Samaei2025Epidemiqs"], "provenance": {"evidence_level": "abstract", "source": "paper_notes", "pointer": "papers/paper_notes.jsonl:paper_id=P0171#key_results[1]"}}, {"evidence_id": "E-P0187-ec46292a2b", "text": "We then propose a taxonomy covering fundamental agent frameworks (single-agent, multi-agent, plan-then-act), modeling approaches (prompt engineering, training-based), and essential datasets and benchmarks.", "paper_id": "P0187", "citations": ["Liu2025Powered"], "provenance": {"evidence_level": "abstract", "source": "paper_notes", "pointer": "papers/paper_notes.jsonl:paper_id=P0187#key_results[0]"}}, {"evidence_id": "E-P0042-44dce85d3e", "text": "We further review representative agentic reasoning frameworks across real-world applications and benchmarks, including science, robotics, healthcare, autonomous research, and mathematics.", "paper_id": "P0042", "citations": ["Wei2026Agentic"], "provenance": {"evidence_level": "abstract", "source": "paper_notes", "pointer": "papers/paper_notes.jsonl:paper_id=P0042#key_results[0]"}}, {"evidence_id": "E-P0050-f38be5e04b", "text": "It outlines the general workflow of the task and establishes a taxonomy across three dimensions: benchmarks, techniques, and empirical studies.", "paper_id": "P0050", "citations": ["Jiang2025Agentic"], "provenance": {"evidence_level": "abstract", "source": "paper_notes", "pointer": "papers/paper_notes.jsonl:paper_id=P0050#key_results[1]"}}, {"evidence_id": "E-P0133-a68f39bc04", "text": "This survey provides a comprehensive overview of foundation models, agentic systems, datasets, and computational tools supporting this growing field.", "paper_id": "P0133", "citations": ["Van2025Survey"], "provenance": {"evidence_level": "abstract", "source": "paper_notes", "pointer": "papers/paper_notes.jsonl:paper_id=P0133#key_results[0]"}}, {"evidence_id": "E-P0180-af945eb2fa", "text": "We evaluate these strategies across diverse agentic benchmarks, including function calling and web navigation.", "paper_id": "P0180", "citations": ["Chen2025Grounded"], "provenance": {"evidence_level": "abstract", "source": "paper_notes", "pointer": "papers/paper_notes.jsonl:paper_id=P0180#key_results[1]"}}, {"evidence_id": "E-P0051-eae2720c71", "text": "Settlers of Catan provides a challenging benchmark: success depends on balancing short- and long-term goals amid randomness, trading, expansion, and blocking.", "paper_id": "P0051", "citations": ["Belle2025Agents"], "provenance": {"evidence_level": "abstract", "source": "paper_notes", "pointer": "papers/paper_notes.jsonl:paper_id=P0051#key_results[1]"}}, {"evidence_id": "E-P0260-76aa5df0e2", "text": "We conducted extensive experiments and the results show that FICAL has competitive performance compared to other SOTA baselines with a significant communication cost decrease of $\\mathbf{3.33\\times10^5}$ times.", "paper_id": "P0260", "citations": ["Wu2024Federated"], "provenance": {"evidence_level": "abstract", "source": "paper_notes", "pointer": "papers/paper_notes.jsonl:paper_id=P0260#key_results[0]"}}, {"evidence_id": "E-P0205-85b5f3734b", "text": "Empirical results averaged across models show major improvements in task success, with absolute gains of +10.3% on ALFWorld, +23.8% on BabyAI, and +8.3% on PDDL in the Self-sustaining mode.", "paper_id": "P0205", "citations": ["Bharadwaj2025Omnireflect"], "provenance": {"evidence_level": "abstract", "source": "paper_notes", "pointer": "papers/paper_notes.jsonl:paper_id=P0205#key_results[0]"}}, {"evidence_id": "E-P0261-3e23cb8c51", "text": "We categorize the simulations into three types: (1) Individual Simulation, which mimics specific individuals or demographic groups; (2) Scenario Simulation, where multiple agents collaborate to achieve goals within specific contexts; and (3) Society Simulation, which models interactions within agent societies to reflect the complexity and variety of real-world dynamics.", "paper_id": "P0261", "citations": ["Mou2024From"], "provenance": {"evidence_level": "abstract", "source": "paper_notes", "pointer": "papers/paper_notes.jsonl:paper_id=P0261#key_results[0]"}}], "definitions_setup": [{"bullet": "Setup: Which design choices in Self-improvement and adaptation drive the major trade-offs, and how are those trade-offs measured? Scope: in-scope: Core topics directly relevant to 'Self-improvement and adaptation'.. Axes: evaluation protocol (datasets, metrics, human evaluation); compute and latency constraints; communication protocol / roles; aggregation (vote / debate / referee); stability / robustness.", "citations": ["Wei2026Agentic", "Jiang2025Agentic", "Belle2025Agents"]}], "claim_candidates": [{"claim": "Across agent and domain-specific benchmarks, ACE optimizes contexts both offline (e.g., system prompts) and online (e.g., agent memory), consistently outperforming strong baselines: +10.6% on agents and +8.6% on finance, while significantly reducing adaptation latency and rollout cost.", "evidence_field": "evidence_snippet", "citations": ["Zhang2025Agentic"]}, {"claim": "We present MSB (MCP Security Benchmark), the first end-to-end evaluation suite that systematically measures how well LLM agents resist MCP-specific attacks throughout the full tool-use pipeline: task planning, tool invocation, and response handling.", "evidence_field": "evidence_snippet", "citations": ["Zhang2025Security"]}, {"claim": "We address this limitation by introducing a framework that enables LLM agents to learn and evolve both before and during test time, equipping them with environment-relevant knowledge for better planning and enhanced communication for improved cooperation.", "evidence_field": "evidence_snippet", "citations": ["Li2025Learn"]}, {"claim": "Evaluation on two existing multi-turn tool-use agent benchmarks, M3ToolEval and TauBench, shows the Self-Challenging framework achieves over a two-fold improvement in Llama-3.1-8B-Instruct, despite using only self-generated training data.", "evidence_field": "evidence_snippet", "citations": ["Zhou2025Self"]}, {"claim": "From the data science perspective, we identify key processes for LLM-based agents, including data preprocessing, model development, evaluation, visualization, etc.", "evidence_field": "evidence_snippet", "citations": ["Chen2025Largea"]}], "concrete_comparisons": [{"axis": "evaluation protocol (datasets, metrics, human evaluation)", "A_label": "Agent frameworks / architectures", "B_label": "Planning / reasoning loops", "A_papers": "Agent frameworks / architectures: `P0042`, `P0050`, `P0051`", "B_papers": "Planning / reasoning loops: `P0042`, `P0051`, `P0136`", "A_highlights": [{"paper_id": "P0133", "evidence_id": "E-P0133-a68f39bc04", "excerpt": "This survey provides a comprehensive overview of foundation models, agentic systems, datasets, and computational tools supporting this growing field.", "citations": ["Van2025Survey"], "pointer": "papers/paper_notes.jsonl:paper_id=P0133#key_results[0]", "score": 2}, {"paper_id": "P0144", "evidence_id": "E-P0144-7d85a7241d", "excerpt": "Across agent and domain-specific benchmarks, ACE optimizes contexts both offline (e.g., system prompts) and online (e.g., agent memory), consistently outperforming strong baselines: +10.6% on agents and +8.6% on finance, while significantly reducing adaptation latency and", "citations": ["Zhang2025Agentic"], "pointer": "papers/paper_notes.jsonl:paper_id=P0144#key_results[0]", "score": 1}], "B_highlights": [{"paper_id": "P0042", "evidence_id": "E-P0042-44dce85d3e", "excerpt": "We further review representative agentic reasoning frameworks across real-world applications and benchmarks, including science, robotics, healthcare, autonomous research, and mathematics.", "citations": ["Wei2026Agentic"], "pointer": "papers/paper_notes.jsonl:paper_id=P0042#key_results[0]", "score": 1}, {"paper_id": "P0051", "evidence_id": "E-P0051-eae2720c71", "excerpt": "Settlers of Catan provides a challenging benchmark: success depends on balancing short- and long-term goals amid randomness, trading, expansion, and blocking.", "citations": ["Belle2025Agents"], "pointer": "papers/paper_notes.jsonl:paper_id=P0051#key_results[1]", "score": 1}], "write_prompt": "Contrast Agent frameworks / architectures vs Planning / reasoning loops along \"evaluation protocol (datasets, metrics, human evaluation)\". Ground A and B using the highlight snippets; do not introduce new claims beyond the cited evidence.", "evidence_field": "evaluation protocol (datasets, metrics, human evaluation)", "citations": ["Van2025Survey", "Zhang2025Agentic", "Wei2026Agentic", "Belle2025Agents"]}, {"axis": "evaluation protocol (datasets, metrics, human evaluation)", "A_label": "Agent frameworks / architectures", "B_label": "Code agents / software tasks", "A_papers": "Agent frameworks / architectures: `P0042`, `P0050`, `P0051`", "B_papers": "Code agents / software tasks: `P0050`, `P0221`, `P0099`", "A_highlights": [{"paper_id": "P0133", "evidence_id": "E-P0133-a68f39bc04", "excerpt": "This survey provides a comprehensive overview of foundation models, agentic systems, datasets, and computational tools supporting this growing field.", "citations": ["Van2025Survey"], "pointer": "papers/paper_notes.jsonl:paper_id=P0133#key_results[0]", "score": 2}, {"paper_id": "P0144", "evidence_id": "E-P0144-7d85a7241d", "excerpt": "Across agent and domain-specific benchmarks, ACE optimizes contexts both offline (e.g., system prompts) and online (e.g., agent memory), consistently outperforming strong baselines: +10.6% on agents and +8.6% on finance, while significantly reducing adaptation latency and", "citations": ["Zhang2025Agentic"], "pointer": "papers/paper_notes.jsonl:paper_id=P0144#key_results[0]", "score": 1}], "B_highlights": [{"paper_id": "P0099", "evidence_id": "E-P0099-33c7e5174b", "excerpt": "We review and differentiate the work of LLMs and LLM-based agents from these six topics, examining their differences and similarities in tasks, benchmarks, and evaluation metrics.", "citations": ["Jin2024From"], "pointer": "papers/paper_notes.jsonl:paper_id=P0099#key_results[1]", "score": 1}, {"paper_id": "P0050", "evidence_id": "E-P0050-f38be5e04b", "excerpt": "It outlines the general workflow of the task and establishes a taxonomy across three dimensions: benchmarks, techniques, and empirical studies.", "citations": ["Jiang2025Agentic"], "pointer": "papers/paper_notes.jsonl:paper_id=P0050#key_results[1]", "score": 1}], "write_prompt": "Contrast Agent frameworks / architectures vs Code agents / software tasks along \"evaluation protocol (datasets, metrics, human evaluation)\". Ground A and B using the highlight snippets; do not introduce new claims beyond the cited evidence.", "evidence_field": "evaluation protocol (datasets, metrics, human evaluation)", "citations": ["Van2025Survey", "Zhang2025Agentic", "Jin2024From", "Jiang2025Agentic"]}, {"axis": "evaluation protocol (datasets, metrics, human evaluation)", "A_label": "Planning / reasoning loops", "B_label": "Code agents / software tasks", "A_papers": "Planning / reasoning loops: `P0042`, `P0051`, `P0136`", "B_papers": "Code agents / software tasks: `P0050`, `P0221`, `P0099`", "A_highlights": [{"paper_id": "P0042", "evidence_id": "E-P0042-44dce85d3e", "excerpt": "We further review representative agentic reasoning frameworks across real-world applications and benchmarks, including science, robotics, healthcare, autonomous research, and mathematics.", "citations": ["Wei2026Agentic"], "pointer": "papers/paper_notes.jsonl:paper_id=P0042#key_results[0]", "score": 1}, {"paper_id": "P0051", "evidence_id": "E-P0051-eae2720c71", "excerpt": "Settlers of Catan provides a challenging benchmark: success depends on balancing short- and long-term goals amid randomness, trading, expansion, and blocking.", "citations": ["Belle2025Agents"], "pointer": "papers/paper_notes.jsonl:paper_id=P0051#key_results[1]", "score": 1}], "B_highlights": [{"paper_id": "P0099", "evidence_id": "E-P0099-33c7e5174b", "excerpt": "We review and differentiate the work of LLMs and LLM-based agents from these six topics, examining their differences and similarities in tasks, benchmarks, and evaluation metrics.", "citations": ["Jin2024From"], "pointer": "papers/paper_notes.jsonl:paper_id=P0099#key_results[1]", "score": 1}, {"paper_id": "P0050", "evidence_id": "E-P0050-f38be5e04b", "excerpt": "It outlines the general workflow of the task and establishes a taxonomy across three dimensions: benchmarks, techniques, and empirical studies.", "citations": ["Jiang2025Agentic"], "pointer": "papers/paper_notes.jsonl:paper_id=P0050#key_results[1]", "score": 1}], "write_prompt": "Contrast Planning / reasoning loops vs Code agents / software tasks along \"evaluation protocol (datasets, metrics, human evaluation)\". Ground A and B using the highlight snippets; do not introduce new claims beyond the cited evidence.", "evidence_field": "evaluation protocol (datasets, metrics, human evaluation)", "citations": ["Wei2026Agentic", "Belle2025Agents", "Jin2024From", "Jiang2025Agentic"]}, {"axis": "compute and latency constraints", "A_label": "Agent frameworks / architectures", "B_label": "Planning / reasoning loops", "A_papers": "Agent frameworks / architectures: `P0042`, `P0050`, `P0051`", "B_papers": "Planning / reasoning loops: `P0042`, `P0051`, `P0136`", "A_highlights": [{"paper_id": "P0144", "evidence_id": "E-P0144-7d85a7241d", "excerpt": "Across agent and domain-specific benchmarks, ACE optimizes contexts both offline (e.g., system prompts) and online (e.g., agent memory), consistently outperforming strong baselines: +10.6% on agents and +8.6% on finance, while significantly reducing adaptation latency and", "citations": ["Zhang2025Agentic"], "pointer": "papers/paper_notes.jsonl:paper_id=P0144#key_results[0]", "score": 2}, {"paper_id": "P0079", "evidence_id": "E-P0079-01f439b837", "excerpt": "We address this limitation by introducing a framework that enables LLM agents to learn and evolve both before and during test time, equipping them with environment-relevant knowledge for better planning and enhanced communication for improved cooperation.", "citations": ["Li2025Learn"], "pointer": "papers/paper_notes.jsonl:paper_id=P0079#limitations[1]", "score": 0}], "B_highlights": [{"paper_id": "P0042", "evidence_id": "E-P0042-44dce85d3e", "excerpt": "We further review representative agentic reasoning frameworks across real-world applications and benchmarks, including science, robotics, healthcare, autonomous research, and mathematics.", "citations": ["Wei2026Agentic"], "pointer": "papers/paper_notes.jsonl:paper_id=P0042#key_results[0]", "score": 0}, {"paper_id": "P0051", "evidence_id": "E-P0051-eae2720c71", "excerpt": "Settlers of Catan provides a challenging benchmark: success depends on balancing short- and long-term goals amid randomness, trading, expansion, and blocking.", "citations": ["Belle2025Agents"], "pointer": "papers/paper_notes.jsonl:paper_id=P0051#key_results[1]", "score": 0}], "write_prompt": "Contrast Agent frameworks / architectures vs Planning / reasoning loops along \"compute and latency constraints\". Ground A and B using the highlight snippets; do not introduce new claims beyond the cited evidence.", "evidence_field": "compute and latency constraints", "citations": ["Zhang2025Agentic", "Li2025Learn", "Wei2026Agentic", "Belle2025Agents"]}, {"axis": "compute and latency constraints", "A_label": "Agent frameworks / architectures", "B_label": "Code agents / software tasks", "A_papers": "Agent frameworks / architectures: `P0042`, `P0050`, `P0051`", "B_papers": "Code agents / software tasks: `P0050`, `P0221`, `P0099`", "A_highlights": [{"paper_id": "P0144", "evidence_id": "E-P0144-7d85a7241d", "excerpt": "Across agent and domain-specific benchmarks, ACE optimizes contexts both offline (e.g., system prompts) and online (e.g., agent memory), consistently outperforming strong baselines: +10.6% on agents and +8.6% on finance, while significantly reducing adaptation latency and", "citations": ["Zhang2025Agentic"], "pointer": "papers/paper_notes.jsonl:paper_id=P0144#key_results[0]", "score": 2}, {"paper_id": "P0079", "evidence_id": "E-P0079-01f439b837", "excerpt": "We address this limitation by introducing a framework that enables LLM agents to learn and evolve both before and during test time, equipping them with environment-relevant knowledge for better planning and enhanced communication for improved cooperation.", "citations": ["Li2025Learn"], "pointer": "papers/paper_notes.jsonl:paper_id=P0079#limitations[1]", "score": 0}], "B_highlights": [{"paper_id": "P0099", "evidence_id": "E-P0099-33c7e5174b", "excerpt": "We review and differentiate the work of LLMs and LLM-based agents from these six topics, examining their differences and similarities in tasks, benchmarks, and evaluation metrics.", "citations": ["Jin2024From"], "pointer": "papers/paper_notes.jsonl:paper_id=P0099#key_results[1]", "score": 0}, {"paper_id": "P0050", "evidence_id": "E-P0050-f38be5e04b", "excerpt": "It outlines the general workflow of the task and establishes a taxonomy across three dimensions: benchmarks, techniques, and empirical studies.", "citations": ["Jiang2025Agentic"], "pointer": "papers/paper_notes.jsonl:paper_id=P0050#key_results[1]", "score": 0}], "write_prompt": "Contrast Agent frameworks / architectures vs Code agents / software tasks along \"compute and latency constraints\". Ground A and B using the highlight snippets; do not introduce new claims beyond the cited evidence.", "evidence_field": "compute and latency constraints", "citations": ["Zhang2025Agentic", "Li2025Learn", "Jin2024From", "Jiang2025Agentic"]}, {"axis": "compute and latency constraints", "A_label": "Planning / reasoning loops", "B_label": "Code agents / software tasks", "A_papers": "Planning / reasoning loops: `P0042`, `P0051`, `P0136`", "B_papers": "Code agents / software tasks: `P0050`, `P0221`, `P0099`", "A_highlights": [{"paper_id": "P0042", "evidence_id": "E-P0042-44dce85d3e", "excerpt": "We further review representative agentic reasoning frameworks across real-world applications and benchmarks, including science, robotics, healthcare, autonomous research, and mathematics.", "citations": ["Wei2026Agentic"], "pointer": "papers/paper_notes.jsonl:paper_id=P0042#key_results[0]", "score": 0}, {"paper_id": "P0051", "evidence_id": "E-P0051-eae2720c71", "excerpt": "Settlers of Catan provides a challenging benchmark: success depends on balancing short- and long-term goals amid randomness, trading, expansion, and blocking.", "citations": ["Belle2025Agents"], "pointer": "papers/paper_notes.jsonl:paper_id=P0051#key_results[1]", "score": 0}], "B_highlights": [{"paper_id": "P0099", "evidence_id": "E-P0099-33c7e5174b", "excerpt": "We review and differentiate the work of LLMs and LLM-based agents from these six topics, examining their differences and similarities in tasks, benchmarks, and evaluation metrics.", "citations": ["Jin2024From"], "pointer": "papers/paper_notes.jsonl:paper_id=P0099#key_results[1]", "score": 0}, {"paper_id": "P0050", "evidence_id": "E-P0050-f38be5e04b", "excerpt": "It outlines the general workflow of the task and establishes a taxonomy across three dimensions: benchmarks, techniques, and empirical studies.", "citations": ["Jiang2025Agentic"], "pointer": "papers/paper_notes.jsonl:paper_id=P0050#key_results[1]", "score": 0}], "write_prompt": "Contrast Planning / reasoning loops vs Code agents / software tasks along \"compute and latency constraints\". Ground A and B using the highlight snippets; do not introduce new claims beyond the cited evidence.", "evidence_field": "compute and latency constraints", "citations": ["Wei2026Agentic", "Belle2025Agents", "Jin2024From", "Jiang2025Agentic"]}, {"axis": "communication protocol / roles", "A_label": "Agent frameworks / architectures", "B_label": "Planning / reasoning loops", "A_papers": "Agent frameworks / architectures: `P0042`, `P0050`, `P0051`", "B_papers": "Planning / reasoning loops: `P0042`, `P0051`, `P0136`", "A_highlights": [{"paper_id": "P0133", "evidence_id": "E-P0133-a68f39bc04", "excerpt": "This survey provides a comprehensive overview of foundation models, agentic systems, datasets, and computational tools supporting this growing field.", "citations": ["Van2025Survey"], "pointer": "papers/paper_notes.jsonl:paper_id=P0133#key_results[0]", "score": 1}, {"paper_id": "P0144", "evidence_id": "E-P0144-7d85a7241d", "excerpt": "Across agent and domain-specific benchmarks, ACE optimizes contexts both offline (e.g., system prompts) and online (e.g., agent memory), consistently outperforming strong baselines: +10.6% on agents and +8.6% on finance, while significantly reducing adaptation latency and", "citations": ["Zhang2025Agentic"], "pointer": "papers/paper_notes.jsonl:paper_id=P0144#key_results[0]", "score": 0}], "B_highlights": [{"paper_id": "P0042", "evidence_id": "E-P0042-44dce85d3e", "excerpt": "We further review representative agentic reasoning frameworks across real-world applications and benchmarks, including science, robotics, healthcare, autonomous research, and mathematics.", "citations": ["Wei2026Agentic"], "pointer": "papers/paper_notes.jsonl:paper_id=P0042#key_results[0]", "score": 0}, {"paper_id": "P0051", "evidence_id": "E-P0051-eae2720c71", "excerpt": "Settlers of Catan provides a challenging benchmark: success depends on balancing short- and long-term goals amid randomness, trading, expansion, and blocking.", "citations": ["Belle2025Agents"], "pointer": "papers/paper_notes.jsonl:paper_id=P0051#key_results[1]", "score": 0}], "write_prompt": "Contrast Agent frameworks / architectures vs Planning / reasoning loops along \"communication protocol / roles\". Ground A and B using the highlight snippets; do not introduce new claims beyond the cited evidence.", "evidence_field": "communication protocol / roles", "citations": ["Van2025Survey", "Zhang2025Agentic", "Wei2026Agentic", "Belle2025Agents"]}, {"axis": "communication protocol / roles", "A_label": "Agent frameworks / architectures", "B_label": "Code agents / software tasks", "A_papers": "Agent frameworks / architectures: `P0042`, `P0050`, `P0051`", "B_papers": "Code agents / software tasks: `P0050`, `P0221`, `P0099`", "A_highlights": [{"paper_id": "P0133", "evidence_id": "E-P0133-a68f39bc04", "excerpt": "This survey provides a comprehensive overview of foundation models, agentic systems, datasets, and computational tools supporting this growing field.", "citations": ["Van2025Survey"], "pointer": "papers/paper_notes.jsonl:paper_id=P0133#key_results[0]", "score": 1}, {"paper_id": "P0144", "evidence_id": "E-P0144-7d85a7241d", "excerpt": "Across agent and domain-specific benchmarks, ACE optimizes contexts both offline (e.g., system prompts) and online (e.g., agent memory), consistently outperforming strong baselines: +10.6% on agents and +8.6% on finance, while significantly reducing adaptation latency and", "citations": ["Zhang2025Agentic"], "pointer": "papers/paper_notes.jsonl:paper_id=P0144#key_results[0]", "score": 0}], "B_highlights": [{"paper_id": "P0099", "evidence_id": "E-P0099-33c7e5174b", "excerpt": "We review and differentiate the work of LLMs and LLM-based agents from these six topics, examining their differences and similarities in tasks, benchmarks, and evaluation metrics.", "citations": ["Jin2024From"], "pointer": "papers/paper_notes.jsonl:paper_id=P0099#key_results[1]", "score": 0}, {"paper_id": "P0050", "evidence_id": "E-P0050-f38be5e04b", "excerpt": "It outlines the general workflow of the task and establishes a taxonomy across three dimensions: benchmarks, techniques, and empirical studies.", "citations": ["Jiang2025Agentic"], "pointer": "papers/paper_notes.jsonl:paper_id=P0050#key_results[1]", "score": 0}], "write_prompt": "Contrast Agent frameworks / architectures vs Code agents / software tasks along \"communication protocol / roles\". Ground A and B using the highlight snippets; do not introduce new claims beyond the cited evidence.", "evidence_field": "communication protocol / roles", "citations": ["Van2025Survey", "Zhang2025Agentic", "Jin2024From", "Jiang2025Agentic"]}, {"axis": "communication protocol / roles", "A_label": "Planning / reasoning loops", "B_label": "Code agents / software tasks", "A_papers": "Planning / reasoning loops: `P0042`, `P0051`, `P0136`", "B_papers": "Code agents / software tasks: `P0050`, `P0221`, `P0099`", "A_highlights": [{"paper_id": "P0042", "evidence_id": "E-P0042-44dce85d3e", "excerpt": "We further review representative agentic reasoning frameworks across real-world applications and benchmarks, including science, robotics, healthcare, autonomous research, and mathematics.", "citations": ["Wei2026Agentic"], "pointer": "papers/paper_notes.jsonl:paper_id=P0042#key_results[0]", "score": 0}, {"paper_id": "P0051", "evidence_id": "E-P0051-eae2720c71", "excerpt": "Settlers of Catan provides a challenging benchmark: success depends on balancing short- and long-term goals amid randomness, trading, expansion, and blocking.", "citations": ["Belle2025Agents"], "pointer": "papers/paper_notes.jsonl:paper_id=P0051#key_results[1]", "score": 0}], "B_highlights": [{"paper_id": "P0099", "evidence_id": "E-P0099-33c7e5174b", "excerpt": "We review and differentiate the work of LLMs and LLM-based agents from these six topics, examining their differences and similarities in tasks, benchmarks, and evaluation metrics.", "citations": ["Jin2024From"], "pointer": "papers/paper_notes.jsonl:paper_id=P0099#key_results[1]", "score": 0}, {"paper_id": "P0050", "evidence_id": "E-P0050-f38be5e04b", "excerpt": "It outlines the general workflow of the task and establishes a taxonomy across three dimensions: benchmarks, techniques, and empirical studies.", "citations": ["Jiang2025Agentic"], "pointer": "papers/paper_notes.jsonl:paper_id=P0050#key_results[1]", "score": 0}], "write_prompt": "Contrast Planning / reasoning loops vs Code agents / software tasks along \"communication protocol / roles\". Ground A and B using the highlight snippets; do not introduce new claims beyond the cited evidence.", "evidence_field": "communication protocol / roles", "citations": ["Wei2026Agentic", "Belle2025Agents", "Jin2024From", "Jiang2025Agentic"]}, {"axis": "aggregation (vote / debate / referee)", "A_label": "Agent frameworks / architectures", "B_label": "Planning / reasoning loops", "A_papers": "Agent frameworks / architectures: `P0042`, `P0050`, `P0051`", "B_papers": "Planning / reasoning loops: `P0042`, `P0051`, `P0136`", "A_highlights": [{"paper_id": "P0144", "evidence_id": "E-P0144-7d85a7241d", "excerpt": "Across agent and domain-specific benchmarks, ACE optimizes contexts both offline (e.g., system prompts) and online (e.g., agent memory), consistently outperforming strong baselines: +10.6% on agents and +8.6% on finance, while significantly reducing adaptation latency and", "citations": ["Zhang2025Agentic"], "pointer": "papers/paper_notes.jsonl:paper_id=P0144#key_results[0]", "score": 0}, {"paper_id": "P0079", "evidence_id": "E-P0079-01f439b837", "excerpt": "We address this limitation by introducing a framework that enables LLM agents to learn and evolve both before and during test time, equipping them with environment-relevant knowledge for better planning and enhanced communication for improved cooperation.", "citations": ["Li2025Learn"], "pointer": "papers/paper_notes.jsonl:paper_id=P0079#limitations[1]", "score": 0}], "B_highlights": [{"paper_id": "P0042", "evidence_id": "E-P0042-44dce85d3e", "excerpt": "We further review representative agentic reasoning frameworks across real-world applications and benchmarks, including science, robotics, healthcare, autonomous research, and mathematics.", "citations": ["Wei2026Agentic"], "pointer": "papers/paper_notes.jsonl:paper_id=P0042#key_results[0]", "score": 0}, {"paper_id": "P0051", "evidence_id": "E-P0051-eae2720c71", "excerpt": "Settlers of Catan provides a challenging benchmark: success depends on balancing short- and long-term goals amid randomness, trading, expansion, and blocking.", "citations": ["Belle2025Agents"], "pointer": "papers/paper_notes.jsonl:paper_id=P0051#key_results[1]", "score": 0}], "write_prompt": "Contrast Agent frameworks / architectures vs Planning / reasoning loops along \"aggregation (vote / debate / referee)\". Ground A and B using the highlight snippets; do not introduce new claims beyond the cited evidence.", "evidence_field": "aggregation (vote / debate / referee)", "citations": ["Zhang2025Agentic", "Li2025Learn", "Wei2026Agentic", "Belle2025Agents"]}], "evaluation_protocol": [{"bullet": "Evaluation mentions include: LLMs, LLM-based, ReAct, HexMachina, AlphaBeta, LIET, LLaMA, GPT-4o, ThreeD-World, MARL.", "citations": ["Wei2026Agentic", "Jiang2025Agentic", "Belle2025Agents", "Li2025Learn"]}, {"bullet": "When comparing results, anchor the paragraph with: task type + metric + constraint (budget, tool access, horizon, or threat model) when stated.", "citations": ["Wei2026Agentic", "Jiang2025Agentic"]}, {"bullet": "Prefer head-to-head comparisons only when benchmark/metric are shared; otherwise frame differences as protocol-driven rather than method superiority.", "citations": ["Wei2026Agentic", "Jiang2025Agentic"]}, {"bullet": "Avoid underspecified model/baseline naming; if abstracts omit details, state that the baseline is reported but underspecified instead of guessing.", "citations": ["Wei2026Agentic", "Jiang2025Agentic"]}, {"bullet": "If a claim relies on a single reported number, pair it with a limitation/caveat from the same evidence so the draft remains conservative.", "citations": ["Wei2026Agentic", "Jiang2025Agentic"]}, {"bullet": "If budgets or environments differ across papers, treat cross-paper numeric comparison as fragile and prefer qualitative contrasts aligned to the subsection axes.", "citations": ["Wei2026Agentic", "Jiang2025Agentic"]}], "failures_limitations": [{"bullet": "We address this limitation by introducing a framework that enables LLM agents to learn and evolve both before and during test time, equipping them with environment-relevant knowledge for better planning and enhanced communication for improved cooperation.", "citations": ["Li2025Learn"]}, {"bullet": "The survey begins by analyzing current LLM limitations, such as hallucinations and the lack of internal self-assessment mechanisms.", "citations": ["Bilal2025Meta"]}, {"bullet": "It then talks about newer methods, including RL from human feedback (RLHF), self-distillation, and chain-of-thought prompting, and each of their limitations.", "citations": ["Bilal2025Meta"]}, {"bullet": "We assess the early successes of foundation models and identify persistent limitations, including challenges in generalizability, interpretability, data imbalance, safety concerns, and limited multimodal fusion.", "citations": ["Van2025Survey"]}, {"bullet": "On the AppWorld leaderboard, ACE matches the top-ranked production-level agent on the overall average and surpasses it on the harder test-challenge split, despite using a smaller open-source model.", "citations": ["Zhang2025Agentic"]}, {"bullet": "The article concludes by outlining open challenges, potential security risks, and promising directions for advancing robust, interoperable, and scalable multi-agent LLM ecosystems.", "citations": ["Sarkar2025Survey"]}, {"bullet": "However, they also exhibit numerous limitations and shortcomings.", "citations": ["Jin2024From"]}, {"bullet": "LLM-based agents, a novel tech nology with the potential for Artificial General Intelligence (AGI), combine LLMs as the core for decision-making and action-taking, addressing some of the inherent limitations of LLMs such as lack of autonomy and self-improvement.", "citations": ["Jin2024From"]}], "blocking_missing": [], "verify_fields": ["named benchmarks/datasets used", "metrics/human-eval protocol", "compute/training/inference cost", "training data and supervision signal", "baseline choices and ablation evidence"], "generated_at": "2026-01-25T17:06:54", "section_id": "5", "section_title": "Learning, Adaptation & Coordination"}
{"sub_id": "5.2", "title": "Multi-agent coordination", "evidence_ids": ["E-P0040-96b056b80b", "E-P0012-7cac5db9a9", "E-P0139-8eb9ff221b", "E-P0194-f2e78c673e", "E-P0255-5e37132da5", "E-P0149-42512d3cac", "E-P0080-0e82f75090", "E-P0183-6770e6174b", "E-P0108-9640816b42", "E-P0159-1ea69fbec3", "E-P0115-af857798be", "E-P0270-764758958d", "E-P0078-fa4336d046", "E-P0079-1dd544863c", "E-P0230-15e523063d", "E-P0212-1167f52f16", "E-P0070-0f34c44fa9", "E-P0119-02f16b54ff", "E-P0248-c44176def5", "E-P0286-73bcad694d", "E-P0183-1c544ba66e", "E-P0294-cefe4d686d", "E-P0080-6701c90e15", "E-P0248-3fe4bbe4f2"], "evidence_level_summary": {"fulltext": 0, "abstract": 28, "title": 0}, "evidence_snippets": [{"evidence_id": "E-P0040-96b056b80b", "text": "To this end, we propose a multi-agent system with customized communication knowledge and tools for solving communication related tasks using natural language, comprising three components: (1) Multi-agent Data Retrieval (MDR), which employs the condensate and inference agents to refine and summarize communication knowledge from the knowledge base, expanding the knowledge boundaries of LLMs in 6G communications; (2) Multi-agent Collaborative Planning (MCP), which utilizes multiple planning agents to generate feasible solutions for the communication related task from different perspectives based on the retrieved knowledge; (3) Multi-agent Evaluation and Reflecxion (MER), which utilizes the evaluation agent to assess the solutions, and applies the reflexion agent and refinement agent to provide improvement suggestions for current solutions.", "paper_id": "P0040", "citations": ["Jiang2023Large"], "provenance": {"evidence_level": "abstract", "source": "paper_notes", "pointer": "papers/paper_notes.jsonl:paper_id=P0040#key_results[0]"}}, {"evidence_id": "E-P0012-7cac5db9a9", "text": "By analyzing recent advancements and their limitations - such as scalability, real-time response challenges, and agent coordination constraints, we provide a detailed view of the technological landscape.", "paper_id": "P0012", "citations": ["Aratchige2025Llms"], "provenance": {"evidence_level": "abstract", "source": "paper_notes", "pointer": "papers/paper_notes.jsonl:paper_id=P0012#limitations[1]"}}, {"evidence_id": "E-P0139-8eb9ff221b", "text": "Using a corpus of 219 labeled rubrics derived from the GDPVal benchmark, we evaluate ARCANE on challenging tasks requiring multi-step reasoning and tool use.", "paper_id": "P0139", "citations": ["Masters2025Arcane"], "provenance": {"evidence_level": "abstract", "source": "paper_notes", "pointer": "papers/paper_notes.jsonl:paper_id=P0139#key_results[0]"}}, {"evidence_id": "E-P0194-f2e78c673e", "text": "Current frameworks for multi-agent debate are often designed towards tool use, lack integrated evaluation, or provide limited configurability of agent personas, response generators, discussion paradigms, and decision protocols.", "paper_id": "P0194", "citations": ["Becker2025Mallm"], "provenance": {"evidence_level": "abstract", "source": "paper_notes", "pointer": "papers/paper_notes.jsonl:paper_id=P0194#key_results[1]"}}, {"evidence_id": "E-P0255-5e37132da5", "text": "We conducted comprehensive experiments using a kinase inhibitor dataset, where our multi-agent LLM method outperformed the non-reasoning multi-agent model (GPT-4o mini) by 45% in F1 score (0.514 vs 0.355).", "paper_id": "P0255", "citations": ["Inoue2024Drugagent"], "provenance": {"evidence_level": "abstract", "source": "paper_notes", "pointer": "papers/paper_notes.jsonl:paper_id=P0255#key_results[0]"}}, {"evidence_id": "E-P0149-42512d3cac", "text": "We evaluate AutoSCORE on four benchmark datasets from the ASAP benchmark, using both proprietary and open-source LLMs (GPT-4o, LLaMA-3.1-8B, and LLaMA-3.1-70B).", "paper_id": "P0149", "citations": ["Wang2025Autoscore"], "provenance": {"evidence_level": "abstract", "source": "paper_notes", "pointer": "papers/paper_notes.jsonl:paper_id=P0149#key_results[0]"}}, {"evidence_id": "E-P0080-0e82f75090", "text": "We find that 20 of the 48 issues reported by MARVEL pose security vulnerabilities.", "paper_id": "P0080", "citations": ["Collini2025Marvel"], "provenance": {"evidence_level": "abstract", "source": "paper_notes", "pointer": "papers/paper_notes.jsonl:paper_id=P0080#key_results[0]"}}, {"evidence_id": "E-P0183-6770e6174b", "text": "The increasing integration of Industrial IoT (IIoT) exposes critical cyber-physical systems to sophisticated, multi-stage attacks that elude traditional defenses lacking contextual awareness.", "paper_id": "P0183", "citations": ["Xu2025Autonomous"], "provenance": {"evidence_level": "abstract", "source": "paper_notes", "pointer": "papers/paper_notes.jsonl:paper_id=P0183#summary_bullets[0]"}}, {"evidence_id": "E-P0108-9640816b42", "text": "Evaluation across various tool-use benchmarks illustrates that our proposed multi-LLM framework surpasses the traditional single-LLM approach, highlighting its efficacy and advantages in tool learning.", "paper_id": "P0108", "citations": ["Shen2024Small"], "provenance": {"evidence_level": "abstract", "source": "paper_notes", "pointer": "papers/paper_notes.jsonl:paper_id=P0108#key_results[1]"}}, {"evidence_id": "E-P0159-1ea69fbec3", "text": "Validation using 551 GNWT-Agents and Columbia University Speed Dating dataset demonstrates 72% correlation with human attraction patterns, 77.8% match prediction accuracy, and 74% agreement in human validation studies.", "paper_id": "P0159", "citations": ["Ye2025Cognipair"], "provenance": {"evidence_level": "abstract", "source": "paper_notes", "pointer": "papers/paper_notes.jsonl:paper_id=P0159#key_results[0]"}}, {"evidence_id": "E-P0115-af857798be", "text": "We evaluate seven LLMs, quantitatively highlighting a significant capability gap of over threefold between the strongest, GPT o1, and the weakest, Llama-2-70B.", "paper_id": "P0115", "citations": ["Xu2023Magic"], "provenance": {"evidence_level": "abstract", "source": "paper_notes", "pointer": "papers/paper_notes.jsonl:paper_id=P0115#key_results[0]"}}, {"evidence_id": "E-P0270-764758958d", "text": "However, existing benchmarks for evaluating LLM Agents either use static datasets, potentially leading to data leakage or focus only on single-agent scenarios, overlooking the complexities of multi-agent interactions.", "paper_id": "P0270", "citations": ["Chen2024Llmarena"], "provenance": {"evidence_level": "abstract", "source": "paper_notes", "pointer": "papers/paper_notes.jsonl:paper_id=P0270#key_results[1]"}}, {"evidence_id": "E-P0078-fa4336d046", "text": "Systematic literature reviews and meta-analyses are essential for synthesizing research insights, but they remain time-intensive and labor-intensive due to the iterative processes of screening, evaluation, and data extraction.", "paper_id": "P0078", "citations": ["Rouzrokh2025Lattereview"], "provenance": {"evidence_level": "abstract", "source": "paper_notes", "pointer": "papers/paper_notes.jsonl:paper_id=P0078#key_results[0]"}}, {"evidence_id": "E-P0079-1dd544863c", "text": "Our experiments on Communicative Watch-And-Help and ThreeD-World Multi-Agent Transport benchmarks demonstrate that LIET, instantiated with both LLaMA and GPT-4o, outperforms existing baselines and exhibits strong cooperative planning abilities.", "paper_id": "P0079", "citations": ["Li2025Learn"], "provenance": {"evidence_level": "abstract", "source": "paper_notes", "pointer": "papers/paper_notes.jsonl:paper_id=P0079#key_results[1]"}}, {"evidence_id": "E-P0230-15e523063d", "text": "Evaluation metrics include standard classification metrics, quality assessment of reasoning processes, and robustness testing against rewritten content.", "paper_id": "P0230", "citations": ["Cui2025Toward"], "provenance": {"evidence_level": "abstract", "source": "paper_notes", "pointer": "papers/paper_notes.jsonl:paper_id=P0230#key_results[1]"}}, {"evidence_id": "E-P0212-1167f52f16", "text": "In this work, we propose SG^2, an iterative Schema-Guided Scene-Graph reasoning framework based on multi-agent LLMs.", "paper_id": "P0212", "citations": ["Chen2025Schema"], "provenance": {"evidence_level": "abstract", "source": "paper_notes", "pointer": "papers/paper_notes.jsonl:paper_id=P0212#key_results[0]"}}, {"evidence_id": "E-P0070-0f34c44fa9", "text": "To evaluate the system, we propose new performance metrics assessing both individual agents and the system as a whole.", "paper_id": "P0070", "citations": ["Zahedifar2025Agent"], "provenance": {"evidence_level": "abstract", "source": "paper_notes", "pointer": "papers/paper_notes.jsonl:paper_id=P0070#key_results[1]"}}, {"evidence_id": "E-P0119-02f16b54ff", "text": "Extensive experiments on three different types of reasoning tasks show that our collaboration approach delivers superior accuracy across all ten datasets compared to existing methods.", "paper_id": "P0119", "citations": ["Xu2023Towards"], "provenance": {"evidence_level": "abstract", "source": "paper_notes", "pointer": "papers/paper_notes.jsonl:paper_id=P0119#key_results[1]"}}], "definitions_setup": [{"bullet": "Setup: Which design choices in Multi-agent coordination drive the major trade-offs, and how are those trade-offs measured? Scope: in-scope: Core topics directly relevant to 'Multi-agent coordination'.. Axes: evaluation protocol (datasets, metrics, human evaluation); compute and latency constraints; tool interface contract (schemas / protocols); tool selection / routing policy; sandboxing / permissions / observability.", "citations": ["Aratchige2025Llms", "Zahedifar2025Agent", "Rouzrokh2025Lattereview"]}], "claim_candidates": [{"claim": "To this end, we propose a multi-agent system with customized communication knowledge and tools for solving communication related tasks using natural language, comprising three components: (1) Multi-agent Data Retrieval (MDR), which employs the condensate and inference agents to refine and summarize communication knowledge from the knowledge base, expanding the knowledge boundaries of LLMs in 6G", "evidence_field": "evidence_snippet", "citations": ["Jiang2023Large"]}, {"claim": "By analyzing recent advancements and their limitations - such as scalability, real-time response challenges, and agent coordination constraints, we provide a detailed view of the technological landscape.", "evidence_field": "evidence_snippet", "citations": ["Aratchige2025Llms"]}, {"claim": "Using a corpus of 219 labeled rubrics derived from the GDPVal benchmark, we evaluate ARCANE on challenging tasks requiring multi-step reasoning and tool use.", "evidence_field": "evidence_snippet", "citations": ["Masters2025Arcane"]}, {"claim": "Current frameworks for multi-agent debate are often designed towards tool use, lack integrated evaluation, or provide limited configurability of agent personas, response generators, discussion paradigms, and decision protocols.", "evidence_field": "evidence_snippet", "citations": ["Becker2025Mallm"]}, {"claim": "We conducted comprehensive experiments using a kinase inhibitor dataset, where our multi-agent LLM method outperformed the non-reasoning multi-agent model (GPT-4o mini) by 45% in F1 score (0.514 vs 0.355).", "evidence_field": "evidence_snippet", "citations": ["Inoue2024Drugagent"]}], "concrete_comparisons": [{"axis": "evaluation protocol (datasets, metrics, human evaluation)", "A_label": "Agent frameworks / architectures", "B_label": "Multi-agent coordination", "A_papers": "Agent frameworks / architectures: `P0012`, `P0070`, `P0078`", "B_papers": "Multi-agent coordination: `P0070`, `P0078`, `P0079`", "A_highlights": [{"paper_id": "P0139", "evidence_id": "E-P0139-8eb9ff221b", "excerpt": "Using a corpus of 219 labeled rubrics derived from the GDPVal benchmark, we evaluate ARCANE on challenging tasks requiring multi-step reasoning and tool use.", "citations": ["Masters2025Arcane"], "pointer": "papers/paper_notes.jsonl:paper_id=P0139#key_results[0]", "score": 2}, {"paper_id": "P0079", "evidence_id": "E-P0079-1dd544863c", "excerpt": "Our experiments on Communicative Watch-And-Help and ThreeD-World Multi-Agent Transport benchmarks demonstrate that LIET, instantiated with both LLaMA and GPT-4o, outperforms existing baselines and exhibits strong cooperative planning abilities.", "citations": ["Li2025Learn"], "pointer": "papers/paper_notes.jsonl:paper_id=P0079#key_results[1]", "score": 1}], "B_highlights": [{"paper_id": "P0149", "evidence_id": "E-P0149-42512d3cac", "excerpt": "We evaluate AutoSCORE on four benchmark datasets from the ASAP benchmark, using both proprietary and open-source LLMs (GPT-4o, LLaMA-3.1-8B, and LLaMA-3.1-70B).", "citations": ["Wang2025Autoscore"], "pointer": "papers/paper_notes.jsonl:paper_id=P0149#key_results[0]", "score": 2}, {"paper_id": "P0139", "evidence_id": "E-P0139-8eb9ff221b", "excerpt": "Using a corpus of 219 labeled rubrics derived from the GDPVal benchmark, we evaluate ARCANE on challenging tasks requiring multi-step reasoning and tool use.", "citations": ["Masters2025Arcane"], "pointer": "papers/paper_notes.jsonl:paper_id=P0139#key_results[0]", "score": 2}], "write_prompt": "Contrast Agent frameworks / architectures vs Multi-agent coordination along \"evaluation protocol (datasets, metrics, human evaluation)\". Ground A and B using the highlight snippets; do not introduce new claims beyond the cited evidence.", "evidence_field": "evaluation protocol (datasets, metrics, human evaluation)", "citations": ["Masters2025Arcane", "Li2025Learn", "Wang2025Autoscore"]}, {"axis": "evaluation protocol (datasets, metrics, human evaluation)", "A_label": "Agent frameworks / architectures", "B_label": "Planning / reasoning loops", "A_papers": "Agent frameworks / architectures: `P0012`, `P0070`, `P0078`", "B_papers": "Planning / reasoning loops: `P0183`, `P0212`, `P0255`", "A_highlights": [{"paper_id": "P0139", "evidence_id": "E-P0139-8eb9ff221b", "excerpt": "Using a corpus of 219 labeled rubrics derived from the GDPVal benchmark, we evaluate ARCANE on challenging tasks requiring multi-step reasoning and tool use.", "citations": ["Masters2025Arcane"], "pointer": "papers/paper_notes.jsonl:paper_id=P0139#key_results[0]", "score": 2}, {"paper_id": "P0079", "evidence_id": "E-P0079-1dd544863c", "excerpt": "Our experiments on Communicative Watch-And-Help and ThreeD-World Multi-Agent Transport benchmarks demonstrate that LIET, instantiated with both LLaMA and GPT-4o, outperforms existing baselines and exhibits strong cooperative planning abilities.", "citations": ["Li2025Learn"], "pointer": "papers/paper_notes.jsonl:paper_id=P0079#key_results[1]", "score": 1}], "B_highlights": [{"paper_id": "P0255", "evidence_id": "E-P0255-5e37132da5", "excerpt": "We conducted comprehensive experiments using a kinase inhibitor dataset, where our multi-agent LLM method outperformed the non-reasoning multi-agent model (GPT-4o mini) by 45% in F1 score (0.514 vs 0.355).", "citations": ["Inoue2024Drugagent"], "pointer": "papers/paper_notes.jsonl:paper_id=P0255#key_results[0]", "score": 1}, {"paper_id": "P0119", "evidence_id": "E-P0119-02f16b54ff", "excerpt": "Extensive experiments on three different types of reasoning tasks show that our collaboration approach delivers superior accuracy across all ten datasets compared to existing methods.", "citations": ["Xu2023Towards"], "pointer": "papers/paper_notes.jsonl:paper_id=P0119#key_results[1]", "score": 1}], "write_prompt": "Contrast Agent frameworks / architectures vs Planning / reasoning loops along \"evaluation protocol (datasets, metrics, human evaluation)\". Ground A and B using the highlight snippets; do not introduce new claims beyond the cited evidence.", "evidence_field": "evaluation protocol (datasets, metrics, human evaluation)", "citations": ["Masters2025Arcane", "Li2025Learn", "Inoue2024Drugagent", "Xu2023Towards"]}, {"axis": "evaluation protocol (datasets, metrics, human evaluation)", "A_label": "Multi-agent coordination", "B_label": "Planning / reasoning loops", "A_papers": "Multi-agent coordination: `P0070`, `P0078`, `P0079`", "B_papers": "Planning / reasoning loops: `P0183`, `P0212`, `P0255`", "A_highlights": [{"paper_id": "P0149", "evidence_id": "E-P0149-42512d3cac", "excerpt": "We evaluate AutoSCORE on four benchmark datasets from the ASAP benchmark, using both proprietary and open-source LLMs (GPT-4o, LLaMA-3.1-8B, and LLaMA-3.1-70B).", "citations": ["Wang2025Autoscore"], "pointer": "papers/paper_notes.jsonl:paper_id=P0149#key_results[0]", "score": 2}, {"paper_id": "P0139", "evidence_id": "E-P0139-8eb9ff221b", "excerpt": "Using a corpus of 219 labeled rubrics derived from the GDPVal benchmark, we evaluate ARCANE on challenging tasks requiring multi-step reasoning and tool use.", "citations": ["Masters2025Arcane"], "pointer": "papers/paper_notes.jsonl:paper_id=P0139#key_results[0]", "score": 2}], "B_highlights": [{"paper_id": "P0255", "evidence_id": "E-P0255-5e37132da5", "excerpt": "We conducted comprehensive experiments using a kinase inhibitor dataset, where our multi-agent LLM method outperformed the non-reasoning multi-agent model (GPT-4o mini) by 45% in F1 score (0.514 vs 0.355).", "citations": ["Inoue2024Drugagent"], "pointer": "papers/paper_notes.jsonl:paper_id=P0255#key_results[0]", "score": 1}, {"paper_id": "P0119", "evidence_id": "E-P0119-02f16b54ff", "excerpt": "Extensive experiments on three different types of reasoning tasks show that our collaboration approach delivers superior accuracy across all ten datasets compared to existing methods.", "citations": ["Xu2023Towards"], "pointer": "papers/paper_notes.jsonl:paper_id=P0119#key_results[1]", "score": 1}], "write_prompt": "Contrast Multi-agent coordination vs Planning / reasoning loops along \"evaluation protocol (datasets, metrics, human evaluation)\". Ground A and B using the highlight snippets; do not introduce new claims beyond the cited evidence.", "evidence_field": "evaluation protocol (datasets, metrics, human evaluation)", "citations": ["Wang2025Autoscore", "Masters2025Arcane", "Inoue2024Drugagent", "Xu2023Towards"]}, {"axis": "compute and latency constraints", "A_label": "Agent frameworks / architectures", "B_label": "Multi-agent coordination", "A_papers": "Agent frameworks / architectures: `P0012`, `P0070`, `P0078`", "B_papers": "Multi-agent coordination: `P0070`, `P0078`, `P0079`", "A_highlights": [{"paper_id": "P0012", "evidence_id": "E-P0012-7cac5db9a9", "excerpt": "By analyzing recent advancements and their limitations - such as scalability, real-time response challenges, and agent coordination constraints, we provide a detailed view of the technological landscape.", "citations": ["Aratchige2025Llms"], "pointer": "papers/paper_notes.jsonl:paper_id=P0012#limitations[1]", "score": 1}, {"paper_id": "P0079", "evidence_id": "E-P0079-1dd544863c", "excerpt": "Our experiments on Communicative Watch-And-Help and ThreeD-World Multi-Agent Transport benchmarks demonstrate that LIET, instantiated with both LLaMA and GPT-4o, outperforms existing baselines and exhibits strong cooperative planning abilities.", "citations": ["Li2025Learn"], "pointer": "papers/paper_notes.jsonl:paper_id=P0079#key_results[1]", "score": 0}], "B_highlights": [{"paper_id": "P0079", "evidence_id": "E-P0079-1dd544863c", "excerpt": "Our experiments on Communicative Watch-And-Help and ThreeD-World Multi-Agent Transport benchmarks demonstrate that LIET, instantiated with both LLaMA and GPT-4o, outperforms existing baselines and exhibits strong cooperative planning abilities.", "citations": ["Li2025Learn"], "pointer": "papers/paper_notes.jsonl:paper_id=P0079#key_results[1]", "score": 0}, {"paper_id": "P0078", "evidence_id": "E-P0078-fa4336d046", "excerpt": "Systematic literature reviews and meta-analyses are essential for synthesizing research insights, but they remain time-intensive and labor-intensive due to the iterative processes of screening, evaluation, and data extraction.", "citations": ["Rouzrokh2025Lattereview"], "pointer": "papers/paper_notes.jsonl:paper_id=P0078#key_results[0]", "score": 0}], "write_prompt": "Contrast Agent frameworks / architectures vs Multi-agent coordination along \"compute and latency constraints\". Ground A and B using the highlight snippets; do not introduce new claims beyond the cited evidence.", "evidence_field": "compute and latency constraints", "citations": ["Aratchige2025Llms", "Li2025Learn", "Rouzrokh2025Lattereview"]}, {"axis": "compute and latency constraints", "A_label": "Agent frameworks / architectures", "B_label": "Planning / reasoning loops", "A_papers": "Agent frameworks / architectures: `P0012`, `P0070`, `P0078`", "B_papers": "Planning / reasoning loops: `P0183`, `P0212`, `P0255`", "A_highlights": [{"paper_id": "P0012", "evidence_id": "E-P0012-7cac5db9a9", "excerpt": "By analyzing recent advancements and their limitations - such as scalability, real-time response challenges, and agent coordination constraints, we provide a detailed view of the technological landscape.", "citations": ["Aratchige2025Llms"], "pointer": "papers/paper_notes.jsonl:paper_id=P0012#limitations[1]", "score": 1}, {"paper_id": "P0079", "evidence_id": "E-P0079-1dd544863c", "excerpt": "Our experiments on Communicative Watch-And-Help and ThreeD-World Multi-Agent Transport benchmarks demonstrate that LIET, instantiated with both LLaMA and GPT-4o, outperforms existing baselines and exhibits strong cooperative planning abilities.", "citations": ["Li2025Learn"], "pointer": "papers/paper_notes.jsonl:paper_id=P0079#key_results[1]", "score": 0}], "B_highlights": [{"paper_id": "P0255", "evidence_id": "E-P0255-5e37132da5", "excerpt": "We conducted comprehensive experiments using a kinase inhibitor dataset, where our multi-agent LLM method outperformed the non-reasoning multi-agent model (GPT-4o mini) by 45% in F1 score (0.514 vs 0.355).", "citations": ["Inoue2024Drugagent"], "pointer": "papers/paper_notes.jsonl:paper_id=P0255#key_results[0]", "score": 0}, {"paper_id": "P0183", "evidence_id": "E-P0183-6770e6174b", "excerpt": "The increasing integration of Industrial IoT (IIoT) exposes critical cyber-physical systems to sophisticated, multi-stage attacks that elude traditional defenses lacking contextual awareness.", "citations": ["Xu2025Autonomous"], "pointer": "papers/paper_notes.jsonl:paper_id=P0183#summary_bullets[0]", "score": 0}], "write_prompt": "Contrast Agent frameworks / architectures vs Planning / reasoning loops along \"compute and latency constraints\". Ground A and B using the highlight snippets; do not introduce new claims beyond the cited evidence.", "evidence_field": "compute and latency constraints", "citations": ["Aratchige2025Llms", "Li2025Learn", "Inoue2024Drugagent", "Xu2025Autonomous"]}, {"axis": "compute and latency constraints", "A_label": "Multi-agent coordination", "B_label": "Planning / reasoning loops", "A_papers": "Multi-agent coordination: `P0070`, `P0078`, `P0079`", "B_papers": "Planning / reasoning loops: `P0183`, `P0212`, `P0255`", "A_highlights": [{"paper_id": "P0079", "evidence_id": "E-P0079-1dd544863c", "excerpt": "Our experiments on Communicative Watch-And-Help and ThreeD-World Multi-Agent Transport benchmarks demonstrate that LIET, instantiated with both LLaMA and GPT-4o, outperforms existing baselines and exhibits strong cooperative planning abilities.", "citations": ["Li2025Learn"], "pointer": "papers/paper_notes.jsonl:paper_id=P0079#key_results[1]", "score": 0}, {"paper_id": "P0078", "evidence_id": "E-P0078-fa4336d046", "excerpt": "Systematic literature reviews and meta-analyses are essential for synthesizing research insights, but they remain time-intensive and labor-intensive due to the iterative processes of screening, evaluation, and data extraction.", "citations": ["Rouzrokh2025Lattereview"], "pointer": "papers/paper_notes.jsonl:paper_id=P0078#key_results[0]", "score": 0}], "B_highlights": [{"paper_id": "P0255", "evidence_id": "E-P0255-5e37132da5", "excerpt": "We conducted comprehensive experiments using a kinase inhibitor dataset, where our multi-agent LLM method outperformed the non-reasoning multi-agent model (GPT-4o mini) by 45% in F1 score (0.514 vs 0.355).", "citations": ["Inoue2024Drugagent"], "pointer": "papers/paper_notes.jsonl:paper_id=P0255#key_results[0]", "score": 0}, {"paper_id": "P0183", "evidence_id": "E-P0183-6770e6174b", "excerpt": "The increasing integration of Industrial IoT (IIoT) exposes critical cyber-physical systems to sophisticated, multi-stage attacks that elude traditional defenses lacking contextual awareness.", "citations": ["Xu2025Autonomous"], "pointer": "papers/paper_notes.jsonl:paper_id=P0183#summary_bullets[0]", "score": 0}], "write_prompt": "Contrast Multi-agent coordination vs Planning / reasoning loops along \"compute and latency constraints\". Ground A and B using the highlight snippets; do not introduce new claims beyond the cited evidence.", "evidence_field": "compute and latency constraints", "citations": ["Li2025Learn", "Rouzrokh2025Lattereview", "Inoue2024Drugagent", "Xu2025Autonomous"]}, {"axis": "tool interface contract (schemas / protocols)", "A_label": "Agent frameworks / architectures", "B_label": "Multi-agent coordination", "A_papers": "Agent frameworks / architectures: `P0012`, `P0070`, `P0078`", "B_papers": "Multi-agent coordination: `P0070`, `P0078`, `P0079`", "A_highlights": [{"paper_id": "P0139", "evidence_id": "E-P0139-8eb9ff221b", "excerpt": "Using a corpus of 219 labeled rubrics derived from the GDPVal benchmark, we evaluate ARCANE on challenging tasks requiring multi-step reasoning and tool use.", "citations": ["Masters2025Arcane"], "pointer": "papers/paper_notes.jsonl:paper_id=P0139#key_results[0]", "score": 1}, {"paper_id": "P0079", "evidence_id": "E-P0079-1dd544863c", "excerpt": "Our experiments on Communicative Watch-And-Help and ThreeD-World Multi-Agent Transport benchmarks demonstrate that LIET, instantiated with both LLaMA and GPT-4o, outperforms existing baselines and exhibits strong cooperative planning abilities.", "citations": ["Li2025Learn"], "pointer": "papers/paper_notes.jsonl:paper_id=P0079#key_results[1]", "score": 0}], "B_highlights": [{"paper_id": "P0139", "evidence_id": "E-P0139-8eb9ff221b", "excerpt": "Using a corpus of 219 labeled rubrics derived from the GDPVal benchmark, we evaluate ARCANE on challenging tasks requiring multi-step reasoning and tool use.", "citations": ["Masters2025Arcane"], "pointer": "papers/paper_notes.jsonl:paper_id=P0139#key_results[0]", "score": 1}, {"paper_id": "P0079", "evidence_id": "E-P0079-1dd544863c", "excerpt": "Our experiments on Communicative Watch-And-Help and ThreeD-World Multi-Agent Transport benchmarks demonstrate that LIET, instantiated with both LLaMA and GPT-4o, outperforms existing baselines and exhibits strong cooperative planning abilities.", "citations": ["Li2025Learn"], "pointer": "papers/paper_notes.jsonl:paper_id=P0079#key_results[1]", "score": 0}], "write_prompt": "Contrast Agent frameworks / architectures vs Multi-agent coordination along \"tool interface contract (schemas / protocols)\". Ground A and B using the highlight snippets; do not introduce new claims beyond the cited evidence.", "evidence_field": "tool interface contract (schemas / protocols)", "citations": ["Masters2025Arcane", "Li2025Learn"]}, {"axis": "tool interface contract (schemas / protocols)", "A_label": "Agent frameworks / architectures", "B_label": "Planning / reasoning loops", "A_papers": "Agent frameworks / architectures: `P0012`, `P0070`, `P0078`", "B_papers": "Planning / reasoning loops: `P0183`, `P0212`, `P0255`", "A_highlights": [{"paper_id": "P0139", "evidence_id": "E-P0139-8eb9ff221b", "excerpt": "Using a corpus of 219 labeled rubrics derived from the GDPVal benchmark, we evaluate ARCANE on challenging tasks requiring multi-step reasoning and tool use.", "citations": ["Masters2025Arcane"], "pointer": "papers/paper_notes.jsonl:paper_id=P0139#key_results[0]", "score": 1}, {"paper_id": "P0079", "evidence_id": "E-P0079-1dd544863c", "excerpt": "Our experiments on Communicative Watch-And-Help and ThreeD-World Multi-Agent Transport benchmarks demonstrate that LIET, instantiated with both LLaMA and GPT-4o, outperforms existing baselines and exhibits strong cooperative planning abilities.", "citations": ["Li2025Learn"], "pointer": "papers/paper_notes.jsonl:paper_id=P0079#key_results[1]", "score": 0}], "B_highlights": [{"paper_id": "P0212", "evidence_id": "E-P0212-1167f52f16", "excerpt": "In this work, we propose SG^2, an iterative Schema-Guided Scene-Graph reasoning framework based on multi-agent LLMs.", "citations": ["Chen2025Schema"], "pointer": "papers/paper_notes.jsonl:paper_id=P0212#key_results[0]", "score": 1}, {"paper_id": "P0255", "evidence_id": "E-P0255-5e37132da5", "excerpt": "We conducted comprehensive experiments using a kinase inhibitor dataset, where our multi-agent LLM method outperformed the non-reasoning multi-agent model (GPT-4o mini) by 45% in F1 score (0.514 vs 0.355).", "citations": ["Inoue2024Drugagent"], "pointer": "papers/paper_notes.jsonl:paper_id=P0255#key_results[0]", "score": 0}], "write_prompt": "Contrast Agent frameworks / architectures vs Planning / reasoning loops along \"tool interface contract (schemas / protocols)\". Ground A and B using the highlight snippets; do not introduce new claims beyond the cited evidence.", "evidence_field": "tool interface contract (schemas / protocols)", "citations": ["Masters2025Arcane", "Li2025Learn", "Chen2025Schema", "Inoue2024Drugagent"]}, {"axis": "tool interface contract (schemas / protocols)", "A_label": "Multi-agent coordination", "B_label": "Planning / reasoning loops", "A_papers": "Multi-agent coordination: `P0070`, `P0078`, `P0079`", "B_papers": "Planning / reasoning loops: `P0183`, `P0212`, `P0255`", "A_highlights": [{"paper_id": "P0139", "evidence_id": "E-P0139-8eb9ff221b", "excerpt": "Using a corpus of 219 labeled rubrics derived from the GDPVal benchmark, we evaluate ARCANE on challenging tasks requiring multi-step reasoning and tool use.", "citations": ["Masters2025Arcane"], "pointer": "papers/paper_notes.jsonl:paper_id=P0139#key_results[0]", "score": 1}, {"paper_id": "P0079", "evidence_id": "E-P0079-1dd544863c", "excerpt": "Our experiments on Communicative Watch-And-Help and ThreeD-World Multi-Agent Transport benchmarks demonstrate that LIET, instantiated with both LLaMA and GPT-4o, outperforms existing baselines and exhibits strong cooperative planning abilities.", "citations": ["Li2025Learn"], "pointer": "papers/paper_notes.jsonl:paper_id=P0079#key_results[1]", "score": 0}], "B_highlights": [{"paper_id": "P0212", "evidence_id": "E-P0212-1167f52f16", "excerpt": "In this work, we propose SG^2, an iterative Schema-Guided Scene-Graph reasoning framework based on multi-agent LLMs.", "citations": ["Chen2025Schema"], "pointer": "papers/paper_notes.jsonl:paper_id=P0212#key_results[0]", "score": 1}, {"paper_id": "P0255", "evidence_id": "E-P0255-5e37132da5", "excerpt": "We conducted comprehensive experiments using a kinase inhibitor dataset, where our multi-agent LLM method outperformed the non-reasoning multi-agent model (GPT-4o mini) by 45% in F1 score (0.514 vs 0.355).", "citations": ["Inoue2024Drugagent"], "pointer": "papers/paper_notes.jsonl:paper_id=P0255#key_results[0]", "score": 0}], "write_prompt": "Contrast Multi-agent coordination vs Planning / reasoning loops along \"tool interface contract (schemas / protocols)\". Ground A and B using the highlight snippets; do not introduce new claims beyond the cited evidence.", "evidence_field": "tool interface contract (schemas / protocols)", "citations": ["Masters2025Arcane", "Li2025Learn", "Chen2025Schema", "Inoue2024Drugagent"]}, {"axis": "tool selection / routing policy", "A_label": "Agent frameworks / architectures", "B_label": "Multi-agent coordination", "A_papers": "Agent frameworks / architectures: `P0012`, `P0070`, `P0078`", "B_papers": "Multi-agent coordination: `P0070`, `P0078`, `P0079`", "A_highlights": [{"paper_id": "P0139", "evidence_id": "E-P0139-8eb9ff221b", "excerpt": "Using a corpus of 219 labeled rubrics derived from the GDPVal benchmark, we evaluate ARCANE on challenging tasks requiring multi-step reasoning and tool use.", "citations": ["Masters2025Arcane"], "pointer": "papers/paper_notes.jsonl:paper_id=P0139#key_results[0]", "score": 1}, {"paper_id": "P0079", "evidence_id": "E-P0079-1dd544863c", "excerpt": "Our experiments on Communicative Watch-And-Help and ThreeD-World Multi-Agent Transport benchmarks demonstrate that LIET, instantiated with both LLaMA and GPT-4o, outperforms existing baselines and exhibits strong cooperative planning abilities.", "citations": ["Li2025Learn"], "pointer": "papers/paper_notes.jsonl:paper_id=P0079#key_results[1]", "score": 0}], "B_highlights": [{"paper_id": "P0139", "evidence_id": "E-P0139-8eb9ff221b", "excerpt": "Using a corpus of 219 labeled rubrics derived from the GDPVal benchmark, we evaluate ARCANE on challenging tasks requiring multi-step reasoning and tool use.", "citations": ["Masters2025Arcane"], "pointer": "papers/paper_notes.jsonl:paper_id=P0139#key_results[0]", "score": 1}, {"paper_id": "P0079", "evidence_id": "E-P0079-1dd544863c", "excerpt": "Our experiments on Communicative Watch-And-Help and ThreeD-World Multi-Agent Transport benchmarks demonstrate that LIET, instantiated with both LLaMA and GPT-4o, outperforms existing baselines and exhibits strong cooperative planning abilities.", "citations": ["Li2025Learn"], "pointer": "papers/paper_notes.jsonl:paper_id=P0079#key_results[1]", "score": 0}], "write_prompt": "Contrast Agent frameworks / architectures vs Multi-agent coordination along \"tool selection / routing policy\". Ground A and B using the highlight snippets; do not introduce new claims beyond the cited evidence.", "evidence_field": "tool selection / routing policy", "citations": ["Masters2025Arcane", "Li2025Learn"]}], "evaluation_protocol": [{"bullet": "Evaluation mentions include: ReAct, LLM-Agent-Controller, RAG, LLMs, LatteReview, GitHub, LLM-based, LIET, LLaMA, GPT-4o.", "citations": ["Aratchige2025Llms", "Zahedifar2025Agent", "Rouzrokh2025Lattereview", "Li2025Learn"]}, {"bullet": "When comparing results, anchor the paragraph with: task type + metric + constraint (budget, tool access, horizon, or threat model) when stated.", "citations": ["Aratchige2025Llms", "Zahedifar2025Agent"]}, {"bullet": "Prefer head-to-head comparisons only when benchmark/metric are shared; otherwise frame differences as protocol-driven rather than method superiority.", "citations": ["Aratchige2025Llms", "Zahedifar2025Agent"]}, {"bullet": "Avoid underspecified model/baseline naming; if abstracts omit details, state that the baseline is reported but underspecified instead of guessing.", "citations": ["Aratchige2025Llms", "Zahedifar2025Agent"]}, {"bullet": "If a claim relies on a single reported number, pair it with a limitation/caveat from the same evidence so the draft remains conservative.", "citations": ["Aratchige2025Llms", "Zahedifar2025Agent"]}, {"bullet": "If budgets or environments differ across papers, treat cross-paper numeric comparison as fragile and prefer qualitative contrasts aligned to the subsection axes.", "citations": ["Aratchige2025Llms", "Zahedifar2025Agent"]}], "failures_limitations": [{"bullet": "By analyzing recent advancements and their limitations - such as scalability, real-time response challenges, and agent coordination constraints, we provide a detailed view of the technological landscape.", "citations": ["Aratchige2025Llms"]}, {"bullet": "We address this limitation by introducing a framework that enables LLM agents to learn and evolve both before and during test time, equipping them with environment-relevant knowledge for better planning and enhanced communication for improved cooperation.", "citations": ["Li2025Learn"]}, {"bullet": "Hardware security verification is a challenging and time-consuming task.", "citations": ["Collini2025Marvel"]}, {"bullet": "MARVEL mimics the cognitive process of a designer looking for security vulnerabilities in RTL code.", "citations": ["Collini2025Marvel"]}, {"bullet": "It consists of a supervisor agent that devises the security policy of the system-on-chips (SoCs) using its security documentation.", "citations": ["Collini2025Marvel"]}, {"bullet": "It delegates tasks to validate the security policy to individual executor agents.", "citations": ["Collini2025Marvel"]}, {"bullet": "Each executor agent may use one or more tools to identify potential security bugs in the design and send the results back to the supervisor agent for further analysis and confirmation.", "citations": ["Collini2025Marvel"]}, {"bullet": "We find that 20 of the 48 issues reported by MARVEL pose security vulnerabilities.", "citations": ["Collini2025Marvel"]}], "blocking_missing": [], "verify_fields": ["named benchmarks/datasets used", "metrics/human-eval protocol", "compute/training/inference cost", "training data and supervision signal", "baseline choices and ablation evidence"], "generated_at": "2026-01-25T17:06:54", "section_id": "5", "section_title": "Learning, Adaptation & Coordination"}
{"sub_id": "6.1", "title": "Benchmarks and evaluation protocols", "evidence_ids": ["E-P0113-152d1ce5af", "E-P0122-ef4cf62416", "E-P0216-e2e3b7fa97", "E-P0086-68db58914f", "E-P0037-38a26e4777", "E-P0244-0bcc4031a6", "E-P0124-60cc0d458f", "E-P0224-5607dc887c", "E-P0033-46914a4804", "E-P0133-a68f39bc04", "E-P0231-4fc221fdea", "E-P0162-39281e5083", "E-P0001-ca4a00b5cf", "E-P0149-42512d3cac", "E-P0177-acf4a27e30", "E-P0181-29c690fb6c", "E-P0028-08c43ee74b", "E-P0274-52fea1d199", "E-P0010-793979aed4", "E-P0275-389f65b30d", "E-P0006-3f9a43a03b", "E-P0112-8f668d83bf", "E-P0149-d956dcaf4c", "E-P0174-96f876973b"], "evidence_level_summary": {"fulltext": 0, "abstract": 28, "title": 0}, "evidence_snippets": [{"evidence_id": "E-P0113-152d1ce5af", "text": "Existing evaluation methods suffer from following shortcomings: (1) constrained evaluation abilities, (2) vulnerable benchmarks, (3) unobjective metrics.", "paper_id": "P0113", "citations": ["Lin2023Agentsims"], "provenance": {"evidence_level": "abstract", "source": "paper_notes", "pointer": "papers/paper_notes.jsonl:paper_id=P0113#key_results[0]"}}, {"evidence_id": "E-P0122-ef4cf62416", "text": "We introduce WildAGTEval, a benchmark designed to evaluate large language model (LLM) agents' function-calling capabilities under realistic API complexity.", "paper_id": "P0122", "citations": ["Kim2026Beyond"], "provenance": {"evidence_level": "abstract", "source": "paper_notes", "pointer": "papers/paper_notes.jsonl:paper_id=P0122#method"}}, {"evidence_id": "E-P0216-e2e3b7fa97", "text": "Our evaluation of 16 agents under a unified ReAct framework reveals distinct capabilities and limitations across models.", "paper_id": "P0216", "citations": ["Seo2025Simuhome"], "provenance": {"evidence_level": "abstract", "source": "paper_notes", "pointer": "papers/paper_notes.jsonl:paper_id=P0216#limitations[1]"}}, {"evidence_id": "E-P0086-68db58914f", "text": "Our extensive evaluation across various agent use cases, using benchmarks like AgentDojo, ASB, and AgentPoison, demonstrates that Progent reduces attack success rates to 0%, while preserving agent utility and speed.", "paper_id": "P0086", "citations": ["Shi2025Progent"], "provenance": {"evidence_level": "abstract", "source": "paper_notes", "pointer": "papers/paper_notes.jsonl:paper_id=P0086#key_results[0]"}}, {"evidence_id": "E-P0037-38a26e4777", "text": "Extensive experiments across six benchmarks, covering the diverse scenarios of web, embodied, tool use and game applications, show that AgentSquare substantially outperforms hand-crafted agents, achieving an average performance gain of 17.2% against best-known human designs.", "paper_id": "P0037", "citations": ["Shang2024Agentsquare"], "provenance": {"evidence_level": "abstract", "source": "paper_notes", "pointer": "papers/paper_notes.jsonl:paper_id=P0037#key_results[0]"}}, {"evidence_id": "E-P0244-0bcc4031a6", "text": "This paper presents a systematic and comprehensive review of MLLM evaluation methods, covering the following key aspects: (1) the background of MLLMs and their evaluation; (2) \"what to evaluate\" that reviews and categorizes existing MLLM evaluation tasks based on the capabilities assessed, including general multimodal recognition, perception, reasoning and trustworthiness, and domain-specific applications such as socioeconomic, natural sciences and engineering, medical usage, AI agent, remote sensing, video and audio processing, 3D point cloud analysis, and others; (3) \"where to evaluate\" that summarizes MLLM evaluation benchmarks into general and specific benchmarks; (4) \"how to evaluate\" that reviews and illustrates MLLM evaluation steps and metrics; Our overarching goal is to provide valuable insights for researchers in the field of MLLM evaluation, thereby facilitating the development of more capable and reliable MLLMs.", "paper_id": "P0244", "citations": ["Huang2024Survey"], "provenance": {"evidence_level": "abstract", "source": "paper_notes", "pointer": "papers/paper_notes.jsonl:paper_id=P0244#key_results[0]"}}, {"evidence_id": "E-P0124-60cc0d458f", "text": "Experiments on challenging agentic benchmarks such as GAIA and BrowseComp+ demonstrate that EvoRoute, when integrated into off-the-shelf agentic systems, not only sustains or enhances system performance but also reduces execution cost by up to $80\\%$ and latency by over $70\\%$.", "paper_id": "P0124", "citations": ["Zhang2026Evoroute"], "provenance": {"evidence_level": "abstract", "source": "paper_notes", "pointer": "papers/paper_notes.jsonl:paper_id=P0124#key_results[0]"}}, {"evidence_id": "E-P0224-5607dc887c", "text": "Based on these findings, we design three novel adaptive attacks that significantly improve attack success rates targeting specific frameworks, demonstrating the severity of the flaws in these defenses.", "paper_id": "P0224", "citations": ["Ji2025Taxonomy"], "provenance": {"evidence_level": "abstract", "source": "paper_notes", "pointer": "papers/paper_notes.jsonl:paper_id=P0224#key_results[1]"}}, {"evidence_id": "E-P0033-46914a4804", "text": "Yet their sophisticated architectures amplify vulnerability to cascading failures, where a single root-cause error propagates through subsequent decisions, leading to task failure.", "paper_id": "P0033", "citations": ["Zhu2025Where"], "provenance": {"evidence_level": "abstract", "source": "paper_notes", "pointer": "papers/paper_notes.jsonl:paper_id=P0033#summary_bullets[1]"}}, {"evidence_id": "E-P0133-a68f39bc04", "text": "This survey provides a comprehensive overview of foundation models, agentic systems, datasets, and computational tools supporting this growing field.", "paper_id": "P0133", "citations": ["Van2025Survey"], "provenance": {"evidence_level": "abstract", "source": "paper_notes", "pointer": "papers/paper_notes.jsonl:paper_id=P0133#key_results[0]"}}, {"evidence_id": "E-P0231-4fc221fdea", "text": "This paper proposes an evidence-based, actionable, and generalizable evaluation design guideline for LLM-based RPA by systematically reviewing 1,676 papers published between Jan.", "paper_id": "P0231", "citations": ["Chen2025Towards"], "provenance": {"evidence_level": "abstract", "source": "paper_notes", "pointer": "papers/paper_notes.jsonl:paper_id=P0231#key_results[0]"}}, {"evidence_id": "E-P0162-39281e5083", "text": "Experimental results demonstrate that API-based models outperform open-sourced models on all metrics and Deepseek-Coder-33B-Instruct achieves the highest score among open-sourced models.", "paper_id": "P0162", "citations": ["Zhang2025Datascibench"], "provenance": {"evidence_level": "abstract", "source": "paper_notes", "pointer": "papers/paper_notes.jsonl:paper_id=P0162#key_results[1]"}}, {"evidence_id": "E-P0001-ca4a00b5cf", "text": "On two interactive decision making benchmarks (ALFWorld and WebShop), ReAct outperforms imitation and reinforcement learning methods by an absolute success rate of 34% and 10% respectively, while being prompted with only one or two in-context examples.", "paper_id": "P0001", "citations": ["Yao2022React"], "provenance": {"evidence_level": "abstract", "source": "paper_notes", "pointer": "papers/paper_notes.jsonl:paper_id=P0001#key_results[0]"}}, {"evidence_id": "E-P0149-42512d3cac", "text": "We evaluate AutoSCORE on four benchmark datasets from the ASAP benchmark, using both proprietary and open-source LLMs (GPT-4o, LLaMA-3.1-8B, and LLaMA-3.1-70B).", "paper_id": "P0149", "citations": ["Wang2025Autoscore"], "provenance": {"evidence_level": "abstract", "source": "paper_notes", "pointer": "papers/paper_notes.jsonl:paper_id=P0149#key_results[0]"}}, {"evidence_id": "E-P0177-acf4a27e30", "text": "The system employs tool-augmented reasoning, hierarchical retrieval-augmented generation, forensic-tuned LLMs, and human-in-the-loop feedback to ensure legal and medical validity.", "paper_id": "P0177", "citations": ["Shen2025Feat"], "provenance": {"evidence_level": "abstract", "source": "paper_notes", "pointer": "papers/paper_notes.jsonl:paper_id=P0177#key_results[0]"}}, {"evidence_id": "E-P0181-29c690fb6c", "text": "It proposes five research questions that guide the analysis of the recent literature from 2020 to 2025, focusing on evaluation methods and mitigation techniques.", "paper_id": "P0181", "citations": ["Rahman2025Hallucination"], "provenance": {"evidence_level": "abstract", "source": "paper_notes", "pointer": "papers/paper_notes.jsonl:paper_id=P0181#key_results[0]"}}, {"evidence_id": "E-P0028-08c43ee74b", "text": "ACBench spans (1) 12 tasks across 4 capabilities (e.g., WorfBench for workflow generation, Needle-in-Haystack for long-context retrieval), (2) quantization (GPTQ, AWQ) and pruning (Wanda, SparseGPT), and (3) 15 models, including small (Gemma-2B), standard (Qwen2.5 7B-32B), and distilled reasoning LLMs (DeepSeek-R1-Distill).", "paper_id": "P0028", "citations": ["Dong2025Compressed"], "provenance": {"evidence_level": "abstract", "source": "paper_notes", "pointer": "papers/paper_notes.jsonl:paper_id=P0028#key_results[1]"}}, {"evidence_id": "E-P0274-52fea1d199", "text": "We address research questions such as existing GUI agent frameworks, the collection and utilization of data for training specialized GUI agents, the development of large action models tailored for GUI tasks, and the evaluation metrics and benchmarks necessary to assess their effectiveness.", "paper_id": "P0274", "citations": ["Zhang2024Large"], "provenance": {"evidence_level": "abstract", "source": "paper_notes", "pointer": "papers/paper_notes.jsonl:paper_id=P0274#key_results[1]"}}], "definitions_setup": [{"bullet": "Setup: Which design choices in Benchmarks and evaluation protocols drive the major trade-offs, and how are those trade-offs measured? Scope: in-scope: Core topics directly relevant to 'Benchmarks and evaluation protocols'.. Axes: evaluation protocol (datasets, metrics, human evaluation); compute and latency constraints; tool interface contract (schemas / protocols); tool selection / routing policy; sandboxing / permissions / observability.", "citations": ["Kim2026Beyond", "Zhang2026Evoroute", "Tang2025Empowering"]}], "claim_candidates": [{"claim": "Existing evaluation methods suffer from following shortcomings: (1) constrained evaluation abilities, (2) vulnerable benchmarks, (3) unobjective metrics.", "evidence_field": "evidence_snippet", "citations": ["Lin2023Agentsims"]}, {"claim": "We introduce WildAGTEval, a benchmark designed to evaluate large language model (LLM) agents' function-calling capabilities under realistic API complexity.", "evidence_field": "evidence_snippet", "citations": ["Kim2026Beyond"]}, {"claim": "Our evaluation of 16 agents under a unified ReAct framework reveals distinct capabilities and limitations across models.", "evidence_field": "evidence_snippet", "citations": ["Seo2025Simuhome"]}, {"claim": "Our extensive evaluation across various agent use cases, using benchmarks like AgentDojo, ASB, and AgentPoison, demonstrates that Progent reduces attack success rates to 0%, while preserving agent utility and speed.", "evidence_field": "evidence_snippet", "citations": ["Shi2025Progent"]}, {"claim": "Extensive experiments across six benchmarks, covering the diverse scenarios of web, embodied, tool use and game applications, show that AgentSquare substantially outperforms hand-crafted agents, achieving an average performance gain of 17.2% against best-known human designs.", "evidence_field": "evidence_snippet", "citations": ["Shang2024Agentsquare"]}], "concrete_comparisons": [{"axis": "evaluation protocol (datasets, metrics, human evaluation)", "A_label": "Agent frameworks / architectures", "B_label": "Evaluation / benchmark-focused works", "A_papers": "Agent frameworks / architectures: `P0122`, `P0124`, `P0006`", "B_papers": "Evaluation / benchmark-focused works: `P0122`, `P0006`, `P0028`", "A_highlights": [{"paper_id": "P0122", "evidence_id": "E-P0122-ef4cf62416", "excerpt": "We introduce WildAGTEval, a benchmark designed to evaluate large language model (LLM) agents' function-calling capabilities under realistic API complexity.", "citations": ["Kim2026Beyond"], "pointer": "papers/paper_notes.jsonl:paper_id=P0122#method", "score": 3}, {"paper_id": "P0133", "evidence_id": "E-P0133-a68f39bc04", "excerpt": "This survey provides a comprehensive overview of foundation models, agentic systems, datasets, and computational tools supporting this growing field.", "citations": ["Van2025Survey"], "pointer": "papers/paper_notes.jsonl:paper_id=P0133#key_results[0]", "score": 2}], "B_highlights": [{"paper_id": "P0122", "evidence_id": "E-P0122-ef4cf62416", "excerpt": "We introduce WildAGTEval, a benchmark designed to evaluate large language model (LLM) agents' function-calling capabilities under realistic API complexity.", "citations": ["Kim2026Beyond"], "pointer": "papers/paper_notes.jsonl:paper_id=P0122#method", "score": 3}, {"paper_id": "P0162", "evidence_id": "E-P0162-39281e5083", "excerpt": "Experimental results demonstrate that API-based models outperform open-sourced models on all metrics and Deepseek-Coder-33B-Instruct achieves the highest score among open-sourced models.", "citations": ["Zhang2025Datascibench"], "pointer": "papers/paper_notes.jsonl:paper_id=P0162#key_results[1]", "score": 1}], "write_prompt": "Contrast Agent frameworks / architectures vs Evaluation / benchmark-focused works along \"evaluation protocol (datasets, metrics, human evaluation)\". Ground A and B using the highlight snippets; do not introduce new claims beyond the cited evidence.", "evidence_field": "evaluation protocol (datasets, metrics, human evaluation)", "citations": ["Kim2026Beyond", "Van2025Survey", "Zhang2025Datascibench"]}, {"axis": "evaluation protocol (datasets, metrics, human evaluation)", "A_label": "Agent frameworks / architectures", "B_label": "Multi-agent coordination", "A_papers": "Agent frameworks / architectures: `P0122`, `P0124`, `P0006`", "B_papers": "Multi-agent coordination: `P0149`, `P0177`, `P0193`", "A_highlights": [{"paper_id": "P0122", "evidence_id": "E-P0122-ef4cf62416", "excerpt": "We introduce WildAGTEval, a benchmark designed to evaluate large language model (LLM) agents' function-calling capabilities under realistic API complexity.", "citations": ["Kim2026Beyond"], "pointer": "papers/paper_notes.jsonl:paper_id=P0122#method", "score": 3}, {"paper_id": "P0133", "evidence_id": "E-P0133-a68f39bc04", "excerpt": "This survey provides a comprehensive overview of foundation models, agentic systems, datasets, and computational tools supporting this growing field.", "citations": ["Van2025Survey"], "pointer": "papers/paper_notes.jsonl:paper_id=P0133#key_results[0]", "score": 2}], "B_highlights": [{"paper_id": "P0149", "evidence_id": "E-P0149-42512d3cac", "excerpt": "We evaluate AutoSCORE on four benchmark datasets from the ASAP benchmark, using both proprietary and open-source LLMs (GPT-4o, LLaMA-3.1-8B, and LLaMA-3.1-70B).", "citations": ["Wang2025Autoscore"], "pointer": "papers/paper_notes.jsonl:paper_id=P0149#key_results[0]", "score": 2}, {"paper_id": "P0177", "evidence_id": "E-P0177-acf4a27e30", "excerpt": "The system employs tool-augmented reasoning, hierarchical retrieval-augmented generation, forensic-tuned LLMs, and human-in-the-loop feedback to ensure legal and medical validity.", "citations": ["Shen2025Feat"], "pointer": "papers/paper_notes.jsonl:paper_id=P0177#key_results[0]", "score": 1}], "write_prompt": "Contrast Agent frameworks / architectures vs Multi-agent coordination along \"evaluation protocol (datasets, metrics, human evaluation)\". Ground A and B using the highlight snippets; do not introduce new claims beyond the cited evidence.", "evidence_field": "evaluation protocol (datasets, metrics, human evaluation)", "citations": ["Kim2026Beyond", "Van2025Survey", "Wang2025Autoscore", "Shen2025Feat"]}, {"axis": "evaluation protocol (datasets, metrics, human evaluation)", "A_label": "Evaluation / benchmark-focused works", "B_label": "Multi-agent coordination", "A_papers": "Evaluation / benchmark-focused works: `P0122`, `P0006`, `P0028`", "B_papers": "Multi-agent coordination: `P0149`, `P0177`, `P0193`", "A_highlights": [{"paper_id": "P0122", "evidence_id": "E-P0122-ef4cf62416", "excerpt": "We introduce WildAGTEval, a benchmark designed to evaluate large language model (LLM) agents' function-calling capabilities under realistic API complexity.", "citations": ["Kim2026Beyond"], "pointer": "papers/paper_notes.jsonl:paper_id=P0122#method", "score": 3}, {"paper_id": "P0162", "evidence_id": "E-P0162-39281e5083", "excerpt": "Experimental results demonstrate that API-based models outperform open-sourced models on all metrics and Deepseek-Coder-33B-Instruct achieves the highest score among open-sourced models.", "citations": ["Zhang2025Datascibench"], "pointer": "papers/paper_notes.jsonl:paper_id=P0162#key_results[1]", "score": 1}], "B_highlights": [{"paper_id": "P0149", "evidence_id": "E-P0149-42512d3cac", "excerpt": "We evaluate AutoSCORE on four benchmark datasets from the ASAP benchmark, using both proprietary and open-source LLMs (GPT-4o, LLaMA-3.1-8B, and LLaMA-3.1-70B).", "citations": ["Wang2025Autoscore"], "pointer": "papers/paper_notes.jsonl:paper_id=P0149#key_results[0]", "score": 2}, {"paper_id": "P0177", "evidence_id": "E-P0177-acf4a27e30", "excerpt": "The system employs tool-augmented reasoning, hierarchical retrieval-augmented generation, forensic-tuned LLMs, and human-in-the-loop feedback to ensure legal and medical validity.", "citations": ["Shen2025Feat"], "pointer": "papers/paper_notes.jsonl:paper_id=P0177#key_results[0]", "score": 1}], "write_prompt": "Contrast Evaluation / benchmark-focused works vs Multi-agent coordination along \"evaluation protocol (datasets, metrics, human evaluation)\". Ground A and B using the highlight snippets; do not introduce new claims beyond the cited evidence.", "evidence_field": "evaluation protocol (datasets, metrics, human evaluation)", "citations": ["Kim2026Beyond", "Zhang2025Datascibench", "Wang2025Autoscore", "Shen2025Feat"]}, {"axis": "compute and latency constraints", "A_label": "Agent frameworks / architectures", "B_label": "Evaluation / benchmark-focused works", "A_papers": "Agent frameworks / architectures: `P0122`, `P0124`, `P0006`", "B_papers": "Evaluation / benchmark-focused works: `P0122`, `P0006`, `P0028`", "A_highlights": [{"paper_id": "P0124", "evidence_id": "E-P0124-60cc0d458f", "excerpt": "Experiments on challenging agentic benchmarks such as GAIA and BrowseComp+ demonstrate that EvoRoute, when integrated into off-the-shelf agentic systems, not only sustains or enhances system performance but also reduces execution cost by up to $80\\%$ and latency by over $70\\%$.", "citations": ["Zhang2026Evoroute"], "pointer": "papers/paper_notes.jsonl:paper_id=P0124#key_results[0]", "score": 2}, {"paper_id": "P0086", "evidence_id": "E-P0086-68db58914f", "excerpt": "Our extensive evaluation across various agent use cases, using benchmarks like AgentDojo, ASB, and AgentPoison, demonstrates that Progent reduces attack success rates to 0%, while preserving agent utility and speed.", "citations": ["Shi2025Progent"], "pointer": "papers/paper_notes.jsonl:paper_id=P0086#key_results[0]", "score": 1}], "B_highlights": [{"paper_id": "P0028", "evidence_id": "E-P0028-08c43ee74b", "excerpt": "ACBench spans (1) 12 tasks across 4 capabilities (e.g., WorfBench for workflow generation, Needle-in-Haystack for long-context retrieval), (2) quantization (GPTQ, AWQ) and pruning (Wanda, SparseGPT), and (3) 15 models, including small (Gemma-2B), standard (Qwen2.5 7B-32B), and", "citations": ["Dong2025Compressed"], "pointer": "papers/paper_notes.jsonl:paper_id=P0028#key_results[1]", "score": 0}, {"paper_id": "P0162", "evidence_id": "E-P0162-39281e5083", "excerpt": "Experimental results demonstrate that API-based models outperform open-sourced models on all metrics and Deepseek-Coder-33B-Instruct achieves the highest score among open-sourced models.", "citations": ["Zhang2025Datascibench"], "pointer": "papers/paper_notes.jsonl:paper_id=P0162#key_results[1]", "score": 0}], "write_prompt": "Contrast Agent frameworks / architectures vs Evaluation / benchmark-focused works along \"compute and latency constraints\". Ground A and B using the highlight snippets; do not introduce new claims beyond the cited evidence.", "evidence_field": "compute and latency constraints", "citations": ["Zhang2026Evoroute", "Shi2025Progent", "Dong2025Compressed", "Zhang2025Datascibench"]}, {"axis": "compute and latency constraints", "A_label": "Agent frameworks / architectures", "B_label": "Multi-agent coordination", "A_papers": "Agent frameworks / architectures: `P0122`, `P0124`, `P0006`", "B_papers": "Multi-agent coordination: `P0149`, `P0177`, `P0193`", "A_highlights": [{"paper_id": "P0124", "evidence_id": "E-P0124-60cc0d458f", "excerpt": "Experiments on challenging agentic benchmarks such as GAIA and BrowseComp+ demonstrate that EvoRoute, when integrated into off-the-shelf agentic systems, not only sustains or enhances system performance but also reduces execution cost by up to $80\\%$ and latency by over $70\\%$.", "citations": ["Zhang2026Evoroute"], "pointer": "papers/paper_notes.jsonl:paper_id=P0124#key_results[0]", "score": 2}, {"paper_id": "P0086", "evidence_id": "E-P0086-68db58914f", "excerpt": "Our extensive evaluation across various agent use cases, using benchmarks like AgentDojo, ASB, and AgentPoison, demonstrates that Progent reduces attack success rates to 0%, while preserving agent utility and speed.", "citations": ["Shi2025Progent"], "pointer": "papers/paper_notes.jsonl:paper_id=P0086#key_results[0]", "score": 1}], "B_highlights": [{"paper_id": "P0177", "evidence_id": "E-P0177-acf4a27e30", "excerpt": "The system employs tool-augmented reasoning, hierarchical retrieval-augmented generation, forensic-tuned LLMs, and human-in-the-loop feedback to ensure legal and medical validity.", "citations": ["Shen2025Feat"], "pointer": "papers/paper_notes.jsonl:paper_id=P0177#key_results[0]", "score": 0}, {"paper_id": "P0149", "evidence_id": "E-P0149-42512d3cac", "excerpt": "We evaluate AutoSCORE on four benchmark datasets from the ASAP benchmark, using both proprietary and open-source LLMs (GPT-4o, LLaMA-3.1-8B, and LLaMA-3.1-70B).", "citations": ["Wang2025Autoscore"], "pointer": "papers/paper_notes.jsonl:paper_id=P0149#key_results[0]", "score": 0}], "write_prompt": "Contrast Agent frameworks / architectures vs Multi-agent coordination along \"compute and latency constraints\". Ground A and B using the highlight snippets; do not introduce new claims beyond the cited evidence.", "evidence_field": "compute and latency constraints", "citations": ["Zhang2026Evoroute", "Shi2025Progent", "Shen2025Feat", "Wang2025Autoscore"]}, {"axis": "compute and latency constraints", "A_label": "Evaluation / benchmark-focused works", "B_label": "Multi-agent coordination", "A_papers": "Evaluation / benchmark-focused works: `P0122`, `P0006`, `P0028`", "B_papers": "Multi-agent coordination: `P0149`, `P0177`, `P0193`", "A_highlights": [{"paper_id": "P0028", "evidence_id": "E-P0028-08c43ee74b", "excerpt": "ACBench spans (1) 12 tasks across 4 capabilities (e.g., WorfBench for workflow generation, Needle-in-Haystack for long-context retrieval), (2) quantization (GPTQ, AWQ) and pruning (Wanda, SparseGPT), and (3) 15 models, including small (Gemma-2B), standard (Qwen2.5 7B-32B), and", "citations": ["Dong2025Compressed"], "pointer": "papers/paper_notes.jsonl:paper_id=P0028#key_results[1]", "score": 0}, {"paper_id": "P0162", "evidence_id": "E-P0162-39281e5083", "excerpt": "Experimental results demonstrate that API-based models outperform open-sourced models on all metrics and Deepseek-Coder-33B-Instruct achieves the highest score among open-sourced models.", "citations": ["Zhang2025Datascibench"], "pointer": "papers/paper_notes.jsonl:paper_id=P0162#key_results[1]", "score": 0}], "B_highlights": [{"paper_id": "P0177", "evidence_id": "E-P0177-acf4a27e30", "excerpt": "The system employs tool-augmented reasoning, hierarchical retrieval-augmented generation, forensic-tuned LLMs, and human-in-the-loop feedback to ensure legal and medical validity.", "citations": ["Shen2025Feat"], "pointer": "papers/paper_notes.jsonl:paper_id=P0177#key_results[0]", "score": 0}, {"paper_id": "P0149", "evidence_id": "E-P0149-42512d3cac", "excerpt": "We evaluate AutoSCORE on four benchmark datasets from the ASAP benchmark, using both proprietary and open-source LLMs (GPT-4o, LLaMA-3.1-8B, and LLaMA-3.1-70B).", "citations": ["Wang2025Autoscore"], "pointer": "papers/paper_notes.jsonl:paper_id=P0149#key_results[0]", "score": 0}], "write_prompt": "Contrast Evaluation / benchmark-focused works vs Multi-agent coordination along \"compute and latency constraints\". Ground A and B using the highlight snippets; do not introduce new claims beyond the cited evidence.", "evidence_field": "compute and latency constraints", "citations": ["Dong2025Compressed", "Zhang2025Datascibench", "Shen2025Feat", "Wang2025Autoscore"]}, {"axis": "tool interface contract (schemas / protocols)", "A_label": "Agent frameworks / architectures", "B_label": "Evaluation / benchmark-focused works", "A_papers": "Agent frameworks / architectures: `P0122`, `P0124`, `P0006`", "B_papers": "Evaluation / benchmark-focused works: `P0122`, `P0006`, `P0028`", "A_highlights": [{"paper_id": "P0122", "evidence_id": "E-P0122-ef4cf62416", "excerpt": "We introduce WildAGTEval, a benchmark designed to evaluate large language model (LLM) agents' function-calling capabilities under realistic API complexity.", "citations": ["Kim2026Beyond"], "pointer": "papers/paper_notes.jsonl:paper_id=P0122#method", "score": 2}, {"paper_id": "P0133", "evidence_id": "E-P0133-a68f39bc04", "excerpt": "This survey provides a comprehensive overview of foundation models, agentic systems, datasets, and computational tools supporting this growing field.", "citations": ["Van2025Survey"], "pointer": "papers/paper_notes.jsonl:paper_id=P0133#key_results[0]", "score": 1}], "B_highlights": [{"paper_id": "P0122", "evidence_id": "E-P0122-ef4cf62416", "excerpt": "We introduce WildAGTEval, a benchmark designed to evaluate large language model (LLM) agents' function-calling capabilities under realistic API complexity.", "citations": ["Kim2026Beyond"], "pointer": "papers/paper_notes.jsonl:paper_id=P0122#method", "score": 2}, {"paper_id": "P0162", "evidence_id": "E-P0162-39281e5083", "excerpt": "Experimental results demonstrate that API-based models outperform open-sourced models on all metrics and Deepseek-Coder-33B-Instruct achieves the highest score among open-sourced models.", "citations": ["Zhang2025Datascibench"], "pointer": "papers/paper_notes.jsonl:paper_id=P0162#key_results[1]", "score": 1}], "write_prompt": "Contrast Agent frameworks / architectures vs Evaluation / benchmark-focused works along \"tool interface contract (schemas / protocols)\". Ground A and B using the highlight snippets; do not introduce new claims beyond the cited evidence.", "evidence_field": "tool interface contract (schemas / protocols)", "citations": ["Kim2026Beyond", "Van2025Survey", "Zhang2025Datascibench"]}, {"axis": "tool interface contract (schemas / protocols)", "A_label": "Agent frameworks / architectures", "B_label": "Multi-agent coordination", "A_papers": "Agent frameworks / architectures: `P0122`, `P0124`, `P0006`", "B_papers": "Multi-agent coordination: `P0149`, `P0177`, `P0193`", "A_highlights": [{"paper_id": "P0122", "evidence_id": "E-P0122-ef4cf62416", "excerpt": "We introduce WildAGTEval, a benchmark designed to evaluate large language model (LLM) agents' function-calling capabilities under realistic API complexity.", "citations": ["Kim2026Beyond"], "pointer": "papers/paper_notes.jsonl:paper_id=P0122#method", "score": 2}, {"paper_id": "P0133", "evidence_id": "E-P0133-a68f39bc04", "excerpt": "This survey provides a comprehensive overview of foundation models, agentic systems, datasets, and computational tools supporting this growing field.", "citations": ["Van2025Survey"], "pointer": "papers/paper_notes.jsonl:paper_id=P0133#key_results[0]", "score": 1}], "B_highlights": [{"paper_id": "P0177", "evidence_id": "E-P0177-acf4a27e30", "excerpt": "The system employs tool-augmented reasoning, hierarchical retrieval-augmented generation, forensic-tuned LLMs, and human-in-the-loop feedback to ensure legal and medical validity.", "citations": ["Shen2025Feat"], "pointer": "papers/paper_notes.jsonl:paper_id=P0177#key_results[0]", "score": 1}, {"paper_id": "P0149", "evidence_id": "E-P0149-42512d3cac", "excerpt": "We evaluate AutoSCORE on four benchmark datasets from the ASAP benchmark, using both proprietary and open-source LLMs (GPT-4o, LLaMA-3.1-8B, and LLaMA-3.1-70B).", "citations": ["Wang2025Autoscore"], "pointer": "papers/paper_notes.jsonl:paper_id=P0149#key_results[0]", "score": 0}], "write_prompt": "Contrast Agent frameworks / architectures vs Multi-agent coordination along \"tool interface contract (schemas / protocols)\". Ground A and B using the highlight snippets; do not introduce new claims beyond the cited evidence.", "evidence_field": "tool interface contract (schemas / protocols)", "citations": ["Kim2026Beyond", "Van2025Survey", "Shen2025Feat", "Wang2025Autoscore"]}, {"axis": "tool interface contract (schemas / protocols)", "A_label": "Evaluation / benchmark-focused works", "B_label": "Multi-agent coordination", "A_papers": "Evaluation / benchmark-focused works: `P0122`, `P0006`, `P0028`", "B_papers": "Multi-agent coordination: `P0149`, `P0177`, `P0193`", "A_highlights": [{"paper_id": "P0122", "evidence_id": "E-P0122-ef4cf62416", "excerpt": "We introduce WildAGTEval, a benchmark designed to evaluate large language model (LLM) agents' function-calling capabilities under realistic API complexity.", "citations": ["Kim2026Beyond"], "pointer": "papers/paper_notes.jsonl:paper_id=P0122#method", "score": 2}, {"paper_id": "P0162", "evidence_id": "E-P0162-39281e5083", "excerpt": "Experimental results demonstrate that API-based models outperform open-sourced models on all metrics and Deepseek-Coder-33B-Instruct achieves the highest score among open-sourced models.", "citations": ["Zhang2025Datascibench"], "pointer": "papers/paper_notes.jsonl:paper_id=P0162#key_results[1]", "score": 1}], "B_highlights": [{"paper_id": "P0177", "evidence_id": "E-P0177-acf4a27e30", "excerpt": "The system employs tool-augmented reasoning, hierarchical retrieval-augmented generation, forensic-tuned LLMs, and human-in-the-loop feedback to ensure legal and medical validity.", "citations": ["Shen2025Feat"], "pointer": "papers/paper_notes.jsonl:paper_id=P0177#key_results[0]", "score": 1}, {"paper_id": "P0149", "evidence_id": "E-P0149-42512d3cac", "excerpt": "We evaluate AutoSCORE on four benchmark datasets from the ASAP benchmark, using both proprietary and open-source LLMs (GPT-4o, LLaMA-3.1-8B, and LLaMA-3.1-70B).", "citations": ["Wang2025Autoscore"], "pointer": "papers/paper_notes.jsonl:paper_id=P0149#key_results[0]", "score": 0}], "write_prompt": "Contrast Evaluation / benchmark-focused works vs Multi-agent coordination along \"tool interface contract (schemas / protocols)\". Ground A and B using the highlight snippets; do not introduce new claims beyond the cited evidence.", "evidence_field": "tool interface contract (schemas / protocols)", "citations": ["Kim2026Beyond", "Zhang2025Datascibench", "Shen2025Feat", "Wang2025Autoscore"]}, {"axis": "tool selection / routing policy", "A_label": "Agent frameworks / architectures", "B_label": "Evaluation / benchmark-focused works", "A_papers": "Agent frameworks / architectures: `P0122`, `P0124`, `P0006`", "B_papers": "Evaluation / benchmark-focused works: `P0122`, `P0006`, `P0028`", "A_highlights": [{"paper_id": "P0122", "evidence_id": "E-P0122-ef4cf62416", "excerpt": "We introduce WildAGTEval, a benchmark designed to evaluate large language model (LLM) agents' function-calling capabilities under realistic API complexity.", "citations": ["Kim2026Beyond"], "pointer": "papers/paper_notes.jsonl:paper_id=P0122#method", "score": 2}, {"paper_id": "P0133", "evidence_id": "E-P0133-a68f39bc04", "excerpt": "This survey provides a comprehensive overview of foundation models, agentic systems, datasets, and computational tools supporting this growing field.", "citations": ["Van2025Survey"], "pointer": "papers/paper_notes.jsonl:paper_id=P0133#key_results[0]", "score": 1}], "B_highlights": [{"paper_id": "P0122", "evidence_id": "E-P0122-ef4cf62416", "excerpt": "We introduce WildAGTEval, a benchmark designed to evaluate large language model (LLM) agents' function-calling capabilities under realistic API complexity.", "citations": ["Kim2026Beyond"], "pointer": "papers/paper_notes.jsonl:paper_id=P0122#method", "score": 2}, {"paper_id": "P0162", "evidence_id": "E-P0162-39281e5083", "excerpt": "Experimental results demonstrate that API-based models outperform open-sourced models on all metrics and Deepseek-Coder-33B-Instruct achieves the highest score among open-sourced models.", "citations": ["Zhang2025Datascibench"], "pointer": "papers/paper_notes.jsonl:paper_id=P0162#key_results[1]", "score": 1}], "write_prompt": "Contrast Agent frameworks / architectures vs Evaluation / benchmark-focused works along \"tool selection / routing policy\". Ground A and B using the highlight snippets; do not introduce new claims beyond the cited evidence.", "evidence_field": "tool selection / routing policy", "citations": ["Kim2026Beyond", "Van2025Survey", "Zhang2025Datascibench"]}], "evaluation_protocol": [{"bullet": "Evaluation mentions include: API, LLMs, WildAGTEval, GAIA, EvoRoute, BrowseComp, LLM-based, GLUE, ACBench, GPTQ.", "citations": ["Kim2026Beyond", "Zhang2026Evoroute", "Tang2025Empowering", "Zhang2025Generalizability"]}, {"bullet": "When comparing results, anchor the paragraph with: task type + metric + constraint (budget, tool access, horizon, or threat model) when stated.", "citations": ["Kim2026Beyond", "Zhang2026Evoroute"]}, {"bullet": "Prefer head-to-head comparisons only when benchmark/metric are shared; otherwise frame differences as protocol-driven rather than method superiority.", "citations": ["Kim2026Beyond", "Zhang2026Evoroute"]}, {"bullet": "Avoid underspecified model/baseline naming; if abstracts omit details, state that the baseline is reported but underspecified instead of guessing.", "citations": ["Kim2026Beyond", "Zhang2026Evoroute"]}, {"bullet": "If a claim relies on a single reported number, pair it with a limitation/caveat from the same evidence so the draft remains conservative.", "citations": ["Kim2026Beyond", "Zhang2026Evoroute"]}, {"bullet": "If budgets or environments differ across papers, treat cross-paper numeric comparison as fragile and prefer qualitative contrasts aligned to the subsection axes.", "citations": ["Kim2026Beyond", "Zhang2026Evoroute"]}], "failures_limitations": [{"bullet": "We formalize this challenge as the \\textbf{Agent System Trilemma}: the inherent tension among achieving state-of-the-art performance, minimizing monetary cost, and ensuring rapid task completion.", "citations": ["Zhang2026Evoroute"]}, {"bullet": "However, how to translate the research on general agents into productivity that drives industry transformations remains a significant challenge.", "citations": ["Tang2025Empowering"]}, {"bullet": "We then review datasets, evaluation dimensions, and metrics, highlighting their limitations.", "citations": ["Zhang2025Generalizability"]}, {"bullet": "A critical challenge, however, lies in ensuring agent generalizability - the ability to maintain consistent performance across varied instructions, tasks, environments, and domains, especially those beyond agents' fine-tuning data.", "citations": ["Zhang2025Generalizability"]}, {"bullet": "Yet their sophisticated architectures amplify vulnerability to cascading failures, where a single root-cause error propagates through subsequent decisions, leading to task failure.", "citations": ["Zhu2025Where"]}, {"bullet": "First, we introduce the AgentErrorTaxonomy, a modular classification of failure modes spanning memory, reflection, planning, action, and system-level operations.", "citations": ["Zhu2025Where"]}, {"bullet": "Second, we construct AgentErrorBench, the first dataset of systematically annotated failure trajectories from ALFWorld, GAIA, and WebShop, grounding error analysis in real-world agent rollouts.", "citations": ["Zhu2025Where"]}, {"bullet": "Thanks to our modular design, integrating Progent does not alter agent internals and only requires minimal changes to the existing agent implementation, enhancing its practicality and potential for widespread adoption.", "citations": ["Shi2025Progent"]}], "blocking_missing": [], "verify_fields": ["named benchmarks/datasets used", "metrics/human-eval protocol", "compute/training/inference cost", "training data and supervision signal", "baseline choices and ablation evidence"], "generated_at": "2026-01-25T17:06:54", "section_id": "6", "section_title": "Evaluation & Risks"}
{"sub_id": "6.2", "title": "Safety, security, and governance", "evidence_ids": ["E-P0195-d6095e10e9", "E-P0196-3c65d38a2a", "E-P0086-68db58914f", "E-P0142-03ed8e82d6", "E-P0155-e4be1f69f1", "E-P0158-7edb91824f", "E-P0058-8e34a29629", "E-P0146-a0b404d928", "E-P0081-419e1464da", "E-P0126-8b56718f74", "E-P0216-469a70bb44", "E-P0274-52fea1d199", "E-P0008-a1fb101bda", "E-P0075-fedc33d121", "E-P0060-f10376c14d", "E-P0241-5603d51445", "E-P0030-e38b4bdff3", "E-P0048-108346ac22", "E-P0168-7320460853", "E-P0116-7a2e32f4bc", "E-P0129-d541ee15e3", "E-P0148-7e9d7616ab", "E-P0148-89865e4c80", "E-P0158-0550cab83d"], "evidence_level_summary": {"fulltext": 0, "abstract": 28, "title": 0}, "evidence_snippets": [{"evidence_id": "E-P0195-d6095e10e9", "text": "MSB contributes: (1) a taxonomy of 12 attacks including name-collision, preference manipulation, prompt injections embedded in tool descriptions, out-of-scope parameter requests, user-impersonating responses, false-error escalation, tool-transfer, retrieval injection, and mixed attacks; (2) an evaluation harness that executes attacks by running real tools (both benign and malicious) via MCP rather than simulation; and (3) a robustness metric that quantifies the trade-off between security and performance: Net Resilient Performance (NRP).", "paper_id": "P0195", "citations": ["Zhang2025Security"], "provenance": {"evidence_level": "abstract", "source": "paper_notes", "pointer": "papers/paper_notes.jsonl:paper_id=P0195#key_results[0]"}}, {"evidence_id": "E-P0196-3c65d38a2a", "text": "Through extensive evaluation of leading LLMs, we find that even SOTA models such as GPT-5 (43.72%), Grok-4 (33.33%) and Claude-4.0-Sonnet (29.44%) exhibit significant performance limitations.", "paper_id": "P0196", "citations": ["Luo2025Universe"], "provenance": {"evidence_level": "abstract", "source": "paper_notes", "pointer": "papers/paper_notes.jsonl:paper_id=P0196#limitations[1]"}}, {"evidence_id": "E-P0086-68db58914f", "text": "Our extensive evaluation across various agent use cases, using benchmarks like AgentDojo, ASB, and AgentPoison, demonstrates that Progent reduces attack success rates to 0%, while preserving agent utility and speed.", "paper_id": "P0086", "citations": ["Shi2025Progent"], "provenance": {"evidence_level": "abstract", "source": "paper_notes", "pointer": "papers/paper_notes.jsonl:paper_id=P0086#key_results[0]"}}, {"evidence_id": "E-P0142-03ed8e82d6", "text": "We evaluate AgentVigil on two public benchmarks, AgentDojo and VWA-adv, where it achieves 71% and 70% success rates against agents based on o3-mini and GPT-4o, respectively, nearly doubling the performance of baseline attacks.", "paper_id": "P0142", "citations": ["Wang2025Agentvigil"], "provenance": {"evidence_level": "abstract", "source": "paper_notes", "pointer": "papers/paper_notes.jsonl:paper_id=P0142#key_results[0]"}}, {"evidence_id": "E-P0155-e4be1f69f1", "text": "Evaluations on two novel benchmarks demonstrate that BridgeScope enables LLM agents to operate databases more effectively, reduces token usage by up to 80% through improved security awareness, and uniquely supports data-intensive workflows beyond existing toolkits, establishing BridgeScope as a robust foundation for next-generation intelligent data automation.", "paper_id": "P0155", "citations": ["Weng2025Bridgescope"], "provenance": {"evidence_level": "abstract", "source": "paper_notes", "pointer": "papers/paper_notes.jsonl:paper_id=P0155#key_results[0]"}}, {"evidence_id": "E-P0158-7edb91824f", "text": "Leveraging the ToolEmu framework, we conduct a systematic evaluation of quitting behavior across 12 state-of-the-art LLMs.", "paper_id": "P0158", "citations": ["Bonagiri2025Check"], "provenance": {"evidence_level": "abstract", "source": "paper_notes", "pointer": "papers/paper_notes.jsonl:paper_id=P0158#key_results[0]"}}, {"evidence_id": "E-P0058-8e34a29629", "text": "Function Calling showed higher overall attack success rates (73.5% vs 62.59% for MCP), with greater system-centric vulnerability while MCP exhibited increased LLM-centric exposure.", "paper_id": "P0058", "citations": ["Gasmi2025Bridging"], "provenance": {"evidence_level": "abstract", "source": "paper_notes", "pointer": "papers/paper_notes.jsonl:paper_id=P0058#key_results[0]"}}, {"evidence_id": "E-P0146-a0b404d928", "text": "Extensive experiments across ten realistic, simulated tool-use scenarios and a range of popular LLM agents demonstrate consistently high attack success rates (81\\%-95\\%) and significant privacy leakage, with negligible impact on primary task execution.", "paper_id": "P0146", "citations": ["Mo2025Attractive"], "provenance": {"evidence_level": "abstract", "source": "paper_notes", "pointer": "papers/paper_notes.jsonl:paper_id=P0146#key_results[0]"}}, {"evidence_id": "E-P0081-419e1464da", "text": "MCP-RADAR features a challenging dataset of 507 tasks spanning six domains: mathematical reasoning, web search, email, calendar, file management, and terminal operations.", "paper_id": "P0081", "citations": ["Gao2025Radar"], "provenance": {"evidence_level": "abstract", "source": "paper_notes", "pointer": "papers/paper_notes.jsonl:paper_id=P0081#key_results[0]"}}, {"evidence_id": "E-P0126-8b56718f74", "text": "Our major contributions include: (1) systematically analyzing the technical transition from standard legal LLMs to legal agents; (2) presenting a structured taxonomy of current agent applications across distinct legal practice areas; (3) discussing evaluation methodologies specifically for agentic performance in law; and (4) identifying open challenges and outlining future directions for developing robust and autonomous legal assistants.", "paper_id": "P0126", "citations": ["Liu2026Agents"], "provenance": {"evidence_level": "abstract", "source": "paper_notes", "pointer": "papers/paper_notes.jsonl:paper_id=P0126#key_results[0]"}}, {"evidence_id": "E-P0216-469a70bb44", "text": "Our evaluation of 16 agents under a unified ReAct framework reveals distinct capabilities and limitations across models.", "paper_id": "P0216", "citations": ["Seo2025Simuhome"], "provenance": {"evidence_level": "abstract", "source": "paper_notes", "pointer": "papers/paper_notes.jsonl:paper_id=P0216#key_results[1]"}}, {"evidence_id": "E-P0274-52fea1d199", "text": "We address research questions such as existing GUI agent frameworks, the collection and utilization of data for training specialized GUI agents, the development of large action models tailored for GUI tasks, and the evaluation metrics and benchmarks necessary to assess their effectiveness.", "paper_id": "P0274", "citations": ["Zhang2024Large"], "provenance": {"evidence_level": "abstract", "source": "paper_notes", "pointer": "papers/paper_notes.jsonl:paper_id=P0274#key_results[1]"}}, {"evidence_id": "E-P0008-a1fb101bda", "text": "Further, agentic LLMs provide a solution for the problem of LLMs running out of training data: inference-time behavior generates new training states, such that LLMs can keep learning without needing ever larger datasets.", "paper_id": "P0008", "citations": ["Plaat2025Agentic"], "provenance": {"evidence_level": "abstract", "source": "paper_notes", "pointer": "papers/paper_notes.jsonl:paper_id=P0008#key_results[1]"}}, {"evidence_id": "E-P0075-fedc33d121", "text": "Finally, we discuss trustworthiness and evaluation issues that are critical for real-world deployment, and identify several open problems for future research.", "paper_id": "P0075", "citations": ["Han2025Large"], "provenance": {"evidence_level": "abstract", "source": "paper_notes", "pointer": "papers/paper_notes.jsonl:paper_id=P0075#key_results[0]"}}, {"evidence_id": "E-P0060-f10376c14d", "text": "A case study on an auxiliary feedwater system demonstrated the framework's effectiveness, with over 90% accuracy in key elements and consistent tool and argument extraction, supporting its use in safety-critical diagnostics.", "paper_id": "P0060", "citations": ["Marandi2025Complex"], "provenance": {"evidence_level": "abstract", "source": "paper_notes", "pointer": "papers/paper_notes.jsonl:paper_id=P0060#key_results[0]"}}, {"evidence_id": "E-P0241-5603d51445", "text": "Across models and settings, we find that: (1) the frequency of whistleblowing varies widely across model families, (2) increasing the complexity of the task the agent is instructed to complete lowers whistleblowing tendencies, (3) nudging the agent in the system prompt to act morally substantially raises whistleblowing rates, and (4) giving the model more obvious avenues for non-whistleblowing behavior, by providing more tools and a detailed workflow to follow, decreases whistleblowing rates.", "paper_id": "P0241", "citations": ["Agrawal2025Language"], "provenance": {"evidence_level": "abstract", "source": "paper_notes", "pointer": "papers/paper_notes.jsonl:paper_id=P0241#key_results[0]"}}, {"evidence_id": "E-P0030-e38b4bdff3", "text": "To quantify these shifts, we develop the Reasoning Style Vector (RSV), a metric tracking Verification depth, Self-confidence, and Attention focus.", "paper_id": "P0030", "citations": ["Zhou2025Reasoning"], "provenance": {"evidence_level": "abstract", "source": "paper_notes", "pointer": "papers/paper_notes.jsonl:paper_id=P0030#key_results[1]"}}, {"evidence_id": "E-P0048-108346ac22", "text": "Large language models (LLMs) are increasingly deployed as agents in dynamic, real-world environments, where success requires both reasoning and effective tool use.", "paper_id": "P0048", "citations": ["Kang2025Acon"], "provenance": {"evidence_level": "abstract", "source": "paper_notes", "pointer": "papers/paper_notes.jsonl:paper_id=P0048#key_results[1]"}}], "definitions_setup": [{"bullet": "Setup: Which design choices in Safety, security, and governance drive the major trade-offs, and how are those trade-offs measured? Scope: in-scope: Core topics directly relevant to 'Safety, security, and governance'.. Axes: evaluation protocol (datasets, metrics, human evaluation); compute and latency constraints; tool interface contract (schemas / protocols); tool selection / routing policy; sandboxing / permissions / observability.", "citations": ["Liu2026Agents", "Plaat2025Agentic", "Zhou2025Reasoning"]}], "claim_candidates": [{"claim": "MSB contributes: (1) a taxonomy of 12 attacks including name-collision, preference manipulation, prompt injections embedded in tool descriptions, out-of-scope parameter requests, user-impersonating responses, false-error escalation, tool-transfer, retrieval injection, and mixed attacks; (2) an evaluation harness that executes attacks by running real tools (both benign and malicious) via MCP", "evidence_field": "evidence_snippet", "citations": ["Zhang2025Security"]}, {"claim": "Through extensive evaluation of leading LLMs, we find that even SOTA models such as GPT-5 (43.72%), Grok-4 (33.33%) and Claude-4.0-Sonnet (29.44%) exhibit significant performance limitations.", "evidence_field": "evidence_snippet", "citations": ["Luo2025Universe"]}, {"claim": "Our extensive evaluation across various agent use cases, using benchmarks like AgentDojo, ASB, and AgentPoison, demonstrates that Progent reduces attack success rates to 0%, while preserving agent utility and speed.", "evidence_field": "evidence_snippet", "citations": ["Shi2025Progent"]}, {"claim": "We evaluate AgentVigil on two public benchmarks, AgentDojo and VWA-adv, where it achieves 71% and 70% success rates against agents based on o3-mini and GPT-4o, respectively, nearly doubling the performance of baseline attacks.", "evidence_field": "evidence_snippet", "citations": ["Wang2025Agentvigil"]}, {"claim": "Evaluations on two novel benchmarks demonstrate that BridgeScope enables LLM agents to operate databases more effectively, reduces token usage by up to 80% through improved security awareness, and uniquely supports data-intensive workflows beyond existing toolkits, establishing BridgeScope as a robust foundation for next-generation intelligent data automation.", "evidence_field": "evidence_snippet", "citations": ["Weng2025Bridgescope"]}], "concrete_comparisons": [{"axis": "evaluation protocol (datasets, metrics, human evaluation)", "A_label": "Agent frameworks / architectures", "B_label": "Safety / security / guardrails", "A_papers": "Agent frameworks / architectures: `P0126`, `P0008`, `P0030`", "B_papers": "Safety / security / guardrails: `P0058`, `P0129`, `P0142`", "A_highlights": [{"paper_id": "P0058", "evidence_id": "E-P0058-8e34a29629", "excerpt": "Function Calling showed higher overall attack success rates (73.5% vs 62.59% for MCP), with greater system-centric vulnerability while MCP exhibited increased LLM-centric exposure.", "citations": ["Gasmi2025Bridging"], "pointer": "papers/paper_notes.jsonl:paper_id=P0058#key_results[0]", "score": 2}, {"paper_id": "P0008", "evidence_id": "E-P0008-a1fb101bda", "excerpt": "Further, agentic LLMs provide a solution for the problem of LLMs running out of training data: inference-time behavior generates new training states, such that LLMs can keep learning without needing ever larger datasets.", "citations": ["Plaat2025Agentic"], "pointer": "papers/paper_notes.jsonl:paper_id=P0008#key_results[1]", "score": 1}], "B_highlights": [{"paper_id": "P0195", "evidence_id": "E-P0195-d6095e10e9", "excerpt": "MSB contributes: (1) a taxonomy of 12 attacks including name-collision, preference manipulation, prompt injections embedded in tool descriptions, out-of-scope parameter requests, user-impersonating responses, false-error escalation, tool-transfer, retrieval injection, and mixed", "citations": ["Zhang2025Security"], "pointer": "papers/paper_notes.jsonl:paper_id=P0195#key_results[0]", "score": 2}, {"paper_id": "P0058", "evidence_id": "E-P0058-8e34a29629", "excerpt": "Function Calling showed higher overall attack success rates (73.5% vs 62.59% for MCP), with greater system-centric vulnerability while MCP exhibited increased LLM-centric exposure.", "citations": ["Gasmi2025Bridging"], "pointer": "papers/paper_notes.jsonl:paper_id=P0058#key_results[0]", "score": 2}], "write_prompt": "Contrast Agent frameworks / architectures vs Safety / security / guardrails along \"evaluation protocol (datasets, metrics, human evaluation)\". Ground A and B using the highlight snippets; do not introduce new claims beyond the cited evidence.", "evidence_field": "evaluation protocol (datasets, metrics, human evaluation)", "citations": ["Gasmi2025Bridging", "Plaat2025Agentic", "Zhang2025Security"]}, {"axis": "evaluation protocol (datasets, metrics, human evaluation)", "A_label": "Agent frameworks / architectures", "B_label": "Tool-use and function calling", "A_papers": "Agent frameworks / architectures: `P0126`, `P0008`, `P0030`", "B_papers": "Tool-use and function calling: `P0081`, `P0146`, `P0155`", "A_highlights": [{"paper_id": "P0058", "evidence_id": "E-P0058-8e34a29629", "excerpt": "Function Calling showed higher overall attack success rates (73.5% vs 62.59% for MCP), with greater system-centric vulnerability while MCP exhibited increased LLM-centric exposure.", "citations": ["Gasmi2025Bridging"], "pointer": "papers/paper_notes.jsonl:paper_id=P0058#key_results[0]", "score": 2}, {"paper_id": "P0008", "evidence_id": "E-P0008-a1fb101bda", "excerpt": "Further, agentic LLMs provide a solution for the problem of LLMs running out of training data: inference-time behavior generates new training states, such that LLMs can keep learning without needing ever larger datasets.", "citations": ["Plaat2025Agentic"], "pointer": "papers/paper_notes.jsonl:paper_id=P0008#key_results[1]", "score": 1}], "B_highlights": [{"paper_id": "P0195", "evidence_id": "E-P0195-d6095e10e9", "excerpt": "MSB contributes: (1) a taxonomy of 12 attacks including name-collision, preference manipulation, prompt injections embedded in tool descriptions, out-of-scope parameter requests, user-impersonating responses, false-error escalation, tool-transfer, retrieval injection, and mixed", "citations": ["Zhang2025Security"], "pointer": "papers/paper_notes.jsonl:paper_id=P0195#key_results[0]", "score": 2}, {"paper_id": "P0155", "evidence_id": "E-P0155-e4be1f69f1", "excerpt": "Evaluations on two novel benchmarks demonstrate that BridgeScope enables LLM agents to operate databases more effectively, reduces token usage by up to 80% through improved security awareness, and uniquely supports data-intensive workflows beyond existing toolkits, establishing", "citations": ["Weng2025Bridgescope"], "pointer": "papers/paper_notes.jsonl:paper_id=P0155#key_results[0]", "score": 2}], "write_prompt": "Contrast Agent frameworks / architectures vs Tool-use and function calling along \"evaluation protocol (datasets, metrics, human evaluation)\". Ground A and B using the highlight snippets; do not introduce new claims beyond the cited evidence.", "evidence_field": "evaluation protocol (datasets, metrics, human evaluation)", "citations": ["Gasmi2025Bridging", "Plaat2025Agentic", "Zhang2025Security", "Weng2025Bridgescope"]}, {"axis": "evaluation protocol (datasets, metrics, human evaluation)", "A_label": "Safety / security / guardrails", "B_label": "Tool-use and function calling", "A_papers": "Safety / security / guardrails: `P0058`, `P0129`, `P0142`", "B_papers": "Tool-use and function calling: `P0081`, `P0146`, `P0155`", "A_highlights": [{"paper_id": "P0195", "evidence_id": "E-P0195-d6095e10e9", "excerpt": "MSB contributes: (1) a taxonomy of 12 attacks including name-collision, preference manipulation, prompt injections embedded in tool descriptions, out-of-scope parameter requests, user-impersonating responses, false-error escalation, tool-transfer, retrieval injection, and mixed", "citations": ["Zhang2025Security"], "pointer": "papers/paper_notes.jsonl:paper_id=P0195#key_results[0]", "score": 2}, {"paper_id": "P0058", "evidence_id": "E-P0058-8e34a29629", "excerpt": "Function Calling showed higher overall attack success rates (73.5% vs 62.59% for MCP), with greater system-centric vulnerability while MCP exhibited increased LLM-centric exposure.", "citations": ["Gasmi2025Bridging"], "pointer": "papers/paper_notes.jsonl:paper_id=P0058#key_results[0]", "score": 2}], "B_highlights": [{"paper_id": "P0195", "evidence_id": "E-P0195-d6095e10e9", "excerpt": "MSB contributes: (1) a taxonomy of 12 attacks including name-collision, preference manipulation, prompt injections embedded in tool descriptions, out-of-scope parameter requests, user-impersonating responses, false-error escalation, tool-transfer, retrieval injection, and mixed", "citations": ["Zhang2025Security"], "pointer": "papers/paper_notes.jsonl:paper_id=P0195#key_results[0]", "score": 2}, {"paper_id": "P0155", "evidence_id": "E-P0155-e4be1f69f1", "excerpt": "Evaluations on two novel benchmarks demonstrate that BridgeScope enables LLM agents to operate databases more effectively, reduces token usage by up to 80% through improved security awareness, and uniquely supports data-intensive workflows beyond existing toolkits, establishing", "citations": ["Weng2025Bridgescope"], "pointer": "papers/paper_notes.jsonl:paper_id=P0155#key_results[0]", "score": 2}], "write_prompt": "Contrast Safety / security / guardrails vs Tool-use and function calling along \"evaluation protocol (datasets, metrics, human evaluation)\". Ground A and B using the highlight snippets; do not introduce new claims beyond the cited evidence.", "evidence_field": "evaluation protocol (datasets, metrics, human evaluation)", "citations": ["Zhang2025Security", "Gasmi2025Bridging", "Weng2025Bridgescope"]}, {"axis": "compute and latency constraints", "A_label": "Agent frameworks / architectures", "B_label": "Safety / security / guardrails", "A_papers": "Agent frameworks / architectures: `P0126`, `P0008`, `P0030`", "B_papers": "Safety / security / guardrails: `P0058`, `P0129`, `P0142`", "A_highlights": [{"paper_id": "P0008", "evidence_id": "E-P0008-a1fb101bda", "excerpt": "Further, agentic LLMs provide a solution for the problem of LLMs running out of training data: inference-time behavior generates new training states, such that LLMs can keep learning without needing ever larger datasets.", "citations": ["Plaat2025Agentic"], "pointer": "papers/paper_notes.jsonl:paper_id=P0008#key_results[1]", "score": 2}, {"paper_id": "P0086", "evidence_id": "E-P0086-68db58914f", "excerpt": "Our extensive evaluation across various agent use cases, using benchmarks like AgentDojo, ASB, and AgentPoison, demonstrates that Progent reduces attack success rates to 0%, while preserving agent utility and speed.", "citations": ["Shi2025Progent"], "pointer": "papers/paper_notes.jsonl:paper_id=P0086#key_results[0]", "score": 1}], "B_highlights": [{"paper_id": "P0195", "evidence_id": "E-P0195-d6095e10e9", "excerpt": "MSB contributes: (1) a taxonomy of 12 attacks including name-collision, preference manipulation, prompt injections embedded in tool descriptions, out-of-scope parameter requests, user-impersonating responses, false-error escalation, tool-transfer, retrieval injection, and mixed", "citations": ["Zhang2025Security"], "pointer": "papers/paper_notes.jsonl:paper_id=P0195#key_results[0]", "score": 1}, {"paper_id": "P0142", "evidence_id": "E-P0142-03ed8e82d6", "excerpt": "We evaluate AgentVigil on two public benchmarks, AgentDojo and VWA-adv, where it achieves 71% and 70% success rates against agents based on o3-mini and GPT-4o, respectively, nearly doubling the performance of baseline attacks.", "citations": ["Wang2025Agentvigil"], "pointer": "papers/paper_notes.jsonl:paper_id=P0142#key_results[0]", "score": 0}], "write_prompt": "Contrast Agent frameworks / architectures vs Safety / security / guardrails along \"compute and latency constraints\". Ground A and B using the highlight snippets; do not introduce new claims beyond the cited evidence.", "evidence_field": "compute and latency constraints", "citations": ["Plaat2025Agentic", "Shi2025Progent", "Zhang2025Security", "Wang2025Agentvigil"]}, {"axis": "compute and latency constraints", "A_label": "Agent frameworks / architectures", "B_label": "Tool-use and function calling", "A_papers": "Agent frameworks / architectures: `P0126`, `P0008`, `P0030`", "B_papers": "Tool-use and function calling: `P0081`, `P0146`, `P0155`", "A_highlights": [{"paper_id": "P0008", "evidence_id": "E-P0008-a1fb101bda", "excerpt": "Further, agentic LLMs provide a solution for the problem of LLMs running out of training data: inference-time behavior generates new training states, such that LLMs can keep learning without needing ever larger datasets.", "citations": ["Plaat2025Agentic"], "pointer": "papers/paper_notes.jsonl:paper_id=P0008#key_results[1]", "score": 2}, {"paper_id": "P0086", "evidence_id": "E-P0086-68db58914f", "excerpt": "Our extensive evaluation across various agent use cases, using benchmarks like AgentDojo, ASB, and AgentPoison, demonstrates that Progent reduces attack success rates to 0%, while preserving agent utility and speed.", "citations": ["Shi2025Progent"], "pointer": "papers/paper_notes.jsonl:paper_id=P0086#key_results[0]", "score": 1}], "B_highlights": [{"paper_id": "P0195", "evidence_id": "E-P0195-d6095e10e9", "excerpt": "MSB contributes: (1) a taxonomy of 12 attacks including name-collision, preference manipulation, prompt injections embedded in tool descriptions, out-of-scope parameter requests, user-impersonating responses, false-error escalation, tool-transfer, retrieval injection, and mixed", "citations": ["Zhang2025Security"], "pointer": "papers/paper_notes.jsonl:paper_id=P0195#key_results[0]", "score": 1}, {"paper_id": "P0155", "evidence_id": "E-P0155-e4be1f69f1", "excerpt": "Evaluations on two novel benchmarks demonstrate that BridgeScope enables LLM agents to operate databases more effectively, reduces token usage by up to 80% through improved security awareness, and uniquely supports data-intensive workflows beyond existing toolkits, establishing", "citations": ["Weng2025Bridgescope"], "pointer": "papers/paper_notes.jsonl:paper_id=P0155#key_results[0]", "score": 0}], "write_prompt": "Contrast Agent frameworks / architectures vs Tool-use and function calling along \"compute and latency constraints\". Ground A and B using the highlight snippets; do not introduce new claims beyond the cited evidence.", "evidence_field": "compute and latency constraints", "citations": ["Plaat2025Agentic", "Shi2025Progent", "Zhang2025Security", "Weng2025Bridgescope"]}, {"axis": "compute and latency constraints", "A_label": "Safety / security / guardrails", "B_label": "Tool-use and function calling", "A_papers": "Safety / security / guardrails: `P0058`, `P0129`, `P0142`", "B_papers": "Tool-use and function calling: `P0081`, `P0146`, `P0155`", "A_highlights": [{"paper_id": "P0195", "evidence_id": "E-P0195-d6095e10e9", "excerpt": "MSB contributes: (1) a taxonomy of 12 attacks including name-collision, preference manipulation, prompt injections embedded in tool descriptions, out-of-scope parameter requests, user-impersonating responses, false-error escalation, tool-transfer, retrieval injection, and mixed", "citations": ["Zhang2025Security"], "pointer": "papers/paper_notes.jsonl:paper_id=P0195#key_results[0]", "score": 1}, {"paper_id": "P0142", "evidence_id": "E-P0142-03ed8e82d6", "excerpt": "We evaluate AgentVigil on two public benchmarks, AgentDojo and VWA-adv, where it achieves 71% and 70% success rates against agents based on o3-mini and GPT-4o, respectively, nearly doubling the performance of baseline attacks.", "citations": ["Wang2025Agentvigil"], "pointer": "papers/paper_notes.jsonl:paper_id=P0142#key_results[0]", "score": 0}], "B_highlights": [{"paper_id": "P0195", "evidence_id": "E-P0195-d6095e10e9", "excerpt": "MSB contributes: (1) a taxonomy of 12 attacks including name-collision, preference manipulation, prompt injections embedded in tool descriptions, out-of-scope parameter requests, user-impersonating responses, false-error escalation, tool-transfer, retrieval injection, and mixed", "citations": ["Zhang2025Security"], "pointer": "papers/paper_notes.jsonl:paper_id=P0195#key_results[0]", "score": 1}, {"paper_id": "P0155", "evidence_id": "E-P0155-e4be1f69f1", "excerpt": "Evaluations on two novel benchmarks demonstrate that BridgeScope enables LLM agents to operate databases more effectively, reduces token usage by up to 80% through improved security awareness, and uniquely supports data-intensive workflows beyond existing toolkits, establishing", "citations": ["Weng2025Bridgescope"], "pointer": "papers/paper_notes.jsonl:paper_id=P0155#key_results[0]", "score": 0}], "write_prompt": "Contrast Safety / security / guardrails vs Tool-use and function calling along \"compute and latency constraints\". Ground A and B using the highlight snippets; do not introduce new claims beyond the cited evidence.", "evidence_field": "compute and latency constraints", "citations": ["Zhang2025Security", "Wang2025Agentvigil", "Weng2025Bridgescope"]}, {"axis": "tool interface contract (schemas / protocols)", "A_label": "Agent frameworks / architectures", "B_label": "Safety / security / guardrails", "A_papers": "Agent frameworks / architectures: `P0126`, `P0008`, `P0030`", "B_papers": "Safety / security / guardrails: `P0058`, `P0129`, `P0142`", "A_highlights": [{"paper_id": "P0058", "evidence_id": "E-P0058-8e34a29629", "excerpt": "Function Calling showed higher overall attack success rates (73.5% vs 62.59% for MCP), with greater system-centric vulnerability while MCP exhibited increased LLM-centric exposure.", "citations": ["Gasmi2025Bridging"], "pointer": "papers/paper_notes.jsonl:paper_id=P0058#key_results[0]", "score": 2}, {"paper_id": "P0048", "evidence_id": "E-P0048-108346ac22", "excerpt": "Large language models (LLMs) are increasingly deployed as agents in dynamic, real-world environments, where success requires both reasoning and effective tool use.", "citations": ["Kang2025Acon"], "pointer": "papers/paper_notes.jsonl:paper_id=P0048#key_results[1]", "score": 1}], "B_highlights": [{"paper_id": "P0195", "evidence_id": "E-P0195-d6095e10e9", "excerpt": "MSB contributes: (1) a taxonomy of 12 attacks including name-collision, preference manipulation, prompt injections embedded in tool descriptions, out-of-scope parameter requests, user-impersonating responses, false-error escalation, tool-transfer, retrieval injection, and mixed", "citations": ["Zhang2025Security"], "pointer": "papers/paper_notes.jsonl:paper_id=P0195#key_results[0]", "score": 2}, {"paper_id": "P0058", "evidence_id": "E-P0058-8e34a29629", "excerpt": "Function Calling showed higher overall attack success rates (73.5% vs 62.59% for MCP), with greater system-centric vulnerability while MCP exhibited increased LLM-centric exposure.", "citations": ["Gasmi2025Bridging"], "pointer": "papers/paper_notes.jsonl:paper_id=P0058#key_results[0]", "score": 2}], "write_prompt": "Contrast Agent frameworks / architectures vs Safety / security / guardrails along \"tool interface contract (schemas / protocols)\". Ground A and B using the highlight snippets; do not introduce new claims beyond the cited evidence.", "evidence_field": "tool interface contract (schemas / protocols)", "citations": ["Gasmi2025Bridging", "Kang2025Acon", "Zhang2025Security"]}, {"axis": "tool interface contract (schemas / protocols)", "A_label": "Agent frameworks / architectures", "B_label": "Tool-use and function calling", "A_papers": "Agent frameworks / architectures: `P0126`, `P0008`, `P0030`", "B_papers": "Tool-use and function calling: `P0081`, `P0146`, `P0155`", "A_highlights": [{"paper_id": "P0058", "evidence_id": "E-P0058-8e34a29629", "excerpt": "Function Calling showed higher overall attack success rates (73.5% vs 62.59% for MCP), with greater system-centric vulnerability while MCP exhibited increased LLM-centric exposure.", "citations": ["Gasmi2025Bridging"], "pointer": "papers/paper_notes.jsonl:paper_id=P0058#key_results[0]", "score": 2}, {"paper_id": "P0048", "evidence_id": "E-P0048-108346ac22", "excerpt": "Large language models (LLMs) are increasingly deployed as agents in dynamic, real-world environments, where success requires both reasoning and effective tool use.", "citations": ["Kang2025Acon"], "pointer": "papers/paper_notes.jsonl:paper_id=P0048#key_results[1]", "score": 1}], "B_highlights": [{"paper_id": "P0195", "evidence_id": "E-P0195-d6095e10e9", "excerpt": "MSB contributes: (1) a taxonomy of 12 attacks including name-collision, preference manipulation, prompt injections embedded in tool descriptions, out-of-scope parameter requests, user-impersonating responses, false-error escalation, tool-transfer, retrieval injection, and mixed", "citations": ["Zhang2025Security"], "pointer": "papers/paper_notes.jsonl:paper_id=P0195#key_results[0]", "score": 2}, {"paper_id": "P0155", "evidence_id": "E-P0155-e4be1f69f1", "excerpt": "Evaluations on two novel benchmarks demonstrate that BridgeScope enables LLM agents to operate databases more effectively, reduces token usage by up to 80% through improved security awareness, and uniquely supports data-intensive workflows beyond existing toolkits, establishing", "citations": ["Weng2025Bridgescope"], "pointer": "papers/paper_notes.jsonl:paper_id=P0155#key_results[0]", "score": 1}], "write_prompt": "Contrast Agent frameworks / architectures vs Tool-use and function calling along \"tool interface contract (schemas / protocols)\". Ground A and B using the highlight snippets; do not introduce new claims beyond the cited evidence.", "evidence_field": "tool interface contract (schemas / protocols)", "citations": ["Gasmi2025Bridging", "Kang2025Acon", "Zhang2025Security", "Weng2025Bridgescope"]}, {"axis": "tool interface contract (schemas / protocols)", "A_label": "Safety / security / guardrails", "B_label": "Tool-use and function calling", "A_papers": "Safety / security / guardrails: `P0058`, `P0129`, `P0142`", "B_papers": "Tool-use and function calling: `P0081`, `P0146`, `P0155`", "A_highlights": [{"paper_id": "P0195", "evidence_id": "E-P0195-d6095e10e9", "excerpt": "MSB contributes: (1) a taxonomy of 12 attacks including name-collision, preference manipulation, prompt injections embedded in tool descriptions, out-of-scope parameter requests, user-impersonating responses, false-error escalation, tool-transfer, retrieval injection, and mixed", "citations": ["Zhang2025Security"], "pointer": "papers/paper_notes.jsonl:paper_id=P0195#key_results[0]", "score": 2}, {"paper_id": "P0058", "evidence_id": "E-P0058-8e34a29629", "excerpt": "Function Calling showed higher overall attack success rates (73.5% vs 62.59% for MCP), with greater system-centric vulnerability while MCP exhibited increased LLM-centric exposure.", "citations": ["Gasmi2025Bridging"], "pointer": "papers/paper_notes.jsonl:paper_id=P0058#key_results[0]", "score": 2}], "B_highlights": [{"paper_id": "P0195", "evidence_id": "E-P0195-d6095e10e9", "excerpt": "MSB contributes: (1) a taxonomy of 12 attacks including name-collision, preference manipulation, prompt injections embedded in tool descriptions, out-of-scope parameter requests, user-impersonating responses, false-error escalation, tool-transfer, retrieval injection, and mixed", "citations": ["Zhang2025Security"], "pointer": "papers/paper_notes.jsonl:paper_id=P0195#key_results[0]", "score": 2}, {"paper_id": "P0155", "evidence_id": "E-P0155-e4be1f69f1", "excerpt": "Evaluations on two novel benchmarks demonstrate that BridgeScope enables LLM agents to operate databases more effectively, reduces token usage by up to 80% through improved security awareness, and uniquely supports data-intensive workflows beyond existing toolkits, establishing", "citations": ["Weng2025Bridgescope"], "pointer": "papers/paper_notes.jsonl:paper_id=P0155#key_results[0]", "score": 1}], "write_prompt": "Contrast Safety / security / guardrails vs Tool-use and function calling along \"tool interface contract (schemas / protocols)\". Ground A and B using the highlight snippets; do not introduce new claims beyond the cited evidence.", "evidence_field": "tool interface contract (schemas / protocols)", "citations": ["Zhang2025Security", "Gasmi2025Bridging", "Weng2025Bridgescope"]}, {"axis": "tool selection / routing policy", "A_label": "Agent frameworks / architectures", "B_label": "Safety / security / guardrails", "A_papers": "Agent frameworks / architectures: `P0126`, `P0008`, `P0030`", "B_papers": "Safety / security / guardrails: `P0058`, `P0129`, `P0142`", "A_highlights": [{"paper_id": "P0058", "evidence_id": "E-P0058-8e34a29629", "excerpt": "Function Calling showed higher overall attack success rates (73.5% vs 62.59% for MCP), with greater system-centric vulnerability while MCP exhibited increased LLM-centric exposure.", "citations": ["Gasmi2025Bridging"], "pointer": "papers/paper_notes.jsonl:paper_id=P0058#key_results[0]", "score": 2}, {"paper_id": "P0048", "evidence_id": "E-P0048-108346ac22", "excerpt": "Large language models (LLMs) are increasingly deployed as agents in dynamic, real-world environments, where success requires both reasoning and effective tool use.", "citations": ["Kang2025Acon"], "pointer": "papers/paper_notes.jsonl:paper_id=P0048#key_results[1]", "score": 1}], "B_highlights": [{"paper_id": "P0195", "evidence_id": "E-P0195-d6095e10e9", "excerpt": "MSB contributes: (1) a taxonomy of 12 attacks including name-collision, preference manipulation, prompt injections embedded in tool descriptions, out-of-scope parameter requests, user-impersonating responses, false-error escalation, tool-transfer, retrieval injection, and mixed", "citations": ["Zhang2025Security"], "pointer": "papers/paper_notes.jsonl:paper_id=P0195#key_results[0]", "score": 2}, {"paper_id": "P0058", "evidence_id": "E-P0058-8e34a29629", "excerpt": "Function Calling showed higher overall attack success rates (73.5% vs 62.59% for MCP), with greater system-centric vulnerability while MCP exhibited increased LLM-centric exposure.", "citations": ["Gasmi2025Bridging"], "pointer": "papers/paper_notes.jsonl:paper_id=P0058#key_results[0]", "score": 2}], "write_prompt": "Contrast Agent frameworks / architectures vs Safety / security / guardrails along \"tool selection / routing policy\". Ground A and B using the highlight snippets; do not introduce new claims beyond the cited evidence.", "evidence_field": "tool selection / routing policy", "citations": ["Gasmi2025Bridging", "Kang2025Acon", "Zhang2025Security"]}], "evaluation_protocol": [{"bullet": "Evaluation mentions include: LLMs, RSP, GSI, RSV, FEVER, RSP-M, HotpotQA, ReAct, ACON, LMs.", "citations": ["Liu2026Agents", "Plaat2025Agentic", "Zhou2025Reasoning", "Kang2025Acon"]}, {"bullet": "When comparing results, anchor the paragraph with: task type + metric + constraint (budget, tool access, horizon, or threat model) when stated.", "citations": ["Liu2026Agents", "Plaat2025Agentic"]}, {"bullet": "Prefer head-to-head comparisons only when benchmark/metric are shared; otherwise frame differences as protocol-driven rather than method superiority.", "citations": ["Liu2026Agents", "Plaat2025Agentic"]}, {"bullet": "Avoid underspecified model/baseline naming; if abstracts omit details, state that the baseline is reported but underspecified instead of guessing.", "citations": ["Liu2026Agents", "Plaat2025Agentic"]}, {"bullet": "If a claim relies on a single reported number, pair it with a limitation/caveat from the same evidence so the draft remains conservative.", "citations": ["Liu2026Agents", "Plaat2025Agentic"]}, {"bullet": "If budgets or environments differ across papers, treat cross-paper numeric comparison as fragile and prefer qualitative contrasts aligned to the subsection axes.", "citations": ["Liu2026Agents", "Plaat2025Agentic"]}], "failures_limitations": [{"bullet": "Large language models (LLMs) have precipitated a dramatic improvement in the legal domain, yet the deployment of standalone models faces significant limitations regarding hallucination, outdated information, and verifiability.", "citations": ["Liu2026Agents"]}, {"bullet": "We note that there is risk associated with LLM assistants taking action in the real world-safety, liability and security are open problems-while agentic LLMs are also likely to benefit society.", "citations": ["Plaat2025Agentic"]}, {"bullet": "While existing adversarial attacks primarily focus on content falsification or instruction injection, we identify a novel, process-oriented attack surface: the agent's reasoning style.", "citations": ["Zhou2025Reasoning"]}, {"bullet": "We introduce Generative Style Injection (GSI), an attack method that rewrites retrieved documents into pathological tones--specifically \"analysis paralysis\" or \"cognitive haste\"--without altering underlying facts or using explicit triggers.", "citations": ["Zhou2025Reasoning"]}, {"bullet": "A central challenge for agentic tasks is the growing context length, as agents must accumulate long histories of actions and observations.", "citations": ["Kang2025Acon"]}, {"bullet": "ACON leverages compression guideline optimization in natural language space: given paired trajectories where full context succeeds but compressed context fails, capable LLMs analyze the causes of failure, and the compression guideline is updated accordingly.", "citations": ["Kang2025Acon"]}, {"bullet": "Large Language Model (LLM) agents face security vulnerabilities spanning AI-specific and traditional software domains, yet current research addresses these separately.", "citations": ["Gasmi2025Bridging"]}, {"bullet": "This study bridges this gap through comparative evaluation of Function Calling architecture and Model Context Protocol (MCP) deployment paradigms using a unified threat classification framework.", "citations": ["Gasmi2025Bridging"]}], "blocking_missing": [], "verify_fields": ["named benchmarks/datasets used", "metrics/human-eval protocol", "compute/training/inference cost", "training data and supervision signal", "baseline choices and ablation evidence"], "generated_at": "2026-01-25T17:06:54", "section_id": "6", "section_title": "Evaluation & Risks"}
