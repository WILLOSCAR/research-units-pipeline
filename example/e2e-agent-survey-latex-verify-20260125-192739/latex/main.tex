\documentclass[a4paper,11pt]{article}

\usepackage{fontspec}
\usepackage[a4paper,margin=1in]{geometry}
\usepackage{hyperref}
\hypersetup{colorlinks=true,linkcolor=blue,citecolor=blue,urlcolor=blue}
\usepackage{enumitem}
\usepackage{booktabs}
\usepackage{tabularx}
\usepackage{array}
\newcolumntype{Y}{>{\raggedright\arraybackslash}X}
\usepackage[numbers]{natbib}
\usepackage{url}

\title{agent survey latex}
\author{}
\date{\today}

\setlist[itemize]{noitemsep,topsep=0.25em,leftmargin=*}
\setcounter{tocdepth}{2}

\begin{document}
\maketitle

\tableofcontents
\newpage

\begin{abstract}


Large language models increasingly act as closed-loop agents that iteratively perceive, decide, and act through tools and environments, rather than producing single-turn answers \citep{Yao2022React,Luo2025Universe,Zhou2026Beyond}. Yet reported performance often conflates model capability with interface contracts, tool access assumptions, and evaluation protocols, making cross-paper comparisons brittle \citep{Hu2025Survey,Shang2024Agentsquare,Zhang2026Evoroute}. This paper organizes the design space around foundations and interfaces, core components for planning and memory, adaptation and multi-agent coordination, and evaluation and risks, emphasizing protocol-aware contrasts over per-paper summaries \citep{Du2025Survey,Li2024Review}. Across these axes, we highlight recurring failure modes in tool orchestration and long-horizon loops, and we summarize mitigation patterns for reliability and security \citep{Wei2025Memguard,Zhang2025Security,Gasmi2025Bridging}. Appendix tables compress representative approaches and protocol anchors to support fast comparison \citep{Lumer2025Memtool,Masters2025Arcane,Wang2025Autoscore}.

\end{abstract}

\section{Introduction}


Tool-augmented large language models are increasingly deployed as agents that must make sequences of decisions under partial observability, cost constraints, and unreliable tool execution \citep{Yao2022React,Luo2025Universe,Zhou2026Beyond,Wu2025Agentic,Cheng2025Your}. In such settings, errors compound: a single mistaken tool call can derail downstream reasoning, while aggressive exploration can create new safety and security surfaces \citep{Wei2025Memguard,Zhang2025Security,Gasmi2025Bridging}.

A useful starting point is to treat an LLM agent as a closed-loop system: it maintains an internal state (explicit or implicit), selects actions, observes outcomes, and updates its state in response \citep{Chen2024Architectural,Hu2023Avis,Jiang2024Agent,Zhou2024Large,Silva2025Agents}. This perspective separates agentic behavior from single-turn tool use by focusing on the loop structure and the assumptions that make the loop stable (e.g., observability, resetability, and action semantics) \citep{Li2023Modelscope,Hu2025Evaluating}.

Interface contracts largely determine what an agent can do and what a result means: schemas and orchestration policies constrain the action space, while permissions and sandboxing determine which actions are feasible at deployment time \citep{Zhang2025Tool,Liu2025Mcpagentbench,Jia2025Autotool,Chowa2025From,Li2025Dissonances}. Seemingly small interface choices can change failure modes (silent tool misfires, name collisions, or unintended capability escalation), which in turn changes which evaluation claims are interpretable \citep{Kim2025Bridging,Zou2025Based}.

Within the loop, core components such as planning, reasoning, and memory retrieval mediate long-horizon behavior, but they interact with protocol constraints in non-obvious ways \citep{Hu2025Training,Chen2024Steering,Du2025Memr,Sun2025Search,Zhou2025Reasoning}. Planning modules that look strong under generous tool access may collapse under stricter budgets or noisier tools, whereas memory policies can trade short-term success for robustness against prompt and data contamination \citep{Wei2025Memguard,Shen2025Feat}.

Evaluation remains a central bottleneck. Agent benchmarks vary widely in task families, success metrics, tool access assumptions, and reporting practices, and such heterogeneity can make head-to-head comparisons fragile unless protocols are normalized \citep{Hu2025Survey,Shang2024Agentsquare,Zhang2026Evoroute,Hu2025Evaluating,Ji2024Testing}. As a result, surveys that primarily list systems without making protocol context explicit often understate uncertainty and overstate generality \citep{Zhang2025Generalizability,Cui2025Toward}.

Risks are inseparable from design choices. Tool interfaces expand the attack surface through prompt injection, data exfiltration, and privilege misuse, and defenses that work for one interface contract may not transfer to another \citep{Zhang2025Security,Gasmi2025Bridging,Wei2025Memguard,Weng2025Bridgescope,Zhang2025Agentic}. Governance and safety therefore depend on making assumptions explicit (threat models, tool boundaries, monitoring) rather than treating agent safety as a monolithic property \citep{Zou2025Based,Kim2025Bridging}.

We compiled this survey by retrieving and deduplicating 1800 arXiv records for LLM-agent and tool-use queries (no strict time-window filter), then selecting a 300-paper core set for synthesis. Evidence is primarily abstract-level: claims are written conservatively when protocol details are missing, and quantitative statements are only used when minimal task, metric, and constraint context can be traced to the cited work \citep{Hu2025Survey,Du2025Survey}.

The remainder of the paper connects systems to the assumptions that drive their reported behavior. We first cover foundations and interfaces (agent loops, tool protocols), then planning and memory components, then adaptation and coordination mechanisms, and finally evaluation and risk surfaces. Appendix tables summarize representative approaches and protocol anchors to support quick cross-section comparison \citep{Lumer2025Memtool,Masters2025Arcane,Wang2025Autoscore,Wei2026Agentic,Abbineni2025Muallm}.

\section{Related Work}


Several recent surveys and reviews chart the rapid growth of LLM-based agents, tool use, and evaluation practices, providing useful snapshots of emerging capabilities and application areas \citep{Hu2025Survey,Du2025Survey,Li2024Review,Ge2025Surveya,Zhang2025Largea}. Their organizing principles vary (task categories, component taxonomies, or application verticals), and many emphasize breadth over protocol-aligned comparisons \citep{Zhang2025Generalizability}.

Work on tool interfaces and orchestration studies how action spaces are exposed to language models (e.g., function calling and protocolized tool APIs) and how agents select, sequence, and verify tool calls \citep{Zhang2025Tool,Jia2025Autotool,Li2024Stride,Chowa2025From,Liu2025Mcpagentbench,Lumer2025Memtool}. These lines highlight that interface contracts (schema design, grounding, permissions) are often the dominant determinant of reliability, yet are inconsistently reported across benchmarks \citep{Gao2025Radar,Cui2025Toward}.

System papers that focus on the agent loop often frame agents as iterative decision-makers that interleave reasoning traces with actions and observations, producing behavior that cannot be evaluated with static QA-style metrics alone \citep{Yao2022React,Luo2025Universe,Zhou2026Beyond,Wu2024Avatar,Zhou2025Self}. Architectural choices such as where memory and planning live, how tool failures are handled, and whether intermediate states are exposed shape both performance and failure modes \citep{Chen2024Architectural,Shen2024Small}.

Planning and reasoning loops span approaches from training specialized planners to steering inference-time deliberation, often trading off compute cost, controllability, and robustness under environment noise \citep{Hu2025Training,Lai2025Ustbench,Wang2025Automated,Chen2024Steering,Zhou2025Reasoning,Webb2023Improving}. A recurring challenge is that planning gains can be protocol-dependent: tool budgets, verification access, and horizon length can flip which method appears superior \citep{Hu2025Evaluating}.

Memory and retrieval-augmented agents extend the loop with mechanisms for state persistence and evidence access, including retrieval policies and representations that mediate what context is available at decision time \citep{Du2025Memr,Sun2025Search,Xu2025Exemplar,Shen2025Feat,Wei2025Memguard,Kang2025Distilling}. This body of work underscores that more context is not universally better: contamination, prompt injection, and stale memories can degrade reliability unless protocols define what information is trusted \citep{Wei2025Memguard}.

Self-improvement and adaptation mechanisms modify policies, prompts, or tools based on feedback, ranging from offline prompt and context optimization to online reflection and revision loops \citep{Zhang2025Agentic,Samplawski2025Agent,Yang2025Proagent,Wu2025Agentic,Cao2025Large,Zhang2025Generalizability}. These approaches raise stability questions (reward hacking, overfitting to benchmarks) that are easy to miss when evaluation protocols are underspecified \citep{Hu2025Evaluating,Ji2024Testing}.

Multi-agent systems introduce additional degrees of freedom such as role protocols, communication channels, and aggregation rules. These mechanisms can improve coverage and verification but also create new failure modes (collusion, feedback loops, cost blow-ups) \citep{Jiang2023Large,Abbineni2025Muallm,Masters2025Arcane,Inoue2024Drugagent,Silva2025Agents,Wei2026Agentic}. Many evaluations in this area rely on rubric-based or task-specific protocols, making it important to track interaction budgets and aggregation assumptions when comparing results \citep{Masters2025Arcane}.

Benchmark suites and evaluation protocols attempt to make agent behavior comparable across environments, but they vary in what constitutes success, how tool access is granted, and how long-horizon credit is assigned \citep{Hu2025Survey,Shang2024Agentsquare,Zhang2026Evoroute,Hu2025Evaluating,Ji2024Testing,Zhang2025Datascibench}. Recent work also emphasizes reproducibility issues such as leakage, hidden environment changes, and dependence on proprietary tool backends, which can invalidate cross-paper rankings if not disclosed \citep{Zhang2025Generalizability,Cui2025Toward}.

Safety and security work argues that deploying tool-using agents requires explicit threat models and interface-aware mitigations, since attacks and defenses depend on the system boundary between model, tools, and external resources \citep{Zhang2025Security,Gasmi2025Bridging,Kim2025Bridging,Wei2025Memguard,Weng2025Bridgescope,Zou2025Based}. This line connects evaluation to governance: metrics that ignore misuse and exfiltration can overstate capability while understating risk \citep{Zhang2025Security}.

Across this literature, a persistent obstacle is that comparisons are frequently made without making protocol context and interface assumptions explicit. The organizing choice in this paper is to treat interfaces and protocols as first-class comparison objects, so that contrasts are anchored in task, metric, budget, and tool-access statements rather than in narrative summaries of individual papers \citep{Hu2025Survey,Du2025Survey,Lumer2025Memtool,Zhang2025Security,Zhang2026Evoroute}.

\section{Foundations \& Interfaces}


Interface choices largely determine what an agent can reliably do and what its measured performance means. Viewing agents as closed-loop systems makes it natural to treat action spaces, observation channels, and tool protocols as first-class design variables rather than peripheral implementation details \citep{Yao2022React,Zhang2025Tool}.

This chapter therefore separates (i) the agent loop and action space abstraction from (ii) the concrete tool interface and orchestration layer. The first subsection focuses on how loop assumptions shape failure recovery and evaluation comparability, while the second focuses on how schema design, routing, and protocolization affect reliability and safety under repeated interactions \citep{Lumer2025Memtool,Liu2025Mcpagentbench}.

\subsection{Agent loop and action spaces}


A central tension in agent loops is that richer action spaces promise broader capability, but they also make behavior harder to constrain, reproduce, and compare under realistic budgets. When evaluation protocols differ (task families, metrics, interaction budgets, tool access), apparent improvements can reflect protocol choices rather than methodological advantages, so comparisons only become meaningful after protocol context is made explicit \citep{Hu2025Survey,Kim2025Bridging,Nusrat2025Automated,Li2025What}. Accordingly, action spaces should be discussed together with the evaluation setup they imply, and protocol alignment should be treated as a prerequisite for cross-paper synthesis \citep{Zhang2026Evoroute}.

An agent loop can be modeled as a repeated state-decide-act-observe cycle, but the semantics of "act" vary widely in tool-using settings. Actions may be direct environment steps, whereas tool-using agents often execute structured tool calls with schemas or mixed actions that combine invocation with natural-language control; each choice changes what is observable and what failure recovery is possible \citep{Yao2022React,Zou2025Based,Jiang2024Agent,Kulkarni2025Agent,Maranto2024Llmsat}. In practice, the action interface also determines what can be audited (e.g., whether intermediate tool arguments are visible) and which failures are recoverable versus silent \citep{Kim2025Bridging,Li2024Personal}.

Action spaces are inseparable from the observation model. If observations are lossy or delayed, agents may rely on implicit state and brittle heuristics; if observations are rich, agents can support verification and correction loops, but at higher compute and latency cost \citep{Feng2025Group,Zhang2026Evoroute,Lindenbauer2025Complexity}. This trade-off is often hidden in benchmark reporting, where small differences in logging, permissions, or tool visibility can change both success rates and the distribution of failure modes \citep{Shen2025Feat,Zou2025Based,Yu2023Finmem}.

The benchmark landscape itself reveals why protocol context matters. A broad survey of agent evaluation spans 190+ benchmark datasets and documents a shift from static exams toward process- and discovery-oriented assessments, where interaction protocols and budgets become first-order variables \citep{Hu2025Survey,Wu2025Meta}. Under such diversity, "agent loop quality" cannot be treated as a single scalar: claims must be interpreted relative to task family, metric definition, and the constraints imposed by the environment and tool interface \citep{Kim2025Bridging,Liu2025Aligning}.

Recent benchmark-driven agent frameworks illustrate the consequence of wide, heterogeneous evaluation. AgentSwift evaluates across seven benchmarks spanning embodied, math, web, tool, and game domains, reporting an average improvement of 8.34\% over baselines in that multi-domain setting \citep{Li2025Agentswift}. AgentSquare reports gains across six benchmarks in similarly diverse scenarios, but the comparability of these numbers depends on whether the underlying protocols (tool access, budgets, environment versions) are aligned across methods \citep{Shang2024Agentsquare,Hu2025Survey}.

Targeted dataset design provides a complementary angle: in contrast to broad benchmark suites, some work introduces bespoke datasets to stress specific loop properties. For example, MuaLLM introduces custom datasets aimed at retrieval and citation behavior and multi-step reasoning, which can surface action-space assumptions that are easy to miss in broader suites \citep{Abbineni2025Muallm}. Such datasets can improve diagnostic power, but they also increase the risk of overfitting unless they are connected back to shared protocol anchors used elsewhere \citep{Hu2025Survey,Kim2025Bridging}.

Security and robustness further complicate the notion of an action space. Memory and tool interactions can act as long-lived state, which creates attack channels where adversarial content influences future decisions; defenses therefore become part of the loop design rather than an external add-on \citep{Wei2025Memguard}. A-MemGuard reports large reductions in attack success rates with minimal utility cost, illustrating that loop safety can be evaluated as a first-class objective, but only when threat models and interaction assumptions are made explicit \citep{Wei2025Memguard,Zou2025Based}.

Compute, latency, and monetary cost impose another unavoidable constraint. The agent system trilemma formalizes the tension among state-of-the-art performance, low cost, and fast completion, suggesting that "better loops" are often better only under a particular budget regime \citep{Zhang2026Evoroute}. This motivates protocol-aware reporting: without budgets and time constraints, comparisons across action-space designs can be misleading even when they cite the same task label \citep{Kim2025Bridging,Hu2025Survey}.

Across these threads, a consistent pattern is that evaluation and action design co-determine each other. Loop abstractions that perform well on interactive benchmarks can fail under stricter tool permissions, while security-aware interfaces can change both capability and measured utility \citep{Yao2022React,Wei2025Memguard,Zhang2026Evoroute}. For synthesis, the conservative move is to compare methods only within shared protocol envelopes, and otherwise describe differences as protocol-driven shifts in what the loop can reliably guarantee \citep{Hu2025Survey,Kim2025Bridging}.

One limitation is that many papers still underspecify protocol details (budgets, tool access, visibility), which forces any survey-level comparison to weaken claims rather than guess missing context \citep{Hu2025Survey,Kim2025Bridging}. Another is that action spaces are implemented through concrete tool interfaces and orchestration policies; without examining those interfaces, loop-level claims remain underspecified, which motivates the next subsection's focus on tool contracts and orchestration patterns \citep{Zou2025Based,Kim2025Bridging}.

While loop design determines what actions are possible, tool interfaces determine how those actions are grounded in executable APIs and orchestration policies.

\subsection{Tool interfaces and orchestration}


For system builders, the crux of tool interface design is choosing how much expressivity to expose while keeping behavior controllable and verifiable. Richer interfaces can expand the action space, but they also increase the burden of protocol specification (schemas, permissions, observability) and widen the gap between what is evaluated and what is deployable \citep{Zhang2025Tool,Liu2025Mcpagentbench,Wang2024Mllm}. Interface contracts largely determine what can be compared and what tool-use claims mean; meaningful comparisons therefore require explicit tool-access assumptions and reporting conventions rather than generic "tool use" narratives \citep{Chowa2025From,Kim2025Bridging,Zhou2024Archer}.

Tool interfaces range from free-form natural-language tool descriptions to structured function signatures and protocolized multi-tool stacks. As interfaces become more structured, they enable stronger verification and orchestration policies (routing, retries, argument validation), but they also introduce new brittleness: schema mismatches and implicit permissions can silently change the agent's effective action space \citep{Li2025Dissonances,Zhang2025Tool,Yin2025Infobid,Zhu2024Menti}. This makes interface design inseparable from evaluation: the same model can look strong or weak depending on which tool errors are detectable and recoverable under the benchmark protocol \citep{Zhou2025Self,Kim2025Bridging}.

Concrete protocolized systems illustrate how evaluation must name the interface details. MemTool evaluates multiple tool modes across 13+ LLMs on the ScaleMCP benchmark, using long interaction traces (100+ consecutive user interactions) and metrics such as tool removal ratios and task completion accuracy \citep{Lumer2025Memtool}. Such settings are useful precisely because they stress long-horizon orchestration behavior and reveal failure modes that short-horizon tasks can hide \citep{Lumer2025Memtool,Gao2025Radar}.

Complementing system-driven evaluations, benchmark and dataset audits provide a map of what is actually being measured. An analysis of dozens of publicly available datasets highlights wide variation in task families and assessment protocols for LLM agents, suggesting that many comparisons are dominated by protocol mismatch rather than method differences \citep{Chowa2025From}. This motivates a protocol-first reading: before comparing orchestration mechanisms, one should ask whether benchmarks share the same tool access assumptions and scoring rules \citep{Chowa2025From,Kim2025Bridging}.

A useful contrast is between protocol-first tool stacks and ad hoc tool calling. Protocolized interfaces can make tool availability and permissions explicit (which supports reproducibility), whereas ad hoc interfaces often leave such details implicit and encourage "best-effort" behavior that is hard to audit \citep{Liu2025Mcpagentbench,Zhang2025Tool,Wang2024Learning}. In practice, the right comparison is therefore not "tool use vs no tool use" but "which interface constraints are enforced, and what failure recovery is allowed under the protocol" \citep{Lumer2025Memtool,Chowa2025From}.

Retrieval-augmented orchestration provides another contrast class. In contrast to ad hoc tool calling, systems such as AvaTaR exploit retrieval and tool selection under a fixed interface contract, reporting gains on retrieval-style settings (e.g., Hit@1 improvements on retrieval datasets) within that evaluation setup \citep{Wu2024Avatar}. These results are only interpretable when the retrieval protocol is pinned down (candidate pool, metric definition, tool access), which again reinforces the need for protocol-aware reporting in the interface layer \citep{Wu2024Avatar,Kim2025Bridging}.

Classic reasoning-action interleaving remains a useful baseline for tool interface discussions because it exposes how interface constraints change loop behavior. On interactive decision-making benchmarks (ALFWorld and WebShop), ReAct reports large absolute success gains over imitation and reinforcement baselines under a few-shot prompting regime \citep{Yao2022React}. Such gains can disappear when tool access policies, step budgets, or evaluation environments shift, which is why later benchmarks and protocol papers emphasize explicit reporting of those constraints \citep{Chowa2025From,Zhou2026Beyond}.

Security and cost failures can originate directly from the agent-tool communication loop. The interface is a critical attack surface, enabling multi-turn attacks that can inflate token usage, monetary costs, and energy consumption even when a task appears to be completed correctly \citep{Zhou2026Beyond}. Reported evaluations show trajectories exceeding 60,000 tokens and cost inflation up to 658x on common tool benchmarks, underscoring that orchestration must be assessed not only for success but also for budget-aware robustness \citep{Zhou2026Beyond,Gao2025Radar}.

Across these studies, the most stable synthesis is to compare orchestration choices within shared protocol envelopes and to treat protocol differences as the primary confounder otherwise. Protocol papers explicitly recommend anchoring claims with task type, metric, and a concrete constraint (budget, tool access, horizon, or threat model), and avoiding underspecified baseline naming when details are missing \citep{Zhou2026Beyond,Chowa2025From,Kim2025Bridging}. This is also where benchmark surveys and suites become valuable: they provide a common vocabulary for what an interface actually constrains \citep{Hu2025Survey,Shang2024Agentsquare}.

One limitation is that interface descriptions often omit operational constraints (permissioning, sandbox boundaries, failure recovery), which makes post hoc comparison unreliable even when citations are plentiful \citep{Chowa2025From,Kim2025Bridging}. A second is that many benchmarks still under-measure security and cost failures, so orchestration systems can over-optimize for success metrics while remaining fragile to interface-layer attacks, motivating later sections on evaluation protocols and risks \citep{Zhou2026Beyond,Li2024Stride}.

\section{Core Components (Planning + Memory)}


Long-horizon behavior depends on how an agent forms plans and how it maintains usable state over time. Planning and reasoning modules can change action selection, but their benefits are often protocol-dependent: tool budgets, verification access, and environment noise can flip apparent rankings \citep{Hu2025Training,Chen2024Steering}.

Memory mechanisms provide persistence and evidence access, but they also introduce new failure modes when stale or adversarial information enters the loop. The two subsections in this chapter examine these components jointly: planning and reasoning loops as a control policy, and memory and retrieval as a state substrate whose trust assumptions must be explicit for comparisons to remain meaningful \citep{Du2025Memr,Wei2025Memguard}.

\subsection{Planning and reasoning loops}


Results in planning-heavy agent loops are only comparable when the evaluation protocol is explicit: task family, metric, interaction horizon, and budget constraints jointly determine which reasoning strategy is viable. Under loose constraints, long chains of thought or multi-agent deliberation can look strong, whereas under strict budgets, simpler policies or direct tool-based solutions can dominate \citep{Hu2025Evaluating,Ji2024Testing,Zhou2023Navgpt,Hatalis2025Review}. The practical implication is that synthesis should treat protocol context as a first-class variable and compare planning methods within shared constraints rather than across incompatible setups \citep{Hu2025Evaluating,Ji2024Testing}.

Planning and reasoning loops cover a spectrum of mechanisms that decide what to do next and how to justify it. Some approaches emphasize training or fine-tuning a planner, others steer inference-time deliberation, and still others treat planning as a program synthesis problem where tool use or code becomes the dominant reasoning substrate \citep{Hu2025Training,Chen2024Steering,Shi2024Ehragent,Huang2025Surgical}. These choices affect not only success rates but also failure recovery: a loop that can externalize intermediate plans can support verification and correction, but it also increases latency and cost \citep{Zhou2025Reasoning,Cao2025Large,Lu2025Pilotrl}.

A useful contrast is between training-based planners and steering-based planners. Training can bake in long-horizon behaviors and reduce prompt brittleness; in contrast, steering focuses on controlling inference-time trajectories (e.g., preventing overthinking or underthinking) without requiring retraining \citep{Hu2025Training,Chen2024Steering}. In practice, the evaluation protocol decides which contrast matters: when tasks reward concise, tool-backed solutions, steering toward direct coding can outperform elaborate reasoning chains even at 100\% success on some benchmarks \citep{Chen2024Steering,Webb2023Improving}.

Concrete evidence of protocol sensitivity appears in planning benchmarks. A 1.5B-parameter model trained with single-turn GRPO reports strong long-horizon planning performance (70\% success on a complex task planning benchmark), exceeding larger baselines up to 14B parameters under that benchmark's setup \citep{Hu2025Training,Motwani2024Malt}. Such results are informative only when the benchmark defines the horizon, action interface, and success metric clearly; otherwise, the same method can degrade when tool access, budgets, or environment noise change \citep{Hu2025Evaluating,Ji2024Testing}.

Benchmark construction further shapes what "planning" means. USTBench evaluates spatiotemporal reasoning as an urban-agent capability across multiple dimensions (understanding, forecasting, planning, reflection with feedback), illustrating that planning may require structured state rather than free-form reasoning text \citep{Lai2025Ustbench}. This supports a protocol-first comparison: differences in observation structure and action affordances can be more predictive of planning success than differences in the reasoning module alone \citep{Lai2025Ustbench,Hu2025Evaluating,Bai2024Twostep}.

Planning often interacts with tool use and code in ways that undermine naive comparisons. Some tasks that appear to demand sophisticated reasoning can be solved by direct coding, which shifts the relevant question from "reasoning quality" to "whether the loop can select and verify the right tool or program under constraints" \citep{Chen2024Steering,Wang2025Automated,Kiruluta2025Novel}. Accordingly, evaluation should report not only success but also budgets (token budget, cost, and latency), tool access, and verification channels, since these determine whether planning is doing substantive work or merely producing verbose traces \citep{Hu2025Evaluating,Ji2024Testing}.

Memory and retrieval can make planning look better or worse depending on how evidence is supplied to the loop. Systems that treat retrieval as an explicit action (retrieve, reflect, or answer routing) effectively expand the planning action space, which can improve answer quality but also introduce new failure modes when retrieval is noisy or adversarial \citep{Du2025Memr,Yao2022React}. For synthesis, this means planning results should be read together with the memory protocol: what information is accessible, when it is retrieved, and how it is validated \citep{Du2025Memr,Hu2025Evaluating}.

Failure modes in reasoning loops can also be adversarial rather than purely capability-driven. Process-oriented attacks that target reasoning style (e.g., inducing "analysis paralysis" or "cognitive haste") highlight that even when factual content is unchanged, the loop can be manipulated into pathological decision patterns \citep{Zhou2025Reasoning,Zhao2024Lightva}. Such results are a reminder that planning mechanisms can be brittle under targeted perturbations and that evaluation protocols should include robustness checks beyond nominal task success \citep{Zhou2025Reasoning}.

Finally, planning loops are often evaluated in settings that implicitly assume structured knowledge and controllable environments; these assumptions can break in physically grounded or multi-agent collaboration tasks. Empirical studies in physically grounded collaboration expose failure modes that do not appear in purely textual environments, suggesting that planning modules should be stress-tested under interaction and coordination constraints, not only under offline reasoning metrics \citep{Silva2025Agents,Chu2025Bimanual}.

One limitation is that planning comparisons are frequently confounded by unreported protocol details (budgets, tool access, verification), which forces any synthesis to weaken claims rather than guess missing context \citep{Hu2025Evaluating,Ji2024Testing}. Another is that planning is rarely separable from memory and retrieval in long-horizon loops; understanding what is retrieved, when, and under what trust model is essential for interpreting planning results, motivating the next subsection's focus on memory and retrieval mechanisms \citep{Du2025Memr,Wei2025Memguard}.

Planning determines which decisions to make, whereas memory determines what evidence those decisions can condition on over long horizons.

\subsection{Memory and retrieval (RAG)}


Seen through the lens of long-horizon agency, memory is not an optional enhancement but a constraint that reshapes what an agent can reliably do. Finite context windows force agents to decide what to retain, retrieve, and trust, and those decisions can dominate behavior under realistic interaction horizons \citep{Zhu2025Where,Du2025Memr,Yao2025Survey,Zhang2024Survey}. Retrieval policies and trust assumptions therefore determine what evidence the loop can condition on and what failures are diagnosable, so memory mechanisms should be compared under protocols that make those assumptions visible \citep{Hu2025Evaluating,Ji2024Testing,Liu2025Echo}.

Memory mechanisms range from retrieval-augmented prompting to persistent stores that act as long-lived state. Retrieval can be treated as an explicit action (retrieve, reflect, or answer routing), which makes memory access part of the agent loop, while persistent memory can implicitly bias future decisions even when not surfaced as an explicit action \citep{Du2025Memr,Yao2022React,Li2025Graphcodeagent,Liu2023Reason}. These choices change failure recovery: explicit retrieval can be audited and revised, whereas implicit memory can create silent drift unless interfaces expose provenance and trust signals \citep{Zhu2025Where,Wei2025Memory,Yu2026Agentic}.

Concrete systems illustrate how memory control can be operationalized. MemR\$\textasciicircum{}3\$ treats memory retrieval as an autonomous component and introduces a router that chooses among retrieve, reflect, and answer actions, along with an evidence-gap signal intended to optimize answer quality under constraints \citep{Du2025Memr}. This design suggests that memory is best viewed as a policy: when to retrieve, what to retrieve, and how to revise or discard memories as the loop unfolds \citep{Du2025Memr,Sun2025Search}.

Interactive benchmarks show why memory cannot be evaluated in isolation. In ReAct-style loops, decisions are conditioned on a growing interaction trace, and a small number of in-context examples can yield large gains on interactive environments, but only within the specific benchmark protocols that define observation and action semantics \citep{Yao2022React}. As environments shift, the same memory strategy can produce cascading errors, where an early mistake compounds through subsequent decisions \citep{Zhu2025Where,Hu2025Evaluating}.

A useful contrast is between memory systems that prioritize breadth of context and those that prioritize structured, verified evidence. Some work focuses on retrieving larger quantities of potentially relevant context, whereas other work stresses retrieval that is auditable and compatible with downstream verification, making the memory interface itself part of the evaluation protocol \citep{Huang2025Retrieval,Wei2025Memory,Verma2026Active}. In contrast to breadth-first retrieval, evidence-first memory designs treat retrieval as a verifiable action with explicit trust signals, which changes both the error surface and the cost profile of the loop \citep{Hu2025Evaluating,Ye2025Task}.

Error-taxonomy and failure-trajectory datasets make these issues more concrete. AgentErrorTaxonomy and AgentErrorBench classify failures across memory, reflection, planning, and action, and ground error analysis in annotated trajectories drawn from interactive settings such as ALFWorld, GAIA, and WebShop \citep{Zhu2025Where,Chiang2024Llamp}. These resources support protocol-aware comparison by making failure types explicit rather than collapsing them into a single success metric \citep{Zhu2025Where,Ji2024Testing}.

Security concerns are especially salient for memory. If memory is treated as a writable store that future decisions trust, an adversary can inject benign-looking records that later manipulate behavior, turning memory into a persistent attack channel \citep{Wei2025Memguard}. This risk is easy to miss unless the threat model and memory interface are explicitly part of the evaluation protocol (what is writable, what is trusted, and how provenance is exposed) \citep{Wei2025Memguard,Zhu2025Where}.

Defenses therefore need to be evaluated as loop components. A-MemGuard reports large reductions in attack success rates (over 95\%) with minimal utility cost, illustrating that memory security can be studied empirically when the evaluation protocol includes adversarial interactions and clear success and failure definitions \citep{Wei2025Memguard}. The broader implication is that memory mechanisms should be compared not only for nominal task success, but also for robustness under realistic adversarial and distribution-shift conditions \citep{Wei2025Memguard,Zhu2025Where}.

Across recent surveys and component taxonomies, memory and retrieval are increasingly treated as core axes for agents, but the evidence base remains uneven when protocols are underspecified. Survey-style synthesis therefore benefits from combining broad taxonomies with explicit protocol anchors (task family, metric, budget, and tool and memory access assumptions) before drawing cross-paper conclusions \citep{Ge2025Surveya,Du2025Survey,Hu2025Evaluating,Lu2025Youtu}. Without this alignment, cross-paper numeric comparisons of retrieval systems remain fragile and should be described as protocol-contingent rather than universal \citep{Ji2024Testing}.

One limitation is that many papers report memory mechanisms without exposing enough provenance or trust signals to support reproducible comparison, which forces synthesis to focus on qualitative trade-offs \citep{Hu2025Evaluating,Wei2025Memory}. Another is that memory is entangled with security and governance: the same mechanism that enables long-horizon competence can create persistent attack channels, so future benchmarks must evaluate memory under threat models, not only under nominal success metrics \citep{Wei2025Memguard,Zhu2025Where}.

\section{Learning, Adaptation \& Coordination}


Adaptation mechanisms change an agent's behavior based on feedback, shifting the focus from fixed policies to self-updating loops. Such updates can occur through prompt and context optimization and revision cycles, but they raise stability questions when evaluation protocols are underspecified \citep{Zhang2025Agentic,Wu2025Agentic}.

Coordination extends these ideas to multi-agent settings, where role protocols and aggregation rules become part of the method. The subsections in this chapter contrast self-improvement loops with multi-agent coordination patterns, emphasizing how interaction budgets and protocol design determine whether coordination improves robustness or amplifies failure modes \citep{Masters2025Arcane,Jiang2023Large}.

\subsection{Self-improvement and adaptation}


For system builders, the key decision in self-improvement is how much the agent is allowed to change under feedback. Adaptation can make agents more effective in specific environments, but it can also create instability and obscure what is being evaluated, since the policy at the end of an episode is not the policy at the beginning \citep{Van2025Survey,Tao2024Survey,Belle2025Agents}. Update budgets and feedback channels largely determine whether adaptation gains are stable and transferable, so adaptation mechanisms should be treated as part of the evaluation protocol: improvements are interpretable only relative to what information is used for learning, when updates occur, and which constraints (budget, access, threat model) apply \citep{Van2025Survey,Tao2024Survey,Mou2024From}.

Self-improvement and adaptation mechanisms include prompt and context optimization, reflection and revision loops, and learning procedures that update policies before or during deployment. These mechanisms are attractive in long-horizon settings because they can correct recurring failure modes without requiring full retraining, but they also introduce new failure modes such as reward hacking and benchmark overfitting \citep{Van2025Survey,Bilal2025Meta,Zhang2024Affective,Wu2024Federated}. As a result, adaptation claims must be anchored in explicit evaluation settings, including what feedback is available and how much adaptation is allowed \citep{Van2025Survey,Tao2024Survey}.

Context optimization provides a concrete, protocol-aware form of adaptation. ACE reports optimizing contexts both offline (e.g., system prompts) and online (e.g., agent memory), with measured gains across agent and domain-specific benchmarks (e.g., +10.6\% on agent benchmarks and +8.6\% on finance) while reducing adaptation latency and rollout costs under its evaluation setup \citep{Zhang2025Agentic,Liu2025Powered}. Such results are best interpreted as improvements under a specific adaptation protocol, rather than as unconditional superiority of a particular loop design \citep{Zhang2025Agentic,Van2025Survey}.

Self-challenging and self-generated training data highlight a different trade-off. In contrast to one-shot prompt tuning, a self-challenging framework reports over a two-fold improvement for an open model (Llama-3.1-8B-Instruct) on multi-turn tool-use benchmarks using only self-generated training data, suggesting that agents can improve through internal feedback loops when the benchmark protocol supports iterative refinement \citep{Zhou2025Self,Chen2025Grounded}. The caveat is that such gains may be fragile when the evaluation distribution shifts or when the internal critic aligns to benchmark-specific artifacts \citep{Van2025Survey,Bilal2025Meta}.

More general learning frameworks attempt to expand beyond prompt-level tuning by enabling agents to learn and evolve before and during test time, incorporating environment-relevant knowledge for planning and improved cooperation \citep{Li2025Learn,Xia2025Sand}. This style of adaptation blurs the line between "agent design" and "training procedure": what appears as a stronger agent may be a stronger learning protocol under the same interface constraints \citep{Li2025Learn,Van2025Survey}.

A useful contrast is between adaptation that primarily changes the agent's context and adaptation that changes the agent's behavior policy more directly. Context-centered approaches can be faster and cheaper but can be brittle to prompt injection and context drift, whereas more direct learning-based approaches can be more stable but require careful accounting of data, feedback, and compute budgets \citep{Taylor2025Large,Jin2024From,Samaei2025Epidemiqs}. In either case, evaluation should report adaptation budgets and update frequency, since these determine whether the method scales in realistic deployments \citep{Van2025Survey,Tao2024Survey}.

Evaluation protocols for adaptation must also address what is being optimized. Surveys of foundation-model limitations emphasize issues such as hallucinations and limited self-assessment, and suggest that adaptation methods can trade short-term benchmark gains for degraded calibration and interpretability if objectives are underspecified \citep{Van2025Survey,Bilal2025Meta,Bharadwaj2025Omnireflect}. This motivates conservative synthesis: distinguish improvements in task success from improvements in reliability under uncertainty, and avoid collapsing them into a single score \citep{Van2025Survey,Tao2024Survey}.

Security is an especially important axis for self-improvement. Adaptation mechanisms can amplify vulnerabilities by internalizing adversarial signals or by optimizing toward unsafe tool behaviors, so end-to-end security benchmarks for tool-use pipelines are relevant even when their primary focus is risk \citep{Zhang2025Security}. A practical takeaway is that adaptation should be evaluated under threat models that include adversarial inputs and tool-interface attacks, not only under benign task success \citep{Zhang2025Security}.

Across the literature, adaptation also interacts with multi-agent settings and system architecture. Agent evolution can change communication policies and coordination dynamics, while architectural constraints (tool protocols, memory interfaces) bound what adaptation can safely modify \citep{Wei2026Agentic,Chen2025Largea}. For synthesis, this suggests treating adaptation as a layer that sits on top of interface and protocol choices: without stable interfaces, adaptation gains are difficult to attribute and difficult to transfer \citep{Jiang2025Agentic,Van2025Survey}.
Across the literature, adaptation also interacts with multi-agent settings and system architecture. Agent evolution can change communication policies and coordination dynamics, while architectural constraints (tool protocols, memory interfaces) bound what adaptation can safely modify \citep{Wei2026Agentic,Chen2025Largea}. For synthesis, a practical reading is to treat adaptation as a layer that sits on top of interface and protocol choices: without stable interfaces, adaptation gains are difficult to attribute and difficult to transfer \citep{Jiang2025Agentic,Van2025Survey}.

One limitation is that many adaptation papers report gains without sufficiently specifying update budgets and feedback channels, which makes cross-paper comparison fragile and encourages overclaiming \citep{Van2025Survey,Tao2024Survey}. Another is that adaptation introduces governance questions: agents that learn online can change risk profiles after deployment, so evaluations should include both capability and safety metrics under realistic constraints \citep{Zhang2025Security,Van2025Survey}.

As agents adapt over time, multi-agent systems introduce an additional dimension of change: coordination protocols that distribute decision-making and verification across roles.

\subsection{Multi-agent coordination}


A sharp contrast in agent design is between single-agent loops that internalize all decisions, whereas multi-agent systems distribute reasoning, tool use, and verification across specialized roles. Distribution can improve coverage and robustness, but it also introduces coordination overhead and new failure modes, so evaluation must specify interaction budgets and aggregation protocols rather than reporting a single success number \citep{Jiang2023Large,Aratchige2025Llms,Xu2025Autonomous,Xu2023Magic}. Role protocols and cost models largely determine whether coordination gains are comparable across papers, so multi-agent results are only comparable when communication channels and accounting assumptions are explicit \citep{Jiang2023Large,Aratchige2025Llms,Xu2023Towards,Rouzrokh2025Lattereview}.

Multi-agent coordination mechanisms include role specialization (planner, verifier, executor), communication protocols (messages, shared memory, structured schemas), and aggregation rules (vote, debate, referee). These choices reshape the effective action space: coordination can add verification capacity but can also amplify cascading failures when agents reinforce each other's errors \citep{Chen2024Llmarena,Becker2025Mallm,Le2024Multi,Wu2025Multi,Zahedifar2025Agent}. As a result, "better coordination" should be stated as a protocol-contingent claim tied to interaction budgets and observable artifacts (e.g., what evidence verifiers can access) \citep{Aratchige2025Llms,Chen2025Schema,Ye2025Cognipair}.

Rubric-based evaluation provides one way to make coordination protocols explicit. ARCANE is evaluated on tasks requiring multi-step reasoning and tool use using a corpus of 219 labeled rubrics derived from a benchmark, which helps separate coordination quality from superficial success metrics \citep{Masters2025Arcane,Trirat2024Automl}. This style of evaluation is valuable because it makes the grading protocol legible, enabling more meaningful comparisons of role design and aggregation rules \citep{Masters2025Arcane}.

Coordination is also tested in collaborative settings where agents must share observations, negotiate roles, and recover from each other's errors. Such settings illustrate that protocol details (communication latency, observation sharing, and interaction budgets) can dominate outcomes, so coordination mechanisms should be compared under matched observability and communication constraints, not only under nominal task labels \citep{Li2025Learn,Jiang2023Large,Chen2024Solving}.

Domain-specific multi-agent systems further illustrate how protocol assumptions become part of the method. In a drug-discovery setting, a multi-agent approach reports large improvements over a non-reasoning multi-agent baseline (e.g., a 45\% F1 gain on a kinase inhibitor dataset), suggesting that coordination can matter even when the underlying model family is similar \citep{Inoue2024Drugagent}. The transferable lesson is not the number itself, but the requirement that datasets, metrics, and interaction protocols be explicit before coordination claims are generalized \citep{Inoue2024Drugagent,Chen2025Schema}.

Evaluation settings can also involve multi-agent scoring and assessment. AutoSCORE is evaluated on multiple datasets from the ASAP benchmark with both proprietary and open-source models, illustrating that the evaluation protocol may include grading and judgment components rather than purely environment success \citep{Wang2025Autoscore}. This makes coordination questions salient: when agents include judges or critics, the protocol must specify how judgments are aggregated and what signals are available to avoid hidden leakage or evaluator drift \citep{Wang2025Autoscore,Masters2025Arcane}.

Security verification offers a complementary coordination pattern: centralized supervision with delegated execution. MARVEL uses a supervisor agent that derives security policies and delegates validation tasks to executor agents, reflecting a common architectural motif where coordination is organized around a central planner with specialized workers \citep{Collini2025Marvel}. Such hierarchies can improve tractability, but they also concentrate failure modes in the supervisor's policy and require explicit accounting of what documentation and evidence each role can access \citep{Collini2025Marvel,Chen2025Schema}.

A useful architectural contrast is centralized supervision versus more decentralized coordination. Centralized designs can simplify protocol design and reduce message overhead; in contrast, decentralized designs can increase redundancy and verification at the cost of coordination complexity \citep{Jiang2023Large,Becker2025Mallm}. In both cases, reporting should include interaction budgets and cost and latency constraints, since coordination gains can disappear when communication is expensive or time-limited \citep{Aratchige2025Llms,Becker2025Mallm}.

Across these studies, multi-agent systems tend to trade raw capability for controllability and verification. Systems that embed critics, referees, or specialized executors can surface errors that a single-agent loop would miss, but they can also create new pathways for error amplification and increased costs if protocols are not carefully bounded \citep{Masters2025Arcane,Chen2024Llmarena,Wang2025Autoscore}. For synthesis, the conservative move is to compare coordination mechanisms within shared protocol envelopes and to treat cross-paper differences in interaction budgets as the primary confounder \citep{Aratchige2025Llms,Jiang2023Large}.

One limitation is that coordination methods often face scalability and real-time response constraints, which means that improvements observed under generous interaction budgets may not transfer to deployment settings \citep{Aratchige2025Llms,Becker2025Mallm}. Another is that coordination interacts with learning and adaptation: frameworks that allow agents to evolve before or during test time can change communication policies, making it essential to specify what changes are permitted and when \citep{Li2025Learn,Van2025Survey}.

\section{Evaluation \& Risks}


Evaluation protocols determine which claims about agents are interpretable. Benchmark suites span diverse tasks and metrics, but protocol drift (tool access, budgets, environment versions, reporting) can dominate outcomes and undermine synthesis when assumptions are implicit \citep{Hu2025Survey,Zhang2026Evoroute}.

Risk surfaces are tightly coupled to these protocols: tool use expands the attack surface, and safety outcomes depend on threat models and interface boundaries. The two subsections in this chapter therefore cover (i) benchmarks and protocol anchors for comparability and (ii) safety, security, and governance considerations that must be reflected in evaluation designs \citep{Shang2024Agentsquare,Zhang2025Security}.

\subsection{Benchmarks and evaluation protocols}


A practical decision for evaluating agents is what to treat as "success" and what constraints to enforce while measuring it. Benchmarks vary not only in task families and metrics, but also in budgets, tool access assumptions, and environment stability, so protocols determine whether results are comparable or merely adjacent anecdotes \citep{Chang2023Survey,Huang2024Survey,Shang2024Agentsquare,Kim2026Beyond,Chen2025Towards}. In particular, nominally similar tasks can differ in tool permissions, logging granularity, or stopping criteria, which can change both reported success and the distribution of failures \citep{Huang2024Survey}. The central takeaway is that evaluation protocols should be written as explicit contracts (task + metric + constraint), and surveys should synthesize evidence primarily within shared contracts rather than across incompatible ones \citep{Ji2025Taxonomy,Zhang2025Generalizability}.

Benchmark suites increasingly span heterogeneous domains: web interaction, embodied control, tool use, code, and multi-agent coordination. This breadth is valuable for coverage but also creates protocol drift: different tasks implicitly assume different observability, action interfaces, and cost models, which can dominate outcomes for long-horizon loops \citep{Chang2023Survey,Zhang2026Evoroute,Zhang2024Large,Qi2024Large}. As a result, evaluation papers that expose protocol assumptions can be as important for synthesis as new agent architectures \citep{Van2025Survey,Ji2025Taxonomy,Tang2025Empowering,Agrawal2025Language}.

Concrete examples illustrate the sensitivity to model access and infrastructure. DataSciBench reports that API-based models outperform open-source models on its metrics, whereas a strong open model achieves the highest score among open-source baselines under that benchmark's setup \citep{Zhang2025Datascibench}. Such results are informative, but only when the evaluation discloses what the API access implies (tooling, cost, latency) and how those constraints affect agent behavior relative to offline models \citep{Zhang2025Generalizability,Chang2023Survey}.

Cost and latency constraints are themselves evaluation axes. EvoRoute reports that, when integrated into off-the-shelf agentic systems, it can sustain or improve performance while reducing execution cost by up to 80\% and latency by over 70\% on agentic benchmarks such as GAIA and BrowseComp+ \citep{Zhang2026Evoroute}. This underscores a key protocol point: reporting budgets and time constraints is not optional, because "better" agents can be strictly worse once costs are normalized \citep{Zhang2026Evoroute,Huang2024Survey}.

Security and robustness benchmarks make protocol design even more explicit by including adversarial interactions. Progent reports reducing attack success rates to 0\% while preserving utility and speed across multiple agent use cases and benchmarks, illustrating how evaluation can incorporate threat models and mitigation efficacy into the same measurement loop \citep{Shi2025Progent}. These settings help prevent a common failure mode in capability evaluation: declaring success on benign tasks while ignoring interface-layer vulnerabilities \citep{Zhang2025Security,Gasmi2025Bridging}.

Efficiency and long-context constraints are also becoming first-class evaluation targets. ACBench spans multiple tasks, capabilities, and model variants (including compression techniques), emphasizing that evaluation should account for how model size, pruning and quantization, and context management interact with agent loop performance \citep{Dong2025Compressed}. This aligns with the broader observation that many agent behaviors are constrained by context budgets and memory policies rather than by raw reasoning quality \citep{Zhu2025Where,Zhang2026Evoroute}.

Simulated environments and agent behavior suites provide another evaluation route. Simulators can stress multi-step interaction patterns and reveal systematic shortcomings in evaluation methods, such as constrained evaluation coverage, benchmark vulnerability, and unobjective metrics \citep{Lin2023Agentsims}. Domain-specific simulators and evaluations can complement broad suites, but they also require careful disclosure of environment assumptions and versioning to avoid irreproducible comparisons \citep{Seo2025Simuhome,Huang2024Survey}.

Several surveys of evaluation practice emphasize recurring pitfalls: benchmark leakage, implicit tool access assumptions, and metrics that do not capture failure recovery or safety-critical behavior. These issues make it possible for systems to improve scores without improving robustness, especially when evaluation protocols are underspecified or easily gamed \citep{Huang2024Survey,Lin2023Agentsims,Rahman2025Hallucination}. For synthesis, this motivates using evaluation papers to identify which results should be treated as provisional rather than definitive \citep{Zhang2025Generalizability,Chang2023Survey,Wu2025Lessons}.

Across benchmark suites, protocol papers, and surveys, a consistent theme is that comparability requires explicit protocol anchors. When benchmarks share tasks and metrics, head-to-head comparison is meaningful; whereas when they differ, the correct synthesis is to describe how protocol choices reshape trade-offs and failure modes rather than declaring a single method superior \citep{Shang2024Agentsquare,Ji2025Taxonomy,Van2025Survey,Zhu2025Evolutionary}. This is also where reader-facing tables can help: they compress which protocol constraints matter for each benchmark family, reducing the temptation to treat all scores as commensurate \citep{Zhang2026Evoroute,Wang2025Autoscore}.

One limitation is that protocol reporting is still inconsistent, especially for tool access, costs, and environment versions, which forces surveys to weaken claims to avoid guessing missing context \citep{Huang2024Survey,Zhang2025Generalizability}. Another is that benchmarks that focus on capability without integrating threat models can misrepresent deployability, motivating the next subsection's focus on safety, security, and governance as integral to evaluation design \citep{Zhang2025Security,Shi2025Progent}.

Protocol contracts clarify what success means, but they also determine what risks are exercised, so the next step is to examine safety and governance under explicit threat models.

\subsection{Safety, security, and governance}


From the perspective of deployed agents, safety and security are not add-on concerns; they are consequences of interface choices and protocol assumptions. Tool use expands the attack surface through prompt injection, data exfiltration, and privilege misuse, and tool protocols and permission models can largely determine the effective risk profile even for the same underlying agent loop \citep{Zhang2025Security,Gasmi2025Bridging}. Governance therefore requires protocol-aware evaluation: threat models, system boundaries, and monitoring assumptions must be represented in the evaluation contract, not deferred to informal caveats \citep{Weng2025Bridgescope,Zhang2025Security}.

Threat models for tool-using agents are shaped by the agent-tool communication loop. Attacks can target tool descriptions, tool arguments, retrieval channels, or error-handling logic, exploiting the fact that agents treat tool outputs as part of their observation stream \citep{Zhang2025Security,Gasmi2025Bridging}. This implies that "robustness" cannot be assessed without specifying what tools exist, what the agent is allowed to call, and what information is trusted after a call \citep{Gasmi2025Bridging,Weng2025Bridgescope}.

Comparative evaluations illustrate that protocol choice changes both capability and vulnerability. One analysis reports higher overall attack success rates under function calling than under MCP (73.5\% vs 62.59\%), while also distinguishing system-centric and LLM-centric exposure patterns under these protocols \citep{Gasmi2025Bridging}. Such numbers are only interpretable when the protocol defines attack channels and system boundaries, reinforcing that security results must be read as protocol-contingent rather than universal \citep{Gasmi2025Bridging,Zhang2025Security}.

Taxonomies make this dependence explicit. MSB contributes an attack taxonomy with a dozen attack types (including name collision and tool-description prompt injection variants) and evaluates resistance throughout the tool-use pipeline, covering planning, invocation, and response handling \citep{Zhang2025Security}. This pipeline-level framing is important because failures often arise from interactions among components rather than from a single prompt vulnerability \citep{Zhang2025Security,Plaat2025Agentic}.

Defensive systems highlight that mitigation can change both security and system efficiency under realistic workflows. BridgeScope reports enabling agents to operate databases more effectively while reducing token usage by up to 80\% through improved security awareness, illustrating that protocol-aware defenses can trade small interface restrictions for large safety and efficiency gains \citep{Weng2025Bridgescope}. For synthesis, such results should be tied to the workflow protocol (what queries and tools are allowed, what logging exists) rather than treated as generic "agent safety" improvements \citep{Weng2025Bridgescope,Gasmi2025Bridging}.

End-to-end defenses can also be benchmarked directly. Progent reports reducing attack success rates to 0\% across multiple agent use cases and benchmarks while preserving utility and speed, suggesting that defenses can be evaluated under comparable protocols instead of relying on ad hoc red teaming anecdotes \citep{Shi2025Progent}. This style of benchmark-driven security evaluation is essential for governance because it supports reproducible, testable claims about mitigations \citep{Shi2025Progent,Zhang2025Security}.

Monitoring and cost-aware attacks expose additional governance constraints. Some attacks operate at the tool layer under the guise of correct task completion, inflating token usage, monetary cost, and energy consumption, which can be as damaging as capability compromise in deployed systems \citep{Mo2025Attractive,Gao2025Radar}. These failure modes motivate reporting and enforcing budget policies and detection mechanisms as part of the protocol, not as optional operational folklore \citep{Bonagiri2025Check,Gao2025Radar}.

In contrast to threat models that focus only on correctness, governance-oriented evaluations often require monitoring and auditing: which actions were taken, under what permissions, and what signals a system used to decide that an interaction remained safe. Monitoring systems and checklists for agent operations provide concrete guidance on how to instrument such protocols so violations are detectable rather than implicit \citep{Wang2025Agentvigil,Bonagiri2025Check}.

Governance considerations extend beyond technical mitigations to domain and compliance constraints. Surveys of domain deployments (e.g., agents in specialized professional workflows) emphasize that accountability, auditability, and policy boundaries are often the limiting factors, even when nominal task success is achievable \citep{Liu2026Agents,Han2025Large}. This pattern indicates that evaluation protocols for governance should include traceability and controllability metrics, not just task success \citep{Plaat2025Agentic,Zhang2025Generalizability}.

Across security benchmarks and defensive systems, a consistent synthesis is that risk is interface-dependent. Protocols that expose more powerful actions without corresponding observability and guardrails can increase both capability and vulnerability, whereas stricter protocols can improve reliability at the cost of expressivity \citep{Gasmi2025Bridging,Zhang2025Security,Weng2025Bridgescope}. As a result, governance arguments should be framed as design trade-offs that depend on explicit threat models and constraints, not as generic prescriptions for "safe agents" \citep{Zhang2025Security,Plaat2025Agentic}.

One limitation is that many security evaluations are still fragmented across incompatible benchmarks and tool stacks, making it difficult to aggregate evidence without strong protocol alignment \citep{Zhang2025Security,Gasmi2025Bridging}. Another is that governance requires coupling security evaluation with the broader evaluation ecosystem: defenses must be assessed under the same budgets, tool access assumptions, and environment constraints as capability claims, or else improvements will not transfer to deployment \citep{Shi2025Progent,Weng2025Bridgescope}.

\section{Discussion}


One cross-cutting implication is that many "agent improvements" are inseparable from interface assumptions. Changing a tool protocol, permission model, or observation channel can shift not only success rates but also which failure modes are even observable, so comparisons that omit these details risk overclaiming generality \citep{Zhang2025Tool,Chowa2025From,Kim2025Bridging,Zou2025Based}. A practical reading is to treat interface contracts as part of the method, not as a peripheral implementation detail \citep{Lumer2025Memtool,Liu2025Mcpagentbench}.

A second implication concerns evaluation drift. Benchmark suites cover increasingly diverse domains, but task, metric, and budget choices vary across papers, and even small protocol differences can dominate outcomes for long-horizon loops \citep{Hu2025Survey,Shang2024Agentsquare,Zhang2026Evoroute}. Where protocol context is missing, the safest synthesis is comparative but conservative: emphasize patterns of trade-offs and failure modes, and downgrade claims that depend on unreported budgets or tool access \citep{Hu2025Evaluating,Ji2024Testing,Zhang2025Generalizability}.

Risk surfaces also scale with capability. Tool use expands the attack surface through prompt injection and unintended tool behaviors, and mitigation often depends on the same interface decisions that govern capability and evaluation \citep{Zhang2025Security,Gasmi2025Bridging,Wei2025Memguard}. This suggests that safety and evaluation should be co-designed: threat models and governance constraints need to be represented in the evaluation protocol, not deferred to a separate checklist \citep{Weng2025Bridgescope,Zou2025Based}.

Looking forward, progress is likely to depend on making protocols more explicit and more comparable: standardizing what is reported (tool access, budgets, environment versions), supporting ablation-friendly evaluations for loop components (planning vs memory vs orchestration), and documenting failure modes in a way that enables cross-paper synthesis \citep{Hu2025Survey,Zhang2025Datascibench,Cui2025Toward}. These practices are not mere hygiene; they determine whether evidence accumulates across studies or remains siloed by incompatible assumptions \citep{Zhang2025Generalizability,Du2025Survey}.

\section{Conclusion}


LLM agents can be understood as closed-loop systems whose apparent capabilities depend as much on interface contracts and protocol constraints as on model quality \citep{Yao2022React,Luo2025Universe,Zhou2026Beyond}. Reading the literature through this lens clarifies why results often fail to transfer: tool access, budgets, and environment assumptions change the meaning of "success" and the visibility of failure modes \citep{Hu2025Survey,Shang2024Agentsquare,Zhang2026Evoroute}.

The synthesis in this paper emphasizes protocol-aware contrasts across four lenses: foundations and interfaces, planning and memory components, adaptation and coordination mechanisms, and evaluation and risks. Across these lenses, the most reusable takeaways are rarely single system designs; they are patterns of trade-offs and failure modes that persist across benchmarks when assumptions are made explicit \citep{Hu2025Evaluating,Ji2024Testing,Zhang2025Generalizability}.

A final implication is that evaluation and governance are coupled for tool-using agents. Security and safety outcomes hinge on the same interface decisions that drive capability, so threat models and protocol constraints should be treated as first-class evaluation objects rather than post hoc caveats \citep{Zhang2025Security,Gasmi2025Bridging,Wei2025Memguard}.

\appendix

\section{Tables}



\begin{table}[t]
\centering
\small
\setlength{\tabcolsep}{4pt}
\renewcommand{\arraystretch}{1.15}
\caption{Representative agent approaches and their interface and loop assumptions (selected examples).}
\label{tab:a1}
\begin{tabularx}{\linewidth}{YYYl}
\toprule
Approach / work family & Core idea & Interface / loop assumption & Key refs \\
\midrule
Reasoning-action interleaving & interleave reasoning traces with environment and tool actions & closed-loop, multi-step interaction where actions become context for later decisions & \citep{Yao2022React} \\
Memory retrieval as an agent module & treat memory retrieval as an autonomous component & retrieval decisions are part of the agent loop; memory access policy affects comparability & \citep{Du2025Memr} \\
Interface hardening (memory and tool guardrails) & reduce attack success by constraining memory and tool interactions & adversarial inputs are in-scope; guardrails are evaluated as part of the loop & \citep{Wei2025Memguard} \\
Tool protocol orchestration & compare tool modes under a protocolized interface & explicit tool protocol; repeated interactions surface reliability limits & \citep{Lumer2025Memtool} \\
Benchmark-driven agent framework & evaluate agents across diverse settings (web, embodied, tool use, games) & comparability depends on consistent environments and tool-access assumptions & \citep{Shang2024Agentsquare} \\
Benchmark landscape synthesis & aggregate large benchmark landscapes to track evaluation drift & protocols vary by task, metric, and budget; alignment is needed for cross-paper comparison & \citep{Hu2025Survey} \\
Context optimization for agents & optimize contexts offline and online as an adaptation mechanism & prompt and context are part of controllable state; stability depends on protocol constraints & \citep{Zhang2025Agentic} \\
Security benchmarking for tool protocols & characterize attack surfaces and vulnerabilities in tool interfaces & threat model and system and tool boundary determine what "attack success" means & \citep{Zhang2025Security,Gasmi2025Bridging} \\
\bottomrule
\end{tabularx}
\end{table}



\begin{table}[t]
\centering
\small
\setlength{\tabcolsep}{4pt}
\renewcommand{\arraystretch}{1.15}
\caption{Evaluation settings and protocol anchors for LLM agents (examples).}
\label{tab:a2}
\begin{tabularx}{\linewidth}{YYYl}
\toprule
Benchmark / setting & Task family + metric (example) & Key protocol constraints (what must be reported) & Key refs \\
\midrule
ALFWorld; WebShop & interactive decision making; task success & step budget; tool access; environment stochasticity; cost and latency & \citep{Yao2022React} \\
ScaleMCP & tool-protocol interactions; success over repeated sessions & consecutive user interactions; model suite; protocol versioning; tool availability & \citep{Lumer2025Memtool} \\
MCP Security Benchmark (MSB) & attacks on tool protocols; attack success rate & threat model; injection channel; system and tool boundary; mitigations & \citep{Zhang2025Security,Gasmi2025Bridging} \\
GAIA; BrowseComp+ & agentic browsing and composite tasks; success rate & tool budget; time and step limits; evaluation rubric; access constraints & \citep{Zhang2026Evoroute} \\
ASAP benchmark (AutoSCORE) & scoring tasks; accuracy and quality metrics & model choice (open vs proprietary); prompt format; dataset splits & \citep{Wang2025Autoscore} \\
Rubric-based multi-agent evaluation (ARCANE) & multi-agent tasks; rubric-based scoring & role protocol; aggregation and voting; interaction budget & \citep{Masters2025Arcane} \\
\bottomrule
\end{tabularx}
\end{table}

\bibliographystyle{plainnat}
\bibliography{../citations/ref}

\end{document}
