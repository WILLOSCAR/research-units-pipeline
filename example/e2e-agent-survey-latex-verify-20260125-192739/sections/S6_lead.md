Evaluation protocols determine which claims about agents are interpretable. Benchmark suites span diverse tasks and metrics, but protocol drift (tool access, budgets, environment versions, reporting) can dominate outcomes and undermine synthesis when assumptions are implicit [@Hu2025Survey; @Zhang2026Evoroute].

Risk surfaces are tightly coupled to these protocols: tool use expands the attack surface, and safety outcomes depend on threat models and interface boundaries. The two subsections in this chapter therefore cover (i) benchmarks and protocol anchors for comparability and (ii) safety, security, and governance considerations that must be reflected in evaluation designs [@Shang2024Agentsquare; @Zhang2025Security].

