A practical decision for evaluating agents is what to treat as "success" and what constraints to enforce while measuring it. Benchmarks vary not only in task families and metrics, but also in budgets, tool access assumptions, and environment stability, so protocols determine whether results are comparable or merely adjacent anecdotes [@Chang2023Survey; @Huang2024Survey; @Shang2024Agentsquare]. In particular, nominally similar tasks can differ in tool permissions, logging granularity, or stopping criteria, which can change both reported success and the distribution of failures [@Huang2024Survey]. The central takeaway is that evaluation protocols should be written as explicit contracts (task + metric + constraint), and surveys should synthesize evidence primarily within shared contracts rather than across incompatible ones [@Ji2025Taxonomy; @Zhang2025Generalizability].

Benchmark suites increasingly span heterogeneous domains: web interaction, embodied control, tool use, code, and multi-agent coordination. This breadth is valuable for coverage but also creates protocol drift: different tasks implicitly assume different observability, action interfaces, and cost models, which can dominate outcomes for long-horizon loops [@Chang2023Survey; @Zhang2026Evoroute]. As a result, evaluation papers that expose protocol assumptions can be as important for synthesis as new agent architectures [@Van2025Survey; @Ji2025Taxonomy].

Concrete examples illustrate the sensitivity to model access and infrastructure. DataSciBench reports that API-based models outperform open-source models on its metrics, whereas a strong open model achieves the highest score among open-source baselines under that benchmark's setup [@Zhang2025Datascibench]. Such results are informative, but only when the evaluation discloses what the API access implies (tooling, cost, latency) and how those constraints affect agent behavior relative to offline models [@Zhang2025Generalizability; @Chang2023Survey].

Cost and latency constraints are themselves evaluation axes. EvoRoute reports that, when integrated into off-the-shelf agentic systems, it can sustain or improve performance while reducing execution cost by up to 80% and latency by over 70% on agentic benchmarks such as GAIA and BrowseComp+ [@Zhang2026Evoroute]. This underscores a key protocol point: reporting budgets and time constraints is not optional, because "better" agents can be strictly worse once costs are normalized [@Zhang2026Evoroute; @Huang2024Survey].

Security and robustness benchmarks make protocol design even more explicit by including adversarial interactions. Progent reports reducing attack success rates to 0% while preserving utility and speed across multiple agent use cases and benchmarks, illustrating how evaluation can incorporate threat models and mitigation efficacy into the same measurement loop [@Shi2025Progent]. These settings help prevent a common failure mode in capability evaluation: declaring success on benign tasks while ignoring interface-layer vulnerabilities [@Zhang2025Security; @Gasmi2025Bridging].

Efficiency and long-context constraints are also becoming first-class evaluation targets. ACBench spans multiple tasks, capabilities, and model variants (including compression techniques), emphasizing that evaluation should account for how model size, pruning and quantization, and context management interact with agent loop performance [@Dong2025Compressed]. This aligns with the broader observation that many agent behaviors are constrained by context budgets and memory policies rather than by raw reasoning quality [@Zhu2025Where; @Zhang2026Evoroute].

Simulated environments and agent behavior suites provide another evaluation route. Simulators can stress multi-step interaction patterns and reveal systematic shortcomings in evaluation methods, such as constrained evaluation coverage, benchmark vulnerability, and unobjective metrics [@Lin2023Agentsims]. Domain-specific simulators and evaluations can complement broad suites, but they also require careful disclosure of environment assumptions and versioning to avoid irreproducible comparisons [@Seo2025Simuhome; @Huang2024Survey].

Several surveys of evaluation practice emphasize recurring pitfalls: benchmark leakage, implicit tool access assumptions, and metrics that do not capture failure recovery or safety-critical behavior. These issues make it possible for systems to improve scores without improving robustness, especially when evaluation protocols are underspecified or easily gamed [@Huang2024Survey; @Lin2023Agentsims]. For synthesis, this motivates using evaluation papers to identify which results should be treated as provisional rather than definitive [@Zhang2025Generalizability; @Chang2023Survey].

Across benchmark suites, protocol papers, and surveys, a consistent theme is that comparability requires explicit protocol anchors. When benchmarks share tasks and metrics, head-to-head comparison is meaningful; whereas when they differ, the correct synthesis is to describe how protocol choices reshape trade-offs and failure modes rather than declaring a single method superior [@Shang2024Agentsquare; @Ji2025Taxonomy; @Van2025Survey]. This is also where reader-facing tables can help: they compress which protocol constraints matter for each benchmark family, reducing the temptation to treat all scores as commensurate [@Zhang2026Evoroute; @Wang2025Autoscore].

One limitation is that protocol reporting is still inconsistent, especially for tool access, costs, and environment versions, which forces surveys to weaken claims to avoid guessing missing context [@Huang2024Survey; @Zhang2025Generalizability]. Another is that benchmarks that focus on capability without integrating threat models can misrepresent deployability, motivating the next subsection's focus on safety, security, and governance as integral to evaluation design [@Zhang2025Security; @Shi2025Progent].
