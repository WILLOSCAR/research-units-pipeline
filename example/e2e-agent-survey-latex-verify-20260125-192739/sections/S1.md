Tool-augmented large language models are increasingly deployed as agents that must make sequences of decisions under partial observability, cost constraints, and unreliable tool execution [@Yao2022React; @Luo2025Universe; @Zhou2026Beyond; @Wu2025Agentic; @Cheng2025Your]. In such settings, errors compound: a single mistaken tool call can derail downstream reasoning, while aggressive exploration can create new safety and security surfaces [@Wei2025Memguard; @Zhang2025Security; @Gasmi2025Bridging].

A useful starting point is to treat an LLM agent as a closed-loop system: it maintains an internal state (explicit or implicit), selects actions, observes outcomes, and updates its state in response [@Chen2024Architectural; @Hu2023Avis; @Jiang2024Agent; @Zhou2024Large; @Silva2025Agents]. This perspective separates agentic behavior from single-turn tool use by focusing on the loop structure and the assumptions that make the loop stable (e.g., observability, resetability, and action semantics) [@Li2023Modelscope; @Hu2025Evaluating].

Interface contracts largely determine what an agent can do and what a result means: schemas and orchestration policies constrain the action space, while permissions and sandboxing determine which actions are feasible at deployment time [@Zhang2025Tool; @Liu2025Mcpagentbench; @Jia2025Autotool; @Chowa2025From; @Li2025Dissonances]. Seemingly small interface choices can change failure modes (silent tool misfires, name collisions, or unintended capability escalation), which in turn changes which evaluation claims are interpretable [@Kim2025Bridging; @Zou2025Based].

Within the loop, core components such as planning, reasoning, and memory retrieval mediate long-horizon behavior, but they interact with protocol constraints in non-obvious ways [@Hu2025Training; @Chen2024Steering; @Du2025Memr; @Sun2025Search; @Zhou2025Reasoning]. Planning modules that look strong under generous tool access may collapse under stricter budgets or noisier tools, whereas memory policies can trade short-term success for robustness against prompt and data contamination [@Wei2025Memguard; @Shen2025Feat].

Evaluation remains a central bottleneck. Agent benchmarks vary widely in task families, success metrics, tool access assumptions, and reporting practices, and such heterogeneity can make head-to-head comparisons fragile unless protocols are normalized [@Hu2025Survey; @Shang2024Agentsquare; @Zhang2026Evoroute; @Hu2025Evaluating; @Ji2024Testing]. As a result, surveys that primarily list systems without making protocol context explicit often understate uncertainty and overstate generality [@Zhang2025Generalizability; @Cui2025Toward].

Risks are inseparable from design choices. Tool interfaces expand the attack surface through prompt injection, data exfiltration, and privilege misuse, and defenses that work for one interface contract may not transfer to another [@Zhang2025Security; @Gasmi2025Bridging; @Wei2025Memguard; @Weng2025Bridgescope; @Zhang2025Agentic]. Governance and safety therefore depend on making assumptions explicit (threat models, tool boundaries, monitoring) rather than treating agent safety as a monolithic property [@Zou2025Based; @Kim2025Bridging].

We compiled this survey by retrieving and deduplicating 1800 arXiv records for LLM-agent and tool-use queries (no strict time-window filter), then selecting a 300-paper core set for synthesis. Evidence is primarily abstract-level: claims are written conservatively when protocol details are missing, and quantitative statements are only used when minimal task, metric, and constraint context can be traced to the cited work [@Hu2025Survey; @Du2025Survey].

The remainder of the paper connects systems to the assumptions that drive their reported behavior. We first cover foundations and interfaces (agent loops, tool protocols), then planning and memory components, then adaptation and coordination mechanisms, and finally evaluation and risk surfaces. Appendix tables summarize representative approaches and protocol anchors to support quick cross-section comparison [@Lumer2025Memtool; @Masters2025Arcane; @Wang2025Autoscore; @Wei2026Agentic; @Abbineni2025Muallm].
