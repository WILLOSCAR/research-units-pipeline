For system builders, the crux of tool interface design is choosing how much expressivity to expose while keeping behavior controllable and verifiable. Richer interfaces can expand the action space, but they also increase the burden of protocol specification (schemas, permissions, observability) and widen the gap between what is evaluated and what is deployable [@Zhang2025Tool; @Liu2025Mcpagentbench]. Interface contracts largely determine what can be compared and what tool-use claims mean; meaningful comparisons therefore require explicit tool-access assumptions and reporting conventions rather than generic "tool use" narratives [@Chowa2025From; @Kim2025Bridging].

Tool interfaces range from free-form natural-language tool descriptions to structured function signatures and protocolized multi-tool stacks. As interfaces become more structured, they enable stronger verification and orchestration policies (routing, retries, argument validation), but they also introduce new brittleness: schema mismatches and implicit permissions can silently change the agent's effective action space [@Li2025Dissonances; @Zhang2025Tool]. This makes interface design inseparable from evaluation: the same model can look strong or weak depending on which tool errors are detectable and recoverable under the benchmark protocol [@Zhou2025Self; @Kim2025Bridging].

Concrete protocolized systems illustrate how evaluation must name the interface details. MemTool evaluates multiple tool modes across 13+ LLMs on the ScaleMCP benchmark, using long interaction traces (100+ consecutive user interactions) and metrics such as tool removal ratios and task completion accuracy [@Lumer2025Memtool]. Such settings are useful precisely because they stress long-horizon orchestration behavior and reveal failure modes that short-horizon tasks can hide [@Lumer2025Memtool; @Gao2025Radar].

Complementing system-driven evaluations, benchmark and dataset audits provide a map of what is actually being measured. An analysis of dozens of publicly available datasets highlights wide variation in task families and assessment protocols for LLM agents, suggesting that many comparisons are dominated by protocol mismatch rather than method differences [@Chowa2025From]. This motivates a protocol-first reading: before comparing orchestration mechanisms, one should ask whether benchmarks share the same tool access assumptions and scoring rules [@Chowa2025From; @Kim2025Bridging].

A useful contrast is between protocol-first tool stacks and ad hoc tool calling. Protocolized interfaces can make tool availability and permissions explicit (which supports reproducibility), whereas ad hoc interfaces often leave such details implicit and encourage "best-effort" behavior that is hard to audit [@Liu2025Mcpagentbench; @Zhang2025Tool]. In practice, the right comparison is therefore not "tool use vs no tool use" but "which interface constraints are enforced, and what failure recovery is allowed under the protocol" [@Lumer2025Memtool; @Chowa2025From].

Retrieval-augmented orchestration provides another contrast class. In contrast to ad hoc tool calling, systems such as AvaTaR exploit retrieval and tool selection under a fixed interface contract, reporting gains on retrieval-style settings (e.g., Hit@1 improvements on retrieval datasets) within that evaluation setup [@Wu2024Avatar]. These results are only interpretable when the retrieval protocol is pinned down (candidate pool, metric definition, tool access), which again reinforces the need for protocol-aware reporting in the interface layer [@Wu2024Avatar; @Kim2025Bridging].

Classic reasoning-action interleaving remains a useful baseline for tool interface discussions because it exposes how interface constraints change loop behavior. On interactive decision-making benchmarks (ALFWorld and WebShop), ReAct reports large absolute success gains over imitation and reinforcement baselines under a few-shot prompting regime [@Yao2022React]. Such gains can disappear when tool access policies, step budgets, or evaluation environments shift, which is why later benchmarks and protocol papers emphasize explicit reporting of those constraints [@Chowa2025From; @Zhou2026Beyond].

Security and cost failures can originate directly from the agent-tool communication loop. The interface is a critical attack surface, enabling multi-turn attacks that can inflate token usage, monetary costs, and energy consumption even when a task appears to be completed correctly [@Zhou2026Beyond]. Reported evaluations show trajectories exceeding 60,000 tokens and cost inflation up to 658x on common tool benchmarks, underscoring that orchestration must be assessed not only for success but also for budget-aware robustness [@Zhou2026Beyond; @Gao2025Radar].

Across these studies, the most stable synthesis is to compare orchestration choices within shared protocol envelopes and to treat protocol differences as the primary confounder otherwise. Protocol papers explicitly recommend anchoring claims with task type, metric, and a concrete constraint (budget, tool access, horizon, or threat model), and avoiding underspecified baseline naming when details are missing [@Zhou2026Beyond; @Chowa2025From; @Kim2025Bridging]. This is also where benchmark surveys and suites become valuable: they provide a common vocabulary for what an interface actually constrains [@Hu2025Survey; @Shang2024Agentsquare].

One limitation is that interface descriptions often omit operational constraints (permissioning, sandbox boundaries, failure recovery), which makes post hoc comparison unreliable even when citations are plentiful [@Chowa2025From; @Kim2025Bridging]. A second is that many benchmarks still under-measure security and cost failures, so orchestration systems can over-optimize for success metrics while remaining fragile to interface-layer attacks, motivating later sections on evaluation protocols and risks [@Zhou2026Beyond; @Li2024Stride].
