A sharp contrast in agent design is between single-agent loops that internalize all decisions, whereas multi-agent systems distribute reasoning, tool use, and verification across specialized roles. Distribution can improve coverage and robustness, but it also introduces coordination overhead and new failure modes, so evaluation must specify interaction budgets and aggregation protocols rather than reporting a single success number [@Jiang2023Large; @Aratchige2025Llms]. Role protocols and cost models largely determine whether coordination gains are comparable across papers, so multi-agent results are only comparable when communication channels and accounting assumptions are explicit [@Jiang2023Large; @Aratchige2025Llms].

Multi-agent coordination mechanisms include role specialization (planner, verifier, executor), communication protocols (messages, shared memory, structured schemas), and aggregation rules (vote, debate, referee). These choices reshape the effective action space: coordination can add verification capacity but can also amplify cascading failures when agents reinforce each other's errors [@Chen2024Llmarena; @Becker2025Mallm; @Le2024Multi]. As a result, "better coordination" should be stated as a protocol-contingent claim tied to interaction budgets and observable artifacts (e.g., what evidence verifiers can access) [@Aratchige2025Llms; @Chen2025Schema].

Rubric-based evaluation provides one way to make coordination protocols explicit. ARCANE is evaluated on tasks requiring multi-step reasoning and tool use using a corpus of 219 labeled rubrics derived from a benchmark, which helps separate coordination quality from superficial success metrics [@Masters2025Arcane]. This style of evaluation is valuable because it makes the grading protocol legible, enabling more meaningful comparisons of role design and aggregation rules [@Masters2025Arcane].

Coordination is also tested in collaborative settings where agents must share observations, negotiate roles, and recover from each other's errors. Such settings illustrate that protocol details (communication latency, observation sharing, and interaction budgets) can dominate outcomes, so coordination mechanisms should be compared under matched observability and communication constraints, not only under nominal task labels [@Li2025Learn; @Jiang2023Large].

Domain-specific multi-agent systems further illustrate how protocol assumptions become part of the method. In a drug-discovery setting, a multi-agent approach reports large improvements over a non-reasoning multi-agent baseline (e.g., a 45% F1 gain on a kinase inhibitor dataset), suggesting that coordination can matter even when the underlying model family is similar [@Inoue2024Drugagent]. The transferable lesson is not the number itself, but the requirement that datasets, metrics, and interaction protocols be explicit before coordination claims are generalized [@Inoue2024Drugagent; @Chen2025Schema].

Evaluation settings can also involve multi-agent scoring and assessment. AutoSCORE is evaluated on multiple datasets from the ASAP benchmark with both proprietary and open-source models, illustrating that the evaluation protocol may include grading and judgment components rather than purely environment success [@Wang2025Autoscore]. This makes coordination questions salient: when agents include judges or critics, the protocol must specify how judgments are aggregated and what signals are available to avoid hidden leakage or evaluator drift [@Wang2025Autoscore; @Masters2025Arcane].

Security verification offers a complementary coordination pattern: centralized supervision with delegated execution. MARVEL uses a supervisor agent that derives security policies and delegates validation tasks to executor agents, reflecting a common architectural motif where coordination is organized around a central planner with specialized workers [@Collini2025Marvel]. Such hierarchies can improve tractability, but they also concentrate failure modes in the supervisor's policy and require explicit accounting of what documentation and evidence each role can access [@Collini2025Marvel; @Chen2025Schema].

A useful architectural contrast is centralized supervision versus more decentralized coordination. Centralized designs can simplify protocol design and reduce message overhead; in contrast, decentralized designs can increase redundancy and verification at the cost of coordination complexity [@Jiang2023Large; @Becker2025Mallm]. In both cases, reporting should include interaction budgets and cost and latency constraints, since coordination gains can disappear when communication is expensive or time-limited [@Aratchige2025Llms; @Becker2025Mallm].

Across these studies, multi-agent systems tend to trade raw capability for controllability and verification. Systems that embed critics, referees, or specialized executors can surface errors that a single-agent loop would miss, but they can also create new pathways for error amplification and increased costs if protocols are not carefully bounded [@Masters2025Arcane; @Chen2024Llmarena; @Wang2025Autoscore]. For synthesis, the conservative move is to compare coordination mechanisms within shared protocol envelopes and to treat cross-paper differences in interaction budgets as the primary confounder [@Aratchige2025Llms; @Jiang2023Large].

One limitation is that coordination methods often face scalability and real-time response constraints, which means that improvements observed under generous interaction budgets may not transfer to deployment settings [@Aratchige2025Llms; @Becker2025Mallm]. Another is that coordination interacts with learning and adaptation: frameworks that allow agents to evolve before or during test time can change communication policies, making it essential to specify what changes are permitted and when [@Li2025Learn; @Van2025Survey].
