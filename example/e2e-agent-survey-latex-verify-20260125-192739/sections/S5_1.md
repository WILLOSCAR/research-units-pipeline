For system builders, the key decision in self-improvement is how much the agent is allowed to change under feedback. Adaptation can make agents more effective in specific environments, but it can also create instability and obscure what is being evaluated, since the policy at the end of an episode is not the policy at the beginning [@Van2025Survey; @Tao2024Survey]. Update budgets and feedback channels largely determine whether adaptation gains are stable and transferable, so adaptation mechanisms should be treated as part of the evaluation protocol: improvements are interpretable only relative to what information is used for learning, when updates occur, and which constraints (budget, access, threat model) apply [@Van2025Survey; @Tao2024Survey].

Self-improvement and adaptation mechanisms include prompt and context optimization, reflection and revision loops, and learning procedures that update policies before or during deployment. These mechanisms are attractive in long-horizon settings because they can correct recurring failure modes without requiring full retraining, but they also introduce new failure modes such as reward hacking and benchmark overfitting [@Van2025Survey; @Bilal2025Meta]. As a result, adaptation claims must be anchored in explicit evaluation settings, including what feedback is available and how much adaptation is allowed [@Van2025Survey; @Tao2024Survey].

Context optimization provides a concrete, protocol-aware form of adaptation. ACE reports optimizing contexts both offline (e.g., system prompts) and online (e.g., agent memory), with measured gains across agent and domain-specific benchmarks (e.g., +10.6% on agent benchmarks and +8.6% on finance) while reducing adaptation latency and rollout costs under its evaluation setup [@Zhang2025Agentic]. Such results are best interpreted as improvements under a specific adaptation protocol, rather than as unconditional superiority of a particular loop design [@Zhang2025Agentic; @Van2025Survey].

Self-challenging and self-generated training data highlight a different trade-off. In contrast to one-shot prompt tuning, a self-challenging framework reports over a two-fold improvement for an open model (Llama-3.1-8B-Instruct) on multi-turn tool-use benchmarks using only self-generated training data, suggesting that agents can improve through internal feedback loops when the benchmark protocol supports iterative refinement [@Zhou2025Self]. The caveat is that such gains may be fragile when the evaluation distribution shifts or when the internal critic aligns to benchmark-specific artifacts [@Van2025Survey; @Bilal2025Meta].

More general learning frameworks attempt to expand beyond prompt-level tuning by enabling agents to learn and evolve before and during test time, incorporating environment-relevant knowledge for planning and improved cooperation [@Li2025Learn]. This style of adaptation blurs the line between "agent design" and "training procedure": what appears as a stronger agent may be a stronger learning protocol under the same interface constraints [@Li2025Learn; @Van2025Survey].

A useful contrast is between adaptation that primarily changes the agent's context and adaptation that changes the agent's behavior policy more directly. Context-centered approaches can be faster and cheaper but can be brittle to prompt injection and context drift, whereas more direct learning-based approaches can be more stable but require careful accounting of data, feedback, and compute budgets [@Taylor2025Large; @Jin2024From]. In either case, evaluation should report adaptation budgets and update frequency, since these determine whether the method scales in realistic deployments [@Van2025Survey; @Tao2024Survey].

Evaluation protocols for adaptation must also address what is being optimized. Surveys of foundation-model limitations emphasize issues such as hallucinations and limited self-assessment, and suggest that adaptation methods can trade short-term benchmark gains for degraded calibration and interpretability if objectives are underspecified [@Van2025Survey; @Bilal2025Meta]. This motivates conservative synthesis: distinguish improvements in task success from improvements in reliability under uncertainty, and avoid collapsing them into a single score [@Van2025Survey; @Tao2024Survey].

Security is an especially important axis for self-improvement. Adaptation mechanisms can amplify vulnerabilities by internalizing adversarial signals or by optimizing toward unsafe tool behaviors, so end-to-end security benchmarks for tool-use pipelines are relevant even when their primary focus is risk [@Zhang2025Security]. A practical takeaway is that adaptation should be evaluated under threat models that include adversarial inputs and tool-interface attacks, not only under benign task success [@Zhang2025Security].

Across the literature, adaptation also interacts with multi-agent settings and system architecture. Agent evolution can change communication policies and coordination dynamics, while architectural constraints (tool protocols, memory interfaces) bound what adaptation can safely modify [@Wei2026Agentic; @Chen2025Largea]. For synthesis, this suggests treating adaptation as a layer that sits on top of interface and protocol choices: without stable interfaces, adaptation gains are difficult to attribute and difficult to transfer [@Jiang2025Agentic; @Van2025Survey].
Across the literature, adaptation also interacts with multi-agent settings and system architecture. Agent evolution can change communication policies and coordination dynamics, while architectural constraints (tool protocols, memory interfaces) bound what adaptation can safely modify [@Wei2026Agentic; @Chen2025Largea]. For synthesis, a practical reading is to treat adaptation as a layer that sits on top of interface and protocol choices: without stable interfaces, adaptation gains are difficult to attribute and difficult to transfer [@Jiang2025Agentic; @Van2025Survey].

One limitation is that many adaptation papers report gains without sufficiently specifying update budgets and feedback channels, which makes cross-paper comparison fragile and encourages overclaiming [@Van2025Survey; @Tao2024Survey]. Another is that adaptation introduces governance questions: agents that learn online can change risk profiles after deployment, so evaluations should include both capability and safety metrics under realistic constraints [@Zhang2025Security; @Van2025Survey].
