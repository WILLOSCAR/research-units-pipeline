A central tension in agent loops is that richer action spaces promise broader capability, but they also make behavior harder to constrain, reproduce, and compare under realistic budgets. When evaluation protocols differ (task families, metrics, interaction budgets, tool access), apparent improvements can reflect protocol choices rather than methodological advantages, so comparisons only become meaningful after protocol context is made explicit [@Hu2025Survey; @Kim2025Bridging]. Accordingly, action spaces should be discussed together with the evaluation setup they imply, and protocol alignment should be treated as a prerequisite for cross-paper synthesis [@Zhang2026Evoroute].

An agent loop can be modeled as a repeated state-decide-act-observe cycle, but the semantics of "act" vary widely in tool-using settings. Actions may be direct environment steps, whereas tool-using agents often execute structured tool calls with schemas or mixed actions that combine invocation with natural-language control; each choice changes what is observable and what failure recovery is possible [@Yao2022React; @Zou2025Based; @Jiang2024Agent]. In practice, the action interface also determines what can be audited (e.g., whether intermediate tool arguments are visible) and which failures are recoverable versus silent [@Kim2025Bridging].

Action spaces are inseparable from the observation model. If observations are lossy or delayed, agents may rely on implicit state and brittle heuristics; if observations are rich, agents can support verification and correction loops, but at higher compute and latency cost [@Feng2025Group; @Zhang2026Evoroute]. This trade-off is often hidden in benchmark reporting, where small differences in logging, permissions, or tool visibility can change both success rates and the distribution of failure modes [@Shen2025Feat; @Zou2025Based].

The benchmark landscape itself reveals why protocol context matters. A broad survey of agent evaluation spans 190+ benchmark datasets and documents a shift from static exams toward process- and discovery-oriented assessments, where interaction protocols and budgets become first-order variables [@Hu2025Survey]. Under such diversity, "agent loop quality" cannot be treated as a single scalar: claims must be interpreted relative to task family, metric definition, and the constraints imposed by the environment and tool interface [@Kim2025Bridging].

Recent benchmark-driven agent frameworks illustrate the consequence of wide, heterogeneous evaluation. AgentSwift evaluates across seven benchmarks spanning embodied, math, web, tool, and game domains, reporting an average improvement of 8.34% over baselines in that multi-domain setting [@Li2025Agentswift]. AgentSquare reports gains across six benchmarks in similarly diverse scenarios, but the comparability of these numbers depends on whether the underlying protocols (tool access, budgets, environment versions) are aligned across methods [@Shang2024Agentsquare; @Hu2025Survey].

Targeted dataset design provides a complementary angle: in contrast to broad benchmark suites, some work introduces bespoke datasets to stress specific loop properties. For example, MuaLLM introduces custom datasets aimed at retrieval and citation behavior and multi-step reasoning, which can surface action-space assumptions that are easy to miss in broader suites [@Abbineni2025Muallm]. Such datasets can improve diagnostic power, but they also increase the risk of overfitting unless they are connected back to shared protocol anchors used elsewhere [@Hu2025Survey; @Kim2025Bridging].

Security and robustness further complicate the notion of an action space. Memory and tool interactions can act as long-lived state, which creates attack channels where adversarial content influences future decisions; defenses therefore become part of the loop design rather than an external add-on [@Wei2025Memguard]. A-MemGuard reports large reductions in attack success rates with minimal utility cost, illustrating that loop safety can be evaluated as a first-class objective, but only when threat models and interaction assumptions are made explicit [@Wei2025Memguard; @Zou2025Based].

Compute, latency, and monetary cost impose another unavoidable constraint. The agent system trilemma formalizes the tension among state-of-the-art performance, low cost, and fast completion, suggesting that "better loops" are often better only under a particular budget regime [@Zhang2026Evoroute]. This motivates protocol-aware reporting: without budgets and time constraints, comparisons across action-space designs can be misleading even when they cite the same task label [@Kim2025Bridging; @Hu2025Survey].

Across these threads, a consistent pattern is that evaluation and action design co-determine each other. Loop abstractions that perform well on interactive benchmarks can fail under stricter tool permissions, while security-aware interfaces can change both capability and measured utility [@Yao2022React; @Wei2025Memguard; @Zhang2026Evoroute]. For synthesis, the conservative move is to compare methods only within shared protocol envelopes, and otherwise describe differences as protocol-driven shifts in what the loop can reliably guarantee [@Hu2025Survey; @Kim2025Bridging].

One limitation is that many papers still underspecify protocol details (budgets, tool access, visibility), which forces any survey-level comparison to weaken claims rather than guess missing context [@Hu2025Survey; @Kim2025Bridging]. Another is that action spaces are implemented through concrete tool interfaces and orchestration policies; without examining those interfaces, loop-level claims remain underspecified, which motivates the next subsection's focus on tool contracts and orchestration patterns [@Zou2025Based; @Kim2025Bridging].
