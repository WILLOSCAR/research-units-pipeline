## Discussion

One cross-cutting implication is that many "agent improvements" are inseparable from interface assumptions. Changing a tool protocol, permission model, or observation channel can shift not only success rates but also which failure modes are even observable, so comparisons that omit these details risk overclaiming generality [@Zhang2025Tool; @Chowa2025From; @Kim2025Bridging; @Zou2025Based]. A practical reading is to treat interface contracts as part of the method, not as a peripheral implementation detail [@Lumer2025Memtool; @Liu2025Mcpagentbench].

A second implication concerns evaluation drift. Benchmark suites cover increasingly diverse domains, but task, metric, and budget choices vary across papers, and even small protocol differences can dominate outcomes for long-horizon loops [@Hu2025Survey; @Shang2024Agentsquare; @Zhang2026Evoroute]. Where protocol context is missing, the safest synthesis is comparative but conservative: emphasize patterns of trade-offs and failure modes, and downgrade claims that depend on unreported budgets or tool access [@Hu2025Evaluating; @Ji2024Testing; @Zhang2025Generalizability].

Risk surfaces also scale with capability. Tool use expands the attack surface through prompt injection and unintended tool behaviors, and mitigation often depends on the same interface decisions that govern capability and evaluation [@Zhang2025Security; @Gasmi2025Bridging; @Wei2025Memguard]. This suggests that safety and evaluation should be co-designed: threat models and governance constraints need to be represented in the evaluation protocol, not deferred to a separate checklist [@Weng2025Bridgescope; @Zou2025Based].

Looking forward, progress is likely to depend on making protocols more explicit and more comparable: standardizing what is reported (tool access, budgets, environment versions), supporting ablation-friendly evaluations for loop components (planning vs memory vs orchestration), and documenting failure modes in a way that enables cross-paper synthesis [@Hu2025Survey; @Zhang2025Datascibench; @Cui2025Toward]. These practices are not mere hygiene; they determine whether evidence accumulates across studies or remains siloed by incompatible assumptions [@Zhang2025Generalizability; @Du2025Survey].
