Seen through the lens of long-horizon agency, memory is not an optional enhancement but a constraint that reshapes what an agent can reliably do. Finite context windows force agents to decide what to retain, retrieve, and trust, and those decisions can dominate behavior under realistic interaction horizons [@Zhu2025Where; @Du2025Memr; @Yao2025Survey]. Retrieval policies and trust assumptions therefore determine what evidence the loop can condition on and what failures are diagnosable, so memory mechanisms should be compared under protocols that make those assumptions visible [@Hu2025Evaluating; @Ji2024Testing].

Memory mechanisms range from retrieval-augmented prompting to persistent stores that act as long-lived state. Retrieval can be treated as an explicit action (retrieve, reflect, or answer routing), which makes memory access part of the agent loop, while persistent memory can implicitly bias future decisions even when not surfaced as an explicit action [@Du2025Memr; @Yao2022React]. These choices change failure recovery: explicit retrieval can be audited and revised, whereas implicit memory can create silent drift unless interfaces expose provenance and trust signals [@Zhu2025Where; @Wei2025Memory].

Concrete systems illustrate how memory control can be operationalized. MemR$^3$ treats memory retrieval as an autonomous component and introduces a router that chooses among retrieve, reflect, and answer actions, along with an evidence-gap signal intended to optimize answer quality under constraints [@Du2025Memr]. This design suggests that memory is best viewed as a policy: when to retrieve, what to retrieve, and how to revise or discard memories as the loop unfolds [@Du2025Memr; @Sun2025Search].

Interactive benchmarks show why memory cannot be evaluated in isolation. In ReAct-style loops, decisions are conditioned on a growing interaction trace, and a small number of in-context examples can yield large gains on interactive environments, but only within the specific benchmark protocols that define observation and action semantics [@Yao2022React]. As environments shift, the same memory strategy can produce cascading errors, where an early mistake compounds through subsequent decisions [@Zhu2025Where; @Hu2025Evaluating].

A useful contrast is between memory systems that prioritize breadth of context and those that prioritize structured, verified evidence. Some work focuses on retrieving larger quantities of potentially relevant context, whereas other work stresses retrieval that is auditable and compatible with downstream verification, making the memory interface itself part of the evaluation protocol [@Huang2025Retrieval; @Wei2025Memory]. In contrast to breadth-first retrieval, evidence-first memory designs treat retrieval as a verifiable action with explicit trust signals, which changes both the error surface and the cost profile of the loop [@Hu2025Evaluating].

Error-taxonomy and failure-trajectory datasets make these issues more concrete. AgentErrorTaxonomy and AgentErrorBench classify failures across memory, reflection, planning, and action, and ground error analysis in annotated trajectories drawn from interactive settings such as ALFWorld, GAIA, and WebShop [@Zhu2025Where]. These resources support protocol-aware comparison by making failure types explicit rather than collapsing them into a single success metric [@Zhu2025Where; @Ji2024Testing].

Security concerns are especially salient for memory. If memory is treated as a writable store that future decisions trust, an adversary can inject benign-looking records that later manipulate behavior, turning memory into a persistent attack channel [@Wei2025Memguard]. This risk is easy to miss unless the threat model and memory interface are explicitly part of the evaluation protocol (what is writable, what is trusted, and how provenance is exposed) [@Wei2025Memguard; @Zhu2025Where].

Defenses therefore need to be evaluated as loop components. A-MemGuard reports large reductions in attack success rates (over 95%) with minimal utility cost, illustrating that memory security can be studied empirically when the evaluation protocol includes adversarial interactions and clear success and failure definitions [@Wei2025Memguard]. The broader implication is that memory mechanisms should be compared not only for nominal task success, but also for robustness under realistic adversarial and distribution-shift conditions [@Wei2025Memguard; @Zhu2025Where].

Across recent surveys and component taxonomies, memory and retrieval are increasingly treated as core axes for agents, but the evidence base remains uneven when protocols are underspecified. Survey-style synthesis therefore benefits from combining broad taxonomies with explicit protocol anchors (task family, metric, budget, and tool and memory access assumptions) before drawing cross-paper conclusions [@Ge2025Surveya; @Du2025Survey; @Hu2025Evaluating]. Without this alignment, cross-paper numeric comparisons of retrieval systems remain fragile and should be described as protocol-contingent rather than universal [@Ji2024Testing].

One limitation is that many papers report memory mechanisms without exposing enough provenance or trust signals to support reproducible comparison, which forces synthesis to focus on qualitative trade-offs [@Hu2025Evaluating; @Wei2025Memory]. Another is that memory is entangled with security and governance: the same mechanism that enables long-horizon competence can create persistent attack channels, so future benchmarks must evaluate memory under threat models, not only under nominal success metrics [@Wei2025Memguard; @Zhu2025Where].
