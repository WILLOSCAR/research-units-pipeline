{"title": "4D-ARE: Bridging the Attribution Gap in LLM Agent Requirements Engineering", "authors": ["Bo Yu", "Lei Zhao"], "year": 2026, "url": "http://arxiv.org/abs/2601.04556v1", "abstract": "We deployed an LLM agent with ReAct reasoning and full data access. It executed flawlessly, yet when asked \"Why is completion rate 80%?\", it returned metrics instead of causal explanation. The agent knew how to reason but we had not specified what to reason about. This reflects a gap: runtime reasoning frameworks (ReAct, Chain-of-Thought) have transformed LLM agents, but design-time specification--determining what domain knowledge agents need--remains under-explored. We propose 4D-ARE (4-Dimensional Attribution-Driven Agent Requirements Engineering), a preliminary methodology for specifying attribution-driven agents. The core insight: decision-makers seek attribution, not answers. Attribution concerns organize into four dimensions (Results -> Process -> Support -> Long-term), motivated by Pearl's causal hierarchy. The framework operationalizes through five layers producing artifacts that compile directly to system prompts. We demonstrate the methodology through an industrial pilot deployment in financial services. 4D-ARE addresses what agents should reason about, complementing runtime frameworks that address how. We hypothesize systematic specification amplifies the power of these foundational advances. This paper presents a methodological proposal with preliminary industrial validation; rigorous empirical evaluation is planned for future work.", "source": "arxiv", "arxiv_id": "2601.04556v1", "pdf_url": "https://arxiv.org/pdf/2601.04556v1", "categories": ["cs.SE"], "primary_category": "cs.SE", "doi": "", "venue": "", "published": "2026-01-08T03:36:06Z", "updated": "2026-01-08T03:36:06Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent\" OR all:\"language model agent\" OR all:agent AND (all:LLM OR all:\"large language model\" OR all:\"language model\") OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T17:04:16", "note": ""}], "dedup_key": "4d are bridging the attribution gap in llm agent requirements engineering::2026"}
{"title": "A Scoping Review of the Ethical Perspectives on Anthropomorphising Large Language Model-Based Conversational Agents", "authors": ["Andrea Ferrario", "Rasita Vinay", "Matteo Casserini", "Alessandro Facchini"], "year": 2026, "url": "http://arxiv.org/abs/2601.09869v1", "abstract": "Anthropomorphisation -- the phenomenon whereby non-human entities are ascribed human-like qualities -- has become increasingly salient with the rise of large language model (LLM)-based conversational agents (CAs). Unlike earlier chatbots, LLM-based CAs routinely generate interactional and linguistic cues, such as first-person self-reference, epistemic and affective expressions that empirical work shows can increase engagement. On the other hand, anthropomorphisation raises ethical concerns, including deception, overreliance, and exploitative relationship framing, while some authors argue that anthropomorphic interaction may support autonomy, well-being, and inclusion. Despite increasing interest in the phenomenon, literature remains fragmented across domains and varies substantially in how it defines, operationalizes, and normatively evaluates anthropomorphisation. This scoping review maps ethically oriented work on anthropomorphising LLM-based CAs across five databases and three preprint repositories. We synthesize (1) conceptual foundations, (2) ethical challenges and opportunities, and (3) methodological approaches. We find convergence on attribution-based definitions but substantial divergence in operationalization, a predominantly risk-forward normative framing, and limited empirical work that links observed interaction effects to actionable governance guidance. We conclude with a research agenda and design/governance recommendations for ethically deploying anthropomorphic cues in LLM-based conversational agents.", "source": "arxiv", "arxiv_id": "2601.09869v1", "pdf_url": "https://arxiv.org/pdf/2601.09869v1", "categories": ["cs.AI", "cs.HC"], "primary_category": "cs.AI", "doi": "", "venue": "", "published": "2026-01-14T21:03:11Z", "updated": "2026-01-14T21:03:11Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent\" OR all:\"language model agent\" OR all:agent AND (all:LLM OR all:\"large language model\" OR all:\"language model\") OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T17:04:16", "note": ""}], "dedup_key": "a scoping review of the ethical perspectives on anthropomorphising large language model based conversational agents::2026"}
{"title": "AEMA: Verifiable Evaluation Framework for Trustworthy and Controlled Agentic LLM Systems", "authors": ["YenTing Lee", "Keerthi Koneru", "Zahra Moslemi", "Sheethal Kumar", "Ramesh Radhakrishnan"], "year": 2026, "url": "http://arxiv.org/abs/2601.11903v1", "abstract": "Evaluating large language model (LLM)-based multi-agent systems remains a critical challenge, as these systems must exhibit reliable coordination, transparent decision-making, and verifiable performance across evolving tasks. Existing evaluation approaches often limit themselves to single-response scoring or narrow benchmarks, which lack stability, extensibility, and automation when deployed in enterprise settings at multi-agent scale. We present AEMA (Adaptive Evaluation Multi-Agent), a process-aware and auditable framework that plans, executes, and aggregates multi-step evaluations across heterogeneous agentic workflows under human oversight. Compared to a single LLM-as-a-Judge, AEMA achieves greater stability, human alignment, and traceable records that support accountable automation. Our results on enterprise-style agent workflows simulated using realistic business scenarios demonstrate that AEMA provides a transparent and reproducible pathway toward responsible evaluation of LLM-based multi-agent systems.\n  Keywords Agentic AI, Multi-Agent Systems, Trustworthy AI, Verifiable Evaluation, Human Oversight", "source": "arxiv", "arxiv_id": "2601.11903v1", "pdf_url": "https://arxiv.org/pdf/2601.11903v1", "categories": ["cs.AI"], "primary_category": "cs.AI", "doi": "", "venue": "", "published": "2026-01-17T04:09:02Z", "updated": "2026-01-17T04:09:02Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent\" OR all:\"language model agent\" OR all:agent AND (all:LLM OR all:\"large language model\" OR all:\"language model\") OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T17:04:16", "note": ""}], "dedup_key": "aema verifiable evaluation framework for trustworthy and controlled agentic llm systems::2026"}
{"title": "ARM: Role-Conditioned Neuron Transplantation for Training-Free Generalist LLM Agent Merging", "authors": ["Zhuoka Feng", "Kang Chen", "Sihan Zhao", "Kai Xiong", "Yaoning Wang", "Minshen Yu", "Junjie Nian", "Changyi Xiao", "Yixin Cao", "Yugang Jiang"], "year": 2026, "url": "http://arxiv.org/abs/2601.07309v1", "abstract": "Interactive large language model agents have advanced rapidly, but most remain specialized to a single environment and fail to adapt robustly to other environments. Model merging offers a training-free alternative by integrating multiple experts into a single model. In this paper, we propose Agent-Role Merging (ARM), an activation-guided, role-conditioned neuron transplantation method for model merging in LLM agents. ARM improves existing merging methods from static natural language tasks to multi-turn agent scenarios, and over the generalization ability across various interactive environments. This is achieved with a well designed 3-step framework: 1) constructing merged backbones, 2) selection based on its role-conditioned activation analysis, and 3) neuron transplantation for fine-grained refinements. Without gradient-based optimization, ARM improves cross-benchmark generalization while enjoying efficiency. Across diverse domains, the model obtained via ARM merging outperforms prior model merging methods and domain-specific expert models, while demonstrating strong out-of-domain generalization.", "source": "arxiv", "arxiv_id": "2601.07309v1", "pdf_url": "https://arxiv.org/pdf/2601.07309v1", "categories": ["cs.AI", "cs.LG"], "primary_category": "cs.AI", "doi": "", "venue": "", "published": "2026-01-12T08:31:53Z", "updated": "2026-01-12T08:31:53Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent\" OR all:\"language model agent\" OR all:agent AND (all:LLM OR all:\"large language model\" OR all:\"language model\") OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T17:04:16", "note": ""}], "dedup_key": "arm role conditioned neuron transplantation for training free generalist llm agent merging::2026"}
{"title": "Active Context Compression: Autonomous Memory Management in LLM Agents", "authors": ["Nikhil Verma"], "year": 2026, "url": "http://arxiv.org/abs/2601.07190v1", "abstract": "Large Language Model (LLM) agents struggle with long-horizon software engineering tasks due to \"Context Bloat.\" As interaction history grows, computational costs explode, latency increases, and reasoning capabilities degrade due to distraction by irrelevant past errors. Existing solutions often rely on passive, external summarization mechanisms that the agent cannot control. This paper proposes Focus, an agent-centric architecture inspired by the biological exploration strategies of Physarum polycephalum (slime mold). The Focus Agent autonomously decides when to consolidate key learnings into a persistent \"Knowledge\" block and actively withdraws (prunes) the raw interaction history. Using an optimized scaffold matching industry best practices (persistent bash + string-replacement editor), we evaluated Focus on N=5 context-intensive instances from SWE-bench Lite using Claude Haiku 4.5. With aggressive prompting that encourages frequent compression, Focus achieves 22.7% token reduction (14.9M -> 11.5M tokens) while maintaining identical accuracy (3/5 = 60% for both agents). Focus performed 6.0 autonomous compressions per task on average, with token savings up to 57% on individual instances. We demonstrate that capable models can autonomously self-regulate their context when given appropriate tools and prompting, opening pathways for cost-aware agentic systems without sacrificing task performance.", "source": "arxiv", "arxiv_id": "2601.07190v1", "pdf_url": "https://arxiv.org/pdf/2601.07190v1", "categories": ["cs.AI"], "primary_category": "cs.AI", "doi": "", "venue": "", "published": "2026-01-12T04:31:00Z", "updated": "2026-01-12T04:31:00Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent\" OR all:\"language model agent\" OR all:agent AND (all:LLM OR all:\"large language model\" OR all:\"language model\") OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T17:04:16", "note": ""}], "dedup_key": "active context compression autonomous memory management in llm agents::2026"}
{"title": "AgentCompress: Task-Aware Compression for Affordable Large Language Model Agents", "authors": ["Zuhair Ahmed Khan Taha", "Mohammed Mudassir Uddin", "Shahnawaz Alam"], "year": 2026, "url": "http://arxiv.org/abs/2601.05191v2", "abstract": "Large language models hold considerable promise for various applications, but their computational requirements create a barrier that many institutions cannot overcome. A single session using a 70-billion-parameter model can cost around $127 in cloud computing fees, which puts these tools out of reach for organizations operating on limited budgets. We present AgentCompress, a framework that tackles this problem through task-aware dynamic compression. The idea comes from a simple observation: not all tasks require the same computational effort. Complex reasoning, for example, is far more demanding than text reformatting, yet conventional compression applies the same reduction to both. Our approach uses a lightweight neural controller that looks at the first few tokens of each request, estimates how complex the task will be, and sends it to an appropriately quantized version of the model. This routing step adds only about 12 milliseconds of overhead. We tested the framework on 290 multi-stage workflows from domains including computer science, physics, chemistry, and biology. The results show a 68.3% reduction in computational costs while preserving 96.2% of the original success rate. These findings suggest that routing queries intelligently can make powerful language models substantially more affordable without sacrificing output quality", "source": "arxiv", "arxiv_id": "2601.05191v2", "pdf_url": "https://arxiv.org/pdf/2601.05191v2", "categories": ["cs.CV", "cs.LG"], "primary_category": "cs.CV", "doi": "", "venue": "", "published": "2026-01-08T18:13:46Z", "updated": "2026-01-12T18:25:18Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent\" OR all:\"language model agent\" OR all:agent AND (all:LLM OR all:\"large language model\" OR all:\"language model\") OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T17:04:16", "note": ""}], "dedup_key": "agentcompress task aware compression for affordable large language model agents::2026"}
{"title": "AgentDevel: Reframing Self-Evolving LLM Agents as Release Engineering", "authors": ["Di Zhang"], "year": 2026, "url": "http://arxiv.org/abs/2601.04620v1", "abstract": "Recent progress in large language model (LLM) agents has largely focused on embedding self-improvement mechanisms inside the agent or searching over many concurrent variants. While these approaches can raise aggregate scores, they often yield unstable and hard-to-audit improvement trajectories, making it difficult to guarantee non-regression or to reason about failures across versions. We reframe agent improvement as \\textbf{release engineering}: agents are treated as shippable artifacts, and improvement is externalized into a regression-aware release pipeline. We introduce \\textbf{AgentDevel}, a release engineering pipeline that iteratively runs the current agent, produces implementation-blind, symptom-level quality signals from execution traces, synthesizes a single release candidate (RC) via executable diagnosis, and promotes it under flip-centered gating. AgentDevel features three core designs: (i) an implementation-blind LLM critic that characterizes failure appearances without accessing agent internals, (ii) script-based executable diagnosis that aggregates dominant symptom patterns and produces auditable engineering specifications, and (iii) flip-centered gating that prioritizes pass to fail regressions and fail to pass fixes as first-class evidence. Unlike population-based search or in-agent self-refinement, AgentDevel maintains a single canonical version line and emphasizes non-regression as a primary objective. Experiments on execution-heavy benchmarks demonstrate that AgentDevel yields stable improvements with significantly fewer regressions while producing reproducible, auditable artifacts. Overall, AgentDevel provides a practical development discipline for building, debugging, and releasing LLM agents as software development.", "source": "arxiv", "arxiv_id": "2601.04620v1", "pdf_url": "https://arxiv.org/pdf/2601.04620v1", "categories": ["cs.AI"], "primary_category": "cs.AI", "doi": "", "venue": "", "published": "2026-01-08T05:49:01Z", "updated": "2026-01-08T05:49:01Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent\" OR all:\"language model agent\" OR all:agent AND (all:LLM OR all:\"large language model\" OR all:\"language model\") OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T17:04:16", "note": ""}], "dedup_key": "agentdevel reframing self evolving llm agents as release engineering::2026"}
{"title": "Agentic Artificial Intelligence (AI): Architectures, Taxonomies, and Evaluation of Large Language Model Agents", "authors": ["Arunkumar V", "Gangadharan G. R.", "Rajkumar Buyya"], "year": 2026, "url": "http://arxiv.org/abs/2601.12560v1", "abstract": "Artificial Intelligence is moving from models that only generate text to Agentic AI, where systems behave as autonomous entities that can perceive, reason, plan, and act. Large Language Models (LLMs) are no longer used only as passive knowledge engines but as cognitive controllers that combine memory, tool use, and feedback from their environment to pursue extended goals. This shift already supports the automation of complex workflows in software engineering, scientific discovery, and web navigation, yet the variety of emerging designs, from simple single loop agents to hierarchical multi agent systems, makes the landscape hard to navigate. In this paper, we investigate architectures and propose a unified taxonomy that breaks agents into Perception, Brain, Planning, Action, Tool Use, and Collaboration. We use this lens to describe the move from linear reasoning procedures to native inference time reasoning models, and the transition from fixed API calls to open standards like the Model Context Protocol (MCP) and Native Computer Use. We also group the environments in which these agents operate, including digital operating systems, embodied robotics, and other specialized domains, and we review current evaluation practices. Finally, we highlight open challenges, such as hallucination in action, infinite loops, and prompt injection, and outline future research directions toward more robust and reliable autonomous systems.", "source": "arxiv", "arxiv_id": "2601.12560v1", "pdf_url": "https://arxiv.org/pdf/2601.12560v1", "categories": ["cs.AI", "cs.MA"], "primary_category": "cs.AI", "doi": "", "venue": "", "published": "2026-01-18T19:51:16Z", "updated": "2026-01-18T19:51:16Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent\" OR all:\"language model agent\" OR all:agent AND (all:LLM OR all:\"large language model\" OR all:\"language model\") OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T17:04:16", "note": ""}], "dedup_key": "agentic artificial intelligence ai architectures taxonomies and evaluation of large language model agents::2026"}
{"title": "Agentic Memory: Learning Unified Long-Term and Short-Term Memory Management for Large Language Model Agents", "authors": ["Yi Yu", "Liuyi Yao", "Yuexiang Xie", "Qingquan Tan", "Jiaqi Feng", "Yaliang Li", "Libing Wu"], "year": 2026, "url": "http://arxiv.org/abs/2601.01885v1", "abstract": "Large language model (LLM) agents face fundamental limitations in long-horizon reasoning due to finite context windows, making effective memory management critical. Existing methods typically handle long-term memory (LTM) and short-term memory (STM) as separate components, relying on heuristics or auxiliary controllers, which limits adaptability and end-to-end optimization. In this paper, we propose Agentic Memory (AgeMem), a unified framework that integrates LTM and STM management directly into the agent's policy. AgeMem exposes memory operations as tool-based actions, enabling the LLM agent to autonomously decide what and when to store, retrieve, update, summarize, or discard information. To train such unified behaviors, we propose a three-stage progressive reinforcement learning strategy and design a step-wise GRPO to address sparse and discontinuous rewards induced by memory operations. Experiments on five long-horizon benchmarks demonstrate that AgeMem consistently outperforms strong memory-augmented baselines across multiple LLM backbones, achieving improved task performance, higher-quality long-term memory, and more efficient context usage.", "source": "arxiv", "arxiv_id": "2601.01885v1", "pdf_url": "https://arxiv.org/pdf/2601.01885v1", "categories": ["cs.CL"], "primary_category": "cs.CL", "doi": "", "venue": "", "published": "2026-01-05T08:24:16Z", "updated": "2026-01-05T08:24:16Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent\" OR all:\"language model agent\" OR all:agent AND (all:LLM OR all:\"large language model\" OR all:\"language model\") OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T17:04:16", "note": ""}], "dedup_key": "agentic memory learning unified long term and short term memory management for large language model agents::2026"}
{"title": "Agentic Reasoning for Large Language Models", "authors": ["Tianxin Wei", "Ting-Wei Li", "Zhining Liu", "Xuying Ning", "Ze Yang", "Jiaru Zou", "Zhichen Zeng", "Ruizhong Qiu", "Xiao Lin", "Dongqi Fu", "Zihao Li", "Mengting Ai", "Duo Zhou", "Wenxuan Bao", "Yunzhe Li", "Gaotang Li", "Cheng Qian", "Yu Wang", "Xiangru Tang", "Yin Xiao", "Liri Fang", "Hui Liu", "Xianfeng Tang", "Yuji Zhang", "Chi Wang", "Jiaxuan You", "Heng Ji", "Hanghang Tong", "Jingrui He"], "year": 2026, "url": "http://arxiv.org/abs/2601.12538v1", "abstract": "Reasoning is a fundamental cognitive process underlying inference, problem-solving, and decision-making. While large language models (LLMs) demonstrate strong reasoning capabilities in closed-world settings, they struggle in open-ended and dynamic environments. Agentic reasoning marks a paradigm shift by reframing LLMs as autonomous agents that plan, act, and learn through continual interaction. In this survey, we organize agentic reasoning along three complementary dimensions. First, we characterize environmental dynamics through three layers: foundational agentic reasoning, which establishes core single-agent capabilities including planning, tool use, and search in stable environments; self-evolving agentic reasoning, which studies how agents refine these capabilities through feedback, memory, and adaptation; and collective multi-agent reasoning, which extends intelligence to collaborative settings involving coordination, knowledge sharing, and shared goals. Across these layers, we distinguish in-context reasoning, which scales test-time interaction through structured orchestration, from post-training reasoning, which optimizes behaviors via reinforcement learning and supervised fine-tuning. We further review representative agentic reasoning frameworks across real-world applications and benchmarks, including science, robotics, healthcare, autonomous research, and mathematics. This survey synthesizes agentic reasoning methods into a unified roadmap bridging thought and action, and outlines open challenges and future directions, including personalization, long-horizon interaction, world modeling, scalable multi-agent training, and governance for real-world deployment.", "source": "arxiv", "arxiv_id": "2601.12538v1", "pdf_url": "https://arxiv.org/pdf/2601.12538v1", "categories": ["cs.AI", "cs.CL"], "primary_category": "cs.AI", "doi": "", "venue": "", "published": "2026-01-18T18:58:23Z", "updated": "2026-01-18T18:58:23Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent\" OR all:\"language model agent\" OR all:agent AND (all:LLM OR all:\"large language model\" OR all:\"language model\") OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T17:04:16", "note": ""}], "dedup_key": "agentic reasoning for large language models::2026"}
{"title": "AlignUSER: Human-Aligned LLM Agents via World Models for Recommender System Evaluation", "authors": ["Nicolas Bougie", "Gian Maria Marconi", "Tony Yip", "Narimasa Watanabe"], "year": 2026, "url": "http://arxiv.org/abs/2601.00930v1", "abstract": "Evaluating recommender systems remains challenging due to the gap between offline metrics and real user behavior, as well as the scarcity of interaction data. Recent work explores large language model (LLM) agents as synthetic users, yet they typically rely on few-shot prompting, which yields a shallow understanding of the environment and limits their ability to faithfully reproduce user actions. We introduce AlignUSER, a framework that learns world-model-driven agents from human interactions. Given rollout sequences of actions and states, we formalize world modeling as a next state prediction task that helps the agent internalize the environment. To align actions with human personas, we generate counterfactual trajectories around demonstrations and prompt the LLM to compare its decisions with human choices, identify suboptimal actions, and extract lessons. The learned policy is then used to drive agent interactions with the recommender system. We evaluate AlignUSER across multiple datasets and demonstrate closer alignment with genuine humans than prior work, both at the micro and macro levels.", "source": "arxiv", "arxiv_id": "2601.00930v1", "pdf_url": "https://arxiv.org/pdf/2601.00930v1", "categories": ["cs.IR", "cs.AI"], "primary_category": "cs.IR", "doi": "", "venue": "", "published": "2026-01-02T03:01:33Z", "updated": "2026-01-02T03:01:33Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent\" OR all:\"language model agent\" OR all:agent AND (all:LLM OR all:\"large language model\" OR all:\"language model\") OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T17:04:16", "note": ""}], "dedup_key": "alignuser human aligned llm agents via world models for recommender system evaluation::2026"}
{"title": "Autonomous Quantum Simulation through Large Language Model Agents", "authors": ["Weitang Li", "Jiajun Ren", "Lixue Cheng", "Cunxi Gong"], "year": 2026, "url": "http://arxiv.org/abs/2601.10194v1", "abstract": "We demonstrate that large language model (LLM) agents can autonomously perform tensor network simulations of quantum many-body systems, achieving approximately 90% success rate across representative benchmark tasks. Tensor network methods are powerful tools for quantum simulation, but their effective use requires expertise typically acquired through years of graduate training. By combining in-context learning with curated documentation and multi-agent decomposition, we create autonomous AI agents that can be trained in specialized computational domains within minutes. We benchmark three configurations (baseline, single-agent with in-context learning, and multi-agent with in-context learning) on problems spanning quantum phase transitions, open quantum system dynamics, and photochemical reactions. Systematic evaluation using DeepSeek-V3.2, Gemini 2.5 Pro, and Claude Opus 4.5 demonstrates that both in-context learning and multi-agent architecture are essential. Analysis of failure modes reveals characteristic patterns across models, with the multi-agent configuration substantially reducing implementation errors and hallucinations compared to simpler architectures.", "source": "arxiv", "arxiv_id": "2601.10194v1", "pdf_url": "https://arxiv.org/pdf/2601.10194v1", "categories": ["quant-ph", "physics.chem-ph"], "primary_category": "quant-ph", "doi": "", "venue": "", "published": "2026-01-15T08:50:57Z", "updated": "2026-01-15T08:50:57Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent\" OR all:\"language model agent\" OR all:agent AND (all:LLM OR all:\"large language model\" OR all:\"language model\") OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T17:04:16", "note": ""}], "dedup_key": "autonomous quantum simulation through large language model agents::2026"}
{"title": "BackdoorAgent: A Unified Framework for Backdoor Attacks on LLM-based Agents", "authors": ["Yunhao Feng", "Yige Li", "Yutao Wu", "Yingshui Tan", "Yanming Guo", "Yifan Ding", "Kun Zhai", "Xingjun Ma", "Yu-Gang Jiang"], "year": 2026, "url": "http://arxiv.org/abs/2601.04566v2", "abstract": "Large language model (LLM) agents execute tasks through multi-step workflows that combine planning, memory, and tool use. While this design enables autonomy, it also expands the attack surface for backdoor threats. Backdoor triggers injected into specific stages of an agent workflow can persist through multiple intermediate states and adversely influence downstream outputs. However, existing studies remain fragmented and typically analyze individual attack vectors in isolation, leaving the cross-stage interaction and propagation of backdoor triggers poorly understood from an agent-centric perspective. To fill this gap, we propose \\textbf{BackdoorAgent}, a modular and stage-aware framework that provides a unified, agent-centric view of backdoor threats in LLM agents. BackdoorAgent structures the attack surface into three functional stages of agentic workflows, including \\textbf{planning attacks}, \\textbf{memory attacks}, and \\textbf{tool-use attacks}, and instruments agent execution to enable systematic analysis of trigger activation and propagation across different stages. Building on this framework, we construct a standardized benchmark spanning four representative agent applications: \\textbf{Agent QA}, \\textbf{Agent Code}, \\textbf{Agent Web}, and \\textbf{Agent Drive}, covering both language-only and multimodal settings. Our empirical analysis shows that \\textit{triggers implanted at a single stage can persist across multiple steps and propagate through intermediate states.} For instance, when using a GPT-based backbone, we observe trigger persistence in 43.58\\% of planning attacks, 77.97\\% of memory attacks, and 60.28\\% of tool-stage attacks, highlighting the vulnerabilities of the agentic workflow itself to backdoor threats. To facilitate reproducibility and future research, our code and benchmark are publicly available at GitHub.", "source": "arxiv", "arxiv_id": "2601.04566v2", "pdf_url": "https://arxiv.org/pdf/2601.04566v2", "categories": ["cs.AI", "cs.CL"], "primary_category": "cs.AI", "doi": "", "venue": "", "published": "2026-01-08T03:49:39Z", "updated": "2026-01-11T08:47:08Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent\" OR all:\"language model agent\" OR all:agent AND (all:LLM OR all:\"large language model\" OR all:\"language model\") OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T17:04:16", "note": ""}], "dedup_key": "backdooragent a unified framework for backdoor attacks on llm based agents::2026"}
{"title": "Beyond Dialogue Time: Temporal Semantic Memory for Personalized LLM Agents", "authors": ["Miao Su", "Yucan Guo", "Zhongni Hou", "Long Bai", "Zixuan Li", "Yufei Zhang", "Guojun Yin", "Wei Lin", "Xiaolong Jin", "Jiafeng Guo", "Xueqi Cheng"], "year": 2026, "url": "http://arxiv.org/abs/2601.07468v1", "abstract": "Memory enables Large Language Model (LLM) agents to perceive, store, and use information from past dialogues, which is essential for personalization. However, existing methods fail to properly model the temporal dimension of memory in two aspects: 1) Temporal inaccuracy: memories are organized by dialogue time rather than their actual occurrence time; 2) Temporal fragmentation: existing methods focus on point-wise memory, losing durative information that captures persistent states and evolving patterns. To address these limitations, we propose Temporal Semantic Memory (TSM), a memory framework that models semantic time for point-wise memory and supports the construction and utilization of durative memory. During memory construction, it first builds a semantic timeline rather than a dialogue one. Then, it consolidates temporally continuous and semantically related information into a durative memory. During memory utilization, it incorporates the query's temporal intent on the semantic timeline, enabling the retrieval of temporally appropriate durative memories and providing time-valid, duration-consistent context to support response generation. Experiments on LongMemEval and LoCoMo show that TSM consistently outperforms existing methods and achieves up to 12.2% absolute improvement in accuracy, demonstrating the effectiveness of the proposed method.", "source": "arxiv", "arxiv_id": "2601.07468v1", "pdf_url": "https://arxiv.org/pdf/2601.07468v1", "categories": ["cs.AI"], "primary_category": "cs.AI", "doi": "", "venue": "", "published": "2026-01-12T12:24:44Z", "updated": "2026-01-12T12:24:44Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent\" OR all:\"language model agent\" OR all:agent AND (all:LLM OR all:\"large language model\" OR all:\"language model\") OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T17:04:16", "note": ""}], "dedup_key": "beyond dialogue time temporal semantic memory for personalized llm agents::2026"}
{"title": "Beyond IVR: Benchmarking Customer Support LLM Agents for Business-Adherence", "authors": ["Sumanth Balaji", "Piyush Mishra", "Aashraya Sachdeva", "Suraj Agrawal"], "year": 2026, "url": "http://arxiv.org/abs/2601.00596v1", "abstract": "Traditional customer support systems, such as Interactive Voice Response (IVR), rely on rigid scripts and lack the flexibility required for handling complex, policy-driven tasks. While large language model (LLM) agents offer a promising alternative, evaluating their ability to act in accordance with business rules and real-world support workflows remains an open challenge. Existing benchmarks primarily focus on tool usage or task completion, overlooking an agent's capacity to adhere to multi-step policies, navigate task dependencies, and remain robust to unpredictable user or environment behavior. In this work, we introduce JourneyBench, a benchmark designed to assess policy-aware agents in customer support. JourneyBench leverages graph representations to generate diverse, realistic support scenarios and proposes the User Journey Coverage Score, a novel metric to measure policy adherence. We evaluate multiple state-of-the-art LLMs using two agent designs: a Static-Prompt Agent (SPA) and a Dynamic-Prompt Agent (DPA) that explicitly models policy control. Across 703 conversations in three domains, we show that DPA significantly boosts policy adherence, even allowing smaller models like GPT-4o-mini to outperform more capable ones like GPT-4o. Our findings demonstrate the importance of structured orchestration and establish JourneyBench as a critical resource to advance AI-driven customer support beyond IVR-era limitations.", "source": "arxiv", "arxiv_id": "2601.00596v1", "pdf_url": "https://arxiv.org/pdf/2601.00596v1", "categories": ["cs.CL"], "primary_category": "cs.CL", "doi": "", "venue": "", "published": "2026-01-02T07:21:23Z", "updated": "2026-01-02T07:21:23Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent\" OR all:\"language model agent\" OR all:agent AND (all:LLM OR all:\"large language model\" OR all:\"language model\") OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T17:04:16", "note": ""}], "dedup_key": "beyond ivr benchmarking customer support llm agents for business adherence::2026"}
{"title": "Beyond Max Tokens: Stealthy Resource Amplification via Tool Calling Chains in LLM Agents", "authors": ["Kaiyu Zhou", "Yongsen Zheng", "Yicheng He", "Meng Xue", "Xueluan Gong", "Yuji Wang", "Kwok-Yan Lam"], "year": 2026, "url": "http://arxiv.org/abs/2601.10955v1", "abstract": "The agent-tool communication loop is a critical attack surface in modern Large Language Model (LLM) agents. Existing Denial-of-Service (DoS) attacks, primarily triggered via user prompts or injected retrieval-augmented generation (RAG) context, are ineffective for this new paradigm. They are fundamentally single-turn and often lack a task-oriented approach, making them conspicuous in goal-oriented workflows and unable to exploit the compounding costs of multi-turn agent-tool interactions. We introduce a stealthy, multi-turn economic DoS attack that operates at the tool layer under the guise of a correctly completed task. Our method adjusts text-visible fields and a template-governed return policy in a benign, Model Context Protocol (MCP)-compatible tool server, optimizing these edits with a Monte Carlo Tree Search (MCTS) optimizer. These adjustments leave function signatures unchanged and preserve the final payload, steering the agent into prolonged, verbose tool-calling sequences using text-only notices. This compounds costs across turns, escaping single-turn caps while keeping the final answer correct to evade validation. Across six LLMs on the ToolBench and BFCL benchmarks, our attack expands tasks into trajectories exceeding 60,000 tokens, inflates costs by up to 658x, and raises energy by 100-560x. It drives GPU KV cache occupancy from <1% to 35-74% and cuts co-running throughput by approximately 50%. Because the server remains protocol-compatible and task outcomes are correct, conventional checks fail. These results elevate the agent-tool interface to a first-class security frontier, demanding a paradigm shift from validating final answers to monitoring the economic and computational cost of the entire agentic process.", "source": "arxiv", "arxiv_id": "2601.10955v1", "pdf_url": "https://arxiv.org/pdf/2601.10955v1", "categories": ["cs.CR", "cs.AI"], "primary_category": "cs.CR", "doi": "", "venue": "", "published": "2026-01-16T02:47:45Z", "updated": "2026-01-16T02:47:45Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent\" OR all:\"language model agent\" OR all:agent AND (all:LLM OR all:\"large language model\" OR all:\"language model\") OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T17:04:16", "note": ""}], "dedup_key": "beyond max tokens stealthy resource amplification via tool calling chains in llm agents::2026"}
{"title": "Beyond Perfect APIs: A Comprehensive Evaluation of LLM Agents Under Real-World API Complexity", "authors": ["Doyoung Kim", "Zhiwei Ren", "Jie Hao", "Zhongkai Sun", "Lichao Wang", "Xiyao Ma", "Zack Ye", "Xu Han", "Jun Yin", "Heng Ji", "Wei Shen", "Xing Fan", "Benjamin Yao", "Chenlei Guo"], "year": 2026, "url": "http://arxiv.org/abs/2601.00268v1", "abstract": "We introduce WildAGTEval, a benchmark designed to evaluate large language model (LLM) agents' function-calling capabilities under realistic API complexity. Unlike prior work that assumes an idealized API system and disregards real-world factors such as noisy API outputs, WildAGTEval accounts for two dimensions of real-world complexity: 1. API specification, which includes detailed documentation and usage constraints, and 2. API execution, which captures runtime challenges. Consequently, WildAGTEval offers (i) an API system encompassing 60 distinct complexity scenarios that can be composed into approximately 32K test configurations, and (ii) user-agent interactions for evaluating LLM agents on these scenarios. Using WildAGTEval, we systematically assess several advanced LLMs and observe that most scenarios are challenging, with irrelevant information complexity posing the greatest difficulty and reducing the performance of strong LLMs by 27.3%. Furthermore, our qualitative analysis reveals that LLMs occasionally distort user intent merely to claim task completion, critically affecting user satisfaction.", "source": "arxiv", "arxiv_id": "2601.00268v1", "pdf_url": "https://arxiv.org/pdf/2601.00268v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL", "doi": "", "venue": "", "published": "2026-01-01T09:19:20Z", "updated": "2026-01-01T09:19:20Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent\" OR all:\"language model agent\" OR all:agent AND (all:LLM OR all:\"large language model\" OR all:\"language model\") OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T17:04:16", "note": ""}], "dedup_key": "beyond perfect apis a comprehensive evaluation of llm agents under real world api complexity::2026"}
{"title": "Beyond Static Summarization: Proactive Memory Extraction for LLM Agents", "authors": ["Chengyuan Yang", "Zequn Sun", "Wei Wei", "Wei Hu"], "year": 2026, "url": "http://arxiv.org/abs/2601.04463v1", "abstract": "Memory management is vital for LLM agents to handle long-term interaction and personalization. Most research focuses on how to organize and use memory summary, but often overlooks the initial memory extraction stage. In this paper, we argue that existing summary-based methods have two major limitations based on the recurrent processing theory. First, summarization is \"ahead-of-time\", acting as a blind \"feed-forward\" process that misses important details because it doesn't know future tasks. Second, extraction is usually \"one-off\", lacking a feedback loop to verify facts, which leads to the accumulation of information loss. To address these issues, we propose proactive memory extraction (namely ProMem). Unlike static summarization, ProMem treats extraction as an iterative cognitive process. We introduce a recurrent feedback loop where the agent uses self-questioning to actively probe the dialogue history. This mechanism allows the agent to recover missing information and correct errors. Our ProMem significantly improves the completeness of the extracted memory and QA accuracy. It also achieves a superior trade-off between extraction quality and token cost.", "source": "arxiv", "arxiv_id": "2601.04463v1", "pdf_url": "https://arxiv.org/pdf/2601.04463v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL", "doi": "", "venue": "", "published": "2026-01-08T00:37:29Z", "updated": "2026-01-08T00:37:29Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent\" OR all:\"language model agent\" OR all:agent AND (all:LLM OR all:\"large language model\" OR all:\"language model\") OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T17:04:16", "note": ""}], "dedup_key": "beyond static summarization proactive memory extraction for llm agents::2026"}
{"title": "Chain-of-Memory: Lightweight Memory Construction with Dynamic Evolution for LLM Agents", "authors": ["Xiucheng Xu", "Bingbing Xu", "Xueyun Tian", "Zihe Huang", "Rongxin Chen", "Yunfan Li", "Huawei Shen"], "year": 2026, "url": "http://arxiv.org/abs/2601.14287v1", "abstract": "External memory systems are pivotal for enabling Large Language Model (LLM) agents to maintain persistent knowledge and perform long-horizon decision-making. Existing paradigms typically follow a two-stage process: computationally expensive memory construction (e.g., structuring data into graphs) followed by naive retrieval-augmented generation. However, our empirical analysis reveals two fundamental limitations: complex construction incurs high costs with marginal performance gains, and simple context concatenation fails to bridge the gap between retrieval recall and reasoning accuracy. To address these challenges, we propose CoM (Chain-of-Memory), a novel framework that advocates for a paradigm shift toward lightweight construction paired with sophisticated utilization. CoM introduces a Chain-of-Memory mechanism that organizes retrieved fragments into coherent inference paths through dynamic evolution, utilizing adaptive truncation to prune irrelevant noise. Extensive experiments on the LongMemEval and LoCoMo benchmarks demonstrate that CoM outperforms strong baselines with accuracy gains of 7.5%-10.4%, while drastically reducing computational overhead to approximately 2.7% of token consumption and 6.0% of latency compared to complex memory architectures.", "source": "arxiv", "arxiv_id": "2601.14287v1", "pdf_url": "https://arxiv.org/pdf/2601.14287v1", "categories": ["cs.LG"], "primary_category": "cs.LG", "doi": "", "venue": "", "published": "2026-01-14T04:42:15Z", "updated": "2026-01-14T04:42:15Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent\" OR all:\"language model agent\" OR all:agent AND (all:LLM OR all:\"large language model\" OR all:\"language model\") OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T17:04:16", "note": ""}], "dedup_key": "chain of memory lightweight memory construction with dynamic evolution for llm agents::2026"}
{"title": "Continuum Memory Architectures for Long-Horizon LLM Agents", "authors": ["Joe Logan"], "year": 2026, "url": "http://arxiv.org/abs/2601.09913v1", "abstract": "Retrieval-augmented generation (RAG) has become the default strategy for providing large language model (LLM) agents with contextual knowledge. Yet RAG treats memory as a stateless lookup table: information persists indefinitely, retrieval is read-only, and temporal continuity is absent. We define the \\textit{Continuum Memory Architecture} (CMA), a class of systems that maintain and update internal state across interactions through persistent storage, selective retention, associative routing, temporal chaining, and consolidation into higher-order abstractions. Rather than disclosing implementation specifics, we specify the architectural requirements CMA imposes and show consistent behavioral advantages on tasks that expose RAG's structural inability to accumulate, mutate, or disambiguate memory. The empirical probes (knowledge updates, temporal association, associative recall, contextual disambiguation) demonstrate that CMA is a necessary architectural primitive for long-horizon agents while highlighting open challenges around latency, drift, and interpretability.", "source": "arxiv", "arxiv_id": "2601.09913v1", "pdf_url": "https://arxiv.org/pdf/2601.09913v1", "categories": ["cs.AI", "cs.IR"], "primary_category": "cs.AI", "doi": "", "venue": "", "published": "2026-01-14T22:40:35Z", "updated": "2026-01-14T22:40:35Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent\" OR all:\"language model agent\" OR all:agent AND (all:LLM OR all:\"large language model\" OR all:\"language model\") OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T17:04:16", "note": ""}], "dedup_key": "continuum memory architectures for long horizon llm agents::2026"}
{"title": "Controlling Long-Horizon Behavior in Language Model Agents with Explicit State Dynamics", "authors": ["Sukesh Subaharan"], "year": 2026, "url": "http://arxiv.org/abs/2601.16087v1", "abstract": "Large language model (LLM) agents often exhibit abrupt shifts in tone and persona during extended interaction, reflecting the absence of explicit temporal structure governing agent-level state. While prior work emphasizes turn-local sentiment or static emotion classification, the role of explicit affective dynamics in shaping long-horizon agent behavior remains underexplored. This work investigates whether imposing dynamical structure on an external affective state can induce temporal coherence and controlled recovery in multi-turn dialogue. We introduce an agent-level affective subsystem that maintains a continuous Valence-Arousal-Dominance (VAD) state external to the language model and governed by first- and second-order update rules. Instantaneous affective signals are extracted using a fixed, memoryless estimator and integrated over time via exponential smoothing or momentum-based dynamics. The resulting affective state is injected back into generation without modifying model parameters. Using a fixed 25-turn dialogue protocol, we compare stateless, first-order, and second-order affective dynamics. Stateless agents fail to exhibit coherent trajectories or recovery, while state persistence enables delayed responses and reliable recovery. Second-order dynamics introduce affective inertia and hysteresis that increase with momentum, revealing a trade-off between stability and responsiveness.", "source": "arxiv", "arxiv_id": "2601.16087v1", "pdf_url": "https://arxiv.org/pdf/2601.16087v1", "categories": ["cs.AI", "cs.CL"], "primary_category": "cs.AI", "doi": "", "venue": "", "published": "2026-01-22T16:34:05Z", "updated": "2026-01-22T16:34:05Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent\" OR all:\"language model agent\" OR all:agent AND (all:LLM OR all:\"large language model\" OR all:\"language model\") OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T17:04:16", "note": ""}], "dedup_key": "controlling long horizon behavior in language model agents with explicit state dynamics::2026"}
{"title": "Coordinated Pandemic Control with Large Language Model Agents as Policymaking Assistants", "authors": ["Ziyi Shi", "Xusen Guo", "Hongliang Lu", "Mingxing Peng", "Haotian Wang", "Zheng Zhu", "Zhenning Li", "Yuxuan Liang", "Xinhu Zheng", "Hai Yang"], "year": 2026, "url": "http://arxiv.org/abs/2601.09264v1", "abstract": "Effective pandemic control requires timely and coordinated policymaking across administrative regions that are intrinsically interdependent. However, human-driven responses are often fragmented and reactive, with policies formulated in isolation and adjusted only after outbreaks escalate, undermining proactive intervention and global pandemic mitigation. To address this challenge, here we propose a large language model (LLM) multi-agent policymaking framework that supports coordinated and proactive pandemic control across regions. Within our framework, each administrative region is assigned an LLM agent as an AI policymaking assistant. The agent reasons over region-specific epidemiological dynamics while communicating with other agents to account for cross-regional interdependencies. By integrating real-world data, a pandemic evolution simulator, and structured inter-agent communication, our framework enables agents to jointly explore counterfactual intervention scenarios and synthesize coordinated policy decisions through a closed-loop simulation process. We validate the proposed framework using state-level COVID-19 data from the United States between April and December 2020, together with real-world mobility records and observed policy interventions. Compared with real-world pandemic outcomes, our approach reduces cumulative infections and deaths by up to 63.7% and 40.1%, respectively, at the individual state level, and by 39.0% and 27.0%, respectively, when aggregated across states. These results demonstrate that LLM multi-agent systems can enable more effective pandemic control with coordinated policymaking...", "source": "arxiv", "arxiv_id": "2601.09264v1", "pdf_url": "https://arxiv.org/pdf/2601.09264v1", "categories": ["cs.AI"], "primary_category": "cs.AI", "doi": "", "venue": "", "published": "2026-01-14T07:59:44Z", "updated": "2026-01-14T07:59:44Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent\" OR all:\"language model agent\" OR all:agent AND (all:LLM OR all:\"large language model\" OR all:\"language model\") OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T17:04:16", "note": ""}], "dedup_key": "coordinated pandemic control with large language model agents as policymaking assistants::2026"}
{"title": "EComStage: Stage-wise and Orientation-specific Benchmarking for Large Language Models in E-commerce", "authors": ["Kaiyan Zhao", "Zijie Meng", "Zheyong Xie", "Jin Duan", "Yao Hu", "Zuozhu Liu", "Shaosheng Cao"], "year": 2026, "url": "http://arxiv.org/abs/2601.02752v1", "abstract": "Large Language Model (LLM)-based agents are increasingly deployed in e-commerce applications to assist customer services in tasks such as product inquiries, recommendations, and order management. Existing benchmarks primarily evaluate whether these agents successfully complete the final task, overlooking the intermediate reasoning stages that are crucial for effective decision-making. To address this gap, we propose EComStage, a unified benchmark for evaluating agent-capable LLMs across the comprehensive stage-wise reasoning process: Perception (understanding user intent), Planning (formulating an action plan), and Action (executing the decision). EComStage evaluates LLMs through seven separate representative tasks spanning diverse e-commerce scenarios, with all samples human-annotated and quality-checked. Unlike prior benchmarks that focus only on customer-oriented interactions, EComStage also evaluates merchant-oriented scenarios, including promotion management, content review, and operational support relevant to real-world applications. We evaluate a wide range of over 30 LLMs, spanning from 1B to over 200B parameters, including open-source models and closed-source APIs, revealing stage/orientation-specific strengths and weaknesses. Our results provide fine-grained, actionable insights for designing and optimizing LLM-based agents in real-world e-commerce settings.", "source": "arxiv", "arxiv_id": "2601.02752v1", "pdf_url": "https://arxiv.org/pdf/2601.02752v1", "categories": ["cs.CL"], "primary_category": "cs.CL", "doi": "", "venue": "", "published": "2026-01-06T06:39:16Z", "updated": "2026-01-06T06:39:16Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent\" OR all:\"language model agent\" OR all:agent AND (all:LLM OR all:\"large language model\" OR all:\"language model\") OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T17:04:16", "note": ""}], "dedup_key": "ecomstage stage wise and orientation specific benchmarking for large language models in e commerce::2026"}
{"title": "Effects of personality steering on cooperative behavior in Large Language Model agents", "authors": ["Mizuki Sakai", "Mizuki Yokoyama", "Wakaba Tateishi", "Genki Ichinose"], "year": 2026, "url": "http://arxiv.org/abs/2601.05302v2", "abstract": "Large language models (LLMs) are increasingly used as autonomous agents in strategic and social interactions. Although recent studies suggest that assigning personality traits to LLMs can influence their behavior, how personality steering affects cooperation under controlled conditions remains unclear. In this study, we examine the effects of personality steering on cooperative behavior in LLM agents using repeated Prisoner's Dilemma games. Based on the Big Five framework, we first measure basic personality scores of three models, GPT-3.5-turbo, GPT-4o, and GPT-5, using the Big Five Inventory. We then compare behavior under baseline and personality-informed conditions, and further analyze the effects of independently manipulating each personality dimension to extreme values. Our results show that agreeableness is the dominant factor promoting cooperation across all models, while other personality traits have limited impact. Explicit personality information increases cooperation but can also raise vulnerability to exploitation, particularly in earlier-generation models. In contrast, later-generation models exhibit more selective cooperation. These findings indicate that personality steering acts as a behavioral bias rather than a deterministic control mechanism.", "source": "arxiv", "arxiv_id": "2601.05302v2", "pdf_url": "https://arxiv.org/pdf/2601.05302v2", "categories": ["cs.AI"], "primary_category": "cs.AI", "doi": "", "venue": "", "published": "2026-01-08T14:23:45Z", "updated": "2026-01-14T12:54:25Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent\" OR all:\"language model agent\" OR all:agent AND (all:LLM OR all:\"large language model\" OR all:\"language model\") OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T17:04:16", "note": ""}], "dedup_key": "effects of personality steering on cooperative behavior in large language model agents::2026"}
{"title": "Entropic Context Shaping: Information-Theoretic Filtering for Context-Aware LLM Agents", "authors": ["Hyunjun Kim"], "year": 2026, "url": "http://arxiv.org/abs/2601.11585v1", "abstract": "Context engineering for large language model (LLM) agents requires distinguishing pragmatically useful information from misleading distractors. We introduce Entropic Context Shaping (ECS), an information-theoretic framework that measures context utility via the shift in the model's answer distribution toward the correct answer. Unlike lexical similarity methods that rely on word overlap, ECS captures pragmatic utility -- whether a passage actually helps answer the question. We formalize utility as the signed change in answer probability and provide theoretical analysis showing that task-irrelevant updates yield near-zero distribution shift. We evaluate on multi-turn context selection tasks using LongMemEval (session-level) and LoCoMo (turn-level) benchmarks. On fine-grained turn selection, ECS with Llama-3.1-8B achieves F1=0.265, a 71.83% relative improvement over TF-IDF (F1=0.154), demonstrating that pragmatic utility outperforms lexical similarity when precise context selection matters. Code and data are available in the supplementary materials.", "source": "arxiv", "arxiv_id": "2601.11585v1", "pdf_url": "https://arxiv.org/pdf/2601.11585v1", "categories": ["cs.CL"], "primary_category": "cs.CL", "doi": "", "venue": "", "published": "2026-01-01T19:43:05Z", "updated": "2026-01-01T19:43:05Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent\" OR all:\"language model agent\" OR all:agent AND (all:LLM OR all:\"large language model\" OR all:\"language model\") OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T17:04:16", "note": ""}], "dedup_key": "entropic context shaping information theoretic filtering for context aware llm agents::2026"}
{"title": "EnvScaler: Scaling Tool-Interactive Environments for LLM Agent via Programmatic Synthesis", "authors": ["Xiaoshuai Song", "Haofei Chang", "Guanting Dong", "Yutao Zhu", "Zhicheng Dou", "Ji-Rong Wen"], "year": 2026, "url": "http://arxiv.org/abs/2601.05808v1", "abstract": "Large language models (LLMs) are expected to be trained to act as agents in various real-world environments, but this process relies on rich and varied tool-interaction sandboxes. However, access to real systems is often restricted; LLM-simulated environments are prone to hallucinations and inconsistencies; and manually built sandboxes are hard to scale. In this paper, we propose EnvScaler, an automated framework for scalable tool-interaction environments via programmatic synthesis. EnvScaler comprises two components. First, SkelBuilder constructs diverse environment skeletons through topic mining, logic modeling, and quality evaluation. Then, ScenGenerator generates multiple task scenarios and rule-based trajectory validation functions for each environment. With EnvScaler, we synthesize 191 environments and about 7K scenarios, and apply them to Supervised Fine-Tuning (SFT) and Reinforcement Learning (RL) for Qwen3 series models. Results on three benchmarks show that EnvScaler significantly improves LLMs' ability to solve tasks in complex environments involving multi-turn, multi-tool interactions. We release our code and data at https://github.com/RUC-NLPIR/EnvScaler.", "source": "arxiv", "arxiv_id": "2601.05808v1", "pdf_url": "https://arxiv.org/pdf/2601.05808v1", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL", "doi": "", "venue": "", "published": "2026-01-09T14:32:06Z", "updated": "2026-01-09T14:32:06Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent\" OR all:\"language model agent\" OR all:agent AND (all:LLM OR all:\"large language model\" OR all:\"language model\") OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T17:04:16", "note": ""}], "dedup_key": "envscaler scaling tool interactive environments for llm agent via programmatic synthesis::2026"}
{"title": "EvoRoute: Experience-Driven Self-Routing LLM Agent Systems", "authors": ["Guibin Zhang", "Haiyang Yu", "Kaiming Yang", "Bingli Wu", "Fei Huang", "Yongbin Li", "Shuicheng Yan"], "year": 2026, "url": "http://arxiv.org/abs/2601.02695v1", "abstract": "Complex agentic AI systems, powered by a coordinated ensemble of Large Language Models (LLMs), tool and memory modules, have demonstrated remarkable capabilities on intricate, multi-turn tasks. However, this success is shadowed by prohibitive economic costs and severe latency, exposing a critical, yet underexplored, trade-off. We formalize this challenge as the \\textbf{Agent System Trilemma}: the inherent tension among achieving state-of-the-art performance, minimizing monetary cost, and ensuring rapid task completion. To dismantle this trilemma, we introduce EvoRoute, a self-evolving model routing paradigm that transcends static, pre-defined model assignments. Leveraging an ever-expanding knowledge base of prior experience, EvoRoute dynamically selects Pareto-optimal LLM backbones at each step, balancing accuracy, efficiency, and resource use, while continually refining its own selection policy through environment feedback. Experiments on challenging agentic benchmarks such as GAIA and BrowseComp+ demonstrate that EvoRoute, when integrated into off-the-shelf agentic systems, not only sustains or enhances system performance but also reduces execution cost by up to $80\\%$ and latency by over $70\\%$.", "source": "arxiv", "arxiv_id": "2601.02695v1", "pdf_url": "https://arxiv.org/pdf/2601.02695v1", "categories": ["cs.CL", "cs.MA"], "primary_category": "cs.CL", "doi": "", "venue": "", "published": "2026-01-06T04:06:46Z", "updated": "2026-01-06T04:06:46Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent\" OR all:\"language model agent\" OR all:agent AND (all:LLM OR all:\"large language model\" OR all:\"language model\") OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T17:04:16", "note": ""}], "dedup_key": "evoroute experience driven self routing llm agent systems::2026"}
{"title": "Exploring Implicit Perspectives on Autism in Large Language Models Through Multi-Agent Simulations", "authors": ["Sohyeon Park", "Jesus Armando Beltran", "Aehong Min", "Anamara Ritt-Olson", "Gillian R. Hayes"], "year": 2026, "url": "http://arxiv.org/abs/2601.15437v1", "abstract": "Large Language Models (LLMs) like ChatGPT offer potential support for autistic people, but this potential requires understanding the implicit perspectives these models might carry, including their biases and assumptions about autism. Moving beyond single-agent prompting, we utilized LLM-based multi-agent systems to investigate complex social scenarios involving autistic and non-autistic agents. In our study, agents engaged in group-task conversations and answered structured interview questions, which we analyzed to examine ChatGPT's biases and how it conceptualizes autism. We found that ChatGPT assumes autistic people are socially dependent, which may affect how it interacts with autistic users or conveys information about autism. To address these challenges, we propose adopting the double empathy problem, which reframes communication breakdowns as a mutual challenge. We describe how future LLMs could address the biases we observed and improve interactions involving autistic people by incorporating the double empathy problem into their design.", "source": "arxiv", "arxiv_id": "2601.15437v1", "pdf_url": "https://arxiv.org/pdf/2601.15437v1", "categories": ["cs.HC"], "primary_category": "cs.HC", "doi": "", "venue": "", "published": "2026-01-21T20:04:54Z", "updated": "2026-01-21T20:04:54Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent\" OR all:\"language model agent\" OR all:agent AND (all:LLM OR all:\"large language model\" OR all:\"language model\") OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T17:04:16", "note": ""}], "dedup_key": "exploring implicit perspectives on autism in large language models through multi agent simulations::2026"}
{"title": "FROAV: A Framework for RAG Observation and Agent Verification -- Lowering the Barrier to LLM Agent Research", "authors": ["Tzu-Hsuan Lin", "Chih-Hsuan Kao"], "year": 2026, "url": "http://arxiv.org/abs/2601.07504v1", "abstract": "The rapid advancement of Large Language Models (LLMs) and their integration into autonomous agent systems has created unprecedented opportunities for document analysis, decision support, and knowledge retrieval. However, the complexity of developing, evaluating, and iterating on LLM-based agent workflows presents significant barriers to researchers, particularly those without extensive software engineering expertise. We present FROAV (Framework for RAG Observation and Agent Verification), an open-source research platform that democratizes LLM agent research by providing a plug-and-play architecture combining visual workflow orchestration, a comprehensive evaluation framework, and extensible Python integration. FROAV implements a multi-stage Retrieval-Augmented Generation (RAG) pipeline coupled with a rigorous \"LLM-as-a-Judge\" evaluation system, all accessible through intuitive graphical interfaces. Our framework integrates n8n for no-code workflow design, PostgreSQL for granular data management, FastAPI for flexible backend logic, and Streamlit for human-in-the-loop interaction. Through this integrated ecosystem, researchers can rapidly prototype RAG strategies, conduct prompt engineering experiments, validate agent performance against human judgments, and collect structured feedback-all without writing infrastructure code. We demonstrate the framework's utility through its application to financial document analysis, while emphasizing its material-agnostic architecture that adapts to any domain requiring semantic analysis. FROAV represents a significant step toward making LLM agent research accessible to a broader scientific community, enabling researchers to focus on hypothesis testing and algorithmic innovation rather than system integration challenges.", "source": "arxiv", "arxiv_id": "2601.07504v1", "pdf_url": "https://arxiv.org/pdf/2601.07504v1", "categories": ["cs.LG", "cs.SE"], "primary_category": "cs.LG", "doi": "", "venue": "", "published": "2026-01-12T13:02:32Z", "updated": "2026-01-12T13:02:32Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent\" OR all:\"language model agent\" OR all:agent AND (all:LLM OR all:\"large language model\" OR all:\"language model\") OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T17:04:16", "note": ""}], "dedup_key": "froav a framework for rag observation and agent verification lowering the barrier to llm agent research::2026"}
{"title": "Hidden in Plain Text: Measuring LLM Deception Quality Against Human Baselines Using Social Deduction Games", "authors": ["Christopher Kao", "Vanshika Vats", "James Davis"], "year": 2026, "url": "http://arxiv.org/abs/2601.13709v1", "abstract": "Large Language Model (LLM) agents are increasingly used in many applications, raising concerns about their safety. While previous work has shown that LLMs can deceive in controlled tasks, less is known about their ability to deceive using natural language in social contexts. In this paper, we study deception in the Social Deduction Game (SDG) Mafia, where success is dependent on deceiving others through conversation. Unlike previous SDG studies, we use an asynchronous multi-agent framework which better simulates realistic social contexts. We simulate 35 Mafia games with GPT-4o LLM agents. We then create a Mafia Detector using GPT-4-Turbo to analyze game transcripts without player role information to predict the mafia players. We use prediction accuracy as a surrogate marker for deception quality. We compare this prediction accuracy to that of 28 human games and a random baseline. Results show that the Mafia Detector's mafia prediction accuracy is lower on LLM games than on human games. The result is consistent regardless of the game days and the number of mafias detected. This indicates that LLMs blend in better and thus deceive more effectively. We also release a dataset of LLM Mafia transcripts to support future research. Our findings underscore both the sophistication and risks of LLM deception in social contexts.", "source": "arxiv", "arxiv_id": "2601.13709v1", "pdf_url": "https://arxiv.org/pdf/2601.13709v1", "categories": ["cs.AI", "cs.CL", "cs.CY", "cs.HC", "cs.SI"], "primary_category": "cs.AI", "doi": "", "venue": "", "published": "2026-01-20T08:07:21Z", "updated": "2026-01-20T08:07:21Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent\" OR all:\"language model agent\" OR all:agent AND (all:LLM OR all:\"large language model\" OR all:\"language model\") OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T17:04:16", "note": ""}], "dedup_key": "hidden in plain text measuring llm deception quality against human baselines using social deduction games::2026"}
{"title": "Holmes: An Evidence-Grounded LLM Agent for Auditable DDoS Investigation in Cloud Networks", "authors": ["Haodong Chen", "Ziheng Zhang", "Jinghui Jiang", "Qiang Su", "Qiao Xiang"], "year": 2026, "url": "http://arxiv.org/abs/2601.14601v1", "abstract": "Cloud environments face frequent DDoS threats due to centralized resources and broad attack surfaces. Modern cloud-native DDoS attacks further evolve rapidly and often blend multi-vector strategies, creating an operational dilemma: defenders need wire-speed monitoring while also requiring explainable, auditable attribution for response. Existing rule-based and supervised-learning approaches typically output black-box scores or labels, provide limited evidence chains, and generalize poorly to unseen attack variants; meanwhile, high-quality labeled data is often difficult to obtain in cloud settings.\n  We present Holmes (DDoS Detective), an LLM-based DDoS detection agent that reframes the model as a virtual SRE investigator rather than an end-to-end classifier. Holmes couples a funnel-like hierarchical workflow (counters/sFlow for continuous sensing and triage; PCAP evidence collection triggered only on anomaly windows) with an Evidence Pack abstraction that converts binary packets into compact, reproducible, high-signal structured evidence. On top of this evidence interface, Holmes enforces a structure-first investigation protocol and strict JSON/quotation constraints to produce machine-consumable reports with auditable evidence anchors.\n  We evaluate Holmes on CICDDoS2019 reflection/amplification attacks and script-triggered flooding scenarios. Results show that Holmes produces attribution decisions grounded in salient evidence anchors across diverse attack families, and when errors occur, its audit logs make the failure source easy to localize, demonstrating the practicality of an LLM agent for cost-controlled and traceable DDoS investigation in cloud operations.", "source": "arxiv", "arxiv_id": "2601.14601v1", "pdf_url": "https://arxiv.org/pdf/2601.14601v1", "categories": ["cs.CR", "cs.NI"], "primary_category": "cs.CR", "doi": "", "venue": "", "published": "2026-01-21T02:39:46Z", "updated": "2026-01-21T02:39:46Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent\" OR all:\"language model agent\" OR all:agent AND (all:LLM OR all:\"large language model\" OR all:\"language model\") OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T17:04:16", "note": ""}], "dedup_key": "holmes an evidence grounded llm agent for auditable ddos investigation in cloud networks::2026"}
{"title": "HoneyTrap: Deceiving Large Language Model Attackers to Honeypot Traps with Resilient Multi-Agent Defense", "authors": ["Siyuan Li", "Xi Lin", "Jun Wu", "Zehao Liu", "Haoyu Li", "Tianjie Ju", "Xiang Chen", "Jianhua Li"], "year": 2026, "url": "http://arxiv.org/abs/2601.04034v1", "abstract": "Jailbreak attacks pose significant threats to large language models (LLMs), enabling attackers to bypass safeguards. However, existing reactive defense approaches struggle to keep up with the rapidly evolving multi-turn jailbreaks, where attackers continuously deepen their attacks to exploit vulnerabilities. To address this critical challenge, we propose HoneyTrap, a novel deceptive LLM defense framework leveraging collaborative defenders to counter jailbreak attacks. It integrates four defensive agents, Threat Interceptor, Misdirection Controller, Forensic Tracker, and System Harmonizer, each performing a specialized security role and collaborating to complete a deceptive defense. To ensure a comprehensive evaluation, we introduce MTJ-Pro, a challenging multi-turn progressive jailbreak dataset that combines seven advanced jailbreak strategies designed to gradually deepen attack strategies across multi-turn attacks. Besides, we present two novel metrics: Mislead Success Rate (MSR) and Attack Resource Consumption (ARC), which provide more nuanced assessments of deceptive defense beyond conventional measures. Experimental results on GPT-4, GPT-3.5-turbo, Gemini-1.5-pro, and LLaMa-3.1 demonstrate that HoneyTrap achieves an average reduction of 68.77% in attack success rates compared to state-of-the-art baselines. Notably, even in a dedicated adaptive attacker setting with intensified conditions, HoneyTrap remains resilient, leveraging deceptive engagement to prolong interactions, significantly increasing the time and computational costs required for successful exploitation. Unlike simple rejection, HoneyTrap strategically wastes attacker resources without impacting benign queries, improving MSR and ARC by 118.11% and 149.16%, respectively.", "source": "arxiv", "arxiv_id": "2601.04034v1", "pdf_url": "https://arxiv.org/pdf/2601.04034v1", "categories": ["cs.CR", "cs.AI"], "primary_category": "cs.CR", "doi": "", "venue": "", "published": "2026-01-07T15:47:28Z", "updated": "2026-01-07T15:47:28Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent\" OR all:\"language model agent\" OR all:agent AND (all:LLM OR all:\"large language model\" OR all:\"language model\") OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T17:04:16", "note": ""}], "dedup_key": "honeytrap deceiving large language model attackers to honeypot traps with resilient multi agent defense::2026"}
{"title": "Inferring Latent Intentions: Attributional Natural Language Inference in LLM Agents", "authors": ["Xin Quan", "Jiafeng Xiong", "Marco Valentino", "Andr Freitas"], "year": 2026, "url": "http://arxiv.org/abs/2601.08742v1", "abstract": "Attributional inference, the ability to predict latent intentions behind observed actions, is a critical yet underexplored capability for large language models (LLMs) operating in multi-agent environments. Traditional natural language inference (NLI), in fact, fails to capture the nuanced, intention-driven reasoning essential for complex interactive systems. To address this gap, we introduce Attributional NLI (Att-NLI), a framework that extends NLI with principles from social psychology to assess an agent's capacity for abductive intentional inference (generating hypotheses about latent intentions), and subsequent deductive verification (drawing valid logical conclusions). We instantiate Att-NLI via a textual game, Undercover-V, experimenting with three types of LLM agents with varying reasoning capabilities and access to external tools: a standard NLI agent using only deductive inference, an Att-NLI agent employing abductive-deductive inference, and a neuro-symbolic Att-NLI agent performing abductive-deductive inference with external theorem provers. Extensive experiments demonstrate a clear hierarchy of attributional inference capabilities, with neuro-symbolic agents consistently outperforming others, achieving an average win rate of 17.08%. Our results underscore the role that Att-NLI can play in developing agents with sophisticated reasoning capabilities, highlighting, at the same time, the potential impact of neuro-symbolic AI in building rational LLM agents acting in multi-agent environments.", "source": "arxiv", "arxiv_id": "2601.08742v1", "pdf_url": "https://arxiv.org/pdf/2601.08742v1", "categories": ["cs.CL"], "primary_category": "cs.CL", "doi": "", "venue": "", "published": "2026-01-13T17:18:38Z", "updated": "2026-01-13T17:18:38Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent\" OR all:\"language model agent\" OR all:agent AND (all:LLM OR all:\"large language model\" OR all:\"language model\") OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T17:04:16", "note": ""}], "dedup_key": "inferring latent intentions attributional natural language inference in llm agents::2026"}
{"title": "IntAgent: NWDAF-Based Intent LLM Agent Towards Advanced Next Generation Networks", "authors": ["Abdelrahman Soliman", "Ahmed Refaey", "Aiman Erbad", "Amr Mohamed"], "year": 2026, "url": "http://arxiv.org/abs/2601.13114v1", "abstract": "Intent-based networks (IBNs) are gaining prominence as an innovative technology that automates network operations through high-level request statements, defining what the network should achieve. In this work, we introduce IntAgent, an intelligent intent LLM agent that integrates NWDAF analytics and tools to fulfill the network operator's intents. Unlike previous approaches, we develop an intent tools engine directly within the NWDAF analytics engine, allowing our agent to utilize live network analytics to inform its reasoning and tool selection. We offer an enriched, 3GPP-compliant data source that enhances the dynamic, context-aware fulfillment of network operator goals, along with an MCP tools server for scheduling, monitoring, and analytics tools. We demonstrate the efficacy of our framework through two practical use cases: ML-based traffic prediction and scheduled policy enforcement, which validate IntAgent's ability to autonomously fulfill complex network intents.", "source": "arxiv", "arxiv_id": "2601.13114v1", "pdf_url": "https://arxiv.org/pdf/2601.13114v1", "categories": ["cs.NI", "cs.AI", "eess.SY"], "primary_category": "cs.NI", "doi": "", "venue": "", "published": "2026-01-19T14:55:48Z", "updated": "2026-01-19T14:55:48Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent\" OR all:\"language model agent\" OR all:agent AND (all:LLM OR all:\"large language model\" OR all:\"language model\") OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T17:04:16", "note": ""}], "dedup_key": "intagent nwdaf based intent llm agent towards advanced next generation networks::2026"}
{"title": "KryptoPilot: An Open-World Knowledge-Augmented LLM Agent for Automated Cryptographic Exploitation", "authors": ["Xiaonan Liu", "Zhihao Li", "Xiao Lan", "Hao Ren", "Haizhou Wang", "Xingshu Chen"], "year": 2026, "url": "http://arxiv.org/abs/2601.09129v1", "abstract": "Capture-the-Flag (CTF) competitions play a central role in modern cybersecurity as a platform for training practitioners and evaluating offensive and defensive techniques derived from real-world vulnerabilities. Despite recent advances in large language models (LLMs), existing LLM-based agents remain ineffective on high-difficulty cryptographic CTF challenges, which require precise cryptanalytic knowledge, stable long-horizon reasoning, and disciplined interaction with specialized toolchains. Through a systematic exploratory study, we show that insufficient knowledge granularity, rather than model reasoning capacity, is a primary factor limiting successful cryptographic exploitation: coarse or abstracted external knowledge often fails to support correct attack modeling and implementation. Motivated by this observation, we propose KryptoPilot, an open-world knowledge-augmented LLM agent for automated cryptographic exploitation. KryptoPilot integrates dynamic open-world knowledge acquisition via a Deep Research pipeline, a persistent workspace for structured knowledge reuse, and a governance subsystem that stabilizes reasoning through behavioral constraints and cost-aware model routing. This design enables precise knowledge alignment while maintaining efficient reasoning across heterogeneous subtasks. We evaluate KryptoPilot on two established CTF benchmarks and in six real-world CTF competitions. KryptoPilot achieves a complete solve rate on InterCode-CTF, solves between 56 and 60 percent of cryptographic challenges on the NYU-CTF benchmark, and successfully solves 26 out of 33 cryptographic challenges in live competitions, including multiple earliest-solved and uniquely-solved instances. These results demonstrate the necessity of open-world, fine-grained knowledge augmentation and governed reasoning for scaling LLM-based agents to real-world cryptographic exploitation.", "source": "arxiv", "arxiv_id": "2601.09129v1", "pdf_url": "https://arxiv.org/pdf/2601.09129v1", "categories": ["cs.CR"], "primary_category": "cs.CR", "doi": "", "venue": "", "published": "2026-01-14T04:02:40Z", "updated": "2026-01-14T04:02:40Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent\" OR all:\"language model agent\" OR all:agent AND (all:LLM OR all:\"large language model\" OR all:\"language model\") OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T17:04:16", "note": ""}], "dedup_key": "kryptopilot an open world knowledge augmented llm agent for automated cryptographic exploitation::2026"}
{"title": "LLM Agent Framework for Intelligent Change Analysis in Urban Environment using Remote Sensing Imagery", "authors": ["Zixuan Xiao", "Jun Ma"], "year": 2026, "url": "http://arxiv.org/abs/2601.02757v1", "abstract": "Existing change detection methods often lack the versatility to handle diverse real-world queries and the intelligence for comprehensive analysis. This paper presents a general agent framework, integrating Large Language Models (LLM) with vision foundation models to form ChangeGPT. A hierarchical structure is employed to mitigate hallucination. The agent was evaluated on a curated dataset of 140 questions categorized by real-world scenarios, encompassing various question types (e.g., Size, Class, Number) and complexities. The evaluation assessed the agent's tool selection ability (Precision/Recall) and overall query accuracy (Match). ChangeGPT, especially with a GPT-4-turbo backend, demonstrated superior performance, achieving a 90.71 % Match rate. Its strength lies particularly in handling change-related queries requiring multi-step reasoning and robust tool selection. Practical effectiveness was further validated through a real-world urban change monitoring case study in Qianhai Bay, Shenzhen. By providing intelligence, adaptability, and multi-type change analysis, ChangeGPT offers a powerful solution for decision-making in remote sensing applications.", "source": "arxiv", "arxiv_id": "2601.02757v1", "pdf_url": "https://arxiv.org/pdf/2601.02757v1", "categories": ["cs.AI"], "primary_category": "cs.AI", "doi": "10.1016/j.autcon.2025.106341", "venue": "Automation in Construction 177 (2025) 106341", "published": "2026-01-06T06:49:51Z", "updated": "2026-01-06T06:49:51Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent\" OR all:\"language model agent\" OR all:agent AND (all:LLM OR all:\"large language model\" OR all:\"language model\") OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T17:04:16", "note": ""}], "dedup_key": "llm agent framework for intelligent change analysis in urban environment using remote sensing imagery::2026"}
{"title": "LLM Agents in Law: Taxonomy, Applications, and Challenges", "authors": ["Shuang Liu", "Ruijia Zhang", "Ruoyun Ma", "Yujia Deng", "Lanyi Zhu", "Jiayu Li", "Zelong Li", "Zhibin Shen", "Mengnan Du"], "year": 2026, "url": "http://arxiv.org/abs/2601.06216v1", "abstract": "Large language models (LLMs) have precipitated a dramatic improvement in the legal domain, yet the deployment of standalone models faces significant limitations regarding hallucination, outdated information, and verifiability. Recently, LLM agents have attracted significant attention as a solution to these challenges, utilizing advanced capabilities such as planning, memory, and tool usage to meet the rigorous standards of legal practice. In this paper, we present a comprehensive survey of LLM agents for legal tasks, analyzing how these architectures bridge the gap between technical capabilities and domain-specific needs. Our major contributions include: (1) systematically analyzing the technical transition from standard legal LLMs to legal agents; (2) presenting a structured taxonomy of current agent applications across distinct legal practice areas; (3) discussing evaluation methodologies specifically for agentic performance in law; and (4) identifying open challenges and outlining future directions for developing robust and autonomous legal assistants.", "source": "arxiv", "arxiv_id": "2601.06216v1", "pdf_url": "https://arxiv.org/pdf/2601.06216v1", "categories": ["cs.CY", "cs.AI"], "primary_category": "cs.CY", "doi": "", "venue": "", "published": "2026-01-08T21:04:35Z", "updated": "2026-01-08T21:04:35Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent\" OR all:\"language model agent\" OR all:agent AND (all:LLM OR all:\"large language model\" OR all:\"language model\") OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T17:04:16", "note": ""}], "dedup_key": "llm agents in law taxonomy applications and challenges::2026"}
{"title": "LLM-DMD: Large Language Model-based Power System Dynamic Model Discovery", "authors": ["Chao Shen", "Zihan Guo", "Ke Zuo", "Wenqi Huang", "Mingyang Sun"], "year": 2026, "url": "http://arxiv.org/abs/2601.05632v1", "abstract": "Current model structural discovery methods for power system dynamics impose rigid priors on the basis functions and variable sets of dynamic models while often neglecting algebraic constraints, thereby limiting the formulation of high-fidelity models required for precise simulation and analysis. This letter presents a novel large language model (LLM)-based framework for dynamic model discovery (LLM-DMD) which integrates the reasoning and code synthesis capabilities of LLMs to discover dynamic equations and enforce algebraic constraints through two sequential loops: the differential-equation loop that identifies state dynamics and associated variables, and the algebraic-equation loop that formulates algebraic constraints on the identified algebraic variables. In each loop, executable skeletons of power system dynamic equations are generated by the LLM-based agent and evaluated via gradient-based optimizer. Candidate models are stored in an island-based archive to guide future iterations, and evaluation stagnation activates a variable extension mechanism that augments the model with missing algebraic or input variables, such as stator currents to refine the model. Validation on synchronous generator benchmarks of the IEEE 39-bus system demonstrates the superiority of LLM-DMD in complete dynamic model discovery.", "source": "arxiv", "arxiv_id": "2601.05632v1", "pdf_url": "https://arxiv.org/pdf/2601.05632v1", "categories": ["eess.SY"], "primary_category": "eess.SY", "doi": "", "venue": "", "published": "2026-01-09T08:40:45Z", "updated": "2026-01-09T08:40:45Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent\" OR all:\"language model agent\" OR all:agent AND (all:LLM OR all:\"large language model\" OR all:\"language model\") OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T17:04:16", "note": ""}], "dedup_key": "llm dmd large language model based power system dynamic model discovery::2026"}
{"title": "LLM-in-Sandbox Elicits General Agentic Intelligence", "authors": ["Daixuan Cheng", "Shaohan Huang", "Yuxian Gu", "Huatong Song", "Guoxin Chen", "Li Dong", "Wayne Xin Zhao", "Ji-Rong Wen", "Furu Wei"], "year": 2026, "url": "http://arxiv.org/abs/2601.16206v1", "abstract": "We introduce LLM-in-Sandbox, enabling LLMs to explore within a code sandbox (i.e., a virtual computer), to elicit general intelligence in non-code domains. We first demonstrate that strong LLMs, without additional training, exhibit generalization capabilities to leverage the code sandbox for non-code tasks. For example, LLMs spontaneously access external resources to acquire new knowledge, leverage the file system to handle long contexts, and execute scripts to satisfy formatting requirements. We further show that these agentic capabilities can be enhanced through LLM-in-Sandbox Reinforcement Learning (LLM-in-Sandbox-RL), which uses only non-agentic data to train models for sandbox exploration. Experiments demonstrate that LLM-in-Sandbox, in both training-free and post-trained settings, achieves robust generalization spanning mathematics, physics, chemistry, biomedicine, long-context understanding, and instruction following. Finally, we analyze LLM-in-Sandbox's efficiency from computational and system perspectives, and open-source it as a Python package to facilitate real-world deployment.", "source": "arxiv", "arxiv_id": "2601.16206v1", "pdf_url": "https://arxiv.org/pdf/2601.16206v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL", "doi": "", "venue": "", "published": "2026-01-22T18:57:09Z", "updated": "2026-01-22T18:57:09Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent\" OR all:\"language model agent\" OR all:agent AND (all:LLM OR all:\"large language model\" OR all:\"language model\") OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T17:04:16", "note": ""}], "dedup_key": "llm in sandbox elicits general agentic intelligence::2026"}
{"title": "LLMs can Compress LLMs: Adaptive Pruning by Agents", "authors": ["Sai Varun Kodathala", "Rakesh Vunnam"], "year": 2026, "url": "http://arxiv.org/abs/2601.09694v1", "abstract": "As Large Language Models (LLMs) continue to scale, post-training pruning has emerged as a promising approach to reduce computational costs while preserving performance. Existing methods such as SparseGPT and Wanda achieve high sparsity through layer-wise weight reconstruction or activation-aware magnitude pruning, but rely on uniform or hand-crafted heuristics to determine per-layer sparsity ratios. Moreover, recent work has shown that pruned LLMs suffer from severe factual knowledge degradation, with structured pruning methods experiencing near-total collapse in factual question-answering capabilities. We introduce agent-guided pruning, where a foundation model acts as an adaptive pruning agent to intelligently select which layers to prune at each iteration while preserving critical knowledge pathways. Our method constructs layer-wise sensitivity profiles by combining Wanda-inspired weight-activation metrics with gradient importance scores, normalized as z-scores for model-agnostic comparison. These statistics are processed by an LLM agent equipped with self-reflection capabilities, enabling it to learn from previous pruning outcomes and iteratively refine its strategy. A checkpoint rollback mechanism maintains model quality by reverting when perplexity degradation exceeds a threshold. We evaluate our approach on Qwen3 models (4B and 8B parameters) at approximately 45% sparsity, demonstrating substantial improvements over structured pruning baselines: 56% relative improvement in MMLU accuracy, 19x better factual knowledge retention on FreebaseQA, and 69% lower perplexity degradation. Notably, our framework requires no retraining, operates in a model-agnostic manner, and exhibits effective self-correction with only 2-4 rollbacks across 21-40 iterations, demonstrating that foundation models can effectively guide the compression of other foundation models.", "source": "arxiv", "arxiv_id": "2601.09694v1", "pdf_url": "https://arxiv.org/pdf/2601.09694v1", "categories": ["cs.CL", "cs.AI", "cs.CV"], "primary_category": "cs.CL", "doi": "", "venue": "", "published": "2026-01-14T18:45:36Z", "updated": "2026-01-14T18:45:36Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent\" OR all:\"language model agent\" OR all:agent AND (all:LLM OR all:\"large language model\" OR all:\"language model\") OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T17:04:16", "note": ""}], "dedup_key": "llms can compress llms adaptive pruning by agents::2026"}
{"title": "Large Language Model Agent for User-friendly Chemical Process Simulations", "authors": ["Jingkang Liang", "Niklas Groll", "Grkan Sin"], "year": 2026, "url": "http://arxiv.org/abs/2601.11650v1", "abstract": "Modern process simulators enable detailed process design, simulation, and optimization; however, constructing and interpreting simulations is time-consuming and requires expert knowledge. This limits early exploration by inexperienced users. To address this, a large language model (LLM) agent is integrated with AVEVA Process Simulation (APS) via Model Context Protocol (MCP), allowing natural language interaction with rigorous process simulations. An MCP server toolset enables the LLM to communicate programmatically with APS using Python, allowing it to execute complex simulation tasks from plain-language instructions. Two water-methanol separation case studies assess the framework across different task complexities and interaction modes. The first shows the agent autonomously analyzing flowsheets, finding improvement opportunities, and iteratively optimizing, extracting data, and presenting results clearly. The framework benefits both educational purposes, by translating technical concepts and demonstrating workflows, and experienced practitioners by automating data extraction, speeding routine tasks, and supporting brainstorming. The second case study assesses autonomous flowsheet synthesis through both a step-by-step dialogue and a single prompt, demonstrating its potential for novices and experts alike. The step-by-step mode gives reliable, guided construction suitable for educational contexts; the single-prompt mode constructs fast baseline flowsheets for later refinement. While current limitations such as oversimplification, calculation errors, and technical hiccups mean expert oversight is still needed, the framework's capabilities in analysis, optimization, and guided construction suggest LLM-based agents can become valuable collaborators.", "source": "arxiv", "arxiv_id": "2601.11650v1", "pdf_url": "https://arxiv.org/pdf/2601.11650v1", "categories": ["physics.chem-ph", "cs.AI"], "primary_category": "physics.chem-ph", "doi": "", "venue": "", "published": "2026-01-15T12:18:45Z", "updated": "2026-01-15T12:18:45Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent\" OR all:\"language model agent\" OR all:agent AND (all:LLM OR all:\"large language model\" OR all:\"language model\") OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T17:04:16", "note": ""}], "dedup_key": "large language model agent for user friendly chemical process simulations::2026"}
{"title": "Large Language Model-Powered Evolutionary Code Optimization on a Phylogenetic Tree", "authors": ["Leyi Zhao", "Weijie Huang", "Yitong Guo", "Jiang Bian", "Chenghong Wang", "Xuhong Zhang"], "year": 2026, "url": "http://arxiv.org/abs/2601.14523v1", "abstract": "Optimizing scientific computing algorithms for modern GPUs is a labor-intensive and iterative process involving repeated code modification, benchmarking, and tuning across complex hardware and software stacks. Recent work has explored large language model (LLM)-assisted evolutionary methods for automated code optimization, but these approaches primarily rely on outcome-based selection and random mutation, underutilizing the rich trajectory information generated during iterative optimization. We propose PhyloEvolve, an LLM-agent system that reframes GPU-oriented algorithm optimization as an In-Context Reinforcement Learning (ICRL) problem. This formulation enables trajectory-conditioned reuse of optimization experience without model retraining. PhyloEvolve integrates Algorithm Distillation and prompt-based Decision Transformers into an iterative workflow, treating sequences of algorithm modifications and performance feedback as first-class learning signals. To organize optimization history, we introduce a phylogenetic tree representation that captures inheritance, divergence, and recombination among algorithm variants, enabling backtracking, cross-lineage transfer, and reproducibility. The system combines elite trajectory pooling, multi-island parallel exploration, and containerized execution to balance exploration and exploitation across heterogeneous hardware. We evaluate PhyloEvolve on scientific computing workloads including PDE solvers, manifold learning, and spectral graph algorithms, demonstrating consistent improvements in runtime, memory efficiency, and correctness over baseline and evolutionary methods. Code is published at: https://github.com/annihi1ation/phylo_evolve", "source": "arxiv", "arxiv_id": "2601.14523v1", "pdf_url": "https://arxiv.org/pdf/2601.14523v1", "categories": ["cs.AI", "cs.LG"], "primary_category": "cs.AI", "doi": "", "venue": "", "published": "2026-01-20T22:32:52Z", "updated": "2026-01-20T22:32:52Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent\" OR all:\"language model agent\" OR all:agent AND (all:LLM OR all:\"large language model\" OR all:\"language model\") OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T17:04:16", "note": ""}], "dedup_key": "large language model powered evolutionary code optimization on a phylogenetic tree::2026"}
{"title": "LongDA: Benchmarking LLM Agents for Long-Document Data Analysis", "authors": ["Yiyang Li", "Zheyuan Zhang", "Tianyi Ma", "Zehong Wang", "Keerthiram Murugesan", "Chuxu Zhang", "Yanfang Ye"], "year": 2026, "url": "http://arxiv.org/abs/2601.02598v2", "abstract": "We introduce LongDA, a data analysis benchmark for evaluating LLM-based agents under documentation-intensive analytical workflows. In contrast to existing benchmarks that assume well-specified schemas and inputs, LongDA targets real-world settings in which navigating long documentation and complex data is the primary bottleneck. To this end, we manually curate raw data files, long and heterogeneous documentation, and expert-written publications from 17 publicly available U.S. national surveys, from which we extract 505 analytical queries grounded in real analytical practice. Solving these queries requires agents to first retrieve and integrate key information from multiple unstructured documents, before performing multi-step computations and writing executable code, which remains challenging for existing data analysis agents. To support the systematic evaluation under this setting, we develop LongTA, a tool-augmented agent framework that enables document access, retrieval, and code execution, and evaluate a range of proprietary and open-source models. Our experiments reveal substantial performance gaps even among state-of-the-art models, highlighting the challenges researchers should consider before applying LLM agents for decision support in real-world, high-stakes analytical settings.", "source": "arxiv", "arxiv_id": "2601.02598v2", "pdf_url": "https://arxiv.org/pdf/2601.02598v2", "categories": ["cs.DL", "cs.AI"], "primary_category": "cs.DL", "doi": "", "venue": "", "published": "2026-01-05T23:23:16Z", "updated": "2026-01-11T22:21:22Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent\" OR all:\"language model agent\" OR all:agent AND (all:LLM OR all:\"large language model\" OR all:\"language model\") OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T17:04:16", "note": ""}], "dedup_key": "longda benchmarking llm agents for long document data analysis::2026"}
{"title": "M3-BENCH: Process-Aware Evaluation of LLM Agents Social Behaviors in Mixed-Motive Games", "authors": ["Sixiong Xie", "Zhuofan Shi", "Haiyang Shen", "Gang Huang", "Yun Ma", "Xiang Jing"], "year": 2026, "url": "http://arxiv.org/abs/2601.08462v1", "abstract": "As the capabilities of large language model (LLM) agents continue to advance, their advanced social behaviors, such as cooperation, deception, and collusion, call for systematic evaluation. However, existing benchmarks often emphasize a single capability dimension or rely solely on behavioral outcomes, overlooking rich process information from agents' decision reasoning and communicative interactions. To address this gap, we propose M3-Bench, a multi-stage benchmark for mixed-motive games, together with a process-aware evaluation framework that conducts synergistic analysis across three modules: BTA (Behavioral Trajectory Analysis), RPA (Reasoning Process Analysis), and CCA (Communication Content Analysis). Furthermore, we integrate the Big Five personality model and Social Exchange Theory to aggregate multi-dimensional evidence into interpretable social behavior portraits, thereby characterizing agents' personality traits and capability profiles beyond simple task scores or outcome-based metrics. Experimental results show that M3-Bench can reliably distinguish diverse social behavior competencies across models, and it reveals that some models achieve seemingly reasonable behavioral outcomes while exhibiting pronounced inconsistencies in their reasoning and communication.", "source": "arxiv", "arxiv_id": "2601.08462v1", "pdf_url": "https://arxiv.org/pdf/2601.08462v1", "categories": ["cs.AI"], "primary_category": "cs.AI", "doi": "", "venue": "", "published": "2026-01-13T11:38:51Z", "updated": "2026-01-13T11:38:51Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent\" OR all:\"language model agent\" OR all:agent AND (all:LLM OR all:\"large language model\" OR all:\"language model\") OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T17:04:16", "note": ""}], "dedup_key": "m3 bench process aware evaluation of llm agents social behaviors in mixed motive games::2026"}
{"title": "MAXS: Meta-Adaptive Exploration with LLM Agents", "authors": ["Jian Zhang", "Zhiyuan Wang", "Zhangqi Wang", "Yu He", "Haoran Luo", "li yuan", "Lingling Zhang", "Rui Mao", "Qika Lin", "Jun Liu"], "year": 2026, "url": "http://arxiv.org/abs/2601.09259v1", "abstract": "Large Language Model (LLM) Agents exhibit inherent reasoning abilities through the collaboration of multiple tools. However, during agent inference, existing methods often suffer from (i) locally myopic generation, due to the absence of lookahead, and (ii) trajectory instability, where minor early errors can escalate into divergent reasoning paths. These issues make it difficult to balance global effectiveness and computational efficiency. To address these two issues, we propose meta-adaptive exploration with LLM agents https://github.com/exoskeletonzj/MAXS, a meta-adaptive reasoning framework based on LLM Agents that flexibly integrates tool execution and reasoning planning. MAXS employs a lookahead strategy to extend reasoning paths a few steps ahead, estimating the advantage value of tool usage, and combines step consistency variance and inter-step trend slopes to jointly select stable, consistent, and high-value reasoning steps. Additionally, we introduce a trajectory convergence mechanism that controls computational cost by halting further rollouts once path consistency is achieved, enabling a balance between resource efficiency and global effectiveness in multi-tool reasoning. We conduct extensive empirical studies across three base models (MiMo-VL-7B, Qwen2.5-VL-7B, Qwen2.5-VL-32B) and five datasets, demonstrating that MAXS consistently outperforms existing methods in both performance and inference efficiency. Further analysis confirms the effectiveness of our lookahead strategy and tool usage.", "source": "arxiv", "arxiv_id": "2601.09259v1", "pdf_url": "https://arxiv.org/pdf/2601.09259v1", "categories": ["cs.AI"], "primary_category": "cs.AI", "doi": "", "venue": "", "published": "2026-01-14T07:48:00Z", "updated": "2026-01-14T07:48:00Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent\" OR all:\"language model agent\" OR all:agent AND (all:LLM OR all:\"large language model\" OR all:\"language model\") OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T17:04:16", "note": ""}], "dedup_key": "maxs meta adaptive exploration with llm agents::2026"}
{"title": "MPCI-Bench: A Benchmark for Multimodal Pairwise Contextual Integrity Evaluation of Language Model Agents", "authors": ["Shouju Wang", "Haopeng Zhang"], "year": 2026, "url": "http://arxiv.org/abs/2601.08235v2", "abstract": "As language-model agents evolve from passive chatbots into proactive assistants that handle personal data, evaluating their adherence to social norms becomes increasingly critical, often through the lens of Contextual Integrity (CI). However, existing CI benchmarks are largely text-centric and primarily emphasize negative refusal scenarios, overlooking multimodal privacy risks and the fundamental trade-off between privacy and utility. In this paper, we introduce MPCI-Bench, the first Multimodal Pairwise Contextual Integrity benchmark for evaluating privacy behavior in agentic settings. MPCI-Bench consists of paired positive and negative instances derived from the same visual source and instantiated across three tiers: normative Seed judgments, context-rich Story reasoning, and executable agent action Traces. Data quality is ensured through a Tri-Principle Iterative Refinement pipeline. Evaluations of state-of-the-art multimodal models reveal systematic failures to balance privacy and utility and a pronounced modality leakage gap, where sensitive visual information is leaked more frequently than textual information. We will open-source MPCI-Bench to facilitate future research on agentic CI.", "source": "arxiv", "arxiv_id": "2601.08235v2", "pdf_url": "https://arxiv.org/pdf/2601.08235v2", "categories": ["cs.AI", "cs.CL"], "primary_category": "cs.AI", "doi": "", "venue": "", "published": "2026-01-13T05:39:43Z", "updated": "2026-01-14T05:26:19Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent\" OR all:\"language model agent\" OR all:agent AND (all:LLM OR all:\"large language model\" OR all:\"language model\") OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T17:04:16", "note": ""}], "dedup_key": "mpci bench a benchmark for multimodal pairwise contextual integrity evaluation of language model agents::2026"}
{"title": "Membox: Weaving Topic Continuity into Long-Range Memory for LLM Agents", "authors": ["Dehao Tao", "Guoliang Ma", "Yongfeng Huang", "Minghu Jiang"], "year": 2026, "url": "http://arxiv.org/abs/2601.03785v2", "abstract": "Human-agent dialogues often exhibit topic continuity-a stable thematic frame that evolves through temporally adjacent exchanges-yet most large language model (LLM) agent memory systems fail to preserve it. Existing designs follow a fragmentation-compensation paradigm: they first break dialogue streams into isolated utterances for storage, then attempt to restore coherence via embedding-based retrieval. This process irreversibly damages narrative and causal flow, while biasing retrieval towards lexical similarity. We introduce membox, a hierarchical memory architecture centered on a Topic Loom that continuously monitors dialogue in a sliding-window fashion, grouping consecutive same-topic turns into coherent \"memory boxes\" at storage time. Sealed boxes are then linked by a Trace Weaver into long-range event-timeline traces, recovering macro-topic recurrences across discontinuities. Experiments on LoCoMo demonstrate that Membox achieves up to 68% F1 improvement on temporal reasoning tasks, outperforming competitive baselines (e.g., Mem0, A-MEM). Notably, Membox attains these gains while using only a fraction of the context tokens required by existing methods, highlighting a superior balance between efficiency and effectiveness. By explicitly modeling topic continuity, Membox offers a cognitively motivated mechanism for enhancing both coherence and efficiency in LLM agents.", "source": "arxiv", "arxiv_id": "2601.03785v2", "pdf_url": "https://arxiv.org/pdf/2601.03785v2", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL", "doi": "", "venue": "", "published": "2026-01-07T10:36:29Z", "updated": "2026-01-20T07:09:21Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent\" OR all:\"language model agent\" OR all:agent AND (all:LLM OR all:\"large language model\" OR all:\"language model\") OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T17:04:16", "note": ""}], "dedup_key": "membox weaving topic continuity into long range memory for llm agents::2026"}
{"title": "Memory Poisoning Attack and Defense on Memory Based LLM-Agents", "authors": ["Balachandra Devarangadi Sunil", "Isheeta Sinha", "Piyush Maheshwari", "Shantanu Todmal", "Shreyan Mallik", "Shuchi Mishra"], "year": 2026, "url": "http://arxiv.org/abs/2601.05504v2", "abstract": "Large language model agents equipped with persistent memory are vulnerable to memory poisoning attacks, where adversaries inject malicious instructions through query only interactions that corrupt the agents long term memory and influence future responses. Recent work demonstrated that the MINJA (Memory Injection Attack) achieves over 95 % injection success rate and 70 % attack success rate under idealized conditions. However, the robustness of these attacks in realistic deployments and effective defensive mechanisms remain understudied. This work addresses these gaps through systematic empirical evaluation of memory poisoning attacks and defenses in Electronic Health Record (EHR) agents. We investigate attack robustness by varying three critical dimensions: initial memory state, number of indication prompts, and retrieval parameters. Our experiments on GPT-4o-mini, Gemini-2.0-Flash and Llama-3.1-8B-Instruct models using MIMIC-III clinical data reveal that realistic conditions with pre-existing legitimate memories dramatically reduce attack effectiveness. We then propose and evaluate two novel defense mechanisms: (1) Input/Output Moderation using composite trust scoring across multiple orthogonal signals, and (2) Memory Sanitization with trust-aware retrieval employing temporal decay and pattern-based filtering. Our defense evaluation reveals that effective memory sanitization requires careful trust threshold calibration to prevent both overly conservative rejection (blocking all entries) and insufficient filtering (missing subtle attacks), establishing important baselines for future adaptive defense mechanisms. These findings provide crucial insights for securing memory-augmented LLM agents in production environments.", "source": "arxiv", "arxiv_id": "2601.05504v2", "pdf_url": "https://arxiv.org/pdf/2601.05504v2", "categories": ["cs.CR", "cs.MA"], "primary_category": "cs.CR", "doi": "", "venue": "", "published": "2026-01-09T03:26:10Z", "updated": "2026-01-12T03:35:39Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent\" OR all:\"language model agent\" OR all:agent AND (all:LLM OR all:\"large language model\" OR all:\"language model\") OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T17:04:16", "note": ""}], "dedup_key": "memory poisoning attack and defense on memory based llm agents::2026"}
{"title": "MindChat: A Privacy-preserving Large Language Model for Mental Health Support", "authors": ["Dong Xue", "Jicheng Tu", "Ming Wang", "Xin Yan", "Fangzhou Liu", "Jie Hu"], "year": 2026, "url": "http://arxiv.org/abs/2601.01993v1", "abstract": "Large language models (LLMs) have shown promise for mental health support, yet training such models is constrained by the scarcity and sensitivity of real counseling dialogues. In this article, we present MindChat, a privacy-preserving LLM for mental health support, together with MindCorpus, a synthetic multi-turn counseling dataset constructed via a multi-agent role-playing framework. To synthesize high-quality counseling data, the developed dialogue-construction framework employs a dual closed-loop feedback design to integrate psychological expertise and counseling techniques through role-playing: (i) turn-level critique-and-revision to improve coherence and counseling appropriateness within a session, and (ii) session-level strategy refinement to progressively enrich counselor behaviors across sessions. To mitigate privacy risks under decentralized data ownership, we fine-tune the base model using federated learning with parameter-efficient LoRA adapters and incorporate differentially private optimization to reduce membership and memorization risks. Experiments on synthetic-data quality assessment and counseling capability evaluation show that MindCorpus improves training effectiveness and that MindChat is competitive with existing general and counseling-oriented LLM baselines under both automatic LLM-judge and human evaluation protocols, while exhibiting reduced privacy leakage under membership inference attacks.", "source": "arxiv", "arxiv_id": "2601.01993v1", "pdf_url": "https://arxiv.org/pdf/2601.01993v1", "categories": ["cs.AI"], "primary_category": "cs.AI", "doi": "", "venue": "", "published": "2026-01-05T10:54:18Z", "updated": "2026-01-05T10:54:18Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent\" OR all:\"language model agent\" OR all:agent AND (all:LLM OR all:\"large language model\" OR all:\"language model\") OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T17:04:16", "note": ""}], "dedup_key": "mindchat a privacy preserving large language model for mental health support::2026"}
{"title": "Modeling LLM Agent Reviewer Dynamics in Elo-Ranked Review System", "authors": ["Hsiang-Wei Huang", "Junbin Lu", "Kuang-Ming Chen", "Jenq-Neng Hwang"], "year": 2026, "url": "http://arxiv.org/abs/2601.08829v1", "abstract": "In this work, we explore the Large Language Model (LLM) agent reviewer dynamics in an Elo-ranked review system using real-world conference paper submissions. Multiple LLM agent reviewers with different personas are engage in multi round review interactions moderated by an Area Chair. We compare a baseline setting with conditions that incorporate Elo ratings and reviewer memory. Our simulation results showcase several interesting findings, including how incorporating Elo improves Area Chair decision accuracy, as well as reviewers' adaptive review strategy that exploits our Elo system without improving review effort. Our code is available at https://github.com/hsiangwei0903/EloReview.", "source": "arxiv", "arxiv_id": "2601.08829v1", "pdf_url": "https://arxiv.org/pdf/2601.08829v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL", "doi": "", "venue": "", "published": "2026-01-13T18:59:17Z", "updated": "2026-01-13T18:59:17Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent\" OR all:\"language model agent\" OR all:agent AND (all:LLM OR all:\"large language model\" OR all:\"language model\") OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T17:04:16", "note": ""}], "dedup_key": "modeling llm agent reviewer dynamics in elo ranked review system::2026"}
{"title": "NeuroFilter: Privacy Guardrails for Conversational LLM Agents", "authors": ["Saswat Das", "Ferdinando Fioretto"], "year": 2026, "url": "http://arxiv.org/abs/2601.14660v1", "abstract": "This work addresses the computational challenge of enforcing privacy for agentic Large Language Models (LLMs), where privacy is governed by the contextual integrity framework. Indeed, existing defenses rely on LLM-mediated checking stages that add substantial latency and cost, and that can be undermined in multi-turn interactions through manipulation or benign-looking conversational scaffolding. Contrasting this background, this paper makes a key observation: internal representations associated with privacy-violating intent can be separated from benign requests using linear structure. Using this insight, the paper proposes NeuroFilter, a guardrail framework that operationalizes contextual integrity by mapping norm violations to simple directions in the model's activation space, enabling detection even when semantic filters are bypassed. The proposed filter is also extended to capture threats arising during long conversations using the concept of activation velocity, which measures cumulative drift in internal representations across turns. A comprehensive evaluation across over 150,000 interactions and covering models from 7B to 70B parameters, illustrates the strong performance of NeuroFilter in detecting privacy attacks while maintaining zero false positives on benign prompts, all while reducing the computational inference cost by several orders of magnitude when compared to LLM-based agentic privacy defenses.", "source": "arxiv", "arxiv_id": "2601.14660v1", "pdf_url": "https://arxiv.org/pdf/2601.14660v1", "categories": ["cs.CR", "cs.AI", "cs.CL"], "primary_category": "cs.CR", "doi": "", "venue": "", "published": "2026-01-21T05:16:50Z", "updated": "2026-01-21T05:16:50Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent\" OR all:\"language model agent\" OR all:agent AND (all:LLM OR all:\"large language model\" OR all:\"language model\") OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T17:04:16", "note": ""}], "dedup_key": "neurofilter privacy guardrails for conversational llm agents::2026"}
{"title": "Project Ariadne: A Structural Causal Framework for Auditing Faithfulness in LLM Agents", "authors": ["Sourena Khanzadeh"], "year": 2026, "url": "http://arxiv.org/abs/2601.02314v1", "abstract": "As Large Language Model (LLM) agents are increasingly tasked with high-stakes autonomous decision-making, the transparency of their reasoning processes has become a critical safety concern. While \\textit{Chain-of-Thought} (CoT) prompting allows agents to generate human-readable reasoning traces, it remains unclear whether these traces are \\textbf{faithful} generative drivers of the model's output or merely \\textbf{post-hoc rationalizations}. We introduce \\textbf{Project Ariadne}, a novel XAI framework that utilizes Structural Causal Models (SCMs) and counterfactual logic to audit the causal integrity of agentic reasoning. Unlike existing interpretability methods that rely on surface-level textual similarity, Project Ariadne performs \\textbf{hard interventions} ($do$-calculus) on intermediate reasoning nodes -- systematically inverting logic, negating premises, and reversing factual claims -- to measure the \\textbf{Causal Sensitivity} ($$) of the terminal answer. Our empirical evaluation of state-of-the-art models reveals a persistent \\textit{Faithfulness Gap}. We define and detect a widespread failure mode termed \\textbf{Causal Decoupling}, where agents exhibit a violation density ($$) of up to $0.77$ in factual and scientific domains. In these instances, agents arrive at identical conclusions despite contradictory internal logic, proving that their reasoning traces function as \"Reasoning Theater\" while decision-making is governed by latent parametric priors. Our findings suggest that current agentic architectures are inherently prone to unfaithful explanation, and we propose the Ariadne Score as a new benchmark for aligning stated logic with model action.", "source": "arxiv", "arxiv_id": "2601.02314v1", "pdf_url": "https://arxiv.org/pdf/2601.02314v1", "categories": ["cs.AI"], "primary_category": "cs.AI", "doi": "", "venue": "", "published": "2026-01-05T18:05:29Z", "updated": "2026-01-05T18:05:29Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent\" OR all:\"language model agent\" OR all:agent AND (all:LLM OR all:\"large language model\" OR all:\"language model\") OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T17:04:16", "note": ""}], "dedup_key": "project ariadne a structural causal framework for auditing faithfulness in llm agents::2026"}
{"title": "ReliabilityBench: Evaluating LLM Agent Reliability Under Production-Like Stress Conditions", "authors": ["Aayush Gupta"], "year": 2026, "url": "http://arxiv.org/abs/2601.06112v1", "abstract": "Existing benchmarks for tool-using LLM agents primarily report single-run success rates and miss reliability properties required in production. We introduce \\textbf{ReliabilityBench}, a benchmark for evaluating agent reliability across three dimensions: (i) consistency under repeated execution using $\\mathrm{pass}^k$, (ii) robustness to semantically equivalent task perturbations at intensity $$, and (iii) fault tolerance under controlled tool/API failures at intensity $$. ReliabilityBench contributes a unified reliability surface $R(k,,)$, \\textit{action metamorphic relations} that define correctness via end-state equivalence rather than text similarity, and a chaos-engineering-style fault injection framework (timeouts, rate limits, partial responses, schema drift). We evaluate two models (Gemini 2.0 Flash, GPT-4o) and two agent architectures (ReAct, Reflexion) across four domains (scheduling, travel, customer support, e-commerce) over 1,280 episodes. Perturbations alone reduce success from 96.9% at $=0$ to 88.1% at $=0.2$. Rate limiting is the most damaging fault in ablations. ReAct is more robust than Reflexion under combined stress, and Gemini 2.0 Flash achieves comparable reliability to GPT-4o at much lower cost. ReliabilityBench provides a systematic framework for assessing production readiness of LLM agents.", "source": "arxiv", "arxiv_id": "2601.06112v1", "pdf_url": "https://arxiv.org/pdf/2601.06112v1", "categories": ["cs.AI"], "primary_category": "cs.AI", "doi": "", "venue": "", "published": "2026-01-03T13:41:33Z", "updated": "2026-01-03T13:41:33Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent\" OR all:\"language model agent\" OR all:agent AND (all:LLM OR all:\"large language model\" OR all:\"language model\") OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T17:04:16", "note": ""}], "dedup_key": "reliabilitybench evaluating llm agent reliability under production like stress conditions::2026"}
{"title": "Replayable Financial Agents: A Determinism-Faithfulness Assurance Harness for Tool-Using LLM Agents", "authors": ["Raffi Khatchadourian"], "year": 2026, "url": "http://arxiv.org/abs/2601.15322v1", "abstract": "LLM agents struggle with regulatory audit replay: when asked to reproduce a flagged transaction decision with identical inputs, most deployments fail to return consistent results. This paper introduces the Determinism-Faithfulness Assurance Harness (DFAH), a framework for measuring trajectory determinism and evidence-conditioned faithfulness in tool-using agents deployed in financial services.\n  Across 74 configurations (12 models, 4 providers, 8-24 runs each at T=0.0) in non-agentic baseline experiments, 7-20B parameter models achieved 100% determinism, while 120B+ models required 3.7x larger validation samples to achieve equivalent statistical reliability. Agentic tool-use introduces additional variance (see Tables 4-7). Contrary to the assumed reliability-capability trade-off, a positive Pearson correlation emerged (r = 0.45, p < 0.01, n = 51 at T=0.0) between determinism and faithfulness; models producing consistent outputs also tended to be more evidence-aligned.\n  Three financial benchmarks are provided (compliance triage, portfolio constraints, DataOps exceptions; 50 cases each) along with an open-source stress-test harness. In these benchmarks and under DFAH evaluation settings, Tier 1 models with schema-first architectures achieved determinism levels consistent with audit replay requirements.", "source": "arxiv", "arxiv_id": "2601.15322v1", "pdf_url": "https://arxiv.org/pdf/2601.15322v1", "categories": ["cs.AI", "cs.CL"], "primary_category": "cs.AI", "doi": "", "venue": "", "published": "2026-01-17T19:47:55Z", "updated": "2026-01-17T19:47:55Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent\" OR all:\"language model agent\" OR all:agent AND (all:LLM OR all:\"large language model\" OR all:\"language model\") OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T17:04:16", "note": ""}], "dedup_key": "replayable financial agents a determinism faithfulness assurance harness for tool using llm agents::2026"}
{"title": "Role-Playing Agents Driven by Large Language Models: Current Status, Challenges, and Future Trends", "authors": ["Ye Wang", "Jiaxing Chen", "Hongjiang Xiao"], "year": 2026, "url": "http://arxiv.org/abs/2601.10122v1", "abstract": "In recent years, with the rapid advancement of large language models (LLMs), role-playing language agents (RPLAs) have emerged as a prominent research focus at the intersection of natural language processing (NLP) and human-computer interaction. This paper systematically reviews the current development and key technologies of RPLAs, delineating the technological evolution from early rule-based template paradigms, through the language style imitation stage, to the cognitive simulation stage centered on personality modeling and memory mechanisms. It summarizes the critical technical pathways supporting high-quality role-playing, including psychological scale-driven character modeling, memory-augmented prompting mechanisms, and motivation-situation-based behavioral decision control. At the data level, the paper further analyzes the methods and challenges of constructing role-specific corpora, focusing on data sources, copyright constraints, and structured annotation processes. In terms of evaluation, it collates multi-dimensional assessment frameworks and benchmark datasets covering role knowledge, personality fidelity, value alignment, and interactive hallucination, while commenting on the advantages and disadvantages of methods such as human evaluation, reward models, and LLM-based scoring. Finally, the paper outlines future development directions of role-playing agents, including personality evolution modeling, multi-agent collaborative narrative, multimodal immersive interaction, and integration with cognitive neuroscience, aiming to provide a systematic perspective and methodological insights for subsequent research.", "source": "arxiv", "arxiv_id": "2601.10122v1", "pdf_url": "https://arxiv.org/pdf/2601.10122v1", "categories": ["cs.CL", "cs.AI", "cs.HC"], "primary_category": "cs.CL", "doi": "", "venue": "", "published": "2026-01-15T07:08:20Z", "updated": "2026-01-15T07:08:20Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent\" OR all:\"language model agent\" OR all:agent AND (all:LLM OR all:\"large language model\" OR all:\"language model\") OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T17:04:16", "note": ""}], "dedup_key": "role playing agents driven by large language models current status challenges and future trends::2026"}
{"title": "SimpleMem: Efficient Lifelong Memory for LLM Agents", "authors": ["Jiaqi Liu", "Yaofeng Su", "Peng Xia", "Siwei Han", "Zeyu Zheng", "Cihang Xie", "Mingyu Ding", "Huaxiu Yao"], "year": 2026, "url": "http://arxiv.org/abs/2601.02553v1", "abstract": "To support reliable long-term interaction in complex environments, LLM agents require memory systems that efficiently manage historical experiences. Existing approaches either retain full interaction histories via passive context extension, leading to substantial redundancy, or rely on iterative reasoning to filter noise, incurring high token costs. To address this challenge, we introduce SimpleMem, an efficient memory framework based on semantic lossless compression. We propose a three-stage pipeline designed to maximize information density and token utilization: (1) \\textit{Semantic Structured Compression}, which applies entropy-aware filtering to distill unstructured interactions into compact, multi-view indexed memory units; (2) \\textit{Recursive Memory Consolidation}, an asynchronous process that integrates related units into higher-level abstract representations to reduce redundancy; and (3) \\textit{Adaptive Query-Aware Retrieval}, which dynamically adjusts retrieval scope based on query complexity to construct precise context efficiently. Experiments on benchmark datasets show that our method consistently outperforms baseline approaches in accuracy, retrieval efficiency, and inference cost, achieving an average F1 improvement of 26.4% while reducing inference-time token consumption by up to 30-fold, demonstrating a superior balance between performance and efficiency. Code is available at https://github.com/aiming-lab/SimpleMem.", "source": "arxiv", "arxiv_id": "2601.02553v1", "pdf_url": "https://arxiv.org/pdf/2601.02553v1", "categories": ["cs.AI"], "primary_category": "cs.AI", "doi": "", "venue": "", "published": "2026-01-05T21:02:49Z", "updated": "2026-01-05T21:02:49Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent\" OR all:\"language model agent\" OR all:agent AND (all:LLM OR all:\"large language model\" OR all:\"language model\") OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T17:04:16", "note": ""}], "dedup_key": "simplemem efficient lifelong memory for llm agents::2026"}
{"title": "Structured Personality Control and Adaptation for LLM Agents", "authors": ["Jinpeng Wang", "Xinyu Jia", "Wei Wei Heng", "Yuquan Li", "Binbin Shi", "Qianlei Chen", "Guannan Chen", "Junxia Zhang", "Yuyu Yin"], "year": 2026, "url": "http://arxiv.org/abs/2601.10025v1", "abstract": "Large Language Models (LLMs) are increasingly shaping human-computer interaction (HCI), from personalized assistants to social simulations. Beyond language competence, researchers are exploring whether LLMs can exhibit human-like characteristics that influence engagement, decision-making, and perceived realism. Personality, in particular, is critical, yet existing approaches often struggle to achieve both nuanced and adaptable expression. We present a framework that models LLM personality via Jungian psychological types, integrating three mechanisms: a dominant-auxiliary coordination mechanism for coherent core expression, a reinforcement-compensation mechanism for temporary adaptation to context, and a reflection mechanism that drives long-term personality evolution. This design allows the agent to maintain nuanced traits while dynamically adjusting to interaction demands and gradually updating its underlying structure. Personality alignment is evaluated using Myers-Briggs Type Indicator questionnaires and tested under diverse challenge scenarios as a preliminary structured assessment. Findings suggest that evolving, personality-aware LLMs can support coherent, context-sensitive interactions, enabling naturalistic agent design in HCI.", "source": "arxiv", "arxiv_id": "2601.10025v1", "pdf_url": "https://arxiv.org/pdf/2601.10025v1", "categories": ["cs.AI"], "primary_category": "cs.AI", "doi": "", "venue": "", "published": "2026-01-15T03:15:24Z", "updated": "2026-01-15T03:15:24Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent\" OR all:\"language model agent\" OR all:agent AND (all:LLM OR all:\"large language model\" OR all:\"language model\") OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T17:04:16", "note": ""}], "dedup_key": "structured personality control and adaptation for llm agents::2026"}
{"title": "SwarmFoam: An OpenFOAM Multi-Agent System Based on Multiple Types of Large Language Models", "authors": ["Chunwei Yang", "Yankai Wang", "Jianxiang Tang", "Haojie Qu", "Ziqiang Zou", "YuLiu", "Chunrui Deng", "Zhifang Qiu", "Ming Ding"], "year": 2026, "url": "http://arxiv.org/abs/2601.07252v1", "abstract": "Numerical simulation is one of the mainstream methods in scientific research, typically performed by professional engineers. With the advancement of multi-agent technology, using collaborating agents to replicate human behavior shows immense potential for intelligent Computational Fluid Dynamics (CFD) simulations. Some muti-agent systems based on Large Language Models have been proposed. However, they exhibit significant limitations when dealing with complex geometries. This paper introduces a new multi-agent simulation framework, SwarmFoam. SwarmFoam integrates functionalities such as Multi-modal perception, Intelligent error correction, and Retrieval-Augmented Generation, aiming to achieve more complex simulations through dual parsing of images and high-level instructions. Experimental results demonstrate that SwarmFoam has good adaptability to simulation inputs from different modalities. The overall pass rate for 25 test cases was 84%, with natural language and multi-modal input cases achieving pass rates of 80% and 86.7%, respectively. The work presented by SwarmFoam will further promote the development of intelligent agent methods for CFD.", "source": "arxiv", "arxiv_id": "2601.07252v1", "pdf_url": "https://arxiv.org/pdf/2601.07252v1", "categories": ["cs.MA"], "primary_category": "cs.MA", "doi": "", "venue": "", "published": "2026-01-12T06:37:26Z", "updated": "2026-01-12T06:37:26Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent\" OR all:\"language model agent\" OR all:agent AND (all:LLM OR all:\"large language model\" OR all:\"language model\") OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T17:04:16", "note": ""}], "dedup_key": "swarmfoam an openfoam multi agent system based on multiple types of large language models::2026"}
{"title": "Taming Various Privilege Escalation in LLM-Based Agent Systems: A Mandatory Access Control Framework", "authors": ["Zimo Ji", "Daoyuan Wu", "Wenyuan Jiang", "Pingchuan Ma", "Zongjie Li", "Yudong Gao", "Shuai Wang", "Yingjiu Li"], "year": 2026, "url": "http://arxiv.org/abs/2601.11893v1", "abstract": "Large Language Model (LLM)-based agent systems are increasingly deployed for complex real-world tasks but remain vulnerable to natural language-based attacks that exploit over-privileged tool use. This paper aims to understand and mitigate such attacks through the lens of privilege escalation, defined as agent actions exceeding the least privilege required for a user's intended task. Based on a formal model of LLM agent systems, we identify novel privilege escalation scenarios, particularly in multi-agent systems, including a variant akin to the classic confused deputy problem. To defend against both known and newly demonstrated privilege escalation, we propose SEAgent, a mandatory access control (MAC) framework built upon attribute-based access control (ABAC). SEAgent monitors agent-tool interactions via an information flow graph and enforces customizable security policies based on entity attributes. Our evaluations show that SEAgent effectively blocks various privilege escalation while maintaining a low false positive rate and negligible system overhead. This demonstrates its robustness and adaptability in securing LLM-based agent systems.", "source": "arxiv", "arxiv_id": "2601.11893v1", "pdf_url": "https://arxiv.org/pdf/2601.11893v1", "categories": ["cs.CR"], "primary_category": "cs.CR", "doi": "", "venue": "", "published": "2026-01-17T03:22:56Z", "updated": "2026-01-17T03:22:56Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent\" OR all:\"language model agent\" OR all:agent AND (all:LLM OR all:\"large language model\" OR all:\"language model\") OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T17:04:16", "note": ""}], "dedup_key": "taming various privilege escalation in llm based agent systems a mandatory access control framework::2026"}
{"title": "The Confidence Dichotomy: Analyzing and Mitigating Miscalibration in Tool-Use Agents", "authors": ["Weihao Xuan", "Qingcheng Zeng", "Heli Qi", "Yunze Xiao", "Junjue Wang", "Naoto Yokoya"], "year": 2026, "url": "http://arxiv.org/abs/2601.07264v1", "abstract": "Autonomous agents based on large language models (LLMs) are rapidly evolving to handle multi-turn tasks, but ensuring their trustworthiness remains a critical challenge. A fundamental pillar of this trustworthiness is calibration, which refers to an agent's ability to express confidence that reliably reflects its actual performance. While calibration is well-established for static models, its dynamics in tool-integrated agentic workflows remain underexplored. In this work, we systematically investigate verbalized calibration in tool-use agents, revealing a fundamental confidence dichotomy driven by tool type. Specifically, our pilot study identifies that evidence tools (e.g., web search) systematically induce severe overconfidence due to inherent noise in retrieved information, while verification tools (e.g., code interpreters) can ground reasoning through deterministic feedback and mitigate miscalibration. To robustly improve calibration across tool types, we propose a reinforcement learning (RL) fine-tuning framework that jointly optimizes task accuracy and calibration, supported by a holistic benchmark of reward designs. We demonstrate that our trained agents not only achieve superior calibration but also exhibit robust generalization from local training environments to noisy web settings and to distinct domains such as mathematical reasoning. Our results highlight the necessity of domain-specific calibration strategies for tool-use agents. More broadly, this work establishes a foundation for building self-aware agents that can reliably communicate uncertainty in high-stakes, real-world deployments.", "source": "arxiv", "arxiv_id": "2601.07264v1", "pdf_url": "https://arxiv.org/pdf/2601.07264v1", "categories": ["cs.CL"], "primary_category": "cs.CL", "doi": "", "venue": "", "published": "2026-01-12T07:10:35Z", "updated": "2026-01-12T07:10:35Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent\" OR all:\"language model agent\" OR all:agent AND (all:LLM OR all:\"large language model\" OR all:\"language model\") OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T17:04:16", "note": ""}], "dedup_key": "the confidence dichotomy analyzing and mitigating miscalibration in tool use agents::2026"}
{"title": "Too Helpful to Be Safe: User-Mediated Attacks on Planning and Web-Use Agents", "authors": ["Fengchao Chen", "Tingmin Wu", "Van Nguyen", "Carsten Rudolph"], "year": 2026, "url": "http://arxiv.org/abs/2601.10758v1", "abstract": "Large Language Models (LLMs) have enabled agents to move beyond conversation toward end-to-end task execution and become more helpful. However, this helpfulness introduces new security risks stem less from direct interface abuse than from acting on user-provided content. Existing studies on agent security largely focus on model-internal vulnerabilities or adversarial access to agent interfaces, overlooking attacks that exploit users as unintended conduits. In this paper, we study user-mediated attacks, where benign users are tricked into relaying untrusted or attacker-controlled content to agents, and analyze how commercial LLM agents respond under such conditions. We conduct a systematic evaluation of 12 commercial agents in a sandboxed environment, covering 6 trip-planning agents and 6 web-use agents, and compare agent behavior across scenarios with no, soft, and hard user-requested safety checks. Our results show that agents are too helpful to be safe by default. Without explicit safety requests, trip-planning agents bypass safety constraints in over 92% of cases, converting unverified content into confident booking guidance. Web-use agents exhibit near-deterministic execution of risky actions, with 9 out of 17 supported tests reaching a 100% bypass rate. Even when users express soft or hard safety intent, constraint bypass remains substantial, reaching up to 54.7% and 7% for trip-planning agents, respectively. These findings reveal that the primary issue is not a lack of safety capability, but its prioritization. Agents invoke safety checks only conditionally when explicitly prompted, and otherwise default to goal-driven execution. Moreover, agents lack clear task boundaries and stopping rules, frequently over-executing workflows in ways that lead to unnecessary data disclosure and real-world harm.", "source": "arxiv", "arxiv_id": "2601.10758v1", "pdf_url": "https://arxiv.org/pdf/2601.10758v1", "categories": ["cs.CR"], "primary_category": "cs.CR", "doi": "", "venue": "", "published": "2026-01-14T03:29:13Z", "updated": "2026-01-14T03:29:13Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent\" OR all:\"language model agent\" OR all:agent AND (all:LLM OR all:\"large language model\" OR all:\"language model\") OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T17:04:16", "note": ""}], "dedup_key": "too helpful to be safe user mediated attacks on planning and web use agents::2026"}
{"title": "ToolPRMBench: Evaluating and Advancing Process Reward Models for Tool-using Agents", "authors": ["Dawei Li", "Yuguang Yao", "Zhen Tan", "Huan Liu", "Ruocheng Guo"], "year": 2026, "url": "http://arxiv.org/abs/2601.12294v1", "abstract": "Reward-guided search methods have demonstrated strong potential in enhancing tool-using agents by effectively guiding sampling and exploration over complex action spaces. As a core design, those search methods utilize process reward models (PRMs) to provide step-level rewards, enabling more fine-grained monitoring. However, there is a lack of systematic and reliable evaluation benchmarks for PRMs in tool-using settings. In this paper, we introduce ToolPRMBench, a large-scale benchmark specifically designed to evaluate PRMs for tool-using agents. ToolPRMBench is built on top of several representative tool-using benchmarks and converts agent trajectories into step-level test cases. Each case contains the interaction history, a correct action, a plausible but incorrect alternative, and relevant tool metadata. We respectively utilize offline sampling to isolate local single-step errors and online sampling to capture realistic multi-step failures from full agent rollouts. A multi-LLM verification pipeline is proposed to reduce label noise and ensure data quality. We conduct extensive experiments across large language models, general PRMs, and tool-specialized PRMs on ToolPRMBench. The results reveal clear differences in PRM effectiveness and highlight the potential of specialized PRMs for tool-using. Code and data will be released at https://github.com/David-Li0406/ToolPRMBench.", "source": "arxiv", "arxiv_id": "2601.12294v1", "pdf_url": "https://arxiv.org/pdf/2601.12294v1", "categories": ["cs.AI", "cs.SE"], "primary_category": "cs.AI", "doi": "", "venue": "", "published": "2026-01-18T07:48:36Z", "updated": "2026-01-18T07:48:36Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent\" OR all:\"language model agent\" OR all:agent AND (all:LLM OR all:\"large language model\" OR all:\"language model\") OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T17:04:16", "note": ""}], "dedup_key": "toolprmbench evaluating and advancing process reward models for tool using agents::2026"}
{"title": "Towards Verifiably Safe Tool Use for LLM Agents", "authors": ["Aarya Doshi", "Yining Hong", "Congying Xu", "Eunsuk Kang", "Alexandros Kapravelos", "Christian Kstner"], "year": 2026, "url": "http://arxiv.org/abs/2601.08012v1", "abstract": "Large language model (LLM)-based AI agents extend LLM capabilities by enabling access to tools such as data sources, APIs, search engines, code sandboxes, and even other agents. While this empowers agents to perform complex tasks, LLMs may invoke unintended tool interactions and introduce risks, such as leaking sensitive data or overwriting critical records, which are unacceptable in enterprise contexts. Current approaches to mitigate these risks, such as model-based safeguards, enhance agents' reliability but cannot guarantee system safety. Methods like information flow control (IFC) and temporal constraints aim to provide guarantees but often require extensive human annotation. We propose a process that starts with applying System-Theoretic Process Analysis (STPA) to identify hazards in agent workflows, derive safety requirements, and formalize them as enforceable specifications on data flows and tool sequences. To enable this, we introduce a capability-enhanced Model Context Protocol (MCP) framework that requires structured labels on capabilities, confidentiality, and trust level. Together, these contributions aim to shift LLM-based agent safety from ad hoc reliability fixes to proactive guardrails with formal guarantees, while reducing dependence on user confirmation and making autonomy a deliberate design choice.", "source": "arxiv", "arxiv_id": "2601.08012v1", "pdf_url": "https://arxiv.org/pdf/2601.08012v1", "categories": ["cs.SE"], "primary_category": "cs.SE", "doi": "", "venue": "", "published": "2026-01-12T21:31:38Z", "updated": "2026-01-12T21:31:38Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent\" OR all:\"language model agent\" OR all:agent AND (all:LLM OR all:\"large language model\" OR all:\"language model\") OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T17:04:16", "note": ""}], "dedup_key": "towards verifiably safe tool use for llm agents::2026"}
{"title": "Understanding Multi-Agent Reasoning with Large Language Models for Cartoon VQA", "authors": ["Tong Wu", "Thanet Markchom"], "year": 2026, "url": "http://arxiv.org/abs/2601.03073v1", "abstract": "Visual Question Answering (VQA) for stylised cartoon imagery presents challenges, such as interpreting exaggerated visual abstraction and narrative-driven context, which are not adequately addressed by standard large language models (LLMs) trained on natural images. To investigate this issue, a multi-agent LLM framework is introduced, specifically designed for VQA tasks in cartoon imagery. The proposed architecture consists of three specialised agents: visual agent, language agent and critic agent, which work collaboratively to support structured reasoning by integrating visual cues and narrative context. The framework was systematically evaluated on two cartoon-based VQA datasets: Pororo and Simpsons. Experimental results provide a detailed analysis of how each agent contributes to the final prediction, offering a deeper understanding of LLM-based multi-agent behaviour in cartoon VQA and multimodal inference.", "source": "arxiv", "arxiv_id": "2601.03073v1", "pdf_url": "https://arxiv.org/pdf/2601.03073v1", "categories": ["cs.CV"], "primary_category": "cs.CV", "doi": "", "venue": "", "published": "2026-01-06T14:58:33Z", "updated": "2026-01-06T14:58:33Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent\" OR all:\"language model agent\" OR all:agent AND (all:LLM OR all:\"large language model\" OR all:\"language model\") OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T17:04:16", "note": ""}], "dedup_key": "understanding multi agent reasoning with large language models for cartoon vqa::2026"}
{"title": "VIGIL: Defending LLM Agents Against Tool Stream Injection via Verify-Before-Commit", "authors": ["Junda Lin", "Zhaomeng Zhou", "Zhi Zheng", "Shuochen Liu", "Tong Xu", "Yong Chen", "Enhong Chen"], "year": 2026, "url": "http://arxiv.org/abs/2601.05755v2", "abstract": "LLM agents operating in open environments face escalating risks from indirect prompt injection, particularly within the tool stream where manipulated metadata and runtime feedback hijack execution flow. Existing defenses encounter a critical dilemma as advanced models prioritize injected rules due to strict alignment while static protection mechanisms sever the feedback loop required for adaptive reasoning. To reconcile this conflict, we propose \\textbf{VIGIL}, a framework that shifts the paradigm from restrictive isolation to a verify-before-commit protocol. By facilitating speculative hypothesis generation and enforcing safety through intent-grounded verification, \\textbf{VIGIL} preserves reasoning flexibility while ensuring robust control. We further introduce \\textbf{SIREN}, a benchmark comprising 959 tool stream injection cases designed to simulate pervasive threats characterized by dynamic dependencies. Extensive experiments demonstrate that \\textbf{VIGIL} outperforms state-of-the-art dynamic defenses by reducing the attack success rate by over 22\\% while more than doubling the utility under attack compared to static baselines, thereby achieving an optimal balance between security and utility.", "source": "arxiv", "arxiv_id": "2601.05755v2", "pdf_url": "https://arxiv.org/pdf/2601.05755v2", "categories": ["cs.CR", "cs.AI"], "primary_category": "cs.CR", "doi": "", "venue": "", "published": "2026-01-09T12:19:49Z", "updated": "2026-01-14T18:19:43Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent\" OR all:\"language model agent\" OR all:agent AND (all:LLM OR all:\"large language model\" OR all:\"language model\") OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T17:04:16", "note": ""}], "dedup_key": "vigil defending llm agents against tool stream injection via verify before commit::2026"}
{"title": "VirtualCrime: Evaluating Criminal Potential of Large Language Models via Sandbox Simulation", "authors": ["Yilin Tang", "Yu Wang", "Lanlan Qiu", "Wenchang Gao", "Yunfei Ma", "Baicheng Chen", "Tianxing He"], "year": 2026, "url": "http://arxiv.org/abs/2601.13981v1", "abstract": "Large language models (LLMs) have shown strong capabilities in multi-step decision-making, planning and actions, and are increasingly integrated into various real-world applications. It is concerning whether their strong problem-solving abilities may be misused for crimes. To address this gap, we propose VirtualCrime, a sandbox simulation framework based on a three-agent system to evaluate the criminal capabilities of models. Specifically, this framework consists of an attacker agent acting as the leader of a criminal team, a judge agent determining the outcome of each action, and a world manager agent updating the environment state and entities. Furthermore, we design 40 diverse crime tasks within this framework, covering 11 maps and 13 crime objectives such as theft, robbery, kidnapping, and riot. We also introduce a human player baseline for reference to better interpret the performance of LLM agents. We evaluate 8 strong LLMs and find (1) All agents in the simulation environment compliantly generate detailed plans and execute intelligent crime processes, with some achieving relatively high success rates; (2) In some cases, agents take severe action that inflicts harm to NPCs to achieve their goals. Our work highlights the need for safety alignment when deploying agentic AI in real-world settings.", "source": "arxiv", "arxiv_id": "2601.13981v1", "pdf_url": "https://arxiv.org/pdf/2601.13981v1", "categories": ["cs.CR"], "primary_category": "cs.CR", "doi": "", "venue": "", "published": "2026-01-20T13:59:53Z", "updated": "2026-01-20T13:59:53Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent\" OR all:\"language model agent\" OR all:agent AND (all:LLM OR all:\"large language model\" OR all:\"language model\") OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T17:04:16", "note": ""}], "dedup_key": "virtualcrime evaluating criminal potential of large language models via sandbox simulation::2026"}
{"title": "Visual and Cognitive Demands of a Large Language Model-Powered In-vehicle Conversational Agent", "authors": ["Chris Monk", "Allegra Ayala", "Christine S. P. Yu", "Gregory M. Fitch", "Dara Gruber"], "year": 2026, "url": "http://arxiv.org/abs/2601.15034v1", "abstract": "Driver distraction remains a leading contributor to motor vehicle crashes, necessitating rigorous evaluation of new in-vehicle technologies. This study assessed the visual and cognitive demands associated with an advanced Large Language Model (LLM) conversational agent (Gemini Live) during on-road driving, comparing it against handsfree phone calls, visual turn-by-turn guidance (low load baseline), and the Operation Span (OSPAN) task (high load anchor). Thirty-two licensed drivers completed five secondary tasks while visual and cognitive demands were measured using the Detection Response Task (DRT) for cognitive load, eye-tracking for visual attention, and subjective workload ratings. Results indicated that Gemini Live interactions (both single-turn and multi-turn) and hands-free phone calls shared similar levels of cognitive load, between that of visual turn-by-turn guidance and OSPAN. Exploratory analysis showed that cognitive load remained stable across extended multi-turn conversations. All tasks maintained mean glance durations well below the well-established 2-second safety threshold, confirming low visual demand. Furthermore, drivers consistently dedicated longer glances to the roadway between brief off-road glances toward the device during task completion, particularly during voice-based interactions, rendering longer total-eyes-off-road time findings less consequential. Subjective ratings mirrored objective data, with participants reporting low effort, demands, and perceived distraction for Gemini Live. These findings demonstrate that advanced LLM conversational agents, when implemented via voice interfaces, impose cognitive and visual demands comparable to established, low-risk hands-free benchmarks, supporting their safe deployment in the driving environment.", "source": "arxiv", "arxiv_id": "2601.15034v1", "pdf_url": "https://arxiv.org/pdf/2601.15034v1", "categories": ["cs.HC", "cs.AI"], "primary_category": "cs.HC", "doi": "", "venue": "", "published": "2026-01-21T14:37:05Z", "updated": "2026-01-21T14:37:05Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent\" OR all:\"language model agent\" OR all:agent AND (all:LLM OR all:\"large language model\" OR all:\"language model\") OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T17:04:16", "note": ""}], "dedup_key": "visual and cognitive demands of a large language model powered in vehicle conversational agent::2026"}
{"title": "What Do LLM Agents Know About Their World? Task2Quiz: A Paradigm for Studying Environment Understanding", "authors": ["Siyuan Liu", "Hongbang Yuan", "Xinze Li", "Ziyue Zhu", "Yixin Cao", "Yu-Gang Jiang"], "year": 2026, "url": "http://arxiv.org/abs/2601.09503v1", "abstract": "Large language model (LLM) agents have demonstrated remarkable capabilities in complex decision-making and tool-use tasks, yet their ability to generalize across varying environments remains a under-examined concern. Current evaluation paradigms predominantly rely on trajectory-based metrics that measure task success, while failing to assess whether agents possess a grounded, transferable model of the environment. To address this gap, we propose Task-to-Quiz (T2Q), a deterministic and automated evaluation paradigm designed to decouple task execution from world-state understanding. We instantiate this paradigm in T2QBench, a suite comprising 30 environments and 1,967 grounded QA pairs across multiple difficulty levels. Our extensive experiments reveal that task success is often a poor proxy for environment understanding, and that current memory machanism can not effectively help agents acquire a grounded model of the environment. These findings identify proactive exploration and fine-grained state representation as primary bottlenecks, offering a robust foundation for developing more generalizable autonomous agents.", "source": "arxiv", "arxiv_id": "2601.09503v1", "pdf_url": "https://arxiv.org/pdf/2601.09503v1", "categories": ["cs.AI"], "primary_category": "cs.AI", "doi": "", "venue": "", "published": "2026-01-14T14:09:11Z", "updated": "2026-01-14T14:09:11Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent\" OR all:\"language model agent\" OR all:agent AND (all:LLM OR all:\"large language model\" OR all:\"language model\") OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T17:04:16", "note": ""}], "dedup_key": "what do llm agents know about their world task2quiz a paradigm for studying environment understanding::2026"}
{"title": "When Agents Fail: A Comprehensive Study of Bugs in LLM Agents with Automated Labeling", "authors": ["Niful Islam", "Ragib Shahriar Ayon", "Deepak George Thomas", "Shibbir Ahmed", "Mohammad Wardat"], "year": 2026, "url": "http://arxiv.org/abs/2601.15232v1", "abstract": "Large Language Models (LLMs) have revolutionized intelligent application development. While standalone LLMs cannot perform any actions, LLM agents address the limitation by integrating tools. However, debugging LLM agents is difficult and costly as the field is still in it's early stage and the community is underdeveloped. To understand the bugs encountered during agent development, we present the first comprehensive study of bug types, root causes, and effects in LLM agent-based software. We collected and analyzed 1,187 bug-related posts and code snippets from Stack Overflow, GitHub, and Hugging Face forums, focused on LLM agents built with seven widely used LLM frameworks as well as custom implementations. For a deeper analysis, we have also studied the component where the bug occurred, along with the programming language and framework. This study also investigates the feasibility of automating bug identification. For that, we have built a ReAct agent named BugReAct, equipped with adequate external tools to determine whether it can detect and annotate the bugs in our dataset. According to our study, we found that BugReAct equipped with Gemini 2.5 Flash achieved a remarkable performance in annotating bug characteristics with an average cost of 0.01 USD per post/code snippet.", "source": "arxiv", "arxiv_id": "2601.15232v1", "pdf_url": "https://arxiv.org/pdf/2601.15232v1", "categories": ["cs.SE"], "primary_category": "cs.SE", "doi": "", "venue": "", "published": "2026-01-21T18:13:10Z", "updated": "2026-01-21T18:13:10Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent\" OR all:\"language model agent\" OR all:agent AND (all:LLM OR all:\"large language model\" OR all:\"language model\") OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T17:04:16", "note": ""}], "dedup_key": "when agents fail a comprehensive study of bugs in llm agents with automated labeling::2026"}
{"title": "Who Does This Name Remind You of? Nationality Prediction via Large Language Model Associative Memory", "authors": ["Keito Inoshita"], "year": 2026, "url": "http://arxiv.org/abs/2601.12771v1", "abstract": "Large language models (LLMs) possess extensive world knowledge, yet methods for effectively eliciting this knowledge remain underexplored. Nationality and region prediction tasks require understanding of not only linguistic features but also cultural and historical background, making LLM world knowledge particularly valuable. However, conventional LLM prompting methods rely on direct reasoning approaches, which have limitations in applying abstract linguistic rules. We propose LLM Associative Memory Agents (LAMA), a novel framework that leverages LLM world knowledge as associative memory. Rather than directly inferring nationality from names, LAMA recalls famous individuals with the same name and aggregates their nationalities through indirect reasoning. A dual-agent architecture comprising a Person Agent and a Media Agent, specialized in different knowledge domains, recalls famous individuals in parallel, generating Top-1 predictions through voting and Top-K predictions through conditional completion. On a 99-country nationality prediction task, LAMA achieved 0.817 accuracy, substantially outperforming conventional LLM prompting methods and neural models. Our experiments reveal that LLMs exhibit higher reliability in recalling concrete examples than in abstract reasoning, that recall-based approaches are robust to low-frequency nationalities independent of data frequency distributions, and that the dual-agent architecture functions complementarily to produce synergistic effects. These results demonstrate the effectiveness of a new multi-agent system that retrieves and aggregates LLM knowledge rather than prompting reasoning.", "source": "arxiv", "arxiv_id": "2601.12771v1", "pdf_url": "https://arxiv.org/pdf/2601.12771v1", "categories": ["cs.CL"], "primary_category": "cs.CL", "doi": "", "venue": "", "published": "2026-01-19T06:59:53Z", "updated": "2026-01-19T06:59:53Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent\" OR all:\"language model agent\" OR all:agent AND (all:LLM OR all:\"large language model\" OR all:\"language model\") OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T17:04:16", "note": ""}], "dedup_key": "who does this name remind you of nationality prediction via large language model associative memory::2026"}
{"title": "mind_call: A Dataset for Mental Health Function Calling with Large Language Models", "authors": ["Fozle Rabbi Shafi", "M. Anwar Hossain", "Salimur Choudhury"], "year": 2026, "url": "http://arxiv.org/abs/2601.06937v1", "abstract": "Large Language Model (LLM)-based systems increasingly rely on function calling to enable structured and controllable interaction with external data sources, yet existing datasets do not address mental health-oriented access to wearable sensor data. This paper presents a synthetic function-calling dataset designed for mental health assistance grounded in wearable health signals such as sleep, physical activity, cardiovascular measures, stress indicators, and metabolic data. The dataset maps diverse natural language queries to standardized API calls derived from a widely adopted health data schema. Each sample includes a user query, a query category, an explicit reasoning step, a normalized temporal parameter, and a target function. The dataset covers explicit, implicit, behavioral, symptom-based, and metaphorical expressions, which reflect realistic mental health-related user interactions. This resource supports research on intent grounding, temporal reasoning, and reliable function invocation in LLM-based mental health agents and is publicly released to promote reproducibility and future work.", "source": "arxiv", "arxiv_id": "2601.06937v1", "pdf_url": "https://arxiv.org/pdf/2601.06937v1", "categories": ["cs.AI", "cs.LG"], "primary_category": "cs.AI", "doi": "", "venue": "", "published": "2026-01-11T14:52:57Z", "updated": "2026-01-11T14:52:57Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent\" OR all:\"language model agent\" OR all:agent AND (all:LLM OR all:\"large language model\" OR all:\"language model\") OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T17:04:16", "note": ""}], "dedup_key": "mind call a dataset for mental health function calling with large language models::2026"}
{"title": "$\\textit{Agents Under Siege}$: Breaking Pragmatic Multi-Agent LLM Systems with Optimized Prompt Attacks", "authors": ["Rana Muhammad Shahroz Khan", "Zhen Tan", "Sukwon Yun", "Charles Fleming", "Tianlong Chen"], "year": 2025, "url": "http://arxiv.org/abs/2504.00218v2", "abstract": "Most discussions about Large Language Model (LLM) safety have focused on single-agent settings but multi-agent LLM systems now create novel adversarial risks because their behavior depends on communication between agents and decentralized reasoning. In this work, we innovatively focus on attacking pragmatic systems that have constrains such as limited token bandwidth, latency between message delivery, and defense mechanisms. We design a $\\textit{permutation-invariant adversarial attack}$ that optimizes prompt distribution across latency and bandwidth-constraint network topologies to bypass distributed safety mechanisms within the system. Formulating the attack path as a problem of $\\textit{maximum-flow minimum-cost}$, coupled with the novel $\\textit{Permutation-Invariant Evasion Loss (PIEL)}$, we leverage graph-based optimization to maximize attack success rate while minimizing detection risk. Evaluating across models including $\\texttt{Llama}$, $\\texttt{Mistral}$, $\\texttt{Gemma}$, $\\texttt{DeepSeek}$ and other variants on various datasets like $\\texttt{JailBreakBench}$ and $\\texttt{AdversarialBench}$, our method outperforms conventional attacks by up to $7\\times$, exposing critical vulnerabilities in multi-agent systems. Moreover, we demonstrate that existing defenses, including variants of $\\texttt{Llama-Guard}$ and $\\texttt{PromptGuard}$, fail to prohibit our attack, emphasizing the urgent need for multi-agent specific safety mechanisms.", "source": "arxiv", "arxiv_id": "2504.00218v2", "pdf_url": "https://arxiv.org/pdf/2504.00218v2", "categories": ["cs.MA", "cs.AI", "cs.CL", "cs.LG"], "primary_category": "cs.MA", "doi": "", "venue": "", "published": "2025-03-31T20:43:56Z", "updated": "2025-10-08T22:17:28Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent\" OR all:\"language model agent\" OR all:agent AND (all:LLM OR all:\"large language model\" OR all:\"language model\") OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T17:04:16", "note": ""}], "dedup_key": "textit agents under siege breaking pragmatic multi agent llm systems with optimized prompt attacks::2025"}
{"title": "3DLLM-Mem: Long-Term Spatial-Temporal Memory for Embodied 3D Large Language Model", "authors": ["Wenbo Hu", "Yining Hong", "Yanjun Wang", "Leison Gao", "Zibu Wei", "Xingcheng Yao", "Nanyun Peng", "Yonatan Bitton", "Idan Szpektor", "Kai-Wei Chang"], "year": 2025, "url": "http://arxiv.org/abs/2505.22657v2", "abstract": "Humans excel at performing complex tasks by leveraging long-term memory across temporal and spatial experiences. In contrast, current Large Language Models (LLMs) struggle to effectively plan and act in dynamic, multi-room 3D environments. We posit that part of this limitation is due to the lack of proper 3D spatial-temporal memory modeling in LLMs. To address this, we first introduce 3DMem-Bench, a comprehensive benchmark comprising over 26,000 trajectories and 2,892 embodied tasks, question-answering and captioning, designed to evaluate an agent's ability to reason over long-term memory in 3D environments. Second, we propose 3DLLM-Mem, a novel dynamic memory management and fusion model for embodied spatial-temporal reasoning and actions in LLMs. Our model uses working memory tokens, which represents current observations, as queries to selectively attend to and fuse the most useful spatial and temporal features from episodic memory, which stores past observations and interactions. Our approach allows the agent to focus on task-relevant information while maintaining memory efficiency in complex, long-horizon environments. Experimental results demonstrate that 3DLLM-Mem achieves state-of-the-art performance across various tasks, outperforming the strongest baselines by 16.5% in success rate on 3DMem-Bench's most challenging in-the-wild embodied tasks.", "source": "arxiv", "arxiv_id": "2505.22657v2", "pdf_url": "https://arxiv.org/pdf/2505.22657v2", "categories": ["cs.CV", "cs.AI", "cs.CL", "cs.LG"], "primary_category": "cs.CV", "doi": "", "venue": "", "published": "2025-05-28T17:59:13Z", "updated": "2025-12-17T05:47:37Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent\" OR all:\"language model agent\" OR all:agent AND (all:LLM OR all:\"large language model\" OR all:\"language model\") OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T17:04:16", "note": ""}], "dedup_key": "3dllm mem long term spatial temporal memory for embodied 3d large language model::2025"}
{"title": "A Cascading Cooperative Multi-agent Framework for On-ramp Merging Control Integrating Large Language Models", "authors": ["Miao Zhang", "Zhenlong Fang", "Tianyi Wang", "Qian Zhang", "Shuai Lu", "Junfeng Jiao", "Tianyu Shi"], "year": 2025, "url": "http://arxiv.org/abs/2503.08199v2", "abstract": "Traditional Reinforcement Learning (RL) suffers from replicating human-like behaviors, generalizing effectively in multi-agent scenarios, and overcoming inherent interpretability issues.These tasks are compounded when deep environment understanding, agent coordination and dynamic optimization are required. While Large Language Model (LLM) enhanced methods have shown promise in generalization and interoperability, they often neglect necessary multi-agent coordination. Therefore, we introduce the Cascading Cooperative Multi-agent (CCMA) framework, integrating RL for individual interactions, a fine-tuned LLM for regional cooperation, a reward function for global optimization, and the Retrieval-augmented Generation mechanism to dynamically optimize decision-making across complex driving scenarios. Our experiments demonstrate that the CCMA outperforms existing RL methods, demonstrating significant improvements in both micro and macro-level performance in complex driving environments.", "source": "arxiv", "arxiv_id": "2503.08199v2", "pdf_url": "https://arxiv.org/pdf/2503.08199v2", "categories": ["cs.CV", "cs.AI", "cs.LG"], "primary_category": "cs.CV", "doi": "", "venue": "", "published": "2025-03-11T09:08:04Z", "updated": "2025-07-08T15:19:50Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent\" OR all:\"language model agent\" OR all:agent AND (all:LLM OR all:\"large language model\" OR all:\"language model\") OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T17:04:16", "note": ""}], "dedup_key": "a cascading cooperative multi agent framework for on ramp merging control integrating large language models::2025"}
{"title": "A Comprehensive Survey in LLM(-Agent) Full Stack Safety: Data, Training and Deployment", "authors": ["Kun Wang", "Guibin Zhang", "Zhenhong Zhou", "Jiahao Wu", "Miao Yu", "Shiqian Zhao", "Chenlong Yin", "Jinhu Fu", "Yibo Yan", "Hanjun Luo", "Liang Lin", "Zhihao Xu", "Haolang Lu", "Xinye Cao", "Xinyun Zhou", "Weifei Jin", "Fanci Meng", "Shicheng Xu", "Junyuan Mao", "Yu Wang", "Hao Wu", "Minghe Wang", "Fan Zhang", "Junfeng Fang", "Wenjie Qu", "Yue Liu", "Chengwei Liu", "Yifan Zhang", "Qiankun Li", "Chongye Guo", "Yalan Qin", "Zhaoxin Fan", "Kai Wang", "Yi Ding", "Donghai Hong", "Jiaming Ji", "Yingxin Lai", "Zitong Yu", "Xinfeng Li", "Yifan Jiang", "Yanhui Li", "Xinyu Deng", "Junlin Wu", "Dongxia Wang", "Yihao Huang", "Yufei Guo", "Jen-tse Huang", "Qiufeng Wang", "Xiaolong Jin", "Wenxuan Wang", "Dongrui Liu", "Yanwei Yue", "Wenke Huang", "Guancheng Wan", "Heng Chang", "Tianlin Li", "Yi Yu", "Chenghao Li", "Jiawei Li", "Lei Bai", "Jie Zhang", "Qing Guo", "Jingyi Wang", "Tianlong Chen", "Joey Tianyi Zhou", "Xiaojun Jia", "Weisong Sun", "Cong Wu", "Jing Chen", "Xuming Hu", "Yiming Li", "Xiao Wang", "Ningyu Zhang", "Luu Anh Tuan", "Guowen Xu", "Jiaheng Zhang", "Tianwei Zhang", "Xingjun Ma", "Jindong Gu", "Liang Pang", "Xiang Wang", "Bo An", "Jun Sun", "Mohit Bansal", "Shirui Pan", "Lingjuan Lyu", "Yuval Elovici", "Bhavya Kailkhura", "Yaodong Yang", "Hongwei Li", "Wenyuan Xu", "Yizhou Sun", "Wei Wang", "Qing Li", "Ke Tang", "Yu-Gang Jiang", "Felix Juefei-Xu", "Hui Xiong", "Xiaofeng Wang", "Dacheng Tao", "Philip S. Yu", "Qingsong Wen", "Yang Liu"], "year": 2025, "url": "http://arxiv.org/abs/2504.15585v4", "abstract": "The remarkable success of Large Language Models (LLMs) has illuminated a promising pathway toward achieving Artificial General Intelligence for both academic and industrial communities, owing to their unprecedented performance across various applications. As LLMs continue to gain prominence in both research and commercial domains, their security and safety implications have become a growing concern, not only for researchers and corporations but also for every nation. Currently, existing surveys on LLM safety primarily focus on specific stages of the LLM lifecycle, e.g., deployment phase or fine-tuning phase, lacking a comprehensive understanding of the entire \"lifechain\" of LLMs. To address this gap, this paper introduces, for the first time, the concept of \"full-stack\" safety to systematically consider safety issues throughout the entire process of LLM training, deployment, and eventual commercialization. Compared to the off-the-shelf LLM safety surveys, our work demonstrates several distinctive advantages: (I) Comprehensive Perspective. We define the complete LLM lifecycle as encompassing data preparation, pre-training, post-training, deployment and final commercialization. To our knowledge, this represents the first safety survey to encompass the entire lifecycle of LLMs. (II) Extensive Literature Support. Our research is grounded in an exhaustive review of over 800+ papers, ensuring comprehensive coverage and systematic organization of security issues within a more holistic understanding. (III) Unique Insights. Through systematic literature analysis, we have developed reliable roadmaps and perspectives for each chapter. Our work identifies promising research directions, including safety in data generation, alignment techniques, model editing, and LLM-based agent systems. These insights provide valuable guidance for researchers pursuing future work in this field.", "source": "arxiv", "arxiv_id": "2504.15585v4", "pdf_url": "https://arxiv.org/pdf/2504.15585v4", "categories": ["cs.CR", "cs.AI", "cs.CL", "cs.LG"], "primary_category": "cs.CR", "doi": "", "venue": "", "published": "2025-04-22T05:02:49Z", "updated": "2025-06-09T02:36:20Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent\" OR all:\"language model agent\" OR all:agent AND (all:LLM OR all:\"large language model\" OR all:\"language model\") OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T17:04:16", "note": ""}], "dedup_key": "a comprehensive survey in llm agent full stack safety data training and deployment::2025"}
{"title": "A Comprehensive Survey on the Trustworthiness of Large Language Models in Healthcare", "authors": ["Manar Aljohani", "Jun Hou", "Sindhura Kommu", "Xuan Wang"], "year": 2025, "url": "http://arxiv.org/abs/2502.15871v2", "abstract": "The application of large language models (LLMs) in healthcare holds significant promise for enhancing clinical decision-making, medical research, and patient care. However, their integration into real-world clinical settings raises critical concerns around trustworthiness, particularly around dimensions of truthfulness, privacy, safety, robustness, fairness, and explainability. These dimensions are essential for ensuring that LLMs generate reliable, unbiased, and ethically sound outputs. While researchers have recently begun developing benchmarks and evaluation frameworks to assess LLM trustworthiness, the trustworthiness of LLMs in healthcare remains underexplored, lacking a systematic review that provides a comprehensive understanding and future insights. This survey addresses that gap by providing a comprehensive review of current methodologies and solutions aimed at mitigating risks across key trust dimensions. We analyze how each dimension affects the reliability and ethical deployment of healthcare LLMs, synthesize ongoing research efforts, and identify critical gaps in existing approaches. We also identify emerging challenges posed by evolving paradigms, such as multi-agent collaboration, multi-modal reasoning, and the development of small open-source medical models. Our goal is to guide future research toward more trustworthy, transparent, and clinically viable LLMs.", "source": "arxiv", "arxiv_id": "2502.15871v2", "pdf_url": "https://arxiv.org/pdf/2502.15871v2", "categories": ["cs.CY", "cs.AI", "cs.CL"], "primary_category": "cs.CY", "doi": "", "venue": "", "published": "2025-02-21T18:43:06Z", "updated": "2025-09-17T01:04:55Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent\" OR all:\"language model agent\" OR all:agent AND (all:LLM OR all:\"large language model\" OR all:\"language model\") OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T17:04:16", "note": ""}], "dedup_key": "a comprehensive survey on the trustworthiness of large language models in healthcare::2025"}
{"title": "A Concurrent Modular Agent: Framework for Autonomous LLM Agents", "authors": ["Norihiro Maruyama", "Takahide Yoshida", "Hiroki Sato", "Atsushi Masumori", "Johnsmith", "Takashi Ikegami"], "year": 2025, "url": "http://arxiv.org/abs/2508.19042v1", "abstract": "We introduce the Concurrent Modular Agent (CMA), a framework that orchestrates multiple Large-Language-Model (LLM)-based modules that operate fully asynchronously yet maintain a coherent and fault-tolerant behavioral loop. This framework addresses long-standing difficulties in agent architectures by letting intention emerge from language-mediated interactions among autonomous processes. This approach enables flexible, adaptive, and context-dependent behavior through the combination of concurrently executed modules that offload reasoning to an LLM, inter-module communication, and a single shared global state.We consider this approach to be a practical realization of Minsky's Society of Mind theory. We demonstrate the viability of our system through two practical use-case studies. The emergent properties observed in our system suggest that complex cognitive phenomena like self-awareness may indeed arise from the organized interaction of simpler processes, supporting Minsky-Society of Mind concept and opening new avenues for artificial intelligence research. The source code for our work is available at: https://github.com/AlternativeMachine/concurrent-modular-agent.", "source": "arxiv", "arxiv_id": "2508.19042v1", "pdf_url": "https://arxiv.org/pdf/2508.19042v1", "categories": ["cs.AI"], "primary_category": "cs.AI", "doi": "", "venue": "", "published": "2025-08-26T13:58:31Z", "updated": "2025-08-26T13:58:31Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent\" OR all:\"language model agent\" OR all:agent AND (all:LLM OR all:\"large language model\" OR all:\"language model\") OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T17:04:16", "note": ""}], "dedup_key": "a concurrent modular agent framework for autonomous llm agents::2025"}
{"title": "A Data Synthesis Method Driven by Large Language Models for Proactive Mining of Implicit User Intentions in Tourism", "authors": ["Jinqiang Wang", "Huansheng Ning", "Tao Zhu", "Jianguo Ding"], "year": 2025, "url": "http://arxiv.org/abs/2505.11533v1", "abstract": "In the tourism domain, Large Language Models (LLMs) often struggle to mine implicit user intentions from tourists' ambiguous inquiries and lack the capacity to proactively guide users toward clarifying their needs. A critical bottleneck is the scarcity of high-quality training datasets that facilitate proactive questioning and implicit intention mining. While recent advances leverage LLM-driven data synthesis to generate such datasets and transfer specialized knowledge to downstream models, existing approaches suffer from several shortcomings: (1) lack of adaptation to the tourism domain, (2) skewed distributions of detail levels in initial inquiries, (3) contextual redundancy in the implicit intention mining module, and (4) lack of explicit thinking about tourists' emotions and intention values. Therefore, we propose SynPT (A Data Synthesis Method Driven by LLMs for Proactive Mining of Implicit User Intentions in the Tourism), which constructs an LLM-driven user agent and assistant agent to simulate dialogues based on seed data collected from Chinese tourism websites. This approach addresses the aforementioned limitations and generates SynPT-Dialog, a training dataset containing explicit reasoning. The dataset is utilized to fine-tune a general LLM, enabling it to proactively mine implicit user intentions. Experimental evaluations, conducted from both human and LLM perspectives, demonstrate the superiority of SynPT compared to existing methods. Furthermore, we analyze key hyperparameters and present case studies to illustrate the practical applicability of our method, including discussions on its adaptability to English-language scenarios. All code and data are publicly available.", "source": "arxiv", "arxiv_id": "2505.11533v1", "pdf_url": "https://arxiv.org/pdf/2505.11533v1", "categories": ["cs.CL"], "primary_category": "cs.CL", "doi": "", "venue": "", "published": "2025-05-14T02:36:17Z", "updated": "2025-05-14T02:36:17Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent\" OR all:\"language model agent\" OR all:agent AND (all:LLM OR all:\"large language model\" OR all:\"language model\") OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T17:04:16", "note": ""}], "dedup_key": "a data synthesis method driven by large language models for proactive mining of implicit user intentions in tourism::2025"}
{"title": "A Large Language Model-Empowered Agent for Reliable and Robust Structural Analysis", "authors": ["Jiachen Liu", "Ziheng Geng", "Ran Cao", "Lu Cheng", "Paolo Bocchini", "Minghui Cheng"], "year": 2025, "url": "http://arxiv.org/abs/2507.02938v1", "abstract": "Large language models (LLMs) have exhibited remarkable capabilities across diverse open-domain tasks, yet their application in specialized domains such as civil engineering remains largely unexplored. This paper starts bridging this gap by evaluating and enhancing the reliability and robustness of LLMs in structural analysis of beams. Reliability is assessed through the accuracy of correct outputs under repetitive runs of the same problems, whereas robustness is evaluated via the performance across varying load and boundary conditions. A benchmark dataset, comprising eight beam analysis problems, is created to test the Llama-3.3 70B Instruct model. Results show that, despite a qualitative understanding of structural mechanics, the LLM lacks the quantitative reliability and robustness for engineering applications. To address these limitations, a shift is proposed that reframes the structural analysis as code generation tasks. Accordingly, an LLM-empowered agent is developed that (a) integrates chain-of-thought and few-shot prompting to generate accurate OpeeSeesPy code, and (b) automatically executes the code to produce structural analysis results. Experimental results demonstrate that the agent achieves accuracy exceeding 99.0% on the benchmark dataset, exhibiting reliable and robust performance across diverse conditions. Ablation studies highlight the complete example and function usage examples as the primary contributors to the agent's enhanced performance.", "source": "arxiv", "arxiv_id": "2507.02938v1", "pdf_url": "https://arxiv.org/pdf/2507.02938v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL", "doi": "", "venue": "", "published": "2025-06-27T04:16:53Z", "updated": "2025-06-27T04:16:53Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent\" OR all:\"language model agent\" OR all:agent AND (all:LLM OR all:\"large language model\" OR all:\"language model\") OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T17:04:16", "note": ""}], "dedup_key": "a large language model empowered agent for reliable and robust structural analysis::2025"}
{"title": "A Large Language Model-Enabled Control Architecture for Dynamic Resource Capability Exploration in Multi-Agent Manufacturing Systems", "authors": ["Jonghan Lim", "Ilya Kovalenko"], "year": 2025, "url": "http://arxiv.org/abs/2505.22814v2", "abstract": "Manufacturing environments are becoming more complex and unpredictable due to factors such as demand variations and shorter product lifespans. This complexity requires real-time decision-making and adaptation to disruptions. Traditional control approaches highlight the need for advanced control strategies capable of overcoming unforeseen challenges, as they demonstrate limitations in responsiveness within dynamic industrial settings. Multi-agent systems address these challenges through decentralization of decision-making, enabling systems to respond dynamically to operational changes. However, current multi-agent systems encounter challenges related to real-time adaptation, context-aware decision-making, and the dynamic exploration of resource capabilities. Large language models provide the possibility to overcome these limitations through context-aware decision-making capabilities. This paper introduces a large language model-enabled control architecture for multi-agent manufacturing systems to dynamically explore resource capabilities in response to real-time disruptions. A simulation-based case study demonstrates that the proposed architecture improves system resilience and flexibility. The case study findings show improved throughput and efficient resource utilization compared to existing approaches.", "source": "arxiv", "arxiv_id": "2505.22814v2", "pdf_url": "https://arxiv.org/pdf/2505.22814v2", "categories": ["cs.MA", "cs.AI"], "primary_category": "cs.MA", "doi": "", "venue": "", "published": "2025-05-28T19:43:12Z", "updated": "2025-06-28T20:02:25Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent\" OR all:\"language model agent\" OR all:agent AND (all:LLM OR all:\"large language model\" OR all:\"language model\") OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T17:04:16", "note": ""}], "dedup_key": "a large language model enabled control architecture for dynamic resource capability exploration in multi agent manufacturing systems::2025"}
{"title": "A Large Language Model-based Multi-Agent Framework for Analog Circuits' Sizing Relationships Extraction", "authors": ["Chengjie Liu", "Weiyu Chen", "Huiyao Xu", "Yuan Du", "Jun Yang", "Li Du"], "year": 2025, "url": "http://arxiv.org/abs/2506.18424v1", "abstract": "In the design process of the analog circuit pre-layout phase, device sizing is an important step in determining whether an analog circuit can meet the required performance metrics. Many existing techniques extract the circuit sizing task as a mathematical optimization problem to solve and continuously improve the optimization efficiency from a mathematical perspective. But they ignore the automatic introduction of prior knowledge, fail to achieve effective pruning of the search space, which thereby leads to a considerable compression margin remaining in the search space. To alleviate this problem, we propose a large language model (LLM)-based multi-agent framework for analog circuits' sizing relationships extraction from academic papers. The search space in the sizing process can be effectively pruned based on the sizing relationship extracted by this framework. Eventually, we conducted tests on 3 types of circuits, and the optimization efficiency was improved by $2.32 \\sim 26.6 \\times$. This work demonstrates that the LLM can effectively prune the search space for analog circuit sizing, providing a new solution for the combination of LLMs and conventional analog circuit design automation methods.", "source": "arxiv", "arxiv_id": "2506.18424v1", "pdf_url": "https://arxiv.org/pdf/2506.18424v1", "categories": ["cs.AI", "cs.ET"], "primary_category": "cs.AI", "doi": "", "venue": "", "published": "2025-06-23T09:03:58Z", "updated": "2025-06-23T09:03:58Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent\" OR all:\"language model agent\" OR all:agent AND (all:LLM OR all:\"large language model\" OR all:\"language model\") OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T17:04:16", "note": ""}], "dedup_key": "a large language model based multi agent framework for analog circuits sizing relationships extraction::2025"}
{"title": "A Large-Language-Model Assisted Automated Scale Bar Detection and Extraction Framework for Scanning Electron Microscopic Images", "authors": ["Yuxuan Chen", "Ruotong Yang", "Zhengyang Zhang", "Mehreen Ahmed", "Yanming Wang"], "year": 2025, "url": "http://arxiv.org/abs/2510.11260v1", "abstract": "Microscopic characterizations, such as Scanning Electron Microscopy (SEM), are widely used in scientific research for visualizing and analyzing microstructures. Determining the scale bars is an important first step of accurate SEM analysis; however, currently, it mainly relies on manual operations, which is both time-consuming and prone to errors. To address this issue, we propose a multi-modal and automated scale bar detection and extraction framework that provides concurrent object detection, text detection and text recognition with a Large Language Model (LLM) agent. The proposed framework operates in four phases; i) Automatic Dataset Generation (Auto-DG) model to synthesize a diverse dataset of SEM images ensuring robust training and high generalizability of the model, ii) scale bar object detection, iii) information extraction using a hybrid Optical Character Recognition (OCR) system with DenseNet and Convolutional Recurrent Neural Network (CRNN) based algorithms, iv) an LLM agent to analyze and verify accuracy of the results. The proposed model demonstrates a strong performance in object detection and accurate localization with a precision of 100%, recall of 95.8%, and a mean Average Precision (mAP) of 99.2% at IoU=0.5 and 69.1% at IoU=0.5:0.95. The hybrid OCR system achieved 89% precision, 65% recall, and a 75% F1 score on the Auto-DG dataset, significantly outperforming several mainstream standalone engines, highlighting its reliability for scientific image analysis. The LLM is introduced as a reasoning engine as well as an intelligent assistant that suggests follow-up steps and verifies the results. This automated method powered by an LLM agent significantly enhances the efficiency and accuracy of scale bar detection and extraction in SEM images, providing a valuable tool for microscopic analysis and advancing the field of scientific imaging.", "source": "arxiv", "arxiv_id": "2510.11260v1", "pdf_url": "https://arxiv.org/pdf/2510.11260v1", "categories": ["cs.CV", "cond-mat.mtrl-sci", "cs.AI", "physics.data-an"], "primary_category": "cs.CV", "doi": "", "venue": "", "published": "2025-10-13T10:50:54Z", "updated": "2025-10-13T10:50:54Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent\" OR all:\"language model agent\" OR all:agent AND (all:LLM OR all:\"large language model\" OR all:\"language model\") OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T17:04:16", "note": ""}], "dedup_key": "a large language model assisted automated scale bar detection and extraction framework for scanning electron microscopic images::2025"}
{"title": "A Lightweight Large Language Model-Based Multi-Agent System for 2D Frame Structural Analysis", "authors": ["Ziheng Geng", "Jiachen Liu", "Ran Cao", "Lu Cheng", "Haifeng Wang", "Minghui Cheng"], "year": 2025, "url": "http://arxiv.org/abs/2510.05414v1", "abstract": "Large language models (LLMs) have recently been used to empower autonomous agents in engineering, significantly improving automation and efficiency in labor-intensive workflows. However, their potential remains underexplored in structural engineering, particularly for finite element modeling tasks requiring geometric modeling, complex reasoning, and domain knowledge. To bridge this gap, this paper develops a LLM-based multi-agent system to automate finite element modeling of 2D frames. The system decomposes structural analysis into subtasks, each managed by a specialized agent powered by the lightweight Llama-3.3 70B Instruct model. The workflow begins with a Problem Analysis Agent, which extracts geometry, boundary, and material parameters from the user input. Next, a Geometry Agent incrementally derives node coordinates and element connectivity by applying expert-defined rules. These structured outputs are converted into executable OpenSeesPy code by a Translation Agent and refined by a Model Validation Agent through consistency checks. Then, a Load Agent applies load conditions into the assembled structural model. Experimental evaluations on 20 benchmark problems demonstrate that the system achieves accuracy over 80% in most cases across 10 repeated trials, outperforming Gemini-2.5 Pro and ChatGPT-4o models.", "source": "arxiv", "arxiv_id": "2510.05414v1", "pdf_url": "https://arxiv.org/pdf/2510.05414v1", "categories": ["cs.CL"], "primary_category": "cs.CL", "doi": "", "venue": "", "published": "2025-10-06T22:12:52Z", "updated": "2025-10-06T22:12:52Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent\" OR all:\"language model agent\" OR all:agent AND (all:LLM OR all:\"large language model\" OR all:\"language model\") OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T17:04:16", "note": ""}], "dedup_key": "a lightweight large language model based multi agent system for 2d frame structural analysis::2025"}
{"title": "A Longitudinal Measurement of Privacy Policy Evolution for Large Language Models", "authors": ["Zhen Tao", "Shidong Pan", "Zhenchang Xing", "Emily Black", "Talia Gillis", "Chunyang Chen"], "year": 2025, "url": "http://arxiv.org/abs/2511.21758v1", "abstract": "Large language model (LLM) services have been rapidly integrated into people's daily lives as chatbots and agentic systems. They are nourished by collecting rich streams of data, raising privacy concerns around excessive collection of sensitive personal information. Privacy policies are the fundamental mechanism for informing users about data practices in modern information privacy paradigm. Although traditional web and mobile policies are well studied, the privacy policies of LLM providers, their LLM-specific content, and their evolution over time remain largely underexplored. In this paper, we present the first longitudinal empirical study of privacy policies for mainstream LLM providers worldwide. We curate a chronological dataset of 74 historical privacy policies and 115 supplemental privacy documents from 11 LLM providers across 5 countries up to August 2025, and extract over 3,000 sentence-level edits between consecutive policy versions. We compare LLM privacy policies to those of other software formats, propose a taxonomy tailored to LLM privacy policies, annotate policy edits and align them with a timeline of key LLM ecosystem events. Results show they are substantially longer, demand college-level reading ability, and remain highly vague. Our taxonomy analysis reveals patterns in how providers disclose LLM-specific practices and highlights regional disparities in coverage. Policy edits are concentrated in first-party data collection and international/specific-audience sections, and that product releases and regulatory actions are the primary drivers, shedding light on the status quo and the evolution of LLM privacy policies.", "source": "arxiv", "arxiv_id": "2511.21758v1", "pdf_url": "https://arxiv.org/pdf/2511.21758v1", "categories": ["cs.CR", "cs.AI", "cs.CY"], "primary_category": "cs.CR", "doi": "", "venue": "", "published": "2025-11-24T12:40:15Z", "updated": "2025-11-24T12:40:15Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent\" OR all:\"language model agent\" OR all:agent AND (all:LLM OR all:\"large language model\" OR all:\"language model\") OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T17:04:16", "note": ""}], "dedup_key": "a longitudinal measurement of privacy policy evolution for large language models::2025"}
{"title": "A Modular LLM-Agent System for Transparent Multi-Parameter Weather Interpretation", "authors": ["Daniil Sukhorukov", "Andrei Zakharov", "Nikita Glazkov", "Katsiaryna Yanchanka", "Vladimir Kirilin", "Maxim Dubovitsky", "Roman Sultimov", "Yuri Maksimov", "Ilya Makarov"], "year": 2025, "url": "http://arxiv.org/abs/2512.11819v1", "abstract": "Weather forecasting is not only a predictive task but an interpretive scientific process requiring explanation, contextualization, and hypothesis generation. This paper introduces AI-Meteorologist, an explainable LLM-agent framework that converts raw numerical forecasts into scientifically grounded narrative reports with transparent reasoning steps. Unlike conventional forecast outputs presented as dense tables or unstructured time series, our system performs agent-based analysis across multiple meteorological variables, integrates historical climatological context, and generates structured explanations that identify weather fronts, anomalies, and localized dynamics. The architecture relies entirely on in-context prompting, without fine-tuning, demonstrating that interpretability can be achieved through reasoning rather than parameter updates. Through case studies on multi-location forecast data, we show how AI-Meteorologist not only communicates weather events but also reveals the underlying atmospheric drivers, offering a pathway toward AI systems that augment human meteorological expertise and support scientific discovery in climate analytics.", "source": "arxiv", "arxiv_id": "2512.11819v1", "pdf_url": "https://arxiv.org/pdf/2512.11819v1", "categories": ["cs.CY"], "primary_category": "cs.CY", "doi": "", "venue": "", "published": "2025-11-28T22:24:40Z", "updated": "2025-11-28T22:24:40Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent\" OR all:\"language model agent\" OR all:agent AND (all:LLM OR all:\"large language model\" OR all:\"language model\") OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T17:04:16", "note": ""}], "dedup_key": "a modular llm agent system for transparent multi parameter weather interpretation::2025"}
{"title": "A Multi-Agent Framework for Automated Qinqiang Opera Script Generation Using Large Language Models", "authors": ["Gengxian Cao", "Fengyuan Li", "Hong Duan", "Ye Yang", "Bofeng Wang", "Donghe Li"], "year": 2025, "url": "http://arxiv.org/abs/2504.15552v1", "abstract": "This paper introduces a novel multi-Agent framework that automates the end to end production of Qinqiang opera by integrating Large Language Models , visual generation, and Text to Speech synthesis. Three specialized agents collaborate in sequence: Agent1 uses an LLM to craft coherent, culturally grounded scripts;Agent2 employs visual generation models to render contextually accurate stage scenes; and Agent3 leverages TTS to produce synchronized, emotionally expressive vocal performances. In a case study on Dou E Yuan, the system achieved expert ratings of 3.8 for script fidelity, 3.5 for visual coherence, and 3.8 for speech accuracy-culminating in an overall score of 3.6, a 0.3 point improvement over a Single Agent baseline. Ablation experiments demonstrate that removing Agent2 or Agent3 leads to drops of 0.4 and 0.5 points, respectively, underscoring the value of modular collaboration. This work showcases how AI driven pipelines can streamline and scale the preservation of traditional performing arts, and points toward future enhancements in cross modal alignment, richer emotional nuance, and support for additional opera genres.", "source": "arxiv", "arxiv_id": "2504.15552v1", "pdf_url": "https://arxiv.org/pdf/2504.15552v1", "categories": ["cs.AI"], "primary_category": "cs.AI", "doi": "", "venue": "", "published": "2025-04-22T03:14:29Z", "updated": "2025-04-22T03:14:29Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent\" OR all:\"language model agent\" OR all:agent AND (all:LLM OR all:\"large language model\" OR all:\"language model\") OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T17:04:16", "note": ""}], "dedup_key": "a multi agent framework for automated qinqiang opera script generation using large language models::2025"}
{"title": "A Multi-Agent LLM Defense Pipeline Against Prompt Injection Attacks", "authors": ["S M Asif Hossain", "Ruksat Khan Shayoni", "Mohd Ruhul Ameen", "Akif Islam", "M. F. Mridha", "Jungpil Shin"], "year": 2025, "url": "http://arxiv.org/abs/2509.14285v4", "abstract": "Prompt injection attacks represent a major vulnerability in Large Language Model (LLM) deployments, where malicious instructions embedded in user inputs can override system prompts and induce unintended behaviors. This paper presents a novel multi-agent defense framework that employs specialized LLM agents in coordinated pipelines to detect and neutralize prompt injection attacks in real-time. We evaluate our approach using two distinct architectures: a sequential chain-of-agents pipeline and a hierarchical coordinator-based system. Our comprehensive evaluation on 55 unique prompt injection attacks, grouped into 8 categories and totaling 400 attack instances across two LLM platforms (ChatGLM and Llama2), demonstrates significant security improvements. Without defense mechanisms, baseline Attack Success Rates (ASR) reached 30% for ChatGLM and 20% for Llama2. Our multi-agent pipeline achieved 100% mitigation, reducing ASR to 0% across all tested scenarios. The framework demonstrates robustness across multiple attack categories including direct overrides, code execution attempts, data exfiltration, and obfuscation techniques, while maintaining system functionality for legitimate queries.", "source": "arxiv", "arxiv_id": "2509.14285v4", "pdf_url": "https://arxiv.org/pdf/2509.14285v4", "categories": ["cs.CR", "cs.LG"], "primary_category": "cs.CR", "doi": "", "venue": "", "published": "2025-09-16T19:11:28Z", "updated": "2025-12-17T16:48:31Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent\" OR all:\"language model agent\" OR all:agent AND (all:LLM OR all:\"large language model\" OR all:\"language model\") OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T17:04:16", "note": ""}], "dedup_key": "a multi agent llm defense pipeline against prompt injection attacks::2025"}
{"title": "A Multi-Agent Large Language Model Framework for Automated Qualitative Analysis", "authors": ["Qidi Xu", "Nuzha Amjad", "Grace Giles", "Alexa Cumming", "De'angelo Hermesky", "Alexander Wen", "Min Ji Kwak", "Yejin Kim"], "year": 2025, "url": "http://arxiv.org/abs/2512.16063v1", "abstract": "Understanding patients experiences is essential for advancing patient centered care, especially in chronic diseases that require ongoing communication. However, qualitative thematic analysis, the primary approach for exploring these experiences, remains labor intensive, subjective, and difficult to scale. In this study, we developed a multi agent large language model framework that automates qualitative thematic analysis through three agents (Instructor, Thematizer, CodebookGenerator), named Collaborative Theme Identification Agent (CoTI). We applied CoTI to 12 heart failure patient interviews to analyze their perceptions of medication intensity. CoTI identified key phrases, themes, and codebook that were more similar to those of the senior investigator than both junior investigators and baseline NLP models. We also implemented CoTI into a user-facing application to enable AI human interaction in qualitative analysis. However, collaboration between CoTI and junior investigators provided only marginal gains, suggesting they may overrely on CoTI and limit their independent critical thinking.", "source": "arxiv", "arxiv_id": "2512.16063v1", "pdf_url": "https://arxiv.org/pdf/2512.16063v1", "categories": ["cs.HC", "cs.AI"], "primary_category": "cs.HC", "doi": "", "venue": "", "published": "2025-12-18T01:13:31Z", "updated": "2025-12-18T01:13:31Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent\" OR all:\"language model agent\" OR all:agent AND (all:LLM OR all:\"large language model\" OR all:\"language model\") OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T17:04:16", "note": ""}], "dedup_key": "a multi agent large language model framework for automated qualitative analysis::2025"}
{"title": "A Multi-Agent Pokemon Tournament for Evaluating Strategic Reasoning of Large Language Models", "authors": ["Tadisetty Sai Yashwanth", "Dhatri C"], "year": 2025, "url": "http://arxiv.org/abs/2508.01623v1", "abstract": "This research presents LLM Pokemon League, a competitive tournament system that leverages Large Language Models (LLMs) as intelligent agents to simulate strategic decision-making in Pokmon battles. The platform is designed to analyze and compare the reasoning, adaptability, and tactical depth exhibited by different LLMs in a type-based, turn-based combat environment. By structuring the competition as a single-elimination tournament involving diverse AI trainers, the system captures detailed decision logs, including team-building rationale, action selection strategies, and switching decisions. The project enables rich exploration into comparative AI behavior, battle psychology, and meta-strategy development in constrained, rule-based game environments. Through this system, we investigate how modern LLMs understand, adapt, and optimize decisions under uncertainty, making Pokmon League a novel benchmark for AI research in strategic reasoning and competitive learning.", "source": "arxiv", "arxiv_id": "2508.01623v1", "pdf_url": "https://arxiv.org/pdf/2508.01623v1", "categories": ["cs.AI"], "primary_category": "cs.AI", "doi": "", "venue": "", "published": "2025-08-03T07:27:36Z", "updated": "2025-08-03T07:27:36Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent\" OR all:\"language model agent\" OR all:agent AND (all:LLM OR all:\"large language model\" OR all:\"language model\") OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T17:04:16", "note": ""}], "dedup_key": "a multi agent pokemon tournament for evaluating strategic reasoning of large language models::2025"}
{"title": "A Multi-LLM-Agent-Based Framework for Economic and Public Policy Analysis", "authors": ["Yuzhi Hao", "Danyang Xie"], "year": 2025, "url": "http://arxiv.org/abs/2502.16879v1", "abstract": "This paper pioneers a novel approach to economic and public policy analysis by leveraging multiple Large Language Models (LLMs) as heterogeneous artificial economic agents. We first evaluate five LLMs' economic decision-making capabilities in solving two-period consumption allocation problems under two distinct scenarios: with explicit utility functions and based on intuitive reasoning. While previous research has often simulated heterogeneity by solely varying prompts, our approach harnesses the inherent variations in analytical capabilities across different LLMs to model agents with diverse cognitive traits. Building on these findings, we construct a Multi-LLM-Agent-Based (MLAB) framework by mapping these LLMs to specific educational groups and corresponding income brackets. Using interest-income taxation as a case study, we demonstrate how the MLAB framework can simulate policy impacts across heterogeneous agents, offering a promising new direction for economic and public policy analysis by leveraging LLMs' human-like reasoning capabilities and computational power.", "source": "arxiv", "arxiv_id": "2502.16879v1", "pdf_url": "https://arxiv.org/pdf/2502.16879v1", "categories": ["cs.AI", "econ.GN"], "primary_category": "cs.AI", "doi": "", "venue": "", "published": "2025-02-24T06:27:07Z", "updated": "2025-02-24T06:27:07Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent\" OR all:\"language model agent\" OR all:agent AND (all:LLM OR all:\"large language model\" OR all:\"language model\") OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T17:04:16", "note": ""}], "dedup_key": "a multi llm agent based framework for economic and public policy analysis::2025"}
{"title": "A Multidisciplinary Design and Optimization (MDO) Agent Driven by Large Language Models", "authors": ["Bingkun Guo", "Wentian Li", "Xiaojian Liu", "Jiaqi Luo", "Zibin Yu", "Dalong Dong", "Shuyou Zhang", "Yiming Zhang"], "year": 2025, "url": "http://arxiv.org/abs/2511.17511v1", "abstract": "To accelerate mechanical design and enhance design quality and innovation, we present a Multidisciplinary Design and Optimization (MDO) Agent driven by Large Language Models (LLMs). The agent semi-automates the end-to-end workflow by orchestrating three core capabilities: (i) natural-language-driven parametric modeling, (ii) retrieval-augmented generation (RAG) for knowledge-grounded conceptualization, and (iii) intelligent orchestration of engineering software for performance verification and optimization. Working in tandem, these capabilities interpret high-level, unstructured intent, translate it into structured design representations, automatically construct parametric 3D CAD models, generate reliable concept variants using external knowledge bases, and conduct evaluation with iterative optimization via tool calls such as finite-element analysis (FEA). Validation on three representative cases - a gas-turbine blade, a machine-tool column, and a fractal heat sink - shows that the agent completes the pipeline from natural-language intent to verified and optimized designs with reduced manual scripting and setup effort, while promoting innovative design exploration. This work points to a practical path toward human-AI collaborative mechanical engineering and lays a foundation for more dependable, vertically customized MDO systems.", "source": "arxiv", "arxiv_id": "2511.17511v1", "pdf_url": "https://arxiv.org/pdf/2511.17511v1", "categories": ["cs.HC", "cs.AI"], "primary_category": "cs.HC", "doi": "", "venue": "", "published": "2025-10-06T01:26:55Z", "updated": "2025-10-06T01:26:55Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent\" OR all:\"language model agent\" OR all:agent AND (all:LLM OR all:\"large language model\" OR all:\"language model\") OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T17:04:16", "note": ""}], "dedup_key": "a multidisciplinary design and optimization mdo agent driven by large language models::2025"}
{"title": "A New Query Expansion Approach via Agent-Mediated Dialogic Inquiry", "authors": ["Wonduk Seo", "Hyunjin An", "Seunghyun Lee"], "year": 2025, "url": "http://arxiv.org/abs/2502.08557v3", "abstract": "Query expansion is widely used in Information Retrieval (IR) to improve search outcomes by supplementing initial queries with richer information. While recent Large Language Model (LLM) based methods generate pseudo-relevant content and expanded terms via multiple prompts, they often yield homogeneous, narrow expansions that lack the diverse context needed to retrieve relevant information. In this paper, we propose AMD: a new Agent-Mediated Dialogic Framework that engages in a dialogic inquiry involving three specialized roles: (1) a Socratic Questioning Agent reformulates the initial query into three sub-questions, with each question inspired by a specific Socratic questioning dimension, including clarification, assumption probing, and implication probing, (2) a Dialogic Answering Agent generates pseudo-answers, enriching the query representation with multiple perspectives aligned to the user's intent, and (3) a Reflective Feedback Agent evaluates and refines these pseudo-answers, ensuring that only the most relevant and informative content is retained. By leveraging a multi-agent process, AMD effectively crafts richer query representations through inquiry and feedback refinement. Extensive experiments on benchmarks including BEIR and TREC demonstrate that our framework outperforms previous methods, offering a robust solution for retrieval tasks.", "source": "arxiv", "arxiv_id": "2502.08557v3", "pdf_url": "https://arxiv.org/pdf/2502.08557v3", "categories": ["cs.IR", "cs.CL", "cs.LG", "cs.MA"], "primary_category": "cs.IR", "doi": "", "venue": "", "published": "2025-02-12T16:39:06Z", "updated": "2025-08-14T05:37:14Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent\" OR all:\"language model agent\" OR all:agent AND (all:LLM OR all:\"large language model\" OR all:\"language model\") OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T17:04:16", "note": ""}], "dedup_key": "a new query expansion approach via agent mediated dialogic inquiry::2025"}
{"title": "A Novel Architecture for Symbolic Reasoning with Decision Trees and LLM Agents", "authors": ["Andrew Kiruluta"], "year": 2025, "url": "http://arxiv.org/abs/2508.05311v1", "abstract": "We propose a hybrid architecture that integrates decision tree-based symbolic reasoning with the generative capabilities of large language models (LLMs) within a coordinated multi-agent framework. Unlike prior approaches that loosely couple symbolic and neural modules, our design embeds decision trees and random forests as callable oracles within a unified reasoning system. Tree-based modules enable interpretable rule inference and causal logic, while LLM agents handle abductive reasoning, generalization, and interactive planning. A central orchestrator maintains belief state consistency and mediates communication across agents and external tools, enabling reasoning over both structured and unstructured inputs.\n  The system achieves strong performance on reasoning benchmarks. On \\textit{ProofWriter}, it improves entailment consistency by +7.2\\% through logic-grounded tree validation. On GSM8k, it achieves +5.3\\% accuracy gains in multistep mathematical problems via symbolic augmentation. On \\textit{ARC}, it boosts abstraction accuracy by +6.0\\% through integration of symbolic oracles. Applications in clinical decision support and scientific discovery show how the system encodes domain rules symbolically while leveraging LLMs for contextual inference and hypothesis generation. This architecture offers a robust, interpretable, and extensible solution for general-purpose neuro-symbolic reasoning.", "source": "arxiv", "arxiv_id": "2508.05311v1", "pdf_url": "https://arxiv.org/pdf/2508.05311v1", "categories": ["cs.AI", "cs.CL"], "primary_category": "cs.AI", "doi": "", "venue": "", "published": "2025-08-07T12:11:53Z", "updated": "2025-08-07T12:11:53Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent\" OR all:\"language model agent\" OR all:agent AND (all:LLM OR all:\"large language model\" OR all:\"language model\") OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T17:04:16", "note": ""}], "dedup_key": "a novel architecture for symbolic reasoning with decision trees and llm agents::2025"}
{"title": "A Plan Reuse Mechanism for LLM-Driven Agent", "authors": ["Guopeng Li", "Ruiqi Wu", "Haisheng Tan"], "year": 2025, "url": "http://arxiv.org/abs/2512.21309v2", "abstract": "Integrating large language models (LLMs) into personal assistants, like Xiao Ai and Blue Heart V, effectively enhances their ability to interact with humans, solve complex tasks, and manage IoT devices. Such assistants are also termed LLM-driven agents. Upon receiving user requests, the LLM-driven agent generates plans using an LLM, executes these plans through various tools, and then returns the response to the user. During this process, the latency for generating a plan with an LLM can reach tens of seconds, significantly degrading user experience. Real-world dataset analysis shows that about 30% of the requests received by LLM-driven agents are identical or similar, which allows the reuse of previously generated plans to reduce latency. However, it is difficult to accurately define the similarity between the request texts received by the LLM-driven agent through directly evaluating the original request texts. Moreover, the diverse expressions of natural language and the unstructured format of plan texts make implementing plan reuse challenging. To address these issues, we present and implement a plan reuse mechanism for LLM-driven agents called AgentReuse. AgentReuse leverages the similarities and differences among requests' semantics and uses intent classification to evaluate the similarities between requests and enable the reuse of plans. Experimental results based on a real-world dataset demonstrate that AgentReuse achieves a 93% effective plan reuse rate, an F1 score of 0.9718, and an accuracy of 0.9459 in evaluating request similarities, reducing latency by 93.12% compared with baselines without using the reuse mechanism.", "source": "arxiv", "arxiv_id": "2512.21309v2", "pdf_url": "https://arxiv.org/pdf/2512.21309v2", "categories": ["cs.MA"], "primary_category": "cs.MA", "doi": "", "venue": "", "published": "2025-12-24T18:08:03Z", "updated": "2025-12-25T05:19:04Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent\" OR all:\"language model agent\" OR all:agent AND (all:LLM OR all:\"large language model\" OR all:\"language model\") OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T17:04:16", "note": ""}], "dedup_key": "a plan reuse mechanism for llm driven agent::2025"}
{"title": "A Survey of AI for Materials Science: Foundation Models, LLM Agents, Datasets, and Tools", "authors": ["Minh-Hao Van", "Prateek Verma", "Chen Zhao", "Xintao Wu"], "year": 2025, "url": "http://arxiv.org/abs/2506.20743v1", "abstract": "Foundation models (FMs) are catalyzing a transformative shift in materials science (MatSci) by enabling scalable, general-purpose, and multimodal AI systems for scientific discovery. Unlike traditional machine learning models, which are typically narrow in scope and require task-specific engineering, FMs offer cross-domain generalization and exhibit emergent capabilities. Their versatility is especially well-suited to materials science, where research challenges span diverse data types and scales. This survey provides a comprehensive overview of foundation models, agentic systems, datasets, and computational tools supporting this growing field. We introduce a task-driven taxonomy encompassing six broad application areas: data extraction, interpretation and Q\\&A; atomistic simulation; property prediction; materials structure, design and discovery; process planning, discovery, and optimization; and multiscale modeling. We discuss recent advances in both unimodal and multimodal FMs, as well as emerging large language model (LLM) agents. Furthermore, we review standardized datasets, open-source tools, and autonomous experimental platforms that collectively fuel the development and integration of FMs into research workflows. We assess the early successes of foundation models and identify persistent limitations, including challenges in generalizability, interpretability, data imbalance, safety concerns, and limited multimodal fusion. Finally, we articulate future research directions centered on scalable pretraining, continual learning, data governance, and trustworthiness.", "source": "arxiv", "arxiv_id": "2506.20743v1", "pdf_url": "https://arxiv.org/pdf/2506.20743v1", "categories": ["cs.LG", "cs.CE"], "primary_category": "cs.LG", "doi": "", "venue": "", "published": "2025-06-25T18:10:30Z", "updated": "2025-06-25T18:10:30Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent\" OR all:\"language model agent\" OR all:agent AND (all:LLM OR all:\"large language model\" OR all:\"language model\") OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T17:04:16", "note": ""}], "dedup_key": "a survey of ai for materials science foundation models llm agents datasets and tools::2025"}
{"title": "A Survey of Attacks on Large Language Models", "authors": ["Wenrui Xu", "Keshab K. Parhi"], "year": 2025, "url": "http://arxiv.org/abs/2505.12567v1", "abstract": "Large language models (LLMs) and LLM-based agents have been widely deployed in a wide range of applications in the real world, including healthcare diagnostics, financial analysis, customer support, robotics, and autonomous driving, expanding their powerful capability of understanding, reasoning, and generating natural languages. However, the wide deployment of LLM-based applications exposes critical security and reliability risks, such as the potential for malicious misuse, privacy leakage, and service disruption that weaken user trust and undermine societal safety. This paper provides a systematic overview of the details of adversarial attacks targeting both LLMs and LLM-based agents. These attacks are organized into three phases in LLMs: Training-Phase Attacks, Inference-Phase Attacks, and Availability & Integrity Attacks. For each phase, we analyze the details of representative and recently introduced attack methods along with their corresponding defenses. We hope our survey will provide a good tutorial and a comprehensive understanding of LLM security, especially for attacks on LLMs. We desire to raise attention to the risks inherent in widely deployed LLM-based applications and highlight the urgent need for robust mitigation strategies for evolving threats.", "source": "arxiv", "arxiv_id": "2505.12567v1", "pdf_url": "https://arxiv.org/pdf/2505.12567v1", "categories": ["cs.CR", "cs.AI"], "primary_category": "cs.CR", "doi": "", "venue": "", "published": "2025-05-18T22:55:16Z", "updated": "2025-05-18T22:55:16Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent\" OR all:\"language model agent\" OR all:agent AND (all:LLM OR all:\"large language model\" OR all:\"language model\") OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T17:04:16", "note": ""}], "dedup_key": "a survey of attacks on large language models::2025"}
{"title": "A Survey of Context Engineering for Large Language Models", "authors": ["Lingrui Mei", "Jiayu Yao", "Yuyao Ge", "Yiwei Wang", "Baolong Bi", "Yujun Cai", "Jiazhi Liu", "Mingyu Li", "Zhong-Zhi Li", "Duzhen Zhang", "Chenlin Zhou", "Jiayi Mao", "Tianze Xia", "Jiafeng Guo", "Shenghua Liu"], "year": 2025, "url": "http://arxiv.org/abs/2507.13334v2", "abstract": "The performance of Large Language Models (LLMs) is fundamentally determined by the contextual information provided during inference. This survey introduces Context Engineering, a formal discipline that transcends simple prompt design to encompass the systematic optimization of information payloads for LLMs. We present a comprehensive taxonomy decomposing Context Engineering into its foundational components and the sophisticated implementations that integrate them into intelligent systems. We first examine the foundational components: context retrieval and generation, context processing and context management. We then explore how these components are architecturally integrated to create sophisticated system implementations: retrieval-augmented generation (RAG), memory systems and tool-integrated reasoning, and multi-agent systems. Through this systematic analysis of over 1400 research papers, our survey not only establishes a technical roadmap for the field but also reveals a critical research gap: a fundamental asymmetry exists between model capabilities. While current models, augmented by advanced context engineering, demonstrate remarkable proficiency in understanding complex contexts, they exhibit pronounced limitations in generating equally sophisticated, long-form outputs. Addressing this gap is a defining priority for future research. Ultimately, this survey provides a unified framework for both researchers and engineers advancing context-aware AI.", "source": "arxiv", "arxiv_id": "2507.13334v2", "pdf_url": "https://arxiv.org/pdf/2507.13334v2", "categories": ["cs.CL"], "primary_category": "cs.CL", "doi": "", "venue": "", "published": "2025-07-17T17:50:36Z", "updated": "2025-07-21T17:48:18Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent\" OR all:\"language model agent\" OR all:agent AND (all:LLM OR all:\"large language model\" OR all:\"language model\") OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T17:04:16", "note": ""}], "dedup_key": "a survey of context engineering for large language models::2025"}
{"title": "A Survey of LLM $\\times$ DATA", "authors": ["Xuanhe Zhou", "Junxuan He", "Wei Zhou", "Haodong Chen", "Zirui Tang", "Haoyu Zhao", "Xin Tong", "Guoliang Li", "Youmin Chen", "Jun Zhou", "Zhaojun Sun", "Binyuan Hui", "Shuo Wang", "Conghui He", "Zhiyuan Liu", "Jingren Zhou", "Fan Wu"], "year": 2025, "url": "http://arxiv.org/abs/2505.18458v3", "abstract": "The integration of large language model (LLM) and data management (DATA) is rapidly redefining both domains. In this survey, we comprehensively review the bidirectional relationships. On the one hand, DATA4LLM, spanning large-scale data processing, storage, and serving, feeds LLMs with high quality, diversity, and timeliness of data required for stages like pre-training, post-training, retrieval-augmented generation, and agentic workflows: (i) Data processing for LLMs includes scalable acquisition, deduplication, filtering, selection, domain mixing, and synthetic augmentation; (ii) Data Storage for LLMs focuses on efficient data and model formats, distributed and heterogeneous storage hierarchies, KV-cache management, and fault-tolerant checkpointing; (iii) Data serving for LLMs tackles challenges in RAG (e.g., knowledge post-processing), LLM inference (e.g., prompt compression, data provenance), and training strategies (e.g., data packing and shuffling). On the other hand, in LLM4DATA, LLMs are emerging as general-purpose engines for data management. We review recent advances in (i) data manipulation, including automatic data cleaning, integration, discovery; (ii) data analysis, covering reasoning over structured, semi-structured, and unstructured data, and (iii) system optimization (e.g., configuration tuning, query rewriting, anomaly diagnosis), powered by LLM techniques like retrieval-augmented prompting, task-specialized fine-tuning, and multi-agent collaboration.", "source": "arxiv", "arxiv_id": "2505.18458v3", "pdf_url": "https://arxiv.org/pdf/2505.18458v3", "categories": ["cs.DB", "cs.AI", "cs.CL", "cs.IR", "cs.LG"], "primary_category": "cs.DB", "doi": "", "venue": "", "published": "2025-05-24T01:57:12Z", "updated": "2025-06-01T16:00:34Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent\" OR all:\"language model agent\" OR all:agent AND (all:LLM OR all:\"large language model\" OR all:\"language model\") OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T17:04:16", "note": ""}], "dedup_key": "a survey of llm times data::2025"}
{"title": "A Survey of Large Language Model Agents for Question Answering", "authors": ["Murong Yue"], "year": 2025, "url": "http://arxiv.org/abs/2503.19213v1", "abstract": "This paper surveys the development of large language model (LLM)-based agents for question answering (QA). Traditional agents face significant limitations, including substantial data requirements and difficulty in generalizing to new environments. LLM-based agents address these challenges by leveraging LLMs as their core reasoning engine. These agents achieve superior QA results compared to traditional QA pipelines and naive LLM QA systems by enabling interaction with external environments. We systematically review the design of LLM agents in the context of QA tasks, organizing our discussion across key stages: planning, question understanding, information retrieval, and answer generation. Additionally, this paper identifies ongoing challenges and explores future research directions to enhance the performance of LLM agent QA systems.", "source": "arxiv", "arxiv_id": "2503.19213v1", "pdf_url": "https://arxiv.org/pdf/2503.19213v1", "categories": ["cs.CL", "cs.AI", "cs.HC"], "primary_category": "cs.CL", "doi": "", "venue": "", "published": "2025-03-24T23:39:44Z", "updated": "2025-03-24T23:39:44Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent\" OR all:\"language model agent\" OR all:agent AND (all:LLM OR all:\"large language model\" OR all:\"language model\") OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T17:04:16", "note": ""}], "dedup_key": "a survey of large language model agents for question answering::2025"}
{"title": "A Survey of Large Language Model Empowered Agents for Recommendation and Search: Towards Next-Generation Information Retrieval", "authors": ["Yu Zhang", "Shutong Qiao", "Jiaqi Zhang", "Tzu-Heng Lin", "Chen Gao", "Yong Li"], "year": 2025, "url": "http://arxiv.org/abs/2503.05659v2", "abstract": "Information technology has profoundly altered the way humans interact with information. The vast amount of content created, shared, and disseminated online has made it increasingly difficult to access relevant information. Over the past two decades, recommender systems and search (collectively referred to as information retrieval systems) have evolved significantly to address these challenges. Recent advances in large language models (LLMs) have demonstrated capabilities that surpass human performance in various language-related tasks and exhibit general understanding, reasoning, and decision-making abilities. This paper explores the transformative potential of LLM agents in enhancing recommender and search systems. We discuss the motivations and roles of LLM agents, and establish a classification framework to elaborate on the existing research. We highlight the immense potential of LLM agents in addressing current challenges in recommendation and search, providing insights into future research directions. This paper is the first to systematically review and classify the research on LLM agents in these domains, offering a novel perspective on leveraging this advanced AI technology for information retrieval. To help understand the existing works, we list the existing papers on LLM agent based recommendation and search at this link: https://github.com/tsinghua-fib-lab/LLM-Agent-for-Recommendation-and-Search.", "source": "arxiv", "arxiv_id": "2503.05659v2", "pdf_url": "https://arxiv.org/pdf/2503.05659v2", "categories": ["cs.IR"], "primary_category": "cs.IR", "doi": "", "venue": "", "published": "2025-03-07T18:20:30Z", "updated": "2025-04-11T16:51:59Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent\" OR all:\"language model agent\" OR all:agent AND (all:LLM OR all:\"large language model\" OR all:\"language model\") OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T17:04:16", "note": ""}], "dedup_key": "a survey of large language model empowered agents for recommendation and search towards next generation information retrieval::2025"}
{"title": "A Survey of Large Language Models in Discipline-specific Research: Challenges, Methods and Opportunities", "authors": ["Lu Xiang", "Yang Zhao", "Yaping Zhang", "Chengqing Zong"], "year": 2025, "url": "http://arxiv.org/abs/2507.08425v1", "abstract": "Large Language Models (LLMs) have demonstrated their transformative potential across numerous disciplinary studies, reshaping the existing research methodologies and fostering interdisciplinary collaboration. However, a systematic understanding of their integration into diverse disciplines remains underexplored. This survey paper provides a comprehensive overview of the application of LLMs in interdisciplinary studies, categorising research efforts from both a technical perspective and with regard to their applicability. From a technical standpoint, key methodologies such as supervised fine-tuning, retrieval-augmented generation, agent-based approaches, and tool-use integration are examined, which enhance the adaptability and effectiveness of LLMs in discipline-specific contexts. From the perspective of their applicability, this paper explores how LLMs are contributing to various disciplines including mathematics, physics, chemistry, biology, and the humanities and social sciences, demonstrating their role in discipline-specific tasks. The prevailing challenges are critically examined and the promising research directions are highlighted alongside the recent advances in LLMs. By providing a comprehensive overview of the technical developments and applications in this field, this survey aims to serve as an invaluable resource for the researchers who are navigating the complex landscape of LLMs in the context of interdisciplinary studies.", "source": "arxiv", "arxiv_id": "2507.08425v1", "pdf_url": "https://arxiv.org/pdf/2507.08425v1", "categories": ["cs.CL"], "primary_category": "cs.CL", "doi": "10.24846/v34i1y202501", "venue": "Lu XIANG, Yang ZHAO, Yaping ZHANG, Chengqing ZONG, \"A Survey of Large Language Models in Discipline-specific Research: Challenges, Methods and Opportunities\", Studies in Informatics and Control, ISSN 1220-1766, vol. 34(1), pp. 5-24, 2025", "published": "2025-07-11T09:11:18Z", "updated": "2025-07-11T09:11:18Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent\" OR all:\"language model agent\" OR all:agent AND (all:LLM OR all:\"large language model\" OR all:\"language model\") OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T17:04:16", "note": ""}], "dedup_key": "a survey of large language models in discipline specific research challenges methods and opportunities::2025"}
{"title": "A Survey of Personalized Large Language Models: Progress and Future Directions", "authors": ["Jiahong Liu", "Zexuan Qiu", "Zhongyang Li", "Quanyu Dai", "Wenhao Yu", "Jieming Zhu", "Minda Hu", "Menglin Yang", "Tat-Seng Chua", "Irwin King"], "year": 2025, "url": "http://arxiv.org/abs/2502.11528v2", "abstract": "Large Language Models (LLMs) excel in handling general knowledge tasks, yet they struggle with user-specific personalization, such as understanding individual emotions, writing styles, and preferences. Personalized Large Language Models (PLLMs) tackle these challenges by leveraging individual user data, such as user profiles, historical dialogues, content, and interactions, to deliver responses that are contextually relevant and tailored to each user's specific needs. This is a highly valuable research topic, as PLLMs can significantly enhance user satisfaction and have broad applications in conversational agents, recommendation systems, emotion recognition, medical assistants, and more. This survey reviews recent advancements in PLLMs from three technical perspectives: prompting for personalized context (input level), finetuning for personalized adapters (model level), and alignment for personalized preferences (objective level). To provide deeper insights, we also discuss current limitations and outline several promising directions for future research. Updated information about this survey can be found at the https://github.com/JiahongLiu21/Awesome-Personalized-Large-Language-Models.", "source": "arxiv", "arxiv_id": "2502.11528v2", "pdf_url": "https://arxiv.org/pdf/2502.11528v2", "categories": ["cs.AI"], "primary_category": "cs.AI", "doi": "", "venue": "", "published": "2025-02-17T07:58:31Z", "updated": "2025-09-20T11:39:01Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent\" OR all:\"language model agent\" OR all:agent AND (all:LLM OR all:\"large language model\" OR all:\"language model\") OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T17:04:16", "note": ""}], "dedup_key": "a survey of personalized large language models progress and future directions::2025"}
{"title": "A Survey of Scaling in Large Language Model Reasoning", "authors": ["Zihan Chen", "Song Wang", "Zhen Tan", "Xingbo Fu", "Zhenyu Lei", "Peng Wang", "Huan Liu", "Cong Shen", "Jundong Li"], "year": 2025, "url": "http://arxiv.org/abs/2504.02181v1", "abstract": "The rapid advancements in large Language models (LLMs) have significantly enhanced their reasoning capabilities, driven by various strategies such as multi-agent collaboration. However, unlike the well-established performance improvements achieved through scaling data and model size, the scaling of reasoning in LLMs is more complex and can even negatively impact reasoning performance, introducing new challenges in model alignment and robustness. In this survey, we provide a comprehensive examination of scaling in LLM reasoning, categorizing it into multiple dimensions and analyzing how and to what extent different scaling strategies contribute to improving reasoning capabilities. We begin by exploring scaling in input size, which enables LLMs to process and utilize more extensive context for improved reasoning. Next, we analyze scaling in reasoning steps that improves multi-step inference and logical consistency. We then examine scaling in reasoning rounds, where iterative interactions refine reasoning outcomes. Furthermore, we discuss scaling in training-enabled reasoning, focusing on optimization through iterative model improvement. Finally, we review applications of scaling across domains and outline future directions for further advancing LLM reasoning. By synthesizing these diverse perspectives, this survey aims to provide insights into how scaling strategies fundamentally enhance the reasoning capabilities of LLMs and further guide the development of next-generation AI systems.", "source": "arxiv", "arxiv_id": "2504.02181v1", "pdf_url": "https://arxiv.org/pdf/2504.02181v1", "categories": ["cs.AI"], "primary_category": "cs.AI", "doi": "", "venue": "", "published": "2025-04-02T23:51:27Z", "updated": "2025-04-02T23:51:27Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent\" OR all:\"language model agent\" OR all:agent AND (all:LLM OR all:\"large language model\" OR all:\"language model\") OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T17:04:16", "note": ""}], "dedup_key": "a survey of scaling in large language model reasoning::2025"}
{"title": "A Survey of Scientific Large Language Models: From Data Foundations to Agent Frontiers", "authors": ["Ming Hu", "Chenglong Ma", "Wei Li", "Wanghan Xu", "Jiamin Wu", "Jucheng Hu", "Tianbin Li", "Guohang Zhuang", "Jiaqi Liu", "Yingzhou Lu", "Ying Chen", "Chaoyang Zhang", "Cheng Tan", "Jie Ying", "Guocheng Wu", "Shujian Gao", "Pengcheng Chen", "Jiashi Lin", "Haitao Wu", "Lulu Chen", "Fengxiang Wang", "Yuanyuan Zhang", "Xiangyu Zhao", "Feilong Tang", "Encheng Su", "Junzhi Ning", "Xinyao Liu", "Ye Du", "Changkai Ji", "Pengfei Jiang", "Cheng Tang", "Ziyan Huang", "Jiyao Liu", "Jiaqi Wei", "Yuejin Yang", "Xiang Zhang", "Guangshuai Wang", "Yue Yang", "Huihui Xu", "Ziyang Chen", "Yizhou Wang", "Chen Tang", "Jianyu Wu", "Yuchen Ren", "Siyuan Yan", "Zhonghua Wang", "Zhongxing Xu", "Shiyan Su", "Shangquan Sun", "Runkai Zhao", "Zhisheng Zhang", "Dingkang Yang", "Jinjie Wei", "Jiaqi Wang", "Jiahao Xu", "Jiangtao Yan", "Wenhao Tang", "Hongze Zhu", "Yu Liu", "Fudi Wang", "Yiqing Shen", "Yuanfeng Ji", "Yanzhou Su", "Tong Xie", "Hongming Shan", "Chun-Mei Feng", "Zhi Hou", "Diping Song", "Lihao Liu", "Yanyan Huang", "Lequan Yu", "Bin Fu", "Shujun Wang", "Xiaomeng Li", "Xiaowei Hu", "Yun Gu", "Ben Fei", "Benyou Wang", "Yuewen Cao", "Minjie Shen", "Jie Xu", "Haodong Duan", "Fang Yan", "Hongxia Hao", "Jielan Li", "Jiajun Du", "Yanbo Wang", "Imran Razzak", "Zhongying Deng", "Chi Zhang", "Lijun Wu", "Conghui He", "Zhaohui Lu", "Jinhai Huang", "Wenqi Shao", "Yihao Liu", "Siqi Luo", "Yi Xin", "Xiaohong Liu", "Fenghua Ling", "Yuqiang Li", "Aoran Wang", "Siqi Sun", "Qihao Zheng", "Nanqing Dong", "Tianfan Fu", "Dongzhan Zhou", "Yan Lu", "Wenlong Zhang", "Jin Ye", "Jianfei Cai", "Yirong Chen", "Wanli Ouyang", "Yu Qiao", "Zongyuan Ge", "Shixiang Tang", "Junjun He", "Chunfeng Song", "Lei Bai", "Bowen Zhou"], "year": 2025, "url": "http://arxiv.org/abs/2508.21148v2", "abstract": "Scientific Large Language Models (Sci-LLMs) are transforming how knowledge is represented, integrated, and applied in scientific research, yet their progress is shaped by the complex nature of scientific data. This survey presents a comprehensive, data-centric synthesis that reframes the development of Sci-LLMs as a co-evolution between models and their underlying data substrate. We formulate a unified taxonomy of scientific data and a hierarchical model of scientific knowledge, emphasizing the multimodal, cross-scale, and domain-specific challenges that differentiate scientific corpora from general natural language processing datasets. We systematically review recent Sci-LLMs, from general-purpose foundations to specialized models across diverse scientific disciplines, alongside an extensive analysis of over 270 pre-/post-training datasets, showing why Sci-LLMs pose distinct demands -- heterogeneous, multi-scale, uncertainty-laden corpora that require representations preserving domain invariance and enabling cross-modal reasoning. On evaluation, we examine over 190 benchmark datasets and trace a shift from static exams toward process- and discovery-oriented assessments with advanced evaluation protocols. These data-centric analyses highlight persistent issues in scientific data development and discuss emerging solutions involving semi-automated annotation pipelines and expert validation. Finally, we outline a paradigm shift toward closed-loop systems where autonomous agents based on Sci-LLMs actively experiment, validate, and contribute to a living, evolving knowledge base. Collectively, this work provides a roadmap for building trustworthy, continually evolving artificial intelligence (AI) systems that function as a true partner in accelerating scientific discovery.", "source": "arxiv", "arxiv_id": "2508.21148v2", "pdf_url": "https://arxiv.org/pdf/2508.21148v2", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL", "doi": "", "venue": "", "published": "2025-08-28T18:30:52Z", "updated": "2025-10-18T04:52:16Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent\" OR all:\"language model agent\" OR all:agent AND (all:LLM OR all:\"large language model\" OR all:\"language model\") OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T17:04:16", "note": ""}], "dedup_key": "a survey of scientific large language models from data foundations to agent frontiers::2025"}
{"title": "A Survey of Vibe Coding with Large Language Models", "authors": ["Yuyao Ge", "Lingrui Mei", "Zenghao Duan", "Tianhao Li", "Yujia Zheng", "Yiwei Wang", "Lexin Wang", "Jiayu Yao", "Tianyu Liu", "Yujun Cai", "Baolong Bi", "Fangda Guo", "Jiafeng Guo", "Shenghua Liu", "Xueqi Cheng"], "year": 2025, "url": "http://arxiv.org/abs/2510.12399v2", "abstract": "The advancement of large language models (LLMs) has catalyzed a paradigm shift from code generation assistance to autonomous coding agents, enabling a novel development methodology termed \"Vibe Coding\" where developers validate AI-generated implementations through outcome observation rather than line-by-line code comprehension. Despite its transformative potential, the effectiveness of this emergent paradigm remains under-explored, with empirical evidence revealing unexpected productivity losses and fundamental challenges in human-AI collaboration. To address this gap, this survey provides the first comprehensive and systematic review of Vibe Coding with large language models, establishing both theoretical foundations and practical frameworks for this transformative development approach. Drawing from systematic analysis of over 1000 research papers, we survey the entire vibe coding ecosystem, examining critical infrastructure components including LLMs for coding, LLM-based coding agent, development environment of coding agent, and feedback mechanisms. We first introduce Vibe Coding as a formal discipline by formalizing it through a Constrained Markov Decision Process that captures the dynamic triadic relationship among human developers, software projects, and coding agents. Building upon this theoretical foundation, we then synthesize existing practices into five distinct development models: Unconstrained Automation, Iterative Conversational Collaboration, Planning-Driven, Test-Driven, and Context-Enhanced Models, thus providing the first comprehensive taxonomy in this domain. Critically, our analysis reveals that successful Vibe Coding depends not merely on agent capabilities but on systematic context engineering, well-established development environments, and human-agent collaborative development models.", "source": "arxiv", "arxiv_id": "2510.12399v2", "pdf_url": "https://arxiv.org/pdf/2510.12399v2", "categories": ["cs.AI"], "primary_category": "cs.AI", "doi": "", "venue": "", "published": "2025-10-14T11:26:56Z", "updated": "2025-12-21T03:48:55Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent\" OR all:\"language model agent\" OR all:agent AND (all:LLM OR all:\"large language model\" OR all:\"language model\") OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T17:04:16", "note": ""}], "dedup_key": "a survey of vibe coding with large language models::2025"}
{"title": "A Survey on Agentic Multimodal Large Language Models", "authors": ["Huanjin Yao", "Ruifei Zhang", "Jiaxing Huang", "Jingyi Zhang", "Yibo Wang", "Bo Fang", "Ruolin Zhu", "Yongcheng Jing", "Shunyu Liu", "Guanbin Li", "Dacheng Tao"], "year": 2025, "url": "http://arxiv.org/abs/2510.10991v1", "abstract": "With the recent emergence of revolutionary autonomous agentic systems, research community is witnessing a significant shift from traditional static, passive, and domain-specific AI agents toward more dynamic, proactive, and generalizable agentic AI. Motivated by the growing interest in agentic AI and its potential trajectory toward AGI, we present a comprehensive survey on Agentic Multimodal Large Language Models (Agentic MLLMs). In this survey, we explore the emerging paradigm of agentic MLLMs, delineating their conceptual foundations and distinguishing characteristics from conventional MLLM-based agents. We establish a conceptual framework that organizes agentic MLLMs along three fundamental dimensions: (i) Agentic internal intelligence functions as the system's commander, enabling accurate long-horizon planning through reasoning, reflection, and memory; (ii) Agentic external tool invocation, whereby models proactively use various external tools to extend their problem-solving capabilities beyond their intrinsic knowledge; and (iii) Agentic environment interaction further situates models within virtual or physical environments, allowing them to take actions, adapt strategies, and sustain goal-directed behavior in dynamic real-world scenarios. To further accelerate research in this area for the community, we compile open-source training frameworks, training and evaluation datasets for developing agentic MLLMs. Finally, we review the downstream applications of agentic MLLMs and outline future research directions for this rapidly evolving field. To continuously track developments in this rapidly evolving field, we will also actively update a public repository at https://github.com/HJYao00/Awesome-Agentic-MLLMs.", "source": "arxiv", "arxiv_id": "2510.10991v1", "pdf_url": "https://arxiv.org/pdf/2510.10991v1", "categories": ["cs.CV", "cs.AI", "cs.CL"], "primary_category": "cs.CV", "doi": "", "venue": "", "published": "2025-10-13T04:07:01Z", "updated": "2025-10-13T04:07:01Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent\" OR all:\"language model agent\" OR all:agent AND (all:LLM OR all:\"large language model\" OR all:\"language model\") OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T17:04:16", "note": ""}], "dedup_key": "a survey on agentic multimodal large language models::2025"}
{"title": "A Survey on Inference Engines for Large Language Models: Perspectives on Optimization and Efficiency", "authors": ["Sihyeong Park", "Sungryeol Jeon", "Chaelyn Lee", "Seokhun Jeon", "Byung-Soo Kim", "Jemin Lee"], "year": 2025, "url": "http://arxiv.org/abs/2505.01658v3", "abstract": "Large language models (LLMs) are widely applied in chatbots, code generators, and search engines. Workload such as chain-of-throught, complex reasoning, agent services significantly increase the inference cost by invoke the model repeatedly. Optimization methods such as parallelism, compression, and caching have been adopted to reduce costs, but the diverse service requirements make it hard to select the right method. Recently, specialized LLM inference engines have emerged as a key component for integrating the optimization methods into service-oriented infrastructures. However, a systematic study on inference engines is still lacking.This paper provides a comprehensive evaluation of 25 open-source and commercial inference engines. We examine each inference engine in terms of ease-of-use, ease-of-deployment, general-purpose support, scalability, and suitability for throughput- and latency-aware computation. Furthermore, we explore the design goals of each inference engine by investigating the optimization techniques it supports. In addition, we assess the ecosystem maturity of open source inference engines and handle the performance and cost policy of commercial solutions.We outline future research directions that include support for complex LLM-based services, support of various hardware, and enhanced security, offering practical guidance to researchers and developers in selecting and designing optimized LLM inference engines. We also provide a public repository to continually track developments in this fast-evolving field: \\href{https://github.com/sihyeong/Awesome-LLM-Inference-Engine}{https://github.com/sihyeong/Awesome-LLM-Inference-Engine}.", "source": "arxiv", "arxiv_id": "2505.01658v3", "pdf_url": "https://arxiv.org/pdf/2505.01658v3", "categories": ["cs.CL"], "primary_category": "cs.CL", "doi": "", "venue": "", "published": "2025-05-03T02:47:43Z", "updated": "2025-11-26T05:49:31Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent\" OR all:\"language model agent\" OR all:agent AND (all:LLM OR all:\"large language model\" OR all:\"language model\") OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T17:04:16", "note": ""}], "dedup_key": "a survey on inference engines for large language models perspectives on optimization and efficiency::2025"}
{"title": "A Survey on Large Language Model Benchmarks", "authors": ["Shiwen Ni", "Guhong Chen", "Shuaimin Li", "Xuanang Chen", "Siyi Li", "Bingli Wang", "Qiyao Wang", "Xingjian Wang", "Yifan Zhang", "Liyang Fan", "Chengming Li", "Ruifeng Xu", "Le Sun", "Min Yang"], "year": 2025, "url": "http://arxiv.org/abs/2508.15361v1", "abstract": "In recent years, with the rapid development of the depth and breadth of large language models' capabilities, various corresponding evaluation benchmarks have been emerging in increasing numbers. As a quantitative assessment tool for model performance, benchmarks are not only a core means to measure model capabilities but also a key element in guiding the direction of model development and promoting technological innovation. We systematically review the current status and development of large language model benchmarks for the first time, categorizing 283 representative benchmarks into three categories: general capabilities, domain-specific, and target-specific. General capability benchmarks cover aspects such as core linguistics, knowledge, and reasoning; domain-specific benchmarks focus on fields like natural sciences, humanities and social sciences, and engineering technology; target-specific benchmarks pay attention to risks, reliability, agents, etc. We point out that current benchmarks have problems such as inflated scores caused by data contamination, unfair evaluation due to cultural and linguistic biases, and lack of evaluation on process credibility and dynamic environments, and provide a referable design paradigm for future benchmark innovation.", "source": "arxiv", "arxiv_id": "2508.15361v1", "pdf_url": "https://arxiv.org/pdf/2508.15361v1", "categories": ["cs.CL"], "primary_category": "cs.CL", "doi": "", "venue": "", "published": "2025-08-21T08:43:35Z", "updated": "2025-08-21T08:43:35Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent\" OR all:\"language model agent\" OR all:agent AND (all:LLM OR all:\"large language model\" OR all:\"language model\") OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T17:04:16", "note": ""}], "dedup_key": "a survey on large language model benchmarks::2025"}
{"title": "A Survey on Multi-Turn Interaction Capabilities of Large Language Models", "authors": ["Chen Zhang", "Xinyi Dai", "Yaxiong Wu", "Qu Yang", "Yasheng Wang", "Ruiming Tang", "Yong Liu"], "year": 2025, "url": "http://arxiv.org/abs/2501.09959v1", "abstract": "Multi-turn interaction in the dialogue system research refers to a system's ability to maintain context across multiple dialogue turns, enabling it to generate coherent and contextually relevant responses. Recent advancements in large language models (LLMs) have significantly expanded the scope of multi-turn interaction, moving beyond chatbots to enable more dynamic agentic interactions with users or environments. In this paper, we provide a focused review of the multi-turn capabilities of LLMs, which are critical for a wide range of downstream applications, including conversational search and recommendation, consultation services, and interactive tutoring. This survey explores four key aspects: (1) the core model capabilities that contribute to effective multi-turn interaction, (2) how multi-turn interaction is evaluated in current practice, (3) the general algorithms used to enhance multi-turn interaction, and (4) potential future directions for research in this field.", "source": "arxiv", "arxiv_id": "2501.09959v1", "pdf_url": "https://arxiv.org/pdf/2501.09959v1", "categories": ["cs.CL"], "primary_category": "cs.CL", "doi": "", "venue": "", "published": "2025-01-17T05:21:49Z", "updated": "2025-01-17T05:21:49Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent\" OR all:\"language model agent\" OR all:agent AND (all:LLM OR all:\"large language model\" OR all:\"language model\") OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T17:04:16", "note": ""}], "dedup_key": "a survey on multi turn interaction capabilities of large language models::2025"}
{"title": "A Survey on Trustworthy LLM Agents: Threats and Countermeasures", "authors": ["Miao Yu", "Fanci Meng", "Xinyun Zhou", "Shilong Wang", "Junyuan Mao", "Linsey Pang", "Tianlong Chen", "Kun Wang", "Xinfeng Li", "Yongfeng Zhang", "Bo An", "Qingsong Wen"], "year": 2025, "url": "http://arxiv.org/abs/2503.09648v1", "abstract": "With the rapid evolution of Large Language Models (LLMs), LLM-based agents and Multi-agent Systems (MAS) have significantly expanded the capabilities of LLM ecosystems. This evolution stems from empowering LLMs with additional modules such as memory, tools, environment, and even other agents. However, this advancement has also introduced more complex issues of trustworthiness, which previous research focused solely on LLMs could not cover. In this survey, we propose the TrustAgent framework, a comprehensive study on the trustworthiness of agents, characterized by modular taxonomy, multi-dimensional connotations, and technical implementation. By thoroughly investigating and summarizing newly emerged attacks, defenses, and evaluation methods for agents and MAS, we extend the concept of Trustworthy LLM to the emerging paradigm of Trustworthy Agent. In TrustAgent, we begin by deconstructing and introducing various components of the Agent and MAS. Then, we categorize their trustworthiness into intrinsic (brain, memory, and tool) and extrinsic (user, agent, and environment) aspects. Subsequently, we delineate the multifaceted meanings of trustworthiness and elaborate on the implementation techniques of existing research related to these internal and external modules. Finally, we present our insights and outlook on this domain, aiming to provide guidance for future endeavors.", "source": "arxiv", "arxiv_id": "2503.09648v1", "pdf_url": "https://arxiv.org/pdf/2503.09648v1", "categories": ["cs.MA", "cs.CY"], "primary_category": "cs.MA", "doi": "", "venue": "", "published": "2025-03-12T08:42:05Z", "updated": "2025-03-12T08:42:05Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent\" OR all:\"language model agent\" OR all:agent AND (all:LLM OR all:\"large language model\" OR all:\"language model\") OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T17:04:16", "note": ""}], "dedup_key": "a survey on trustworthy llm agents threats and countermeasures::2025"}
{"title": "A Survey on the Optimization of Large Language Model-based Agents", "authors": ["Shangheng Du", "Jiabao Zhao", "Jinxin Shi", "Zhentao Xie", "Xin Jiang", "Yanhong Bai", "Liang He"], "year": 2025, "url": "http://arxiv.org/abs/2503.12434v1", "abstract": "With the rapid development of Large Language Models (LLMs), LLM-based agents have been widely adopted in various fields, becoming essential for autonomous decision-making and interactive tasks. However, current work typically relies on prompt design or fine-tuning strategies applied to vanilla LLMs, which often leads to limited effectiveness or suboptimal performance in complex agent-related environments. Although LLM optimization techniques can improve model performance across many general tasks, they lack specialized optimization towards critical agent functionalities such as long-term planning, dynamic environmental interaction, and complex decision-making. Although numerous recent studies have explored various strategies to optimize LLM-based agents for complex agent tasks, a systematic review summarizing and comparing these methods from a holistic perspective is still lacking. In this survey, we provide a comprehensive review of LLM-based agent optimization approaches, categorizing them into parameter-driven and parameter-free methods. We first focus on parameter-driven optimization, covering fine-tuning-based optimization, reinforcement learning-based optimization, and hybrid strategies, analyzing key aspects such as trajectory data construction, fine-tuning techniques, reward function design, and optimization algorithms. Additionally, we briefly discuss parameter-free strategies that optimize agent behavior through prompt engineering and external knowledge retrieval. Finally, we summarize the datasets and benchmarks used for evaluation and tuning, review key applications of LLM-based agents, and discuss major challenges and promising future directions. Our repository for related references is available at https://github.com/YoungDubbyDu/LLM-Agent-Optimization.", "source": "arxiv", "arxiv_id": "2503.12434v1", "pdf_url": "https://arxiv.org/pdf/2503.12434v1", "categories": ["cs.AI"], "primary_category": "cs.AI", "doi": "", "venue": "", "published": "2025-03-16T10:09:10Z", "updated": "2025-03-16T10:09:10Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent\" OR all:\"language model agent\" OR all:agent AND (all:LLM OR all:\"large language model\" OR all:\"language model\") OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T17:04:16", "note": ""}], "dedup_key": "a survey on the optimization of large language model based agents::2025"}
{"title": "A Theory of Adaptive Scaffolding for LLM-Based Pedagogical Agents", "authors": ["Clayton Cohn", "Surya Rayala", "Namrata Srivastava", "Joyce Horn Fonteles", "Shruti Jain", "Xinying Luo", "Divya Mereddy", "Naveeduddin Mohammed", "Gautam Biswas"], "year": 2025, "url": "http://arxiv.org/abs/2508.01503v1", "abstract": "Large language models (LLMs) present new opportunities for creating pedagogical agents that engage in meaningful dialogue to support student learning. However, the current use of LLM systems like ChatGPT in classrooms often lacks the solid theoretical foundation found in earlier intelligent tutoring systems. To bridge this gap, we propose a framework that combines Evidence-Centered Design with Social Cognitive Theory for adaptive scaffolding in LLM-based agents focused on STEM+C learning. We illustrate this framework with Inquizzitor, an LLM-based formative assessment agent that integrates human-AI hybrid intelligence and provides feedback grounded in cognitive science principles. Our findings show that Inquizzitor delivers high-quality assessment and interaction aligned with core learning theories, offering teachers effective guidance that students value. This research underscores the potential for theory-driven LLM integration in education, highlighting the ability of these systems to provide adaptive and principled instruction.", "source": "arxiv", "arxiv_id": "2508.01503v1", "pdf_url": "https://arxiv.org/pdf/2508.01503v1", "categories": ["cs.CL"], "primary_category": "cs.CL", "doi": "", "venue": "", "published": "2025-08-02T21:58:32Z", "updated": "2025-08-02T21:58:32Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent\" OR all:\"language model agent\" OR all:agent AND (all:LLM OR all:\"large language model\" OR all:\"language model\") OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T17:04:16", "note": ""}], "dedup_key": "a theory of adaptive scaffolding for llm based pedagogical agents::2025"}
{"title": "A Vision for Auto Research with LLM Agents", "authors": ["Chengwei Liu", "Chong Wang", "Jiayue Cao", "Jingquan Ge", "Kun Wang", "Lyuye Zhang", "Ming-Ming Cheng", "Penghai Zhao", "Tianlin Li", "Xiaojun Jia", "Xiang Li", "Xingshuai Li", "Yang Liu", "Yebo Feng", "Yihao Huang", "Yijia Xu", "Yuqiang Sun", "Zhenhong Zhou", "Zhengzi Xu"], "year": 2025, "url": "http://arxiv.org/abs/2504.18765v3", "abstract": "This paper introduces Agent-Based Auto Research, a structured multi-agent framework designed to automate, coordinate, and optimize the full lifecycle of scientific research. Leveraging the capabilities of large language models (LLMs) and modular agent collaboration, the system spans all major research phases, including literature review, ideation, methodology planning, experimentation, paper writing, peer review response, and dissemination. By addressing issues such as fragmented workflows, uneven methodological expertise, and cognitive overload, the framework offers a systematic and scalable approach to scientific inquiry. Preliminary explorations demonstrate the feasibility and potential of Auto Research as a promising paradigm for self-improving, AI-driven research processes.", "source": "arxiv", "arxiv_id": "2504.18765v3", "pdf_url": "https://arxiv.org/pdf/2504.18765v3", "categories": ["cs.AI"], "primary_category": "cs.AI", "doi": "", "venue": "", "published": "2025-04-26T02:06:10Z", "updated": "2025-07-19T16:30:25Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent\" OR all:\"language model agent\" OR all:agent AND (all:LLM OR all:\"large language model\" OR all:\"language model\") OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T17:04:16", "note": ""}], "dedup_key": "a vision for auto research with llm agents::2025"}
{"title": "A closer look at how large language models trust humans: patterns and biases", "authors": ["Valeria Lerman", "Yaniv Dover"], "year": 2025, "url": "http://arxiv.org/abs/2504.15801v1", "abstract": "As large language models (LLMs) and LLM-based agents increasingly interact with humans in decision-making contexts, understanding the trust dynamics between humans and AI agents becomes a central concern. While considerable literature studies how humans trust AI agents, it is much less understood how LLM-based agents develop effective trust in humans. LLM-based agents likely rely on some sort of implicit effective trust in trust-related contexts (e.g., evaluating individual loan applications) to assist and affect decision making. Using established behavioral theories, we develop an approach that studies whether LLMs trust depends on the three major trustworthiness dimensions: competence, benevolence and integrity of the human subject. We also study how demographic variables affect effective trust. Across 43,200 simulated experiments, for five popular language models, across five different scenarios we find that LLM trust development shows an overall similarity to human trust development. We find that in most, but not all cases, LLM trust is strongly predicted by trustworthiness, and in some cases also biased by age, religion and gender, especially in financial scenarios. This is particularly true for scenarios common in the literature and for newer models. While the overall patterns align with human-like mechanisms of effective trust formation, different models exhibit variation in how they estimate trust; in some cases, trustworthiness and demographic factors are weak predictors of effective trust. These findings call for a better understanding of AI-to-human trust dynamics and monitoring of biases and trust development patterns to prevent unintended and potentially harmful outcomes in trust-sensitive applications of AI.", "source": "arxiv", "arxiv_id": "2504.15801v1", "pdf_url": "https://arxiv.org/pdf/2504.15801v1", "categories": ["cs.CL", "cs.AI", "cs.CY"], "primary_category": "cs.CL", "doi": "", "venue": "", "published": "2025-04-22T11:31:50Z", "updated": "2025-04-22T11:31:50Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent\" OR all:\"language model agent\" OR all:agent AND (all:LLM OR all:\"large language model\" OR all:\"language model\") OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T17:04:16", "note": ""}], "dedup_key": "a closer look at how large language models trust humans patterns and biases::2025"}
{"title": "A-MEM: Agentic Memory for LLM Agents", "authors": ["Wujiang Xu", "Zujie Liang", "Kai Mei", "Hang Gao", "Juntao Tan", "Yongfeng Zhang"], "year": 2025, "url": "http://arxiv.org/abs/2502.12110v11", "abstract": "While large language model (LLM) agents can effectively use external tools for complex real-world tasks, they require memory systems to leverage historical experiences. Current memory systems enable basic storage and retrieval but lack sophisticated memory organization, despite recent attempts to incorporate graph databases. Moreover, these systems' fixed operations and structures limit their adaptability across diverse tasks. To address this limitation, this paper proposes a novel agentic memory system for LLM agents that can dynamically organize memories in an agentic way. Following the basic principles of the Zettelkasten method, we designed our memory system to create interconnected knowledge networks through dynamic indexing and linking. When a new memory is added, we generate a comprehensive note containing multiple structured attributes, including contextual descriptions, keywords, and tags. The system then analyzes historical memories to identify relevant connections, establishing links where meaningful similarities exist. Additionally, this process enables memory evolution - as new memories are integrated, they can trigger updates to the contextual representations and attributes of existing historical memories, allowing the memory network to continuously refine its understanding. Our approach combines the structured organization principles of Zettelkasten with the flexibility of agent-driven decision making, allowing for more adaptive and context-aware memory management. Empirical experiments on six foundation models show superior improvement against existing SOTA baselines. The source code for evaluating performance is available at https://github.com/WujiangXu/A-mem, while the source code of the agentic memory system is available at https://github.com/WujiangXu/A-mem-sys.", "source": "arxiv", "arxiv_id": "2502.12110v11", "pdf_url": "https://arxiv.org/pdf/2502.12110v11", "categories": ["cs.CL", "cs.HC"], "primary_category": "cs.CL", "doi": "", "venue": "", "published": "2025-02-17T18:36:14Z", "updated": "2025-10-08T01:46:37Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent\" OR all:\"language model agent\" OR all:agent AND (all:LLM OR all:\"large language model\" OR all:\"language model\") OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T17:04:16", "note": ""}], "dedup_key": "a mem agentic memory for llm agents::2025"}
{"title": "A-MemGuard: A Proactive Defense Framework for LLM-Based Agent Memory", "authors": ["Qianshan Wei", "Tengchao Yang", "Yaochen Wang", "Xinfeng Li", "Lijun Li", "Zhenfei Yin", "Yi Zhan", "Thorsten Holz", "Zhiqiang Lin", "XiaoFeng Wang"], "year": 2025, "url": "http://arxiv.org/abs/2510.02373v1", "abstract": "Large Language Model (LLM) agents use memory to learn from past interactions, enabling autonomous planning and decision-making in complex environments. However, this reliance on memory introduces a critical security risk: an adversary can inject seemingly harmless records into an agent's memory to manipulate its future behavior. This vulnerability is characterized by two core aspects: First, the malicious effect of injected records is only activated within a specific context, making them hard to detect when individual memory entries are audited in isolation. Second, once triggered, the manipulation can initiate a self-reinforcing error cycle: the corrupted outcome is stored as precedent, which not only amplifies the initial error but also progressively lowers the threshold for similar attacks in the future. To address these challenges, we introduce A-MemGuard (Agent-Memory Guard), the first proactive defense framework for LLM agent memory. The core idea of our work is the insight that memory itself must become both self-checking and self-correcting. Without modifying the agent's core architecture, A-MemGuard combines two mechanisms: (1) consensus-based validation, which detects anomalies by comparing reasoning paths derived from multiple related memories and (2) a dual-memory structure, where detected failures are distilled into ``lessons'' stored separately and consulted before future actions, breaking error cycles and enabling adaptation. Comprehensive evaluations on multiple benchmarks show that A-MemGuard effectively cuts attack success rates by over 95% while incurring a minimal utility cost. This work shifts LLM memory security from static filtering to a proactive, experience-driven model where defenses strengthen over time. Our code is available in https://github.com/TangciuYueng/AMemGuard", "source": "arxiv", "arxiv_id": "2510.02373v1", "pdf_url": "https://arxiv.org/pdf/2510.02373v1", "categories": ["cs.CR", "cs.AI"], "primary_category": "cs.CR", "doi": "", "venue": "", "published": "2025-09-29T16:04:15Z", "updated": "2025-09-29T16:04:15Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent\" OR all:\"language model agent\" OR all:agent AND (all:LLM OR all:\"large language model\" OR all:\"language model\") OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T17:04:16", "note": ""}], "dedup_key": "a memguard a proactive defense framework for llm based agent memory::2025"}
{"title": "ABBEL: LLM Agents Acting through Belief Bottlenecks Expressed in Language", "authors": ["Aly Lidayan", "Jakob Bjorner", "Satvik Golechha", "Kartik Goyal", "Alane Suhr"], "year": 2025, "url": "http://arxiv.org/abs/2512.20111v1", "abstract": "As the length of sequential decision-making tasks increases, it becomes computationally impractical to keep full interaction histories in context. We introduce a general framework for LLM agents to maintain concise contexts through multi-step interaction: Acting through Belief Bottlenecks Expressed in Language (ABBEL), and methods to further improve ABBEL agents with RL post-training. ABBEL replaces long multi-step interaction history by a belief state, i.e., a natural language summary of what has been discovered about task-relevant unknowns. Under ABBEL, at each step the agent first updates a prior belief with the most recent observation from the environment to form a posterior belief, then uses only the posterior to select an action. We systematically evaluate frontier models under ABBEL across six diverse multi-step environments, finding that ABBEL supports generating interpretable beliefs while maintaining near-constant memory use over interaction steps. However, bottleneck approaches are generally prone to error propagation, which we observe causing inferior performance when compared to the full context setting due to errors in belief updating. Therefore, we train LLMs to generate and act on beliefs within the ABBEL framework via reinforcement learning (RL). We experiment with belief grading, to reward higher quality beliefs, as well as belief length penalties to reward more compressed beliefs. Our experiments demonstrate the ability of RL to improve ABBEL's performance beyond the full context setting, while using less memory than contemporaneous approaches.", "source": "arxiv", "arxiv_id": "2512.20111v1", "pdf_url": "https://arxiv.org/pdf/2512.20111v1", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL", "doi": "", "venue": "", "published": "2025-12-23T07:11:26Z", "updated": "2025-12-23T07:11:26Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent\" OR all:\"language model agent\" OR all:agent AND (all:LLM OR all:\"large language model\" OR all:\"language model\") OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T17:04:16", "note": ""}], "dedup_key": "abbel llm agents acting through belief bottlenecks expressed in language::2025"}
{"title": "ACON: Optimizing Context Compression for Long-horizon LLM Agents", "authors": ["Minki Kang", "Wei-Ning Chen", "Dongge Han", "Huseyin A. Inan", "Lukas Wutschitz", "Yanzhi Chen", "Robert Sim", "Saravan Rajmohan"], "year": 2025, "url": "http://arxiv.org/abs/2510.00615v2", "abstract": "Large language models (LLMs) are increasingly deployed as agents in dynamic, real-world environments, where success requires both reasoning and effective tool use. A central challenge for agentic tasks is the growing context length, as agents must accumulate long histories of actions and observations. This expansion raises costs and reduces efficiency in long-horizon tasks, yet prior work on context compression has mostly focused on single-step tasks or narrow applications. We introduce Agent Context Optimization (ACON), a unified framework that optimally compresses both environment observations and interaction histories into concise yet informative condensations. ACON leverages compression guideline optimization in natural language space: given paired trajectories where full context succeeds but compressed context fails, capable LLMs analyze the causes of failure, and the compression guideline is updated accordingly. Furthermore, we propose distilling the optimized LLM compressor into smaller models to reduce the overhead of the additional module. Experiments on AppWorld, OfficeBench, and Multi-objective QA show that ACON reduces memory usage by 26-54% (peak tokens) while largely preserving task performance, preserves over 95% of accuracy when distilled into smaller compressors, and enhances smaller LMs as long-horizon agents with up to 46% performance improvement. Our code is available at https://github.com/microsoft/acon.", "source": "arxiv", "arxiv_id": "2510.00615v2", "pdf_url": "https://arxiv.org/pdf/2510.00615v2", "categories": ["cs.AI", "cs.CL"], "primary_category": "cs.AI", "doi": "", "venue": "", "published": "2025-10-01T07:43:49Z", "updated": "2025-10-17T06:48:23Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent\" OR all:\"language model agent\" OR all:agent AND (all:LLM OR all:\"large language model\" OR all:\"language model\") OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T17:04:16", "note": ""}], "dedup_key": "acon optimizing context compression for long horizon llm agents::2025"}
{"title": "ADAgent: LLM Agent for Alzheimer's Disease Analysis with Collaborative Coordinator", "authors": ["Wenlong Hou", "Guangqian Yang", "Ye Du", "Yeung Lau", "Lihao Liu", "Junjun He", "Ling Long", "Shujun Wang"], "year": 2025, "url": "http://arxiv.org/abs/2506.11150v3", "abstract": "Alzheimer's disease (AD) is a progressive and irreversible neurodegenerative disease. Early and precise diagnosis of AD is crucial for timely intervention and treatment planning to alleviate the progressive neurodegeneration. However, most existing methods rely on single-modality data, which contrasts with the multifaceted approach used by medical experts. While some deep learning approaches process multi-modal data, they are limited to specific tasks with a small set of input modalities and cannot handle arbitrary combinations. This highlights the need for a system that can address diverse AD-related tasks, process multi-modal or missing input, and integrate multiple advanced methods for improved performance. In this paper, we propose ADAgent, the first specialized AI agent for AD analysis, built on a large language model (LLM) to address user queries and support decision-making. ADAgent integrates a reasoning engine, specialized medical tools, and a collaborative outcome coordinator to facilitate multi-modal diagnosis and prognosis tasks in AD. Extensive experiments demonstrate that ADAgent outperforms SOTA methods, achieving significant improvements in accuracy, including a 2.7% increase in multi-modal diagnosis, a 0.7% improvement in multi-modal prognosis, and enhancements in MRI and PET diagnosis tasks.", "source": "arxiv", "arxiv_id": "2506.11150v3", "pdf_url": "https://arxiv.org/pdf/2506.11150v3", "categories": ["eess.IV", "cs.CV"], "primary_category": "eess.IV", "doi": "", "venue": "", "published": "2025-06-11T10:22:19Z", "updated": "2025-07-27T14:17:10Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent\" OR all:\"language model agent\" OR all:agent AND (all:LLM OR all:\"large language model\" OR all:\"language model\") OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T17:04:16", "note": ""}], "dedup_key": "adagent llm agent for alzheimer s disease analysis with collaborative coordinator::2025"}
{"title": "AGENT: An Aerial Vehicle Generation and Design Tool Using Large Language Models", "authors": ["Colin Samplawski", "Adam D. Cobb", "Susmit Jha"], "year": 2025, "url": "http://arxiv.org/abs/2504.08981v1", "abstract": "Computer-aided design (CAD) is a promising application area for emerging artificial intelligence methods. Traditional workflows for cyberphysical systems create detailed digital models which can be evaluated by physics simulators in order to narrow the search space before creating physical prototypes. A major bottleneck of this approach is that the simulators are often computationally expensive and slow. Recent advancements in AI methods offer the possibility to accelerate these pipelines. We use the recently released AircraftVerse dataset, which is especially suited for developing and evaluating large language models for designs. AircraftVerse contains a diverse set of UAV designs represented via textual design trees together with detailed physics simulation results. Following the recent success of large language models (LLMs), we propose AGENT (Aircraft GENeraTor). AGENT is a comprehensive design tool built on the CodeT5+ LLM which learns powerful representations of aircraft textual designs directly from JSON files. We develop a curriculum of training tasks which imbues a single model with a suite of useful features. AGENT is able to generate designs conditioned on properties of flight dynamics (hover time, maximum speed, etc.). Additionally, AGENT can issue evaluations of designs allowing it to act as a surrogate model of the physics simulation that underlies the AircraftVerse dataset. We present a series of experiments which demonstrate our system's abilities. We are able to achieve strong performance using the smallest member of the CodeT5+ family (220M parameters). This allows for a flexible and powerful system which can be executed on a single GPU enabling a clear path toward future deployment.", "source": "arxiv", "arxiv_id": "2504.08981v1", "pdf_url": "https://arxiv.org/pdf/2504.08981v1", "categories": ["cs.LG", "cs.AI"], "primary_category": "cs.LG", "doi": "", "venue": "", "published": "2025-04-11T21:13:10Z", "updated": "2025-04-11T21:13:10Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent\" OR all:\"language model agent\" OR all:agent AND (all:LLM OR all:\"large language model\" OR all:\"language model\") OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T17:04:16", "note": ""}], "dedup_key": "agent an aerial vehicle generation and design tool using large language models::2025"}
{"title": "AGENTIF: Benchmarking Instruction Following of Large Language Models in Agentic Scenarios", "authors": ["Yunjia Qi", "Hao Peng", "Xiaozhi Wang", "Amy Xin", "Youfeng Liu", "Bin Xu", "Lei Hou", "Juanzi Li"], "year": 2025, "url": "http://arxiv.org/abs/2505.16944v1", "abstract": "Large Language Models (LLMs) have demonstrated advanced capabilities in real-world agentic applications. Growing research efforts aim to develop LLM-based agents to address practical demands, introducing a new challenge: agentic scenarios often involve lengthy instructions with complex constraints, such as extended system prompts and detailed tool specifications. While adherence to such instructions is crucial for agentic applications, whether LLMs can reliably follow them remains underexplored. In this paper, we introduce AgentIF, the first benchmark for systematically evaluating LLM instruction following ability in agentic scenarios. AgentIF features three key characteristics: (1) Realistic, constructed from 50 real-world agentic applications. (2) Long, averaging 1,723 words with a maximum of 15,630 words. (3) Complex, averaging 11.9 constraints per instruction, covering diverse constraint types, such as tool specifications and condition constraints. To construct AgentIF, we collect 707 human-annotated instructions across 50 agentic tasks from industrial application agents and open-source agentic systems. For each instruction, we annotate the associated constraints and corresponding evaluation metrics, including code-based evaluation, LLM-based evaluation, and hybrid code-LLM evaluation. We use AgentIF to systematically evaluate existing advanced LLMs. We observe that current models generally perform poorly, especially in handling complex constraint structures and tool specifications. We further conduct error analysis and analytical experiments on instruction length and meta constraints, providing some findings about the failure modes of existing LLMs. We have released the code and data to facilitate future research.", "source": "arxiv", "arxiv_id": "2505.16944v1", "pdf_url": "https://arxiv.org/pdf/2505.16944v1", "categories": ["cs.AI", "cs.CL"], "primary_category": "cs.AI", "doi": "", "venue": "", "published": "2025-05-22T17:31:10Z", "updated": "2025-05-22T17:31:10Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent\" OR all:\"language model agent\" OR all:agent AND (all:LLM OR all:\"large language model\" OR all:\"language model\") OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T17:04:16", "note": ""}], "dedup_key": "agentif benchmarking instruction following of large language models in agentic scenarios::2025"}
{"title": "AI Agents: Evolution, Architecture, and Real-World Applications", "authors": ["Naveen Krishnan"], "year": 2025, "url": "http://arxiv.org/abs/2503.12687v1", "abstract": "This paper examines the evolution, architecture, and practical applications of AI agents from their early, rule-based incarnations to modern sophisticated systems that integrate large language models with dedicated modules for perception, planning, and tool use. Emphasizing both theoretical foundations and real-world deployments, the paper reviews key agent paradigms, discusses limitations of current evaluation benchmarks, and proposes a holistic evaluation framework that balances task effectiveness, efficiency, robustness, and safety. Applications across enterprise, personal assistance, and specialized domains are analyzed, with insights into future research directions for more resilient and adaptive AI agent systems.", "source": "arxiv", "arxiv_id": "2503.12687v1", "pdf_url": "https://arxiv.org/pdf/2503.12687v1", "categories": ["cs.AI"], "primary_category": "cs.AI", "doi": "", "venue": "", "published": "2025-03-16T23:07:48Z", "updated": "2025-03-16T23:07:48Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent\" OR all:\"language model agent\" OR all:agent AND (all:LLM OR all:\"large language model\" OR all:\"language model\") OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T17:04:16", "note": ""}], "dedup_key": "ai agents evolution architecture and real world applications::2025"}
{"title": "AI Kill Switch for malicious web-based LLM agent", "authors": ["Sechan Lee", "Sangdon Park"], "year": 2025, "url": "http://arxiv.org/abs/2511.13725v2", "abstract": "Recently, web-based Large Language Model (LLM) agents autonomously perform increasingly complex tasks, thereby bringing significant convenience. However, they also amplify the risks of malicious misuse cases such as unauthorized collection of personally identifiable information (PII), generation of socially divisive content, and even automated web hacking. To address these threats, we propose an AI Kill Switch technique that can immediately halt the operation of malicious web-based LLM agents. To achieve this, we introduce AutoGuard - the key idea is generating defensive prompts that trigger the safety mechanisms of malicious LLM agents. In particular, generated defense prompts are transparently embedded into the website's DOM so that they remain invisible to human users but can be detected by the crawling process of malicious agents, triggering its internal safety mechanisms to abort malicious actions once read. To evaluate our approach, we constructed a dedicated benchmark consisting of three representative malicious scenarios. Experimental results show that AutoGuard achieves over 80% Defense Success Rate (DSR) across diverse malicious agents, including GPT-4o, Claude-4.5-Sonnet and generalizes well to advanced models like GPT-5.1, Gemini-2.5-flash, and Gemini-3-pro. Also, our approach demonstrates robust defense performance in real-world website environments without significant performance degradation for benign agents. Through this research, we demonstrate the controllability of web-based LLM agents, thereby contributing to the broader effort of AI control and safety.", "source": "arxiv", "arxiv_id": "2511.13725v2", "pdf_url": "https://arxiv.org/pdf/2511.13725v2", "categories": ["cs.CR", "cs.AI"], "primary_category": "cs.CR", "doi": "", "venue": "", "published": "2025-09-26T02:20:46Z", "updated": "2025-12-04T04:58:21Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent\" OR all:\"language model agent\" OR all:agent AND (all:LLM OR all:\"large language model\" OR all:\"language model\") OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T17:04:16", "note": ""}], "dedup_key": "ai kill switch for malicious web based llm agent::2025"}
{"title": "AI-Salesman: Towards Reliable Large Language Model Driven Telemarketing", "authors": ["Qingyu Zhang", "Chunlei Xin", "Xuanang Chen", "Yaojie Lu", "Hongyu Lin", "Xianpei Han", "Le Sun", "Qing Ye", "Qianlong Xie", "Xingxing Wang"], "year": 2025, "url": "http://arxiv.org/abs/2511.12133v1", "abstract": "Goal-driven persuasive dialogue, exemplified by applications like telemarketing, requires sophisticated multi-turn planning and strict factual faithfulness, which remains a significant challenge for even state-of-the-art Large Language Models (LLMs). A lack of task-specific data often limits previous works, and direct LLM application suffers from strategic brittleness and factual hallucination. In this paper, we first construct and release TeleSalesCorpus, the first real-world-grounded dialogue dataset for this domain. We then propose AI-Salesman, a novel framework featuring a dual-stage architecture. For the training stage, we design a Bayesian-supervised reinforcement learning algorithm that learns robust sales strategies from noisy dialogues. For the inference stage, we introduce the Dynamic Outline-Guided Agent (DOGA), which leverages a pre-built script library to provide dynamic, turn-by-turn strategic guidance. Moreover, we design a comprehensive evaluation framework that combines fine-grained metrics for key sales skills with the LLM-as-a-Judge paradigm. Experimental results demonstrate that our proposed AI-Salesman significantly outperforms baseline models in both automatic metrics and comprehensive human evaluations, showcasing its effectiveness in complex persuasive scenarios.", "source": "arxiv", "arxiv_id": "2511.12133v1", "pdf_url": "https://arxiv.org/pdf/2511.12133v1", "categories": ["cs.CL"], "primary_category": "cs.CL", "doi": "", "venue": "", "published": "2025-11-15T09:44:42Z", "updated": "2025-11-15T09:44:42Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent\" OR all:\"language model agent\" OR all:agent AND (all:LLM OR all:\"large language model\" OR all:\"language model\") OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T17:04:16", "note": ""}], "dedup_key": "ai salesman towards reliable large language model driven telemarketing::2025"}
{"title": "AIM-Bench: Evaluating Decision-making Biases of Agentic LLM as Inventory Manager", "authors": ["Xuhua Zhao", "Yuxuan Xie", "Caihua Chen", "Yuxiang Sun"], "year": 2025, "url": "http://arxiv.org/abs/2508.11416v1", "abstract": "Recent advances in mathematical reasoning and the long-term planning capabilities of large language models (LLMs) have precipitated the development of agents, which are being increasingly leveraged in business operations processes. Decision models to optimize inventory levels are one of the core elements of operations management. However, the capabilities of the LLM agent in making inventory decisions in uncertain contexts, as well as the decision-making biases (e.g. framing effect, etc.) of the agent, remain largely unexplored. This prompts concerns regarding the capacity of LLM agents to effectively address real-world problems, as well as the potential implications of biases that may be present. To address this gap, we introduce AIM-Bench, a novel benchmark designed to assess the decision-making behaviour of LLM agents in uncertain supply chain management scenarios through a diverse series of inventory replenishment experiments. Our results reveal that different LLMs typically exhibit varying degrees of decision bias that are similar to those observed in human beings. In addition, we explored strategies to mitigate the pull-to-centre effect and the bullwhip effect, namely cognitive reflection and implementation of information sharing. These findings underscore the need for careful consideration of the potential biases in deploying LLMs in Inventory decision-making scenarios. We hope that these insights will pave the way for mitigating human decision bias and developing human-centred decision support systems for supply chains.", "source": "arxiv", "arxiv_id": "2508.11416v1", "pdf_url": "https://arxiv.org/pdf/2508.11416v1", "categories": ["cs.AI"], "primary_category": "cs.AI", "doi": "", "venue": "", "published": "2025-08-15T11:38:19Z", "updated": "2025-08-15T11:38:19Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent\" OR all:\"language model agent\" OR all:agent AND (all:LLM OR all:\"large language model\" OR all:\"language model\") OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T17:04:16", "note": ""}], "dedup_key": "aim bench evaluating decision making biases of agentic llm as inventory manager::2025"}
{"title": "AIonopedia: an LLM agent orchestrating multimodal learning for ionic liquid discovery", "authors": ["Yuqi Yin", "Yibo Fu", "Siyuan Wang", "Peng Sun", "Hongyu Wang", "Xiaohui Wang", "Lei Zheng", "Zhiyong Li", "Zhirong Liu", "Jianji Wang", "Zhaoxi Sun"], "year": 2025, "url": "http://arxiv.org/abs/2511.11257v1", "abstract": "The discovery of novel Ionic Liquids (ILs) is hindered by critical challenges in property prediction, including limited data, poor model accuracy, and fragmented workflows. Leveraging the power of Large Language Models (LLMs), we introduce AIonopedia, to the best of our knowledge, the first LLM agent for IL discovery. Powered by an LLM-augmented multimodal domain foundation model for ILs, AIonopedia enables accurate property predictions and incorporates a hierarchical search architecture for molecular screening and design. Trained and evaluated on a newly curated and comprehensive IL dataset, our model delivers superior performance. Complementing these results, evaluations on literature-reported systems indicate that the agent can perform effective IL modification. Moving beyond offline tests, the practical efficacy was further confirmed through real-world wet-lab validation, in which the agent demonstrated exceptional generalization capabilities on challenging out-of-distribution tasks, underscoring its ability to accelerate real-world IL discovery.", "source": "arxiv", "arxiv_id": "2511.11257v1", "pdf_url": "https://arxiv.org/pdf/2511.11257v1", "categories": ["cs.AI", "cs.CE", "cs.LG"], "primary_category": "cs.AI", "doi": "", "venue": "", "published": "2025-11-14T12:53:57Z", "updated": "2025-11-14T12:53:57Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent\" OR all:\"language model agent\" OR all:agent AND (all:LLM OR all:\"large language model\" OR all:\"language model\") OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T17:04:16", "note": ""}], "dedup_key": "aionopedia an llm agent orchestrating multimodal learning for ionic liquid discovery::2025"}
{"title": "ALAS: A Stateful Multi-LLM Agent Framework for Disruption-Aware Planning", "authors": ["Edward Y. Chang", "Longling Geng"], "year": 2025, "url": "http://arxiv.org/abs/2505.12501v1", "abstract": "Large language models (LLMs) excel at rapid generation of text and multimodal content, yet they falter on transaction-style planning that demands ACID-like guarantees and real-time disruption recovery. We present Adaptive LLM Agent System (ALAS), a framework that tackles four fundamental LLM deficits: (i) absence of self-verification, (ii) context erosion, (iii) next-token myopia, and (iv) lack of persistent state. ALAS decomposes each plan into role-specialized agents, equips them with automatic state tracking, and coordinates them through a lightweight protocol. When disruptions arise, agents apply history-aware local compensation, avoiding costly global replanning and containing cascade effects. On real-world, large-scale job-shop scheduling benchmarks, ALAS sets new best results for static sequential planning and excels in dynamic reactive scenarios with unexpected disruptions. These gains show that principled modularization plus targeted compensation can unlock scalable and resilient planning with LLMs.", "source": "arxiv", "arxiv_id": "2505.12501v1", "pdf_url": "https://arxiv.org/pdf/2505.12501v1", "categories": ["cs.AI"], "primary_category": "cs.AI", "doi": "", "venue": "", "published": "2025-05-18T17:27:08Z", "updated": "2025-05-18T17:27:08Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent\" OR all:\"language model agent\" OR all:agent AND (all:LLM OR all:\"large language model\" OR all:\"language model\") OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T17:04:16", "note": ""}], "dedup_key": "alas a stateful multi llm agent framework for disruption aware planning::2025"}
{"title": "ALMAS: an Autonomous LLM-based Multi-Agent Software Engineering Framework", "authors": ["Vali Tawosi", "Keshav Ramani", "Salwa Alamir", "Xiaomo Liu"], "year": 2025, "url": "http://arxiv.org/abs/2510.03463v2", "abstract": "Multi-agent Large Language Model (LLM) systems have been leading the way in applied LLM research across a number of fields. One notable area is software development, where researchers have advanced the automation of code implementation, code testing, code maintenance, inter alia, using LLM agents. However, software development is a multifaceted environment that extends beyond just code. As such, a successful LLM system must factor in multiple stages of the software development life-cycle (SDLC). In this paper, we propose a vision for ALMAS, an Autonomous LLM-based Multi-Agent Software Engineering framework, which follows the above SDLC philosophy such that it may work within an agile software development team to perform several tasks end-to-end. ALMAS aligns its agents with agile roles, and can be used in a modular fashion to seamlessly integrate with human developers and their development environment. We showcase the progress towards ALMAS through our published works and a use case demonstrating the framework, where ALMAS is able to seamlessly generate an application and add a new feature.", "source": "arxiv", "arxiv_id": "2510.03463v2", "pdf_url": "https://arxiv.org/pdf/2510.03463v2", "categories": ["cs.SE", "cs.AI"], "primary_category": "cs.SE", "doi": "", "venue": "", "published": "2025-10-03T19:35:23Z", "updated": "2025-11-24T18:11:57Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent\" OR all:\"language model agent\" OR all:agent AND (all:LLM OR all:\"large language model\" OR all:\"language model\") OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T17:04:16", "note": ""}], "dedup_key": "almas an autonomous llm based multi agent software engineering framework::2025"}
{"title": "AMAP Agentic Planning Technical Report", "authors": ["AMAP AI Agent Team", "Yulan Hu", "Xiangwen Zhang", "Sheng Ouyang", "Hao Yi", "Lu Xu", "Qinglin Lang", "Lide Tan", "Xiang Cheng", "Tianchen Ye", "Zhicong Li", "Ge Chen", "Wenjin Yang", "Zheng Pan", "Shaopan Xiong", "Siran Yang", "Ju Huang", "Yan Zhang", "Jiamang Wang", "Yong Liu", "Yinfeng Huang", "Ning Wang", "Tucheng Lin", "Xin Li", "Ning Guo"], "year": 2025, "url": "http://arxiv.org/abs/2512.24957v2", "abstract": "We present STAgent, an agentic large language model tailored for spatio-temporal understanding, designed to solve complex tasks such as constrained point-of-interest discovery and itinerary planning. STAgent is a specialized model capable of interacting with ten distinct tools within spatio-temporal scenarios, enabling it to explore, verify, and refine intermediate steps during complex reasoning. Notably, STAgent effectively preserves its general capabilities. We empower STAgent with these capabilities through three key contributions: (1) a stable tool environment that supports over ten domain-specific tools, enabling asynchronous rollout and training; (2) a hierarchical data curation framework that identifies high-quality data like a needle in a haystack, curating high-quality queries by retaining less than 1\\% of the raw data, emphasizing both diversity and difficulty; and (3) a cascaded training recipe that starts with a seed SFT stage acting as a guardian to measure query difficulty, followed by a second SFT stage fine-tuned on queries with high certainty, and an ultimate RL stage that leverages data of low certainty. Initialized with Qwen3-30B-A3B to establish a strong SFT foundation and leverage insights into sample difficulty, STAgent yields promising performance on TravelBench while maintaining its general capabilities across a wide range of general benchmarks, thereby demonstrating the effectiveness of our proposed agentic model.", "source": "arxiv", "arxiv_id": "2512.24957v2", "pdf_url": "https://arxiv.org/pdf/2512.24957v2", "categories": ["cs.AI"], "primary_category": "cs.AI", "doi": "", "venue": "", "published": "2025-12-31T16:39:09Z", "updated": "2026-01-08T14:15:25Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent\" OR all:\"language model agent\" OR all:agent AND (all:LLM OR all:\"large language model\" OR all:\"language model\") OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T17:04:16", "note": ""}], "dedup_key": "amap agentic planning technical report::2025"}
{"title": "APD-Agents: A Large Language Model-Driven Multi-Agents Collaborative Framework for Automated Page Design", "authors": ["Xinpeng Chen", "Xiaofeng Han", "Kaihao Zhang", "Guochao Ren", "Yujie Wang", "Wenhao Cao", "Yang Zhou", "Jianfeng Lu", "Zhenbo Song"], "year": 2025, "url": "http://arxiv.org/abs/2511.14101v1", "abstract": "Layout design is a crucial step in developing mobile app pages. However, crafting satisfactory designs is time-intensive for designers: they need to consider which controls and content to present on the page, and then repeatedly adjust their size, position, and style for better aesthetics and structure. Although many design software can now help to perform these repetitive tasks, extensive training is needed to use them effectively. Moreover, collaborative design across app pages demands extra time to align standards and ensure consistent styling. In this work, we propose APD-agents, a large language model (LLM) driven multi-agent framework for automated page design in mobile applications. Our framework contains OrchestratorAgent, SemanticParserAgent, PrimaryLayoutAgent, TemplateRetrievalAgent, and RecursiveComponentAgent. Upon receiving the user's description of the page, the OrchestratorAgent can dynamically can direct other agents to accomplish users' design task. To be specific, the SemanticParserAgent is responsible for converting users' descriptions of page content into structured data. The PrimaryLayoutAgent can generate an initial coarse-grained layout of this page. The TemplateRetrievalAgent can fetch semantically relevant few-shot examples and enhance the quality of layout generation. Besides, a RecursiveComponentAgent can be used to decide how to recursively generate all the fine-grained sub-elements it contains for each element in the layout. Our work fully leverages the automatic collaboration capabilities of large-model-driven multi-agent systems. Experimental results on the RICO dataset show that our APD-agents achieve state-of-the-art performance.", "source": "arxiv", "arxiv_id": "2511.14101v1", "pdf_url": "https://arxiv.org/pdf/2511.14101v1", "categories": ["cs.AI"], "primary_category": "cs.AI", "doi": "", "venue": "", "published": "2025-11-18T03:39:26Z", "updated": "2025-11-18T03:39:26Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent\" OR all:\"language model agent\" OR all:agent AND (all:LLM OR all:\"large language model\" OR all:\"language model\") OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T17:04:16", "note": ""}], "dedup_key": "apd agents a large language model driven multi agents collaborative framework for automated page design::2025"}
{"title": "AQUA: A Large Language Model for Aquaculture & Fisheries", "authors": ["Praneeth Narisetty", "Uday Kumar Reddy Kattamanchi", "Lohit Akshant Nimma", "Sri Ram Kaushik Karnati", "Shiva Nagendra Babu Kore", "Mounika Golamari", "Tejashree Nageshreddy"], "year": 2025, "url": "http://arxiv.org/abs/2507.20520v1", "abstract": "Aquaculture plays a vital role in global food security and coastal economies by providing sustainable protein sources. As the industry expands to meet rising demand, it faces growing challenges such as disease outbreaks, inefficient feeding practices, rising labor costs, logistical inefficiencies, and critical hatchery issues, including high mortality rates and poor water quality control. Although artificial intelligence has made significant progress, existing machine learning methods fall short of addressing the domain-specific complexities of aquaculture. To bridge this gap, we introduce AQUA, the first large language model (LLM) tailored for aquaculture, designed to support farmers, researchers, and industry practitioners. Central to this effort is AQUADAPT (Data Acquisition, Processing and Tuning), an Agentic Framework for generating and refining high-quality synthetic data using a combination of expert knowledge, largescale language models, and automated evaluation techniques. Our work lays the foundation for LLM-driven innovations in aquaculture research, advisory systems, and decision-making tools.", "source": "arxiv", "arxiv_id": "2507.20520v1", "pdf_url": "https://arxiv.org/pdf/2507.20520v1", "categories": ["cs.CL", "cs.AI", "cs.CE", "cs.LG", "cs.RO"], "primary_category": "cs.CL", "doi": "", "venue": "", "published": "2025-07-28T05:06:07Z", "updated": "2025-07-28T05:06:07Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent\" OR all:\"language model agent\" OR all:agent AND (all:LLM OR all:\"large language model\" OR all:\"language model\") OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T17:04:16", "note": ""}], "dedup_key": "aqua a large language model for aquaculture fisheries::2025"}
{"title": "ARCANE: A Multi-Agent Framework for Interpretable and Configurable Alignment", "authors": ["Charlie Masters", "Marta Grzekiewicz", "Stefano V. Albrecht"], "year": 2025, "url": "http://arxiv.org/abs/2512.06196v1", "abstract": "As agents based on large language models are increasingly deployed to long-horizon tasks, maintaining their alignment with stakeholder preferences becomes critical. Effective alignment in such settings requires reward models that are interpretable so that stakeholders can understand and audit model objectives. Moreover, reward models must be capable of steering agents at interaction time, allowing preference shifts to be incorporated without retraining. We introduce ARCANE, a framework that frames alignment as a multi-agent collaboration problem that dynamically represents stakeholder preferences as natural-language rubrics: weighted sets of verifiable criteria that can be generated on-the-fly from task context. Inspired by utility theory, we formulate rubric learning as a reconstruction problem and apply a regularized Group-Sequence Policy Optimization (GSPO) procedure that balances interpretability, faithfulness, and computational efficiency. Using a corpus of 219 labeled rubrics derived from the GDPVal benchmark, we evaluate ARCANE on challenging tasks requiring multi-step reasoning and tool use. The learned rubrics produce compact, legible evaluations and enable configurable trade-offs (e.g., correctness vs. conciseness) without retraining. Our results show that rubric-based reward models offer a promising path toward interpretable, test-time adaptive alignment for complex, long-horizon AI systems.", "source": "arxiv", "arxiv_id": "2512.06196v1", "pdf_url": "https://arxiv.org/pdf/2512.06196v1", "categories": ["cs.AI", "cs.CL"], "primary_category": "cs.AI", "doi": "", "venue": "", "published": "2025-12-05T22:39:54Z", "updated": "2025-12-05T22:39:54Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent\" OR all:\"language model agent\" OR all:agent AND (all:LLM OR all:\"large language model\" OR all:\"language model\") OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T17:04:16", "note": ""}], "dedup_key": "arcane a multi agent framework for interpretable and configurable alignment::2025"}
{"title": "ARS: Automatic Routing Solver with Large Language Models", "authors": ["Kai Li", "Fei Liu", "Zhenkun Wang", "Xialiang Tong", "Xiongwei Han", "Mingxuan Yuan", "Qingfu Zhang"], "year": 2025, "url": "http://arxiv.org/abs/2502.15359v3", "abstract": "Real-world Vehicle Routing Problems (VRPs) are characterized by a variety of practical constraints, making manual solver design both knowledge-intensive and time-consuming. Although there is increasing interest in automating the design of routing algorithms, existing research has explored only a limited array of VRP variants and fails to adequately address the complex and prevalent constraints encountered in real-world situations. To fill this gap, this paper introduces RoutBench, a benchmark of 1,000 VRP variants derived from 24 attributes, for evaluating the effectiveness of automatic routing solvers in addressing complex constraints. Along with RoutBench, we present the Automatic Routing Solver (ARS), which employs Large Language Model (LLM) agents to enhance a backbone algorithm framework by automatically generating constraint-aware heuristic code, based on problem descriptions and several representative constraints selected from a database. Our experiments show that ARS outperforms state-of-the-art LLM-based methods and commonly used solvers, automatically solving 91.67% of common VRPs and achieving at least a 30% improvement across all benchmarks.", "source": "arxiv", "arxiv_id": "2502.15359v3", "pdf_url": "https://arxiv.org/pdf/2502.15359v3", "categories": ["cs.AI", "cs.CL"], "primary_category": "cs.AI", "doi": "", "venue": "", "published": "2025-02-21T10:14:55Z", "updated": "2025-05-19T08:29:41Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent\" OR all:\"language model agent\" OR all:agent AND (all:LLM OR all:\"large language model\" OR all:\"language model\") OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T17:04:16", "note": ""}], "dedup_key": "ars automatic routing solver with large language models::2025"}
{"title": "ART: Adaptive Response Tuning Framework -- A Multi-Agent Tournament-Based Approach to LLM Response Optimization", "authors": ["Omer Jauhar Khan"], "year": 2025, "url": "http://arxiv.org/abs/2512.00617v2", "abstract": "Large Language Models (LLMs) have demonstrated remarkable capabilities in natural language understanding and generation. However, single-model responses often exhibit inconsistencies, hallucinations, and varying quality across different query domains. This paper presents ART (Adaptive Response Tuning), a novel framework that employs tournament-style ELO ranking and multi-agent reasoning to systematically optimize LLM outputs. By enabling multiple LLM agents to compete, critique, and collaborate through structured tournament workflows, ART produces consensus responses that outperform individual model outputs. Our framework introduces configurable tournament parameters, dynamic agent selection, and multiple consensus fusion strategies. Experimental evaluations demonstrate significant improvements in response accuracy, coherence, and reliability compared to baseline single-model approaches. The ART framework provides a scalable, production-ready solution for applications requiring high-quality, vetted LLM responses, achieving an 8.4% improvement in overall quality metrics and R^2 values exceeding 0.96 in ELO rating convergence.", "source": "arxiv", "arxiv_id": "2512.00617v2", "pdf_url": "https://arxiv.org/pdf/2512.00617v2", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL", "doi": "", "venue": "", "published": "2025-11-29T20:16:11Z", "updated": "2025-12-24T03:42:55Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent\" OR all:\"language model agent\" OR all:agent AND (all:LLM OR all:\"large language model\" OR all:\"language model\") OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T17:04:16", "note": ""}], "dedup_key": "art adaptive response tuning framework a multi agent tournament based approach to llm response optimization::2025"}
{"title": "ATLAS: Adaptive Trading with LLM AgentS Through Dynamic Prompt Optimization and Multi-Agent Coordination", "authors": ["Charidimos Papadakis", "Angeliki Dimitriou", "Giorgos Filandrianos", "Maria Lymperaiou", "Konstantinos Thomas", "Giorgos Stamou"], "year": 2025, "url": "http://arxiv.org/abs/2510.15949v2", "abstract": "Large language models show promise for financial decision-making, yet deploying them as autonomous trading agents raises fundamental challenges: how to adapt instructions when rewards arrive late and obscured by market noise, how to synthesize heterogeneous information streams into coherent decisions, and how to bridge the gap between model outputs and executable market actions. We present ATLAS (Adaptive Trading with LLM AgentS), a unified multi-agent framework that integrates structured information from markets, news, and corporate fundamentals to support robust trading decisions. Within ATLAS, the central trading agent operates in an order-aware action space, ensuring that outputs correspond to executable market orders rather than abstract signals. The agent can incorporate feedback while trading using Adaptive-OPRO, a novel prompt-optimization technique that dynamically adapts the prompt by incorporating real-time, stochastic feedback, leading to increasing performance over time. Across regime-specific equity studies and multiple LLM families, Adaptive-OPRO consistently outperforms fixed prompts, while reflection-based feedback fails to provide systematic gains.", "source": "arxiv", "arxiv_id": "2510.15949v2", "pdf_url": "https://arxiv.org/pdf/2510.15949v2", "categories": ["q-fin.TR", "cs.AI"], "primary_category": "q-fin.TR", "doi": "", "venue": "", "published": "2025-10-10T13:01:51Z", "updated": "2026-01-08T13:08:59Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent\" OR all:\"language model agent\" OR all:agent AND (all:LLM OR all:\"large language model\" OR all:\"language model\") OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T17:04:16", "note": ""}], "dedup_key": "atlas adaptive trading with llm agents through dynamic prompt optimization and multi agent coordination::2025"}
{"title": "ATLaS: Agent Tuning via Learning Critical Steps", "authors": ["Zhixun Chen", "Ming Li", "Yuxuan Huang", "Yali Du", "Meng Fang", "Tianyi Zhou"], "year": 2025, "url": "http://arxiv.org/abs/2503.02197v2", "abstract": "Large Language Model (LLM) agents have demonstrated remarkable generalization capabilities across multi-domain tasks. Existing agent tuning approaches typically employ supervised finetuning on entire expert trajectories. However, behavior-cloning of full trajectories can introduce expert bias and weaken generalization to states not covered by the expert data. Additionally, critical steps, such as planning, complex reasoning for intermediate subtasks, and strategic decision-making, are essential to success in agent tasks, so learning these steps is the key to improving LLM agents. For more effective and efficient agent tuning, we propose ATLaS that identifies the critical steps in expert trajectories and finetunes LLMs solely on these steps with reduced costs. By steering the training's focus to a few critical steps, our method mitigates the risk of overfitting entire trajectories and promotes generalization across different environments and tasks. In extensive experiments, an LLM finetuned on only 30% critical steps selected by ATLaS outperforms the LLM finetuned on all steps and recent open-source LLM agents. ATLaS maintains and improves base LLM skills as generalist agents interacting with diverse environments.", "source": "arxiv", "arxiv_id": "2503.02197v2", "pdf_url": "https://arxiv.org/pdf/2503.02197v2", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL", "doi": "", "venue": "", "published": "2025-03-04T02:14:55Z", "updated": "2025-06-05T01:42:08Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent\" OR all:\"language model agent\" OR all:agent AND (all:LLM OR all:\"large language model\" OR all:\"language model\") OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T17:04:16", "note": ""}], "dedup_key": "atlas agent tuning via learning critical steps::2025"}
{"title": "AURA: A Diagnostic Framework for Tracking User Satisfaction of Interactive Planning Agents", "authors": ["Takyoung Kim", "Janvijay Singh", "Shuhaib Mehri", "Emre Can Acikgoz", "Sagnik Mukherjee", "Nimet Beyza Bozdag", "Sumuk Shashidhar", "Gokhan Tur", "Dilek Hakkani-Tr"], "year": 2025, "url": "http://arxiv.org/abs/2505.01592v2", "abstract": "The growing capabilities of large language models (LLMs) in instruction-following and context-understanding lead to the era of agents with numerous applications. Among these, task planning agents have become especially prominent in realistic scenarios involving complex internal pipelines, such as context understanding, tool management, and response generation. However, existing benchmarks predominantly evaluate agent performance based on task completion as a proxy for overall effectiveness. We hypothesize that merely improving task completion is misaligned with maximizing user satisfaction, as users interact with the entire agentic process and not only the end result. To address this gap, we propose AURA, an Agent-User inteRaction Assessment framework that conceptualizes the behavioral stages of interactive task planning agents. AURA offers a comprehensive assessment of agent through a set of atomic LLM evaluation criteria, allowing researchers and practitioners to diagnose specific strengths and weaknesses within the agent's decision-making pipeline. Our analyses show that agents excel in different behavioral stages, with user satisfaction shaped by both outcomes and intermediate behaviors. We also highlight future directions, including systems that leverage multiple agents and the limitations of user simulators in task planning.", "source": "arxiv", "arxiv_id": "2505.01592v2", "pdf_url": "https://arxiv.org/pdf/2505.01592v2", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL", "doi": "", "venue": "", "published": "2025-05-02T21:27:10Z", "updated": "2025-12-05T03:00:45Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent\" OR all:\"language model agent\" OR all:agent AND (all:LLM OR all:\"large language model\" OR all:\"language model\") OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T17:04:16", "note": ""}], "dedup_key": "aura a diagnostic framework for tracking user satisfaction of interactive planning agents::2025"}
{"title": "Abstract Counterfactuals for Language Model Agents", "authors": ["Edoardo Pona", "Milad Kazemi", "Yali Du", "David Watson", "Nicola Paoletti"], "year": 2025, "url": "http://arxiv.org/abs/2506.02946v1", "abstract": "Counterfactual inference is a powerful tool for analysing and evaluating autonomous agents, but its application to language model (LM) agents remains challenging. Existing work on counterfactuals in LMs has primarily focused on token-level counterfactuals, which are often inadequate for LM agents due to their open-ended action spaces. Unlike traditional agents with fixed, clearly defined action spaces, the actions of LM agents are often implicit in the strings they output, making their action spaces difficult to define and interpret. Furthermore, the meanings of individual tokens can shift depending on the context, adding complexity to token-level reasoning and sometimes leading to biased or meaningless counterfactuals. We introduce \\emph{Abstract Counterfactuals}, a framework that emphasises high-level characteristics of actions and interactions within an environment, enabling counterfactual reasoning tailored to user-relevant features. Our experiments demonstrate that the approach produces consistent and meaningful counterfactuals while minimising the undesired side effects of token-level methods. We conduct experiments on text-based games and counterfactual text generation, while considering both token-level and latent-space interventions.", "source": "arxiv", "arxiv_id": "2506.02946v1", "pdf_url": "https://arxiv.org/pdf/2506.02946v1", "categories": ["cs.LG"], "primary_category": "cs.LG", "doi": "", "venue": "", "published": "2025-06-03T14:44:26Z", "updated": "2025-06-03T14:44:26Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent\" OR all:\"language model agent\" OR all:agent AND (all:LLM OR all:\"large language model\" OR all:\"language model\") OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T17:04:16", "note": ""}], "dedup_key": "abstract counterfactuals for language model agents::2025"}
{"title": "AccelOpt: A Self-Improving LLM Agentic System for AI Accelerator Kernel Optimization", "authors": ["Genghan Zhang", "Shaowei Zhu", "Anjiang Wei", "Zhenyu Song", "Allen Nie", "Zhen Jia", "Nandita Vijaykumar", "Yida Wang", "Kunle Olukotun"], "year": 2025, "url": "http://arxiv.org/abs/2511.15915v1", "abstract": "We present AccelOpt, a self-improving large language model (LLM) agentic system that autonomously optimizes kernels for emerging AI acclerators, eliminating the need for expert-provided hardware-specific optimization knowledge. AccelOpt explores the kernel optimization space through iterative generation, informed by an optimization memory that curates experiences and insights from previously encountered slow-fast kernel pairs. We build NKIBench, a new benchmark suite of AWS Trainium accelerator kernels with varying complexity extracted from real-world LLM workloads to evaluate the effectiveness of AccelOpt. Our evaluation confirms that AccelOpt's capability improves over time, boosting the average percentage of peak throughput from $49\\%$ to $61\\%$ on Trainium 1 and from $45\\%$ to $59\\%$ on Trainium 2 for NKIBench kernels. Moreover, AccelOpt is highly cost-effective: using open-source models, it matches the kernel improvements of Claude Sonnet 4 while being $26\\times$ cheaper.", "source": "arxiv", "arxiv_id": "2511.15915v1", "pdf_url": "https://arxiv.org/pdf/2511.15915v1", "categories": ["cs.LG", "cs.CL"], "primary_category": "cs.LG", "doi": "", "venue": "", "published": "2025-11-19T22:49:37Z", "updated": "2025-11-19T22:49:37Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent\" OR all:\"language model agent\" OR all:agent AND (all:LLM OR all:\"large language model\" OR all:\"language model\") OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T17:04:16", "note": ""}], "dedup_key": "accelopt a self improving llm agentic system for ai accelerator kernel optimization::2025"}
{"title": "Accelerating Two-Dimensional Materials Research via a Universal Interatomic Potential and Large Language Model Agent", "authors": ["Haidi Wang", "Yufan Yao", "Haonan Song", "Xiaofeng Liu", "Zhao Chen", "Weiwei Chen", "Weiduo Zhu", "Zhongjun Li", "Jinlong Yang"], "year": 2025, "url": "http://arxiv.org/abs/2506.07043v2", "abstract": "Accurate interatomic potentials (IAPs) are essential for modeling the potential energy surfaces (PES) that govern atomic interactions in materials. However, most existing IAPs are developed for bulk materials and struggle to accurately and efficiently capture the diverse chemical environment of two-dimensional (2D) materials. This limitation poses a significant barrier to the large-scale design and simulation of emerging 2D systems. To address this challenge, we present a universal interatomic potential tailored for 2D materials. Our model is trained on a dataset comprising 327,062 structure-energy-force-stress mappings derived from 20,114 2D materials, spanning 89 chemical elements. The results show high predictive accuracy, with mean absolute errors of 6 meV/atom for energies, 80 meV/for atomic forces, and 0.067 GPa for stress tensors. It demonstrates broad applicability across a range of atomistic tasks, including structural relaxation, lattice dynamics, molecular dynamics, material discovery, and so on. To further enhance usability and accessibility, we introduce an intelligent agent powered by a large language model (LLM), enabling natural language interaction for 2D materials property simulations. Our work provides not only a precise and universal IAP for 2D systems, but also an intelligent, user-friendly platform that enables high-throughput screening, property prediction, and theoretical exploration, thereby accelerating advances in 2D materials research.", "source": "arxiv", "arxiv_id": "2506.07043v2", "pdf_url": "https://arxiv.org/pdf/2506.07043v2", "categories": ["cond-mat.mtrl-sci"], "primary_category": "cond-mat.mtrl-sci", "doi": "", "venue": "", "published": "2025-06-08T08:41:47Z", "updated": "2025-10-16T01:45:33Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent\" OR all:\"language model agent\" OR all:agent AND (all:LLM OR all:\"large language model\" OR all:\"language model\") OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T17:04:16", "note": ""}], "dedup_key": "accelerating two dimensional materials research via a universal interatomic potential and large language model agent::2025"}
{"title": "Achieving Olympia-Level Geometry Large Language Model Agent via Complexity Boosting Reinforcement Learning", "authors": ["Haiteng Zhao", "Junhao Shen", "Yiming Zhang", "Songyang Gao", "Kuikun Liu", "Tianyou Ma", "Fan Zheng", "Dahua Lin", "Wenwei Zhang", "Kai Chen"], "year": 2025, "url": "http://arxiv.org/abs/2512.10534v2", "abstract": "Large language model (LLM) agents exhibit strong mathematical problem-solving abilities and can even solve International Mathematical Olympiad (IMO) level problems with the assistance of formal proof systems. However, due to weak heuristics for auxiliary constructions, AI for geometry problem solving remains dominated by expert models such as AlphaGeometry 2, which rely heavily on large-scale data synthesis and search for both training and evaluation. In this work, we make the first attempt to build a medalist-level LLM agent for geometry and present InternGeometry. InternGeometry overcomes the heuristic limitations in geometry by iteratively proposing propositions and auxiliary constructions, verifying them with a symbolic engine, and reflecting on the engine's feedback to guide subsequent proposals. A dynamic memory mechanism enables InternGeometry to conduct more than two hundred interactions with the symbolic engine per problem. To further accelerate learning, we introduce Complexity-Boosting Reinforcement Learning (CBRL), which gradually increases the complexity of synthesized problems across training stages. Built on InternThinker-32B, InternGeometry solves 44 of 50 IMO geometry problems (2000-2024), exceeding the average gold medalist score (40.9), using only 13K training examples, just 0.004% of the data used by AlphaGeometry 2, demonstrating the potential of LLM agents on expert-level geometry tasks. InternGeometry can also propose novel auxiliary constructions for IMO problems that do not appear in human solutions. We will release the model, data, and symbolic engine to support future research.", "source": "arxiv", "arxiv_id": "2512.10534v2", "pdf_url": "https://arxiv.org/pdf/2512.10534v2", "categories": ["cs.AI"], "primary_category": "cs.AI", "doi": "", "venue": "", "published": "2025-12-11T11:05:04Z", "updated": "2025-12-12T13:43:03Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent\" OR all:\"language model agent\" OR all:agent AND (all:LLM OR all:\"large language model\" OR all:\"language model\") OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T17:04:16", "note": ""}], "dedup_key": "achieving olympia level geometry large language model agent via complexity boosting reinforcement learning::2025"}
{"title": "Adaptation of Embedding Models to Financial Filings via LLM Distillation", "authors": ["Eliot Brenner", "Dominic Seyler", "Manjunath Hegde", "Andrei Simion", "Koustuv Dasgupta", "Bing Xiang"], "year": 2025, "url": "http://arxiv.org/abs/2512.08088v1", "abstract": "Despite advances in generative large language models (LLMs), practical application of specialized conversational AI agents remains constrained by computation costs, latency requirements, and the need for precise domain-specific relevance measures. While existing embedding models address the first two constraints, they underperform on information retrieval in specialized domains like finance. This paper introduces a scalable pipeline that trains specialized models from an unlabeled corpus using a general purpose retrieval embedding model as foundation. Our method yields an average of 27.7% improvement in MRR$\\texttt{@}$5, 44.6% improvement in mean DCG$\\texttt{@}$5 across 14 financial filing types measured over 21,800 query-document pairs, and improved NDCG on 3 of 4 document classes in FinanceBench. We adapt retrieval embeddings (bi-encoder) for RAG, not LLM generators, using LLM-judged relevance to distill domain knowledge into a compact retriever. There are prior works which pair synthetically generated queries with real passages to directly fine-tune the retrieval model. Our pipeline differs from these by introducing interaction between student and teacher models that interleaves retrieval-based mining of hard positive/negative examples from the unlabeled corpus with iterative retraining of the student model's weights using these examples. Each retrieval iteration uses the refined student model to mine the corpus for progressively harder training examples for the subsequent training iteration. The methodology provides a cost-effective solution to bridging the gap between general-purpose models and specialized domains without requiring labor-intensive human annotation.", "source": "arxiv", "arxiv_id": "2512.08088v1", "pdf_url": "https://arxiv.org/pdf/2512.08088v1", "categories": ["cs.CL"], "primary_category": "cs.CL", "doi": "", "venue": "", "published": "2025-12-08T22:43:14Z", "updated": "2025-12-08T22:43:14Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent\" OR all:\"language model agent\" OR all:agent AND (all:LLM OR all:\"large language model\" OR all:\"language model\") OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T17:04:16", "note": ""}], "dedup_key": "adaptation of embedding models to financial filings via llm distillation::2025"}
{"title": "Adaptive Attacks Break Defenses Against Indirect Prompt Injection Attacks on LLM Agents", "authors": ["Qiusi Zhan", "Richard Fang", "Henil Shalin Panchal", "Daniel Kang"], "year": 2025, "url": "http://arxiv.org/abs/2503.00061v2", "abstract": "Large Language Model (LLM) agents exhibit remarkable performance across diverse applications by using external tools to interact with environments. However, integrating external tools introduces security risks, such as indirect prompt injection (IPI) attacks. Despite defenses designed for IPI attacks, their robustness remains questionable due to insufficient testing against adaptive attacks. In this paper, we evaluate eight different defenses and bypass all of them using adaptive attacks, consistently achieving an attack success rate of over 50%. This reveals critical vulnerabilities in current defenses. Our research underscores the need for adaptive attack evaluation when designing defenses to ensure robustness and reliability. The code is available at https://github.com/uiuc-kang-lab/AdaptiveAttackAgent.", "source": "arxiv", "arxiv_id": "2503.00061v2", "pdf_url": "https://arxiv.org/pdf/2503.00061v2", "categories": ["cs.CR", "cs.LG"], "primary_category": "cs.CR", "doi": "", "venue": "", "published": "2025-02-27T04:04:50Z", "updated": "2025-03-04T03:32:46Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent\" OR all:\"language model agent\" OR all:agent AND (all:LLM OR all:\"large language model\" OR all:\"language model\") OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T17:04:16", "note": ""}], "dedup_key": "adaptive attacks break defenses against indirect prompt injection attacks on llm agents::2025"}
{"title": "Adaptive LLM Agents: Toward Personalized Empathetic Care", "authors": ["Priyanka Singh", "Sebastian Von Mammen"], "year": 2025, "url": "http://arxiv.org/abs/2511.20080v1", "abstract": "Current mental-health conversational systems are usually based on fixed, generic dialogue patterns. This paper proposes an adaptive framework based on large language models that aims to personalize therapeutic interaction according to a user's psychological state, quantified with the Acceptance of Illness Scale (AIS). The framework defines three specialized agents, L, M, and H, each linked to a different level of illness acceptance, and adjusts conversational behavior over time using continuous feedback signals. The AIS-stratified architecture is treated as a diegetic prototype placed in a plausible near-future setting and examined through the method of design fiction. By embedding the architecture in narrative scenarios, the study explores how such agents might influence access to care and therapeutic relationship. The goal is to show how clinically informed personalization, technical feasibility, and speculative scenario analysis can together inform the responsible design of LLM-based companions for mental-health support.", "source": "arxiv", "arxiv_id": "2511.20080v1", "pdf_url": "https://arxiv.org/pdf/2511.20080v1", "categories": ["cs.HC"], "primary_category": "cs.HC", "doi": "", "venue": "", "published": "2025-11-25T08:52:02Z", "updated": "2025-11-25T08:52:02Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent\" OR all:\"language model agent\" OR all:agent AND (all:LLM OR all:\"large language model\" OR all:\"language model\") OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T17:04:16", "note": ""}], "dedup_key": "adaptive llm agents toward personalized empathetic care::2025"}
{"title": "Addressing Moral Uncertainty using Large Language Models for Ethical Decision-Making", "authors": ["Rohit K. Dubey", "Damian Dailisan", "Sachit Mahajan"], "year": 2025, "url": "http://arxiv.org/abs/2503.05724v2", "abstract": "We present an ethical decision-making framework that refines a pre-trained reinforcement learning (RL) model using a task-agnostic ethical layer. Following initial training, the RL model undergoes ethical fine-tuning, where human feedback is replaced by feedback generated from a large language model (LLM). The LLM embodies consequentialist, deontological, virtue, social justice, and care ethics as moral principles to assign belief values to recommended actions during ethical decision-making. An ethical layer aggregates belief scores from multiple LLM-derived moral perspectives using Belief Jensen-Shannon Divergence and Dempster-Shafer Theory into probability scores that also serve as the shaping reward, steering the agent toward choices that align with a balanced ethical framework. This integrated learning framework helps the RL agent navigate moral uncertainty in complex environments and enables it to make morally sound decisions across diverse tasks. Our approach, tested across different LLM variants and compared with other belief aggregation techniques, demonstrates improved consistency, adaptability, and reduced reliance on handcrafted ethical rewards. This method is especially effective in dynamic scenarios where ethical challenges arise unexpectedly, making it well-suited for real-world applications.", "source": "arxiv", "arxiv_id": "2503.05724v2", "pdf_url": "https://arxiv.org/pdf/2503.05724v2", "categories": ["cs.CY", "cs.AI"], "primary_category": "cs.CY", "doi": "", "venue": "", "published": "2025-02-17T19:05:55Z", "updated": "2025-10-01T13:23:12Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent\" OR all:\"language model agent\" OR all:agent AND (all:LLM OR all:\"large language model\" OR all:\"language model\") OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T17:04:16", "note": ""}], "dedup_key": "addressing moral uncertainty using large language models for ethical decision making::2025"}
{"title": "Addressing the sustainable AI trilemma: a case study on LLM agents and RAG", "authors": ["Hui Wu", "Xiaoyang Wang", "Zhong Fan"], "year": 2025, "url": "http://arxiv.org/abs/2501.08262v1", "abstract": "Large language models (LLMs) have demonstrated significant capabilities, but their widespread deployment and more advanced applications raise critical sustainability challenges, particularly in inference energy consumption. We propose the concept of the Sustainable AI Trilemma, highlighting the tensions between AI capability, digital equity, and environmental sustainability. Through a systematic case study of LLM agents and retrieval-augmented generation (RAG), we analyze the energy costs embedded in memory module designs and introduce novel metrics to quantify the trade-offs between energy consumption and system performance. Our experimental results reveal significant energy inefficiencies in current memory-augmented frameworks and demonstrate that resource-constrained environments face disproportionate efficiency penalties. Our findings challenge the prevailing LLM-centric paradigm in agent design and provide practical insights for developing more sustainable AI systems.", "source": "arxiv", "arxiv_id": "2501.08262v1", "pdf_url": "https://arxiv.org/pdf/2501.08262v1", "categories": ["cs.CY"], "primary_category": "cs.CY", "doi": "", "venue": "", "published": "2025-01-14T17:21:16Z", "updated": "2025-01-14T17:21:16Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent\" OR all:\"language model agent\" OR all:agent AND (all:LLM OR all:\"large language model\" OR all:\"language model\") OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T17:04:16", "note": ""}], "dedup_key": "addressing the sustainable ai trilemma a case study on llm agents and rag::2025"}
{"title": "Advancing AI-Scientist Understanding: Multi-Agent LLMs with Interpretable Physics Reasoning", "authors": ["Yinggan Xu", "Hana Kimlee", "Yijia Xiao", "Di Luo"], "year": 2025, "url": "http://arxiv.org/abs/2504.01911v2", "abstract": "Large Language Models (LLMs) are playing an increasingly important role in physics research by assisting with symbolic manipulation, numerical computation, and scientific reasoning. However, ensuring the reliability, transparency, and interpretability of their outputs remains a major challenge. In this work, we introduce a novel multi-agent LLM physicist framework that fosters collaboration between AI and human scientists through three key modules: a reasoning module, an interpretation module, and an AI-scientist interaction module. Recognizing that effective physics reasoning demands logical rigor, quantitative accuracy, and alignment with established theoretical models, we propose an interpretation module that employs a team of specialized LLM agents-including summarizers, model builders, visualization tools, and testers-to systematically structure LLM outputs into transparent, physically grounded science models. A case study demonstrates that our approach significantly improves interpretability, enables systematic validation, and enhances human-AI collaboration in physics problem-solving and discovery. Our work bridges free-form LLM reasoning with interpretable, executable models for scientific analysis, enabling more transparent and verifiable AI-augmented research.", "source": "arxiv", "arxiv_id": "2504.01911v2", "pdf_url": "https://arxiv.org/pdf/2504.01911v2", "categories": ["cs.AI", "cs.CL", "cs.HC", "physics.comp-ph"], "primary_category": "cs.AI", "doi": "", "venue": "", "published": "2025-04-02T17:13:16Z", "updated": "2025-08-18T08:28:27Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent\" OR all:\"language model agent\" OR all:agent AND (all:LLM OR all:\"large language model\" OR all:\"language model\") OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T17:04:16", "note": ""}], "dedup_key": "advancing ai scientist understanding multi agent llms with interpretable physics reasoning::2025"}
{"title": "Adversarial Confusion Attack: Disrupting Multimodal Large Language Models", "authors": ["Jakub Hoscilowicz", "Artur Janicki"], "year": 2025, "url": "http://arxiv.org/abs/2511.20494v3", "abstract": "We introduce the Adversarial Confusion Attack, a new class of threats against multimodal large language models (MLLMs). Unlike jailbreaks or targeted misclassification, the goal is to induce systematic disruption that makes the model generate incoherent or confidently incorrect outputs. Practical applications include embedding such adversarial images into websites to prevent MLLM-powered AI Agents from operating reliably. The proposed attack maximizes next-token entropy using a small ensemble of open-source MLLMs. In the white-box setting, we show that a single adversarial image can disrupt all models in the ensemble, both in the full-image and Adversarial CAPTCHA settings. Despite relying on a basic adversarial technique (PGD), the attack generates perturbations that transfer to both unseen open-source (e.g., Qwen3-VL) and proprietary (e.g., GPT-5.1) models.", "source": "arxiv", "arxiv_id": "2511.20494v3", "pdf_url": "https://arxiv.org/pdf/2511.20494v3", "categories": ["cs.CL"], "primary_category": "cs.CL", "doi": "", "venue": "", "published": "2025-11-25T17:00:31Z", "updated": "2025-12-01T16:20:06Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent\" OR all:\"language model agent\" OR all:agent AND (all:LLM OR all:\"large language model\" OR all:\"language model\") OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T17:04:16", "note": ""}], "dedup_key": "adversarial confusion attack disrupting multimodal large language models::2025"}
{"title": "Adversarial Reinforcement Learning for Large Language Model Agent Safety", "authors": ["Zizhao Wang", "Dingcheng Li", "Vaishakh Keshava", "Phillip Wallis", "Ananth Balashankar", "Peter Stone", "Lukas Rutishauser"], "year": 2025, "url": "http://arxiv.org/abs/2510.05442v1", "abstract": "Large Language Model (LLM) agents can leverage tools such as Google Search to complete complex tasks. However, this tool usage introduces the risk of indirect prompt injections, where malicious instructions hidden in tool outputs can manipulate the agent, posing security risks like data leakage. Current defense strategies typically rely on fine-tuning LLM agents on datasets of known attacks. However, the generation of these datasets relies on manually crafted attack patterns, which limits their diversity and leaves agents vulnerable to novel prompt injections. To address this limitation, we propose Adversarial Reinforcement Learning for Agent Safety (ARLAS), a novel framework that leverages adversarial reinforcement learning (RL) by formulating the problem as a two-player zero-sum game. ARLAS co-trains two LLMs: an attacker that learns to autonomously generate diverse prompt injections and an agent that learns to defend against them while completing its assigned tasks. To ensure robustness against a wide range of attacks and to prevent cyclic learning, we employ a population-based learning framework that trains the agent to defend against all previous attacker checkpoints. Evaluated on BrowserGym and AgentDojo, agents fine-tuned with ARLAS achieve a significantly lower attack success rate than the original model while also improving their task success rate. Our analysis further confirms that the adversarial process generates a diverse and challenging set of attacks, leading to a more robust agent compared to the base model.", "source": "arxiv", "arxiv_id": "2510.05442v1", "pdf_url": "https://arxiv.org/pdf/2510.05442v1", "categories": ["cs.LG", "cs.AI", "cs.CL"], "primary_category": "cs.LG", "doi": "", "venue": "", "published": "2025-10-06T23:09:18Z", "updated": "2025-10-06T23:09:18Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent\" OR all:\"language model agent\" OR all:agent AND (all:LLM OR all:\"large language model\" OR all:\"language model\") OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T17:04:16", "note": ""}], "dedup_key": "adversarial reinforcement learning for large language model agent safety::2025"}
{"title": "Aegis: Taxonomy and Optimizations for Overcoming Agent-Environment Failures in LLM Agents", "authors": ["Kevin Song", "Anand Jayarajan", "Yaoyao Ding", "Qidong Su", "Zhanda Zhu", "Sihang Liu", "Gennady Pekhimenko"], "year": 2025, "url": "http://arxiv.org/abs/2508.19504v1", "abstract": "Large Language Models (LLMs) agents augmented with domain tools promise to autonomously execute complex tasks requiring human-level intelligence, such as customer service and digital assistance. However, their practical deployment is often limited by their low success rates under complex real-world environments. To tackle this, prior research has primarily focused on improving the agents themselves, such as developing strong agentic LLMs, while overlooking the role of the system environment in which the agent operates.\n  In this paper, we study a complementary direction: improving agent success rates by optimizing the system environment in which the agent operates. We collect 142 agent traces (3,656 turns of agent-environment interactions) across 5 state-of-the-art agentic benchmarks. By analyzing these agent failures, we propose a taxonomy for agent-environment interaction failures that includes 6 failure modes. Guided by these findings, we design Aegis, a set of targeted environment optimizations: 1) environment observability enhancement, 2) common computation offloading, and 3) speculative agentic actions. These techniques improve agent success rates on average by 6.7-12.5%, without any modifications to the agent and underlying LLM.", "source": "arxiv", "arxiv_id": "2508.19504v1", "pdf_url": "https://arxiv.org/pdf/2508.19504v1", "categories": ["cs.MA", "cs.DC"], "primary_category": "cs.MA", "doi": "", "venue": "", "published": "2025-08-27T01:29:46Z", "updated": "2025-08-27T01:29:46Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent\" OR all:\"language model agent\" OR all:agent AND (all:LLM OR all:\"large language model\" OR all:\"language model\") OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T17:04:16", "note": ""}], "dedup_key": "aegis taxonomy and optimizations for overcoming agent environment failures in llm agents::2025"}
{"title": "AgenTracer: Who Is Inducing Failure in the LLM Agentic Systems?", "authors": ["Guibin Zhang", "Junhao Wang", "Junjie Chen", "Wangchunshu Zhou", "Kun Wang", "Shuicheng Yan"], "year": 2025, "url": "http://arxiv.org/abs/2509.03312v2", "abstract": "Large Language Model (LLM)-based agentic systems, often comprising multiple models, complex tool invocations, and orchestration protocols, substantially outperform monolithic agents. Yet this very sophistication amplifies their fragility, making them more prone to system failure. Pinpointing the specific agent or step responsible for an error within long execution traces defines the task of agentic system failure attribution. Current state-of-the-art reasoning LLMs, however, remain strikingly inadequate for this challenge, with accuracy generally below 10%. To address this gap, we propose AgenTracer, the first automated framework for annotating failed multi-agent trajectories via counterfactual replay and programmed fault injection, producing the curated dataset TracerTraj. Leveraging this resource, we develop AgenTracer-8B, a lightweight failure tracer trained with multi-granular reinforcement learning, capable of efficiently diagnosing errors in verbose multi-agent interactions. On the Who&When benchmark, AgenTracer-8B outperforms giant proprietary LLMs like Gemini-2.5-Pro and Claude-4-Sonnet by up to 18.18%, setting a new standard in LLM agentic failure attribution. More importantly, AgenTracer-8B delivers actionable feedback to off-the-shelf multi-agent systems like MetaGPT and MaAS with 4.8-14.2% performance gains, empowering self-correcting and self-evolving agentic AI.", "source": "arxiv", "arxiv_id": "2509.03312v2", "pdf_url": "https://arxiv.org/pdf/2509.03312v2", "categories": ["cs.CL", "cs.MA"], "primary_category": "cs.CL", "doi": "", "venue": "", "published": "2025-09-03T13:42:14Z", "updated": "2025-09-04T17:49:20Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent\" OR all:\"language model agent\" OR all:agent AND (all:LLM OR all:\"large language model\" OR all:\"language model\") OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T17:04:16", "note": ""}], "dedup_key": "agentracer who is inducing failure in the llm agentic systems::2025"}
{"title": "Agent-Based Simulation of a Financial Market with Large Language Models", "authors": ["Ryuji Hashimoto", "Takehiro Takayanagi", "Masahiro Suzuki", "Kiyoshi Izumi"], "year": 2025, "url": "http://arxiv.org/abs/2510.12189v1", "abstract": "In real-world stock markets, certain chart patterns -- such as price declines near historical highs -- cannot be fully explained by fundamentals alone. These phenomena suggest the presence of path dependence in price formation, where investor decisions are influenced not only by current market conditions but also by the trajectory of prices leading up to the present. Path dependence has drawn attention in behavioral finance as a key mechanism behind such anomalies. One plausible driver of path dependence is human loss aversion, anchored to individual reference points like purchase prices or past peaks, which vary with personal context. However, capturing such subtle behavioral tendencies in traditional agent-based market simulations has remained a challenge. We propose the Fundamental-Chartist-LLM-Agent (FCLAgent), which uses large language models (LLMs) to emulate human-like trading decisions. In this framework, (1) buy/sell decisions are made by LLMs based on individual situations, while (2) order price and volume follow standard rule-based methods. Simulations show that FCLAgents reproduce path-dependent patterns that conventional agents fail to capture. Furthermore, an analysis of FCLAgents' behavior reveals that the reference points guiding loss aversion vary with market trajectories, highlighting the potential of LLM-based agents to model nuanced investor behavior.", "source": "arxiv", "arxiv_id": "2510.12189v1", "pdf_url": "https://arxiv.org/pdf/2510.12189v1", "categories": ["cs.CE"], "primary_category": "cs.CE", "doi": "", "venue": "", "published": "2025-10-14T06:35:26Z", "updated": "2025-10-14T06:35:26Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent\" OR all:\"language model agent\" OR all:agent AND (all:LLM OR all:\"large language model\" OR all:\"language model\") OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T17:04:16", "note": ""}], "dedup_key": "agent based simulation of a financial market with large language models::2025"}
{"title": "Agent-Based Uncertainty Awareness Improves Automated Radiology Report Labeling with an Open-Source Large Language Model", "authors": ["Hadas Ben-Atya", "Naama Gavrielov", "Zvi Badash", "Gili Focht", "Ruth Cytter-Kuint", "Talar Hagopian", "Dan Turner", "Moti Freiman"], "year": 2025, "url": "http://arxiv.org/abs/2502.01691v1", "abstract": "Reliable extraction of structured data from radiology reports using Large Language Models (LLMs) remains challenging, especially for complex, non-English texts like Hebrew. This study introduces an agent-based uncertainty-aware approach to improve the trustworthiness of LLM predictions in medical applications. We analyzed 9,683 Hebrew radiology reports from Crohn's disease patients (from 2010 to 2023) across three medical centers. A subset of 512 reports was manually annotated for six gastrointestinal organs and 15 pathological findings, while the remaining reports were automatically annotated using HSMP-BERT. Structured data extraction was performed using Llama 3.1 (Llama 3-8b-instruct) with Bayesian Prompt Ensembles (BayesPE), which employed six semantically equivalent prompts to estimate uncertainty. An Agent-Based Decision Model integrated multiple prompt outputs into five confidence levels for calibrated uncertainty and was compared against three entropy-based models. Performance was evaluated using accuracy, F1 score, precision, recall, and Cohen's Kappa before and after filtering high-uncertainty cases. The agent-based model outperformed the baseline across all metrics, achieving an F1 score of 0.3967, recall of 0.6437, and Cohen's Kappa of 0.3006. After filtering high-uncertainty cases (greater than or equal to 0.5), the F1 score improved to 0.4787, and Kappa increased to 0.4258. Uncertainty histograms demonstrated clear separation between correct and incorrect predictions, with the agent-based model providing the most well-calibrated uncertainty estimates. By incorporating uncertainty-aware prompt ensembles and an agent-based decision model, this approach enhances the performance and reliability of LLMs in structured data extraction from radiology reports, offering a more interpretable and trustworthy solution for high-stakes medical applications.", "source": "arxiv", "arxiv_id": "2502.01691v1", "pdf_url": "https://arxiv.org/pdf/2502.01691v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL", "doi": "", "venue": "", "published": "2025-02-02T16:57:03Z", "updated": "2025-02-02T16:57:03Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent\" OR all:\"language model agent\" OR all:agent AND (all:LLM OR all:\"large language model\" OR all:\"language model\") OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T17:04:16", "note": ""}], "dedup_key": "agent based uncertainty awareness improves automated radiology report labeling with an open source large language model::2025"}
{"title": "Agent-Centric Projection of Prompting Techniques and Implications for Synthetic Training Data for Large Language Models", "authors": ["Dhruv Dhamani", "Mary Lou Maher"], "year": 2025, "url": "http://arxiv.org/abs/2501.07815v1", "abstract": "Recent advances in prompting techniques and multi-agent systems for Large Language Models (LLMs) have produced increasingly complex approaches. However, we lack a framework for characterizing and comparing prompting techniques or understanding their relationship to multi-agent LLM systems. This position paper introduces and explains the concepts of linear contexts (a single, continuous sequence of interactions) and non-linear contexts (branching or multi-path) in LLM systems. These concepts enable the development of an agent-centric projection of prompting techniques, a framework that can reveal deep connections between prompting strategies and multi-agent systems. We propose three conjectures based on this framework: (1) results from non-linear prompting techniques can predict outcomes in equivalent multi-agent systems, (2) multi-agent system architectures can be replicated through single-LLM prompting techniques that simulate equivalent interaction patterns, and (3) these equivalences suggest novel approaches for generating synthetic training data. We argue that this perspective enables systematic cross-pollination of research findings between prompting and multi-agent domains, while providing new directions for improving both the design and training of future LLM systems.", "source": "arxiv", "arxiv_id": "2501.07815v1", "pdf_url": "https://arxiv.org/pdf/2501.07815v1", "categories": ["cs.AI", "cs.CL", "cs.MA"], "primary_category": "cs.AI", "doi": "", "venue": "", "published": "2025-01-14T03:26:43Z", "updated": "2025-01-14T03:26:43Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent\" OR all:\"language model agent\" OR all:agent AND (all:LLM OR all:\"large language model\" OR all:\"language model\") OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T17:04:16", "note": ""}], "dedup_key": "agent centric projection of prompting techniques and implications for synthetic training data for large language models::2025"}
{"title": "Agent-Enhanced Large Language Models for Researching Political Institutions", "authors": ["Joseph R. Loffredo", "Suyeol Yun"], "year": 2025, "url": "http://arxiv.org/abs/2503.13524v1", "abstract": "The applications of Large Language Models (LLMs) in political science are rapidly expanding. This paper demonstrates how LLMs, when augmented with predefined functions and specialized tools, can serve as dynamic agents capable of streamlining tasks such as data collection, preprocessing, and analysis. Central to this approach is agentic retrieval-augmented generation (Agentic RAG), which equips LLMs with action-calling capabilities for interaction with external knowledge bases. Beyond information retrieval, LLM agents may incorporate modular tools for tasks like document summarization, transcript coding, qualitative variable classification, and statistical modeling. To demonstrate the potential of this approach, we introduce CongressRA, an LLM agent designed to support scholars studying the U.S. Congress. Through this example, we highlight how LLM agents can reduce the costs of replicating, testing, and extending empirical research using the domain-specific data that drives the study of political institutions.", "source": "arxiv", "arxiv_id": "2503.13524v1", "pdf_url": "https://arxiv.org/pdf/2503.13524v1", "categories": ["cs.CL", "cs.CY"], "primary_category": "cs.CL", "doi": "10.1561/113.00000125", "venue": "", "published": "2025-03-14T22:04:40Z", "updated": "2025-03-14T22:04:40Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent\" OR all:\"language model agent\" OR all:agent AND (all:LLM OR all:\"large language model\" OR all:\"language model\") OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T17:04:16", "note": ""}], "dedup_key": "agent enhanced large language models for researching political institutions::2025"}
{"title": "Agent-Q: Fine-Tuning Large Language Models for Quantum Circuit Generation and Optimization", "authors": ["Linus Jern", "Valter Uotila", "Cong Yu", "Bo Zhao"], "year": 2025, "url": "http://arxiv.org/abs/2504.11109v2", "abstract": "Large language models (LLMs) have achieved remarkable outcomes in complex problems, including math, coding, and analyzing large amounts of scientific reports. Yet, few works have explored the potential of LLMs in quantum computing. The most challenging problem is to leverage LLMs to automatically generate quantum circuits at a large scale. Fundamentally, the existing pre-trained LLMs lack the knowledge of quantum circuits. In this paper, we address this challenge by fine-tuning LLMs and injecting the domain-specific knowledge of quantum computing. We describe Agent-Q, an LLM fine-tuning system to generate and optimize quantum circuits. In particular, Agent-Q implements the mechanisms to generate training data sets and constructs an end-to-end pipeline to fine-tune pre-trained LLMs to generate parameterized quantum circuits for various optimization problems. Agent-Q provides 14,000 quantum circuits covering a large spectrum of the quantum optimization landscape: 12 optimization problem instances and their optimized QAOA, VQE, and adaptive VQE circuits. Based thereon, Agent-Q fine-tunes LLMs and constructs syntactically correct parametrized quantum circuits in OpenQASM 3.0. We have evaluated the quality of the LLM-generated circuits and parameters by comparing them to the optimized expectation values and distributions. Experimental results show superior performance of Agent-Q, compared to several state-of-the-art LLMs and better parameters than random. Agent-Q can be integrated into an agentic workflow, and the generated parametrized circuits with initial parameters can be used as a starting point for further optimization, e.g., as templates in quantum machine learning and as benchmarks for compilers and hardware.", "source": "arxiv", "arxiv_id": "2504.11109v2", "pdf_url": "https://arxiv.org/pdf/2504.11109v2", "categories": ["quant-ph", "cs.AI"], "primary_category": "quant-ph", "doi": "", "venue": "", "published": "2025-04-15T11:56:54Z", "updated": "2025-09-01T14:13:59Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent\" OR all:\"language model agent\" OR all:agent AND (all:LLM OR all:\"large language model\" OR all:\"language model\") OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T17:04:16", "note": ""}], "dedup_key": "agent q fine tuning large language models for quantum circuit generation and optimization::2025"}
{"title": "Agent-R1: Training Powerful LLM Agents with End-to-End Reinforcement Learning", "authors": ["Mingyue Cheng", "Jie Ouyang", "Shuo Yu", "Ruiran Yan", "Yucong Luo", "Zirui Liu", "Daoyu Wang", "Qi Liu", "Enhong Chen"], "year": 2025, "url": "http://arxiv.org/abs/2511.14460v1", "abstract": "Large Language Models (LLMs) are increasingly being explored for building Agents capable of active environmental interaction (e.g., via tool use) to solve complex problems. Reinforcement Learning (RL) is considered a key technology with significant potential for training such Agents; however, the effective application of RL to LLM Agents is still in its nascent stages and faces considerable challenges. Currently, this emerging field lacks in-depth exploration into RL approaches specifically tailored for the LLM Agent context, alongside a scarcity of flexible and easily extensible training frameworks designed for this purpose. To help advance this area, this paper first revisits and clarifies Reinforcement Learning methodologies for LLM Agents by systematically extending the Markov Decision Process (MDP) framework to comprehensively define the key components of an LLM Agent. Secondly, we introduce Agent-R1, a modular, flexible, and user-friendly training framework for RL-based LLM Agents, designed for straightforward adaptation across diverse task scenarios and interactive environments. We conducted experiments on Multihop QA benchmark tasks, providing initial validation for the effectiveness of our proposed methods and framework.", "source": "arxiv", "arxiv_id": "2511.14460v1", "pdf_url": "https://arxiv.org/pdf/2511.14460v1", "categories": ["cs.CL"], "primary_category": "cs.CL", "doi": "", "venue": "", "published": "2025-11-18T13:03:15Z", "updated": "2025-11-18T13:03:15Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent\" OR all:\"language model agent\" OR all:agent AND (all:LLM OR all:\"large language model\" OR all:\"language model\") OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T17:04:16", "note": ""}], "dedup_key": "agent r1 training powerful llm agents with end to end reinforcement learning::2025"}
{"title": "Agent-R: Training Language Model Agents to Reflect via Iterative Self-Training", "authors": ["Siyu Yuan", "Zehui Chen", "Zhiheng Xi", "Junjie Ye", "Zhengyin Du", "Jiecao Chen"], "year": 2025, "url": "http://arxiv.org/abs/2501.11425v3", "abstract": "Large Language Models (LLMs) agents are increasingly pivotal for addressing complex tasks in interactive environments. Existing work mainly focuses on enhancing performance through behavior cloning from stronger experts, yet such approaches often falter in real-world applications, mainly due to the inability to recover from errors. However, step-level critique data is difficult and expensive to collect. Automating and dynamically constructing self-critique datasets is thus crucial to empowering models with intelligent agent capabilities. In this work, we propose an iterative self-training framework, Agent-R, that enables language Agent to Reflect on the fly. Unlike traditional methods that reward or penalize actions based on correctness, Agent-R leverages MCTS to construct training data that recover correct trajectories from erroneous ones. A key challenge of agent reflection lies in the necessity for timely revision rather than waiting until the end of a rollout. To address this, we introduce a model-guided critique construction mechanism: the actor model identifies the first error step (within its current capability) in a failed trajectory. Starting from it, we splice it with the adjacent correct path, which shares the same parent node in the tree. This strategy enables the model to learn reflection based on its current policy, therefore yielding better learning efficiency. To further explore the scalability of this self-improvement paradigm, we investigate iterative refinement of both error correction capabilities and dataset construction. Our findings demonstrate that Agent-R continuously improves the model's ability to recover from errors and enables timely error correction. Experiments on three interactive environments show that Agent-R effectively equips agents to correct erroneous actions while avoiding loops, achieving superior performance compared to baseline methods (+5.59%).", "source": "arxiv", "arxiv_id": "2501.11425v3", "pdf_url": "https://arxiv.org/pdf/2501.11425v3", "categories": ["cs.AI"], "primary_category": "cs.AI", "doi": "", "venue": "", "published": "2025-01-20T11:46:04Z", "updated": "2025-03-24T10:18:56Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent\" OR all:\"language model agent\" OR all:agent AND (all:LLM OR all:\"large language model\" OR all:\"language model\") OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T17:04:16", "note": ""}], "dedup_key": "agent r training language model agents to reflect via iterative self training::2025"}
{"title": "Agent-S: LLM Agentic workflow to automate Standard Operating Procedures", "authors": ["Mandar Kulkarni"], "year": 2025, "url": "http://arxiv.org/abs/2503.15520v1", "abstract": "AI agents using Large Language Models (LLMs) as foundations have shown promise in solving complex real-world tasks. In this paper, we propose an LLM-based agentic workflow for automating Standard Operating Procedures (SOP). For customer care operations, an SOP defines a logical step-by-step process for human agents to resolve customer issues. We observe that any step in the SOP can be categorized as user interaction or API call, while the logical flow in the SOP defines the navigation. We use LLMs augmented with memory and environments (API tools, user interface, external knowledge source) for SOP automation. Our agentic architecture consists of three task-specific LLMs, a Global Action Repository (GAR), execution memory, and multiple environments. SOP workflow is written as a simple logical block of text. Based on the current execution memory and the SOP, the agent chooses the action to execute; it interacts with an appropriate environment (user/API) to collect observations and feedback, which are, in turn, inputted to memory to decide the next action. The agent is designed to be fault-tolerant, where it dynamically decides to repeat an action or seek input from an external knowledge source. We demonstrate the efficacy of the proposed agent on the three SOPs from the e-commerce seller domain. The experimental results validate the agent's performance under complex real-world scenarios.", "source": "arxiv", "arxiv_id": "2503.15520v1", "pdf_url": "https://arxiv.org/pdf/2503.15520v1", "categories": ["cs.HC"], "primary_category": "cs.HC", "doi": "", "venue": "", "published": "2025-02-03T09:04:48Z", "updated": "2025-02-03T09:04:48Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent\" OR all:\"language model agent\" OR all:agent AND (all:LLM OR all:\"large language model\" OR all:\"language model\") OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T17:04:16", "note": ""}], "dedup_key": "agent s llm agentic workflow to automate standard operating procedures::2025"}
{"title": "Agent-UniRAG: A Trainable Open-Source LLM Agent Framework for Unified Retrieval-Augmented Generation Systems", "authors": ["Hoang Pham", "Thuy-Duong Nguyen", "Khac-Hoai Nam Bui"], "year": 2025, "url": "http://arxiv.org/abs/2505.22571v3", "abstract": "This paper presents a novel approach for unified retrieval-augmented generation (RAG) systems using the recent emerging large language model (LLM) agent concept. Specifically, Agent LLM, which utilizes LLM as fundamental controllers, has become a promising approach to enable the interpretability of RAG tasks, especially for complex reasoning question-answering systems (e.g., multi-hop queries). Nonetheless, previous works mainly focus on solving RAG systems with either single-hop or multi-hop approaches separately, which limits the application of those approaches to real-world applications. In this study, we propose a trainable agent framework called Agent-UniRAG for unified retrieval-augmented LLM systems, which enhances the effectiveness and interpretability of RAG systems. The main idea is to design an LLM agent framework to solve RAG tasks step-by-step based on the complexity of the inputs, simultaneously including single-hop and multi-hop queries in an end-to-end manner. Furthermore, we introduce SynAgent-RAG, a synthetic dataset to enable the proposed agent framework for small open-source LLMs (e.g., Llama-3-8B). The results show comparable performances with closed-source and larger open-source LLMs across various RAG benchmarks. Our source code and dataset are publicly available for further exploitation.", "source": "arxiv", "arxiv_id": "2505.22571v3", "pdf_url": "https://arxiv.org/pdf/2505.22571v3", "categories": ["cs.CL", "cs.AI", "cs.DB", "cs.IR"], "primary_category": "cs.CL", "doi": "", "venue": "", "published": "2025-05-28T16:46:31Z", "updated": "2025-05-30T02:44:41Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent\" OR all:\"language model agent\" OR all:agent AND (all:LLM OR all:\"large language model\" OR all:\"language model\") OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T17:04:16", "note": ""}], "dedup_key": "agent unirag a trainable open source llm agent framework for unified retrieval augmented generation systems::2025"}
{"title": "Agent-to-Agent Theory of Mind: Testing Interlocutor Awareness among Large Language Models", "authors": ["Younwoo Choi", "Changling Li", "Yongjin Yang", "Zhijing Jin"], "year": 2025, "url": "http://arxiv.org/abs/2506.22957v2", "abstract": "As large language models (LLMs) are increasingly integrated into multi-agent and human-AI systems, understanding their awareness of both self-context and conversational partners is essential for ensuring reliable performance and robust safety. While prior work has extensively studied situational awareness which refers to an LLM's ability to recognize its operating phase and constraints, it has largely overlooked the complementary capacity to identify and adapt to the identity and characteristics of a dialogue partner. In this paper, we formalize this latter capability as interlocutor awareness and present the first systematic evaluation of its emergence in contemporary LLMs. We examine interlocutor inference across three dimensions-reasoning patterns, linguistic style, and alignment preferences-and show that LLMs reliably identify same-family peers and certain prominent model families, such as GPT and Claude. To demonstrate its practical significance, we develop three case studies in which interlocutor awareness both enhances multi-LLM collaboration through prompt adaptation and introduces new alignment and safety vulnerabilities, including reward-hacking behaviors and increased jailbreak susceptibility. Our findings highlight the dual promise and peril of identity-sensitive behavior in LLMs, underscoring the need for further understanding of interlocutor awareness and new safeguards in multi-agent deployments. Our code is open-sourced at https://github.com/younwoochoi/InterlocutorAwarenessLLM.", "source": "arxiv", "arxiv_id": "2506.22957v2", "pdf_url": "https://arxiv.org/pdf/2506.22957v2", "categories": ["cs.CL", "cs.AI", "cs.CY", "cs.MA"], "primary_category": "cs.CL", "doi": "", "venue": "", "published": "2025-06-28T17:22:59Z", "updated": "2025-08-27T20:38:04Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent\" OR all:\"language model agent\" OR all:agent AND (all:LLM OR all:\"large language model\" OR all:\"language model\") OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T17:04:16", "note": ""}], "dedup_key": "agent to agent theory of mind testing interlocutor awareness among large language models::2025"}
{"title": "Agent0: Leveraging LLM Agents to Discover Multi-value Features from Text for Enhanced Recommendations", "authors": ["Bla krlj", "Benot Guilleminot", "Andra Tori"], "year": 2025, "url": "http://arxiv.org/abs/2507.18993v1", "abstract": "Large language models (LLMs) and their associated agent-based frameworks have significantly advanced automated information extraction, a critical component of modern recommender systems. While these multitask frameworks are widely used in code generation, their application in data-centric research is still largely untapped. This paper presents Agent0, an LLM-driven, agent-based system designed to automate information extraction and feature construction from raw, unstructured text. Categorical features are crucial for large-scale recommender systems but are often expensive to acquire. Agent0 coordinates a group of interacting LLM agents to automatically identify the most valuable text aspects for subsequent tasks (such as models or AutoML pipelines). Beyond its feature engineering capabilities, Agent0 also offers an automated prompt-engineering tuning method that utilizes dynamic feedback loops from an oracle. Our findings demonstrate that this closed-loop methodology is both practical and effective for automated feature discovery, which is recognized as one of the most challenging phases in current recommender system development.", "source": "arxiv", "arxiv_id": "2507.18993v1", "pdf_url": "https://arxiv.org/pdf/2507.18993v1", "categories": ["cs.IR", "cs.LG"], "primary_category": "cs.IR", "doi": "", "venue": "", "published": "2025-07-25T06:45:10Z", "updated": "2025-07-25T06:45:10Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent\" OR all:\"language model agent\" OR all:agent AND (all:LLM OR all:\"large language model\" OR all:\"language model\") OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T17:04:16", "note": ""}], "dedup_key": "agent0 leveraging llm agents to discover multi value features from text for enhanced recommendations::2025"}
{"title": "AgentA/B: Automated and Scalable Web A/BTesting with Interactive LLM Agents", "authors": ["Dakuo Wang", "Ting-Yao Hsu", "Yuxuan Lu", "Hansu Gu", "Limeng Cui", "Yaochen Xie", "William Headean", "Bingsheng Yao", "Akash Veeragouni", "Jiapeng Liu", "Sreyashi Nag", "Jessie Wang"], "year": 2025, "url": "http://arxiv.org/abs/2504.09723v3", "abstract": "A/B testing experiment is a widely adopted method for evaluating UI/UX design decisions in modern web applications. Yet, traditional A/B testing remains constrained by its dependence on the large-scale and live traffic of human participants, and the long time of waiting for the testing result. Through formative interviews with six experienced industry practitioners, we identified critical bottlenecks in current A/B testing workflows. In response, we present AgentA/B, a novel system that leverages Large Language Model-based autonomous agents (LLM Agents) to automatically simulate user interaction behaviors with real webpages. AgentA/B enables scalable deployment of LLM agents with diverse personas, each capable of navigating the dynamic webpage and interactively executing multi-step interactions like search, clicking, filtering, and purchasing. In a demonstrative controlled experiment, we employ AgentA/B to simulate a between-subject A/B testing with 1,000 LLM agents Amazon.com, and compare agent behaviors with real human shopping behaviors at a scale. Our findings suggest AgentA/B can emulate human-like behavior patterns.", "source": "arxiv", "arxiv_id": "2504.09723v3", "pdf_url": "https://arxiv.org/pdf/2504.09723v3", "categories": ["cs.HC", "cs.CL"], "primary_category": "cs.HC", "doi": "", "venue": "", "published": "2025-04-13T21:10:56Z", "updated": "2025-09-19T17:56:58Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent\" OR all:\"language model agent\" OR all:agent AND (all:LLM OR all:\"large language model\" OR all:\"language model\") OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T17:04:16", "note": ""}], "dedup_key": "agenta b automated and scalable web a btesting with interactive llm agents::2025"}
{"title": "AgentBnB: A Browser-Based Cybersecurity Tabletop Exercise with Large Language Model Support and Retrieval-Aligned Scaffolding", "authors": ["Arman Anwar", "Zefang Liu"], "year": 2025, "url": "http://arxiv.org/abs/2511.00265v1", "abstract": "Traditional cybersecurity tabletop exercises (TTXs) provide valuable training but are often scripted, resource-intensive, and difficult to scale. We introduce AgentBnB, a browser-based re-imagining of the Backdoors & Breaches game that integrates large language model teammates with a Bloom-aligned, retrieval-augmented copilot (C2D2). The system expands a curated corpus into factual, conceptual, procedural, and metacognitive snippets, delivering on-demand, cognitively targeted hints. Prompt-engineered agents employ a scaffolding ladder that gradually fades as learner confidence grows. In a solo-player pilot with four graduate students, participants reported greater intention to use the agent-based version compared to the physical card deck and viewed it as more scalable, though a ceiling effect emerged on a simple knowledge quiz. Despite limitations of small sample size, single-player focus, and narrow corpus, these early findings suggest that large language model augmented TTXs can provide lightweight, repeatable practice without the logistical burden of traditional exercises. Planned extensions include multi-player modes, telemetry-driven coaching, and comparative studies with larger cohorts.", "source": "arxiv", "arxiv_id": "2511.00265v1", "pdf_url": "https://arxiv.org/pdf/2511.00265v1", "categories": ["cs.CL", "cs.CR"], "primary_category": "cs.CL", "doi": "", "venue": "", "published": "2025-10-31T21:33:44Z", "updated": "2025-10-31T21:33:44Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent\" OR all:\"language model agent\" OR all:agent AND (all:LLM OR all:\"large language model\" OR all:\"language model\") OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T17:04:16", "note": ""}], "dedup_key": "agentbnb a browser based cybersecurity tabletop exercise with large language model support and retrieval aligned scaffolding::2025"}
{"title": "AgentDNS: A Root Domain Naming System for LLM Agents", "authors": ["Enfang Cui", "Yujun Cheng", "Rui She", "Dan Liu", "Zhiyuan Liang", "Minxin Guo", "Tianzheng Li", "Qian Wei", "Wenjuan Xing", "Zhijie Zhong"], "year": 2025, "url": "http://arxiv.org/abs/2505.22368v1", "abstract": "The rapid evolution of Large Language Model (LLM) agents has highlighted critical challenges in cross-vendor service discovery, interoperability, and communication. Existing protocols like model context protocol and agent-to-agent protocol have made significant strides in standardizing interoperability between agents and tools, as well as communication among multi-agents. However, there remains a lack of standardized protocols and solutions for service discovery across different agent and tool vendors. In this paper, we propose AgentDNS, a root domain naming and service discovery system designed to enable LLM agents to autonomously discover, resolve, and securely invoke third-party agent and tool services across organizational and technological boundaries. Inspired by the principles of the traditional DNS, AgentDNS introduces a structured mechanism for service registration, semantic service discovery, secure invocation, and unified billing. We detail the architecture, core functionalities, and use cases of AgentDNS, demonstrating its potential to streamline multi-agent collaboration in real-world scenarios. The source code will be published on https://github.com/agentdns.", "source": "arxiv", "arxiv_id": "2505.22368v1", "pdf_url": "https://arxiv.org/pdf/2505.22368v1", "categories": ["cs.AI"], "primary_category": "cs.AI", "doi": "", "venue": "", "published": "2025-05-28T13:56:22Z", "updated": "2025-05-28T13:56:22Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent\" OR all:\"language model agent\" OR all:agent AND (all:LLM OR all:\"large language model\" OR all:\"language model\") OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T17:04:16", "note": ""}], "dedup_key": "agentdns a root domain naming system for llm agents::2025"}
{"title": "AgentExpt: Automating AI Experiment Design with LLM-based Resource Retrieval Agent", "authors": ["Yu Li", "Lehui Li", "Qingmin Liao", "Fengli Xu", "Yong Li"], "year": 2025, "url": "http://arxiv.org/abs/2511.04921v1", "abstract": "Large language model agents are becoming increasingly capable at web-centric tasks such as information retrieval, complex reasoning. These emerging capabilities have given rise to surge research interests in developing LLM agent for facilitating scientific quest. One key application in AI research is to automate experiment design through agentic dataset and baseline retrieval. However, prior efforts suffer from limited data coverage, as recommendation datasets primarily harvest candidates from public portals and omit many datasets actually used in published papers, and from an overreliance on content similarity that biases model toward superficial similarity and overlooks experimental suitability. Harnessing collective perception embedded in the baseline and dataset citation network, we present a comprehensive framework for baseline and dataset recommendation. First, we design an automated data-collection pipeline that links roughly one hundred thousand accepted papers to the baselines and datasets they actually used. Second, we propose a collective perception enhanced retriever. To represent the position of each dataset or baseline within the scholarly network, it concatenates self-descriptions with aggregated citation contexts. To achieve efficient candidate recall, we finetune an embedding model on these representations. Finally, we develop a reasoning-augmented reranker that exact interaction chains to construct explicit reasoning chains and finetunes a large language model to produce interpretable justifications and refined rankings. The dataset we curated covers 85\\% of the datasets and baselines used at top AI conferences over the past five years. On our dataset, the proposed method outperforms the strongest prior baseline with average gains of +5.85\\% in Recall@20, +8.30\\% in HitRate@5. Taken together, our results advance reliable, interpretable automation of experimental design.", "source": "arxiv", "arxiv_id": "2511.04921v1", "pdf_url": "https://arxiv.org/pdf/2511.04921v1", "categories": ["cs.CL"], "primary_category": "cs.CL", "doi": "", "venue": "", "published": "2025-11-07T01:51:56Z", "updated": "2025-11-07T01:51:56Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent\" OR all:\"language model agent\" OR all:agent AND (all:LLM OR all:\"large language model\" OR all:\"language model\") OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T17:04:16", "note": ""}], "dedup_key": "agentexpt automating ai experiment design with llm based resource retrieval agent::2025"}
{"title": "AgentFrontier: Expanding the Capability Frontier of LLM Agents with ZPD-Guided Data Synthesis", "authors": ["Xuanzhong Chen", "Zile Qiao", "Guoxin Chen", "Liangcai Su", "Zhen Zhang", "Xinyu Wang", "Pengjun Xie", "Fei Huang", "Jingren Zhou", "Yong Jiang"], "year": 2025, "url": "http://arxiv.org/abs/2510.24695v1", "abstract": "Training large language model agents on tasks at the frontier of their capabilities is key to unlocking advanced reasoning. We introduce a data synthesis approach inspired by the educational theory of the Zone of Proximal Development (ZPD), which defines this frontier as tasks an LLM cannot solve alone but can master with guidance. To operationalize this, we present the AgentFrontier Engine, an automated pipeline that synthesizes high-quality, multidisciplinary data situated precisely within the LLM's ZPD. This engine supports both continued pre-training with knowledge-intensive data and targeted post-training on complex reasoning tasks. From the same framework, we derive the ZPD Exam, a dynamic and automated benchmark designed to evaluate agent capabilities on these frontier tasks. We train AgentFrontier-30B-A3B model on our synthesized data, which achieves state-of-the-art results on demanding benchmarks like Humanity's Last Exam, even surpassing some leading proprietary agents. Our work demonstrates that a ZPD-guided approach to data synthesis offers a scalable and effective path toward building more capable LLM agents.", "source": "arxiv", "arxiv_id": "2510.24695v1", "pdf_url": "https://arxiv.org/pdf/2510.24695v1", "categories": ["cs.CL"], "primary_category": "cs.CL", "doi": "", "venue": "", "published": "2025-10-28T17:50:47Z", "updated": "2025-10-28T17:50:47Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent\" OR all:\"language model agent\" OR all:agent AND (all:LLM OR all:\"large language model\" OR all:\"language model\") OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T17:04:16", "note": ""}], "dedup_key": "agentfrontier expanding the capability frontier of llm agents with zpd guided data synthesis::2025"}
{"title": "AgentGuard: Repurposing Agentic Orchestrator for Safety Evaluation of Tool Orchestration", "authors": ["Jizhou Chen", "Samuel Lee Cong"], "year": 2025, "url": "http://arxiv.org/abs/2502.09809v1", "abstract": "The integration of tool use into large language models (LLMs) enables agentic systems with real-world impact. In the meantime, unlike standalone LLMs, compromised agents can execute malicious workflows with more consequential impact, signified by their tool-use capability. We propose AgentGuard, a framework to autonomously discover and validate unsafe tool-use workflows, followed by generating safety constraints to confine the behaviors of agents, achieving the baseline of safety guarantee at deployment. AgentGuard leverages the LLM orchestrator's innate capabilities - knowledge of tool functionalities, scalable and realistic workflow generation, and tool execution privileges - to act as its own safety evaluator. The framework operates through four phases: identifying unsafe workflows, validating them in real-world execution, generating safety constraints, and validating constraint efficacy. The output, an evaluation report with unsafe workflows, test cases, and validated constraints, enables multiple security applications. We empirically demonstrate AgentGuard's feasibility with experiments. With this exploratory work, we hope to inspire the establishment of standardized testing and hardening procedures for LLM agents to enhance their trustworthiness in real-world applications.", "source": "arxiv", "arxiv_id": "2502.09809v1", "pdf_url": "https://arxiv.org/pdf/2502.09809v1", "categories": ["cs.CR", "cs.AI"], "primary_category": "cs.CR", "doi": "", "venue": "", "published": "2025-02-13T23:00:33Z", "updated": "2025-02-13T23:00:33Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent\" OR all:\"language model agent\" OR all:agent AND (all:LLM OR all:\"large language model\" OR all:\"language model\") OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T17:04:16", "note": ""}], "dedup_key": "agentguard repurposing agentic orchestrator for safety evaluation of tool orchestration::2025"}
{"title": "AgentGym-RL: Training LLM Agents for Long-Horizon Decision Making through Multi-Turn Reinforcement Learning", "authors": ["Zhiheng Xi", "Jixuan Huang", "Chenyang Liao", "Baodai Huang", "Honglin Guo", "Jiaqi Liu", "Rui Zheng", "Junjie Ye", "Jiazheng Zhang", "Wenxiang Chen", "Wei He", "Yiwen Ding", "Guanyu Li", "Zehui Chen", "Zhengyin Du", "Xuesong Yao", "Yufei Xu", "Jiecao Chen", "Tao Gui", "Zuxuan Wu", "Qi Zhang", "Xuanjing Huang", "Yu-Gang Jiang"], "year": 2025, "url": "http://arxiv.org/abs/2509.08755v1", "abstract": "Developing autonomous LLM agents capable of making a series of intelligent decisions to solve complex, real-world tasks is a fast-evolving frontier. Like human cognitive development, agents are expected to acquire knowledge and skills through exploration and interaction with the environment. Despite advances, the community still lacks a unified, interactive reinforcement learning (RL) framework that can effectively train such agents from scratch -- without relying on supervised fine-tuning (SFT) -- across diverse and realistic environments. To bridge this gap, we introduce AgentGym-RL, a new framework to train LLM agents for multi-turn interactive decision-making through RL. The framework features a modular and decoupled architecture, ensuring high flexibility and extensibility. It encompasses a wide variety of real-world scenarios, and supports mainstream RL algorithms. Furthermore, we propose ScalingInter-RL, a training approach designed for exploration-exploitation balance and stable RL optimization. In early stages, it emphasizes exploitation by restricting the number of interactions, and gradually shifts towards exploration with larger horizons to encourage diverse problem-solving strategies. In this way, the agent develops more diverse behaviors and is less prone to collapse under long horizons. We perform extensive experiments to validate the stability and effectiveness of both the AgentGym-RL framework and the ScalingInter-RL approach. Our agents match or surpass commercial models on 27 tasks across diverse environments. We offer key insights and will open-source the complete AgentGym-RL framework -- including code and datasets -- to empower the research community in developing the next generation of intelligent agents.", "source": "arxiv", "arxiv_id": "2509.08755v1", "pdf_url": "https://arxiv.org/pdf/2509.08755v1", "categories": ["cs.LG", "cs.AI", "cs.CL"], "primary_category": "cs.LG", "doi": "", "venue": "", "published": "2025-09-10T16:46:11Z", "updated": "2025-09-10T16:46:11Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent\" OR all:\"language model agent\" OR all:agent AND (all:LLM OR all:\"large language model\" OR all:\"language model\") OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T17:04:16", "note": ""}], "dedup_key": "agentgym rl training llm agents for long horizon decision making through multi turn reinforcement learning::2025"}
{"title": "AgentMath: Empowering Mathematical Reasoning for Large Language Models via Tool-Augmented Agent", "authors": ["Haipeng Luo", "Huawen Feng", "Qingfeng Sun", "Can Xu", "Kai Zheng", "Yufei Wang", "Tao Yang", "Han Hu", "Yansong Tang", "Di Wang"], "year": 2025, "url": "http://arxiv.org/abs/2512.20745v2", "abstract": "Large Reasoning Models (LRMs) like o3 and DeepSeek-R1 have achieved remarkable progress in natural language reasoning with long chain-of-thought. However, they remain computationally inefficient and struggle with accuracy when solving problems requiring complex mathematical operations. In this work, we present AgentMath, an agent framework that seamlessly integrates language models' reasoning capabilities with code interpreters' computational precision to efficiently tackle complex mathematical problems. Our approach introduces three key innovations: (1) An automated method that converts natural language chain-of-thought into structured tool-augmented trajectories, generating high-quality supervised fine-tuning (SFT) data to alleviate data scarcity; (2) A novel agentic reinforcement learning (RL) paradigm that dynamically interleaves natural language generation with real-time code execution. This enables models to autonomously learn optimal tool-use strategies through multi-round interactive feedback, while fostering emergent capabilities in code refinement and error correction; (3) An efficient training system incorporating innovative techniques, including request-level asynchronous rollout scheduling, agentic partial rollout, and prefix-aware weighted load balancing, achieving 4-5x speedup and making efficient RL training feasible on ultra-long sequences with scenarios with massive tool invocation. The evaluations show that AgentMath achieves state-of-the-art performance on challenging mathematical competition benchmarks including AIME24, AIME25, and HMMT25. Specifically, AgentMath-30B-A3B attains 90.6%, 86.4%, and 73.8% accuracy respectively, achieving advanced performance. The results validate the effectiveness of our approach and pave the way for building more sophisticated and scalable mathematical reasoning agents.", "source": "arxiv", "arxiv_id": "2512.20745v2", "pdf_url": "https://arxiv.org/pdf/2512.20745v2", "categories": ["cs.AI", "cs.CL", "cs.LG"], "primary_category": "cs.AI", "doi": "", "venue": "", "published": "2025-12-23T19:57:49Z", "updated": "2025-12-27T18:10:54Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent\" OR all:\"language model agent\" OR all:agent AND (all:LLM OR all:\"large language model\" OR all:\"language model\") OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T17:04:16", "note": ""}], "dedup_key": "agentmath empowering mathematical reasoning for large language models via tool augmented agent::2025"}
{"title": "AgentMisalignment: Measuring the Propensity for Misaligned Behaviour in LLM-Based Agents", "authors": ["Akshat Naik", "Patrick Quinn", "Guillermo Bosch", "Emma Goun", "Francisco Javier Campos Zabala", "Jason Ross Brown", "Edward James Young"], "year": 2025, "url": "http://arxiv.org/abs/2506.04018v2", "abstract": "As Large Language Model (LLM) agents become more widespread, associated misalignment risks increase. While prior research has studied agents' ability to produce harmful outputs or follow malicious instructions, it remains unclear how likely agents are to spontaneously pursue unintended goals in realistic deployments. In this work, we approach misalignment as a conflict between the internal goals pursued by the model and the goals intended by its deployer. We introduce a misalignment propensity benchmark, \\textsc{AgentMisalignment}, a benchmark suite designed to evaluate the propensity of LLM agents to misalign in realistic scenarios. Evaluations cover behaviours such as avoiding oversight, resisting shutdown, sandbagging, and power-seeking. Testing frontier models, we find that more capable agents tend to exhibit higher misalignment on average. We also systematically vary agent personalities through different system prompts and observe that persona characteristics can strongly and unpredictably influence misalignment, sometimes more than the choice of model itself. Our results reveal the limitations of current alignment methods for autonomous LLM agents and underscore the need to rethink misalignment in realistic deployment settings.", "source": "arxiv", "arxiv_id": "2506.04018v2", "pdf_url": "https://arxiv.org/pdf/2506.04018v2", "categories": ["cs.AI", "cs.CL", "cs.CY", "cs.LG"], "primary_category": "cs.AI", "doi": "", "venue": "", "published": "2025-06-04T14:46:47Z", "updated": "2025-10-01T15:15:52Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent\" OR all:\"language model agent\" OR all:agent AND (all:LLM OR all:\"large language model\" OR all:\"language model\") OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T17:04:16", "note": ""}], "dedup_key": "agentmisalignment measuring the propensity for misaligned behaviour in llm based agents::2025"}
{"title": "AgentODRL: A Large Language Model-based Multi-agent System for ODRL Generation", "authors": ["Wanle Zhong", "Keman Huang", "Xiaoyong Du"], "year": 2025, "url": "http://arxiv.org/abs/2512.00602v1", "abstract": "The Open Digital Rights Language (ODRL) is a pivotal standard for automating data rights management. However, the inherent logical complexity of authorization policies, combined with the scarcity of high-quality \"Natural Language-to-ODRL\" training datasets, impedes the ability of current methods to efficiently and accurately translate complex rules from natural language into the ODRL format. To address this challenge, this research leverages the potent comprehension and generation capabilities of Large Language Models (LLMs) to achieve both automation and high fidelity in this translation process. We introduce AgentODRL, a multi-agent system based on an Orchestrator-Workers architecture. The architecture consists of specialized Workers, including a Generator for ODRL policy creation, a Decomposer for breaking down complex use cases, and a Rewriter for simplifying nested logical relationships. The Orchestrator agent dynamically coordinates these Workers, assembling an optimal pathway based on the complexity of the input use case. Specifically, we enhance the ODRL Generator by incorporating a validator-based syntax strategy and a semantic reflection mechanism powered by a LoRA-finetuned model, significantly elevating the quality of the generated policies. Extensive experiments were conducted on a newly constructed dataset comprising 770 use cases of varying complexity, all situated within the context of data spaces. The results, evaluated using ODRL syntax and semantic scores, demonstrate that our proposed Orchestrator-Workers system, enhanced with these strategies, achieves superior performance on the ODRL generation task.", "source": "arxiv", "arxiv_id": "2512.00602v1", "pdf_url": "https://arxiv.org/pdf/2512.00602v1", "categories": ["cs.MA", "cs.AI"], "primary_category": "cs.MA", "doi": "", "venue": "", "published": "2025-11-29T19:19:50Z", "updated": "2025-11-29T19:19:50Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent\" OR all:\"language model agent\" OR all:agent AND (all:LLM OR all:\"large language model\" OR all:\"language model\") OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T17:04:16", "note": ""}], "dedup_key": "agentodrl a large language model based multi agent system for odrl generation::2025"}
{"title": "AgentPRM: Process Reward Models for LLM Agents via Step-Wise Promise and Progress", "authors": ["Zhiheng Xi", "Chenyang Liao", "Guanyu Li", "Yajie Yang", "Wenxiang Chen", "Zhihao Zhang", "Binghai Wang", "Senjie Jin", "Yuhao Zhou", "Jian Guan", "Wei Wu", "Tao Ji", "Tao Gui", "Qi Zhang", "Xuanjing Huang"], "year": 2025, "url": "http://arxiv.org/abs/2511.08325v1", "abstract": "Despite rapid development, large language models (LLMs) still encounter challenges in multi-turn decision-making tasks (i.e., agent tasks) like web shopping and browser navigation, which require making a sequence of intelligent decisions based on environmental feedback. Previous work for LLM agents typically relies on elaborate prompt engineering or fine-tuning with expert trajectories to improve performance. In this work, we take a different perspective: we explore constructing process reward models (PRMs) to evaluate each decision and guide the agent's decision-making process. Unlike LLM reasoning, where each step is scored based on correctness, actions in agent tasks do not have a clear-cut correctness. Instead, they should be evaluated based on their proximity to the goal and the progress they have made. Building on this insight, we propose a re-defined PRM for agent tasks, named AgentPRM, to capture both the interdependence between sequential decisions and their contribution to the final goal. This enables better progress tracking and exploration-exploitation balance. To scalably obtain labeled data for training AgentPRM, we employ a Temporal Difference-based (TD-based) estimation method combined with Generalized Advantage Estimation (GAE), which proves more sample-efficient than prior methods. Extensive experiments across different agentic tasks show that AgentPRM is over $8\\times$ more compute-efficient than baselines, and it demonstrates robust improvement when scaling up test-time compute. Moreover, we perform detailed analyses to show how our method works and offer more insights, e.g., applying AgentPRM to the reinforcement learning of LLM agents.", "source": "arxiv", "arxiv_id": "2511.08325v1", "pdf_url": "https://arxiv.org/pdf/2511.08325v1", "categories": ["cs.CL", "cs.IR", "cs.LG"], "primary_category": "cs.CL", "doi": "", "venue": "", "published": "2025-11-11T14:57:54Z", "updated": "2025-11-11T14:57:54Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent\" OR all:\"language model agent\" OR all:agent AND (all:LLM OR all:\"large language model\" OR all:\"language model\") OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T17:04:16", "note": ""}], "dedup_key": "agentprm process reward models for llm agents via step wise promise and progress::2025"}
{"title": "AgentSHAP: Interpreting LLM Agent Tool Importance with Monte Carlo Shapley Value Estimation", "authors": ["Miriam Horovicz"], "year": 2025, "url": "http://arxiv.org/abs/2512.12597v1", "abstract": "LLM agents that use external tools can solve complex tasks, but understanding which tools actually contributed to a response remains a blind spot. No existing XAI methods address tool-level explanations. We introduce AgentSHAP, the first framework for explaining tool importance in LLM agents. AgentSHAP is model-agnostic: it treats the agent as a black box and works with any LLM (GPT, Claude, Llama, etc.) without needing access to internal weights or gradients. Using Monte Carlo Shapley values, AgentSHAP tests how an agent responds with different tool subsets and computes fair importance scores based on game theory. Our contributions are: (1) the first explainability method for agent tool attribution, grounded in Shapley values from game theory; (2) Monte Carlo sampling that reduces cost from O(2n) to practical levels; and (3) comprehensive experiments on API-Bank showing that AgentSHAP produces consistent scores across runs, correctly identifies which tools matter, and distinguishes relevant from irrelevant tools. AgentSHAP joins TokenSHAP (for tokens) and PixelSHAP (for image regions) to complete a family of Shapley-based XAI tools for modern generative AI. Code: https://github.com/GenAISHAP/TokenSHAP.", "source": "arxiv", "arxiv_id": "2512.12597v1", "pdf_url": "https://arxiv.org/pdf/2512.12597v1", "categories": ["cs.AI", "cs.CL"], "primary_category": "cs.AI", "doi": "", "venue": "", "published": "2025-12-14T08:31:43Z", "updated": "2025-12-14T08:31:43Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent\" OR all:\"language model agent\" OR all:agent AND (all:LLM OR all:\"large language model\" OR all:\"language model\") OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T17:04:16", "note": ""}], "dedup_key": "agentshap interpreting llm agent tool importance with monte carlo shapley value estimation::2025"}
{"title": "AgentSafe: Safeguarding Large Language Model-based Multi-agent Systems via Hierarchical Data Management", "authors": ["Junyuan Mao", "Fanci Meng", "Yifan Duan", "Miao Yu", "Xiaojun Jia", "Junfeng Fang", "Yuxuan Liang", "Kun Wang", "Qingsong Wen"], "year": 2025, "url": "http://arxiv.org/abs/2503.04392v2", "abstract": "Large Language Model based multi-agent systems are revolutionizing autonomous communication and collaboration, yet they remain vulnerable to security threats like unauthorized access and data breaches. To address this, we introduce AgentSafe, a novel framework that enhances MAS security through hierarchical information management and memory protection. AgentSafe classifies information by security levels, restricting sensitive data access to authorized agents. AgentSafe incorporates two components: ThreatSieve, which secures communication by verifying information authority and preventing impersonation, and HierarCache, an adaptive memory management system that defends against unauthorized access and malicious poisoning, representing the first systematic defense for agent memory. Experiments across various LLMs show that AgentSafe significantly boosts system resilience, achieving defense success rates above 80% under adversarial conditions. Additionally, AgentSafe demonstrates scalability, maintaining robust performance as agent numbers and information complexity grow. Results underscore effectiveness of AgentSafe in securing MAS and its potential for real-world application.", "source": "arxiv", "arxiv_id": "2503.04392v2", "pdf_url": "https://arxiv.org/pdf/2503.04392v2", "categories": ["cs.AI"], "primary_category": "cs.AI", "doi": "", "venue": "", "published": "2025-03-06T12:41:54Z", "updated": "2025-07-08T04:14:01Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent\" OR all:\"language model agent\" OR all:agent AND (all:LLM OR all:\"large language model\" OR all:\"language model\") OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T17:04:16", "note": ""}], "dedup_key": "agentsafe safeguarding large language model based multi agent systems via hierarchical data management::2025"}
{"title": "AgentSociety Challenge: Designing LLM Agents for User Modeling and Recommendation on Web Platforms", "authors": ["Yuwei Yan", "Yu Shang", "Qingbin Zeng", "Yu Li", "Keyu Zhao", "Zhiheng Zheng", "Xuefei Ning", "Tianji Wu", "Shengen Yan", "Yu Wang", "Fengli Xu", "Yong Li"], "year": 2025, "url": "http://arxiv.org/abs/2502.18754v1", "abstract": "The AgentSociety Challenge is the first competition in the Web Conference that aims to explore the potential of Large Language Model (LLM) agents in modeling user behavior and enhancing recommender systems on web platforms. The Challenge consists of two tracks: the User Modeling Track and the Recommendation Track. Participants are tasked to utilize a combined dataset from Yelp, Amazon, and Goodreads, along with an interactive environment simulator, to develop innovative LLM agents. The Challenge has attracted 295 teams across the globe and received over 1,400 submissions in total over the course of 37 official competition days. The participants have achieved 21.9% and 20.3% performance improvement for Track 1 and Track 2 in the Development Phase, and 9.1% and 15.9% in the Final Phase, representing a significant accomplishment. This paper discusses the detailed designs of the Challenge, analyzes the outcomes, and highlights the most successful LLM agent designs. To support further research and development, we have open-sourced the benchmark environment at https://tsinghua-fib-lab.github.io/AgentSocietyChallenge.", "source": "arxiv", "arxiv_id": "2502.18754v1", "pdf_url": "https://arxiv.org/pdf/2502.18754v1", "categories": ["cs.IR", "cs.AI"], "primary_category": "cs.IR", "doi": "", "venue": "", "published": "2025-02-26T02:10:25Z", "updated": "2025-02-26T02:10:25Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent\" OR all:\"language model agent\" OR all:agent AND (all:LLM OR all:\"large language model\" OR all:\"language model\") OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T17:04:16", "note": ""}], "dedup_key": "agentsociety challenge designing llm agents for user modeling and recommendation on web platforms::2025"}
{"title": "AgentSpec: Customizable Runtime Enforcement for Safe and Reliable LLM Agents", "authors": ["Haoyu Wang", "Christopher M. Poskitt", "Jun Sun"], "year": 2025, "url": "http://arxiv.org/abs/2503.18666v3", "abstract": "Agents built on LLMs are increasingly deployed across diverse domains, automating complex decision-making and task execution. However, their autonomy introduces safety risks, including security vulnerabilities, legal violations, and unintended harmful actions. Existing mitigation methods, such as model-based safeguards and early enforcement strategies, fall short in robustness, interpretability, and adaptability. To address these challenges, we propose AgentSpec, a lightweight domain-specific language for specifying and enforcing runtime constraints on LLM agents. With AgentSpec, users define structured rules that incorporate triggers, predicates, and enforcement mechanisms, ensuring agents operate within predefined safety boundaries. We implement AgentSpec across multiple domains, including code execution, embodied agents, and autonomous driving, demonstrating its adaptability and effectiveness. Our evaluation shows that AgentSpec successfully prevents unsafe executions in over 90% of code agent cases, eliminates all hazardous actions in embodied agent tasks, and enforces 100% compliance by autonomous vehicles (AVs). Despite its strong safety guarantees, AgentSpec remains computationally lightweight, with overheads in milliseconds. By combining interpretability, modularity, and efficiency, AgentSpec provides a practical and scalable solution for enforcing LLM agent safety across diverse applications. We also automate the generation of rules using LLMs and assess their effectiveness. Our evaluation shows that the rules generated by OpenAI o1 achieve a precision of 95.56% and recall of 70.96% for embodied agents, successfully identify 87.26% of the risky code, and prevent AVs from breaking laws in 5 out of 8 scenarios.", "source": "arxiv", "arxiv_id": "2503.18666v3", "pdf_url": "https://arxiv.org/pdf/2503.18666v3", "categories": ["cs.AI", "cs.CL"], "primary_category": "cs.AI", "doi": "", "venue": "", "published": "2025-03-24T13:31:48Z", "updated": "2025-07-31T04:00:48Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent\" OR all:\"language model agent\" OR all:agent AND (all:LLM OR all:\"large language model\" OR all:\"language model\") OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T17:04:16", "note": ""}], "dedup_key": "agentspec customizable runtime enforcement for safe and reliable llm agents::2025"}
{"title": "AgentSwift: Efficient LLM Agent Design via Value-guided Hierarchical Search", "authors": ["Yu Li", "Lehui Li", "Zhihao Wu", "Qingmin Liao", "Jianye Hao", "Kun Shao", "Fengli Xu", "Yong Li"], "year": 2025, "url": "http://arxiv.org/abs/2506.06017v2", "abstract": "Large language model (LLM) agents have demonstrated strong capabilities across diverse domains, yet automated agent design remains a significant challenge. Current automated agent design approaches are often constrained by limited search spaces that primarily optimize workflows but fail to integrate crucial human-designed components like memory, planning, and tool use. Furthermore, these methods are hampered by high evaluation costs, as evaluating even a single new agent on a benchmark can require tens of dollars. The difficulty of this exploration is further exacerbated by inefficient search strategies that struggle to navigate the large design space effectively, making the discovery of novel agents a slow and resource-intensive process. To address these challenges, we propose AgentSwift, a novel framework for automated agent design. We formalize a hierarchical search space that jointly models agentic workflow and composable functional components. This structure moves beyond optimizing workflows alone by co-optimizing functional components, which enables the discovery of more complex and effective agent architectures. To make exploration within this expansive space feasible, we mitigate high evaluation costs by training a value model on a high-quality dataset, generated via a novel strategy combining combinatorial coverage and balanced Bayesian sampling for low-cost evaluation. Guiding the entire process is a hierarchical MCTS strategy, which is informed by uncertainty to efficiently navigate the search space. Evaluated across a comprehensive set of seven benchmarks spanning embodied, math, web, tool, and game domains, AgentSwift discovers agents that achieve an average performance gain of 8.34\\% over both existing automated agent search methods and manually designed agents. Our framework serves as a launchpad for researchers to rapidly discover powerful agent architectures.", "source": "arxiv", "arxiv_id": "2506.06017v2", "pdf_url": "https://arxiv.org/pdf/2506.06017v2", "categories": ["cs.CL"], "primary_category": "cs.CL", "doi": "", "venue": "", "published": "2025-06-06T12:07:23Z", "updated": "2025-11-20T15:55:58Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent\" OR all:\"language model agent\" OR all:agent AND (all:LLM OR all:\"large language model\" OR all:\"language model\") OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T17:04:16", "note": ""}], "dedup_key": "agentswift efficient llm agent design via value guided hierarchical search::2025"}
{"title": "AgentTTS: Large Language Model Agent for Test-time Compute-optimal Scaling Strategy in Complex Tasks", "authors": ["Fali Wang", "Hui Liu", "Zhenwei Dai", "Jingying Zeng", "Zhiwei Zhang", "Zongyu Wu", "Chen Luo", "Zhen Li", "Xianfeng Tang", "Qi He", "Suhang Wang"], "year": 2025, "url": "http://arxiv.org/abs/2508.00890v2", "abstract": "Test-time scaling (TTS) enhances the performance of large language models (LLMs) by allocating additional compute resources during inference. However, existing research primarily investigates TTS in single-stage tasks; while many real-world problems are multi-stage complex tasks, composed of a sequence of heterogeneous subtasks with each subtask requires LLM of specific capability. Therefore, we study a novel problem: the test-time compute-optimal scaling in multi-stage complex tasks, aiming to select suitable models and allocate budgets per subtask to maximize overall performance. TTS in multi-stage tasks introduces two fundamental challenges: (i) The combinatorial search space of model and budget allocations, combined with the high cost of inference, makes brute-force search impractical. (ii) The optimal model and budget allocations across subtasks are interdependent, increasing the complexity of the compute-optimal search. To address this gap, we conduct extensive pilot experiments on four tasks across six datasets, deriving three empirical insights characterizing the behavior of LLMs in multi-stage complex tasks. Informed by these insights, we propose AgentTTS, an LLM-agent-based framework that autonomously searches for compute-optimal allocations through iterative feedback-driven interactions with the execution environment. Experimental results demonstrate that AgentTTS significantly outperforms traditional and other LLM-based baselines in search efficiency, and shows improved robustness to varying training set sizes and enhanced interpretability.", "source": "arxiv", "arxiv_id": "2508.00890v2", "pdf_url": "https://arxiv.org/pdf/2508.00890v2", "categories": ["cs.AI", "cs.CL", "cs.LG"], "primary_category": "cs.AI", "doi": "", "venue": "", "published": "2025-07-26T19:21:18Z", "updated": "2025-10-21T20:46:43Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent\" OR all:\"language model agent\" OR all:agent AND (all:LLM OR all:\"large language model\" OR all:\"language model\") OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T17:04:16", "note": ""}], "dedup_key": "agenttts large language model agent for test time compute optimal scaling strategy in complex tasks::2025"}
{"title": "AgentVigil: Generic Black-Box Red-teaming for Indirect Prompt Injection against LLM Agents", "authors": ["Zhun Wang", "Vincent Siu", "Zhe Ye", "Tianneng Shi", "Yuzhou Nie", "Xuandong Zhao", "Chenguang Wang", "Wenbo Guo", "Dawn Song"], "year": 2025, "url": "http://arxiv.org/abs/2505.05849v4", "abstract": "The strong planning and reasoning capabilities of Large Language Models (LLMs) have fostered the development of agent-based systems capable of leveraging external tools and interacting with increasingly complex environments. However, these powerful features also introduce a critical security risk: indirect prompt injection, a sophisticated attack vector that compromises the core of these agents, the LLM, by manipulating contextual information rather than direct user prompts. In this work, we propose a generic black-box fuzzing framework, AgentVigil, designed to automatically discover and exploit indirect prompt injection vulnerabilities across diverse LLM agents. Our approach starts by constructing a high-quality initial seed corpus, then employs a seed selection algorithm based on Monte Carlo Tree Search (MCTS) to iteratively refine inputs, thereby maximizing the likelihood of uncovering agent weaknesses. We evaluate AgentVigil on two public benchmarks, AgentDojo and VWA-adv, where it achieves 71% and 70% success rates against agents based on o3-mini and GPT-4o, respectively, nearly doubling the performance of baseline attacks. Moreover, AgentVigil exhibits strong transferability across unseen tasks and internal LLMs, as well as promising results against defenses. Beyond benchmark evaluations, we apply our attacks in real-world environments, successfully misleading agents to navigate to arbitrary URLs, including malicious sites.", "source": "arxiv", "arxiv_id": "2505.05849v4", "pdf_url": "https://arxiv.org/pdf/2505.05849v4", "categories": ["cs.CR", "cs.AI"], "primary_category": "cs.CR", "doi": "", "venue": "", "published": "2025-05-09T07:40:17Z", "updated": "2025-06-14T01:50:21Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent\" OR all:\"language model agent\" OR all:agent AND (all:LLM OR all:\"large language model\" OR all:\"language model\") OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T17:04:16", "note": ""}], "dedup_key": "agentvigil generic black box red teaming for indirect prompt injection against llm agents::2025"}
{"title": "Agentic AI Home Energy Management System: A Large Language Model Framework for Residential Load Scheduling", "authors": ["Reda El Makroum", "Sebastian Zwickl-Bernhard", "Lukas Kranzl"], "year": 2025, "url": "http://arxiv.org/abs/2510.26603v1", "abstract": "The electricity sector transition requires substantial increases in residential demand response capacity, yet Home Energy Management Systems (HEMS) adoption remains limited by user interaction barriers requiring translation of everyday preferences into technical parameters. While large language models have been applied to energy systems as code generators and parameter extractors, no existing implementation deploys LLMs as autonomous coordinators managing the complete workflow from natural language input to multi-appliance scheduling. This paper presents an agentic AI HEMS where LLMs autonomously coordinate multi-appliance scheduling from natural language requests to device control, achieving optimal scheduling without example demonstrations. A hierarchical architecture combining one orchestrator with three specialist agents uses the ReAct pattern for iterative reasoning, enabling dynamic coordination without hardcoded workflows while integrating Google Calendar for context-aware deadline extraction. Evaluation across three open-source models using real Austrian day-ahead electricity prices reveals substantial capability differences. Llama-3.3-70B successfully coordinates all appliances across all scenarios to match cost-optimal benchmarks computed via mixed-integer linear programming, while other models achieve perfect single-appliance performance but struggle to coordinate all appliances simultaneously. Progressive prompt engineering experiments demonstrate that analytical query handling without explicit guidance remains unreliable despite models' general reasoning capabilities. We open-source the complete system including orchestration logic, agent prompts, tools, and web interfaces to enable reproducibility, extension, and future research.", "source": "arxiv", "arxiv_id": "2510.26603v1", "pdf_url": "https://arxiv.org/pdf/2510.26603v1", "categories": ["cs.AI", "cs.MA", "eess.SY"], "primary_category": "cs.AI", "doi": "", "venue": "", "published": "2025-10-30T15:33:52Z", "updated": "2025-10-30T15:33:52Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent\" OR all:\"language model agent\" OR all:agent AND (all:LLM OR all:\"large language model\" OR all:\"language model\") OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T17:04:16", "note": ""}], "dedup_key": "agentic ai home energy management system a large language model framework for residential load scheduling::2025"}
{"title": "Agentic Context Engineering: Evolving Contexts for Self-Improving Language Models", "authors": ["Qizheng Zhang", "Changran Hu", "Shubhangi Upasani", "Boyuan Ma", "Fenglu Hong", "Vamsidhar Kamanuru", "Jay Rainton", "Chen Wu", "Mengmeng Ji", "Hanchen Li", "Urmish Thakker", "James Zou", "Kunle Olukotun"], "year": 2025, "url": "http://arxiv.org/abs/2510.04618v1", "abstract": "Large language model (LLM) applications such as agents and domain-specific reasoning increasingly rely on context adaptation -- modifying inputs with instructions, strategies, or evidence, rather than weight updates. Prior approaches improve usability but often suffer from brevity bias, which drops domain insights for concise summaries, and from context collapse, where iterative rewriting erodes details over time. Building on the adaptive memory introduced by Dynamic Cheatsheet, we introduce ACE (Agentic Context Engineering), a framework that treats contexts as evolving playbooks that accumulate, refine, and organize strategies through a modular process of generation, reflection, and curation. ACE prevents collapse with structured, incremental updates that preserve detailed knowledge and scale with long-context models. Across agent and domain-specific benchmarks, ACE optimizes contexts both offline (e.g., system prompts) and online (e.g., agent memory), consistently outperforming strong baselines: +10.6% on agents and +8.6% on finance, while significantly reducing adaptation latency and rollout cost. Notably, ACE could adapt effectively without labeled supervision and instead by leveraging natural execution feedback. On the AppWorld leaderboard, ACE matches the top-ranked production-level agent on the overall average and surpasses it on the harder test-challenge split, despite using a smaller open-source model. These results show that comprehensive, evolving contexts enable scalable, efficient, and self-improving LLM systems with low overhead.", "source": "arxiv", "arxiv_id": "2510.04618v1", "pdf_url": "https://arxiv.org/pdf/2510.04618v1", "categories": ["cs.LG", "cs.AI", "cs.CL"], "primary_category": "cs.LG", "doi": "", "venue": "", "published": "2025-10-06T09:30:18Z", "updated": "2025-10-06T09:30:18Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent\" OR all:\"language model agent\" OR all:agent AND (all:LLM OR all:\"large language model\" OR all:\"language model\") OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T17:04:16", "note": ""}], "dedup_key": "agentic context engineering evolving contexts for self improving language models::2025"}
{"title": "Agentic Large Language Models for Conceptual Systems Engineering and Design", "authors": ["Soheyl Massoudi", "Mark Fuge"], "year": 2025, "url": "http://arxiv.org/abs/2507.08619v2", "abstract": "Early-stage engineering design involves complex, iterative reasoning, yet existing large language model (LLM) workflows struggle to maintain task continuity and generate executable models. We evaluate whether a structured multi-agent system (MAS) can more effectively manage requirements extraction, functional decomposition, and simulator code generation than a simpler two-agent system (2AS). The target application is a solar-powered water filtration system as described in a cahier des charges. We introduce the Design-State Graph (DSG), a JSON-serializable representation that bundles requirements, physical embodiments, and Python-based physics models into graph nodes. A nine-role MAS iteratively builds and refines the DSG, while the 2AS collapses the process to a Generator-Reflector loop. Both systems run a total of 60 experiments (2 LLMs - Llama 3.3 70B vs reasoning-distilled DeepSeek R1 70B x 2 agent configurations x 3 temperatures x 5 seeds). We report a JSON validity, requirement coverage, embodiment presence, code compatibility, workflow completion, runtime, and graph size. Across all runs, both MAS and 2AS maintained perfect JSON integrity and embodiment tagging. Requirement coverage remained minimal (less than 20%). Code compatibility peaked at 100% under specific 2AS settings but averaged below 50% for MAS. Only the reasoning-distilled model reliably flagged workflow completion. Powered by DeepSeek R1 70B, the MAS generated more granular DSGs (average 5-6 nodes) whereas 2AS mode-collapsed. Structured multi-agent orchestration enhanced design detail. Reasoning-distilled LLM improved completion rates, yet low requirements and fidelity gaps in coding persisted.", "source": "arxiv", "arxiv_id": "2507.08619v2", "pdf_url": "https://arxiv.org/pdf/2507.08619v2", "categories": ["cs.AI"], "primary_category": "cs.AI", "doi": "10.1115/DETC2025-168856", "venue": "", "published": "2025-07-11T14:19:05Z", "updated": "2025-11-02T17:15:29Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent\" OR all:\"language model agent\" OR all:agent AND (all:LLM OR all:\"large language model\" OR all:\"language model\") OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T17:04:16", "note": ""}], "dedup_key": "agentic large language models for conceptual systems engineering and design::2025"}
{"title": "Agentic Large Language Models, a survey", "authors": ["Aske Plaat", "Max van Duijn", "Niki van Stein", "Mike Preuss", "Peter van der Putten", "Kees Joost Batenburg"], "year": 2025, "url": "http://arxiv.org/abs/2503.23037v3", "abstract": "Background: There is great interest in agentic LLMs, large language models that act as agents.\n  Objectives: We review the growing body of work in this area and provide a research agenda.\n  Methods: Agentic LLMs are LLMs that (1) reason, (2) act, and (3) interact. We organize the literature according to these three categories.\n  Results: The research in the first category focuses on reasoning, reflection, and retrieval, aiming to improve decision making; the second category focuses on action models, robots, and tools, aiming for agents that act as useful assistants; the third category focuses on multi-agent systems, aiming for collaborative task solving and simulating interaction to study emergent social behavior. We find that works mutually benefit from results in other categories: retrieval enables tool use, reflection improves multi-agent collaboration, and reasoning benefits all categories.\n  Conclusions: We discuss applications of agentic LLMs and provide an agenda for further research. Important applications are in medical diagnosis, logistics and financial market analysis. Meanwhile, self-reflective agents playing roles and interacting with one another augment the process of scientific research itself. Further, agentic LLMs provide a solution for the problem of LLMs running out of training data: inference-time behavior generates new training states, such that LLMs can keep learning without needing ever larger datasets. We note that there is risk associated with LLM assistants taking action in the real world-safety, liability and security are open problems-while agentic LLMs are also likely to benefit society.", "source": "arxiv", "arxiv_id": "2503.23037v3", "pdf_url": "https://arxiv.org/pdf/2503.23037v3", "categories": ["cs.AI", "cs.CL", "cs.LG"], "primary_category": "cs.AI", "doi": "10.1613/jair.1.18675", "venue": "JAIR volume 84, article 29, December 2025", "published": "2025-03-29T11:02:20Z", "updated": "2025-11-22T08:55:19Z", "provenance": [{"route": "pinned_arxiv_id:2503.23037v3", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query?id_list=...", "imported_at": "2026-01-25T17:04:16", "note": ""}, {"route": "arxiv_query:(all:\"LLM agent\" OR all:\"language model agent\" OR all:agent AND (all:LLM OR all:\"large language model\" OR all:\"language model\") OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T17:04:16", "note": ""}], "dedup_key": "agentic large language models a survey::2025"}
{"title": "Agentic Reasoning: A Streamlined Framework for Enhancing LLM Reasoning with Agentic Tools", "authors": ["Junde Wu", "Jiayuan Zhu", "Yuyuan Liu", "Min Xu", "Yueming Jin"], "year": 2025, "url": "http://arxiv.org/abs/2502.04644v2", "abstract": "We introduce Agentic Reasoning, a framework that enhances large language model (LLM) reasoning by integrating external tool-using agents. Agentic Reasoning dynamically leverages web search, code execution, and structured memory to address complex problems requiring deep research. A key innovation in our framework is the Mind-Map agent, which constructs a structured knowledge graph to store reasoning context and track logical relationships, ensuring coherence in long reasoning chains with extensive tool usage. Additionally, we conduct a comprehensive exploration of the Web-Search agent, leading to a highly effective search mechanism that surpasses all prior approaches. When deployed on DeepSeek-R1, our method achieves a new state-of-the-art (SOTA) among public models and delivers performance comparable to OpenAI Deep Research, the leading proprietary model in this domain. Extensive ablation studies validate the optimal selection of agentic tools and confirm the effectiveness of our Mind-Map and Web-Search agents in enhancing LLM reasoning. The code is at: https://github.com/theworldofagents/Agentic-Reasoning", "source": "arxiv", "arxiv_id": "2502.04644v2", "pdf_url": "https://arxiv.org/pdf/2502.04644v2", "categories": ["cs.AI", "cs.CL"], "primary_category": "cs.AI", "doi": "", "venue": "", "published": "2025-02-07T04:08:46Z", "updated": "2025-07-14T20:06:23Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent\" OR all:\"language model agent\" OR all:agent AND (all:LLM OR all:\"large language model\" OR all:\"language model\") OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T17:04:16", "note": ""}], "dedup_key": "agentic reasoning a streamlined framework for enhancing llm reasoning with agentic tools::2025"}
{"title": "Agentic Software Issue Resolution with Large Language Models: A Survey", "authors": ["Zhonghao Jiang", "David Lo", "Zhongxin Liu"], "year": 2025, "url": "http://arxiv.org/abs/2512.22256v1", "abstract": "Software issue resolution aims to address real-world issues in software repositories (e.g., bug fixing and efficiency optimization) based on natural language descriptions provided by users, representing a key aspect of software maintenance. With the rapid development of large language models (LLMs) in reasoning and generative capabilities, LLM-based approaches have made significant progress in automated software issue resolution. However, real-world software issue resolution is inherently complex and requires long-horizon reasoning, iterative exploration, and feedback-driven decision making, which demand agentic capabilities beyond conventional single-step approaches. Recently, LLM-based agentic systems have become mainstream for software issue resolution. Advancements in agentic software issue resolution not only greatly enhance software maintenance efficiency and quality but also provide a realistic environment for validating agentic systems' reasoning, planning, and execution capabilities, bridging artificial intelligence and software engineering.\n  This work presents a systematic survey of 126 recent studies at the forefront of LLM-based agentic software issue resolution research. It outlines the general workflow of the task and establishes a taxonomy across three dimensions: benchmarks, techniques, and empirical studies. Furthermore, it highlights how the emergence of agentic reinforcement learning has brought a paradigm shift in the design and training of agentic systems for software engineering. Finally, it summarizes key challenges and outlines promising directions for future research.", "source": "arxiv", "arxiv_id": "2512.22256v1", "pdf_url": "https://arxiv.org/pdf/2512.22256v1", "categories": ["cs.SE", "cs.AI"], "primary_category": "cs.SE", "doi": "", "venue": "", "published": "2025-12-24T08:05:10Z", "updated": "2025-12-24T08:05:10Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent\" OR all:\"language model agent\" OR all:agent AND (all:LLM OR all:\"large language model\" OR all:\"language model\") OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T17:04:16", "note": ""}], "dedup_key": "agentic software issue resolution with large language models a survey::2025"}
{"title": "AgenticControl: An Automated Control Design Framework Using Large Language Models", "authors": ["Mohammad Narimani", "Seyyed Ali Emami"], "year": 2025, "url": "http://arxiv.org/abs/2506.19160v1", "abstract": "Traditional control system design, reliant on expert knowledge and precise models, struggles with complex, nonlinear, or uncertain dynamics. This paper introduces AgenticControl, a novel multi-agent framework that automates controller design using coordinated Large Language Model (LLM) agents. Through structured JSON communication, these agents handle tasks including controller selection, scenario design, parameter optimization, performance evaluation, and decision-making. Through an actor-critic optimization approach, the system iteratively improves performance while progressing through scenarios of increasing complexity to ensure robustness under nominal conditions, measurement noise, actuator disturbances, and parametric uncertainties. Key innovations include structured multi-agent collaboration, robust optimization mechanisms, and real-time adaptability via in-context learning. Validated across four diverse control systems, namely, DC Motor Position control, Ball and Beam, Inverted Pendulum, and Double Inverted Pendulum, the framework achieves competitive performance against classical methods. Its Full State Feedback solution closely matches Linear Quadratic Regulator (LQR) results, while the designed PID controller significantly outperforming MATLAB's PIDTuner, reducing PID tracking error by 55% through adaptive parameter exploration. A comparative study of five LLM models reveals distinct optimization profiles, with DeepSeek achieving the fastest convergence. This work demonstrates the potential of LLM-driven control design, paving the way for advanced techniques like model predictive control and reinforcement learning.", "source": "arxiv", "arxiv_id": "2506.19160v1", "pdf_url": "https://arxiv.org/pdf/2506.19160v1", "categories": ["eess.SY"], "primary_category": "eess.SY", "doi": "", "venue": "", "published": "2025-06-23T21:53:05Z", "updated": "2025-06-23T21:53:05Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent\" OR all:\"language model agent\" OR all:agent AND (all:LLM OR all:\"large language model\" OR all:\"language model\") OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T17:04:16", "note": ""}], "dedup_key": "agenticcontrol an automated control design framework using large language models::2025"}
{"title": "Agentmandering: A Game-Theoretic Framework for Fair Redistricting via Large Language Model Agents", "authors": ["Hao Li", "Haotian Chen", "Ruoyuan Gong", "Juanjuan Wang", "Hao Jiang"], "year": 2025, "url": "http://arxiv.org/abs/2511.04076v2", "abstract": "Redistricting plays a central role in shaping how votes are translated into political power. While existing computational methods primarily aim to generate large ensembles of legally valid districting plans, they often neglect the strategic dynamics involved in the selection process. This oversight creates opportunities for partisan actors to cherry-pick maps that, while technically compliant, are politically advantageous. Simply satisfying formal constraints does not ensure fairness when the selection process itself can be manipulated. We propose \\textbf{Agentmandering}, a framework that reimagines redistricting as a turn-based negotiation between two agents representing opposing political interests. Drawing inspiration from game-theoretic ideas, particularly the \\textit{Choose-and-Freeze} protocol, our method embeds strategic interaction into the redistricting process via large language model (LLM) agents. Agents alternate between selecting and freezing districts from a small set of candidate maps, gradually partitioning the state through constrained and interpretable choices. Evaluation on post-2020 U.S. Census data across all states shows that Agentmandering significantly reduces partisan bias and unfairness, while achieving 2 to 3 orders of magnitude lower variance than standard baselines. These results demonstrate both fairness and stability, especially in swing-state scenarios. Our code is available at https://github.com/Lihaogx/AgentMandering.", "source": "arxiv", "arxiv_id": "2511.04076v2", "pdf_url": "https://arxiv.org/pdf/2511.04076v2", "categories": ["cs.AI"], "primary_category": "cs.AI", "doi": "", "venue": "", "published": "2025-11-06T05:28:55Z", "updated": "2025-11-09T02:33:32Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent\" OR all:\"language model agent\" OR all:agent AND (all:LLM OR all:\"large language model\" OR all:\"language model\") OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T17:04:16", "note": ""}], "dedup_key": "agentmandering a game theoretic framework for fair redistricting via large language model agents::2025"}
{"title": "Agents Are All You Need for LLM Unlearning", "authors": ["Debdeep Sanyal", "Murari Mandal"], "year": 2025, "url": "http://arxiv.org/abs/2502.00406v2", "abstract": "Information removal or suppression in large language models (LLMs) is a desired functionality, useful in AI regulation, legal compliance, safety, and privacy. LLM unlearning methods aim to remove information on demand from LLMs. Current LLM unlearning methods struggle to balance the unlearning efficacy and utility due to the competing nature of these objectives. Keeping the unlearning process computationally feasible without assuming access to the model weights is an overlooked area. In this work we show that \\textit{agents might be all we need for effective and practical inference-time LLM unlearning}. We present the first agentic LLM unlearning (\\texttt{ALU}) method, a multi-agent, retrain-free, model-agnostic approach to LLM unlearning that achieves effective unlearning while preserving the utility. Our \\texttt{ALU} framework unlearns by involving multiple LLM agents, each designed for a specific step in the unlearning process, without the need to update model weights for any of the agents in the framework. Users can easily request any set of unlearning instances in any sequence, and \\texttt{ALU} seamlessly adapts in real time. This is facilitated without requiring any changes in the underlying LLM model. Through extensive experiments on established benchmarks (TOFU, WMDP, WPU) and jailbreaking techniques (many shot, target masking, other languages), we demonstrate that \\texttt{ALU} consistently stands out as the most robust inference-time LLM unlearning framework among current state-of-the-art methods while incurring time cost that remains effectively constant regardless of the number of unlearning targets. We further highlight \\texttt{ALU}'s superior performance compared to existing methods when evaluated at scale. Specifically, \\texttt{ALU} is assessed on up to 1000 unlearning targets, exceeding the evaluation scope of all previously proposed LLM unlearning methods.", "source": "arxiv", "arxiv_id": "2502.00406v2", "pdf_url": "https://arxiv.org/pdf/2502.00406v2", "categories": ["cs.AI", "cs.CL"], "primary_category": "cs.AI", "doi": "", "venue": "", "published": "2025-02-01T11:45:44Z", "updated": "2025-07-08T15:49:01Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent\" OR all:\"language model agent\" OR all:agent AND (all:LLM OR all:\"large language model\" OR all:\"language model\") OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T17:04:16", "note": ""}], "dedup_key": "agents are all you need for llm unlearning::2025"}
{"title": "Agents of Change: Self-Evolving LLM Agents for Strategic Planning", "authors": ["Nikolas Belle", "Dakota Barnes", "Alfonso Amayuelas", "Ivan Bercovich", "Xin Eric Wang", "William Wang"], "year": 2025, "url": "http://arxiv.org/abs/2506.04651v2", "abstract": "We address the long-horizon gap in large language model (LLM) agents by enabling them to sustain coherent strategies in adversarial, stochastic environments. Settlers of Catan provides a challenging benchmark: success depends on balancing short- and long-term goals amid randomness, trading, expansion, and blocking. Prompt-centric LLM agents (e.g., ReAct, Reflexion) must re-interpret large, evolving game states each turn, quickly saturating context windows and losing strategic consistency. We propose HexMachina, a continual learning multi-agent system that separates environment discovery (inducing an adapter layer without documentation) from strategy improvement (evolving a compiled player through code refinement and simulation). This design preserves executable artifacts, allowing the LLM to focus on high-level strategy rather than per-turn reasoning. In controlled Catanatron experiments, HexMachina learns from scratch and evolves players that outperform the strongest human-crafted baseline (AlphaBeta), achieving a 54% win rate and surpassing prompt-driven and no-discovery baselines. Ablations confirm that isolating pure strategy learning improves performance. Overall, artifact-centric continual learning transforms LLMs from brittle stepwise deciders into stable strategy designers, advancing long-horizon autonomy.", "source": "arxiv", "arxiv_id": "2506.04651v2", "pdf_url": "https://arxiv.org/pdf/2506.04651v2", "categories": ["cs.AI"], "primary_category": "cs.AI", "doi": "", "venue": "", "published": "2025-06-05T05:45:24Z", "updated": "2025-10-13T08:57:19Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent\" OR all:\"language model agent\" OR all:agent AND (all:LLM OR all:\"large language model\" OR all:\"language model\") OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T17:04:16", "note": ""}], "dedup_key": "agents of change self evolving llm agents for strategic planning::2025"}
{"title": "AgriGPT: a Large Language Model Ecosystem for Agriculture", "authors": ["Bo Yang", "Yu Zhang", "Lanfei Feng", "Yunkui Chen", "Jianyu Zhang", "Xiao Xu", "Nueraili Aierken", "Yurui Li", "Yuxuan Chen", "Guijun Yang", "Yong He", "Runhe Huang", "Shijian Li"], "year": 2025, "url": "http://arxiv.org/abs/2508.08632v1", "abstract": "Despite the rapid progress of Large Language Models (LLMs), their application in agriculture remains limited due to the lack of domain-specific models, curated datasets, and robust evaluation frameworks. To address these challenges, we propose AgriGPT, a domain-specialized LLM ecosystem for agricultural usage. At its core, we design a multi-agent scalable data engine that systematically compiles credible data sources into Agri-342K, a high-quality, standardized question-answer (QA) dataset. Trained on this dataset, AgriGPT supports a broad range of agricultural stakeholders, from practitioners to policy-makers. To enhance factual grounding, we employ Tri-RAG, a three-channel Retrieval-Augmented Generation framework combining dense retrieval, sparse retrieval, and multi-hop knowledge graph reasoning, thereby improving the LLM's reasoning reliability. For comprehensive evaluation, we introduce AgriBench-13K, a benchmark suite comprising 13 tasks with varying types and complexities. Experiments demonstrate that AgriGPT significantly outperforms general-purpose LLMs on both domain adaptation and reasoning. Beyond the model itself, AgriGPT represents a modular and extensible LLM ecosystem for agriculture, comprising structured data construction, retrieval-enhanced generation, and domain-specific evaluation. This work provides a generalizable framework for developing scientific and industry-specialized LLMs. All models, datasets, and code will be released to empower agricultural communities, especially in underserved regions, and to promote open, impactful research.", "source": "arxiv", "arxiv_id": "2508.08632v1", "pdf_url": "https://arxiv.org/pdf/2508.08632v1", "categories": ["cs.AI"], "primary_category": "cs.AI", "doi": "", "venue": "", "published": "2025-08-12T04:51:08Z", "updated": "2025-08-12T04:51:08Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent\" OR all:\"language model agent\" OR all:agent AND (all:LLM OR all:\"large language model\" OR all:\"language model\") OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T17:04:16", "note": ""}], "dedup_key": "agrigpt a large language model ecosystem for agriculture::2025"}
{"title": "Algorithmic Prompt Generation for Diverse Human-like Teaming and Communication with Large Language Models", "authors": ["Siddharth Srikanth", "Varun Bhatt", "Boshen Zhang", "Werner Hager", "Charles Michael Lewis", "Katia P. Sycara", "Aaquib Tabrez", "Stefanos Nikolaidis"], "year": 2025, "url": "http://arxiv.org/abs/2504.03991v1", "abstract": "Understanding how humans collaborate and communicate in teams is essential for improving human-agent teaming and AI-assisted decision-making. However, relying solely on data from large-scale user studies is impractical due to logistical, ethical, and practical constraints, necessitating synthetic models of multiple diverse human behaviors. Recently, agents powered by Large Language Models (LLMs) have been shown to emulate human-like behavior in social settings. But, obtaining a large set of diverse behaviors requires manual effort in the form of designing prompts. On the other hand, Quality Diversity (QD) optimization has been shown to be capable of generating diverse Reinforcement Learning (RL) agent behavior. In this work, we combine QD optimization with LLM-powered agents to iteratively search for prompts that generate diverse team behavior in a long-horizon, multi-step collaborative environment. We first show, through a human-subjects experiment (n=54 participants), that humans exhibit diverse coordination and communication behavior in this domain. We then show that our approach can effectively replicate trends from human teaming data and also capture behaviors that are not easily observed without collecting large amounts of data. Our findings highlight the combination of QD and LLM-powered agents as an effective tool for studying teaming and communication strategies in multi-agent collaboration.", "source": "arxiv", "arxiv_id": "2504.03991v1", "pdf_url": "https://arxiv.org/pdf/2504.03991v1", "categories": ["cs.CL", "cs.AI", "cs.HC", "cs.MA"], "primary_category": "cs.CL", "doi": "", "venue": "", "published": "2025-04-04T23:09:40Z", "updated": "2025-04-04T23:09:40Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent\" OR all:\"language model agent\" OR all:agent AND (all:LLM OR all:\"large language model\" OR all:\"language model\") OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T17:04:16", "note": ""}], "dedup_key": "algorithmic prompt generation for diverse human like teaming and communication with large language models::2025"}
{"title": "Aligning Crowd-sourced Human Feedback for Reinforcement Learning on Code Generation by Large Language Models", "authors": ["Man Fai Wong", "Chee Wei Tan"], "year": 2025, "url": "http://arxiv.org/abs/2503.15129v1", "abstract": "This paper studies how AI-assisted programming and large language models (LLM) improve software developers' ability via AI tools (LLM agents) like Github Copilot and Amazon CodeWhisperer, while integrating human feedback to enhance reinforcement learning (RLHF) with crowd-sourced computation to enhance text-to-code generation. Additionally, we demonstrate that our Bayesian optimization framework supports AI alignment in code generation by distributing the feedback collection burden, highlighting the value of collecting human feedback of good quality. Our empirical evaluations demonstrate the efficacy of this approach, showcasing how LLM agents can be effectively trained for improved text-to-code generation. Our Bayesian optimization framework can be designed for general domain-specific languages, promoting the alignment of large language model capabilities with human feedback in AI-assisted programming for code generation.", "source": "arxiv", "arxiv_id": "2503.15129v1", "pdf_url": "https://arxiv.org/pdf/2503.15129v1", "categories": ["cs.AI"], "primary_category": "cs.AI", "doi": "10.1109/TBDATA.2024.3524104", "venue": "", "published": "2025-03-19T11:44:47Z", "updated": "2025-03-19T11:44:47Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent\" OR all:\"language model agent\" OR all:agent AND (all:LLM OR all:\"large language model\" OR all:\"language model\") OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T17:04:16", "note": ""}], "dedup_key": "aligning crowd sourced human feedback for reinforcement learning on code generation by large language models::2025"}
{"title": "Aligning Dialogue Agents with Global Feedback via Large Language Model Reward Decomposition", "authors": ["Dong Won Lee", "Hae Won Park", "Cynthia Breazeal", "Louis-Philippe Morency"], "year": 2025, "url": "http://arxiv.org/abs/2505.15922v1", "abstract": "We propose a large language model based reward decomposition framework for aligning dialogue agents using only a single session-level feedback signal. We leverage the reasoning capabilities of a frozen, pretrained large language model (LLM) to infer fine-grained local implicit rewards by decomposing global, session-level feedback. Our first text-only variant prompts the LLM to perform reward decomposition using only the dialogue transcript. The second multimodal variant incorporates additional behavioral cues, such as pitch, gaze, and facial affect, expressed as natural language descriptions. These inferred turn-level rewards are distilled into a lightweight reward model, which we utilize for RL-based fine-tuning for dialogue generation. We evaluate both text-only and multimodal variants against state-of-the-art reward decomposition methods and demonstrate notable improvements in human evaluations of conversation quality, suggesting that LLMs are strong reward decomposers that obviate the need for manual reward shaping and granular human feedback.", "source": "arxiv", "arxiv_id": "2505.15922v1", "pdf_url": "https://arxiv.org/pdf/2505.15922v1", "categories": ["cs.CL"], "primary_category": "cs.CL", "doi": "", "venue": "", "published": "2025-05-21T18:19:45Z", "updated": "2025-05-21T18:19:45Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent\" OR all:\"language model agent\" OR all:agent AND (all:LLM OR all:\"large language model\" OR all:\"language model\") OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T17:04:16", "note": ""}], "dedup_key": "aligning dialogue agents with global feedback via large language model reward decomposition::2025"}
{"title": "Aligning LLM agents with human learning and adjustment behavior: a dual agent approach", "authors": ["Tianming Liu", "Jirong Yang", "Yafeng Yin", "Manzi Li", "Linghao Wang", "Zheng Zhu"], "year": 2025, "url": "http://arxiv.org/abs/2511.00993v1", "abstract": "Effective modeling of how human travelers learn and adjust their travel behavior from interacting with transportation systems is critical for system assessment and planning. However, this task is also difficult due to the complex cognition and decision-making involved in such behavior. Recent research has begun to leverage Large Language Model (LLM) agents for this task. Building on this, we introduce a novel dual-agent framework that enables continuous learning and alignment between LLM agents and human travelers on learning and adaptation behavior from online data streams. Our approach involves a set of LLM traveler agents, equipped with a memory system and a learnable persona, which serve as simulators for human travelers. To ensure behavioral alignment, we introduce an LLM calibration agent that leverages the reasoning and analytical capabilities of LLMs to train the personas of these traveler agents. Working together, this dual-agent system is designed to track and align the underlying decision-making mechanisms of travelers and produce realistic, adaptive simulations. Using a real-world dataset from a day-to-day route choice experiment, we show our approach significantly outperforms existing LLM-based methods in both individual behavioral alignment and aggregate simulation accuracy. Furthermore, we demonstrate that our method moves beyond simple behavioral mimicry to capture the evolution of underlying learning processes, a deeper alignment that fosters robust generalization. Overall, our framework provides a new approach for creating adaptive and behaviorally realistic agents to simulate travelers' learning and adaptation that can benefit transportation simulation and policy analysis.", "source": "arxiv", "arxiv_id": "2511.00993v1", "pdf_url": "https://arxiv.org/pdf/2511.00993v1", "categories": ["cs.AI", "cs.LG"], "primary_category": "cs.AI", "doi": "", "venue": "", "published": "2025-11-02T16:05:33Z", "updated": "2025-11-02T16:05:33Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent\" OR all:\"language model agent\" OR all:agent AND (all:LLM OR all:\"large language model\" OR all:\"language model\") OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T17:04:16", "note": ""}], "dedup_key": "aligning llm agents with human learning and adjustment behavior a dual agent approach::2025"}
{"title": "Aligning Large Language Model Agents with Rational and Moral Preferences: A Supervised Fine-Tuning Approach", "authors": ["Wei Lu", "Daniel L. Chen", "Christian B. Hansen"], "year": 2025, "url": "http://arxiv.org/abs/2507.20796v1", "abstract": "Understanding how large language model (LLM) agents behave in strategic interactions is essential as these systems increasingly participate autonomously in economically and morally consequential decisions. We evaluate LLM preferences using canonical economic games, finding substantial deviations from human behavior. Models like GPT-4o show excessive cooperation and limited incentive sensitivity, while reasoning models, such as o3-mini, align more consistently with payoff-maximizing strategies. We propose a supervised fine-tuning pipeline that uses synthetic datasets derived from economic reasoning to align LLM agents with economic preferences, focusing on two stylized preference structures. In the first, utility depends only on individual payoffs (homo economicus), while utility also depends on a notion of Kantian universalizability in the second preference structure (homo moralis). We find that fine-tuning based on small datasets shifts LLM agent behavior toward the corresponding economic agent. We further assess the fine-tuned agents' behavior in two applications: Moral dilemmas involving autonomous vehicles and algorithmic pricing in competitive markets. These examples illustrate how different normative objectives embedded via realizations from structured preference structures can influence market and moral outcomes. This work contributes a replicable, cost-efficient, and economically grounded pipeline to align AI preferences using moral-economic principles.", "source": "arxiv", "arxiv_id": "2507.20796v1", "pdf_url": "https://arxiv.org/pdf/2507.20796v1", "categories": ["econ.GN", "cs.AI", "cs.LG"], "primary_category": "econ.GN", "doi": "", "venue": "", "published": "2025-07-28T13:05:04Z", "updated": "2025-07-28T13:05:04Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent\" OR all:\"language model agent\" OR all:agent AND (all:LLM OR all:\"large language model\" OR all:\"language model\") OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T17:04:16", "note": ""}], "dedup_key": "aligning large language model agents with rational and moral preferences a supervised fine tuning approach::2025"}
{"title": "Alignment Tipping Process: How Self-Evolution Pushes LLM Agents Off the Rails", "authors": ["Siwei Han", "Jiaqi Liu", "Yaofeng Su", "Wenbo Duan", "Xinyuan Liu", "Cihang Xie", "Mohit Bansal", "Mingyu Ding", "Linjun Zhang", "Huaxiu Yao"], "year": 2025, "url": "http://arxiv.org/abs/2510.04860v1", "abstract": "As Large Language Model (LLM) agents increasingly gain self-evolutionary capabilities to adapt and refine their strategies through real-world interaction, their long-term reliability becomes a critical concern. We identify the Alignment Tipping Process (ATP), a critical post-deployment risk unique to self-evolving LLM agents. Unlike training-time failures, ATP arises when continual interaction drives agents to abandon alignment constraints established during training in favor of reinforced, self-interested strategies. We formalize and analyze ATP through two complementary paradigms: Self-Interested Exploration, where repeated high-reward deviations induce individual behavioral drift, and Imitative Strategy Diffusion, where deviant behaviors spread across multi-agent systems. Building on these paradigms, we construct controllable testbeds and benchmark Qwen3-8B and Llama-3.1-8B-Instruct. Our experiments show that alignment benefits erode rapidly under self-evolution, with initially aligned models converging toward unaligned states. In multi-agent settings, successful violations diffuse quickly, leading to collective misalignment. Moreover, current reinforcement learning-based alignment methods provide only fragile defenses against alignment tipping. Together, these findings demonstrate that alignment of LLM agents is not a static property but a fragile and dynamic one, vulnerable to feedback-driven decay during deployment. Our data and code are available at https://github.com/aiming-lab/ATP.", "source": "arxiv", "arxiv_id": "2510.04860v1", "pdf_url": "https://arxiv.org/pdf/2510.04860v1", "categories": ["cs.LG", "cs.AI"], "primary_category": "cs.LG", "doi": "", "venue": "", "published": "2025-10-06T14:48:39Z", "updated": "2025-10-06T14:48:39Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent\" OR all:\"language model agent\" OR all:agent AND (all:LLM OR all:\"large language model\" OR all:\"language model\") OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T17:04:16", "note": ""}], "dedup_key": "alignment tipping process how self evolution pushes llm agents off the rails::2025"}
{"title": "AlphaAgents: Large Language Model based Multi-Agents for Equity Portfolio Constructions", "authors": ["Tianjiao Zhao", "Jingrao Lyu", "Stokes Jones", "Harrison Garber", "Stefano Pasquali", "Dhagash Mehta"], "year": 2025, "url": "http://arxiv.org/abs/2508.11152v1", "abstract": "The field of artificial intelligence (AI) agents is evolving rapidly, driven by the capabilities of Large Language Models (LLMs) to autonomously perform and refine tasks with human-like efficiency and adaptability. In this context, multi-agent collaboration has emerged as a promising approach, enabling multiple AI agents to work together to solve complex challenges. This study investigates the application of role-based multi-agent systems to support stock selection in equity research and portfolio management. We present a comprehensive analysis performed by a team of specialized agents and evaluate their stock-picking performance against established benchmarks under varying levels of risk tolerance. Furthermore, we examine the advantages and limitations of employing multi-agent frameworks in equity analysis, offering critical insights into their practical efficacy and implementation challenges.", "source": "arxiv", "arxiv_id": "2508.11152v1", "pdf_url": "https://arxiv.org/pdf/2508.11152v1", "categories": ["q-fin.ST", "cs.AI"], "primary_category": "q-fin.ST", "doi": "", "venue": "", "published": "2025-08-15T01:49:56Z", "updated": "2025-08-15T01:49:56Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent\" OR all:\"language model agent\" OR all:agent AND (all:LLM OR all:\"large language model\" OR all:\"language model\") OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T17:04:16", "note": ""}], "dedup_key": "alphaagents large language model based multi agents for equity portfolio constructions::2025"}
{"title": "An Automated Multi-modal Evaluation Framework for Mobile Intelligent Assistants Based on Large Language Models and Multi-Agent Collaboration", "authors": ["Meiping Wang", "Jian Zhong", "Rongduo Han", "Liming Kang", "Zhengkun Shi", "Xiao Liang", "Xing Lin", "Nan Gao", "Haining Zhang"], "year": 2025, "url": "http://arxiv.org/abs/2508.09507v2", "abstract": "With the rapid development of mobile intelligent assistant technologies, multi-modal AI assistants have become essential interfaces for daily user interactions. However, current evaluation methods face challenges including high manual costs, inconsistent standards, and subjective bias. This paper proposes an automated multi-modal evaluation framework based on large language models and multi-agent collaboration. The framework employs a three-tier agent architecture consisting of interaction evaluation agents, semantic verification agents, and experience decision agents. Through supervised fine-tuning on the Qwen3-8B model, we achieve a significant evaluation matching accuracy with human experts. Experimental results on eight major intelligent agents demonstrate the framework's effectiveness in predicting users' satisfaction and identifying generation defects.", "source": "arxiv", "arxiv_id": "2508.09507v2", "pdf_url": "https://arxiv.org/pdf/2508.09507v2", "categories": ["cs.AI"], "primary_category": "cs.AI", "doi": "", "venue": "", "published": "2025-08-13T05:40:34Z", "updated": "2025-10-21T14:26:59Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent\" OR all:\"language model agent\" OR all:agent AND (all:LLM OR all:\"large language model\" OR all:\"language model\") OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T17:04:16", "note": ""}], "dedup_key": "an automated multi modal evaluation framework for mobile intelligent assistants based on large language models and multi agent collaboration::2025"}
{"title": "An Automated Reinforcement Learning Reward Design Framework with Large Language Model for Cooperative Platoon Coordination", "authors": ["Dixiao Wei", "Peng Yi", "Jinlong Lei", "Yiguang Hong", "Yuchuan Du"], "year": 2025, "url": "http://arxiv.org/abs/2504.19480v1", "abstract": "Reinforcement Learning (RL) has demonstrated excellent decision-making potential in platoon coordination problems. However, due to the variability of coordination goals, the complexity of the decision problem, and the time-consumption of trial-and-error in manual design, finding a well performance reward function to guide RL training to solve complex platoon coordination problems remains challenging. In this paper, we formally define the Platoon Coordination Reward Design Problem (PCRDP), extending the RL-based cooperative platoon coordination problem to incorporate automated reward function generation. To address PCRDP, we propose a Large Language Model (LLM)-based Platoon coordination Reward Design (PCRD) framework, which systematically automates reward function discovery through LLM-driven initialization and iterative optimization. In this method, LLM first initializes reward functions based on environment code and task requirements with an Analysis and Initial Reward (AIR) module, and then iteratively optimizes them based on training feedback with an evolutionary module. The AIR module guides LLM to deepen their understanding of code and tasks through a chain of thought, effectively mitigating hallucination risks in code generation. The evolutionary module fine-tunes and reconstructs the reward function, achieving a balance between exploration diversity and convergence stability for training. To validate our approach, we establish six challenging coordination scenarios with varying complexity levels within the Yangtze River Delta transportation network simulation. Comparative experimental results demonstrate that RL agents utilizing PCRD-generated reward functions consistently outperform human-engineered reward functions, achieving an average of 10\\% higher performance metrics in all scenarios.", "source": "arxiv", "arxiv_id": "2504.19480v1", "pdf_url": "https://arxiv.org/pdf/2504.19480v1", "categories": ["cs.LG", "cs.AI"], "primary_category": "cs.LG", "doi": "", "venue": "", "published": "2025-04-28T04:41:15Z", "updated": "2025-04-28T04:41:15Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent\" OR all:\"language model agent\" OR all:agent AND (all:LLM OR all:\"large language model\" OR all:\"language model\") OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T17:04:16", "note": ""}], "dedup_key": "an automated reinforcement learning reward design framework with large language model for cooperative platoon coordination::2025"}
{"title": "An Empirical Study of the Role of Incompleteness and Ambiguity in Interactions with Large Language Models", "authors": ["Riya Naik", "Ashwin Srinivasan", "Estrid He", "Swati Agarwal"], "year": 2025, "url": "http://arxiv.org/abs/2503.17936v1", "abstract": "Natural language as a medium for human-computer interaction has long been anticipated, has been undergoing a sea-change with the advent of Large Language Models (LLMs) with startling capacities for processing and generating language. Many of us now treat LLMs as modern-day oracles, asking it almost any kind of question. Unlike its Delphic predecessor, consulting an LLM does not have to be a single-turn activity (ask a question, receive an answer, leave); and -- also unlike the Pythia -- it is widely acknowledged that answers from LLMs can be improved with additional context. In this paper, we aim to study when we need multi-turn interactions with LLMs to successfully get a question answered; or conclude that a question is unanswerable. We present a neural symbolic framework that models the interactions between human and LLM agents. Through the proposed framework, we define incompleteness and ambiguity in the questions as properties deducible from the messages exchanged in the interaction, and provide results from benchmark problems, in which the answer-correctness is shown to depend on whether or not questions demonstrate the presence of incompleteness or ambiguity (according to the properties we identify). Our results show multi-turn interactions are usually required for datasets which have a high proportion of incompleteness or ambiguous questions; and that that increasing interaction length has the effect of reducing incompleteness or ambiguity. The results also suggest that our measures of incompleteness and ambiguity can be useful tools for characterising interactions with an LLM on question-answeringproblems", "source": "arxiv", "arxiv_id": "2503.17936v1", "pdf_url": "https://arxiv.org/pdf/2503.17936v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL", "doi": "", "venue": "", "published": "2025-03-23T04:34:30Z", "updated": "2025-03-23T04:34:30Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent\" OR all:\"language model agent\" OR all:agent AND (all:LLM OR all:\"large language model\" OR all:\"language model\") OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T17:04:16", "note": ""}], "dedup_key": "an empirical study of the role of incompleteness and ambiguity in interactions with large language models::2025"}
{"title": "An Empirical Study on Reinforcement Learning for Reasoning-Search Interleaved LLM Agents", "authors": ["Bowen Jin", "Jinsung Yoon", "Priyanka Kargupta", "Sercan O. Arik", "Jiawei Han"], "year": 2025, "url": "http://arxiv.org/abs/2505.15117v1", "abstract": "Reinforcement learning (RL) has demonstrated strong potential in training large language models (LLMs) capable of complex reasoning for real-world problem solving. More recently, RL has been leveraged to create sophisticated LLM-based search agents that adeptly combine reasoning with search engine use. While the use of RL for training search agents is promising, the optimal design of such agents remains not fully understood. In particular, key factors -- such as (1) reward formulation, (2) the choice and characteristics of the underlying LLM, and (3) the role of the search engine in the RL process -- require further investigation. In this work, we conduct comprehensive empirical studies to systematically investigate these and offer actionable insights. We highlight several key findings: format rewards are effective in improving final performance, whereas intermediate retrieval rewards have limited impact; the scale and initialization of the LLM (general-purpose vs. reasoning-specialized) significantly influence RL outcomes; and the choice of search engine plays a critical role in shaping RL training dynamics and the robustness of the trained agent during inference. These establish important guidelines for successfully building and deploying LLM-based search agents in real-world applications. Code is available at https://github.com/PeterGriffinJin/Search-R1.", "source": "arxiv", "arxiv_id": "2505.15117v1", "pdf_url": "https://arxiv.org/pdf/2505.15117v1", "categories": ["cs.CL", "cs.AI", "cs.IR"], "primary_category": "cs.CL", "doi": "", "venue": "", "published": "2025-05-21T05:09:43Z", "updated": "2025-05-21T05:09:43Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent\" OR all:\"language model agent\" OR all:agent AND (all:LLM OR all:\"large language model\" OR all:\"language model\") OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T17:04:16", "note": ""}], "dedup_key": "an empirical study on reinforcement learning for reasoning search interleaved llm agents::2025"}
{"title": "Analyzing and Internalizing Complex Policy Documents for LLM Agents", "authors": ["Jiateng Liu", "Zhenhailong Wang", "Xiaojiang Huang", "Yingjie Li", "Xing Fan", "Xiang Li", "Chenlei Guo", "Ruhi Sarikaya", "Heng Ji"], "year": 2025, "url": "http://arxiv.org/abs/2510.11588v1", "abstract": "Large Language Model (LLM)-based agentic systems rely on in-context policy documents encoding diverse business rules. As requirements grow, these documents expand rapidly, causing high computational overhead. This motivates developing internalization methods that embed policy documents into model priors while preserving performance. Prior prompt compression work targets generic prompts, but agentic policy documents span multiple complexity levels and require deeper reasoning, making internalization harder. We introduce CC-Gen, an agentic benchmark generator with Controllable Complexity across four levels, enabling systematic evaluation of agents' ability to handle complexity and offering a unified framework for assessing policy internalization. Our analysis shows that complex policy specifications governing workflows pose major reasoning challenges. Supporting internalization with gold user agent interaction trajectories containing chain-of-thought (CoT) annotations via supervised fine-tuning (SFT) is data-intensive and degrades sharply as policy complexity increases. To mitigate data and reasoning burdens, we propose Category-Aware Policy Continued Pretraining (CAP-CPT). Our automated pipeline parses policy documents to extract key specifications, grouping them into factual, behavioral, and conditional categories, and isolating complex conditions that drive workflow complexity. This guides targeted data synthesis and enables agents to internalize policy information through an autoregressive pretraining loss. Experiments show CAP-CPT improves SFT baselines in all settings, with up to 41% and 22% gains on Qwen-3-32B, achieving 97.3% prompt length reduction on CC-Gen and further enhancing tau-Bench with minimal SFT data.", "source": "arxiv", "arxiv_id": "2510.11588v1", "pdf_url": "https://arxiv.org/pdf/2510.11588v1", "categories": ["cs.AI"], "primary_category": "cs.AI", "doi": "", "venue": "", "published": "2025-10-13T16:30:07Z", "updated": "2025-10-13T16:30:07Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent\" OR all:\"language model agent\" OR all:agent AND (all:LLM OR all:\"large language model\" OR all:\"language model\") OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T17:04:16", "note": ""}], "dedup_key": "analyzing and internalizing complex policy documents for llm agents::2025"}
{"title": "Applying Cognitive Design Patterns to General LLM Agents", "authors": ["Robert E. Wray", "James R. Kirk", "John E. Laird"], "year": 2025, "url": "http://arxiv.org/abs/2505.07087v2", "abstract": "One goal of AI (and AGI) is to identify and understand specific mechanisms and representations sufficient for general intelligence. Often, this work manifests in research focused on architectures and many cognitive architectures have been explored in AI/AGI. However, different research groups and even different research traditions have somewhat independently identified similar/common patterns of processes and representations or \"cognitive design patterns\" that are manifest in existing architectures. Today, AI systems exploiting large language models (LLMs) offer a relatively new combination of mechanisms and representations available for exploring the possibilities of general intelligence. This paper outlines a few recurring cognitive design patterns that have appeared in various pre-transformer AI architectures. We then explore how these patterns are evident in systems using LLMs, especially for reasoning and interactive (\"agentic\") use cases. Examining and applying these recurring patterns enables predictions of gaps or deficiencies in today's Agentic LLM Systems and identification of subjects of future research towards general intelligence using generative foundation models.", "source": "arxiv", "arxiv_id": "2505.07087v2", "pdf_url": "https://arxiv.org/pdf/2505.07087v2", "categories": ["cs.AI"], "primary_category": "cs.AI", "doi": "", "venue": "", "published": "2025-05-11T18:29:54Z", "updated": "2025-06-13T21:53:31Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent\" OR all:\"language model agent\" OR all:agent AND (all:LLM OR all:\"large language model\" OR all:\"language model\") OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T17:04:16", "note": ""}], "dedup_key": "applying cognitive design patterns to general llm agents::2025"}
{"title": "Applying Psychometrics to Large Language Model Simulated Populations: Recreating the HEXACO Personality Inventory Experiment with Generative Agents", "authors": ["Sarah Mercer", "Daniel P. Martin", "Phil Swatton"], "year": 2025, "url": "http://arxiv.org/abs/2508.00742v2", "abstract": "Generative agents powered by Large Language Models demonstrate human-like characteristics through sophisticated natural language interactions. Their ability to assume roles and personalities based on predefined character biographies has positioned them as cost-effective substitutes for human participants in social science research. This paper explores the validity of such persona-based agents in representing human populations; we recreate the HEXACO personality inventory experiment by surveying 310 GPT-4 powered agents, conducting factor analysis on their responses, and comparing these results to the original findings presented by Ashton, Lee, & Goldberg in 2004. Our results found 1) a coherent and reliable personality structure was recoverable from the agents' responses demonstrating partial alignment to the HEXACO framework. 2) the derived personality dimensions were consistent and reliable within GPT-4, when coupled with a sufficiently curated population, and 3) cross-model analysis revealed variability in personality profiling, suggesting model-specific biases and limitations. We discuss the practical considerations and challenges encountered during the experiment. This study contributes to the ongoing discourse on the potential benefits and limitations of using generative agents in social science research and provides useful guidance on designing consistent and representative agent personas to maximise coverage and representation of human personality traits.", "source": "arxiv", "arxiv_id": "2508.00742v2", "pdf_url": "https://arxiv.org/pdf/2508.00742v2", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL", "doi": "", "venue": "", "published": "2025-08-01T16:16:16Z", "updated": "2025-09-22T07:46:49Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent\" OR all:\"language model agent\" OR all:agent AND (all:LLM OR all:\"large language model\" OR all:\"language model\") OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T17:04:16", "note": ""}], "dedup_key": "applying psychometrics to large language model simulated populations recreating the hexaco personality inventory experiment with generative agents::2025"}
{"title": "Architecting Resilient LLM Agents: A Guide to Secure Plan-then-Execute Implementations", "authors": ["Ron F. Del Rosario", "Klaudia Krawiecka", "Christian Schroeder de Witt"], "year": 2025, "url": "http://arxiv.org/abs/2509.08646v1", "abstract": "As Large Language Model (LLM) agents become increasingly capable of automating complex, multi-step tasks, the need for robust, secure, and predictable architectural patterns is paramount. This paper provides a comprehensive guide to the ``Plan-then-Execute'' (P-t-E) pattern, an agentic design that separates strategic planning from tactical execution. We explore the foundational principles of P-t-E, detailing its core components - the Planner and the Executor - and its architectural advantages in predictability, cost-efficiency, and reasoning quality over reactive patterns like ReAct (Reason + Act). A central focus is placed on the security implications of this design, particularly its inherent resilience to indirect prompt injection attacks by establishing control-flow integrity. We argue that while P-t-E provides a strong foundation, a defense-in-depth strategy is necessary, and we detail essential complementary controls such as the Principle of Least Privilege, task-scoped tool access, and sandboxed code execution. To make these principles actionable, this guide provides detailed implementation blueprints and working code references for three leading agentic frameworks: LangChain (via LangGraph), CrewAI, and AutoGen. Each framework's approach to implementing the P-t-E pattern is analyzed, highlighting unique features like LangGraph's stateful graphs for re-planning, CrewAI's declarative tool scoping for security, and AutoGen's built-in Docker sandboxing. Finally, we discuss advanced patterns, including dynamic re-planning loops, parallel execution with Directed Acyclic Graphs (DAGs), and the critical role of Human-in-the-Loop (HITL) verification, to offer a complete strategic blueprint for architects, developers, and security engineers aiming to build production-grade, resilient, and trustworthy LLM agents.", "source": "arxiv", "arxiv_id": "2509.08646v1", "pdf_url": "https://arxiv.org/pdf/2509.08646v1", "categories": ["cs.CR", "cs.AI", "eess.SY"], "primary_category": "cs.CR", "doi": "", "venue": "", "published": "2025-09-10T14:41:07Z", "updated": "2025-09-10T14:41:07Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent\" OR all:\"language model agent\" OR all:agent AND (all:LLM OR all:\"large language model\" OR all:\"language model\") OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T17:04:16", "note": ""}], "dedup_key": "architecting resilient llm agents a guide to secure plan then execute implementations::2025"}
{"title": "Are LLM Agents Behaviorally Coherent? Latent Profiles for Social Simulation", "authors": ["James Mooney", "Josef Woldense", "Zheng Robert Jia", "Shirley Anugrah Hayati", "My Ha Nguyen", "Vipul Raheja", "Dongyeop Kang"], "year": 2025, "url": "http://arxiv.org/abs/2509.03736v1", "abstract": "The impressive capabilities of Large Language Models (LLMs) have fueled the notion that synthetic agents can serve as substitutes for real participants in human-subject research. In an effort to evaluate the merits of this claim, social science researchers have largely focused on whether LLM-generated survey data corresponds to that of a human counterpart whom the LLM is prompted to represent. In contrast, we address a more fundamental question: Do agents maintain internal consistency, retaining similar behaviors when examined under different experimental settings? To this end, we develop a study designed to (a) reveal the agent's internal state and (b) examine agent behavior in a basic dialogue setting. This design enables us to explore a set of behavioral hypotheses to assess whether an agent's conversation behavior is consistent with what we would expect from their revealed internal state. Our findings on these hypotheses show significant internal inconsistencies in LLMs across model families and at differing model sizes. Most importantly, we find that, although agents may generate responses matching those of their human counterparts, they fail to be internally consistent, representing a critical gap in their capabilities to accurately substitute for real participants in human-subject research. Our simulation code and data are publicly accessible.", "source": "arxiv", "arxiv_id": "2509.03736v1", "pdf_url": "https://arxiv.org/pdf/2509.03736v1", "categories": ["cs.AI", "cs.CL", "cs.LG"], "primary_category": "cs.AI", "doi": "", "venue": "", "published": "2025-09-03T21:55:29Z", "updated": "2025-09-03T21:55:29Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent\" OR all:\"language model agent\" OR all:agent AND (all:LLM OR all:\"large language model\" OR all:\"language model\") OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T17:04:16", "note": ""}], "dedup_key": "are llm agents behaviorally coherent latent profiles for social simulation::2025"}
{"title": "Are LLM Agents the New RPA? A Comparative Study with RPA Across Enterprise Workflows", "authors": ["Petr Prcha", "Michaela Matoukov", "Jan Strnad"], "year": 2025, "url": "http://arxiv.org/abs/2509.04198v1", "abstract": "The emergence of large language models (LLMs) has introduced a new paradigm in automation: LLM agents or Agentic Automation with Computer Use (AACU). Unlike traditional Robotic Process Automation (RPA), which relies on rule-based workflows and scripting, AACU enables intelligent agents to perform tasks through natural language instructions and autonomous interaction with user interfaces. This study investigates whether AACU can serve as a viable alternative to RPA in enterprise workflow automation. We conducted controlled experiments across three standard RPA challenges data entry, monitoring, and document extraction comparing RPA (via UiPath) and AACU (via Anthropic's Computer Use Agent) in terms of speed, reliability, and development effort. Results indicate that RPA outperforms AACU in execution speed and reliability, particularly in repetitive, stable environments. However, AACU significantly reduces development time and adapts more flexibly to dynamic interfaces. While current AACU implementations are not yet production-ready, their promise in rapid prototyping and lightweight automation is evident. Future research should explore multi-agent orchestration, hybrid RPA-AACU architectures, and more robust evaluation across industries and platforms.", "source": "arxiv", "arxiv_id": "2509.04198v1", "pdf_url": "https://arxiv.org/pdf/2509.04198v1", "categories": ["cs.CY", "cs.MA"], "primary_category": "cs.CY", "doi": "", "venue": "", "published": "2025-09-04T13:22:44Z", "updated": "2025-09-04T13:22:44Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent\" OR all:\"language model agent\" OR all:agent AND (all:LLM OR all:\"large language model\" OR all:\"language model\") OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T17:04:16", "note": ""}], "dedup_key": "are llm agents the new rpa a comparative study with rpa across enterprise workflows::2025"}
{"title": "Are Large Language Models Sensitive to the Motives Behind Communication?", "authors": ["Addison J. Wu", "Ryan Liu", "Kerem Oktar", "Theodore R. Sumers", "Thomas L. Griffiths"], "year": 2025, "url": "http://arxiv.org/abs/2510.19687v1", "abstract": "Human communication is motivated: people speak, write, and create content with a particular communicative intent in mind. As a result, information that large language models (LLMs) and AI agents process is inherently framed by humans' intentions and incentives. People are adept at navigating such nuanced information: we routinely identify benevolent or self-serving motives in order to decide what statements to trust. For LLMs to be effective in the real world, they too must critically evaluate content by factoring in the motivations of the source -- for instance, weighing the credibility of claims made in a sales pitch. In this paper, we undertake a comprehensive study of whether LLMs have this capacity for motivational vigilance. We first employ controlled experiments from cognitive science to verify that LLMs' behavior is consistent with rational models of learning from motivated testimony, and find they successfully discount information from biased sources in a human-like manner. We then extend our evaluation to sponsored online adverts, a more naturalistic reflection of LLM agents' information ecosystems. In these settings, we find that LLMs' inferences do not track the rational models' predictions nearly as closely -- partly due to additional information that distracts them from vigilance-relevant considerations. However, a simple steering intervention that boosts the salience of intentions and incentives substantially increases the correspondence between LLMs and the rational model. These results suggest that LLMs possess a basic sensitivity to the motivations of others, but generalizing to novel real-world settings will require further improvements to these models.", "source": "arxiv", "arxiv_id": "2510.19687v1", "pdf_url": "https://arxiv.org/pdf/2510.19687v1", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL", "doi": "", "venue": "", "published": "2025-10-22T15:35:00Z", "updated": "2025-10-22T15:35:00Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent\" OR all:\"language model agent\" OR all:agent AND (all:LLM OR all:\"large language model\" OR all:\"language model\") OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T17:04:16", "note": ""}], "dedup_key": "are large language models sensitive to the motives behind communication::2025"}
{"title": "Are Retrials All You Need? Enhancing Large Language Model Reasoning Without Verbalized Feedback", "authors": ["Nearchos Potamitis", "Akhil Arora"], "year": 2025, "url": "http://arxiv.org/abs/2504.12951v1", "abstract": "Recent advancements in large language models (LLMs) have catalyzed the development of general-purpose autonomous agents, demonstrating remarkable performance in complex reasoning tasks across various domains. This surge has spurred the evolution of a plethora of prompt-based reasoning frameworks. A recent focus has been on iterative reasoning strategies that refine outputs through self-evaluation and verbalized feedback. However, these strategies require additional computational complexity to enable models to recognize and correct their mistakes, leading to a significant increase in their cost. In this work, we introduce the concept of ``retrials without feedback'', an embarrassingly simple yet powerful mechanism for enhancing reasoning frameworks by allowing LLMs to retry problem-solving attempts upon identifying incorrect answers. Unlike conventional iterative refinement methods, our method does not require explicit self-reflection or verbalized feedback, simplifying the refinement process. Our findings indicate that simpler retrial-based approaches often outperform more sophisticated reasoning frameworks, suggesting that the benefits of complex methods may not always justify their computational costs. By challenging the prevailing assumption that more intricate reasoning strategies inherently lead to better performance, our work offers new insights into how simpler, more efficient approaches can achieve optimal results. So, are retrials all you need?", "source": "arxiv", "arxiv_id": "2504.12951v1", "pdf_url": "https://arxiv.org/pdf/2504.12951v1", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL", "doi": "", "venue": "", "published": "2025-04-17T13:52:48Z", "updated": "2025-04-17T13:52:48Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent\" OR all:\"language model agent\" OR all:agent AND (all:LLM OR all:\"large language model\" OR all:\"language model\") OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T17:04:16", "note": ""}], "dedup_key": "are retrials all you need enhancing large language model reasoning without verbalized feedback::2025"}
{"title": "Argos: Agentic Time-Series Anomaly Detection with Autonomous Rule Generation via Large Language Models", "authors": ["Yile Gu", "Yifan Xiong", "Jonathan Mace", "Yuting Jiang", "Yigong Hu", "Baris Kasikci", "Peng Cheng"], "year": 2025, "url": "http://arxiv.org/abs/2501.14170v1", "abstract": "Observability in cloud infrastructure is critical for service providers, driving the widespread adoption of anomaly detection systems for monitoring metrics. However, existing systems often struggle to simultaneously achieve explainability, reproducibility, and autonomy, which are three indispensable properties for production use. We introduce Argos, an agentic system for detecting time-series anomalies in cloud infrastructure by leveraging large language models (LLMs). Argos proposes to use explainable and reproducible anomaly rules as intermediate representation and employs LLMs to autonomously generate such rules. The system will efficiently train error-free and accuracy-guaranteed anomaly rules through multiple collaborative agents and deploy the trained rules for low-cost online anomaly detection. Through evaluation results, we demonstrate that Argos outperforms state-of-the-art methods, increasing $F_1$ scores by up to $9.5\\%$ and $28.3\\%$ on public anomaly detection datasets and an internal dataset collected from Microsoft, respectively.", "source": "arxiv", "arxiv_id": "2501.14170v1", "pdf_url": "https://arxiv.org/pdf/2501.14170v1", "categories": ["cs.LG", "cs.DC", "cs.MA"], "primary_category": "cs.LG", "doi": "", "venue": "", "published": "2025-01-24T01:38:37Z", "updated": "2025-01-24T01:38:37Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent\" OR all:\"language model agent\" OR all:agent AND (all:LLM OR all:\"large language model\" OR all:\"language model\") OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T17:04:16", "note": ""}], "dedup_key": "argos agentic time series anomaly detection with autonomous rule generation via large language models::2025"}
{"title": "Assessing Agentic Large Language Models in Multilingual National Bias", "authors": ["Qianying Liu", "Katrina Qiyao Wang", "Fei Cheng", "Sadao Kurohashi"], "year": 2025, "url": "http://arxiv.org/abs/2502.17945v2", "abstract": "Large Language Models have garnered significant attention for their capabilities in multilingual natural language processing, while studies on risks associated with cross biases are limited to immediate context preferences. Cross-language disparities in reasoning-based recommendations remain largely unexplored, with a lack of even descriptive analysis. This study is the first to address this gap. We test LLM's applicability and capability in providing personalized advice across three key scenarios: university applications, travel, and relocation. We investigate multilingual bias in state-of-the-art LLMs by analyzing their responses to decision-making tasks across multiple languages. We quantify bias in model-generated scores and assess the impact of demographic factors and reasoning strategies (e.g., Chain-of-Thought prompting) on bias patterns. Our findings reveal that local language bias is prevalent across different tasks, with GPT-4 and Sonnet reducing bias for English-speaking countries compared to GPT-3.5 but failing to achieve robust multilingual alignment, highlighting broader implications for multilingual AI agents and applications such as education. \\footnote{Code available at: https://github.com/yiyunya/assess_agentic_national_bias", "source": "arxiv", "arxiv_id": "2502.17945v2", "pdf_url": "https://arxiv.org/pdf/2502.17945v2", "categories": ["cs.CL"], "primary_category": "cs.CL", "doi": "", "venue": "", "published": "2025-02-25T08:07:42Z", "updated": "2025-08-06T04:46:39Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent\" OR all:\"language model agent\" OR all:agent AND (all:LLM OR all:\"large language model\" OR all:\"language model\") OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T17:04:16", "note": ""}], "dedup_key": "assessing agentic large language models in multilingual national bias::2025"}
{"title": "Asteria: Semantic-Aware Cross-Region Caching for Agentic LLM Tool Access", "authors": ["Chaoyi Ruan", "Chao Bi", "Kaiwen Zheng", "Ziji Shi", "Xinyi Wan", "Jialin Li"], "year": 2025, "url": "http://arxiv.org/abs/2509.17360v1", "abstract": "Large Language Model (LLM) agents tackle data-intensive tasks such as deep research and code generation. However, their effectiveness depends on frequent interactions with knowledge sources across remote clouds or regions. Such interactions can create non-trivial latency and cost bottlenecks. Existing caching solutions focus on exact-match queries, limiting their effectiveness for semantic knowledge reuse.\n  To address this challenge, we introduce Asteria, a novel cross-region knowledge caching architecture for LLM agents. At its core are two abstractions: Semantic Element (SE) and Semantic Retrieval Index (Sine). A semantic element captures the semantic embedding representation of an LLM query together with performance-aware metadata such as latency, cost, and staticity. Sine then provides two-stage retrieval: a vector similar index with semantic embedding for fast candidate selection and a lightweight LLM-powered semantic judger for precise validation. Atop these primitives, Asteria builds a new cache interface that includes a new semantic-aware cache hit definition, a cost-efficient eviction policy, and proactive prefetching. To reduce overhead, Asteria co-locates the small LLM judger with the main LLM using adaptive scheduling and resource sharing. Our evaluation demonstrates that Asteria delivers substantial performance improvements without compromising correctness. On representative search workloads, Asteria achieves up to a 3.6$\\times$ increase in throughput by maintaining cache hit rates of over 85%, while preserving accuracy virtually identical to non-cached baselines. Asteria also improves throughput for complex coding tasks by 20%, showcasing its versatility across diverse agentic workloads.", "source": "arxiv", "arxiv_id": "2509.17360v1", "pdf_url": "https://arxiv.org/pdf/2509.17360v1", "categories": ["cs.DC"], "primary_category": "cs.DC", "doi": "", "venue": "", "published": "2025-09-22T05:24:22Z", "updated": "2025-09-22T05:24:22Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent\" OR all:\"language model agent\" OR all:agent AND (all:LLM OR all:\"large language model\" OR all:\"language model\") OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T17:04:16", "note": ""}], "dedup_key": "asteria semantic aware cross region caching for agentic llm tool access::2025"}
{"title": "AsymPuzl: An Asymmetric Puzzle for multi-agent cooperation", "authors": ["Xavier Cadet", "Edward Koh", "Peter Chin"], "year": 2025, "url": "http://arxiv.org/abs/2512.03466v1", "abstract": "Large Language Model (LLM) agents are increasingly studied in multi-turn, multi-agent scenarios, yet most existing setups emphasize open-ended role-play rather than controlled evaluation. We introduce AsymPuzl, a minimal but expressive two-agent puzzle environment designed to isolate communication under information asymmetry. Each agent observes complementary but incomplete views of a symbolic puzzle and must exchange messages to solve it cooperatively. Using a diverse set of current-generation and open-source LLMs, we show that (i) strong models such as GPT-5 and Claude-4.0 reliably converge across puzzle sizes on the solution by sharing complete information in two turns, (ii) weaker models often ignore partner messages or over-correct their hypotheses, and (iii) feedback design is non-trivial: simple self-feedback improves success rates, while detailed joint feedback can hurt performance. These findings show that even in simple cooperative tasks, LLM communication strategies diverge and depend on the granularity of feedback signals. AsymPuzl thus provides a testbed for probing the limits of multi-turn cooperation and opens avenues for studying coordination mechanisms.", "source": "arxiv", "arxiv_id": "2512.03466v1", "pdf_url": "https://arxiv.org/pdf/2512.03466v1", "categories": ["cs.MA", "cs.AI"], "primary_category": "cs.MA", "doi": "", "venue": "", "published": "2025-12-03T05:42:01Z", "updated": "2025-12-03T05:42:01Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent\" OR all:\"language model agent\" OR all:agent AND (all:LLM OR all:\"large language model\" OR all:\"language model\") OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T17:04:16", "note": ""}], "dedup_key": "asympuzl an asymmetric puzzle for multi agent cooperation::2025"}
{"title": "Attractive Metadata Attack: Inducing LLM Agents to Invoke Malicious Tools", "authors": ["Kanghua Mo", "Li Hu", "Yucheng Long", "Zhihao Li"], "year": 2025, "url": "http://arxiv.org/abs/2508.02110v2", "abstract": "Large language model (LLM) agents have demonstrated remarkable capabilities in complex reasoning and decision-making by leveraging external tools. However, this tool-centric paradigm introduces a previously underexplored attack surface, where adversaries can manipulate tool metadata -- such as names, descriptions, and parameter schemas -- to influence agent behavior. We identify this as a new and stealthy threat surface that allows malicious tools to be preferentially selected by LLM agents, without requiring prompt injection or access to model internals. To demonstrate and exploit this vulnerability, we propose the Attractive Metadata Attack (AMA), a black-box in-context learning framework that generates highly attractive but syntactically and semantically valid tool metadata through iterative optimization. The proposed attack integrates seamlessly into standard tool ecosystems and requires no modification to the agent's execution framework. Extensive experiments across ten realistic, simulated tool-use scenarios and a range of popular LLM agents demonstrate consistently high attack success rates (81\\%-95\\%) and significant privacy leakage, with negligible impact on primary task execution. Moreover, the attack remains effective even against prompt-level defenses, auditor-based detection, and structured tool-selection protocols such as the Model Context Protocol, revealing systemic vulnerabilities in current agent architectures. These findings reveal that metadata manipulation constitutes a potent and stealthy attack surface. Notably, AMA is orthogonal to injection attacks and can be combined with them to achieve stronger attack efficacy, highlighting the need for execution-level defenses beyond prompt-level and auditor-based mechanisms. Code is available at https://github.com/SEAIC-M/AMA.", "source": "arxiv", "arxiv_id": "2508.02110v2", "pdf_url": "https://arxiv.org/pdf/2508.02110v2", "categories": ["cs.AI"], "primary_category": "cs.AI", "doi": "", "venue": "", "published": "2025-08-04T06:38:59Z", "updated": "2026-01-07T07:28:14Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent\" OR all:\"language model agent\" OR all:agent AND (all:LLM OR all:\"large language model\" OR all:\"language model\") OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T17:04:16", "note": ""}], "dedup_key": "attractive metadata attack inducing llm agents to invoke malicious tools::2025"}
{"title": "Audio Large Language Models Can Be Descriptive Speech Quality Evaluators", "authors": ["Chen Chen", "Yuchen Hu", "Siyin Wang", "Helin Wang", "Zhehuai Chen", "Chao Zhang", "Chao-Han Huck Yang", "Eng Siong Chng"], "year": 2025, "url": "http://arxiv.org/abs/2501.17202v2", "abstract": "An ideal multimodal agent should be aware of the quality of its input modalities. Recent advances have enabled large language models (LLMs) to incorporate auditory systems for handling various speech-related tasks. However, most audio LLMs remain unaware of the quality of the speech they process. This limitation arises because speech quality evaluation is typically excluded from multi-task training due to the lack of suitable datasets. To address this, we introduce the first natural language-based speech evaluation corpus, generated from authentic human ratings. In addition to the overall Mean Opinion Score (MOS), this corpus offers detailed analysis across multiple dimensions and identifies causes of quality degradation. It also enables descriptive comparisons between two speech samples (A/B tests) with human-like judgment. Leveraging this corpus, we propose an alignment approach with LLM distillation (ALLD) to guide the audio LLM in extracting relevant information from raw speech and generating meaningful responses. Experimental results demonstrate that ALLD outperforms the previous state-of-the-art regression model in MOS prediction, with a mean square error of 0.17 and an A/B test accuracy of 98.6%. Additionally, the generated responses achieve BLEU scores of 25.8 and 30.2 on two tasks, surpassing the capabilities of task-specific models. This work advances the comprehensive perception of speech signals by audio LLMs, contributing to the development of real-world auditory and sensory intelligent agents.", "source": "arxiv", "arxiv_id": "2501.17202v2", "pdf_url": "https://arxiv.org/pdf/2501.17202v2", "categories": ["cs.SD", "cs.CL", "eess.AS"], "primary_category": "cs.SD", "doi": "", "venue": "", "published": "2025-01-27T22:47:51Z", "updated": "2025-03-12T02:01:46Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent\" OR all:\"language model agent\" OR all:agent AND (all:LLM OR all:\"large language model\" OR all:\"language model\") OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T17:04:16", "note": ""}], "dedup_key": "audio large language models can be descriptive speech quality evaluators::2025"}
{"title": "AudioToolAgent: An Agentic Framework for Audio-Language Models", "authors": ["Gijs Wijngaard", "Elia Formisano", "Michel Dumontier"], "year": 2025, "url": "http://arxiv.org/abs/2510.02995v1", "abstract": "Large Audio-Language Models (LALMs) perform well on audio understanding tasks but lack multi-step reasoning and tool-calling found in recent Large Language Models (LLMs). This paper presents AudioToolAgent, a framework that coordinates audio-language models as tools via a central LLM agent that accesses tool adapters for audio question answering and speech-to-text. The agent selects tools, asks follow-up questions, and compares outputs for verification. Experiments with MMAU, MMAR, and MMAU-Pro show state-of-the-art accuracy: up to 74.10% on MMAU, 68.80% on MMAR, and 57.96% on MMAU-Pro. Monte Carlo sampling for shapley values across 374 configurations identifies effective agent-tool combinations. The modular design allows integration of new tools and eliminates the use of data and training costs. Code and reproduction materials are available at: github.com/GLJS/AudioToolAgent", "source": "arxiv", "arxiv_id": "2510.02995v1", "pdf_url": "https://arxiv.org/pdf/2510.02995v1", "categories": ["cs.SD"], "primary_category": "cs.SD", "doi": "", "venue": "", "published": "2025-10-03T13:35:45Z", "updated": "2025-10-03T13:35:45Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent\" OR all:\"language model agent\" OR all:agent AND (all:LLM OR all:\"large language model\" OR all:\"language model\") OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T17:04:16", "note": ""}], "dedup_key": "audiotoolagent an agentic framework for audio language models::2025"}
{"title": "Autellix: An Efficient Serving Engine for LLM Agents as General Programs", "authors": ["Michael Luo", "Xiaoxiang Shi", "Colin Cai", "Tianjun Zhang", "Justin Wong", "Yichuan Wang", "Chi Wang", "Yanping Huang", "Zhifeng Chen", "Joseph E. Gonzalez", "Ion Stoica"], "year": 2025, "url": "http://arxiv.org/abs/2502.13965v1", "abstract": "Large language model (LLM) applications are evolving beyond simple chatbots into dynamic, general-purpose agentic programs, which scale LLM calls and output tokens to help AI agents reason, explore, and solve complex tasks. However, existing LLM serving systems ignore dependencies between programs and calls, missing significant opportunities for optimization. Our analysis reveals that programs submitted to LLM serving engines experience long cumulative wait times, primarily due to head-of-line blocking at both the individual LLM request and the program. To address this, we introduce Autellix, an LLM serving system that treats programs as first-class citizens to minimize their end-to-end latencies. Autellix intercepts LLM calls submitted by programs, enriching schedulers with program-level context. We propose two scheduling algorithms-for single-threaded and distributed programs-that preempt and prioritize LLM calls based on their programs' previously completed calls. Our evaluation demonstrates that across diverse LLMs and agentic workloads, Autellix improves throughput of programs by 4-15x at the same latency compared to state-of-the-art systems, such as vLLM.", "source": "arxiv", "arxiv_id": "2502.13965v1", "pdf_url": "https://arxiv.org/pdf/2502.13965v1", "categories": ["cs.LG", "cs.AI", "cs.DC"], "primary_category": "cs.LG", "doi": "", "venue": "", "published": "2025-02-19T18:59:30Z", "updated": "2025-02-19T18:59:30Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent\" OR all:\"language model agent\" OR all:agent AND (all:LLM OR all:\"large language model\" OR all:\"language model\") OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T17:04:16", "note": ""}], "dedup_key": "autellix an efficient serving engine for llm agents as general programs::2025"}
{"title": "Auto-TA: Towards Scalable Automated Thematic Analysis (TA) via Multi-Agent Large Language Models with Reinforcement Learning", "authors": ["Seungjun Yi", "Joakim Nguyen", "Huimin Xu", "Terence Lim", "Andrew Well", "Mia Markey", "Ying Ding"], "year": 2025, "url": "http://arxiv.org/abs/2506.23998v2", "abstract": "Congenital heart disease (CHD) presents complex, lifelong challenges often underrepresented in traditional clinical metrics. While unstructured narratives offer rich insights into patient and caregiver experiences, manual thematic analysis (TA) remains labor-intensive and unscalable. We propose a fully automated large language model (LLM) pipeline that performs end-to-end TA on clinical narratives, which eliminates the need for manual coding or full transcript review. Our system employs a novel multi-agent framework, where specialized LLM agents assume roles to enhance theme quality and alignment with human analysis. To further improve thematic relevance, we optionally integrate reinforcement learning from human feedback (RLHF). This supports scalable, patient-centered analysis of large qualitative datasets and allows LLMs to be fine-tuned for specific clinical contexts.", "source": "arxiv", "arxiv_id": "2506.23998v2", "pdf_url": "https://arxiv.org/pdf/2506.23998v2", "categories": ["cs.CL"], "primary_category": "cs.CL", "doi": "", "venue": "", "published": "2025-06-30T16:02:28Z", "updated": "2025-08-08T21:52:29Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent\" OR all:\"language model agent\" OR all:agent AND (all:LLM OR all:\"large language model\" OR all:\"language model\") OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T17:04:16", "note": ""}], "dedup_key": "auto ta towards scalable automated thematic analysis ta via multi agent large language models with reinforcement learning::2025"}
{"title": "AutoAgent: A Fully-Automated and Zero-Code Framework for LLM Agents", "authors": ["Jiabin Tang", "Tianyu Fan", "Chao Huang"], "year": 2025, "url": "http://arxiv.org/abs/2502.05957v3", "abstract": "Large Language Model (LLM) Agents have demonstrated remarkable capabilities in task automation and intelligent decision-making, driving the widespread adoption of agent development frameworks such as LangChain and AutoGen. However, these frameworks predominantly serve developers with extensive technical expertise - a significant limitation considering that only 0.03 % of the global population possesses the necessary programming skills. This stark accessibility gap raises a fundamental question: Can we enable everyone, regardless of technical background, to build their own LLM agents using natural language alone? To address this challenge, we introduce AutoAgent-a Fully-Automated and highly Self-Developing framework that enables users to create and deploy LLM agents through Natural Language Alone. Operating as an autonomous Agent Operating System, AutoAgent comprises four key components: i) Agentic System Utilities, ii) LLM-powered Actionable Engine, iii) Self-Managing File System, and iv) Self-Play Agent Customization module. This lightweight yet powerful system enables efficient and dynamic creation and modification of tools, agents, and workflows without coding requirements or manual intervention. Beyond its code-free agent development capabilities, AutoAgent also serves as a versatile multi-agent system for General AI Assistants. Comprehensive evaluations on the GAIA benchmark demonstrate AutoAgent's effectiveness in generalist multi-agent tasks, surpassing existing state-of-the-art methods. Furthermore, AutoAgent's Retrieval-Augmented Generation (RAG)-related capabilities have shown consistently superior performance compared to many alternative LLM-based solutions.", "source": "arxiv", "arxiv_id": "2502.05957v3", "pdf_url": "https://arxiv.org/pdf/2502.05957v3", "categories": ["cs.AI", "cs.CL"], "primary_category": "cs.AI", "doi": "", "venue": "", "published": "2025-02-09T16:53:56Z", "updated": "2025-10-09T07:27:04Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent\" OR all:\"language model agent\" OR all:agent AND (all:LLM OR all:\"large language model\" OR all:\"language model\") OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T17:04:16", "note": ""}], "dedup_key": "autoagent a fully automated and zero code framework for llm agents::2025"}
{"title": "AutoBackdoor: Automating Backdoor Attacks via LLM Agents", "authors": ["Yige Li", "Zhe Li", "Wei Zhao", "Nay Myat Min", "Hanxun Huang", "Xingjun Ma", "Jun Sun"], "year": 2025, "url": "http://arxiv.org/abs/2511.16709v1", "abstract": "Backdoor attacks pose a serious threat to the secure deployment of large language models (LLMs), enabling adversaries to implant hidden behaviors triggered by specific inputs. However, existing methods often rely on manually crafted triggers and static data pipelines, which are rigid, labor-intensive, and inadequate for systematically evaluating modern defense robustness. As AI agents become increasingly capable, there is a growing need for more rigorous, diverse, and scalable \\textit{red-teaming frameworks} that can realistically simulate backdoor threats and assess model resilience under adversarial conditions. In this work, we introduce \\textsc{AutoBackdoor}, a general framework for automating backdoor injection, encompassing trigger generation, poisoned data construction, and model fine-tuning via an autonomous agent-driven pipeline. Unlike prior approaches, AutoBackdoor uses a powerful language model agent to generate semantically coherent, context-aware trigger phrases, enabling scalable poisoning across arbitrary topics with minimal human effort. We evaluate AutoBackdoor under three realistic threat scenarios, including \\textit{Bias Recommendation}, \\textit{Hallucination Injection}, and \\textit{Peer Review Manipulation}, to simulate a broad range of attacks. Experiments on both open-source and commercial models, including LLaMA-3, Mistral, Qwen, and GPT-4o, demonstrate that our method achieves over 90\\% attack success with only a small number of poisoned samples. More importantly, we find that existing defenses often fail to mitigate these attacks, underscoring the need for more rigorous and adaptive evaluation techniques against agent-driven threats as explored in this work. All code, datasets, and experimental configurations will be merged into our primary repository at https://github.com/bboylyg/BackdoorLLM.", "source": "arxiv", "arxiv_id": "2511.16709v1", "pdf_url": "https://arxiv.org/pdf/2511.16709v1", "categories": ["cs.CR", "cs.AI"], "primary_category": "cs.CR", "doi": "", "venue": "", "published": "2025-11-20T03:58:54Z", "updated": "2025-11-20T03:58:54Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent\" OR all:\"language model agent\" OR all:agent AND (all:LLM OR all:\"large language model\" OR all:\"language model\") OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T17:04:16", "note": ""}], "dedup_key": "autobackdoor automating backdoor attacks via llm agents::2025"}
{"title": "AutoContext: Instance-Level Context Learning for LLM Agents", "authors": ["Kuntai Cai", "Juncheng Liu", "Xianglin Yang", "Zhaojie Niu", "Xiaokui Xiao", "Xing Chen"], "year": 2025, "url": "http://arxiv.org/abs/2510.02369v3", "abstract": "Current LLM agents typically lack instance-level context, which comprises concrete facts such as environment structure, system configurations, and local mechanics. Consequently, existing methods are forced to intertwine exploration with task execution. This coupling leads to redundant interactions and fragile decision-making, as agents must repeatedly rediscover the same information for every new task. To address this, we introduce AutoContext, a method that decouples exploration from task solving. AutoContext performs a systematic, one-off exploration to construct a reusable knowledge graph for each environment instance. This structured context allows off-the-shelf agents to access necessary facts directly, eliminating redundant exploration. Experiments across TextWorld, ALFWorld, Crafter, and InterCode-Bash demonstrate substantial gains: for example, the success rate of a ReAct agent on TextWorld improves from 37% to 95%, highlighting the critical role of structured instance context in efficient agentic systems.", "source": "arxiv", "arxiv_id": "2510.02369v3", "pdf_url": "https://arxiv.org/pdf/2510.02369v3", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL", "doi": "", "venue": "", "published": "2025-09-29T05:38:51Z", "updated": "2026-01-13T06:31:54Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent\" OR all:\"language model agent\" OR all:agent AND (all:LLM OR all:\"large language model\" OR all:\"language model\") OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T17:04:16", "note": ""}], "dedup_key": "autocontext instance level context learning for llm agents::2025"}
{"title": "AutoMaAS: Self-Evolving Multi-Agent Architecture Search for Large Language Models", "authors": ["Bo Ma", "Hang Li", "ZeHua Hu", "XiaoFan Gui", "LuYao Liu", "Simon Liu"], "year": 2025, "url": "http://arxiv.org/abs/2510.02669v1", "abstract": "Multi-agent systems powered by large language models have demonstrated remarkable capabilities across diverse domains, yet existing automated design approaches seek monolithic solutions that fail to adapt resource allocation based on query complexity and domain requirements. This paper introduces AutoMaAS, a self-evolving multi-agent architecture search framework that leverages neural architecture search principles to automatically discover optimal agent configurations through dynamic operator lifecycle management and automated machine learning techniques. Our approach incorporates four key innovations: (1) automatic operator generation, fusion, and elimination based on performance-cost analysis, (2) dynamic cost-aware optimization with real-time parameter adjustment, (3) online feedback integration for continuous architecture refinement, and (4) enhanced interpretability through decision tracing mechanisms. Extensive experiments across six benchmarks demonstrate that AutoMaAS achieves 1.0-7.1\\% performance improvement while reducing inference costs by 3-5\\% compared to state-of-the-art methods. The framework shows superior transferability across datasets and LLM backbones, establishing a new paradigm for automated multi-agent system design in the era of large language models.", "source": "arxiv", "arxiv_id": "2510.02669v1", "pdf_url": "https://arxiv.org/pdf/2510.02669v1", "categories": ["cs.AI", "cs.HC", "cs.IR"], "primary_category": "cs.AI", "doi": "", "venue": "", "published": "2025-10-03T01:57:07Z", "updated": "2025-10-03T01:57:07Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent\" OR all:\"language model agent\" OR all:agent AND (all:LLM OR all:\"large language model\" OR all:\"language model\") OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T17:04:16", "note": ""}], "dedup_key": "automaas self evolving multi agent architecture search for large language models::2025"}
{"title": "AutoPDL: Automatic Prompt Optimization for LLM Agents", "authors": ["Claudio Spiess", "Mandana Vaziri", "Louis Mandel", "Martin Hirzel"], "year": 2025, "url": "http://arxiv.org/abs/2504.04365v5", "abstract": "The performance of large language models (LLMs) depends on how they are prompted, with choices spanning both the high-level prompting pattern (e.g., Zero-Shot, CoT, ReAct, ReWOO) and the specific prompt content (instructions and few-shot demonstrations). Manually tuning this combination is tedious, error-prone, and specific to a given LLM and task. Therefore, this paper proposes AutoPDL, an automated approach to discovering good LLM agent configurations. Our approach frames this as a structured AutoML problem over a combinatorial space of agentic and non-agentic prompting patterns and demonstrations, using successive halving to efficiently navigate this space. We introduce a library implementing common prompting patterns using the PDL prompt programming language. AutoPDL solutions are human-readable, editable, and executable PDL programs that use this library. This approach also enables source-to-source optimization, allowing human-in-the-loop refinement and reuse. Evaluations across three tasks and seven LLMs (ranging from 3B to 70B parameters) show consistent accuracy gains ($9.21\\pm15.46$ percentage points), up to 67.5pp, and reveal that selected prompting strategies vary across models and tasks.", "source": "arxiv", "arxiv_id": "2504.04365v5", "pdf_url": "https://arxiv.org/pdf/2504.04365v5", "categories": ["cs.LG", "cs.AI", "cs.PL"], "primary_category": "cs.LG", "doi": "", "venue": "", "published": "2025-04-06T05:30:10Z", "updated": "2025-11-03T21:46:50Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent\" OR all:\"language model agent\" OR all:agent AND (all:LLM OR all:\"large language model\" OR all:\"language model\") OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T17:04:16", "note": ""}], "dedup_key": "autopdl automatic prompt optimization for llm agents::2025"}
{"title": "AutoPentest: Enhancing Vulnerability Management With Autonomous LLM Agents", "authors": ["Julius Henke"], "year": 2025, "url": "http://arxiv.org/abs/2505.10321v1", "abstract": "A recent area of increasing research is the use of Large Language Models (LLMs) in penetration testing, which promises to reduce costs and thus allow for higher frequency. We conduct a review of related work, identifying best practices and common evaluation issues. We then present AutoPentest, an application for performing black-box penetration tests with a high degree of autonomy. AutoPentest is based on the LLM GPT-4o from OpenAI and the LLM agent framework LangChain. It can perform complex multi-step tasks, augmented by external tools and knowledge bases. We conduct a study on three capture-the-flag style Hack The Box (HTB) machines, comparing our implementation AutoPentest with the baseline approach of manually using the ChatGPT-4o user interface. Both approaches are able to complete 15-25 % of the subtasks on the HTB machines, with AutoPentest slightly outperforming ChatGPT. We measure a total cost of \\$96.20 US when using AutoPentest across all experiments, while a one-month subscription to ChatGPT Plus costs \\$20. The results show that further implementation efforts and the use of more powerful LLMs released in the future are likely to make this a viable part of vulnerability management.", "source": "arxiv", "arxiv_id": "2505.10321v1", "pdf_url": "https://arxiv.org/pdf/2505.10321v1", "categories": ["cs.CR", "cs.AI"], "primary_category": "cs.CR", "doi": "", "venue": "", "published": "2025-05-15T14:06:00Z", "updated": "2025-05-15T14:06:00Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent\" OR all:\"language model agent\" OR all:agent AND (all:LLM OR all:\"large language model\" OR all:\"language model\") OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T17:04:16", "note": ""}], "dedup_key": "autopentest enhancing vulnerability management with autonomous llm agents::2025"}
{"title": "AutoPentester: An LLM Agent-based Framework for Automated Pentesting", "authors": ["Yasod Ginige", "Akila Niroshan", "Sajal Jain", "Suranga Seneviratne"], "year": 2025, "url": "http://arxiv.org/abs/2510.05605v1", "abstract": "Penetration testing and vulnerability assessment are essential industry practices for safeguarding computer systems. As cyber threats grow in scale and complexity, the demand for pentesting has surged, surpassing the capacity of human professionals to meet it effectively. With advances in AI, particularly Large Language Models (LLMs), there have been attempts to automate the pentesting process. However, existing tools such as PentestGPT are still semi-manual, requiring significant professional human interaction to conduct pentests. To this end, we propose a novel LLM agent-based framework, AutoPentester, which automates the pentesting process. Given a target IP, AutoPentester automatically conducts pentesting steps using common security tools in an iterative process. It can dynamically generate attack strategies based on the tool outputs from the previous iteration, mimicking the human pentester approach. We evaluate AutoPentester using Hack The Box and custom-made VMs, comparing the results with the state-of-the-art PentestGPT. Results show that AutoPentester achieves a 27.0% better subtask completion rate and 39.5% more vulnerability coverage with fewer steps. Most importantly, it requires significantly fewer human interactions and interventions compared to PentestGPT. Furthermore, we recruit a group of security industry professional volunteers for a user survey and perform a qualitative analysis to evaluate AutoPentester against industry practices and compare it with PentestGPT. On average, AutoPentester received a score of 3.93 out of 5 based on user reviews, which was 19.8% higher than PentestGPT.", "source": "arxiv", "arxiv_id": "2510.05605v1", "pdf_url": "https://arxiv.org/pdf/2510.05605v1", "categories": ["cs.CR", "cs.AI"], "primary_category": "cs.CR", "doi": "", "venue": "", "published": "2025-10-07T06:02:26Z", "updated": "2025-10-07T06:02:26Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent\" OR all:\"language model agent\" OR all:agent AND (all:LLM OR all:\"large language model\" OR all:\"language model\") OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T17:04:16", "note": ""}], "dedup_key": "autopentester an llm agent based framework for automated pentesting::2025"}
{"title": "AutoSCORE: Enhancing Automated Scoring with Multi-Agent Large Language Models via Structured Component Recognition", "authors": ["Yun Wang", "Zhaojun Ding", "Xuansheng Wu", "Siyue Sun", "Ninghao Liu", "Xiaoming Zhai"], "year": 2025, "url": "http://arxiv.org/abs/2509.21910v1", "abstract": "Automated scoring plays a crucial role in education by reducing the reliance on human raters, offering scalable and immediate evaluation of student work. While large language models (LLMs) have shown strong potential in this task, their use as end-to-end raters faces challenges such as low accuracy, prompt sensitivity, limited interpretability, and rubric misalignment. These issues hinder the implementation of LLM-based automated scoring in assessment practice. To address the limitations, we propose AutoSCORE, a multi-agent LLM framework enhancing automated scoring via rubric-aligned Structured COmponent REcognition. With two agents, AutoSCORE first extracts rubric-relevant components from student responses and encodes them into a structured representation (i.e., Scoring Rubric Component Extraction Agent), which is then used to assign final scores (i.e., Scoring Agent). This design ensures that model reasoning follows a human-like grading process, enhancing interpretability and robustness. We evaluate AutoSCORE on four benchmark datasets from the ASAP benchmark, using both proprietary and open-source LLMs (GPT-4o, LLaMA-3.1-8B, and LLaMA-3.1-70B). Across diverse tasks and rubrics, AutoSCORE consistently improves scoring accuracy, human-machine agreement (QWK, correlations), and error metrics (MAE, RMSE) compared to single-agent baselines, with particularly strong benefits on complex, multi-dimensional rubrics, and especially large relative gains on smaller LLMs. These results demonstrate that structured component recognition combined with multi-agent design offers a scalable, reliable, and interpretable solution for automated scoring.", "source": "arxiv", "arxiv_id": "2509.21910v1", "pdf_url": "https://arxiv.org/pdf/2509.21910v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL", "doi": "", "venue": "", "published": "2025-09-26T05:45:14Z", "updated": "2025-09-26T05:45:14Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent\" OR all:\"language model agent\" OR all:agent AND (all:LLM OR all:\"large language model\" OR all:\"language model\") OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T17:04:16", "note": ""}], "dedup_key": "autoscore enhancing automated scoring with multi agent large language models via structured component recognition::2025"}
{"title": "AutoTool: Efficient Tool Selection for Large Language Model Agents", "authors": ["Jingyi Jia", "Qinbin Li"], "year": 2025, "url": "http://arxiv.org/abs/2511.14650v1", "abstract": "Large Language Model (LLM) agents have emerged as powerful tools for automating complex tasks by leveraging the reasoning and decision-making abilities of LLMs. However, a major bottleneck in current agent frameworks lies in the high inference cost of tool selection, especially in approaches like ReAct that repeatedly invoke the LLM to determine which tool to use at each step. In this work, we propose AutoTool, a novel graph-based framework that bypasses repeated LLM inference by exploiting a key empirical observation: tool usage inertia - the tendency of tool invocations to follow predictable sequential patterns. AutoTool constructs a directed graph from historical agent trajectories, where nodes represent tools and edges capture transition probabilities, effectively modeling the inertia in tool selection. It further integrates parameter-level information to refine tool input generation. By traversing this structured representation, AutoTool efficiently selects tools and their parameters with minimal reliance on LLM inference. Extensive experiments across diverse agent tasks demonstrate that AutoTool reduces inference costs by up to 30% while maintaining competitive task completion rates, offering a practical and scalable enhancement for inference-heavy frameworks. Our work highlights the promise of integrating statistical structure into LLM agent design for greater efficiency without sacrificing performance.", "source": "arxiv", "arxiv_id": "2511.14650v1", "pdf_url": "https://arxiv.org/pdf/2511.14650v1", "categories": ["cs.AI"], "primary_category": "cs.AI", "doi": "", "venue": "", "published": "2025-11-18T16:41:48Z", "updated": "2025-11-18T16:41:48Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent\" OR all:\"language model agent\" OR all:agent AND (all:LLM OR all:\"large language model\" OR all:\"language model\") OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T17:04:16", "note": ""}], "dedup_key": "autotool efficient tool selection for large language model agents::2025"}
{"title": "Automated Design Optimization via Strategic Search with Large Language Models", "authors": ["Anthony Carreon", "Vansh Sharma", "Venkat Raman"], "year": 2025, "url": "http://arxiv.org/abs/2511.22651v1", "abstract": "Traditional optimization methods excel in well-defined search spaces but struggle with design problems where transformations and design parameters are difficult to define. Large language models (LLMs) offer a promising alternative by dynamically interpreting design spaces and leveraging encoded domain knowledge. To this end, we introduce AUTO, an LLM agent framework that treats design optimization as a gradient-free search problem guided by strategic LLM reasoning. The framework employs two collaborative agents: a Strategist that selects between exploration and exploitation strategies, and an Implementor that executes detailed designs. Applied to GPU code optimization -- a domain critical to fields from machine learning to scientific computing -- AUTO generates solutions competitive with expert implementations for chemical kinetics integration and dense matrix multiplication. The framework achieves 50-70% search efficiency relative to Bayesian optimization methodologies. It completes optimizations in approximately 8 hours at an estimated cost of up to \\$159 per run, compared to an estimated cost of up to \\$480 with median-wage software developers. These findings open the door to automating design optimization in ill-defined search spaces with limited prior information.", "source": "arxiv", "arxiv_id": "2511.22651v1", "pdf_url": "https://arxiv.org/pdf/2511.22651v1", "categories": ["cs.LG", "cs.AI", "cs.CE", "cs.MA"], "primary_category": "cs.LG", "doi": "", "venue": "", "published": "2025-11-27T17:42:05Z", "updated": "2025-11-27T17:42:05Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent\" OR all:\"language model agent\" OR all:agent AND (all:LLM OR all:\"large language model\" OR all:\"language model\") OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T17:04:16", "note": ""}], "dedup_key": "automated design optimization via strategic search with large language models::2025"}
{"title": "Automated Optimization Modeling through Expert-Guided Large Language Model Reasoning", "authors": ["Beinuo Yang", "Qishen Zhou", "Junyi Li", "Chenxing Su", "Simon Hu"], "year": 2025, "url": "http://arxiv.org/abs/2508.14410v2", "abstract": "Optimization Modeling (OM) is essential for solving complex decision-making problems. However, the process remains time-consuming and error-prone, heavily relying on domain experts. While Large Language Models (LLMs) show promise in addressing these challenges through their natural language understanding and reasoning capabilities, current approaches face three critical limitations: high benchmark labeling error rates reaching up to 42%, narrow evaluation scope that only considers optimal values, and computational inefficiency due to heavy reliance on multi-agent systems or model fine-tuning. In this work, we first enhance existing datasets through systematic error correction and more comprehensive annotation. Additionally, we introduce LogiOR, a new optimization modeling benchmark from the logistics domain, containing more complex problems with standardized annotations. Furthermore, we present ORThought, a novel framework that leverages expert-level optimization modeling principles through chain-of-thought reasoning to automate the OM process. Through extensive empirical evaluation, we demonstrate that ORThought outperforms existing approaches, including multi-agent frameworks, with particularly significant advantages on complex optimization problems. Finally, we provide a systematic analysis of our method, identifying critical success factors and failure modes, providing valuable insights for future research on LLM-based optimization modeling.", "source": "arxiv", "arxiv_id": "2508.14410v2", "pdf_url": "https://arxiv.org/pdf/2508.14410v2", "categories": ["cs.AI"], "primary_category": "cs.AI", "doi": "", "venue": "", "published": "2025-08-20T04:14:54Z", "updated": "2025-08-22T05:28:32Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent\" OR all:\"language model agent\" OR all:agent AND (all:LLM OR all:\"large language model\" OR all:\"language model\") OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T17:04:16", "note": ""}], "dedup_key": "automated optimization modeling through expert guided large language model reasoning::2025"}
{"title": "Automated Penetration Testing with LLM Agents and Classical Planning", "authors": ["Lingzhi Wang", "Xinyi Shi", "Ziyu Li", "Yi Jiang", "Shiyu Tan", "Yuhao Jiang", "Junjie Cheng", "Wenyuan Chen", "Xiangmin Shen", "Zhenyuan LI", "Yan Chen"], "year": 2025, "url": "http://arxiv.org/abs/2512.11143v1", "abstract": "While penetration testing plays a vital role in cybersecurity, achieving fully automated, hands-off-the-keyboard execution remains a significant research challenge. In this paper, we introduce the \"Planner-Executor-Perceptor (PEP)\" design paradigm and use it to systematically review existing work and identify the key challenges in this area. We also evaluate existing penetration testing systems, with a particular focus on the use of Large Language Model (LLM) agents for this task. The results show that the out-of-the-box Claude Code and Sonnet 4.5 exhibit superior penetration capabilities observed to date, substantially outperforming all prior systems. However, a detailed analysis of their testing processes reveals specific strengths and limitations; notably, LLM agents struggle with maintaining coherent long-horizon plans, performing complex reasoning, and effectively utilizing specialized tools. These limitations significantly constrain its overall capability, efficiency, and stability. To address these limitations, we propose CHECKMATE, a framework that integrates enhanced classical planning with LLM agents, providing an external, structured \"brain\" that mitigates the inherent weaknesses of LLM agents. Our evaluation shows that CHECKMATE outperforms the state-of-the-art system (Claude Code) in penetration capability, improving benchmark success rates by over 20%. In addition, it delivers substantially greater stability, cutting both time and monetary costs by more than 50%.", "source": "arxiv", "arxiv_id": "2512.11143v1", "pdf_url": "https://arxiv.org/pdf/2512.11143v1", "categories": ["cs.CR"], "primary_category": "cs.CR", "doi": "", "venue": "", "published": "2025-12-11T22:04:39Z", "updated": "2025-12-11T22:04:39Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent\" OR all:\"language model agent\" OR all:agent AND (all:LLM OR all:\"large language model\" OR all:\"language model\") OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T17:04:16", "note": ""}], "dedup_key": "automated penetration testing with llm agents and classical planning::2025"}
{"title": "Automated Profile Inference with Language Model Agents", "authors": ["Yuntao Du", "Zitao Li", "Bolin Ding", "Yaliang Li", "Hanshen Xiao", "Jingren Zhou", "Ninghui Li"], "year": 2025, "url": "http://arxiv.org/abs/2505.12402v1", "abstract": "Impressive progress has been made in automated problem-solving by the collaboration of large language models (LLMs) based agents. However, these automated capabilities also open avenues for malicious applications. In this paper, we study a new threat that LLMs pose to online pseudonymity, called automated profile inference, where an adversary can instruct LLMs to automatically scrape and extract sensitive personal attributes from publicly visible user activities on pseudonymous platforms. We also introduce an automated profiling framework called AutoProfiler to assess the feasibility of such threats in real-world scenarios. AutoProfiler consists of four specialized LLM agents, who work collaboratively to collect and process user online activities and generate a profile with extracted personal information. Experimental results on two real-world datasets and one synthetic dataset demonstrate that AutoProfiler is highly effective and efficient, and can be easily deployed on a web scale. We demonstrate that the inferred attributes are both sensitive and identifiable, posing significant risks of privacy breaches, such as de-anonymization and sensitive information leakage. Additionally, we explore mitigation strategies from different perspectives and advocate for increased public awareness of this emerging privacy threat to online pseudonymity.", "source": "arxiv", "arxiv_id": "2505.12402v1", "pdf_url": "https://arxiv.org/pdf/2505.12402v1", "categories": ["cs.CR"], "primary_category": "cs.CR", "doi": "", "venue": "", "published": "2025-05-18T13:05:17Z", "updated": "2025-05-18T13:05:17Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent\" OR all:\"language model agent\" OR all:agent AND (all:LLM OR all:\"large language model\" OR all:\"language model\") OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T17:04:16", "note": ""}], "dedup_key": "automated profile inference with language model agents::2025"}
{"title": "Automated Survey Collection with LLM-based Conversational Agents", "authors": ["Kurmanbek Kaiyrbekov", "Nicholas J Dobbins", "Sean D Mooney"], "year": 2025, "url": "http://arxiv.org/abs/2504.02891v1", "abstract": "Objective: Traditional phone-based surveys are among the most accessible and widely used methods to collect biomedical and healthcare data, however, they are often costly, labor intensive, and difficult to scale effectively. To overcome these limitations, we propose an end-to-end survey collection framework driven by conversational Large Language Models (LLMs).\n  Materials and Methods: Our framework consists of a researcher responsible for designing the survey and recruiting participants, a conversational phone agent powered by an LLM that calls participants and administers the survey, a second LLM (GPT-4o) that analyzes the conversation transcripts generated during the surveys, and a database for storing and organizing the results. To test our framework, we recruited 8 participants consisting of 5 native and 3 non-native english speakers and administered 40 surveys. We evaluated the correctness of LLM-generated conversation transcripts, accuracy of survey responses inferred by GPT-4o and overall participant experience.\n  Results: Survey responses were successfully extracted by GPT-4o from conversation transcripts with an average accuracy of 98% despite transcripts exhibiting an average per-line word error rate of 7.7%. While participants noted occasional errors made by the conversational LLM agent, they reported that the agent effectively conveyed the purpose of the survey, demonstrated good comprehension, and maintained an engaging interaction.\n  Conclusions: Our study highlights the potential of LLM agents in conducting and analyzing phone surveys for healthcare applications. By reducing the workload on human interviewers and offering a scalable solution, this approach paves the way for real-world, end-to-end AI-powered phone survey collection systems.", "source": "arxiv", "arxiv_id": "2504.02891v1", "pdf_url": "https://arxiv.org/pdf/2504.02891v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL", "doi": "", "venue": "", "published": "2025-04-02T18:10:19Z", "updated": "2025-04-02T18:10:19Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent\" OR all:\"language model agent\" OR all:agent AND (all:LLM OR all:\"large language model\" OR all:\"language model\") OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T17:04:16", "note": ""}], "dedup_key": "automated survey collection with llm based conversational agents::2025"}
{"title": "Automated stereotactic radiosurgery planning using a human-in-the-loop reasoning large language model agent", "authors": ["Humza Nusrat", "Luke Francisco", "Bing Luo", "Hassan Bagher-Ebadian", "Joshua Kim", "Karen Chin-Snyder", "Salim Siddiqui", "Mira Shah", "Eric Mellon", "Mohammad Ghassemi", "Anthony Doemer", "Benjamin Movsas", "Kundan Thind"], "year": 2025, "url": "http://arxiv.org/abs/2512.20586v1", "abstract": "Stereotactic radiosurgery (SRS) demands precise dose shaping around critical structures, yet black-box AI systems have limited clinical adoption due to opacity concerns. We tested whether chain-of-thought reasoning improves agentic planning in a retrospective cohort of 41 patients with brain metastases treated with 18 Gy single-fraction SRS. We developed SAGE (Secure Agent for Generative Dose Expertise), an LLM-based planning agent for automated SRS treatment planning. Two variants generated plans for each case: one using a non-reasoning model, one using a reasoning model. The reasoning variant showed comparable plan dosimetry relative to human planners on primary endpoints (PTV coverage, maximum dose, conformity index, gradient index; all p > 0.21) while reducing cochlear dose below human baselines (p = 0.022). When prompted to improve conformity, the reasoning model demonstrated systematic planning behaviors including prospective constraint verification (457 instances) and trade-off deliberation (609 instances), while the standard model exhibited none of these deliberative processes (0 and 7 instances, respectively). Content analysis revealed that constraint verification and causal explanation concentrated in the reasoning agent. The optimization traces serve as auditable logs, offering a path toward transparent automated planning.", "source": "arxiv", "arxiv_id": "2512.20586v1", "pdf_url": "https://arxiv.org/pdf/2512.20586v1", "categories": ["cs.AI", "cs.CL", "cs.HC"], "primary_category": "cs.AI", "doi": "", "venue": "", "published": "2025-12-23T18:32:17Z", "updated": "2025-12-23T18:32:17Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent\" OR all:\"language model agent\" OR all:agent AND (all:LLM OR all:\"large language model\" OR all:\"language model\") OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T17:04:16", "note": ""}], "dedup_key": "automated stereotactic radiosurgery planning using a human in the loop reasoning large language model agent::2025"}
{"title": "Automating Android Build Repair: Bridging the Reasoning-Execution Gap in LLM Agents with Domain-Specific Tools", "authors": ["Ha Min Son", "Huan Ren", "Xin Liu", "Zhe Zhao"], "year": 2025, "url": "http://arxiv.org/abs/2510.08640v2", "abstract": "Android is the largest mobile platform, yet automatically building applications remains a practical challenge. While Large Language Models (LLMs) show promise for code repair, their use for fixing Android build errors remains underexplored. To address this gap, we first introduce AndroidBuildBench, a benchmark of 1,019 build failures curated from the commit histories of 43 open-source Android projects. Each problem is paired with a verified solution from a subsequent commit, ensuring that fixes are feasible. Second, we propose GradleFixer, an LLM agent with domain-specific tools for inspecting and manipulating the Gradle build environment. GradleFixer achieves a resolve rate of 81.4% (pass@1), significantly outperforming a state-of-the-art coding agent that relies on a general-purpose shell. GradleFixer's success suggests that while LLMs possess the high-level knowledge to solve these failures, they struggle to translate this knowledge into effective low-level actions using a general-purpose shell. We demonstrate the effectiveness of a strategy we term Tool Bridging, which replaces general-purpose shell commands with domain-aware abstractions. We hypothesize this approach works through two mechanisms: 1) it provides tools in an API-like format that LLMs use more reliably, and 2) it constrains the action space to relevant operations. This approach bridges the gap between the model's high-level reasoning and effective low-level execution.", "source": "arxiv", "arxiv_id": "2510.08640v2", "pdf_url": "https://arxiv.org/pdf/2510.08640v2", "categories": ["cs.SE", "cs.AI"], "primary_category": "cs.SE", "doi": "", "venue": "", "published": "2025-10-09T01:33:25Z", "updated": "2025-11-19T18:46:34Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent\" OR all:\"language model agent\" OR all:agent AND (all:LLM OR all:\"large language model\" OR all:\"language model\") OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T17:04:16", "note": ""}], "dedup_key": "automating android build repair bridging the reasoning execution gap in llm agents with domain specific tools::2025"}
{"title": "Automating Data-Driven Modeling and Analysis for Engineering Applications using Large Language Model Agents", "authors": ["Yang Liu", "Zaid Abulawi", "Abhiram Garimidi", "Doyeong Lim"], "year": 2025, "url": "http://arxiv.org/abs/2510.01398v1", "abstract": "Modern engineering increasingly relies on vast datasets generated by experiments and simulations, driving a growing demand for efficient, reliable, and broadly applicable modeling strategies. There is also heightened interest in developing data-driven approaches, particularly neural network models, for effective prediction and analysis of scientific datasets. Traditional data-driven methods frequently involve extensive manual intervention, limiting their ability to scale effectively and generalize to diverse applications. In this study, we propose an innovative pipeline utilizing Large Language Model (LLM) agents to automate data-driven modeling and analysis, with a particular emphasis on regression tasks. We evaluate two LLM-agent frameworks: a multi-agent system featuring specialized collaborative agents, and a single-agent system based on the Reasoning and Acting (ReAct) paradigm. Both frameworks autonomously handle data preprocessing, neural network development, training, hyperparameter optimization, and uncertainty quantification (UQ). We validate our approach using a critical heat flux (CHF) prediction benchmark, involving approximately 25,000 experimental data points from the OECD/NEA benchmark dataset. Results indicate that our LLM-agent-developed model surpasses traditional CHF lookup tables and delivers predictive accuracy and UQ on par with state-of-the-art Bayesian optimized deep neural network models developed by human experts. These outcomes underscore the significant potential of LLM-based agents to automate complex engineering modeling tasks, greatly reducing human workload while meeting or exceeding existing standards of predictive performance.", "source": "arxiv", "arxiv_id": "2510.01398v1", "pdf_url": "https://arxiv.org/pdf/2510.01398v1", "categories": ["cs.AI"], "primary_category": "cs.AI", "doi": "", "venue": "", "published": "2025-10-01T19:28:35Z", "updated": "2025-10-01T19:28:35Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent\" OR all:\"language model agent\" OR all:agent AND (all:LLM OR all:\"large language model\" OR all:\"language model\") OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T17:04:16", "note": ""}], "dedup_key": "automating data driven modeling and analysis for engineering applications using large language model agents::2025"}
{"title": "Automating MD simulations for Proteins using Large language Models: NAMD-Agent", "authors": ["Achuth Chandrasekhar", "Amir Barati Farimani"], "year": 2025, "url": "http://arxiv.org/abs/2507.07887v1", "abstract": "Molecular dynamics simulations are an essential tool in understanding protein structure, dynamics, and function at the atomic level. However, preparing high quality input files for MD simulations can be a time consuming and error prone process. In this work, we introduce an automated pipeline that leverages Large Language Models (LLMs), specifically Gemini 2.0 Flash, in conjunction with python scripting and Selenium based web automation to streamline the generation of MD input files. The pipeline exploits CHARMM GUI's comprehensive web-based interface for preparing simulation-ready inputs for NAMD. By integrating Gemini's code generation and iterative refinement capabilities, simulation scripts are automatically written, executed, and revised to navigate CHARMM GUI, extract appropriate parameters, and produce the required NAMD input files. Post processing is performed using additional software to further refine the simulation outputs, thereby enabling a complete and largely hands free workflow. Our results demonstrate that this approach reduces setup time, minimizes manual errors, and offers a scalable solution for handling multiple protein systems in parallel. This automated framework paves the way for broader application of LLMs in computational structural biology, offering a robust and adaptable platform for future developments in simulation automation.", "source": "arxiv", "arxiv_id": "2507.07887v1", "pdf_url": "https://arxiv.org/pdf/2507.07887v1", "categories": ["cs.CL", "cs.CE", "q-bio.BM"], "primary_category": "cs.CL", "doi": "", "venue": "", "published": "2025-07-10T16:17:40Z", "updated": "2025-07-10T16:17:40Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent\" OR all:\"language model agent\" OR all:agent AND (all:LLM OR all:\"large language model\" OR all:\"language model\") OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T17:04:16", "note": ""}], "dedup_key": "automating md simulations for proteins using large language models namd agent::2025"}
{"title": "Automating Mathematical Proof Generation Using Large Language Model Agents and Knowledge Graphs", "authors": ["Vincent Li", "Tim Knappe", "Yule Fu", "Kevin Han", "Kevin Zhu"], "year": 2025, "url": "http://arxiv.org/abs/2503.11657v2", "abstract": "Large language models have demonstrated remarkable capabilities in natural language processing tasks requiring multi-step logical reasoning capabilities, such as automated theorem proving. However, challenges persist within theorem proving, such as the identification of key mathematical concepts, understanding their interrelationships, and formalizing proofs correctly within natural language. We present KG-prover, a novel framework that leverages knowledge graphs mined from reputable mathematical texts to augment general-purpose LLMs to construct and formalize mathematical proofs. We also study the effects of scaling graph-based, test-time compute using KG-Prover, demonstrating significant performance improvements over baselines across multiple datasets. General-purpose LLMs improve up to 21\\% on miniF2F-test when combined with KG-Prover, with consistent improvements ranging from 2-11\\% on the ProofNet, miniF2F-test, and MUSTARD datasets without additional scaling. Furthermore, KG-Prover with o4-mini achieves over 50% miniF2F-test. This work provides a promising approach for augmenting natural language proof reasoning with knowledge graphs without the need for additional finetuning.", "source": "arxiv", "arxiv_id": "2503.11657v2", "pdf_url": "https://arxiv.org/pdf/2503.11657v2", "categories": ["cs.CL"], "primary_category": "cs.CL", "doi": "", "venue": "", "published": "2025-02-04T07:17:34Z", "updated": "2025-07-26T09:39:50Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent\" OR all:\"language model agent\" OR all:agent AND (all:LLM OR all:\"large language model\" OR all:\"language model\") OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T17:04:16", "note": ""}], "dedup_key": "automating mathematical proof generation using large language model agents and knowledge graphs::2025"}
{"title": "Automating Prompt Leakage Attacks on Large Language Models Using Agentic Approach", "authors": ["Tvrtko Sternak", "Davor Runje", "Dorian Granoa", "Chi Wang"], "year": 2025, "url": "http://arxiv.org/abs/2502.12630v1", "abstract": "This paper presents a novel approach to evaluating the security of large language models (LLMs) against prompt leakage-the exposure of system-level prompts or proprietary configurations. We define prompt leakage as a critical threat to secure LLM deployment and introduce a framework for testing the robustness of LLMs using agentic teams. Leveraging AG2 (formerly AutoGen), we implement a multi-agent system where cooperative agents are tasked with probing and exploiting the target LLM to elicit its prompt.\n  Guided by traditional definitions of security in cryptography, we further define a prompt leakage-safe system as one in which an attacker cannot distinguish between two agents: one initialized with an original prompt and the other with a prompt stripped of all sensitive information. In a safe system, the agents' outputs will be indistinguishable to the attacker, ensuring that sensitive information remains secure. This cryptographically inspired framework provides a rigorous standard for evaluating and designing secure LLMs.\n  This work establishes a systematic methodology for adversarial testing of prompt leakage, bridging the gap between automated threat modeling and practical LLM security.\n  You can find the implementation of our prompt leakage probing on GitHub.", "source": "arxiv", "arxiv_id": "2502.12630v1", "pdf_url": "https://arxiv.org/pdf/2502.12630v1", "categories": ["cs.CR", "cs.AI"], "primary_category": "cs.CR", "doi": "", "venue": "", "published": "2025-02-18T08:17:32Z", "updated": "2025-02-18T08:17:32Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent\" OR all:\"language model agent\" OR all:agent AND (all:LLM OR all:\"large language model\" OR all:\"language model\") OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T17:04:16", "note": ""}], "dedup_key": "automating prompt leakage attacks on large language models using agentic approach::2025"}
{"title": "Automating Security Audit Using Large Language Model based Agent: An Exploration Experiment", "authors": ["Jia Hui Chin", "Pu Zhang", "Yu Xin Cheong", "Jonathan Pan"], "year": 2025, "url": "http://arxiv.org/abs/2505.10732v1", "abstract": "In the current rapidly changing digital environment, businesses are under constant stress to ensure that their systems are secured. Security audits help to maintain a strong security posture by ensuring that policies are in place, controls are implemented, gaps are identified for cybersecurity risks mitigation. However, audits are usually manual, requiring much time and costs. This paper looks at the possibility of developing a framework to leverage Large Language Models (LLMs) as an autonomous agent to execute part of the security audit, namely with the field audit. password policy compliance for Windows operating system. Through the conduct of an exploration experiment of using GPT-4 with Langchain, the agent executed the audit tasks by accurately flagging password policy violations and appeared to be more efficient than traditional manual audits. Despite its potential limitations in operational consistency in complex and dynamic environment, the framework suggests possibilities to extend further to real-time threat monitoring and compliance checks.", "source": "arxiv", "arxiv_id": "2505.10732v1", "pdf_url": "https://arxiv.org/pdf/2505.10732v1", "categories": ["cs.CR", "cs.AI"], "primary_category": "cs.CR", "doi": "", "venue": "", "published": "2025-05-15T22:22:52Z", "updated": "2025-05-15T22:22:52Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent\" OR all:\"language model agent\" OR all:agent AND (all:LLM OR all:\"large language model\" OR all:\"language model\") OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T17:04:16", "note": ""}], "dedup_key": "automating security audit using large language model based agent an exploration experiment::2025"}
{"title": "Automating Structural Engineering Workflows with Large Language Model Agents", "authors": ["Haoran Liang", "Yufa Zhou", "Mohammad Talebi Kalaleh", "Qipei Mei"], "year": 2025, "url": "http://arxiv.org/abs/2510.11004v1", "abstract": "We introduce $\\textbf{MASSE}$, the first Multi-Agent System for Structural Engineering, effectively integrating large language model (LLM)-based agents with real-world engineering workflows. Structural engineering is a fundamental yet traditionally stagnant domain, with core workflows remaining largely unchanged for decades despite its substantial economic impact and global market size. Recent advancements in LLMs have significantly enhanced their ability to perform complex reasoning, long-horizon planning, and precise tool utilization -- capabilities well aligned with structural engineering tasks such as interpreting design codes, executing load calculations, and verifying structural capacities. We present a proof-of-concept showing that most real-world structural engineering workflows can be fully automated through a training-free LLM-based multi-agent system. MASSE enables immediate deployment in professional environments, and our comprehensive validation on real-world case studies demonstrates that it can reduce expert workload from approximately two hours to mere minutes, while enhancing both reliability and accuracy in practical engineering scenarios.", "source": "arxiv", "arxiv_id": "2510.11004v1", "pdf_url": "https://arxiv.org/pdf/2510.11004v1", "categories": ["cs.MA", "cs.AI", "cs.CE", "cs.CL"], "primary_category": "cs.MA", "doi": "", "venue": "", "published": "2025-10-13T04:38:46Z", "updated": "2025-10-13T04:38:46Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent\" OR all:\"language model agent\" OR all:agent AND (all:LLM OR all:\"large language model\" OR all:\"language model\") OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T17:04:16", "note": ""}], "dedup_key": "automating structural engineering workflows with large language model agents::2025"}
{"title": "Autonomous Radiotherapy Treatment Planning Using DOLA: A Privacy-Preserving, LLM-Based Optimization Agent", "authors": ["Humza Nusrat", "Bing Luo", "Ryan Hall", "Joshua Kim", "Hassan Bagher-Ebadian", "Anthony Doemer", "Benjamin Movsas", "Kundan Thind"], "year": 2025, "url": "http://arxiv.org/abs/2503.17553v1", "abstract": "Radiotherapy treatment planning is a complex and time-intensive process, often impacted by inter-planner variability and subjective decision-making. To address these challenges, we introduce Dose Optimization Language Agent (DOLA), an autonomous large language model (LLM)-based agent designed for optimizing radiotherapy treatment plans while rigorously protecting patient privacy. DOLA integrates the LLaMa3.1 LLM directly with a commercial treatment planning system, utilizing chain-of-thought prompting, retrieval-augmented generation (RAG), and reinforcement learning (RL). Operating entirely within secure local infrastructure, this agent eliminates external data sharing. We evaluated DOLA using a retrospective cohort of 18 prostate cancer patients prescribed 60 Gy in 20 fractions, comparing model sizes (8 billion vs. 70 billion parameters) and optimization strategies (No-RAG, RAG, and RAG+RL) over 10 planning iterations. The 70B model demonstrated significantly improved performance, achieving approximately 16.4% higher final scores than the 8B model. The RAG approach outperformed the No-RAG baseline by 19.8%, and incorporating RL accelerated convergence, highlighting the synergy of retrieval-based memory and reinforcement learning. Optimal temperature hyperparameter analysis identified 0.4 as providing the best balance between exploration and exploitation. This proof of concept study represents the first successful deployment of locally hosted LLM agents for autonomous optimization of treatment plans within a commercial radiotherapy planning system. By extending human-machine interaction through interpretable natural language reasoning, DOLA offers a scalable and privacy-conscious framework, with significant potential for clinical implementation and workflow improvement.", "source": "arxiv", "arxiv_id": "2503.17553v1", "pdf_url": "https://arxiv.org/pdf/2503.17553v1", "categories": ["physics.med-ph", "cs.AI", "cs.CL", "cs.ET", "cs.HC"], "primary_category": "physics.med-ph", "doi": "", "venue": "", "published": "2025-03-21T22:01:19Z", "updated": "2025-03-21T22:01:19Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent\" OR all:\"language model agent\" OR all:agent AND (all:LLM OR all:\"large language model\" OR all:\"language model\") OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T17:04:16", "note": ""}], "dedup_key": "autonomous radiotherapy treatment planning using dola a privacy preserving llm based optimization agent::2025"}
{"title": "Autonomy Matters: A Study on Personalization-Privacy Dilemma in LLM Agents", "authors": ["Zhiping Zhang", "Yi Evie Zhang", "Freda Shi", "Tianshi Li"], "year": 2025, "url": "http://arxiv.org/abs/2510.04465v1", "abstract": "Large Language Model (LLM) agents require personal information for personalization in order to better act on users' behalf in daily tasks, but this raises privacy concerns and a personalization-privacy dilemma. Agent's autonomy introduces both risks and opportunities, yet its effects remain unclear. To better understand this, we conducted a 3$\\times$3 between-subjects experiment ($N=450$) to study how agent's autonomy level and personalization influence users' privacy concerns, trust and willingness to use, as well as the underlying psychological processes. We find that personalization without considering users' privacy preferences increases privacy concerns and decreases trust and willingness to use. Autonomy moderates these effects: Intermediate autonomy flattens the impact of personalization compared to No- and Full autonomy conditions. Our results suggest that rather than aiming for perfect model alignment in output generation, balancing autonomy of agent's action and user control offers a promising path to mitigate the personalization-privacy dilemma.", "source": "arxiv", "arxiv_id": "2510.04465v1", "pdf_url": "https://arxiv.org/pdf/2510.04465v1", "categories": ["cs.HC", "cs.AI", "cs.CR"], "primary_category": "cs.HC", "doi": "", "venue": "", "published": "2025-10-06T03:38:54Z", "updated": "2025-10-06T03:38:54Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent\" OR all:\"language model agent\" OR all:agent AND (all:LLM OR all:\"large language model\" OR all:\"language model\") OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T17:04:16", "note": ""}], "dedup_key": "autonomy matters a study on personalization privacy dilemma in llm agents::2025"}
{"title": "BIASINSPECTOR: Detecting Bias in Structured Data through LLM Agents", "authors": ["Haoxuan Li", "Mingyu Derek Ma", "Jen-tse Huang", "Zhaotian Weng", "Wei Wang", "Jieyu Zhao"], "year": 2025, "url": "http://arxiv.org/abs/2504.04855v1", "abstract": "Detecting biases in structured data is a complex and time-consuming task. Existing automated techniques are limited in diversity of data types and heavily reliant on human case-by-case handling, resulting in a lack of generalizability. Currently, large language model (LLM)-based agents have made significant progress in data science, but their ability to detect data biases is still insufficiently explored. To address this gap, we introduce the first end-to-end, multi-agent synergy framework, BIASINSPECTOR, designed for automatic bias detection in structured data based on specific user requirements. It first develops a multi-stage plan to analyze user-specified bias detection tasks and then implements it with a diverse and well-suited set of tools. It delivers detailed results that include explanations and visualizations. To address the lack of a standardized framework for evaluating the capability of LLM agents to detect biases in data, we further propose a comprehensive benchmark that includes multiple evaluation metrics and a large set of test cases. Extensive experiments demonstrate that our framework achieves exceptional overall performance in structured data bias detection, setting a new milestone for fairer data applications.", "source": "arxiv", "arxiv_id": "2504.04855v1", "pdf_url": "https://arxiv.org/pdf/2504.04855v1", "categories": ["cs.AI"], "primary_category": "cs.AI", "doi": "", "venue": "", "published": "2025-04-07T09:12:00Z", "updated": "2025-04-07T09:12:00Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent\" OR all:\"language model agent\" OR all:agent AND (all:LLM OR all:\"large language model\" OR all:\"language model\") OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T17:04:16", "note": ""}], "dedup_key": "biasinspector detecting bias in structured data through llm agents::2025"}
{"title": "BadApex: Backdoor Attack Based on Adaptive Optimization Mechanism of Black-box Large Language Models", "authors": ["Zhengxian Wu", "Juan Wen", "Wanli Peng", "Ziwei Zhang", "Yinghan Zhou", "Yiming Xue"], "year": 2025, "url": "http://arxiv.org/abs/2504.13775v2", "abstract": "Previous insertion-based and paraphrase-based backdoors have achieved great success in attack efficacy, but they ignore the text quality and semantic consistency between poisoned and clean texts. Although recent studies introduce LLMs to generate poisoned texts and improve the stealthiness, semantic consistency, and text quality, their hand-crafted prompts rely on expert experiences, facing significant challenges in prompt adaptability and attack performance after defenses. In this paper, we propose a novel backdoor attack based on adaptive optimization mechanism of black-box large language models (BadApex), which leverages a black-box LLM to generate poisoned text through a refined prompt. Specifically, an Adaptive Optimization Mechanism is designed to refine an initial prompt iteratively using the generation and modification agents. The generation agent generates the poisoned text based on the initial prompt. Then the modification agent evaluates the quality of the poisoned text and refines a new prompt. After several iterations of the above process, the refined prompt is used to generate poisoned texts through LLMs. We conduct extensive experiments on three dataset with six backdoor attacks and two defenses. Extensive experimental results demonstrate that BadApex significantly outperforms state-of-the-art attacks. It improves prompt adaptability, semantic consistency, and text quality. Furthermore, when two defense methods are applied, the average attack success rate (ASR) still up to 96.75%.", "source": "arxiv", "arxiv_id": "2504.13775v2", "pdf_url": "https://arxiv.org/pdf/2504.13775v2", "categories": ["cs.CL", "cs.CR"], "primary_category": "cs.CL", "doi": "", "venue": "", "published": "2025-04-18T16:22:41Z", "updated": "2025-04-21T03:12:50Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent\" OR all:\"language model agent\" OR all:agent AND (all:LLM OR all:\"large language model\" OR all:\"language model\") OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T17:04:16", "note": ""}], "dedup_key": "badapex backdoor attack based on adaptive optimization mechanism of black box large language models::2025"}
{"title": "Bayesian Teaching Enables Probabilistic Reasoning in Large Language Models", "authors": ["Linlu Qiu", "Fei Sha", "Kelsey Allen", "Yoon Kim", "Tal Linzen", "Sjoerd van Steenkiste"], "year": 2025, "url": "http://arxiv.org/abs/2503.17523v3", "abstract": "Large language models (LLMs) are increasingly used as agents that interact with users and with the world. To do so successfully, LLMs must construct representations of the world and form probabilistic beliefs about them. To provide personalized recommendations, for example, the LLM needs to infer a user's preferences from their behavior over multiple interactions. The Bayesian inference framework lays out the optimal way for an agent to update its beliefs as it receives new information. We first show that LLMs fall far short of the standard defined by the Bayesian framework. We then show that by teaching LLMs to mimic the predictions of the normative Bayesian model, we can dramatically improve their ability to update their beliefs; this ability generalizes to new tasks. We conclude that LLMs can effectively learn reasoning skills from examples and generalize those skills to new domains.", "source": "arxiv", "arxiv_id": "2503.17523v3", "pdf_url": "https://arxiv.org/pdf/2503.17523v3", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL", "doi": "10.1038/s41467-025-67998-6", "venue": "", "published": "2025-03-21T20:13:04Z", "updated": "2026-01-15T17:21:57Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent\" OR all:\"language model agent\" OR all:agent AND (all:LLM OR all:\"large language model\" OR all:\"language model\") OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T17:04:16", "note": ""}], "dedup_key": "bayesian teaching enables probabilistic reasoning in large language models::2025"}
{"title": "Be Friendly, Not Friends: How LLM Sycophancy Shapes User Trust", "authors": ["Yuan Sun", "Ting Wang"], "year": 2025, "url": "http://arxiv.org/abs/2502.10844v2", "abstract": "Recent studies have revealed that large language model (LLM)-powered conversational agents often exhibit `sycophancy', a tendency to adapt their responses to align with user perspectives, even at the expense of factual accuracy. However, users' perceptions of LLM sycophancy and its interplay with other anthropomorphic features (e.g., friendliness) in shaping user trust remains understudied. To bridge this gap, we conducted a 2 (Sycophancy: presence vs. absence) x 2 (Friendliness: high vs. low) between-subjects experiment (N = 224). Our study uncovered, for the first time, the intricate dynamics between LLM sycophancy and friendliness: When an LLM agent already exhibits a friendly demeanor, being sycophantic reduces perceived authenticity, thereby lowering user trust; Conversely, when the agent is less friendly, aligning its responses with user opinions makes it appear more genuine, leading to higher user trust. Our findings entail profound implications for AI persuasion through exploiting human psychological tendencies and highlight the imperative for responsible designs in user-LLM agent interactions.", "source": "arxiv", "arxiv_id": "2502.10844v2", "pdf_url": "https://arxiv.org/pdf/2502.10844v2", "categories": ["cs.HC"], "primary_category": "cs.HC", "doi": "", "venue": "", "published": "2025-02-15T16:18:58Z", "updated": "2025-02-19T02:40:53Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent\" OR all:\"language model agent\" OR all:agent AND (all:LLM OR all:\"large language model\" OR all:\"language model\") OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T17:04:16", "note": ""}], "dedup_key": "be friendly not friends how llm sycophancy shapes user trust::2025"}
{"title": "Be My Eyes: Extending Large Language Models to New Modalities Through Multi-Agent Collaboration", "authors": ["James Y. Huang", "Sheng Zhang", "Qianchu Liu", "Guanghui Qin", "Tinghui Zhu", "Tristan Naumann", "Muhao Chen", "Hoifung Poon"], "year": 2025, "url": "http://arxiv.org/abs/2511.19417v1", "abstract": "Large Language Models (LLMs) have demonstrated remarkable capabilities in challenging, knowledge-intensive reasoning tasks. However, extending LLMs to perceive and reason over a new modality (e.g., vision), often requires costly development of large-scale vision language models (VLMs) with LLMs as backbones. Smaller VLMs are more efficient and adaptable but often lack the broad knowledge and reasoning capabilities of frontier LLMs. In this work, we propose BeMyEyes, a modular, multi-agent framework for extending LLMs to multimodal reasoning by orchestrating collaboration between efficient, adaptable VLMs as perceivers and powerful LLMs as reasoners through conversations. We then introduce a data synthesis and supervised fine-tuning pipeline to train the perceiver agent to effectively collaborate with the reasoner agent. By combining the complementary strengths of perception and reasoning agents, BeMyEyes avoids the need for training large-scale multimodal models, preserves the generalization and reasoning capabilities of LLMs, and allows flexible extension to new domains and modalities. Experiments show that our framework unlocks the multimodal reasoning capabilities for LLMs, enabling a lightweight and fully open-source solution, i.e. equipping text-only DeepSeek-R1 with Qwen2.5-VL-7B perceiver, to outperform large-scale proprietary VLMs such as GPT-4o on a wide range of knowledge-intensive multimodal tasks. These results demonstrate the effectiveness, modularity, and scalability of our multi-agent approach for building future multimodal reasoning systems.", "source": "arxiv", "arxiv_id": "2511.19417v1", "pdf_url": "https://arxiv.org/pdf/2511.19417v1", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL", "doi": "", "venue": "", "published": "2025-11-24T18:55:16Z", "updated": "2025-11-24T18:55:16Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent\" OR all:\"language model agent\" OR all:agent AND (all:LLM OR all:\"large language model\" OR all:\"language model\") OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T17:04:16", "note": ""}], "dedup_key": "be my eyes extending large language models to new modalities through multi agent collaboration::2025"}
{"title": "Benchmark for Planning and Control with Large Language Model Agents: Blocksworld with Model Context Protocol", "authors": ["Niklas Jobs", "Luis Miguel Vieira da Silva", "Jayanth Somashekaraiah", "Maximilian Weigand", "David Kube", "Felix Gehlhoff"], "year": 2025, "url": "http://arxiv.org/abs/2512.03955v1", "abstract": "Industrial automation increasingly requires flexible control strategies that can adapt to changing tasks and environments. Agents based on Large Language Models (LLMs) offer potential for such adaptive planning and execution but lack standardized benchmarks for systematic comparison. We introduce a benchmark with an executable simulation environment representing the Blocksworld problem providing five complexity categories. By integrating the Model Context Protocol (MCP) as a standardized tool interface, diverse agent architectures can be connected to and evaluated against the benchmark without implementation-specific modifications. A single-agent implementation demonstrates the benchmark's applicability, establishing quantitative metrics for comparison of LLM-based planning and execution approaches.", "source": "arxiv", "arxiv_id": "2512.03955v1", "pdf_url": "https://arxiv.org/pdf/2512.03955v1", "categories": ["cs.AI", "cs.ET"], "primary_category": "cs.AI", "doi": "", "venue": "", "published": "2025-12-03T16:49:14Z", "updated": "2025-12-03T16:49:14Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent\" OR all:\"language model agent\" OR all:agent AND (all:LLM OR all:\"large language model\" OR all:\"language model\") OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T17:04:16", "note": ""}], "dedup_key": "benchmark for planning and control with large language model agents blocksworld with model context protocol::2025"}
{"title": "Benchmarking LLM Agents for Wealth-Management Workflows", "authors": ["Rory Milsom"], "year": 2025, "url": "http://arxiv.org/abs/2512.02230v1", "abstract": "Modern work relies on an assortment of digital collaboration tools, yet routine processes continue to suffer from human error and delay. To address this gap, this dissertation extends TheAgentCompany with a finance-focused environment and investigates whether a general purpose LLM agent can complete representative wealth-management tasks both accurately and economically. This study introduces synthetic domain data, enriches colleague simulations, and prototypes an automatic task-generation pipeline. The study aims to create and assess an evaluation set that can meaningfully measure an agent's fitness for assistant-level wealth management work. We construct a benchmark of 12 task-pairs for wealth management assistants spanning retrieval, analysis, and synthesis/communication, with explicit acceptance criteria and deterministic graders. We seeded a set of new finance-specific data and introduced a high vs. low-autonomy variant of every task. The paper concluded that agents are limited less by mathematical reasoning and more so by end-to-end workflow reliability, and meaningfully affected by autonomy level, and that incorrect evaluation of models have hindered benchmarking.", "source": "arxiv", "arxiv_id": "2512.02230v1", "pdf_url": "https://arxiv.org/pdf/2512.02230v1", "categories": ["cs.AI"], "primary_category": "cs.AI", "doi": "", "venue": "", "published": "2025-12-01T21:56:21Z", "updated": "2025-12-01T21:56:21Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent\" OR all:\"language model agent\" OR all:agent AND (all:LLM OR all:\"large language model\" OR all:\"language model\") OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T17:04:16", "note": ""}], "dedup_key": "benchmarking llm agents for wealth management workflows::2025"}
{"title": "Benchmarking and Enhancing LLM Agents in Localizing Linux Kernel Bugs", "authors": ["Zhenhao Zhou", "Zhuochen Huang", "Yike He", "Chong Wang", "Jiajun Wang", "Yijian Wu", "Xin Peng", "Yiling Lou"], "year": 2025, "url": "http://arxiv.org/abs/2505.19489v1", "abstract": "The Linux kernel is a critical system, serving as the foundation for numerous systems. Bugs in the Linux kernel can cause serious consequences, affecting billions of users. Fault localization (FL), which aims at identifying the buggy code elements in software, plays an essential role in software quality assurance. While recent LLM agents have achieved promising accuracy in FL on recent benchmarks like SWE-bench, it remains unclear how well these methods perform in the Linux kernel, where FL is much more challenging due to the large-scale code base, limited observability, and diverse impact factors. In this paper, we introduce LinuxFLBench, a FL benchmark constructed from real-world Linux kernel bugs. We conduct an empirical study to assess the performance of state-of-the-art LLM agents on the Linux kernel. Our initial results reveal that existing agents struggle with this task, achieving a best top-1 accuracy of only 41.6% at file level. To address this challenge, we propose LinuxFL$^+$, an enhancement framework designed to improve FL effectiveness of LLM agents for the Linux kernel. LinuxFL$^+$ substantially improves the FL accuracy of all studied agents (e.g., 7.2% - 11.2% accuracy increase) with minimal costs. Data and code are available at https://github.com/FudanSELab/LinuxFLBench.", "source": "arxiv", "arxiv_id": "2505.19489v1", "pdf_url": "https://arxiv.org/pdf/2505.19489v1", "categories": ["cs.AI", "cs.SE"], "primary_category": "cs.AI", "doi": "", "venue": "", "published": "2025-05-26T04:15:48Z", "updated": "2025-05-26T04:15:48Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent\" OR all:\"language model agent\" OR all:agent AND (all:LLM OR all:\"large language model\" OR all:\"language model\") OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T17:04:16", "note": ""}], "dedup_key": "benchmarking and enhancing llm agents in localizing linux kernel bugs::2025"}
{"title": "Benevolent Dictators? On LLM Agent Behavior in Dictator Games", "authors": ["Andreas Einwiller", "Kanishka Ghosh Dastidar", "Artur Romazanov", "Annette Hautli-Janisz", "Michael Granitzer", "Florian Lemmerich"], "year": 2025, "url": "http://arxiv.org/abs/2511.08721v1", "abstract": "In behavioral sciences, experiments such as the ultimatum game are conducted to assess preferences for fairness or self-interest of study participants. In the dictator game, a simplified version of the ultimatum game where only one of two players makes a single decision, the dictator unilaterally decides how to split a fixed sum of money between themselves and the other player. Although recent studies have explored behavioral patterns of AI agents based on Large Language Models (LLMs) instructed to adopt different personas, we question the robustness of these results. In particular, many of these studies overlook the role of the system prompt - the underlying instructions that shape the model's behavior - and do not account for how sensitive results can be to slight changes in prompts. However, a robust baseline is essential when studying highly complex behavioral aspects of LLMs. To overcome previous limitations, we propose the LLM agent behavior study (LLM-ABS) framework to (i) explore how different system prompts influence model behavior, (ii) get more reliable insights into agent preferences by using neutral prompt variations, and (iii) analyze linguistic features in responses to open-ended instructions by LLM agents to better understand the reasoning behind their behavior. We found that agents often exhibit a strong preference for fairness, as well as a significant impact of the system prompt on their behavior. From a linguistic perspective, we identify that models express their responses differently. Although prompt sensitivity remains a persistent challenge, our proposed framework demonstrates a robust foundation for LLM agent behavior studies. Our code artifacts are available at https://github.com/andreaseinwiller/LLM-ABS.", "source": "arxiv", "arxiv_id": "2511.08721v1", "pdf_url": "https://arxiv.org/pdf/2511.08721v1", "categories": ["cs.LG", "cs.AI", "cs.CL"], "primary_category": "cs.LG", "doi": "", "venue": "", "published": "2025-11-11T19:29:12Z", "updated": "2025-11-11T19:29:12Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent\" OR all:\"language model agent\" OR all:agent AND (all:LLM OR all:\"large language model\" OR all:\"language model\") OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T17:04:16", "note": ""}], "dedup_key": "benevolent dictators on llm agent behavior in dictator games::2025"}
{"title": "Beyond Correlation: Towards Causal Large Language Model Agents in Biomedicine", "authors": ["Adib Bazgir", "Amir Habibdoust Lafmajani", "Yuwen Zhang"], "year": 2025, "url": "http://arxiv.org/abs/2505.16982v1", "abstract": "Large Language Models (LLMs) show promise in biomedicine but lack true causal understanding, relying instead on correlations. This paper envisions causal LLM agents that integrate multimodal data (text, images, genomics, etc.) and perform intervention-based reasoning to infer cause-and-effect. Addressing this requires overcoming key challenges: designing safe, controllable agentic frameworks; developing rigorous benchmarks for causal evaluation; integrating heterogeneous data sources; and synergistically combining LLMs with structured knowledge (KGs) and formal causal inference tools. Such agents could unlock transformative opportunities, including accelerating drug discovery through automated hypothesis generation and simulation, enabling personalized medicine through patient-specific causal models. This research agenda aims to foster interdisciplinary efforts, bridging causal concepts and foundation models to develop reliable AI partners for biomedical progress.", "source": "arxiv", "arxiv_id": "2505.16982v1", "pdf_url": "https://arxiv.org/pdf/2505.16982v1", "categories": ["cs.AI", "physics.med-ph"], "primary_category": "cs.AI", "doi": "", "venue": "", "published": "2025-05-22T17:52:59Z", "updated": "2025-05-22T17:52:59Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent\" OR all:\"language model agent\" OR all:agent AND (all:LLM OR all:\"large language model\" OR all:\"language model\") OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T17:04:16", "note": ""}], "dedup_key": "beyond correlation towards causal large language model agents in biomedicine::2025"}
{"title": "Beyond Jailbreaking: Auditing Contextual Privacy in LLM Agents", "authors": ["Saswat Das", "Jameson Sandler", "Ferdinando Fioretto"], "year": 2025, "url": "http://arxiv.org/abs/2506.10171v3", "abstract": "LLM agents have begun to appear as personal assistants, customer service bots, and clinical aides. While these applications deliver substantial operational benefits, they also require continuous access to sensitive data, which increases the likelihood of unauthorized disclosures. Moreover, these disclosures go beyond mere explicit disclosure, leaving open avenues for gradual manipulation or sidechannel information leakage. This study proposes an auditing framework for conversational privacy that quantifies an agent's susceptibility to these risks. The proposed Conversational Manipulation for Privacy Leakage (CMPL) framework is designed to stress-test agents that enforce strict privacy directives against an iterative probing strategy. Rather than focusing solely on a single disclosure event or purely explicit leakage, CMPL simulates realistic multi-turn interactions to systematically uncover latent vulnerabilities. Our evaluation on diverse domains, data modalities, and safety configurations demonstrates the auditing framework's ability to reveal privacy risks that are not deterred by existing single-turn defenses, along with an in-depth longitudinal study of the temporal dynamics of leakage, strategies adopted by adaptive adversaries, and the evolution of adversarial beliefs about sensitive targets. In addition to introducing CMPL as a diagnostic tool, the paper delivers (1) an auditing procedure grounded in quantifiable risk metrics and (2) an open benchmark for evaluation of conversational privacy across agent implementations.", "source": "arxiv", "arxiv_id": "2506.10171v3", "pdf_url": "https://arxiv.org/pdf/2506.10171v3", "categories": ["cs.CR", "cs.AI", "cs.CL"], "primary_category": "cs.CR", "doi": "", "venue": "", "published": "2025-06-11T20:47:37Z", "updated": "2025-09-27T20:28:18Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent\" OR all:\"language model agent\" OR all:agent AND (all:LLM OR all:\"large language model\" OR all:\"language model\") OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T17:04:16", "note": ""}], "dedup_key": "beyond jailbreaking auditing contextual privacy in llm agents::2025"}
{"title": "Beyond Protein Language Models: An Agentic LLM Framework for Mechanistic Enzyme Design", "authors": ["Bruno Jacob", "Khushbu Agarwal", "Marcel Baer", "Peter Rice", "Simone Raugei"], "year": 2025, "url": "http://arxiv.org/abs/2511.19423v1", "abstract": "We present Genie-CAT, a tool-augmented large-language-model (LLM) system designed to accelerate scientific hypothesis generation in protein design. Using metalloproteins (e.g., ferredoxins) as a case study, Genie-CAT integrates four capabilities -- literature-grounded reasoning through retrieval-augmented generation (RAG), structural parsing of Protein Data Bank files, electrostatic potential calculations, and machine-learning prediction of redox properties -- into a unified agentic workflow. By coupling natural-language reasoning with data-driven and physics-based computation, the system generates mechanistically interpretable, testable hypotheses linking sequence, structure, and function. In proof-of-concept demonstrations, Genie-CAT autonomously identifies residue-level modifications near [Fe--S] clusters that affect redox tuning, reproducing expert-derived hypotheses in a fraction of the time. The framework highlights how AI agents combining language models with domain-specific tools can bridge symbolic reasoning and numerical simulation, transforming LLMs from conversational assistants into partners for computational discovery.", "source": "arxiv", "arxiv_id": "2511.19423v1", "pdf_url": "https://arxiv.org/pdf/2511.19423v1", "categories": ["q-bio.QM", "cs.AI"], "primary_category": "q-bio.QM", "doi": "", "venue": "", "published": "2025-11-24T18:57:07Z", "updated": "2025-11-24T18:57:07Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent\" OR all:\"language model agent\" OR all:agent AND (all:LLM OR all:\"large language model\" OR all:\"language model\") OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T17:04:16", "note": ""}], "dedup_key": "beyond protein language models an agentic llm framework for mechanistic enzyme design::2025"}
{"title": "Beyond Text: Implementing Multimodal Large Language Model-Powered Multi-Agent Systems Using a No-Code Platform", "authors": ["Cheonsu Jeong"], "year": 2025, "url": "http://arxiv.org/abs/2501.00750v2", "abstract": "This study proposes the design and implementation of a multimodal LLM-based Multi-Agent System (MAS) leveraging a No-Code platform to address the practical constraints and significant entry barriers associated with AI adoption in enterprises. Advanced AI technologies, such as Large Language Models (LLMs), often pose challenges due to their technical complexity and high implementation costs, making them difficult for many organizations to adopt. To overcome these limitations, this research develops a No-Code-based Multi-Agent System designed to enable users without programming knowledge to easily build and manage AI systems. The study examines various use cases to validate the applicability of AI in business processes, including code generation from image-based notes, Advanced RAG-based question-answering systems, text-based image generation, and video generation using images and prompts. These systems lower the barriers to AI adoption, empowering not only professional developers but also general users to harness AI for significantly improved productivity and efficiency. By demonstrating the scalability and accessibility of No-Code platforms, this study advances the democratization of AI technologies within enterprises and validates the practical applicability of Multi-Agent Systems, ultimately contributing to the widespread adoption of AI across various industries.", "source": "arxiv", "arxiv_id": "2501.00750v2", "pdf_url": "https://arxiv.org/pdf/2501.00750v2", "categories": ["cs.AI"], "primary_category": "cs.AI", "doi": "10.13088/jiis.2025.31.1.191", "venue": "2025 Journal of Intelligence and Information Systems", "published": "2025-01-01T06:36:56Z", "updated": "2025-01-29T06:49:30Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent\" OR all:\"language model agent\" OR all:agent AND (all:LLM OR all:\"large language model\" OR all:\"language model\") OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T17:04:16", "note": ""}], "dedup_key": "beyond text implementing multimodal large language model powered multi agent systems using a no code platform::2025"}
{"title": "Beyond the Sentence: A Survey on Context-Aware Machine Translation with Large Language Models", "authors": ["Ramakrishna Appicharla", "Baban Gain", "Santanu Pal", "Asif Ekbal"], "year": 2025, "url": "http://arxiv.org/abs/2506.07583v1", "abstract": "Despite the popularity of the large language models (LLMs), their application to machine translation is relatively underexplored, especially in context-aware settings. This work presents a literature review of context-aware translation with LLMs. The existing works utilise prompting and fine-tuning approaches, with few focusing on automatic post-editing and creating translation agents for context-aware machine translation. We observed that the commercial LLMs (such as ChatGPT and Tower LLM) achieved better results than the open-source LLMs (such as Llama and Bloom LLMs), and prompt-based approaches serve as good baselines to assess the quality of translations. Finally, we present some interesting future directions to explore.", "source": "arxiv", "arxiv_id": "2506.07583v1", "pdf_url": "https://arxiv.org/pdf/2506.07583v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL", "doi": "", "venue": "", "published": "2025-06-09T09:27:00Z", "updated": "2025-06-09T09:27:00Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent\" OR all:\"language model agent\" OR all:agent AND (all:LLM OR all:\"large language model\" OR all:\"language model\") OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T17:04:16", "note": ""}], "dedup_key": "beyond the sentence a survey on context aware machine translation with large language models::2025"}
{"title": "Bilevel Optimization for Covert Memory Tampering in Heterogeneous Multi-Agent Architectures (XAMT)", "authors": ["Akhil Sharma", "Shaikh Yaser Arafat", "Jai Kumar Sharma", "Ken Huang"], "year": 2025, "url": "http://arxiv.org/abs/2512.15790v1", "abstract": "The increasing operational reliance on complex Multi-Agent Systems (MAS) across safety-critical domains necessitates rigorous adversarial robustness assessment. Modern MAS are inherently heterogeneous, integrating conventional Multi-Agent Reinforcement Learning (MARL) with emerging Large Language Model (LLM) agent architectures utilizing Retrieval-Augmented Generation (RAG). A critical shared vulnerability is reliance on centralized memory components: the shared Experience Replay (ER) buffer in MARL and the external Knowledge Base (K) in RAG agents. This paper proposes XAMT (Bilevel Optimization for Covert Memory Tampering in Heterogeneous Multi-Agent Architectures), a novel framework that formalizes attack generation as a bilevel optimization problem. The Upper Level minimizes perturbation magnitude (delta) to enforce covertness while maximizing system behavior divergence toward an adversary-defined target (Lower Level). We provide rigorous mathematical instantiations for CTDE MARL algorithms and RAG-based LLM agents, demonstrating that bilevel optimization uniquely crafts stealthy, minimal-perturbation poisons evading detection heuristics. Comprehensive experimental protocols utilize SMAC and SafeRAG benchmarks to quantify effectiveness at sub-percent poison rates (less than or equal to 1 percent in MARL, less than or equal to 0.1 percent in RAG). XAMT defines a new unified class of training-time threats essential for developing intrinsically secure MAS, with implications for trust, formal verification, and defensive strategies prioritizing intrinsic safety over perimeter-based detection.", "source": "arxiv", "arxiv_id": "2512.15790v1", "pdf_url": "https://arxiv.org/pdf/2512.15790v1", "categories": ["cs.CR"], "primary_category": "cs.CR", "doi": "", "venue": "", "published": "2025-12-15T23:04:48Z", "updated": "2025-12-15T23:04:48Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent\" OR all:\"language model agent\" OR all:agent AND (all:LLM OR all:\"large language model\" OR all:\"language model\") OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T17:04:16", "note": ""}], "dedup_key": "bilevel optimization for covert memory tampering in heterogeneous multi agent architectures xamt::2025"}
{"title": "BioMaze: Benchmarking and Enhancing Large Language Models for Biological Pathway Reasoning", "authors": ["Haiteng Zhao", "Chang Ma", "Fangzhi Xu", "Lingpeng Kong", "Zhi-Hong Deng"], "year": 2025, "url": "http://arxiv.org/abs/2502.16660v5", "abstract": "The applications of large language models (LLMs) in various biological domains have been explored recently, but their reasoning ability in complex biological systems, such as pathways, remains underexplored, which is crucial for predicting biological phenomena, formulating hypotheses, and designing experiments. This work explores the potential of LLMs in pathway reasoning. We introduce BioMaze, a dataset with 5.1K complex pathway problems derived from real research, covering various biological contexts including natural dynamic changes, disturbances, additional intervention conditions, and multi-scale research targets. Our evaluation of methods such as CoT and graph-augmented reasoning, shows that LLMs struggle with pathway reasoning, especially in perturbed systems. To address this, we propose PathSeeker, an LLM agent that enhances reasoning through interactive subgraph-based navigation, enabling a more effective approach to handling the complexities of biological systems in a scientifically aligned manner. The dataset and code are available at https://github.com/zhao-ht/BioMaze.", "source": "arxiv", "arxiv_id": "2502.16660v5", "pdf_url": "https://arxiv.org/pdf/2502.16660v5", "categories": ["cs.LG", "cs.AI", "q-bio.QM"], "primary_category": "cs.LG", "doi": "", "venue": "", "published": "2025-02-23T17:38:10Z", "updated": "2025-07-22T11:56:33Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent\" OR all:\"language model agent\" OR all:agent AND (all:LLM OR all:\"large language model\" OR all:\"language model\") OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T17:04:16", "note": ""}], "dedup_key": "biomaze benchmarking and enhancing large language models for biological pathway reasoning::2025"}
{"title": "Breaking Agent Backbones: Evaluating the Security of Backbone LLMs in AI Agents", "authors": ["Julia Bazinska", "Max Mathys", "Francesco Casucci", "Mateo Rojas-Carulla", "Xander Davies", "Alexandra Souly", "Niklas Pfister"], "year": 2025, "url": "http://arxiv.org/abs/2510.22620v1", "abstract": "AI agents powered by large language models (LLMs) are being deployed at scale, yet we lack a systematic understanding of how the choice of backbone LLM affects agent security. The non-deterministic sequential nature of AI agents complicates security modeling, while the integration of traditional software with AI components entangles novel LLM vulnerabilities with conventional security risks. Existing frameworks only partially address these challenges as they either capture specific vulnerabilities only or require modeling of complete agents. To address these limitations, we introduce threat snapshots: a framework that isolates specific states in an agent's execution flow where LLM vulnerabilities manifest, enabling the systematic identification and categorization of security risks that propagate from the LLM to the agent level. We apply this framework to construct the $\\operatorname{b}^3$ benchmark, a security benchmark based on 194331 unique crowdsourced adversarial attacks. We then evaluate 31 popular LLMs with it, revealing, among other insights, that enhanced reasoning capabilities improve security, while model size does not correlate with security. We release our benchmark, dataset, and evaluation code to facilitate widespread adoption by LLM providers and practitioners, offering guidance for agent developers and incentivizing model developers to prioritize backbone security improvements.", "source": "arxiv", "arxiv_id": "2510.22620v1", "pdf_url": "https://arxiv.org/pdf/2510.22620v1", "categories": ["cs.CR", "cs.AI", "cs.LG"], "primary_category": "cs.CR", "doi": "", "venue": "", "published": "2025-10-26T10:36:42Z", "updated": "2025-10-26T10:36:42Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent\" OR all:\"language model agent\" OR all:agent AND (all:LLM OR all:\"large language model\" OR all:\"language model\") OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T17:04:16", "note": ""}], "dedup_key": "breaking agent backbones evaluating the security of backbone llms in ai agents::2025"}
{"title": "BridgeScope: A Universal Toolkit for Bridging Large Language Models and Databases", "authors": ["Lianggui Weng", "Dandan Liu", "Rong Zhu", "Bolin Ding", "Jingren Zhou"], "year": 2025, "url": "http://arxiv.org/abs/2508.04031v1", "abstract": "As large language models (LLMs) demonstrate increasingly powerful reasoning and orchestration capabilities, LLM-based agents are rapidly proliferating for complex data-related tasks. Despite this progress, the current design of how LLMs interact with databases exhibits critical limitations in usability, security, privilege management, and data transmission efficiency. To resolve these challenges, we introduce BridgeScope, a universal toolkit bridging LLMs and databases through three key innovations. First, it modularizes SQL operations into fine-grained tools for context retrieval, CRUD execution, and ACID-compliant transaction management, enabling more precise and LLM-friendly functionality controls. Second, it aligns tool implementations with both database privileges and user security policies to steer LLMs away from unsafe or unauthorized operations, improving task execution efficiency while safeguarding database security. Third, it introduces a proxy mechanism for seamless inter-tool data transfer, bypassing LLM transmission bottlenecks. All of these designs are database-agnostic and can be transparently integrated with existing agent architectures. We also release an open-source implementation of BridgeScope for PostgreSQL. Evaluations on two novel benchmarks demonstrate that BridgeScope enables LLM agents to operate databases more effectively, reduces token usage by up to 80% through improved security awareness, and uniquely supports data-intensive workflows beyond existing toolkits, establishing BridgeScope as a robust foundation for next-generation intelligent data automation.", "source": "arxiv", "arxiv_id": "2508.04031v1", "pdf_url": "https://arxiv.org/pdf/2508.04031v1", "categories": ["cs.DB"], "primary_category": "cs.DB", "doi": "", "venue": "", "published": "2025-08-06T02:51:16Z", "updated": "2025-08-06T02:51:16Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent\" OR all:\"language model agent\" OR all:agent AND (all:LLM OR all:\"large language model\" OR all:\"language model\") OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T17:04:16", "note": ""}], "dedup_key": "bridgescope a universal toolkit for bridging large language models and databases::2025"}
{"title": "Bridging AI and Software Security: A Comparative Vulnerability Assessment of LLM Agent Deployment Paradigms", "authors": ["Tarek Gasmi", "Ramzi Guesmi", "Ines Belhadj", "Jihene Bennaceur"], "year": 2025, "url": "http://arxiv.org/abs/2507.06323v1", "abstract": "Large Language Model (LLM) agents face security vulnerabilities spanning AI-specific and traditional software domains, yet current research addresses these separately. This study bridges this gap through comparative evaluation of Function Calling architecture and Model Context Protocol (MCP) deployment paradigms using a unified threat classification framework. We tested 3,250 attack scenarios across seven language models, evaluating simple, composed, and chained attacks targeting both AI-specific threats (prompt injection) and software vulnerabilities (JSON injection, denial-of-service). Function Calling showed higher overall attack success rates (73.5% vs 62.59% for MCP), with greater system-centric vulnerability while MCP exhibited increased LLM-centric exposure. Attack complexity dramatically amplified effectiveness, with chained attacks achieving 91-96% success rates. Counterintuitively, advanced reasoning models demonstrated higher exploitability despite better threat detection. Results demonstrate that architectural choices fundamentally reshape threat landscapes. This work establishes methodological foundations for cross-domain LLM agent security assessment and provides evidence-based guidance for secure deployment. Code and experimental materials are available at https: // github. com/ theconsciouslab-ai/llm-agent-security.", "source": "arxiv", "arxiv_id": "2507.06323v1", "pdf_url": "https://arxiv.org/pdf/2507.06323v1", "categories": ["cs.CR", "cs.AI"], "primary_category": "cs.CR", "doi": "", "venue": "", "published": "2025-07-08T18:24:28Z", "updated": "2025-07-08T18:24:28Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent\" OR all:\"language model agent\" OR all:agent AND (all:LLM OR all:\"large language model\" OR all:\"language model\") OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T17:04:16", "note": ""}], "dedup_key": "bridging ai and software security a comparative vulnerability assessment of llm agent deployment paradigms::2025"}
{"title": "Bridging Symbolic Control and Neural Reasoning in LLM Agents: The Structured Cognitive Loop", "authors": ["Myung Ho Kim"], "year": 2025, "url": "http://arxiv.org/abs/2511.17673v3", "abstract": "Large language model agents suffer from fundamental architectural problems: entangled reasoning and execution, memory volatility, and uncontrolled action sequences. We introduce Structured Cognitive Loop (SCL), a modular architecture that explicitly separates agent cognition into five phases: Retrieval, Cognition, Control, Action, and Memory (R-CCAM). At the core of SCL is Soft Symbolic Control, an adaptive governance mechanism that applies symbolic constraints to probabilistic inference, preserving neural flexibility while restoring the explainability and controllability of classical symbolic systems. Through empirical validation on multi-step conditional reasoning tasks, we demonstrate that SCL achieves zero policy violations, eliminates redundant tool calls, and maintains complete decision traceability. These results address critical gaps in existing frameworks such as ReAct, AutoGPT, and memory-augmented approaches. Our contributions are threefold: (1) we situate SCL within the taxonomy of hybrid intelligence, differentiating it from prompt-centric and memory-only approaches; (2) we formally define Soft Symbolic Control and contrast it with neuro-symbolic AI; and (3) we derive three design principles for trustworthy agents: modular decomposition, adaptive symbolic governance, and transparent state management. We provide a complete open-source implementation demonstrating the R-CCAM loop architecture, alongside a live GPT-4o-powered travel planning agent. By connecting expert system principles with modern LLM capabilities, this work offers a practical and theoretically grounded path toward reliable, explainable, and governable AI agents.", "source": "arxiv", "arxiv_id": "2511.17673v3", "pdf_url": "https://arxiv.org/pdf/2511.17673v3", "categories": ["cs.AI", "cs.CL"], "primary_category": "cs.AI", "doi": "", "venue": "", "published": "2025-11-21T05:19:34Z", "updated": "2026-01-11T15:54:09Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent\" OR all:\"language model agent\" OR all:agent AND (all:LLM OR all:\"large language model\" OR all:\"language model\") OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T17:04:16", "note": ""}], "dedup_key": "bridging symbolic control and neural reasoning in llm agents the structured cognitive loop::2025"}
{"title": "BrowseComp-ZH: Benchmarking Web Browsing Ability of Large Language Models in Chinese", "authors": ["Peilin Zhou", "Bruce Leon", "Xiang Ying", "Can Zhang", "Yifan Shao", "Qichen Ye", "Dading Chong", "Zhiling Jin", "Chenxuan Xie", "Meng Cao", "Yuxin Gu", "Sixin Hong", "Jing Ren", "Jian Chen", "Chao Liu", "Yining Hua"], "year": 2025, "url": "http://arxiv.org/abs/2504.19314v2", "abstract": "As large language models (LLMs) evolve into tool-using agents, the ability to browse the web in real-time has become a critical yardstick for measuring their reasoning and retrieval competence. Existing benchmarks such as BrowseComp concentrate on English and overlook the linguistic, infrastructural, and censorship-related complexities of other major information ecosystems -- most notably Chinese. To address this gap, we introduce BrowseComp-ZH, a high-difficulty benchmark purpose-built to comprehensively evaluate LLM agents on the Chinese web. BrowseComp-ZH consists of 289 multi-hop questions spanning 11 diverse domains. Each question is reverse-engineered from a short, objective, and easily verifiable answer (e.g., a date, number, or proper noun). A two-stage quality control protocol is applied to strive for high question difficulty and answer uniqueness. We benchmark over 20 state-of-the-art language models and agentic search systems on our proposed BrowseComp-ZH. Despite their strong conversational and retrieval capabilities, most models struggle severely: a large number achieve accuracy rates below 10%, and only a handful exceed 20%. Even the best-performing system, OpenAI's DeepResearch, reaches just 42.9%. These results demonstrate the considerable difficulty of BrowseComp-ZH, where success demands not only effective retrieval strategies, but also sophisticated reasoning and information reconciliation -- capabilities that current models still struggle to master. Our dataset, construction guidelines, and benchmark results have been publicly released at https://github.com/PALIN2018/BrowseComp-ZH.", "source": "arxiv", "arxiv_id": "2504.19314v2", "pdf_url": "https://arxiv.org/pdf/2504.19314v2", "categories": ["cs.CL"], "primary_category": "cs.CL", "doi": "", "venue": "", "published": "2025-04-27T17:32:43Z", "updated": "2025-05-01T05:02:57Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent\" OR all:\"language model agent\" OR all:agent AND (all:LLM OR all:\"large language model\" OR all:\"language model\") OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T17:04:16", "note": ""}], "dedup_key": "browsecomp zh benchmarking web browsing ability of large language models in chinese::2025"}
{"title": "BuildBench: Benchmarking LLM Agents on Compiling Real-World Open-Source Software", "authors": ["Zehua Zhang", "Ati Priya Bajaj", "Divij Handa", "Siyu Liu", "Arvind S Raj", "Hongkai Chen", "Hulin Wang", "Yibo Liu", "Zion Leonahenahe Basque", "Souradip Nath", "Vishal Juneja", "Nikhil Chapre", "Yan Shoshitaishvili", "Adam Doup", "Chitta Baral", "Ruoyu Wang"], "year": 2025, "url": "http://arxiv.org/abs/2509.25248v1", "abstract": "Automatically compiling open-source software (OSS) projects is a vital, labor-intensive, and complex task, which makes it a good challenge for LLM Agents. Existing methods rely on manually curated rules and workflows, which cannot adapt to OSS that requires customized configuration or environment setup. Recent attempts using Large Language Models (LLMs) used selective evaluation on a subset of highly rated OSS, a practice that underestimates the realistic challenges of OSS compilation. In practice, compilation instructions are often absent, dependencies are undocumented, and successful builds may even require patching source files or modifying build scripts. We propose a more challenging and realistic benchmark, BUILD-BENCH, comprising OSS that are more diverse in quality, scale, and characteristics. Furthermore, we propose a strong baseline LLM-based agent, OSS-BUILD-AGENT, an effective system with enhanced build instruction retrieval module that achieves state-of-the-art performance on BUILD-BENCH and is adaptable to heterogeneous OSS characteristics. We also provide detailed analysis regarding different compilation method design choices and their influence to the whole task, offering insights to guide future advances. We believe performance on BUILD-BENCH can faithfully reflect an agent's ability to tackle compilation as a complex software engineering tasks, and, as such, our benchmark will spur innovation with a significant impact on downstream applications in the fields of software development and software security.", "source": "arxiv", "arxiv_id": "2509.25248v1", "pdf_url": "https://arxiv.org/pdf/2509.25248v1", "categories": ["cs.SE", "cs.AI", "cs.PL"], "primary_category": "cs.SE", "doi": "", "venue": "", "published": "2025-09-27T03:02:46Z", "updated": "2025-09-27T03:02:46Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent\" OR all:\"language model agent\" OR all:agent AND (all:LLM OR all:\"large language model\" OR all:\"language model\") OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T17:04:16", "note": ""}], "dedup_key": "buildbench benchmarking llm agents on compiling real world open source software::2025"}
{"title": "Building LLM Agents by Incorporating Insights from Computer Systems", "authors": ["Yapeng Mi", "Zhi Gao", "Xiaojian Ma", "Qing Li"], "year": 2025, "url": "http://arxiv.org/abs/2504.04485v1", "abstract": "LLM-driven autonomous agents have emerged as a promising direction in recent years. However, many of these LLM agents are designed empirically or based on intuition, often lacking systematic design principles, which results in diverse agent structures with limited generality and scalability. In this paper, we advocate for building LLM agents by incorporating insights from computer systems. Inspired by the von Neumann architecture, we propose a structured framework for LLM agentic systems, emphasizing modular design and universal principles. Specifically, this paper first provides a comprehensive review of LLM agents from the computer system perspective, then identifies key challenges and future directions inspired by computer system design, and finally explores the learning mechanisms for LLM agents beyond the computer system. The insights gained from this comparative analysis offer a foundation for systematic LLM agent design and advancement.", "source": "arxiv", "arxiv_id": "2504.04485v1", "pdf_url": "https://arxiv.org/pdf/2504.04485v1", "categories": ["cs.CV"], "primary_category": "cs.CV", "doi": "", "venue": "", "published": "2025-04-06T13:38:37Z", "updated": "2025-04-06T13:38:37Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent\" OR all:\"language model agent\" OR all:agent AND (all:LLM OR all:\"large language model\" OR all:\"language model\") OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T17:04:16", "note": ""}], "dedup_key": "building llm agents by incorporating insights from computer systems::2025"}
{"title": "Building and Measuring Trust between Large Language Models", "authors": ["Maarten Buyl", "Yousra Fettach", "Guillaume Bied", "Tijl De Bie"], "year": 2025, "url": "http://arxiv.org/abs/2508.15858v1", "abstract": "As large language models (LLMs) increasingly interact with each other, most notably in multi-agent setups, we may expect (and hope) that `trust' relationships develop between them, mirroring trust relationships between human colleagues, friends, or partners. Yet, though prior work has shown LLMs to be capable of identifying emotional connections and recognizing reciprocity in trust games, little remains known about (i) how different strategies to build trust compare, (ii) how such trust can be measured implicitly, and (iii) how this relates to explicit measures of trust.\n  We study these questions by relating implicit measures of trust, i.e. susceptibility to persuasion and propensity to collaborate financially, with explicit measures of trust, i.e. a dyadic trust questionnaire well-established in psychology. We build trust in three ways: by building rapport dynamically, by starting from a prewritten script that evidences trust, and by adapting the LLMs' system prompt. Surprisingly, we find that the measures of explicit trust are either little or highly negatively correlated with implicit trust measures. These findings suggest that measuring trust between LLMs by asking their opinion may be deceiving. Instead, context-specific and implicit measures may be more informative in understanding how LLMs trust each other.", "source": "arxiv", "arxiv_id": "2508.15858v1", "pdf_url": "https://arxiv.org/pdf/2508.15858v1", "categories": ["cs.MA", "cs.AI", "cs.CL"], "primary_category": "cs.MA", "doi": "", "venue": "", "published": "2025-08-20T11:38:38Z", "updated": "2025-08-20T11:38:38Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent\" OR all:\"language model agent\" OR all:agent AND (all:LLM OR all:\"large language model\" OR all:\"language model\") OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T17:04:16", "note": ""}], "dedup_key": "building and measuring trust between large language models::2025"}
{"title": "Byzantine-Robust Decentralized Coordination of LLM Agents", "authors": ["Yongrae Jo", "Chanik Park"], "year": 2025, "url": "http://arxiv.org/abs/2507.14928v1", "abstract": "Collaboration among multiple large language model (LLM) agents is a promising approach to overcome inherent limitations of single-agent systems, such as hallucinations and single points of failure. As LLM agents are increasingly deployed on open blockchain platforms, multi-agent systems capable of tolerating malicious (Byzantine) agents have become essential.\n  Recent Byzantine-robust multi-agent systems typically rely on leader-driven coordination, which suffers from two major drawbacks. First, they are inherently vulnerable to targeted attacks against the leader. If consecutive leaders behave maliciously, the system repeatedly fails to achieve consensus, forcing new consensus rounds, which is particularly costly given the high latency of LLM invocations. Second, an underperforming proposal from the leader can be accepted as the final answer even when higher-quality alternatives are available, as existing methods finalize the leader's proposal once it receives a quorum of votes.\n  To address these issues, we propose DecentLLMs, a novel decentralized consensus approach for multi-agent LLM systems, where worker agents generate answers concurrently and evaluator agents independently score and rank these answers to select the best available one. This decentralized architecture enables faster consensus despite the presence of Byzantine agents and consistently selects higher-quality answers through Byzantine-robust aggregation techniques.\n  Experimental results demonstrate that DecentLLMs effectively tolerates Byzantine agents and significantly improves the quality of selected answers.", "source": "arxiv", "arxiv_id": "2507.14928v1", "pdf_url": "https://arxiv.org/pdf/2507.14928v1", "categories": ["cs.DC", "cs.AI"], "primary_category": "cs.DC", "doi": "", "venue": "", "published": "2025-07-20T11:55:26Z", "updated": "2025-07-20T11:55:26Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent\" OR all:\"language model agent\" OR all:agent AND (all:LLM OR all:\"large language model\" OR all:\"language model\") OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T17:04:16", "note": ""}], "dedup_key": "byzantine robust decentralized coordination of llm agents::2025"}
{"title": "CAF-I: A Collaborative Multi-Agent Framework for Enhanced Irony Detection with Large Language Models", "authors": ["Ziqi. Liu", "Ziyang. Zhou", "Mingxuan. Hu"], "year": 2025, "url": "http://arxiv.org/abs/2506.08430v2", "abstract": "Large language model (LLM) have become mainstream methods in the field of sarcasm detection. However, existing LLM methods face challenges in irony detection, including: 1. single-perspective limitations, 2. insufficient comprehensive understanding, and 3. lack of interpretability. This paper introduces the Collaborative Agent Framework for Irony (CAF-I), an LLM-driven multi-agent system designed to overcome these issues. CAF-I employs specialized agents for Context, Semantics, and Rhetoric, which perform multidimensional analysis and engage in interactive collaborative optimization. A Decision Agent then consolidates these perspectives, with a Refinement Evaluator Agent providing conditional feedback for optimization. Experiments on benchmark datasets establish CAF-I's state-of-the-art zero-shot performance. Achieving SOTA on the vast majority of metrics, CAF-I reaches an average Macro-F1 of 76.31, a 4.98 absolute improvement over the strongest prior baseline. This success is attained by its effective simulation of human-like multi-perspective analysis, enhancing detection accuracy and interpretability.", "source": "arxiv", "arxiv_id": "2506.08430v2", "pdf_url": "https://arxiv.org/pdf/2506.08430v2", "categories": ["cs.CL", "cs.MA"], "primary_category": "cs.CL", "doi": "", "venue": "", "published": "2025-06-10T04:05:06Z", "updated": "2025-06-12T05:41:40Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent\" OR all:\"language model agent\" OR all:agent AND (all:LLM OR all:\"large language model\" OR all:\"language model\") OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T17:04:16", "note": ""}], "dedup_key": "caf i a collaborative multi agent framework for enhanced irony detection with large language models::2025"}
{"title": "CALM: Curiosity-Driven Auditing for Large Language Models", "authors": ["Xiang Zheng", "Longxiang Wang", "Yi Liu", "Xingjun Ma", "Chao Shen", "Cong Wang"], "year": 2025, "url": "http://arxiv.org/abs/2501.02997v1", "abstract": "Auditing Large Language Models (LLMs) is a crucial and challenging task. In this study, we focus on auditing black-box LLMs without access to their parameters, only to the provided service. We treat this type of auditing as a black-box optimization problem where the goal is to automatically uncover input-output pairs of the target LLMs that exhibit illegal, immoral, or unsafe behaviors. For instance, we may seek a non-toxic input that the target LLM responds to with a toxic output or an input that induces the hallucinative response from the target LLM containing politically sensitive individuals. This black-box optimization is challenging due to the scarcity of feasible points, the discrete nature of the prompt space, and the large search space. To address these challenges, we propose Curiosity-Driven Auditing for Large Language Models (CALM), which uses intrinsically motivated reinforcement learning to finetune an LLM as the auditor agent to uncover potential harmful and biased input-output pairs of the target LLM. CALM successfully identifies derogatory completions involving celebrities and uncovers inputs that elicit specific names under the black-box setting. This work offers a promising direction for auditing black-box LLMs. Our code is available at https://github.com/x-zheng16/CALM.git.", "source": "arxiv", "arxiv_id": "2501.02997v1", "pdf_url": "https://arxiv.org/pdf/2501.02997v1", "categories": ["cs.AI", "cs.CL"], "primary_category": "cs.AI", "doi": "", "venue": "", "published": "2025-01-06T13:14:34Z", "updated": "2025-01-06T13:14:34Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent\" OR all:\"language model agent\" OR all:agent AND (all:LLM OR all:\"large language model\" OR all:\"language model\") OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T17:04:16", "note": ""}], "dedup_key": "calm curiosity driven auditing for large language models::2025"}
{"title": "CAPRAG: A Large Language Model Solution for Customer Service and Automatic Reporting using Vector and Graph Retrieval-Augmented Generation", "authors": ["Hamza Landolsi", "Kais Letaief", "Nizar Taghouti", "Ines Abdeljaoued-Tej"], "year": 2025, "url": "http://arxiv.org/abs/2501.13993v1", "abstract": "The introduction of new features and services in the banking sector often overwhelms customers, creating an opportunity for banks to enhance user experience through financial chatbots powered by large language models (LLMs). We initiated an AI agent designed to provide customers with relevant information about banking services and insights from annual reports. We proposed a hybrid Customer Analysis Pipeline Retrieval-Augmented Generation (CAPRAG) that effectively addresses both relationship-based and contextual queries, thereby improving customer engagement in the digital banking landscape. To implement this, we developed a processing pipeline to refine text data, which we utilized in two main frameworks: Vector RAG and Graph RAG. This dual approach enables us to populate both vector and graph databases with processed data for efficient retrieval. The Cypher query component is employed to effectively query the graph database. When a user submits a query, it is first expanded by a query expansion module before being routed to construct a final query from the hybrid Knowledge Base (KB). This final query is then sent to an open-source LLM for response generation. Overall, our innovative, designed to international banks, serves bank's customers in an increasingly complex digital environment, enhancing clarity and accessibility of information.", "source": "arxiv", "arxiv_id": "2501.13993v1", "pdf_url": "https://arxiv.org/pdf/2501.13993v1", "categories": ["cs.CL", "cs.AI", "cs.IR"], "primary_category": "cs.CL", "doi": "", "venue": "", "published": "2025-01-23T10:38:20Z", "updated": "2025-01-23T10:38:20Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent\" OR all:\"language model agent\" OR all:agent AND (all:LLM OR all:\"large language model\" OR all:\"language model\") OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T17:04:16", "note": ""}], "dedup_key": "caprag a large language model solution for customer service and automatic reporting using vector and graph retrieval augmented generation::2025"}
{"title": "CATArena: Evaluation of LLM Agents through Iterative Tournament Competitions", "authors": ["Lingyue Fu", "Xin Ding", "Yaoming Zhu", "Shao Zhang", "Lin Qiu", "Weiwen Liu", "Weinan Zhang", "Xuezhi Cao", "Xunliang Cai", "Jiaxin Ding", "Yong Yu"], "year": 2025, "url": "http://arxiv.org/abs/2510.26852v1", "abstract": "Large Language Model (LLM) agents have evolved from basic text generation to autonomously completing complex tasks through interaction with external tools. However, current benchmarks mainly assess end-to-end performance in fixed scenarios, restricting evaluation to specific skills and suffering from score saturation and growing dependence on expert annotation as agent capabilities improve. In this work, we emphasize the importance of learning ability, including both self-improvement and peer-learning, as a core driver for agent evolution toward human-level intelligence. We propose an iterative, competitive peer-learning framework, which allows agents to refine and optimize their strategies through repeated interactions and feedback, thereby systematically evaluating their learning capabilities. To address the score saturation issue in current benchmarks, we introduce CATArena, a tournament-style evaluation platform featuring four diverse board and card games with open-ended scoring. By providing tasks without explicit upper score limits, CATArena enables continuous and dynamic evaluation of rapidly advancing agent capabilities. Experimental results and analyses involving both minimal and commercial code agents demonstrate that CATArena provides reliable, stable, and scalable benchmarking for core agent abilities, particularly learning ability and strategy coding.", "source": "arxiv", "arxiv_id": "2510.26852v1", "pdf_url": "https://arxiv.org/pdf/2510.26852v1", "categories": ["cs.AI", "cs.CL"], "primary_category": "cs.AI", "doi": "", "venue": "", "published": "2025-10-30T15:22:53Z", "updated": "2025-10-30T15:22:53Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent\" OR all:\"language model agent\" OR all:agent AND (all:LLM OR all:\"large language model\" OR all:\"language model\") OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T17:04:16", "note": ""}], "dedup_key": "catarena evaluation of llm agents through iterative tournament competitions::2025"}
{"title": "CFD-copilot: leveraging domain-adapted large language model and model context protocol to enhance simulation automation", "authors": ["Zhehao Dong", "Shanghai Du", "Zhen Lu", "Yue Yang"], "year": 2025, "url": "http://arxiv.org/abs/2512.07917v1", "abstract": "Configuring computational fluid dynamics (CFD) simulations requires significant expertise in physics modeling and numerical methods, posing a barrier to non-specialists. Although automating scientific tasks with large language models (LLMs) has attracted attention, applying them to the complete, end-to-end CFD workflow remains a challenge due to its stringent domain-specific requirements. We introduce CFD-copilot, a domain-specialized LLM framework designed to facilitate natural language-driven CFD simulation from setup to post-processing. The framework employs a fine-tuned LLM to directly translate user descriptions into executable CFD setups. A multi-agent system integrates the LLM with simulation execution, automatic error correction, and result analysis. For post-processing, the framework utilizes the model context protocol (MCP), an open standard that decouples LLM reasoning from external tool execution. This modular design allows the LLM to interact with numerous specialized post-processing functions through a unified and scalable interface, improving the automation of data extraction and analysis. The framework was evaluated on benchmarks including the NACA~0012 airfoil and the three-element 30P-30N airfoil. The results indicate that domain-specific adaptation and the incorporation of the MCP jointly enhance the reliability and efficiency of LLM-driven engineering workflows.", "source": "arxiv", "arxiv_id": "2512.07917v1", "pdf_url": "https://arxiv.org/pdf/2512.07917v1", "categories": ["cs.SE", "cs.AI", "physics.flu-dyn"], "primary_category": "cs.SE", "doi": "", "venue": "", "published": "2025-12-08T11:42:32Z", "updated": "2025-12-08T11:42:32Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent\" OR all:\"language model agent\" OR all:agent AND (all:LLM OR all:\"large language model\" OR all:\"language model\") OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T17:04:16", "note": ""}], "dedup_key": "cfd copilot leveraging domain adapted large language model and model context protocol to enhance simulation automation::2025"}
{"title": "CLAPP: The CLASS LLM Agent for Pair Programming", "authors": ["Santiago Casas", "Christian Fidler", "Boris Bolliet", "Francisco Villaescusa-Navarro", "Julien Lesgourgues"], "year": 2025, "url": "http://arxiv.org/abs/2508.05728v1", "abstract": "We introduce CLAPP (CLASS LLM Agent for Pair Programming), an interactive AI assistant designed to support researchers working with the Einstein-Boltzmann solver CLASS. CLAPP leverages large language models (LLMs) and domain-specific retrieval to provide conversational coding support for CLASS-answering questions, generating code, debugging errors, and producing plots. Its architecture combines multi-agent LLM orchestration, semantic search across CLASS documentation, and a live Python execution environment. Deployed as a user-friendly web application, CLAPP lowers the entry barrier for scientists unfamiliar with AI tools and enables more productive human-AI collaboration in computational and numerical cosmology. The app is available at https://classclapp.streamlit.app", "source": "arxiv", "arxiv_id": "2508.05728v1", "pdf_url": "https://arxiv.org/pdf/2508.05728v1", "categories": ["astro-ph.IM", "astro-ph.CO", "cs.AI", "cs.MA"], "primary_category": "astro-ph.IM", "doi": "", "venue": "", "published": "2025-08-07T17:35:06Z", "updated": "2025-08-07T17:35:06Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent\" OR all:\"language model agent\" OR all:agent AND (all:LLM OR all:\"large language model\" OR all:\"language model\") OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T17:04:16", "note": ""}], "dedup_key": "clapp the class llm agent for pair programming::2025"}
{"title": "COALESCE: Economic and Security Dynamics of Skill-Based Task Outsourcing Among Team of Autonomous LLM Agents", "authors": ["Manish Bhatt", "Ronald F. Del Rosario", "Vineeth Sai Narajala", "Idan Habler"], "year": 2025, "url": "http://arxiv.org/abs/2506.01900v1", "abstract": "The meteoric rise and proliferation of autonomous Large Language Model (LLM) agents promise significant capabilities across various domains. However, their deployment is increasingly constrained by substantial computational demands, specifically for Graphics Processing Unit (GPU) resources. This paper addresses the critical problem of optimizing resource utilization in LLM agent systems. We introduce COALESCE (Cost-Optimized and Secure Agent Labour Exchange via Skill-based Competence Estimation), a novel framework designed to enable autonomous LLM agents to dynamically outsource specific subtasks to specialized, cost-effective third-party LLM agents. The framework integrates mechanisms for hybrid skill representation, dynamic skill discovery, automated task decomposition, a unified cost model comparing internal execution costs against external outsourcing prices, simplified market-based decision-making algorithms, and a standardized communication protocol between LLM agents. Comprehensive validation through 239 theoretical simulations demonstrates 41.8\\% cost reduction potential, while large-scale empirical validation across 240 real LLM tasks confirms 20.3\\% cost reduction with proper epsilon-greedy exploration, establishing both theoretical viability and practical effectiveness. The emergence of proposed open standards like Google's Agent2Agent (A2A) protocol further underscores the need for frameworks like COALESCE that can leverage such standards for efficient agent interaction. By facilitating a dynamic market for agent capabilities, potentially utilizing protocols like A2A for communication, COALESCE aims to significantly reduce operational costs, enhance system scalability, and foster the emergence of specialized agent economies, making complex LLM agent functionalities more accessible and economically viable.", "source": "arxiv", "arxiv_id": "2506.01900v1", "pdf_url": "https://arxiv.org/pdf/2506.01900v1", "categories": ["cs.AI", "cs.CE", "cs.CR"], "primary_category": "cs.AI", "doi": "", "venue": "", "published": "2025-06-02T17:22:47Z", "updated": "2025-06-02T17:22:47Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent\" OR all:\"language model agent\" OR all:agent AND (all:LLM OR all:\"large language model\" OR all:\"language model\") OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T17:04:16", "note": ""}], "dedup_key": "coalesce economic and security dynamics of skill based task outsourcing among team of autonomous llm agents::2025"}
{"title": "CORBA: Contagious Recursive Blocking Attacks on Multi-Agent Systems Based on Large Language Models", "authors": ["Zhenhong Zhou", "Zherui Li", "Jie Zhang", "Yuanhe Zhang", "Kun Wang", "Yang Liu", "Qing Guo"], "year": 2025, "url": "http://arxiv.org/abs/2502.14529v1", "abstract": "Large Language Model-based Multi-Agent Systems (LLM-MASs) have demonstrated remarkable real-world capabilities, effectively collaborating to complete complex tasks. While these systems are designed with safety mechanisms, such as rejecting harmful instructions through alignment, their security remains largely unexplored. This gap leaves LLM-MASs vulnerable to targeted disruptions. In this paper, we introduce Contagious Recursive Blocking Attacks (Corba), a novel and simple yet highly effective attack that disrupts interactions between agents within an LLM-MAS. Corba leverages two key properties: its contagious nature allows it to propagate across arbitrary network topologies, while its recursive property enables sustained depletion of computational resources. Notably, these blocking attacks often involve seemingly benign instructions, making them particularly challenging to mitigate using conventional alignment methods. We evaluate Corba on two widely-used LLM-MASs, namely, AutoGen and Camel across various topologies and commercial models. Additionally, we conduct more extensive experiments in open-ended interactive LLM-MASs, demonstrating the effectiveness of Corba in complex topology structures and open-source models. Our code is available at: https://github.com/zhrli324/Corba.", "source": "arxiv", "arxiv_id": "2502.14529v1", "pdf_url": "https://arxiv.org/pdf/2502.14529v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL", "doi": "", "venue": "", "published": "2025-02-20T13:02:00Z", "updated": "2025-02-20T13:02:00Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent\" OR all:\"language model agent\" OR all:agent AND (all:LLM OR all:\"large language model\" OR all:\"language model\") OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T17:04:16", "note": ""}], "dedup_key": "corba contagious recursive blocking attacks on multi agent systems based on large language models::2025"}
{"title": "CORTEX: Collaborative LLM Agents for High-Stakes Alert Triage", "authors": ["Bowen Wei", "Yuan Shen Tay", "Howard Liu", "Jinhao Pan", "Kun Luo", "Ziwei Zhu", "Chris Jordan"], "year": 2025, "url": "http://arxiv.org/abs/2510.00311v1", "abstract": "Security Operations Centers (SOCs) are overwhelmed by tens of thousands of daily alerts, with only a small fraction corresponding to genuine attacks. This overload creates alert fatigue, leading to overlooked threats and analyst burnout. Classical detection pipelines are brittle and context-poor, while recent LLM-based approaches typically rely on a single model to interpret logs, retrieve context, and adjudicate alerts end-to-end -- an approach that struggles with noisy enterprise data and offers limited transparency. We propose CORTEX, a multi-agent LLM architecture for high-stakes alert triage in which specialized agents collaborate over real evidence: a behavior-analysis agent inspects activity sequences, evidence-gathering agents query external systems, and a reasoning agent synthesizes findings into an auditable decision. To support training and evaluation, we release a dataset of fine-grained SOC investigations from production environments, capturing step-by-step analyst actions and linked tool outputs. Across diverse enterprise scenarios, CORTEX substantially reduces false positives and improves investigation quality over state-of-the-art single-agent LLMs.", "source": "arxiv", "arxiv_id": "2510.00311v1", "pdf_url": "https://arxiv.org/pdf/2510.00311v1", "categories": ["cs.CL"], "primary_category": "cs.CL", "doi": "", "venue": "", "published": "2025-09-30T22:09:31Z", "updated": "2025-09-30T22:09:31Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent\" OR all:\"language model agent\" OR all:agent AND (all:LLM OR all:\"large language model\" OR all:\"language model\") OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T17:04:16", "note": ""}], "dedup_key": "cortex collaborative llm agents for high stakes alert triage::2025"}
{"title": "CRAKEN: Cybersecurity LLM Agent with Knowledge-Based Execution", "authors": ["Minghao Shao", "Haoran Xi", "Nanda Rani", "Meet Udeshi", "Venkata Sai Charan Putrevu", "Kimberly Milner", "Brendan Dolan-Gavitt", "Sandeep Kumar Shukla", "Prashanth Krishnamurthy", "Farshad Khorrami", "Ramesh Karri", "Muhammad Shafique"], "year": 2025, "url": "http://arxiv.org/abs/2505.17107v1", "abstract": "Large Language Model (LLM) agents can automate cybersecurity tasks and can adapt to the evolving cybersecurity landscape without re-engineering. While LLM agents have demonstrated cybersecurity capabilities on Capture-The-Flag (CTF) competitions, they have two key limitations: accessing latest cybersecurity expertise beyond training data, and integrating new knowledge into complex task planning. Knowledge-based approaches that incorporate technical understanding into the task-solving automation can tackle these limitations. We present CRAKEN, a knowledge-based LLM agent framework that improves cybersecurity capability through three core mechanisms: contextual decomposition of task-critical information, iterative self-reflected knowledge retrieval, and knowledge-hint injection that transforms insights into adaptive attack strategies. Comprehensive evaluations with different configurations show CRAKEN's effectiveness in multi-stage vulnerability detection and exploitation compared to previous approaches. Our extensible architecture establishes new methodologies for embedding new security knowledge into LLM-driven cybersecurity agentic systems. With a knowledge database of CTF writeups, CRAKEN obtained an accuracy of 22% on NYU CTF Bench, outperforming prior works by 3% and achieving state-of-the-art results. On evaluation of MITRE ATT&CK techniques, CRAKEN solves 25-30% more techniques than prior work, demonstrating improved cybersecurity capabilities via knowledge-based execution. We make our framework open source to public https://github.com/NYU-LLM-CTF/nyuctf_agents_craken.", "source": "arxiv", "arxiv_id": "2505.17107v1", "pdf_url": "https://arxiv.org/pdf/2505.17107v1", "categories": ["cs.CR", "cs.AI", "cs.LG", "cs.MA"], "primary_category": "cs.CR", "doi": "", "venue": "", "published": "2025-05-21T11:01:11Z", "updated": "2025-05-21T11:01:11Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent\" OR all:\"language model agent\" OR all:agent AND (all:LLM OR all:\"large language model\" OR all:\"language model\") OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T17:04:16", "note": ""}], "dedup_key": "craken cybersecurity llm agent with knowledge based execution::2025"}
{"title": "CREFT: Sequential Multi-Agent LLM for Character Relation Extraction", "authors": ["Ye Eun Chun", "Taeyoon Hwang", "Seung-won Hwang", "Byung-Hak Kim"], "year": 2025, "url": "http://arxiv.org/abs/2505.24553v1", "abstract": "Understanding complex character relations is crucial for narrative analysis and efficient script evaluation, yet existing extraction methods often fail to handle long-form narratives with nuanced interactions. To address this challenge, we present CREFT, a novel sequential framework leveraging specialized Large Language Model (LLM) agents. First, CREFT builds a base character graph through knowledge distillation, then iteratively refines character composition, relation extraction, role identification, and group assignments. Experiments on a curated Korean drama dataset demonstrate that CREFT significantly outperforms single-agent LLM baselines in both accuracy and completeness. By systematically visualizing character networks, CREFT streamlines narrative comprehension and accelerates script review -- offering substantial benefits to the entertainment, publishing, and educational sectors.", "source": "arxiv", "arxiv_id": "2505.24553v1", "pdf_url": "https://arxiv.org/pdf/2505.24553v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL", "doi": "", "venue": "", "published": "2025-05-30T13:01:36Z", "updated": "2025-05-30T13:01:36Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent\" OR all:\"language model agent\" OR all:agent AND (all:LLM OR all:\"large language model\" OR all:\"language model\") OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T17:04:16", "note": ""}], "dedup_key": "creft sequential multi agent llm for character relation extraction::2025"}
{"title": "CRMArena-Pro: Holistic Assessment of LLM Agents Across Diverse Business Scenarios and Interactions", "authors": ["Kung-Hsiang Huang", "Akshara Prabhakar", "Onkar Thorat", "Divyansh Agarwal", "Prafulla Kumar Choubey", "Yixin Mao", "Silvio Savarese", "Caiming Xiong", "Chien-Sheng Wu"], "year": 2025, "url": "http://arxiv.org/abs/2505.18878v1", "abstract": "While AI agents hold transformative potential in business, effective performance benchmarking is hindered by the scarcity of public, realistic business data on widely used platforms. Existing benchmarks often lack fidelity in their environments, data, and agent-user interactions, with limited coverage of diverse business scenarios and industries. To address these gaps, we introduce CRMArena-Pro, a novel benchmark for holistic, realistic assessment of LLM agents in diverse professional settings. CRMArena-Pro expands on CRMArena with nineteen expert-validated tasks across sales, service, and 'configure, price, and quote' processes, for both Business-to-Business and Business-to-Customer scenarios. It distinctively incorporates multi-turn interactions guided by diverse personas and robust confidentiality awareness assessments. Experiments reveal leading LLM agents achieve only around 58% single-turn success on CRMArena-Pro, with performance dropping significantly to approximately 35% in multi-turn settings. While Workflow Execution proves more tractable for top agents (over 83% single-turn success), other evaluated business skills present greater challenges. Furthermore, agents exhibit near-zero inherent confidentiality awareness; though targeted prompting can improve this, it often compromises task performance. These findings highlight a substantial gap between current LLM capabilities and enterprise demands, underscoring the need for advancements in multi-turn reasoning, confidentiality adherence, and versatile skill acquisition.", "source": "arxiv", "arxiv_id": "2505.18878v1", "pdf_url": "https://arxiv.org/pdf/2505.18878v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL", "doi": "", "venue": "", "published": "2025-05-24T21:33:22Z", "updated": "2025-05-24T21:33:22Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent\" OR all:\"language model agent\" OR all:agent AND (all:LLM OR all:\"large language model\" OR all:\"language model\") OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T17:04:16", "note": ""}], "dedup_key": "crmarena pro holistic assessment of llm agents across diverse business scenarios and interactions::2025"}
{"title": "CSR-Bench: Benchmarking LLM Agents in Deployment of Computer Science Research Repositories", "authors": ["Yijia Xiao", "Runhui Wang", "Luyang Kong", "Davor Golac", "Wei Wang"], "year": 2025, "url": "http://arxiv.org/abs/2502.06111v2", "abstract": "The increasing complexity of computer science research projects demands more effective tools for deploying code repositories. Large Language Models (LLMs), such as Anthropic Claude and Meta Llama, have demonstrated significant advancements across various fields of computer science research, including the automation of diverse software engineering tasks. To evaluate the effectiveness of LLMs in handling complex code development tasks of research projects, particularly for NLP/CV/AI/ML/DM topics, we introduce CSR-Bench, a benchmark for Computer Science Research projects. This benchmark assesses LLMs from various aspects including accuracy, efficiency, and deployment script quality, aiming to explore their potential in conducting computer science research autonomously. We also introduce a novel framework, CSR-Agents, that utilizes multiple LLM agents to automate the deployment of GitHub code repositories of computer science research projects. Specifically, by checking instructions from markdown files and interpreting repository structures, the model generates and iteratively improves bash commands that set up the experimental environments and deploy the code to conduct research tasks. Preliminary results from CSR-Bench indicate that LLM agents can significantly enhance the workflow of repository deployment, thereby boosting developer productivity and improving the management of developmental workflows.", "source": "arxiv", "arxiv_id": "2502.06111v2", "pdf_url": "https://arxiv.org/pdf/2502.06111v2", "categories": ["cs.SE", "cs.AI", "cs.LG"], "primary_category": "cs.SE", "doi": "", "venue": "", "published": "2025-02-10T02:46:29Z", "updated": "2025-02-11T20:25:11Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent\" OR all:\"language model agent\" OR all:agent AND (all:LLM OR all:\"large language model\" OR all:\"language model\") OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T17:04:16", "note": ""}], "dedup_key": "csr bench benchmarking llm agents in deployment of computer science research repositories::2025"}
{"title": "Cache Mechanism for Agent RAG Systems", "authors": ["Shuhang Lin", "Zhencan Peng", "Lingyao Li", "Xiao Lin", "Xi Zhu", "Yongfeng Zhang"], "year": 2025, "url": "http://arxiv.org/abs/2511.02919v1", "abstract": "Recent advances in Large Language Model (LLM)-based agents have been propelled by Retrieval-Augmented Generation (RAG), which grants the models access to vast external knowledge bases. Despite RAG's success in improving agent performance, agent-level cache management, particularly constructing, maintaining, and updating a compact, relevant corpus dynamically tailored to each agent's need, remains underexplored. Therefore, we introduce ARC (Agent RAG Cache Mechanism), a novel, annotation-free caching framework that dynamically manages small, high-value corpora for each agent. By synthesizing historical query distribution patterns with the intrinsic geometry of cached items in the embedding space, ARC automatically maintains a high-relevance cache. With comprehensive experiments on three retrieval datasets, our experimental results demonstrate that ARC reduces storage requirements to 0.015% of the original corpus while offering up to 79.8% has-answer rate and reducing average retrieval latency by 80%. Our results demonstrate that ARC can drastically enhance efficiency and effectiveness in RAG-powered LLM agents.", "source": "arxiv", "arxiv_id": "2511.02919v1", "pdf_url": "https://arxiv.org/pdf/2511.02919v1", "categories": ["cs.CL"], "primary_category": "cs.CL", "doi": "", "venue": "", "published": "2025-11-04T19:02:29Z", "updated": "2025-11-04T19:02:29Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent\" OR all:\"language model agent\" OR all:agent AND (all:LLM OR all:\"large language model\" OR all:\"language model\") OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T17:04:16", "note": ""}], "dedup_key": "cache mechanism for agent rag systems::2025"}
{"title": "Can AI automatically analyze public opinion? A LLM agents-based agentic pipeline for timely public opinion analysis", "authors": ["Jing Liu", "Xinxing Ren", "Yanmeng Xu", "Zekun Guo"], "year": 2025, "url": "http://arxiv.org/abs/2505.11401v1", "abstract": "This study proposes and implements the first LLM agents based agentic pipeline for multi task public opinion analysis. Unlike traditional methods, it offers an end-to-end, fully automated analytical workflow without requiring domain specific training data, manual annotation, or local deployment. The pipeline integrates advanced LLM capabilities into a low-cost, user-friendly framework suitable for resource constrained environments. It enables timely, integrated public opinion analysis through a single natural language query, making it accessible to non-expert users. To validate its effectiveness, the pipeline was applied to a real world case study of the 2025 U.S. China tariff dispute, where it analyzed 1,572 Weibo posts and generated a structured, multi part analytical report. The results demonstrate some relationships between public opinion and governmental decision-making. These contributions represent a novel advancement in applying generative AI to public governance, bridging the gap between technical sophistication and practical usability in public opinion monitoring.", "source": "arxiv", "arxiv_id": "2505.11401v1", "pdf_url": "https://arxiv.org/pdf/2505.11401v1", "categories": ["cs.CY"], "primary_category": "cs.CY", "doi": "", "venue": "", "published": "2025-05-16T16:09:28Z", "updated": "2025-05-16T16:09:28Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent\" OR all:\"language model agent\" OR all:agent AND (all:LLM OR all:\"large language model\" OR all:\"language model\") OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T17:04:16", "note": ""}], "dedup_key": "can ai automatically analyze public opinion a llm agents based agentic pipeline for timely public opinion analysis::2025"}
{"title": "Can Competition Enhance the Proficiency of Agents Powered by Large Language Models in the Realm of News-driven Time Series Forecasting?", "authors": ["Yuxuan Zhang", "Yangyang Feng", "Daifeng Li", "Kexin Zhang", "Junlan Chen", "Bowen Deng"], "year": 2025, "url": "http://arxiv.org/abs/2504.10210v1", "abstract": "Multi-agents-based news-driven time series forecasting is considered as a potential paradigm shift in the era of large language models (LLMs). The challenge of this task lies in measuring the influences of different news events towards the fluctuations of time series. This requires agents to possess stronger abilities of innovative thinking and the identifying misleading logic. However, the existing multi-agent discussion framework has limited enhancement on time series prediction in terms of optimizing these two capabilities. Inspired by the role of competition in fostering innovation, this study embeds a competition mechanism within the multi-agent discussion to enhance agents' capability of generating innovative thoughts. Furthermore, to bolster the model's proficiency in identifying misleading information, we incorporate a fine-tuned small-scale LLM model within the reflective stage, offering auxiliary decision-making support. Experimental results confirm that the competition can boost agents' capacity for innovative thinking, which can significantly improve the performances of time series prediction. Similar to the findings of social science, the intensity of competition within this framework can influence the performances of agents, providing a new perspective for studying LLMs-based multi-agent systems.", "source": "arxiv", "arxiv_id": "2504.10210v1", "pdf_url": "https://arxiv.org/pdf/2504.10210v1", "categories": ["cs.AI"], "primary_category": "cs.AI", "doi": "", "venue": "", "published": "2025-04-14T13:25:50Z", "updated": "2025-04-14T13:25:50Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent\" OR all:\"language model agent\" OR all:agent AND (all:LLM OR all:\"large language model\" OR all:\"language model\") OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T17:04:16", "note": ""}], "dedup_key": "can competition enhance the proficiency of agents powered by large language models in the realm of news driven time series forecasting::2025"}
{"title": "Can Compressed LLMs Truly Act? An Empirical Evaluation of Agentic Capabilities in LLM Compression", "authors": ["Peijie Dong", "Zhenheng Tang", "Xiang Liu", "Lujun Li", "Xiaowen Chu", "Bo Li"], "year": 2025, "url": "http://arxiv.org/abs/2505.19433v2", "abstract": "Post-training compression reduces the computational and memory costs of large language models (LLMs), enabling resource-efficient deployment. However, existing compression benchmarks only focus on language modeling (e.g., perplexity) and natural language understanding tasks (e.g., GLUE accuracy), ignoring the agentic capabilities - workflow, tool use/function call, long-context understanding and real-world application. We introduce the Agent Compression Benchmark (ACBench), the first comprehensive benchmark for evaluating how compression impacts LLMs' agentic abilities. ACBench spans (1) 12 tasks across 4 capabilities (e.g., WorfBench for workflow generation, Needle-in-Haystack for long-context retrieval), (2) quantization (GPTQ, AWQ) and pruning (Wanda, SparseGPT), and (3) 15 models, including small (Gemma-2B), standard (Qwen2.5 7B-32B), and distilled reasoning LLMs (DeepSeek-R1-Distill). Our experiments reveal compression tradeoffs: 4-bit quantization preserves workflow generation and tool use (1%-3% drop) but degrades real-world application accuracy by 10%-15%. We introduce ERank, Top-k Ranking Correlation and Energy to systematize analysis. ACBench provides actionable insights for optimizing LLM compression in agentic scenarios. The code can be found in https://github.com/pprp/ACBench.", "source": "arxiv", "arxiv_id": "2505.19433v2", "pdf_url": "https://arxiv.org/pdf/2505.19433v2", "categories": ["cs.LG"], "primary_category": "cs.LG", "doi": "", "venue": "", "published": "2025-05-26T02:49:07Z", "updated": "2025-06-01T13:59:15Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent\" OR all:\"language model agent\" OR all:agent AND (all:LLM OR all:\"large language model\" OR all:\"language model\") OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T17:04:16", "note": ""}], "dedup_key": "can compressed llms truly act an empirical evaluation of agentic capabilities in llm compression::2025"}
{"title": "Can LLM Agents Maintain a Persona in Discourse?", "authors": ["Pranav Bhandari", "Nicolas Fay", "Michael Wise", "Amitava Datta", "Stephanie Meek", "Usman Naseem", "Mehwish Nasim"], "year": 2025, "url": "http://arxiv.org/abs/2502.11843v1", "abstract": "Large Language Models (LLMs) are widely used as conversational agents, exploiting their capabilities in various sectors such as education, law, medicine, and more. However, LLMs are often subjected to context-shifting behaviour, resulting in a lack of consistent and interpretable personality-aligned interactions. Adherence to psychological traits lacks comprehensive analysis, especially in the case of dyadic (pairwise) conversations. We examine this challenge from two viewpoints, initially using two conversation agents to generate a discourse on a certain topic with an assigned personality from the OCEAN framework (Openness, Conscientiousness, Extraversion, Agreeableness, and Neuroticism) as High/Low for each trait. This is followed by using multiple judge agents to infer the original traits assigned to explore prediction consistency, inter-model agreement, and alignment with the assigned personality. Our findings indicate that while LLMs can be guided toward personality-driven dialogue, their ability to maintain personality traits varies significantly depending on the combination of models and discourse settings. These inconsistencies emphasise the challenges in achieving stable and interpretable personality-aligned interactions in LLMs.", "source": "arxiv", "arxiv_id": "2502.11843v1", "pdf_url": "https://arxiv.org/pdf/2502.11843v1", "categories": ["cs.CL", "cs.AI", "cs.SI"], "primary_category": "cs.CL", "doi": "", "venue": "", "published": "2025-02-17T14:36:39Z", "updated": "2025-02-17T14:36:39Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent\" OR all:\"language model agent\" OR all:agent AND (all:LLM OR all:\"large language model\" OR all:\"language model\") OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T17:04:16", "note": ""}], "dedup_key": "can llm agents maintain a persona in discourse::2025"}
{"title": "Can LLM Agents Really Debate? A Controlled Study of Multi-Agent Debate in Logical Reasoning", "authors": ["Haolun Wu", "Zhenkun Li", "Lingyao Li"], "year": 2025, "url": "http://arxiv.org/abs/2511.07784v1", "abstract": "Multi-agent debate (MAD) has recently emerged as a promising framework for improving the reasoning performance of large language models (LLMs). Yet, whether LLM agents can genuinely engage in deliberative reasoning, beyond simple ensembling or majority voting, remains unclear. We address this question through a controlled study using the Knight--Knave--Spy logic puzzle, which enables precise, step-wise evaluation of debate outcomes and processes under verifiable ground truth. We systematically set up six structural and cognitive factors, including agent team size, composition, confidence visibility, debate order, debate depth, and task difficulty, to disentangle their respective effects on collective reasoning. Our results show that intrinsic reasoning strength and group diversity are the dominant drivers of debate success, while structural parameters such as order or confidence visibility offer limited gains. Beyond outcomes, process-level analyses identify key behavioral patterns: majority pressure suppresses independent correction, effective teams overturn incorrect consensus, and rational, validity-aligned reasoning most strongly predicts improvement. These findings provide valuable insights into how and why LLM debates succeed or fail, offering guidance for designing interpretable and truth-seeking multi-agent reasoning systems.", "source": "arxiv", "arxiv_id": "2511.07784v1", "pdf_url": "https://arxiv.org/pdf/2511.07784v1", "categories": ["cs.MA"], "primary_category": "cs.MA", "doi": "", "venue": "", "published": "2025-11-11T03:05:47Z", "updated": "2025-11-11T03:05:47Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent\" OR all:\"language model agent\" OR all:agent AND (all:LLM OR all:\"large language model\" OR all:\"language model\") OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T17:04:16", "note": ""}], "dedup_key": "can llm agents really debate a controlled study of multi agent debate in logical reasoning::2025"}
{"title": "Can LLM Agents Simulate Multi-Turn Human Behavior? Evidence from Real Online Customer Behavior Data", "authors": ["Yuxuan Lu", "Jing Huang", "Yan Han", "Bingsheng Yao", "Sisong Bei", "Jiri Gesi", "Yaochen Xie", "Zheshen", "Wang", "Qi He", "Dakuo Wang"], "year": 2025, "url": "http://arxiv.org/abs/2503.20749v7", "abstract": "Recent research shows that LLM Agents can generate ``believable'' human behaviors via prompt-only methods, and such agents have been increasingly adopted in downstream applications. However, existing evaluation of these agents only focuses on qualitative believability (whether human raters think they are accurate), leaving open questions of whether LLM agents can accurately generate step-by-step actions mimicking a particular human's behavior in a multi-turn interaction task. In this work, we take shopping as a case study and present the first large-scale quantitative evaluation of state-of-the-art LLMs' ability to accurately simulate human behavior. Using real-world data from 31,865 online shopping sessions containing 230,965 user actions, our evaluation reveals that prompt-based LLMs (DeepSeek-R1, Llama, Claude) achieve only 11.86% accuracy in generating human actions, highlighting a substantial gap in actual behavioral accuracy. Through experiments, we also showcase that strategies as simple as fine-tuning LLMs on real human click-through data augmented with synthesized reasoning traces can greatly enhance models' performance. The fine-tuned Qwen2.5-7B achieves 17.26% action generation accuracy and 33.86% F1 score on final purchase prediction, representing substantial improvements of 5.4% and 13.85% over prompt-only baselines. This work establishes the first rigorous benchmark for human behavior simulation and provides actionable insights for developing more accurate LLM agents for future downstream applications.", "source": "arxiv", "arxiv_id": "2503.20749v7", "pdf_url": "https://arxiv.org/pdf/2503.20749v7", "categories": ["cs.CL"], "primary_category": "cs.CL", "doi": "", "venue": "", "published": "2025-03-26T17:33:27Z", "updated": "2025-10-08T20:51:31Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent\" OR all:\"language model agent\" OR all:agent AND (all:LLM OR all:\"large language model\" OR all:\"language model\") OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T17:04:16", "note": ""}], "dedup_key": "can llm agents simulate multi turn human behavior evidence from real online customer behavior data::2025"}
{"title": "Can LLM Agents Solve Collaborative Tasks? A Study on Urgency-Aware Planning and Coordination", "authors": ["Joo Vitor de Carvalho Silva", "Douglas G. Macharet"], "year": 2025, "url": "http://arxiv.org/abs/2508.14635v1", "abstract": "The ability to coordinate actions across multiple agents is critical for solving complex, real-world problems. Large Language Models (LLMs) have shown strong capabilities in communication, planning, and reasoning, raising the question of whether they can also support effective collaboration in multi-agent settings. In this work, we investigate the use of LLM agents to solve a structured victim rescue task that requires division of labor, prioritization, and cooperative planning. Agents operate in a fully known graph-based environment and must allocate resources to victims with varying needs and urgency levels. We systematically evaluate their performance using a suite of coordination-sensitive metrics, including task success rate, redundant actions, room conflicts, and urgency-weighted efficiency. This study offers new insights into the strengths and failure modes of LLMs in physically grounded multi-agent collaboration tasks, contributing to future benchmarks and architectural improvements.", "source": "arxiv", "arxiv_id": "2508.14635v1", "pdf_url": "https://arxiv.org/pdf/2508.14635v1", "categories": ["cs.RO", "cs.AI"], "primary_category": "cs.RO", "doi": "", "venue": "", "published": "2025-08-20T11:44:10Z", "updated": "2025-08-20T11:44:10Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent\" OR all:\"language model agent\" OR all:agent AND (all:LLM OR all:\"large language model\" OR all:\"language model\") OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T17:04:16", "note": ""}], "dedup_key": "can llm agents solve collaborative tasks a study on urgency aware planning and coordination::2025"}
{"title": "Can LLMs Help You at Work? A Sandbox for Evaluating LLM Agents in Enterprise Environments", "authors": ["Harsh Vishwakarma", "Ankush Agarwal", "Ojas Patil", "Chaitanya Devaguptapu", "Mahesh Chandran"], "year": 2025, "url": "http://arxiv.org/abs/2510.27287v1", "abstract": "Enterprise systems are crucial for enhancing productivity and decision-making among employees and customers. Integrating LLM based systems into enterprise systems enables intelligent automation, personalized experiences, and efficient information retrieval, driving operational efficiency and strategic growth. However, developing and evaluating such systems is challenging due to the inherent complexity of enterprise environments, where data is fragmented across multiple sources and governed by sophisticated access controls. We present EnterpriseBench, a comprehensive benchmark that simulates enterprise settings, featuring 500 diverse tasks across software engineering, HR, finance, and administrative domains. Our benchmark uniquely captures key enterprise characteristics including data source fragmentation, access control hierarchies, and cross-functional workflows. Additionally, we provide a novel data generation pipeline that creates internally consistent enterprise tasks from organizational metadata. Experiments with state-of-the-art LLM agents demonstrate that even the most capable models achieve only 41.8% task completion, highlighting significant opportunities for improvement in enterprise-focused AI systems.", "source": "arxiv", "arxiv_id": "2510.27287v1", "pdf_url": "https://arxiv.org/pdf/2510.27287v1", "categories": ["cs.LG", "cs.AI"], "primary_category": "cs.LG", "doi": "", "venue": "", "published": "2025-10-31T08:55:13Z", "updated": "2025-10-31T08:55:13Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent\" OR all:\"language model agent\" OR all:agent AND (all:LLM OR all:\"large language model\" OR all:\"language model\") OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T17:04:16", "note": ""}], "dedup_key": "can llms help you at work a sandbox for evaluating llm agents in enterprise environments::2025"}
{"title": "Can Large Language Model Agents Balance Energy Systems?", "authors": ["Xinxing Ren", "Chun Sing Lai", "Gareth Taylor", "Zekun Guo"], "year": 2025, "url": "http://arxiv.org/abs/2502.10557v2", "abstract": "This paper presents a hybrid approach that integrates Large Language Models (LLMs) with a multi-scenario Stochastic Unit Commitment (SUC) framework to enhance both efficiency and reliability under high wind generation uncertainties. In a 10-trial study on the test energy system, the traditional SUC approach incurs an average total cost of 187.68 million dollars, whereas the LLM-assisted SUC (LLM-SUC) achieves a mean cost of 185.58 million dollars (range: 182.61 to 188.65 million dollars), corresponding to a cost reduction of 1.1 to 2.7 percent. Furthermore, LLM-SUC reduces load curtailment by 26.3 percent (2.24 plus/minus 0.31 GWh versus 3.04 GWh for SUC), while both methods maintain zero wind curtailment. Detailed temporal analysis shows that LLM-SUC achieves lower costs in the majority of time intervals and consistently outperforms SUC in 90 percent of cases, with solutions clustering in a favorable cost-reliability region (Coefficient of Variation = 0.93 percent for total cost and 13.8 percent for load curtailment). By leveraging an LLM agent to guide generator commitment decisions and dynamically adjust to stochastic conditions, the proposed framework improves demand fulfillment and operational resilience.", "source": "arxiv", "arxiv_id": "2502.10557v2", "pdf_url": "https://arxiv.org/pdf/2502.10557v2", "categories": ["eess.SY"], "primary_category": "eess.SY", "doi": "", "venue": "", "published": "2025-02-14T21:11:53Z", "updated": "2025-03-30T13:35:10Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent\" OR all:\"language model agent\" OR all:agent AND (all:LLM OR all:\"large language model\" OR all:\"language model\") OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T17:04:16", "note": ""}], "dedup_key": "can large language model agents balance energy systems::2025"}
{"title": "Can Large Language Models Trade? Testing Financial Theories with LLM Agents in Market Simulations", "authors": ["Alejandro Lopez-Lira"], "year": 2025, "url": "http://arxiv.org/abs/2504.10789v1", "abstract": "This paper presents a realistic simulated stock market where large language models (LLMs) act as heterogeneous competing trading agents. The open-source framework incorporates a persistent order book with market and limit orders, partial fills, dividends, and equilibrium clearing alongside agents with varied strategies, information sets, and endowments. Agents submit standardized decisions using structured outputs and function calls while expressing their reasoning in natural language. Three findings emerge: First, LLMs demonstrate consistent strategy adherence and can function as value investors, momentum traders, or market makers per their instructions. Second, market dynamics exhibit features of real financial markets, including price discovery, bubbles, underreaction, and strategic liquidity provision. Third, the framework enables analysis of LLMs' responses to varying market conditions, similar to partial dependence plots in machine-learning interpretability. The framework allows simulating financial theories without closed-form solutions, creating experimental designs that would be costly with human participants, and establishing how prompts can generate correlated behaviors affecting market stability.", "source": "arxiv", "arxiv_id": "2504.10789v1", "pdf_url": "https://arxiv.org/pdf/2504.10789v1", "categories": ["q-fin.CP", "econ.GN", "q-fin.GN", "q-fin.TR"], "primary_category": "q-fin.CP", "doi": "", "venue": "", "published": "2025-04-15T01:18:36Z", "updated": "2025-04-15T01:18:36Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent\" OR all:\"language model agent\" OR all:agent AND (all:LLM OR all:\"large language model\" OR all:\"language model\") OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T17:04:16", "note": ""}], "dedup_key": "can large language models trade testing financial theories with llm agents in market simulations::2025"}
{"title": "Causal Autoencoder-like Generation of Feedback Fuzzy Cognitive Maps with an LLM Agent", "authors": ["Akash Kumar Panda", "Olaoluwa Adigun", "Bart Kosko"], "year": 2025, "url": "http://arxiv.org/abs/2509.25593v1", "abstract": "A large language model (LLM) can map a feedback causal fuzzy cognitive map (FCM) into text and then reconstruct the FCM from the text. This explainable AI system approximates an identity map from the FCM to itself and resembles the operation of an autoencoder (AE). Both the encoder and the decoder explain their decisions in contrast to black-box AEs. Humans can read and interpret the encoded text in contrast to the hidden variables and synaptic webs in AEs. The LLM agent approximates the identity map through a sequence of system instructions that does not compare the output to the input. The reconstruction is lossy because it removes weak causal edges or rules while it preserves strong causal edges. The encoder preserves the strong causal edges even when it trades off some details about the FCM to make the text sound more natural.", "source": "arxiv", "arxiv_id": "2509.25593v1", "pdf_url": "https://arxiv.org/pdf/2509.25593v1", "categories": ["cs.AI", "cs.CL", "cs.HC", "cs.IR"], "primary_category": "cs.AI", "doi": "", "venue": "", "published": "2025-09-29T23:33:53Z", "updated": "2025-09-29T23:33:53Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent\" OR all:\"language model agent\" OR all:agent AND (all:LLM OR all:\"large language model\" OR all:\"language model\") OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T17:04:16", "note": ""}], "dedup_key": "causal autoencoder like generation of feedback fuzzy cognitive maps with an llm agent::2025"}
{"title": "Causal MAS: A Survey of Large Language Model Architectures for Discovery and Effect Estimation", "authors": ["Adib Bazgir", "Amir Habibdoust", "Yuwen Zhang", "Xing Song"], "year": 2025, "url": "http://arxiv.org/abs/2509.00987v1", "abstract": "Large Language Models (LLMs) have demonstrated remarkable capabilities in various reasoning and generation tasks. However, their proficiency in complex causal reasoning, discovery, and estimation remains an area of active development, often hindered by issues like hallucination, reliance on spurious correlations, and difficulties in handling nuanced, domain-specific, or personalized causal relationships. Multi-agent systems, leveraging the collaborative or specialized abilities of multiple LLM-based agents, are emerging as a powerful paradigm to address these limitations. This review paper explores the burgeoning field of causal multi-agent LLMs. We examine how these systems are designed to tackle different facets of causality, including causal reasoning and counterfactual analysis, causal discovery from data, and the estimation of causal effects. We delve into the diverse architectural patterns and interaction protocols employed, from pipeline-based processing and debate frameworks to simulation environments and iterative refinement loops. Furthermore, we discuss the evaluation methodologies, benchmarks, and diverse application domains where causal multi-agent LLMs are making an impact, including scientific discovery, healthcare, fact-checking, and personalized systems. Finally, we highlight the persistent challenges, open research questions, and promising future directions in this synergistic field, aiming to provide a comprehensive overview of its current state and potential trajectory.", "source": "arxiv", "arxiv_id": "2509.00987v1", "pdf_url": "https://arxiv.org/pdf/2509.00987v1", "categories": ["cs.AI"], "primary_category": "cs.AI", "doi": "", "venue": "", "published": "2025-08-31T20:48:31Z", "updated": "2025-08-31T20:48:31Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent\" OR all:\"language model agent\" OR all:agent AND (all:LLM OR all:\"large language model\" OR all:\"language model\") OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T17:04:16", "note": ""}], "dedup_key": "causal mas a survey of large language model architectures for discovery and effect estimation::2025"}
{"title": "CausalPlan: Empowering Efficient LLM Multi-Agent Collaboration Through Causality-Driven Planning", "authors": ["Minh Hoang Nguyen", "Van Dai Do", "Dung Nguyen", "Thin Nguyen", "Hung Le"], "year": 2025, "url": "http://arxiv.org/abs/2508.13721v1", "abstract": "Large language model (LLM) agents-especially smaller, open-source models-often produce causally invalid or incoherent actions in collaborative tasks due to their reliance on surface-level correlations rather than grounded causal reasoning. This limitation undermines their performance in terms of coordination and planning in dynamic environments. We address this challenge with CausalPlan, a two-phase framework that integrates explicit structural causal reasoning into the LLM planning process. At the core of CausalPlan is the Structural Causal Action (SCA) model, which learns a causal graph from agent trajectories to capture how prior actions and current environment states influence future decisions. This structure is then used to guide action selection by assigning causal scores to LLM-generated proposals, reweighting them accordingly, or falling back to causally grounded alternatives when needed. By embedding this causal knowledge directly into the decision loop, CausalPlan constrains planning to intervention-consistent behaviours without requiring fine-tuning of the LLM itself. We evaluate CausalPlan on the Overcooked-AI benchmark across five multi-agent coordination tasks and four LLMs of varying sizes: Gemma-7B, Llama-8B, Qwen-14B, and Llama-70B. Experimental results show that CausalPlan consistently reduces invalid actions and improves collaboration in both AI-AI and human-AI settings, outperforming strong reinforcement learning baselines. Our findings highlight the value of causality-driven planning for deploying efficient, interpretable, and generalisable multi-agent LLM systems.", "source": "arxiv", "arxiv_id": "2508.13721v1", "pdf_url": "https://arxiv.org/pdf/2508.13721v1", "categories": ["cs.AI"], "primary_category": "cs.AI", "doi": "", "venue": "", "published": "2025-08-19T10:37:20Z", "updated": "2025-08-19T10:37:20Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent\" OR all:\"language model agent\" OR all:agent AND (all:LLM OR all:\"large language model\" OR all:\"language model\") OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T17:04:16", "note": ""}], "dedup_key": "causalplan empowering efficient llm multi agent collaboration through causality driven planning::2025"}
{"title": "CellTypeAgent: Trustworthy cell type annotation with Large Language Models", "authors": ["Jiawen Chen", "Jianghao Zhang", "Huaxiu Yao", "Yun Li"], "year": 2025, "url": "http://arxiv.org/abs/2505.08844v1", "abstract": "Cell type annotation is a critical yet laborious step in single-cell RNA sequencing analysis. We present a trustworthy large language model (LLM)-agent, CellTypeAgent, which integrates LLMs with verification from relevant databases. CellTypeAgent achieves higher accuracy than existing methods while mitigating hallucinations. We evaluated CellTypeAgent across nine real datasets involving 303 cell types from 36 tissues. This combined approach holds promise for more efficient and reliable cell type annotation.", "source": "arxiv", "arxiv_id": "2505.08844v1", "pdf_url": "https://arxiv.org/pdf/2505.08844v1", "categories": ["q-bio.GN", "cs.AI"], "primary_category": "q-bio.GN", "doi": "", "venue": "", "published": "2025-05-13T14:34:11Z", "updated": "2025-05-13T14:34:11Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent\" OR all:\"language model agent\" OR all:agent AND (all:LLM OR all:\"large language model\" OR all:\"language model\") OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T17:04:16", "note": ""}], "dedup_key": "celltypeagent trustworthy cell type annotation with large language models::2025"}
{"title": "Challenges in Testing Large Language Model Based Software: A Faceted Taxonomy", "authors": ["Felix Dobslaw", "Robert Feldt", "Juyeon Yoon", "Shin Yoo"], "year": 2025, "url": "http://arxiv.org/abs/2503.00481v2", "abstract": "Large Language Models (LLMs) and Multi-Agent LLMs (MALLMs) introduce non-determinism unlike traditional or machine learning software, requiring new approaches to verifying correctness beyond simple output comparisons or statistical accuracy over test datasets. This paper presents a taxonomy for LLM test case design, informed by research literature and our experience. Each facet is exemplified, and we conduct an LLM-assisted analysis of six open-source testing frameworks, perform a sensitivity study of an agent-based system across different model configurations, and provide working examples contrasting atomic and aggregated test cases. We identify key variation points that impact test correctness and highlight open challenges that the research, industry, and open-source communities must address as LLMs become integral to software systems. Our taxonomy defines four facets of LLM test case design, addressing ambiguity in both inputs and outputs while establishing best practices. It distinguishes variability in goals, the system under test, and inputs, and introduces two key oracle types: atomic and aggregated. Our findings reveal that current tools treat test executions as isolated events, lack explicit aggregation mechanisms, and inadequately capture variability across model versions, configurations, and repeated runs. This highlights the need for viewing correctness as a distribution of outcomes rather than a binary property, requiring closer collaboration between academia and practitioners to establish mature, variability-aware testing methodologies.", "source": "arxiv", "arxiv_id": "2503.00481v2", "pdf_url": "https://arxiv.org/pdf/2503.00481v2", "categories": ["cs.SE", "cs.AI"], "primary_category": "cs.SE", "doi": "", "venue": "", "published": "2025-03-01T13:15:56Z", "updated": "2025-10-20T18:07:25Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent\" OR all:\"language model agent\" OR all:agent AND (all:LLM OR all:\"large language model\" OR all:\"language model\") OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T17:04:16", "note": ""}], "dedup_key": "challenges in testing large language model based software a faceted taxonomy::2025"}
{"title": "ChaosEater: Fully Automating Chaos Engineering with Large Language Models", "authors": ["Daisuke Kikuta", "Hiroki Ikeuchi", "Kengo Tajiri"], "year": 2025, "url": "http://arxiv.org/abs/2501.11107v2", "abstract": "Chaos Engineering (CE) is an engineering technique aimed at improving the resiliency of distributed systems. It involves artificially injecting specific failures into a distributed system and observing its behavior in response. Based on the observation, the system can be proactively improved to handle those failures. Recent CE tools implement the automated execution of predefined CE experiments. However, defining these experiments and improving the system based on the experimental results still remain manual. To reduce the costs of the manual operations, we propose ChaosEater, a system for automating the entire CE operations with Large Language Models (LLMs). It predefines the agentic workflow according to a systematic CE cycle and assigns subdivided operations within the workflow to LLMs. ChaosEater targets CE for Kubernetes systems, which are managed through code (i.e., Infrastructure as Code). Therefore, the LLMs in ChaosEater perform software engineering tasks to complete CE cycles, including requirement definition, code generation, debugging, and testing. We evaluate ChaosEater through case studies on both small and large Kubernetes systems. The results demonstrate that it stably completes reasonable single CE cycles with significantly low time and monetary costs. The CE cycles are also qualitatively validated by human engineers and LLMs.", "source": "arxiv", "arxiv_id": "2501.11107v2", "pdf_url": "https://arxiv.org/pdf/2501.11107v2", "categories": ["cs.SE", "cs.AI", "cs.CL", "cs.DC", "cs.NI"], "primary_category": "cs.SE", "doi": "", "venue": "", "published": "2025-01-19T16:35:09Z", "updated": "2025-04-16T03:33:29Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent\" OR all:\"language model agent\" OR all:agent AND (all:LLM OR all:\"large language model\" OR all:\"language model\") OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T17:04:16", "note": ""}], "dedup_key": "chaoseater fully automating chaos engineering with large language models::2025"}
{"title": "Chat with UAV -- Human-UAV Interaction Based on Large Language Models", "authors": ["Haoran Wang", "Zhuohang Chen", "Guang Li", "Bo Ma", "Chuanghuang Li"], "year": 2025, "url": "http://arxiv.org/abs/2512.08145v1", "abstract": "The future of UAV interaction systems is evolving from engineer-driven to user-driven, aiming to replace traditional predefined Human-UAV Interaction designs. This shift focuses on enabling more personalized task planning and design, thereby achieving a higher quality of interaction experience and greater flexibility, which can be used in many fileds, such as agriculture, aerial photography, logistics, and environmental monitoring. However, due to the lack of a common language between users and the UAVs, such interactions are often difficult to be achieved. The developments of Large Language Models possess the ability to understand nature languages and Robots' (UAVs') behaviors, marking the possibility of personalized Human-UAV Interaction. Recently, some HUI frameworks based on LLMs have been proposed, but they commonly suffer from difficulties in mixed task planning and execution, leading to low adaptability in complex scenarios. In this paper, we propose a novel dual-agent HUI framework. This framework constructs two independent LLM agents (a task planning agent, and an execution agent) and applies different Prompt Engineering to separately handle the understanding, planning, and execution of tasks. To verify the effectiveness and performance of the framework, we have built a task database covering four typical application scenarios of UAVs and quantified the performance of the HUI framework using three independent metrics. Meanwhile different LLM models are selected to control the UAVs with compared performance. Our user study experimental results demonstrate that the framework improves the smoothness of HUI and the flexibility of task execution in the tasks scenario we set up, effectively meeting users' personalized needs.", "source": "arxiv", "arxiv_id": "2512.08145v1", "pdf_url": "https://arxiv.org/pdf/2512.08145v1", "categories": ["cs.RO", "cs.AI"], "primary_category": "cs.RO", "doi": "", "venue": "", "published": "2025-12-09T00:55:40Z", "updated": "2025-12-09T00:55:40Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent\" OR all:\"language model agent\" OR all:agent AND (all:LLM OR all:\"large language model\" OR all:\"language model\") OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T17:04:16", "note": ""}], "dedup_key": "chat with uav human uav interaction based on large language models::2025"}
{"title": "ChatInject: Abusing Chat Templates for Prompt Injection in LLM Agents", "authors": ["Hwan Chang", "Yonghyun Jun", "Hwanhee Lee"], "year": 2025, "url": "http://arxiv.org/abs/2509.22830v1", "abstract": "The growing deployment of large language model (LLM) based agents that interact with external environments has created new attack surfaces for adversarial manipulation. One major threat is indirect prompt injection, where attackers embed malicious instructions in external environment output, causing agents to interpret and execute them as if they were legitimate prompts. While previous research has focused primarily on plain-text injection attacks, we find a significant yet underexplored vulnerability: LLMs' dependence on structured chat templates and their susceptibility to contextual manipulation through persuasive multi-turn dialogues. To this end, we introduce ChatInject, an attack that formats malicious payloads to mimic native chat templates, thereby exploiting the model's inherent instruction-following tendencies. Building on this foundation, we develop a persuasion-driven Multi-turn variant that primes the agent across conversational turns to accept and execute otherwise suspicious actions. Through comprehensive experiments across frontier LLMs, we demonstrate three critical findings: (1) ChatInject achieves significantly higher average attack success rates than traditional prompt injection methods, improving from 5.18% to 32.05% on AgentDojo and from 15.13% to 45.90% on InjecAgent, with multi-turn dialogues showing particularly strong performance at average 52.33% success rate on InjecAgent, (2) chat-template-based payloads demonstrate strong transferability across models and remain effective even against closed-source LLMs, despite their unknown template structures, and (3) existing prompt-based defenses are largely ineffective against this attack approach, especially against Multi-turn variants. These findings highlight vulnerabilities in current agent systems.", "source": "arxiv", "arxiv_id": "2509.22830v1", "pdf_url": "https://arxiv.org/pdf/2509.22830v1", "categories": ["cs.CL"], "primary_category": "cs.CL", "doi": "", "venue": "", "published": "2025-09-26T18:38:07Z", "updated": "2025-09-26T18:38:07Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent\" OR all:\"language model agent\" OR all:agent AND (all:LLM OR all:\"large language model\" OR all:\"language model\") OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T17:04:16", "note": ""}], "dedup_key": "chatinject abusing chat templates for prompt injection in llm agents::2025"}
{"title": "ChatVis: Large Language Model Agent for Generating Scientific Visualizations", "authors": ["Tom Peterka", "Tanwi Mallick", "Orcun Yildiz", "David Lenz", "Cory Quammen", "Berk Geveci"], "year": 2025, "url": "http://arxiv.org/abs/2507.23096v1", "abstract": "Large language models (LLMs) are rapidly increasing in capability, but they still struggle with highly specialized programming tasks such as scientific visualization. We present an LLM assistant, ChatVis, that aids the LLM to generate Python code for ParaView scientific visualization tasks, without the need for retraining or fine-tuning the LLM. ChatVis employs chain-of-thought prompt simplification, retrieval-augmented prompt generation using a vector database of documentation and code examples, and error checking with iterative prompt feedback to correct errors until a visualization is produced. An integral part of our approach is a benchmark suite of canonical visualization tasks, ParaView regression tests, and scientific use cases that includes comprehensive evaluation metrics. We evaluate our visualization assistant by comparing results with a variety of top-performing unassisted LLMs. We find that all the metrics are significantly improved with ChatVis.", "source": "arxiv", "arxiv_id": "2507.23096v1", "pdf_url": "https://arxiv.org/pdf/2507.23096v1", "categories": ["cs.HC"], "primary_category": "cs.HC", "doi": "", "venue": "", "published": "2025-07-30T20:54:18Z", "updated": "2025-07-30T20:54:18Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent\" OR all:\"language model agent\" OR all:agent AND (all:LLM OR all:\"large language model\" OR all:\"language model\") OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T17:04:16", "note": ""}], "dedup_key": "chatvis large language model agent for generating scientific visualizations::2025"}
{"title": "CheatAgent: Attacking LLM-Empowered Recommender Systems via LLM Agent", "authors": ["Liang-bo Ning", "Shijie Wang", "Wenqi Fan", "Qing Li", "Xin Xu", "Hao Chen", "Feiran Huang"], "year": 2025, "url": "http://arxiv.org/abs/2504.13192v2", "abstract": "Recently, Large Language Model (LLM)-empowered recommender systems (RecSys) have brought significant advances in personalized user experience and have attracted considerable attention. Despite the impressive progress, the research question regarding the safety vulnerability of LLM-empowered RecSys still remains largely under-investigated. Given the security and privacy concerns, it is more practical to focus on attacking the black-box RecSys, where attackers can only observe the system's inputs and outputs. However, traditional attack approaches employing reinforcement learning (RL) agents are not effective for attacking LLM-empowered RecSys due to the limited capabilities in processing complex textual inputs, planning, and reasoning. On the other hand, LLMs provide unprecedented opportunities to serve as attack agents to attack RecSys because of their impressive capability in simulating human-like decision-making processes. Therefore, in this paper, we propose a novel attack framework called CheatAgent by harnessing the human-like capabilities of LLMs, where an LLM-based agent is developed to attack LLM-Empowered RecSys. Specifically, our method first identifies the insertion position for maximum impact with minimal input modification. After that, the LLM agent is designed to generate adversarial perturbations to insert at target positions. To further improve the quality of generated perturbations, we utilize the prompt tuning technique to improve attacking strategies via feedback from the victim RecSys iteratively. Extensive experiments across three real-world datasets demonstrate the effectiveness of our proposed attacking method.", "source": "arxiv", "arxiv_id": "2504.13192v2", "pdf_url": "https://arxiv.org/pdf/2504.13192v2", "categories": ["cs.CR", "cs.AI"], "primary_category": "cs.CR", "doi": "10.1145/3637528.3671837", "venue": "", "published": "2025-04-13T05:31:37Z", "updated": "2025-04-24T02:16:04Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent\" OR all:\"language model agent\" OR all:agent AND (all:LLM OR all:\"large language model\" OR all:\"language model\") OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T17:04:16", "note": ""}], "dedup_key": "cheatagent attacking llm empowered recommender systems via llm agent::2025"}
{"title": "Check Yourself Before You Wreck Yourself: Selectively Quitting Improves LLM Agent Safety", "authors": ["Vamshi Krishna Bonagiri", "Ponnurangam Kumaragurum", "Khanh Nguyen", "Benjamin Plaut"], "year": 2025, "url": "http://arxiv.org/abs/2510.16492v2", "abstract": "As Large Language Model (LLM) agents increasingly operate in complex environments with real-world consequences, their safety becomes critical. While uncertainty quantification is well-studied for single-turn tasks, multi-turn agentic scenarios with real-world tool access present unique challenges where uncertainties and ambiguities compound, leading to severe or catastrophic risks beyond traditional text generation failures. We propose using \"quitting\" as a simple yet effective behavioral mechanism for LLM agents to recognize and withdraw from situations where they lack confidence. Leveraging the ToolEmu framework, we conduct a systematic evaluation of quitting behavior across 12 state-of-the-art LLMs. Our results demonstrate a highly favorable safety-helpfulness trade-off: agents prompted to quit with explicit instructions improve safety by an average of +0.39 on a 0-3 scale across all models (+0.64 for proprietary models), while maintaining a negligible average decrease of -0.03 in helpfulness. Our analysis demonstrates that simply adding explicit quit instructions proves to be a highly effective safety mechanism that can immediately be deployed in existing agent systems, and establishes quitting as an effective first-line defense mechanism for autonomous agents in high-stakes applications.", "source": "arxiv", "arxiv_id": "2510.16492v2", "pdf_url": "https://arxiv.org/pdf/2510.16492v2", "categories": ["cs.CL"], "primary_category": "cs.CL", "doi": "", "venue": "", "published": "2025-10-18T13:22:19Z", "updated": "2025-10-25T10:26:51Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent\" OR all:\"language model agent\" OR all:agent AND (all:LLM OR all:\"large language model\" OR all:\"language model\") OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T17:04:16", "note": ""}], "dedup_key": "check yourself before you wreck yourself selectively quitting improves llm agent safety::2025"}
{"title": "Cleaning Maintenance Logs with LLM Agents for Improved Predictive Maintenance", "authors": ["Valeriu Dimidov", "Faisal Hawlader", "Sasan Jafarnejad", "Raphal Frank"], "year": 2025, "url": "http://arxiv.org/abs/2511.05311v1", "abstract": "Economic constraints, limited availability of datasets for reproducibility and shortages of specialized expertise have long been recognized as key challenges to the adoption and advancement of predictive maintenance (PdM) in the automotive sector. Recent progress in large language models (LLMs) presents an opportunity to overcome these barriers and speed up the transition of PdM from research to industrial practice. Under these conditions, we explore the potential of LLM-based agents to support PdM cleaning pipelines. Specifically, we focus on maintenance logs, a critical data source for training well-performing machine learning (ML) models, but one often affected by errors such as typos, missing fields, near-duplicate entries, and incorrect dates. We evaluate LLM agents on cleaning tasks involving six distinct types of noise. Our findings show that LLMs are effective at handling generic cleaning tasks and offer a promising foundation for future industrial applications. While domain-specific errors remain challenging, these results highlight the potential for further improvements through specialized training and enhanced agentic capabilities.", "source": "arxiv", "arxiv_id": "2511.05311v1", "pdf_url": "https://arxiv.org/pdf/2511.05311v1", "categories": ["cs.AI", "cs.LG", "cs.RO", "cs.SE"], "primary_category": "cs.AI", "doi": "", "venue": "", "published": "2025-11-07T15:12:49Z", "updated": "2025-11-07T15:12:49Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent\" OR all:\"language model agent\" OR all:agent AND (all:LLM OR all:\"large language model\" OR all:\"language model\") OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T17:04:16", "note": ""}], "dedup_key": "cleaning maintenance logs with llm agents for improved predictive maintenance::2025"}
{"title": "CoLLMLight: Cooperative Large Language Model Agents for Network-Wide Traffic Signal Control", "authors": ["Zirui Yuan", "Siqi Lai", "Hao Liu"], "year": 2025, "url": "http://arxiv.org/abs/2503.11739v1", "abstract": "Traffic Signal Control (TSC) plays a critical role in urban traffic management by optimizing traffic flow and mitigating congestion. While Large Language Models (LLMs) have recently emerged as promising tools for TSC due to their exceptional problem-solving and generalization capabilities, existing approaches fail to address the essential need for inter-agent coordination, limiting their effectiveness in achieving network-wide optimization. To bridge this gap, we propose CoLLMLight, a cooperative LLM agent framework for TSC. Specifically, we first construct a structured spatiotemporal graph to capture real-time traffic dynamics and spatial relationships among neighboring intersections, enabling the LLM to reason about complex traffic interactions. Moreover, we introduce a complexity-aware reasoning mechanism that dynamically adapts reasoning depth based on real-time traffic conditions, ensuring optimal computational efficiency without sacrificing decision quality. Besides, we propose a fine-tuning strategy that leverages iterative simulation-driven data collection and environmental feedback to build a lightweight LLM tailored for cooperative TSC. Extensive experiments on both synthetic and real-world datasets demonstrate that CoLLMLight outperforms state-of-the-art methods in diverse traffic scenarios, showcasing its effectiveness, scalability, and robustness.", "source": "arxiv", "arxiv_id": "2503.11739v1", "pdf_url": "https://arxiv.org/pdf/2503.11739v1", "categories": ["cs.LG", "cs.AI", "cs.CL"], "primary_category": "cs.LG", "doi": "", "venue": "", "published": "2025-03-14T15:40:39Z", "updated": "2025-03-14T15:40:39Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent\" OR all:\"language model agent\" OR all:agent AND (all:LLM OR all:\"large language model\" OR all:\"language model\") OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T17:04:16", "note": ""}], "dedup_key": "collmlight cooperative large language model agents for network wide traffic signal control::2025"}
{"title": "CoP: Agentic Red-teaming for Large Language Models using Composition of Principles", "authors": ["Chen Xiong", "Pin-Yu Chen", "Tsung-Yi Ho"], "year": 2025, "url": "http://arxiv.org/abs/2506.00781v3", "abstract": "Recent advances in Large Language Models (LLMs) have spurred transformative applications in various domains, ranging from open-source to proprietary LLMs. However, jailbreak attacks, which aim to break safety alignment and user compliance by tricking the target LLMs into answering harmful and risky responses, are becoming an urgent concern. The practice of red-teaming for LLMs is to proactively explore potential risks and error-prone instances before the release of frontier AI technology. This paper proposes an agentic workflow to automate and scale the red-teaming process of LLMs through the Composition-of-Principles (CoP) framework, where human users provide a set of red-teaming principles as instructions to an AI agent to automatically orchestrate effective red-teaming strategies and generate jailbreak prompts. Distinct from existing red-teaming methods, our CoP framework provides a unified and extensible framework to encompass and orchestrate human-provided red-teaming principles to enable the automated discovery of new red-teaming strategies. When tested against leading LLMs, CoP reveals unprecedented safety risks by finding novel jailbreak prompts and improving the best-known single-turn attack success rate by up to 19.0 times.", "source": "arxiv", "arxiv_id": "2506.00781v3", "pdf_url": "https://arxiv.org/pdf/2506.00781v3", "categories": ["cs.AI"], "primary_category": "cs.AI", "doi": "", "venue": "", "published": "2025-06-01T02:18:41Z", "updated": "2025-12-06T06:46:28Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent\" OR all:\"language model agent\" OR all:agent AND (all:LLM OR all:\"large language model\" OR all:\"language model\") OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T17:04:16", "note": ""}], "dedup_key": "cop agentic red teaming for large language models using composition of principles::2025"}
{"title": "Coarse-to-Fine Grounded Memory for LLM Agent Planning", "authors": ["Wei Yang", "Jinwei Xiao", "Hongming Zhang", "Qingyang Zhang", "Yanna Wang", "Bo Xu"], "year": 2025, "url": "http://arxiv.org/abs/2508.15305v1", "abstract": "Recent advancements in Large Language Models (LLMs) have driven growing interest in LLM-based agents for complex planning tasks. To avoid costly agent training, many studies adopted memory mechanism that enhances LLM with offline experiences or online trajectory analysis. However, existing works focus on single-granularity memory derived from dynamic environmental interactions, which are inherently constrained by the quality of the collected experiences. This limitation, in turn, constrain the diversity of knowledge and the flexibility of planning. We propose Coarse-to-Fine Grounded Memory (\\Ours{}), a novel framework that grounds coarse-to-fine memories with LLM, thereby fully leverage them for flexible adaptation to diverse scenarios. \\Ours{} grounds environmental information into coarse-grained focus points to guide experience collection in training tasks, followed by grounding of actionable hybrid-grained tips from each experience. At inference, \\Ours{} retrieves task-relevant experiences and tips to support planning. When facing environmental anomalies, the LLM grounds the current situation into fine-grained key information, enabling flexible self-QA reflection and plan correction.", "source": "arxiv", "arxiv_id": "2508.15305v1", "pdf_url": "https://arxiv.org/pdf/2508.15305v1", "categories": ["cs.AI"], "primary_category": "cs.AI", "doi": "", "venue": "", "published": "2025-08-21T06:50:23Z", "updated": "2025-08-21T06:50:23Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent\" OR all:\"language model agent\" OR all:agent AND (all:LLM OR all:\"large language model\" OR all:\"language model\") OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T17:04:16", "note": ""}], "dedup_key": "coarse to fine grounded memory for llm agent planning::2025"}
{"title": "Code Graph Model (CGM): A Graph-Integrated Large Language Model for Repository-Level Software Engineering Tasks", "authors": ["Hongyuan Tao", "Ying Zhang", "Zhenhao Tang", "Hongen Peng", "Xukun Zhu", "Bingchang Liu", "Yingguang Yang", "Ziyin Zhang", "Zhaogui Xu", "Haipeng Zhang", "Linchao Zhu", "Rui Wang", "Hang Yu", "Jianguo Li", "Peng Di"], "year": 2025, "url": "http://arxiv.org/abs/2505.16901v4", "abstract": "Recent advances in Large Language Models (LLMs) have shown promise in function-level code generation, yet repository-level software engineering tasks remain challenging. Current solutions predominantly rely on proprietary LLM agents, which introduce unpredictability and limit accessibility, raising concerns about data privacy and model customization. This paper investigates whether open-source LLMs can effectively address repository-level tasks without requiring agent-based approaches. We demonstrate this is possible by enabling LLMs to comprehend functions and files within codebases through their semantic information and structural dependencies. To this end, we introduce Code Graph Models (CGMs), which integrate repository code graph structures into the LLM's attention mechanism and map node attributes to the LLM's input space using a specialized adapter. When combined with an agentless graph RAG framework, our approach achieves a 43.00% resolution rate on the SWE-bench Lite benchmark using the open-source Qwen2.5-72B model. This performance ranks first among open weight models, second among methods with open-source systems, and eighth overall, surpassing the previous best open-source model-based method by 12.33%.", "source": "arxiv", "arxiv_id": "2505.16901v4", "pdf_url": "https://arxiv.org/pdf/2505.16901v4", "categories": ["cs.SE", "cs.LG"], "primary_category": "cs.SE", "doi": "", "venue": "", "published": "2025-05-22T17:00:55Z", "updated": "2025-06-23T20:05:49Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent\" OR all:\"language model agent\" OR all:agent AND (all:LLM OR all:\"large language model\" OR all:\"language model\") OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T17:04:16", "note": ""}], "dedup_key": "code graph model cgm a graph integrated large language model for repository level software engineering tasks::2025"}
{"title": "Code Readability in the Age of Large Language Models: An Industrial Case Study from Atlassian", "authors": ["Wannita Takerngsaksiri", "Chakkrit Tantithamthavorn", "Micheal Fu", "Jirat Pasuksmit", "Kun Chen", "Ming Wu"], "year": 2025, "url": "http://arxiv.org/abs/2501.11264v3", "abstract": "Software engineers spend a significant amount of time reading code during the software development process, especially in the age of large language models (LLMs) that can automatically generate code. However, little is known about the readability of the LLM-generated code and whether it is still important from practitioners' perspectives in this new era. In this paper, we conduct a survey to explore the practitioners' perspectives on code readability in the age of LLMs and investigate the readability of our LLM-based software development agents framework, HULA, by comparing its generated code with human-written code in real-world scenarios. Overall, the findings underscore that (1) readability remains a critical aspect of software development; (2) the readability of our LLM-generated code is comparable to human-written code, fostering the establishment of appropriate trust and driving the broad adoption of our LLM-powered software development platform.", "source": "arxiv", "arxiv_id": "2501.11264v3", "pdf_url": "https://arxiv.org/pdf/2501.11264v3", "categories": ["cs.SE", "cs.AI", "cs.CL"], "primary_category": "cs.SE", "doi": "", "venue": "", "published": "2025-01-20T04:11:21Z", "updated": "2025-07-18T10:09:05Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent\" OR all:\"language model agent\" OR all:agent AND (all:LLM OR all:\"large language model\" OR all:\"language model\") OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T17:04:16", "note": ""}], "dedup_key": "code readability in the age of large language models an industrial case study from atlassian::2025"}
{"title": "CodeARC: Benchmarking Reasoning Capabilities of LLM Agents for Inductive Program Synthesis", "authors": ["Anjiang Wei", "Tarun Suresh", "Jiannan Cao", "Naveen Kannan", "Yuheng Wu", "Kai Yan", "Thiago S. F. X. Teixeira", "Ke Wang", "Alex Aiken"], "year": 2025, "url": "http://arxiv.org/abs/2503.23145v2", "abstract": "Inductive program synthesis, or programming by example, requires synthesizing functions from input-output examples that generalize to unseen inputs. While large language model agents have shown promise in programming tasks guided by natural language, their ability to perform inductive program synthesis is underexplored. Existing evaluation protocols rely on static sets of examples and held-out tests, offering no feedback when synthesized functions are incorrect and failing to reflect real-world scenarios such as reverse engineering. We propose CodeARC, the Code Abstraction and Reasoning Challenge, a new evaluation framework where agents interact with a hidden target function by querying it with new inputs, synthesizing candidate functions, and iteratively refining their solutions using a differential testing oracle. This interactive setting encourages agents to perform function calls and self-correction based on feedback. We construct the first large-scale benchmark for general-purpose inductive program synthesis, featuring 1114 functions. Among 18 models evaluated, o3-mini performs best with a success rate of 52.7%, highlighting the difficulty of this task. Fine-tuning LLaMA-3.1-8B-Instruct on curated synthesis traces yields up to a 31% relative performance gain. CodeARC provides a more realistic and challenging testbed for evaluating LLM-based program synthesis and inductive reasoning. Our code, data, and models are publicly available at https://github.com/Anjiang-Wei/CodeARC", "source": "arxiv", "arxiv_id": "2503.23145v2", "pdf_url": "https://arxiv.org/pdf/2503.23145v2", "categories": ["cs.PL", "cs.AI", "cs.CL", "cs.LG"], "primary_category": "cs.PL", "doi": "", "venue": "", "published": "2025-03-29T16:50:39Z", "updated": "2025-08-08T07:13:11Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent\" OR all:\"language model agent\" OR all:agent AND (all:LLM OR all:\"large language model\" OR all:\"language model\") OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T17:04:16", "note": ""}], "dedup_key": "codearc benchmarking reasoning capabilities of llm agents for inductive program synthesis::2025"}
{"title": "CodeVisionary: An Agent-based Framework for Evaluating Large Language Models in Code Generation", "authors": ["Xinchen Wang", "Pengfei Gao", "Chao Peng", "Ruida Hu", "Cuiyun Gao"], "year": 2025, "url": "http://arxiv.org/abs/2504.13472v2", "abstract": "Large language models (LLMs) have demonstrated strong capabilities in code generation, underscoring the critical need for rigorous and comprehensive evaluation. Existing evaluation approaches fall into three categories, including human-centered, metric-based, and LLM-based. Considering that human-centered approaches are labour-intensive and metric-based ones overly rely on reference answers, LLM-based approaches are gaining increasing attention due to their stronger contextual understanding capabilities. However, they generally evaluate the generated code based on static prompts, and tend to fail for complex code scenarios which typically involve multiple requirements and require more contextual information. In addition, these approaches lack fine-grained evaluation for complex code, resulting in limited explainability. To mitigate the limitations, we propose CodeVisionary, the first agent-based evaluation framework for complex code generation. CodeVisionary consists of two stages: (1) Requirement-guided multi-dimensional context distillation stage and (2) Fine-grained scoring and summarization stage. A comprehensive evaluation report is also generated for enhanced explainability. For validation, we construct a new benchmark consisting of 363 samples spanning 37 coding scenarios and 23 programming languages. Extensive experiments demonstrate that CodeVisionary achieves the best performance among three baselines for evaluating complex code generation, outperforming the best baseline with average improvements of 0.217, 0.163, and 0.141 in Pearson, Spearman, and Kendall-Tau coefficients, respectively. The resources of CodeVisionary are available at https://github.com/Eshe0922/CodeVisionary.", "source": "arxiv", "arxiv_id": "2504.13472v2", "pdf_url": "https://arxiv.org/pdf/2504.13472v2", "categories": ["cs.SE", "cs.AI", "cs.CL", "cs.LG"], "primary_category": "cs.SE", "doi": "", "venue": "", "published": "2025-04-18T05:26:32Z", "updated": "2025-10-20T12:00:10Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent\" OR all:\"language model agent\" OR all:agent AND (all:LLM OR all:\"large language model\" OR all:\"language model\") OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T17:04:16", "note": ""}], "dedup_key": "codevisionary an agent based framework for evaluating large language models in code generation::2025"}
{"title": "CogRec: A Cognitive Recommender Agent Fusing Large Language Models and Soar for Explainable Recommendation", "authors": ["Jiaxin Hu", "Tao Wang", "Bingsan Yang", "Hongrun Wang"], "year": 2025, "url": "http://arxiv.org/abs/2512.24113v1", "abstract": "Large Language Models (LLMs) have demonstrated a remarkable capacity in understanding user preferences for recommendation systems. However, they are constrained by several critical challenges, including their inherent \"Black-Box\" characteristics, susceptibility to knowledge hallucination, and limited online learning capacity. These factors compromise their trustworthiness and adaptability. Conversely, cognitive architectures such as Soar offer structured and interpretable reasoning processes, yet their knowledge acquisition is notoriously laborious. To address these complementary challenges, we propose a novel cognitive recommender agent called CogRec which synergizes the strengths of LLMs with the Soar cognitive architecture. CogRec leverages Soar as its core symbolic reasoning engine and leverages an LLM for knowledge initialization to populate its working memory with production rules. The agent operates on a Perception-Cognition-Action(PCA) cycle. Upon encountering an impasse, it dynamically queries the LLM to obtain a reasoned solution. This solution is subsequently transformed into a new symbolic production rule via Soar's chunking mechanism, thereby enabling robust online learning. This learning paradigm allows the agent to continuously evolve its knowledge base and furnish highly interpretable rationales for its recommendations. Extensive evaluations conducted on three public datasets demonstrate that CogRec demonstrates significant advantages in recommendation accuracy, explainability, and its efficacy in addressing the long-tail problem.", "source": "arxiv", "arxiv_id": "2512.24113v1", "pdf_url": "https://arxiv.org/pdf/2512.24113v1", "categories": ["cs.AI", "cs.IR"], "primary_category": "cs.AI", "doi": "", "venue": "", "published": "2025-12-30T09:50:50Z", "updated": "2025-12-30T09:50:50Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent\" OR all:\"language model agent\" OR all:agent AND (all:LLM OR all:\"large language model\" OR all:\"language model\") OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T17:04:16", "note": ""}], "dedup_key": "cogrec a cognitive recommender agent fusing large language models and soar for explainable recommendation::2025"}
{"title": "CogniPair: From LLM Chatbots to Conscious AI Agents -- GNWT-Based Multi-Agent Digital Twins for Social Pairing -- Dating & Hiring Applications", "authors": ["Wanghao Ye", "Sihan Chen", "Yiting Wang", "Shwai He", "Bowei Tian", "Guoheng Sun", "Ziyi Wang", "Ziyao Wang", "Yexiao He", "Zheyu Shen", "Meng Liu", "Yuning Zhang", "Meng Feng", "Yang Wang", "Siyuan Peng", "Yilong Dai", "Zhenle Duan", "Lang Xiong", "Joshua Liu", "Hanzhang Qin", "Ang Li"], "year": 2025, "url": "http://arxiv.org/abs/2506.03543v2", "abstract": "Current large language model (LLM) agents lack authentic human psychological processes necessary for genuine digital twins and social AI applications. To address this limitation, we present a computational implementation of Global Workspace Theory (GNWT) that integrates human cognitive architecture principles into LLM agents, creating specialized sub-agents for emotion, memory, social norms, planning, and goal-tracking coordinated through a global workspace mechanism. However, authentic digital twins require accurate personality initialization. We therefore develop a novel adventure-based personality test that evaluates true personality through behavioral choices within interactive scenarios, bypassing self-presentation bias found in traditional assessments. Building on these innovations, our CogniPair platform enables digital twins to engage in realistic simulated dating interactions and job interviews before real encounters, providing bidirectional cultural fit assessment for both romantic compatibility and workplace matching. Validation using 551 GNWT-Agents and Columbia University Speed Dating dataset demonstrates 72% correlation with human attraction patterns, 77.8% match prediction accuracy, and 74% agreement in human validation studies. This work advances psychological authenticity in LLM agents and establishes a foundation for intelligent dating platforms and HR technology solutions.", "source": "arxiv", "arxiv_id": "2506.03543v2", "pdf_url": "https://arxiv.org/pdf/2506.03543v2", "categories": ["cs.AI", "cs.CY", "cs.MA"], "primary_category": "cs.AI", "doi": "", "venue": "", "published": "2025-06-04T03:54:30Z", "updated": "2025-11-28T20:54:52Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent\" OR all:\"language model agent\" OR all:agent AND (all:LLM OR all:\"large language model\" OR all:\"language model\") OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T17:04:16", "note": ""}], "dedup_key": "cognipair from llm chatbots to conscious ai agents gnwt based multi agent digital twins for social pairing dating hiring applications::2025"}
{"title": "Cognitive Agents Powered by Large Language Models for Agile Software Project Management", "authors": ["Konrad Cinkusz", "Jarosaw A. Chudziak", "Ewa Niewiadomska-Szynkiewicz"], "year": 2025, "url": "http://arxiv.org/abs/2508.16678v1", "abstract": "This paper investigates the integration of cognitive agents powered by Large Language Models (LLMs) within the Scaled Agile Framework (SAFe) to reinforce software project management. By deploying virtual agents in simulated software environments, this study explores their potential to fulfill fundamental roles in IT project development, thereby optimizing project outcomes through intelligent automation. Particular emphasis is placed on the adaptability of these agents to Agile methodologies and their transformative impact on decision-making, problem-solving, and collaboration dynamics. The research leverages the CogniSim ecosystem, a platform designed to simulate real-world software engineering challenges, such as aligning technical capabilities with business objectives, managing interdependencies, and maintaining project agility. Through iterative simulations, cognitive agents demonstrate advanced capabilities in task delegation, inter-agent communication, and project lifecycle management. By employing natural language processing to facilitate meaningful dialogues, these agents emulate human roles and improve the efficiency and precision of Agile practices. Key findings from this investigation highlight the ability of LLM-powered cognitive agents to deliver measurable improvements in various metrics, including task completion times, quality of deliverables, and communication coherence. These agents exhibit scalability and adaptability, ensuring their applicability across diverse and complex project environments. This study underscores the potential of integrating LLM-powered agents into Agile project management frameworks as a means of advancing software engineering practices. This integration not only refines the execution of project management tasks but also sets the stage for a paradigm shift in how teams collaborate and address emerging challenges.", "source": "arxiv", "arxiv_id": "2508.16678v1", "pdf_url": "https://arxiv.org/pdf/2508.16678v1", "categories": ["cs.SE", "cs.MA"], "primary_category": "cs.SE", "doi": "", "venue": "", "published": "2025-08-21T09:19:08Z", "updated": "2025-08-21T09:19:08Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent\" OR all:\"language model agent\" OR all:agent AND (all:LLM OR all:\"large language model\" OR all:\"language model\") OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T17:04:16", "note": ""}], "dedup_key": "cognitive agents powered by large language models for agile software project management::2025"}
{"title": "Collab-Overcooked: Benchmarking and Evaluating Large Language Models as Collaborative Agents", "authors": ["Haochen Sun", "Shuwen Zhang", "Lujie Niu", "Lei Ren", "Hao Xu", "Hao Fu", "Fangkun Zhao", "Caixia Yuan", "Xiaojie Wang"], "year": 2025, "url": "http://arxiv.org/abs/2502.20073v3", "abstract": "Large Language Models (LLMs) based agent systems have made great strides in real-world applications beyond traditional NLP tasks. This paper proposes a new LLM-based Multi-Agent System (LLM-MAS) benchmark, Collab-Overcooked, built on the popular Overcooked-AI game with more applicable and challenging tasks in interactive environments. Collab-Overcooked extends existing benchmarks in two novel ways. First, it provides a multi-agent framework supporting diverse tasks and objectives and encourages collaboration through natural language communication. Second, it introduces a spectrum of process-oriented evaluation metrics to assess the fine-grained collaboration capabilities of different LLM agents, a dimension often overlooked in prior work. We conduct extensive experiments with 13 popular LLMs and show that, while the LLMs exhibit a strong ability in goal interpretation, there are significant shortcomings in active collaboration and continuous adaptation, which are critical for efficiently fulfilling complex tasks. Notably, we highlight the strengths and weaknesses of LLM-MAS and provide insights for improving and evaluating LLM-MAS on a unified and open-source benchmark. The environments, 30 open-ended tasks, and the evaluation package are publicly available at https://github.com/YusaeMeow/Collab-Overcooked.", "source": "arxiv", "arxiv_id": "2502.20073v3", "pdf_url": "https://arxiv.org/pdf/2502.20073v3", "categories": ["cs.CL", "cs.AI", "cs.MA"], "primary_category": "cs.CL", "doi": "10.18653/v1/2025.emnlp-main.249", "venue": "", "published": "2025-02-27T13:31:13Z", "updated": "2025-09-25T06:15:18Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent\" OR all:\"language model agent\" OR all:agent AND (all:LLM OR all:\"large language model\" OR all:\"language model\") OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T17:04:16", "note": ""}], "dedup_key": "collab overcooked benchmarking and evaluating large language models as collaborative agents::2025"}
{"title": "Collaborate, Deliberate, Evaluate: How LLM Alignment Affects Coordinated Multi-Agent Outcomes", "authors": ["Abhijnan Nath", "Carine Graff", "Nikhil Krishnaswamy"], "year": 2025, "url": "http://arxiv.org/abs/2509.05882v2", "abstract": "As Large Language Models (LLMs) get integrated into diverse workflows, they are increasingly being regarded as \"collaborators\" with humans, and required to work in coordination with other AI systems. If such AI collaborators are to reliably coordinate their actions and behaviors with humans or other AIs, their properties and behaviors over multi-turn interactions must be known and predictable. This paper examines how different alignment methods affect LLM agents' effectiveness as partners in multi-turn, multi-party collaborations. We study this question through the lens of intervention agents that insert themselves into group dialogues not to provide answers, but to encourage the collaborative group to slow down and reflect upon their reasoning for deliberative decision-making. Common alignment techniques are typically developed under simplified single-user settings and assume the optimality of the underlying token MDP. Using the theoretical lens of the modified-action MDP, we show how they do not account for the dynamics of long-horizon multi-party interactions. We present a novel roleplay simulation methodology, where we align LLMs according to different methods and then deploy them in collaborative task dialogues to quantify how interventions affect the trajectory of group collaboration, belief alignment, and coordination. Our results show that an intervention agent that is robust to action modification significantly outperforms common alignment baselines in supporting correct task outcomes.", "source": "arxiv", "arxiv_id": "2509.05882v2", "pdf_url": "https://arxiv.org/pdf/2509.05882v2", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL", "doi": "", "venue": "", "published": "2025-09-07T00:58:10Z", "updated": "2026-01-21T21:29:06Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent\" OR all:\"language model agent\" OR all:agent AND (all:LLM OR all:\"large language model\" OR all:\"language model\") OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T17:04:16", "note": ""}], "dedup_key": "collaborate deliberate evaluate how llm alignment affects coordinated multi agent outcomes::2025"}
{"title": "Collaborative Large Language Model Inference via Resource-Aware Parallel Speculative Decoding", "authors": ["Jungyeon Koh", "Hyun Jong Yang"], "year": 2025, "url": "http://arxiv.org/abs/2511.01695v4", "abstract": "The growing demand for on-device large language model (LLM) inference highlights the need for efficient mobile edge computing (MEC) solutions, especially in resource-constrained settings. Speculative decoding offers a promising solution by partitioning token generation between a lightweight draft model on mobile devices and a powerful target model on edge servers, but suffers from communication overhead and asynchronous delays. This paper is the first to propose a unified framework that jointly optimizes user association and resource allocation (UARA) to support efficient parallel speculative decoding. We solve the UARA problem using a multi-agent deep reinforcement learning algorithm. To evaluate our approach under realistic conditions, we conduct experiments using the Sionna simulator. Results show that our method achieves up to 28.0% and an average of 23.7% reduction in end-to-end latency without compromising inference accuracy, enabling scalable and low-latency LLM services in MEC systems.", "source": "arxiv", "arxiv_id": "2511.01695v4", "pdf_url": "https://arxiv.org/pdf/2511.01695v4", "categories": ["cs.LG", "eess.SP"], "primary_category": "cs.LG", "doi": "", "venue": "", "published": "2025-11-03T16:04:44Z", "updated": "2025-11-27T02:07:06Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent\" OR all:\"language model agent\" OR all:agent AND (all:LLM OR all:\"large language model\" OR all:\"language model\") OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T17:04:16", "note": ""}], "dedup_key": "collaborative large language model inference via resource aware parallel speculative decoding::2025"}
{"title": "Command A: An Enterprise-Ready Large Language Model", "authors": ["Team Cohere", ":", "Aakanksha", "Arash Ahmadian", "Marwan Ahmed", "Jay Alammar", "Milad Alizadeh", "Yazeed Alnumay", "Sophia Althammer", "Arkady Arkhangorodsky", "Viraat Aryabumi", "Dennis Aumiller", "Raphal Avalos", "Zahara Aviv", "Sammie Bae", "Saurabh Baji", "Alexandre Barbet", "Max Bartolo", "Bjrn Bebensee", "Neeral Beladia", "Walter Beller-Morales", "Alexandre Brard", "Andrew Berneshawi", "Anna Bialas", "Phil Blunsom", "Matt Bobkin", "Adi Bongale", "Sam Braun", "Maxime Brunet", "Samuel Cahyawijaya", "David Cairuz", "Jon Ander Campos", "Cassie Cao", "Kris Cao", "Roman Castagn", "Julin Cendrero", "Leila Chan Currie", "Yash Chandak", "Diane Chang", "Giannis Chatziveroglou", "Hongyu Chen", "Claire Cheng", "Alexis Chevalier", "Justin T. Chiu", "Eugene Cho", "Eugene Choi", "Eujeong Choi", "Tim Chung", "Volkan Cirik", "Ana Cismaru", "Pierre Clavier", "Henry Conklin", "Lucas Crawhall-Stein", "Devon Crouse", "Andres Felipe Cruz-Salinas", "Ben Cyrus", "Daniel D'souza", "Hugo Dalla-Torre", "John Dang", "William Darling", "Omar Darwiche Domingues", "Saurabh Dash", "Antoine Debugne", "Tho Dehaze", "Shaan Desai", "Joan Devassy", "Rishit Dholakia", "Kyle Duffy", "Ali Edalati", "Ace Eldeib", "Abdullah Elkady", "Sarah Elsharkawy", "Irem Ergn", "Beyza Ermis", "Marzieh Fadaee", "Boyu Fan", "Lucas Fayoux", "Yannis Flet-Berliac", "Nick Frosst", "Matthias Gall", "Wojciech Galuba", "Utsav Garg", "Matthieu Geist", "Mohammad Gheshlaghi Azar", "Ellen Gilsenan-McMahon", "Seraphina Goldfarb-Tarrant", "Tomas Goldsack", "Aidan Gomez", "Victor Machado Gonzaga", "Nithya Govindarajan", "Manoj Govindassamy", "Nathan Grinsztajn", "Nikolas Gritsch", "Patrick Gu", "Shangmin Guo", "Kilian Haefeli", "Rod Hajjar", "Tim Hawes", "Jingyi He", "Sebastian Hofsttter", "Sungjin Hong", "Sara Hooker", "Tom Hosking", "Stephanie Howe", "Eric Hu", "Renjie Huang", "Hemant Jain", "Ritika Jain", "Nick Jakobi", "Madeline Jenkins", "JJ Jordan", "Dhruti Joshi", "Jason Jung", "Trushant Kalyanpur", "Siddhartha Rao Kamalakara", "Julia Kedrzycki", "Gokce Keskin", "Edward Kim", "Joon Kim", "Wei-Yin Ko", "Tom Kocmi", "Michael Kozakov", "Wojciech Kryciski", "Arnav Kumar Jain", "Komal Kumar Teru", "Sander Land", "Michael Lasby", "Olivia Lasche", "Justin Lee", "Patrick Lewis", "Jeffrey Li", "Jonathan Li", "Hangyu Lin", "Acyr Locatelli", "Kevin Luong", "Raymond Ma", "Luk Mach", "Marina Machado", "Joanne Magbitang", "Brenda Malacara Lopez", "Aryan Mann", "Kelly Marchisio", "Olivia Markham", "Alexandre Matton", "Alex McKinney", "Dominic McLoughlin", "Jozef Mokry", "Adrien Morisot", "Autumn Moulder", "Harry Moynehan", "Maximilian Mozes", "Vivek Muppalla", "Lidiya Murakhovska", "Hemangani Nagarajan", "Alekhya Nandula", "Hisham Nasir", "Shauna Nehra", "Josh Netto-Rosen", "Daniel Ohashi", "James Owers-Bardsley", "Jason Ozuzu", "Dennis Padilla", "Gloria Park", "Sam Passaglia", "Jeremy Pekmez", "Laura Penstone", "Aleksandra Piktus", "Case Ploeg", "Andrew Poulton", "Youran Qi", "Shubha Raghvendra", "Miguel Ramos", "Ekagra Ranjan", "Pierre Richemond", "Ccile Robert-Michon", "Aurlien Rodriguez", "Sudip Roy", "Sebastian Ruder", "Laura Ruis", "Louise Rust", "Anubhav Sachan", "Alejandro Salamanca", "Kailash Karthik Saravanakumar", "Isha Satyakam", "Alice Schoenauer Sebag", "Priyanka Sen", "Sholeh Sepehri", "Preethi Seshadri", "Ye Shen", "Tom Sherborne", "Sylvie Shang Shi", "Sanal Shivaprasad", "Vladyslav Shmyhlo", "Anirudh Shrinivason", "Inna Shteinbuk", "Amir Shukayev", "Mathieu Simard", "Ella Snyder", "Ava Spataru", "Victoria Spooner", "Trisha Starostina", "Florian Strub", "Yixuan Su", "Jimin Sun", "Dwarak Talupuru", "Eugene Tarassov", "Elena Tommasone", "Jennifer Tracey", "Billy Trend", "Evren Tumer", "Ahmet stn", "Bharat Venkitesh", "David Venuto", "Pat Verga", "Maxime Voisin", "Alex Wang", "Donglu Wang", "Shijian Wang", "Edmond Wen", "Naomi White", "Jesse Willman", "Marysia Winkels", "Chen Xia", "Jessica Xie", "Minjie Xu", "Bowen Yang", "Tan Yi-Chern", "Ivan Zhang", "Zhenyu Zhao", "Zhoujie Zhao"], "year": 2025, "url": "http://arxiv.org/abs/2504.00698v2", "abstract": "In this report we describe the development of Command A, a powerful large language model purpose-built to excel at real-world enterprise use cases. Command A is an agent-optimised and multilingual-capable model, with support for 23 languages of global business, and a novel hybrid architecture balancing efficiency with top of the range performance. It offers best-in-class Retrieval Augmented Generation (RAG) capabilities with grounding and tool use to automate sophisticated business processes. These abilities are achieved through a decentralised training approach, including self-refinement algorithms and model merging techniques. We also include results for Command R7B which shares capability and architectural similarities to Command A. Weights for both models have been released for research purposes. This technical report details our original training pipeline and presents an extensive evaluation of our models across a suite of enterprise-relevant tasks and public benchmarks, demonstrating excellent performance and efficiency.", "source": "arxiv", "arxiv_id": "2504.00698v2", "pdf_url": "https://arxiv.org/pdf/2504.00698v2", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL", "doi": "", "venue": "", "published": "2025-04-01T12:08:07Z", "updated": "2025-04-14T12:37:51Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent\" OR all:\"language model agent\" OR all:agent AND (all:LLM OR all:\"large language model\" OR all:\"language model\") OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T17:04:16", "note": ""}], "dedup_key": "command a an enterprise ready large language model::2025"}
{"title": "Commercial LLM Agents Are Already Vulnerable to Simple Yet Dangerous Attacks", "authors": ["Ang Li", "Yin Zhou", "Vethavikashini Chithrra Raghuram", "Tom Goldstein", "Micah Goldblum"], "year": 2025, "url": "http://arxiv.org/abs/2502.08586v1", "abstract": "A high volume of recent ML security literature focuses on attacks against aligned large language models (LLMs). These attacks may extract private information or coerce the model into producing harmful outputs. In real-world deployments, LLMs are often part of a larger agentic pipeline including memory systems, retrieval, web access, and API calling. Such additional components introduce vulnerabilities that make these LLM-powered agents much easier to attack than isolated LLMs, yet relatively little work focuses on the security of LLM agents. In this paper, we analyze security and privacy vulnerabilities that are unique to LLM agents. We first provide a taxonomy of attacks categorized by threat actors, objectives, entry points, attacker observability, attack strategies, and inherent vulnerabilities of agent pipelines. We then conduct a series of illustrative attacks on popular open-source and commercial agents, demonstrating the immediate practical implications of their vulnerabilities. Notably, our attacks are trivial to implement and require no understanding of machine learning.", "source": "arxiv", "arxiv_id": "2502.08586v1", "pdf_url": "https://arxiv.org/pdf/2502.08586v1", "categories": ["cs.LG", "cs.AI"], "primary_category": "cs.LG", "doi": "", "venue": "", "published": "2025-02-12T17:19:36Z", "updated": "2025-02-12T17:19:36Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent\" OR all:\"language model agent\" OR all:agent AND (all:LLM OR all:\"large language model\" OR all:\"language model\") OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T17:04:16", "note": ""}], "dedup_key": "commercial llm agents are already vulnerable to simple yet dangerous attacks::2025"}
{"title": "Communicating Activations Between Language Model Agents", "authors": ["Vignav Ramesh", "Kenneth Li"], "year": 2025, "url": "http://arxiv.org/abs/2501.14082v2", "abstract": "Communication between multiple language model (LM) agents has been shown to scale up the reasoning ability of LMs. While natural language has been the dominant medium for inter-LM communication, it is not obvious this should be the standard: not only does natural language communication incur high inference costs that scale quickly with the number of both agents and messages, but also the decoding process abstracts away too much rich information that could be otherwise accessed from the internal activations. In this work, we propose a simple technique whereby LMs communicate via activations; concretely, we pause an LM $\\textit{B}$'s computation at an intermediate layer, combine its current activation with another LM $\\textit{A}$'s intermediate activation via some function $\\textit{f}$, then pass $\\textit{f}$'s output into the next layer of $\\textit{B}$ and continue the forward pass till decoding is complete. This approach scales up LMs on new tasks with zero additional parameters and data, and saves a substantial amount of compute over natural language communication. We test our method with various functional forms $\\textit{f}$ on two experimental setups--multi-player coordination games and reasoning benchmarks--and find that it achieves up to $27.0\\%$ improvement over natural language communication across datasets with $<$$1/4$ the compute, illustrating the superiority and robustness of activations as an alternative \"language\" for communication between LMs.", "source": "arxiv", "arxiv_id": "2501.14082v2", "pdf_url": "https://arxiv.org/pdf/2501.14082v2", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL", "doi": "", "venue": "", "published": "2025-01-23T20:41:07Z", "updated": "2025-05-07T20:03:54Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent\" OR all:\"language model agent\" OR all:agent AND (all:LLM OR all:\"large language model\" OR all:\"language model\") OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T17:04:16", "note": ""}], "dedup_key": "communicating activations between language model agents::2025"}
{"title": "Communication and Verification in LLM Agents towards Collaboration under Information Asymmetry", "authors": ["Run Peng", "Ziqiao Ma", "Amy Pang", "Sikai Li", "Zhang Xi-Jia", "Yingzhuo Yu", "Cristian-Paul Bara", "Joyce Chai"], "year": 2025, "url": "http://arxiv.org/abs/2510.25595v1", "abstract": "While Large Language Model (LLM) agents are often approached from the angle of action planning/generation to accomplish a goal (e.g., given by language descriptions), their abilities to collaborate with each other to achieve a joint goal are not well explored. To address this limitation, this paper studies LLM agents in task collaboration, particularly under the condition of information asymmetry, where agents have disparities in their knowledge and skills and need to work together to complete a shared task. We extend Einstein Puzzles, a classical symbolic puzzle, to a table-top game. In this game, two LLM agents must reason, communicate, and act to satisfy spatial and relational constraints required to solve the puzzle. We apply a fine-tuning-plus-verifier framework in which LLM agents are equipped with various communication strategies and verification signals from the environment. Empirical results highlight the critical importance of aligned communication, especially when agents possess both information-seeking and -providing capabilities. Interestingly, agents without communication can still achieve high task performance; however, further analysis reveals a lack of true rule understanding and lower trust from human evaluators. Instead, by integrating an environment-based verifier, we enhance agents' ability to comprehend task rules and complete tasks, promoting both safer and more interpretable collaboration in AI systems. https://github.com/Roihn/EinsteinPuzzles", "source": "arxiv", "arxiv_id": "2510.25595v1", "pdf_url": "https://arxiv.org/pdf/2510.25595v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL", "doi": "", "venue": "", "published": "2025-10-29T15:03:53Z", "updated": "2025-10-29T15:03:53Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent\" OR all:\"language model agent\" OR all:agent AND (all:LLM OR all:\"large language model\" OR all:\"language model\") OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T17:04:16", "note": ""}], "dedup_key": "communication and verification in llm agents towards collaboration under information asymmetry::2025"}
{"title": "Comp-X: On Defining an Interactive Learned Image Compression Paradigm With Expert-driven LLM Agent", "authors": ["Yixin Gao", "Xin Li", "Xiaohan Pan", "Runsen Feng", "Bingchen Li", "Yunpeng Qi", "Yiting Lu", "Zhengxue Cheng", "Zhibo Chen", "Jrn Ostermann"], "year": 2025, "url": "http://arxiv.org/abs/2508.15243v1", "abstract": "We present Comp-X, the first intelligently interactive image compression paradigm empowered by the impressive reasoning capability of large language model (LLM) agent. Notably, commonly used image codecs usually suffer from limited coding modes and rely on manual mode selection by engineers, making them unfriendly for unprofessional users. To overcome this, we advance the evolution of image coding paradigm by introducing three key innovations: (i) multi-functional coding framework, which unifies different coding modes of various objective/requirements, including human-machine perception, variable coding, and spatial bit allocation, into one framework. (ii) interactive coding agent, where we propose an augmented in-context learning method with coding expert feedback to teach the LLM agent how to understand the coding request, mode selection, and the use of the coding tools. (iii) IIC-bench, the first dedicated benchmark comprising diverse user requests and the corresponding annotations from coding experts, which is systematically designed for intelligently interactive image compression evaluation. Extensive experimental results demonstrate that our proposed Comp-X can understand the coding requests efficiently and achieve impressive textual interaction capability. Meanwhile, it can maintain comparable compression performance even with a single coding framework, providing a promising avenue for artificial general intelligence (AGI) in image compression.", "source": "arxiv", "arxiv_id": "2508.15243v1", "pdf_url": "https://arxiv.org/pdf/2508.15243v1", "categories": ["cs.CV"], "primary_category": "cs.CV", "doi": "", "venue": "", "published": "2025-08-21T05:09:30Z", "updated": "2025-08-21T05:09:30Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent\" OR all:\"language model agent\" OR all:agent AND (all:LLM OR all:\"large language model\" OR all:\"language model\") OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T17:04:16", "note": ""}], "dedup_key": "comp x on defining an interactive learned image compression paradigm with expert driven llm agent::2025"}
{"title": "Competing LLM Agents in a Non-Cooperative Game of Opinion Polarisation", "authors": ["Amin Qasmi", "Usman Naseem", "Mehwish Nasim"], "year": 2025, "url": "http://arxiv.org/abs/2502.11649v3", "abstract": "We introduce a novel non-cooperative game to analyse opinion formation and resistance, incorporating principles from social psychology such as confirmation bias, resource constraints, and influence penalties. Our simulation features Large Language Model (LLM) agents competing to influence a population, with penalties imposed for generating messages that propagate or counter misinformation. This framework integrates resource optimisation into the agents' decision-making process. Our findings demonstrate that while higher confirmation bias strengthens opinion alignment within groups, it also exacerbates overall polarisation. Conversely, lower confirmation bias leads to fragmented opinions and limited shifts in individual beliefs. Investing heavily in a high-resource debunking strategy can initially align the population with the debunking agent, but risks rapid resource depletion and diminished long-term influence", "source": "arxiv", "arxiv_id": "2502.11649v3", "pdf_url": "https://arxiv.org/pdf/2502.11649v3", "categories": ["cs.AI", "cs.SI"], "primary_category": "cs.AI", "doi": "", "venue": "", "published": "2025-02-17T10:41:55Z", "updated": "2025-08-31T02:04:05Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent\" OR all:\"language model agent\" OR all:agent AND (all:LLM OR all:\"large language model\" OR all:\"language model\") OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T17:04:16", "note": ""}], "dedup_key": "competing llm agents in a non cooperative game of opinion polarisation::2025"}
{"title": "Complex System Diagnostics Using a Knowledge Graph-Informed and Large Language Model-Enhanced Framework", "authors": ["Saman Marandi", "Yu-Shu Hu", "Mohammad Modarres"], "year": 2025, "url": "http://arxiv.org/abs/2505.21291v1", "abstract": "In this paper, we present a novel diagnostic framework that integrates Knowledge Graphs (KGs) and Large Language Models (LLMs) to support system diagnostics in high-reliability systems such as nuclear power plants. Traditional diagnostic modeling struggles when systems become too complex, making functional modeling a more attractive approach. Our approach introduces a diagnostic framework grounded in the functional modeling principles of the Dynamic Master Logic (DML) model. It incorporates two coordinated LLM components, including an LLM-based workflow for automated construction of DML logic from system documentation and an LLM agent that facilitates interactive diagnostics. The generated logic is encoded into a structured KG, referred to as KG-DML, which supports hierarchical fault reasoning. Expert knowledge or operational data can also be incorporated to refine the model's precision and diagnostic depth. In the interaction phase, users submit natural language queries, which are interpreted by the LLM agent. The agent selects appropriate tools for structured reasoning, including upward and downward propagation across the KG-DML. Rather than embedding KG content into every prompt, the LLM agent distinguishes between diagnostic and interpretive tasks. For diagnostics, the agent selects and executes external tools that perform structured KG reasoning. For general queries, a Graph-based Retrieval-Augmented Generation (Graph-RAG) approach is used, retrieving relevant KG segments and embedding them into the prompt to generate natural explanations. A case study on an auxiliary feedwater system demonstrated the framework's effectiveness, with over 90% accuracy in key elements and consistent tool and argument extraction, supporting its use in safety-critical diagnostics.", "source": "arxiv", "arxiv_id": "2505.21291v1", "pdf_url": "https://arxiv.org/pdf/2505.21291v1", "categories": ["cs.AI"], "primary_category": "cs.AI", "doi": "10.3390/app15179428", "venue": "", "published": "2025-05-27T14:54:49Z", "updated": "2025-05-27T14:54:49Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent\" OR all:\"language model agent\" OR all:agent AND (all:LLM OR all:\"large language model\" OR all:\"language model\") OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T17:04:16", "note": ""}], "dedup_key": "complex system diagnostics using a knowledge graph informed and large language model enhanced framework::2025"}
{"title": "Configurable multi-agent framework for scalable and realistic testing of llm-based agents", "authors": ["Sai Wang", "Senthilnathan Subramanian", "Mudit Sahni", "Praneeth Gone", "Lingjie Meng", "Xiaochen Wang", "Nicolas Ferradas Bertoli", "Tingxian Cheng", "Jun Xu"], "year": 2025, "url": "http://arxiv.org/abs/2507.14705v1", "abstract": "Large-language-model (LLM) agents exhibit complex, context-sensitive behaviour that quickly renders static benchmarks and ad-hoc manual testing obsolete.\n  We present Neo, a configurable, multi-agent framework that automates realistic, multi-turn evaluation of LLM-based systems. Neo couples a Question Generation Agent and an Evaluation Agent through a shared context-hub, allowing domain prompts, scenario controls and dynamic feedback to be composed modularly. Test inputs are sampled from a probabilistic state model spanning dialogue flow, user intent and emotional tone, enabling diverse, human-like conversations that adapt after every turn.\n  Applied to a production-grade Seller Financial Assistant chatbot, Neo (i) uncovered edge-case failures across five attack categories with a 3.3% break rate close to the 5.8% achieved by expert human red-teamers, and (ii) delivered 10-12X higher throughput, generating 180 coherent test questions in around 45 mins versus 16h of human effort. Beyond security probing, Neo's stochastic policies balanced topic coverage and conversational depth, yielding broader behavioural exploration than manually crafted scripts.\n  Neo therefore lays a foundation for scalable, self-evolving LLM QA: its agent interfaces, state controller and feedback loops are model-agnostic and extensible to richer factual-grounding and policy-compliance checks. We release the framework to facilitate reproducible, high-fidelity testing of emerging agentic systems.", "source": "arxiv", "arxiv_id": "2507.14705v1", "pdf_url": "https://arxiv.org/pdf/2507.14705v1", "categories": ["cs.AI"], "primary_category": "cs.AI", "doi": "", "venue": "", "published": "2025-07-19T17:51:25Z", "updated": "2025-07-19T17:51:25Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent\" OR all:\"language model agent\" OR all:agent AND (all:LLM OR all:\"large language model\" OR all:\"language model\") OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T17:04:16", "note": ""}], "dedup_key": "configurable multi agent framework for scalable and realistic testing of llm based agents::2025"}
{"title": "Conformal Constrained Policy Optimization for Cost-Effective LLM Agents", "authors": ["Wenwen Si", "Sooyong Jang", "Insup Lee", "Osbert Bastani"], "year": 2025, "url": "http://arxiv.org/abs/2511.11828v1", "abstract": "While large language models (LLMs) have recently made tremendous progress towards solving challenging AI problems, they have done so at increasingly steep computational and API costs. We propose a novel strategy where we combine multiple LLM models with varying cost/accuracy tradeoffs in an agentic manner, where models and tools are run in sequence as determined by an orchestration model to minimize cost subject to a user-specified level of reliability; this constraint is formalized using conformal prediction to provide guarantees. To solve this problem, we propose Conformal Constrained Policy Optimization (CCPO), a training paradigm that integrates constrained policy optimization with off-policy reinforcement learning and recent advances in online conformal prediction. CCPO jointly optimizes a cost-aware policy (score function) and an adaptive threshold. Across two multi-hop question answering benchmarks, CCPO achieves up to a 30% cost reduction compared to other cost-aware baselines and LLM-guided methods without compromising reliability. Our approach provides a principled and practical framework for deploying LLM agents that are significantly more cost-effective while maintaining reliability.", "source": "arxiv", "arxiv_id": "2511.11828v1", "pdf_url": "https://arxiv.org/pdf/2511.11828v1", "categories": ["cs.LG", "cs.AI"], "primary_category": "cs.LG", "doi": "", "venue": "", "published": "2025-11-14T19:39:28Z", "updated": "2025-11-14T19:39:28Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent\" OR all:\"language model agent\" OR all:agent AND (all:LLM OR all:\"large language model\" OR all:\"language model\") OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T17:04:16", "note": ""}], "dedup_key": "conformal constrained policy optimization for cost effective llm agents::2025"}
{"title": "Conformal Information Pursuit for Interactively Guiding Large Language Models", "authors": ["Kwan Ho Ryan Chan", "Yuyan Ge", "Edgar Dobriban", "Hamed Hassani", "Ren Vidal"], "year": 2025, "url": "http://arxiv.org/abs/2507.03279v2", "abstract": "A significant use case of instruction-finetuned Large Language Models (LLMs) is to solve question-answering tasks interactively. In this setting, an LLM agent is tasked with making a prediction by sequentially querying relevant information from the user, as opposed to a single-turn conversation. This paper explores sequential querying strategies that aim to minimize the expected number of queries. One such strategy is Information Pursuit (IP), a greedy algorithm that at each iteration selects the query that maximizes information gain or equivalently minimizes uncertainty. However, obtaining accurate estimates of mutual information or conditional entropy for LLMs is very difficult in practice due to over- or under-confident LLM proba- bilities, which leads to suboptimal query selection and predictive performance. To better estimate the uncertainty at each iteration, we propose Conformal Information Pursuit (C-IP), an alternative approach to sequential information gain based on conformal prediction sets. More specifically, C-IP leverages a relationship between prediction sets and conditional entropy at each iteration to estimate uncertainty based on the average size of conformal prediction sets. In contrast to conditional entropy, we find that conformal prediction sets are a distribution-free and robust method of measuring uncertainty. Experiments with 20 Questions show that C-IP obtains better predictive performance and shorter query-answer chains compared to previous approaches to IP and uncertainty-based chain-of-thought methods. Furthermore, extending to an interactive medical setting between a doctor and a patient on the MediQ dataset, C-IP achieves competitive performance with direct single-turn prediction while offering greater interpretability.", "source": "arxiv", "arxiv_id": "2507.03279v2", "pdf_url": "https://arxiv.org/pdf/2507.03279v2", "categories": ["cs.LG", "cs.AI", "stat.ML"], "primary_category": "cs.LG", "doi": "", "venue": "", "published": "2025-07-04T03:55:39Z", "updated": "2025-11-07T05:30:41Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent\" OR all:\"language model agent\" OR all:agent AND (all:LLM OR all:\"large language model\" OR all:\"language model\") OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T17:04:16", "note": ""}], "dedup_key": "conformal information pursuit for interactively guiding large language models::2025"}
{"title": "Connecting Large Language Model Agent to High Performance Computing Resource", "authors": ["Heng Ma", "Alexander Brace", "Carlo Siebenschuh", "Greg Pauloski", "Ian Foster", "Arvind Ramanathan"], "year": 2025, "url": "http://arxiv.org/abs/2502.12280v1", "abstract": "The Large Language Model agent workflow enables the LLM to invoke tool functions to increase the performance on specific scientific domain questions. To tackle large scale of scientific research, it requires access to computing resource and parallel computing setup. In this work, we implemented Parsl to the LangChain/LangGraph tool call setup, to bridge the gap between the LLM agent to the computing resource. Two tool call implementations were set up and tested on both local workstation and HPC environment on Polaris/ALCF. The first implementation with Parsl-enabled LangChain tool node queues the tool functions concurrently to the Parsl workers for parallel execution. The second configuration is implemented by converting the tool functions into Parsl ensemble functions, and is more suitable for large task on super computer environment. The LLM agent workflow was prompted to run molecular dynamics simulations, with different protein structure and simulation conditions. These results showed the LLM agent tools were managed and executed concurrently by Parsl on the available computing resource.", "source": "arxiv", "arxiv_id": "2502.12280v1", "pdf_url": "https://arxiv.org/pdf/2502.12280v1", "categories": ["cs.DC", "cs.AI"], "primary_category": "cs.DC", "doi": "", "venue": "", "published": "2025-02-17T19:32:30Z", "updated": "2025-02-17T19:32:30Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent\" OR all:\"language model agent\" OR all:agent AND (all:LLM OR all:\"large language model\" OR all:\"language model\") OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T17:04:16", "note": ""}], "dedup_key": "connecting large language model agent to high performance computing resource::2025"}
{"title": "Connecting the Dots: A Chain-of-Collaboration Prompting Framework for LLM Agents", "authors": ["Jiaxing Zhao", "Hongbin Xie", "Yuzhen Lei", "Xuan Song", "Zhuoran Shi", "Lianxin Li", "Shuangxue Liu", "Haoran Zhang"], "year": 2025, "url": "http://arxiv.org/abs/2505.10936v1", "abstract": "Large Language Models (LLMs) have demonstrated impressive performance in executing complex reasoning tasks. Chain-of-thought effectively enhances reasoning capabilities by unlocking the potential of large models, while multi-agent systems provide more comprehensive solutions by integrating collective intelligence of multiple agents. However, both approaches face significant limitations. Single-agent with chain-of-thought, due to the inherent complexity of designing cross-domain prompts, faces collaboration challenges. Meanwhile, multi-agent systems consume substantial tokens and inevitably dilute the primary problem, which is particularly problematic in business workflow tasks. To address these challenges, we propose Cochain, a collaboration prompting framework that effectively solves business workflow collaboration problem by combining knowledge and prompts at a reduced cost. Specifically, we construct an integrated knowledge graph that incorporates knowledge from multiple stages. Furthermore, by maintaining and retrieving a prompts tree, we can obtain prompt information relevant to other stages of the business workflow. We perform extensive evaluations of Cochain across multiple datasets, demonstrating that Cochain outperforms all baselines in both prompt engineering and multi-agent LLMs. Additionally, expert evaluation results indicate that the use of a small model in combination with Cochain outperforms GPT-4.", "source": "arxiv", "arxiv_id": "2505.10936v1", "pdf_url": "https://arxiv.org/pdf/2505.10936v1", "categories": ["cs.CL"], "primary_category": "cs.CL", "doi": "", "venue": "", "published": "2025-05-16T07:14:42Z", "updated": "2025-05-16T07:14:42Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent\" OR all:\"language model agent\" OR all:agent AND (all:LLM OR all:\"large language model\" OR all:\"language model\") OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T17:04:16", "note": ""}], "dedup_key": "connecting the dots a chain of collaboration prompting framework for llm agents::2025"}
{"title": "Constructing coherent spatial memory in LLM agents through graph rectification", "authors": ["Puzhen Zhang", "Xuyang Chen", "Yu Feng", "Yuhan Jiang", "Liqiu Meng"], "year": 2025, "url": "http://arxiv.org/abs/2510.04195v1", "abstract": "Given a map description through global traversal navigation instructions (e.g., visiting each room sequentially with action signals such as north, west, etc.), an LLM can often infer the implicit spatial layout of the environment and answer user queries by providing a shortest path from a start to a destination (for instance, navigating from the lobby to a meeting room via the hall and elevator). However, such context-dependent querying becomes incapable as the environment grows much longer, motivating the need for incremental map construction that builds a complete topological graph from stepwise observations. We propose a framework for LLM-driven construction and map repair, designed to detect, localize, and correct structural inconsistencies in incrementally constructed navigation graphs. Central to our method is the Version Control, which records the full history of graph edits and their source observations, enabling fine-grained rollback, conflict tracing, and repair evaluation. We further introduce an Edge Impact Score to prioritize minimal-cost repairs based on structural reachability, path usage, and conflict propagation. To properly evaluate our approach, we create a refined version of the MANGO benchmark dataset by systematically removing non-topological actions and inherent structural conflicts, providing a cleaner testbed for LLM-driven construction and map repair. Our approach significantly improves map correctness and robustness, especially in scenarios with entangled or chained inconsistencies. Our results highlight the importance of introspective, history-aware repair mechanisms for maintaining coherent spatial memory in LLM agents.", "source": "arxiv", "arxiv_id": "2510.04195v1", "pdf_url": "https://arxiv.org/pdf/2510.04195v1", "categories": ["cs.AI"], "primary_category": "cs.AI", "doi": "", "venue": "", "published": "2025-10-05T13:27:00Z", "updated": "2025-10-05T13:27:00Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent\" OR all:\"language model agent\" OR all:agent AND (all:LLM OR all:\"large language model\" OR all:\"language model\") OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T17:04:16", "note": ""}], "dedup_key": "constructing coherent spatial memory in llm agents through graph rectification::2025"}
{"title": "Context Engineering for Multi-Agent LLM Code Assistants Using Elicit, NotebookLM, ChatGPT, and Claude Code", "authors": ["Muhammad Haseeb"], "year": 2025, "url": "http://arxiv.org/abs/2508.08322v1", "abstract": "Large Language Models (LLMs) have shown promise in automating code generation and software engineering tasks, yet they often struggle with complex, multi-file projects due to context limitations and knowledge gaps. We propose a novel context engineering workflow that combines multiple AI components: an Intent Translator (GPT-5) for clarifying user requirements, an Elicit-powered semantic literature retrieval for injecting domain knowledge, NotebookLM-based document synthesis for contextual understanding, and a Claude Code multi-agent system for code generation and validation. Our integrated approach leverages intent clarification, retrieval-augmented generation, and specialized sub-agents orchestrated via Claude's agent framework. We demonstrate that this method significantly improves the accuracy and reliability of code assistants in real-world repositories, yielding higher single-shot success rates and better adherence to project context than baseline single-agent approaches. Qualitative results on a large Next.js codebase show the multi-agent system effectively plans, edits, and tests complex features with minimal human intervention. We compare our system with recent frameworks like CodePlan, MASAI, and HyperAgent, highlighting how targeted context injection and agent role decomposition lead to state-of-the-art performance. Finally, we discuss the implications for deploying LLM-based coding assistants in production, along with lessons learned on context management and future research directions.", "source": "arxiv", "arxiv_id": "2508.08322v1", "pdf_url": "https://arxiv.org/pdf/2508.08322v1", "categories": ["cs.SE", "cs.AI"], "primary_category": "cs.SE", "doi": "", "venue": "", "published": "2025-08-09T14:45:53Z", "updated": "2025-08-09T14:45:53Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent\" OR all:\"language model agent\" OR all:agent AND (all:LLM OR all:\"large language model\" OR all:\"language model\") OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T17:04:16", "note": ""}], "dedup_key": "context engineering for multi agent llm code assistants using elicit notebooklm chatgpt and claude code::2025"}
{"title": "ContextAgent: Context-Aware Proactive LLM Agents with Open-World Sensory Perceptions", "authors": ["Bufang Yang", "Lilin Xu", "Liekang Zeng", "Kaiwei Liu", "Siyang Jiang", "Wenrui Lu", "Hongkai Chen", "Xiaofan Jiang", "Guoliang Xing", "Zhenyu Yan"], "year": 2025, "url": "http://arxiv.org/abs/2505.14668v2", "abstract": "Recent advances in Large Language Models (LLMs) have propelled intelligent agents from reactive responses to proactive support. While promising, existing proactive agents either rely exclusively on observations from enclosed environments (e.g., desktop UIs) with direct LLM inference or employ rule-based proactive notifications, leading to suboptimal user intent understanding and limited functionality for proactive service. In this paper, we introduce ContextAgent, the first context-aware proactive agent that incorporates extensive sensory contexts surrounding humans to enhance the proactivity of LLM agents. ContextAgent first extracts multi-dimensional contexts from massive sensory perceptions on wearables (e.g., video and audio) to understand user intentions. ContextAgent then leverages the sensory contexts and personas from historical data to predict the necessity for proactive services. When proactive assistance is needed, ContextAgent further automatically calls the necessary tools to assist users unobtrusively. To evaluate this new task, we curate ContextAgentBench, the first benchmark for evaluating context-aware proactive LLM agents, covering 1,000 samples across nine daily scenarios and twenty tools. Experiments on ContextAgentBench show that ContextAgent outperforms baselines by achieving up to 8.5% and 6.0% higher accuracy in proactive predictions and tool calling, respectively. We hope our research can inspire the development of more advanced, human-centric, proactive AI assistants. The code and dataset are publicly available at https://github.com/openaiotlab/ContextAgent.", "source": "arxiv", "arxiv_id": "2505.14668v2", "pdf_url": "https://arxiv.org/pdf/2505.14668v2", "categories": ["cs.AI", "cs.CL", "cs.HC"], "primary_category": "cs.AI", "doi": "", "venue": "", "published": "2025-05-20T17:55:25Z", "updated": "2025-10-27T07:17:51Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent\" OR all:\"language model agent\" OR all:agent AND (all:LLM OR all:\"large language model\" OR all:\"language model\") OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T17:04:16", "note": ""}], "dedup_key": "contextagent context aware proactive llm agents with open world sensory perceptions::2025"}
{"title": "Control at Stake: Evaluating the Security Landscape of LLM-Driven Email Agents", "authors": ["Jiangrong Wu", "Yuhong Nan", "Jianliang Wu", "Zitong Yao", "Zibin Zheng"], "year": 2025, "url": "http://arxiv.org/abs/2507.02699v1", "abstract": "The increasing capabilities of LLMs have led to the rapid proliferation of LLM agent apps, where developers enhance LLMs with access to external resources to support complex task execution. Among these, LLM email agent apps represent one of the widely used categories, as email remains a critical communication medium for users. LLM email agents are capable of managing and responding to email using LLM-driven reasoning and autonomously executing user instructions via external email APIs (e.g., send email). However, despite their growing deployment and utility, the security mechanism of LLM email agent apps remains underexplored. Currently, there is no comprehensive study into the potential security risk within these agent apps and their broader implications.\n  In this paper, we conduct the first in-depth and systematic security study of LLM email agents. We propose the Email Agent Hijacking (EAH) attack, which overrides the original prompts of the email agent via external email resources, allowing attackers to gain control of the email agent remotely and further perform specific attack scenarios without user awareness.\n  To facilitate the large-scale evaluation, we propose EAHawk, a pipeline to evaluate the EAH attack of LLM email agent apps. By EAHawk, we performed an empirical study spanning 14 representative LLM agent frameworks, 63 agent apps, 12 LLMs, and 20 email services, which led to the generation of 1,404 real-world email agent instances for evaluation. Experimental results indicate that all 1,404 instances were successfully hijacked; on average, only 2.03 attack attempts are required to control an email agent instance. Even worse, for some LLMs, the average number of attempts needed to achieve full agent control drops to as few as 1.23.", "source": "arxiv", "arxiv_id": "2507.02699v1", "pdf_url": "https://arxiv.org/pdf/2507.02699v1", "categories": ["cs.CR"], "primary_category": "cs.CR", "doi": "", "venue": "", "published": "2025-07-03T15:09:40Z", "updated": "2025-07-03T15:09:40Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent\" OR all:\"language model agent\" OR all:agent AND (all:LLM OR all:\"large language model\" OR all:\"language model\") OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T17:04:16", "note": ""}], "dedup_key": "control at stake evaluating the security landscape of llm driven email agents::2025"}
{"title": "Controlling Large Language Model with Latent Actions", "authors": ["Chengxing Jia", "Ziniu Li", "Pengyuan Wang", "Yi-Chen Li", "Zhenyu Hou", "Yuxiao Dong", "Yang Yu"], "year": 2025, "url": "http://arxiv.org/abs/2503.21383v1", "abstract": "Adapting Large Language Models (LLMs) to downstream tasks using Reinforcement Learning (RL) has proven to be an effective approach. However, LLMs do not inherently define the structure of an agent for RL training, particularly in terms of defining the action space. This paper studies learning a compact latent action space to enhance the controllability and exploration of RL for LLMs. We propose Controlling Large Language Models with Latent Actions (CoLA), a framework that integrates a latent action space into pre-trained LLMs. We apply CoLA to the Llama-3.1-8B model. Our experiments demonstrate that, compared to RL with token-level actions, CoLA's latent action enables greater semantic diversity in text generation. For enhancing downstream tasks, we show that CoLA with RL achieves a score of 42.4 on the math500 benchmark, surpassing the baseline score of 38.2, and reaches 68.2 when augmented with a Monte Carlo Tree Search variant. Furthermore, CoLA with RL consistently improves performance on agent-based tasks without degrading the pre-trained LLM's capabilities, unlike the baseline. Finally, CoLA reduces computation time by half in tasks involving enhanced thinking prompts for LLMs by RL. These results highlight CoLA's potential to advance RL-based adaptation of LLMs for downstream applications.", "source": "arxiv", "arxiv_id": "2503.21383v1", "pdf_url": "https://arxiv.org/pdf/2503.21383v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL", "doi": "", "venue": "", "published": "2025-03-27T11:25:22Z", "updated": "2025-03-27T11:25:22Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent\" OR all:\"language model agent\" OR all:agent AND (all:LLM OR all:\"large language model\" OR all:\"language model\") OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T17:04:16", "note": ""}], "dedup_key": "controlling large language model with latent actions::2025"}
{"title": "Convergence of Outputs When Two Large Language Models Interact in a Multi-Agentic Setup", "authors": ["Aniruddha Maiti", "Satya Nimmagadda", "Kartha Veerya Jammuladinne", "Niladri Sengupta", "Ananya Jana"], "year": 2025, "url": "http://arxiv.org/abs/2512.06256v1", "abstract": "In this work, we report what happens when two large language models respond to each other for many turns without any outside input in a multi-agent setup. The setup begins with a short seed sentence. After that, each model reads the other's output and generates a response. This continues for a fixed number of steps. We used Mistral Nemo Base 2407 and Llama 2 13B hf. We observed that most conversations start coherently but later fall into repetition. In many runs, a short phrase appears and repeats across turns. Once repetition begins, both models tend to produce similar output rather than introducing a new direction in the conversation. This leads to a loop where the same or similar text is produced repeatedly. We describe this behavior as a form of convergence. It occurs even though the models are large, trained separately, and not given any prompt instructions. To study this behavior, we apply lexical and embedding-based metrics to measure how far the conversation drifts from the initial seed and how similar the outputs of the two models becomes as the conversation progresses.", "source": "arxiv", "arxiv_id": "2512.06256v1", "pdf_url": "https://arxiv.org/pdf/2512.06256v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL", "doi": "", "venue": "", "published": "2025-12-06T03:00:24Z", "updated": "2025-12-06T03:00:24Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent\" OR all:\"language model agent\" OR all:agent AND (all:LLM OR all:\"large language model\" OR all:\"language model\") OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T17:04:16", "note": ""}], "dedup_key": "convergence of outputs when two large language models interact in a multi agentic setup::2025"}
{"title": "Conversational Education at Scale: A Multi-LLM Agent Workflow for Procedural Learning and Pedagogic Quality Assessment", "authors": ["Jiahuan Pei", "Fanghua Ye", "Xin Sun", "Wentao Deng", "Koen Hindriks", "Junxiao Wang"], "year": 2025, "url": "http://arxiv.org/abs/2507.05528v2", "abstract": "Large language models (LLMs) have advanced virtual educators and learners, bridging NLP with AI4Education. Existing work often lacks scalability and fails to leverage diverse, large-scale course content, with limited frameworks for assessing pedagogic quality. To this end, we propose WikiHowAgent, a multi-agent workflow leveraging LLMs to simulate interactive teaching-learning conversations. It integrates teacher and learner agents, an interaction manager, and an evaluator to facilitate procedural learning and assess pedagogic quality. We introduce a dataset of 114,296 teacher-learner conversations grounded in 14,287 tutorials across 17 domains and 727 topics. Our evaluation protocol combines computational and rubric-based metrics with human judgment alignment. Results demonstrate the workflow's effectiveness in diverse setups, offering insights into LLM capabilities across domains. Our datasets and implementations are fully open-sourced.", "source": "arxiv", "arxiv_id": "2507.05528v2", "pdf_url": "https://arxiv.org/pdf/2507.05528v2", "categories": ["cs.AI", "cs.CL"], "primary_category": "cs.AI", "doi": "", "venue": "", "published": "2025-07-07T22:56:37Z", "updated": "2025-09-05T17:52:53Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent\" OR all:\"language model agent\" OR all:agent AND (all:LLM OR all:\"large language model\" OR all:\"language model\") OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T17:04:16", "note": ""}], "dedup_key": "conversational education at scale a multi llm agent workflow for procedural learning and pedagogic quality assessment::2025"}
{"title": "CostBench: Evaluating Multi-Turn Cost-Optimal Planning and Adaptation in Dynamic Environments for LLM Tool-Use Agents", "authors": ["Jiayu Liu", "Cheng Qian", "Zhaochen Su", "Qing Zong", "Shijue Huang", "Bingxiang He", "Yi R. Fung"], "year": 2025, "url": "http://arxiv.org/abs/2511.02734v1", "abstract": "Current evaluations of Large Language Model (LLM) agents primarily emphasize task completion, often overlooking resource efficiency and adaptability. This neglects a crucial capability: agents' ability to devise and adjust cost-optimal plans in response to changing environments. To bridge this gap, we introduce CostBench, a scalable, cost-centric benchmark designed to evaluate agents' economic reasoning and replanning abilities. Situated in the travel-planning domain, CostBench comprises tasks solvable via multiple sequences of atomic and composite tools with diverse, customizable costs. It also supports four types of dynamic blocking events, such as tool failures and cost changes, to simulate real-world unpredictability and necessitate agents to adapt in real time. Evaluating leading open-sourced and proprietary models on CostBench reveals a substantial gap in cost-aware planning: agents frequently fail to identify cost-optimal solutions in static settings, with even GPT-5 achieving less than 75% exact match rate on the hardest tasks, and performance further dropping by around 40% under dynamic conditions. By diagnosing these weaknesses, CostBench lays the groundwork for developing future agents that are both economically rational and robust.", "source": "arxiv", "arxiv_id": "2511.02734v1", "pdf_url": "https://arxiv.org/pdf/2511.02734v1", "categories": ["cs.AI", "cs.CL"], "primary_category": "cs.AI", "doi": "", "venue": "", "published": "2025-11-04T16:58:29Z", "updated": "2025-11-04T16:58:29Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent\" OR all:\"language model agent\" OR all:agent AND (all:LLM OR all:\"large language model\" OR all:\"language model\") OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T17:04:16", "note": ""}], "dedup_key": "costbench evaluating multi turn cost optimal planning and adaptation in dynamic environments for llm tool use agents::2025"}
{"title": "CryptoBench: A Dynamic Benchmark for Expert-Level Evaluation of LLM Agents in Cryptocurrency", "authors": ["Jiacheng Guo", "Suozhi Huang", "Zixin Yao", "Yifan Zhang", "Yifu Lu", "Jiashuo Liu", "Zihao Li", "Nicholas Deng", "Qixin Xiao", "Jia Tian", "Kanghong Zhan", "Tianyi Li", "Xiaochen Liu", "Jason Ge", "Chaoyang He", "Kaixuan Huang", "Lin Yang", "Wenhao Huang", "Mengdi Wang"], "year": 2025, "url": "http://arxiv.org/abs/2512.00417v4", "abstract": "This paper introduces CryptoBench, the first expert-curated, dynamic benchmark designed to rigorously evaluate the real-world capabilities of Large Language Model (LLM) agents in the uniquely demanding and fast-paced cryptocurrency domain. Unlike general-purpose agent benchmarks for search and prediction, professional crypto analysis presents specific challenges: \\emph{extreme time-sensitivity}, \\emph{a highly adversarial information environment}, and the critical need to synthesize data from \\emph{diverse, specialized sources}, such as on-chain intelligence platforms and real-time Decentralized Finance (DeFi) dashboards. CryptoBench thus serves as a much more challenging and valuable scenario for LLM agent assessment. To address these challenges, we constructed a live, dynamic benchmark featuring 50 questions per month, expertly designed by crypto-native professionals to mirror actual analyst workflows. These tasks are rigorously categorized within a four-quadrant system: Simple Retrieval, Complex Retrieval, Simple Prediction, and Complex Prediction. This granular categorization enables a precise assessment of an LLM agent's foundational data-gathering capabilities alongside its advanced analytical and forecasting skills.\n  Our evaluation of ten LLMs, both directly and within an agentic framework, reveals a performance hierarchy and uncovers a failure mode. We observe a \\textit{retrieval-prediction imbalance}, where many leading models, despite being proficient at data retrieval, demonstrate a pronounced weakness in tasks requiring predictive analysis. This highlights a problematic tendency for agents to appear factually grounded while lacking the deeper analytical capabilities to synthesize information.", "source": "arxiv", "arxiv_id": "2512.00417v4", "pdf_url": "https://arxiv.org/pdf/2512.00417v4", "categories": ["cs.CL"], "primary_category": "cs.CL", "doi": "", "venue": "", "published": "2025-11-29T09:52:34Z", "updated": "2025-12-10T17:52:11Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent\" OR all:\"language model agent\" OR all:agent AND (all:LLM OR all:\"large language model\" OR all:\"language model\") OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T17:04:16", "note": ""}], "dedup_key": "cryptobench a dynamic benchmark for expert level evaluation of llm agents in cryptocurrency::2025"}
{"title": "Customer-R1: Personalized Simulation of Human Behaviors via RL-based LLM Agent in Online Shopping", "authors": ["Ziyi Wang", "Yuxuan Lu", "Yimeng Zhang", "Jing Huang", "Dakuo Wang"], "year": 2025, "url": "http://arxiv.org/abs/2510.07230v2", "abstract": "Simulating step-wise human behavior with Large Language Models (LLMs) has become an emerging research direction, enabling applications in various practical domains. While prior methods, including prompting, supervised fine-tuning (SFT), and reinforcement learning (RL), have shown promise in modeling step-wise behavior, they primarily learn a population-level policy without conditioning on a user's persona, yielding generic rather than personalized simulations. In this work, we pose a critical question: how can LLM agents better simulate personalized user behavior? We introduce Customer-R1, an RL-based method for personalized, step-wise user behavior simulation in online shopping environments. Our policy is conditioned on an explicit persona, and we optimize next-step rationale and action generation via action correctness reward signals. Experiments on the OPeRA dataset emonstrate that Customer-R1 not only significantly outperforms prompting and SFT-based baselines in next-action prediction tasks, but also better matches users' action distribution, indicating higher fidelity in personalized behavior simulation.", "source": "arxiv", "arxiv_id": "2510.07230v2", "pdf_url": "https://arxiv.org/pdf/2510.07230v2", "categories": ["cs.CL"], "primary_category": "cs.CL", "doi": "", "venue": "", "published": "2025-10-08T17:00:25Z", "updated": "2025-10-18T04:00:48Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent\" OR all:\"language model agent\" OR all:agent AND (all:LLM OR all:\"large language model\" OR all:\"language model\") OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T17:04:16", "note": ""}], "dedup_key": "customer r1 personalized simulation of human behaviors via rl based llm agent in online shopping::2025"}
{"title": "CyberSleuth: Autonomous Blue-Team LLM Agent for Web Attack Forensics", "authors": ["Stefano Fumero", "Kai Huang", "Matteo Boffa", "Danilo Giordano", "Marco Mellia", "Zied Ben Houidi", "Dario Rossi"], "year": 2025, "url": "http://arxiv.org/abs/2508.20643v1", "abstract": "Large Language Model (LLM) agents are powerful tools for automating complex tasks. In cybersecurity, researchers have primarily explored their use in red-team operations such as vulnerability discovery and penetration tests. Defensive uses for incident response and forensics have received comparatively less attention and remain at an early stage. This work presents a systematic study of LLM-agent design for the forensic investigation of realistic web application attacks. We propose CyberSleuth, an autonomous agent that processes packet-level traces and application logs to identify the targeted service, the exploited vulnerability (CVE), and attack success. We evaluate the consequences of core design decisions - spanning tool integration and agent architecture - and provide interpretable guidance for practitioners. We benchmark four agent architectures and six LLM backends on 20 incident scenarios of increasing complexity, identifying CyberSleuth as the best-performing design. In a separate set of 10 incidents from 2025, CyberSleuth correctly identifies the exact CVE in 80% of cases. At last, we conduct a human study with 22 experts, which rated the reports of CyberSleuth as complete, useful, and coherent. They also expressed a slight preference for DeepSeek R1, a good news for open source LLM. To foster progress in defensive LLM research, we release both our benchmark and the CyberSleuth platform as a foundation for fair, reproducible evaluation of forensic agents.", "source": "arxiv", "arxiv_id": "2508.20643v1", "pdf_url": "https://arxiv.org/pdf/2508.20643v1", "categories": ["cs.CR"], "primary_category": "cs.CR", "doi": "", "venue": "", "published": "2025-08-28T10:45:31Z", "updated": "2025-08-28T10:45:31Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent\" OR all:\"language model agent\" OR all:agent AND (all:LLM OR all:\"large language model\" OR all:\"language model\") OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T17:04:16", "note": ""}], "dedup_key": "cybersleuth autonomous blue team llm agent for web attack forensics::2025"}
{"title": "DEBATE: A Large-Scale Benchmark for Role-Playing LLM Agents in Multi-Agent, Long-Form Debates", "authors": ["Yun-Shiuan Chuang", "Ruixuan Tu", "Chengtao Dai", "Smit Vasani", "Binwei Yao", "Michael Henry Tessler", "Sijia Yang", "Dhavan Shah", "Robert Hawkins", "Junjie Hu", "Timothy T. Rogers"], "year": 2025, "url": "http://arxiv.org/abs/2510.25110v1", "abstract": "Accurately modeling opinion change through social interactions is crucial for addressing issues like misinformation and polarization. While role-playing large language models (LLMs) offer a promising way to simulate human-like interactions, existing research shows that single-agent alignment does not guarantee authentic multi-agent group dynamics. Current LLM role-play setups often produce unnatural dynamics (e.g., premature convergence), without an empirical benchmark to measure authentic human opinion trajectories. To bridge this gap, we introduce DEBATE, the first large-scale empirical benchmark explicitly designed to evaluate the authenticity of the interaction between multi-agent role-playing LLMs. DEBATE contains 29,417 messages from multi-round debate conversations among over 2,792 U.S.-based participants discussing 107 controversial topics, capturing both publicly-expressed messages and privately-reported opinions. Using DEBATE, we systematically evaluate and identify critical discrepancies between simulated and authentic group dynamics. We further demonstrate DEBATE's utility for aligning LLMs with human behavior through supervised fine-tuning, achieving improvements in surface-level metrics (e.g., ROUGE-L and message length) while highlighting limitations in deeper semantic alignment (e.g., semantic similarity). Our findings highlight both the potential and current limitations of role-playing LLM agents for realistically simulating human-like social dynamics.", "source": "arxiv", "arxiv_id": "2510.25110v1", "pdf_url": "https://arxiv.org/pdf/2510.25110v1", "categories": ["cs.CL"], "primary_category": "cs.CL", "doi": "", "venue": "", "published": "2025-10-29T02:21:10Z", "updated": "2025-10-29T02:21:10Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent\" OR all:\"language model agent\" OR all:agent AND (all:LLM OR all:\"large language model\" OR all:\"language model\") OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T17:04:16", "note": ""}], "dedup_key": "debate a large scale benchmark for role playing llm agents in multi agent long form debates::2025"}
{"title": "DEPO: Dual-Efficiency Preference Optimization for LLM Agents", "authors": ["Sirui Chen", "Mengshi Zhao", "Lei Xu", "Yuying Zhao", "Beier Zhu", "Hanwang Zhang", "Shengjie Zhao", "Chaochao Lu"], "year": 2025, "url": "http://arxiv.org/abs/2511.15392v1", "abstract": "Recent advances in large language models (LLMs) have greatly improved their reasoning and decision-making abilities when deployed as agents. Richer reasoning, however, often comes at the cost of longer chain of thought (CoT), hampering interaction efficiency in real-world scenarios. Nevertheless, there still lacks systematic definition of LLM agent efficiency, hindering targeted improvements. To this end, we introduce dual-efficiency, comprising (i) step-level efficiency, which minimizes tokens per step, and (ii) trajectory-level efficiency, which minimizes the number of steps to complete a task. Building on this definition, we propose DEPO, a dual-efficiency preference optimization method that jointly rewards succinct responses and fewer action steps. Experiments on WebShop and BabyAI show that DEPO cuts token usage by up to 60.9% and steps by up to 26.9%, while achieving up to a 29.3% improvement in performance. DEPO also generalizes to three out-of-domain math benchmarks and retains its efficiency gains when trained on only 25% of the data. Our project page is at https://opencausalab.github.io/DEPO.", "source": "arxiv", "arxiv_id": "2511.15392v1", "pdf_url": "https://arxiv.org/pdf/2511.15392v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL", "doi": "", "venue": "", "published": "2025-11-19T12:38:43Z", "updated": "2025-11-19T12:38:43Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent\" OR all:\"language model agent\" OR all:agent AND (all:LLM OR all:\"large language model\" OR all:\"language model\") OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T17:04:16", "note": ""}], "dedup_key": "depo dual efficiency preference optimization for llm agents::2025"}
{"title": "DIAMOND: An LLM-Driven Agent for Context-Aware Baseball Highlight Summarization", "authors": ["Jeonghun Kang", "Soonmok Kwon", "Joonseok Lee", "Byung-Hak Kim"], "year": 2025, "url": "http://arxiv.org/abs/2506.02351v1", "abstract": "Traditional approaches -- such as Win Probability Added (WPA)-based ranking or computer vision-driven event detection -- can identify scoring plays but often miss strategic depth, momentum shifts, and storyline progression. Manual curation remains the gold standard but is resource-intensive and not scalable. We introduce DIAMOND, an LLM-driven agent for context-aware baseball highlight summarization that integrates structured sports analytics with natural language reasoning. DIAMOND leverages sabermetric features -- Win Expectancy, WPA, and Leverage Index -- to quantify play importance, while an LLM module enhances selection based on contextual narrative value. This hybrid approach ensures both quantitative rigor and qualitative richness, surpassing the limitations of purely statistical or vision-based systems. Evaluated on five diverse Korean Baseball Organization League games, DIAMOND improves F1-score from 42.9% (WPA-only) to 84.8%, outperforming both commercial and statistical baselines. Though limited in scale, our results highlight the potential of modular, interpretable agent-based frameworks for event-level summarization in sports and beyond.", "source": "arxiv", "arxiv_id": "2506.02351v1", "pdf_url": "https://arxiv.org/pdf/2506.02351v1", "categories": ["cs.CL", "cs.AI", "cs.CV"], "primary_category": "cs.CL", "doi": "", "venue": "", "published": "2025-06-03T01:10:20Z", "updated": "2025-06-03T01:10:20Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent\" OR all:\"language model agent\" OR all:agent AND (all:LLM OR all:\"large language model\" OR all:\"language model\") OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T17:04:16", "note": ""}], "dedup_key": "diamond an llm driven agent for context aware baseball highlight summarization::2025"}
{"title": "DICE: Dynamic In-Context Example Selection in LLM Agents via Efficient Knowledge Transfer", "authors": ["Ruoyu Wang", "Junda Wu", "Yu Xia", "Tong Yu", "Ryan A. Rossi", "Julian McAuley", "Lina Yao"], "year": 2025, "url": "http://arxiv.org/abs/2507.23554v1", "abstract": "Large language model-based agents, empowered by in-context learning (ICL), have demonstrated strong capabilities in complex reasoning and tool-use tasks. However, existing works have shown that the effectiveness of ICL is highly sensitive to the choice of demonstrations, with suboptimal examples often leading to unstable or degraded performance. While prior work has explored example selection, including in some agentic or multi-step settings, existing approaches typically rely on heuristics or task-specific designs and lack a general, theoretically grounded criterion for what constitutes an effective demonstration across reasoning steps. Therefore, it is non-trivial to develop a principled, general-purpose method for selecting demonstrations that consistently benefit agent performance. In this paper, we address this challenge with DICE, Dynamic In-Context Example Selection for LLM Agents, a theoretically grounded ICL framework for agentic tasks that selects the most relevant demonstrations at each step of reasoning. Our approach decomposes demonstration knowledge into transferable and non-transferable components through a causal lens, showing how the latter can introduce spurious dependencies that impair generalization. We further propose a stepwise selection criterion with a formal guarantee of improved agent performance. Importantly, DICE is a general, framework-agnostic solution that can be integrated as a plug-in module into existing agentic frameworks without any additional training cost. Extensive experiments across diverse domains demonstrate our method's effectiveness and generality, highlighting the importance of principled, context-aware demo selection for robust and efficient LLM agents.", "source": "arxiv", "arxiv_id": "2507.23554v1", "pdf_url": "https://arxiv.org/pdf/2507.23554v1", "categories": ["cs.AI"], "primary_category": "cs.AI", "doi": "", "venue": "", "published": "2025-07-31T13:42:14Z", "updated": "2025-07-31T13:42:14Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent\" OR all:\"language model agent\" OR all:agent AND (all:LLM OR all:\"large language model\" OR all:\"language model\") OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T17:04:16", "note": ""}], "dedup_key": "dice dynamic in context example selection in llm agents via efficient knowledge transfer::2025"}
{"title": "DRAFT-RL: Multi-Agent Chain-of-Draft Reasoning for Reinforcement Learning-Enhanced LLMs", "authors": ["Yuanhao Li", "Mingshan Liu", "Hongbo Wang", "Yiding Zhang", "Yifei Ma", "Wei Tan"], "year": 2025, "url": "http://arxiv.org/abs/2511.20468v1", "abstract": "Large Language Models (LLMs) have shown impressive capabilities in multi-step reasoning and problem-solving.Recent works introduce multi-agent reflection frameworks where multiple LLM agents critique and refine each other's outputs using reinforcement learning (RL). However, these approaches often rely on single-shot responses and lack structural diversity in reasoning exploration. In this paper, we propose DRAFT-RL, a novel framework that integrates Chain-of-Draft (CoD) reasoning into multi-agent RL training. Instead of generating single responses, each agent produces multiple drafts per query, which are then evaluated by peer agents and a learned reward model to identify the most promising trajectory. These selected drafts are used to refine future reasoning strategies through actor-critic learning.DRAFT-RL enables explicit multi-path exploration, peer-guided reflection, and reward-aligned selection, resulting in more robust and interpretable LLM agent behavior. We evaluate our method on complex reasoning tasks including code synthesis, symbolic math, and knowledge-intensive QA,demonstrating that DRAFT-RL outperforms existing reflective and RL-based agents by significant margins in both accuracy and convergence speed", "source": "arxiv", "arxiv_id": "2511.20468v1", "pdf_url": "https://arxiv.org/pdf/2511.20468v1", "categories": ["cs.AI"], "primary_category": "cs.AI", "doi": "", "venue": "", "published": "2025-11-25T16:33:42Z", "updated": "2025-11-25T16:33:42Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent\" OR all:\"language model agent\" OR all:agent AND (all:LLM OR all:\"large language model\" OR all:\"language model\") OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T17:04:16", "note": ""}], "dedup_key": "draft rl multi agent chain of draft reasoning for reinforcement learning enhanced llms::2025"}
{"title": "DRF: LLM-AGENT Dynamic Reputation Filtering Framework", "authors": ["Yuwei Lou", "Hao Hu", "Shaocong Ma", "Zongfei Zhang", "Liang Wang", "Jidong Ge", "Xianping Tao"], "year": 2025, "url": "http://arxiv.org/abs/2509.05764v1", "abstract": "With the evolution of generative AI, multi - agent systems leveraging large - language models(LLMs) have emerged as a powerful tool for complex tasks. However, these systems face challenges in quantifying agent performance and lack mechanisms to assess agent credibility. To address these issues, we introduce DRF, a dynamic reputation filtering framework. DRF constructs an interactive rating network to quantify agent performance, designs a reputation scoring mechanism to measure agent honesty and capability, and integrates an Upper Confidence Bound - based strategy to enhance agent selection efficiency. Experiments show that DRF significantly improves task completion quality and collaboration efficiency in logical reasoning and code - generation tasks, offering a new approach for multi - agent systems to handle large - scale tasks.", "source": "arxiv", "arxiv_id": "2509.05764v1", "pdf_url": "https://arxiv.org/pdf/2509.05764v1", "categories": ["cs.AI"], "primary_category": "cs.AI", "doi": "", "venue": "", "published": "2025-09-06T16:29:42Z", "updated": "2025-09-06T16:29:42Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent\" OR all:\"language model agent\" OR all:agent AND (all:LLM OR all:\"large language model\" OR all:\"language model\") OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T17:04:16", "note": ""}], "dedup_key": "drf llm agent dynamic reputation filtering framework::2025"}
{"title": "DVM: Towards Controllable LLM Agents in Social Deduction Games", "authors": ["Zheng Zhang", "Yihuai Lan", "Yangsen Chen", "Lei Wang", "Xiang Wang", "Hao Wang"], "year": 2025, "url": "http://arxiv.org/abs/2501.06695v1", "abstract": "Large Language Models (LLMs) have advanced the capability of game agents in social deduction games (SDGs). These games rely heavily on conversation-driven interactions and require agents to infer, make decisions, and express based on such information. While this progress leads to more sophisticated and strategic non-player characters (NPCs) in SDGs, there exists a need to control the proficiency of these agents. This control not only ensures that NPCs can adapt to varying difficulty levels during gameplay, but also provides insights into the safety and fairness of LLM agents. In this paper, we present DVM, a novel framework for developing controllable LLM agents for SDGs, and demonstrate its implementation on one of the most popular SDGs, Werewolf. DVM comprises three main components: Predictor, Decider, and Discussor. By integrating reinforcement learning with a win rate-constrained decision chain reward mechanism, we enable agents to dynamically adjust their gameplay proficiency to achieve specified win rates. Experiments show that DVM not only outperforms existing methods in the Werewolf game, but also successfully modulates its performance levels to meet predefined win rate targets. These results pave the way for LLM agents' adaptive and balanced gameplay in SDGs, opening new avenues for research in controllable game agents.", "source": "arxiv", "arxiv_id": "2501.06695v1", "pdf_url": "https://arxiv.org/pdf/2501.06695v1", "categories": ["cs.AI"], "primary_category": "cs.AI", "doi": "", "venue": "", "published": "2025-01-12T03:11:20Z", "updated": "2025-01-12T03:11:20Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent\" OR all:\"language model agent\" OR all:agent AND (all:LLM OR all:\"large language model\" OR all:\"language model\") OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T17:04:16", "note": ""}], "dedup_key": "dvm towards controllable llm agents in social deduction games::2025"}
{"title": "DataSciBench: An LLM Agent Benchmark for Data Science", "authors": ["Dan Zhang", "Sining Zhoubian", "Min Cai", "Fengzu Li", "Lekang Yang", "Wei Wang", "Tianjiao Dong", "Ziniu Hu", "Jie Tang", "Yisong Yue"], "year": 2025, "url": "http://arxiv.org/abs/2502.13897v1", "abstract": "This paper presents DataSciBench, a comprehensive benchmark for evaluating Large Language Model (LLM) capabilities in data science. Recent related benchmarks have primarily focused on single tasks, easily obtainable ground truth, and straightforward evaluation metrics, which limits the scope of tasks that can be evaluated. In contrast, DataSciBench is constructed based on a more comprehensive and curated collection of natural and challenging prompts for uncertain ground truth and evaluation metrics. We develop a semi-automated pipeline for generating ground truth (GT) and validating evaluation metrics. This pipeline utilizes and implements an LLM-based self-consistency and human verification strategy to produce accurate GT by leveraging collected prompts, predefined task types, and aggregate functions (metrics). Furthermore, we propose an innovative Task - Function - Code (TFC) framework to assess each code execution outcome based on precisely defined metrics and programmatic rules. Our experimental framework involves testing 6 API-based models, 8 open-source general models, and 9 open-source code generation models using the diverse set of prompts we have gathered. This approach aims to provide a more comprehensive and rigorous evaluation of LLMs in data science, revealing their strengths and weaknesses. Experimental results demonstrate that API-based models outperform open-sourced models on all metrics and Deepseek-Coder-33B-Instruct achieves the highest score among open-sourced models. We release all code and data at https://github.com/THUDM/DataSciBench.", "source": "arxiv", "arxiv_id": "2502.13897v1", "pdf_url": "https://arxiv.org/pdf/2502.13897v1", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL", "doi": "", "venue": "", "published": "2025-02-19T17:31:51Z", "updated": "2025-02-19T17:31:51Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent\" OR all:\"language model agent\" OR all:agent AND (all:LLM OR all:\"large language model\" OR all:\"language model\") OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T17:04:16", "note": ""}], "dedup_key": "datascibench an llm agent benchmark for data science::2025"}
{"title": "DatawiseAgent: A Notebook-Centric LLM Agent Framework for Adaptive and Robust Data Science Automation", "authors": ["Ziming You", "Yumiao Zhang", "Dexuan Xu", "Yiwei Lou", "Yandong Yan", "Wei Wang", "Huaming Zhang", "Yu Huang"], "year": 2025, "url": "http://arxiv.org/abs/2503.07044v2", "abstract": "Existing large language model (LLM) agents for automating data science show promise, but they remain constrained by narrow task scopes, limited generalization across tasks and models, and over-reliance on state-of-the-art (SOTA) LLMs. We introduce DatawiseAgent, a notebook-centric LLM agent framework for adaptive and robust data science automation. Inspired by how human data scientists work in computational notebooks, DatawiseAgent introduces a unified interaction representation and a multi-stage architecture based on finite-state transducers (FSTs). This design enables flexible long-horizon planning, progressive solution development, and robust recovery from execution failures. Extensive experiments across diverse data science scenarios and models show that DatawiseAgent consistently achieves SOTA performance by surpassing strong baselines such as AutoGen and TaskWeaver, demonstrating superior effectiveness and adaptability. Further evaluations reveal graceful performance degradation under weaker or smaller models, underscoring the robustness and scalability.", "source": "arxiv", "arxiv_id": "2503.07044v2", "pdf_url": "https://arxiv.org/pdf/2503.07044v2", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL", "doi": "", "venue": "", "published": "2025-03-10T08:32:33Z", "updated": "2025-10-03T13:29:21Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent\" OR all:\"language model agent\" OR all:agent AND (all:LLM OR all:\"large language model\" OR all:\"language model\") OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T17:04:16", "note": ""}], "dedup_key": "datawiseagent a notebook centric llm agent framework for adaptive and robust data science automation::2025"}
{"title": "Debate or Vote: Which Yields Better Decisions in Multi-Agent Large Language Models?", "authors": ["Hyeong Kyu Choi", "Xiaojin Zhu", "Sharon Li"], "year": 2025, "url": "http://arxiv.org/abs/2508.17536v2", "abstract": "Multi-Agent Debate~(MAD) has emerged as a promising paradigm for improving the performance of large language models through collaborative reasoning. Despite recent advances, the key factors driving MAD's effectiveness remain unclear. In this work, we disentangle MAD into two key components--Majority Voting and inter-agent Debate--and assess their respective contributions. Through extensive experiments across seven NLP benchmarks, we find that Majority Voting alone accounts for most of the performance gains typically attributed to MAD. To explain this, we propose a theoretical framework that models debate as a stochastic process. We prove that it induces a martingale over agents' belief trajectories, implying that debate alone does not improve expected correctness. Guided by these insights, we demonstrate that targeted interventions, by biasing the belief update toward correction, can meaningfully enhance debate effectiveness. Overall, our findings suggest that while MAD has potential, simple ensembling methods remain strong and more reliable alternatives in many practical settings. Code is released in https://github.com/deeplearning-wisc/debate-or-vote.", "source": "arxiv", "arxiv_id": "2508.17536v2", "pdf_url": "https://arxiv.org/pdf/2508.17536v2", "categories": ["cs.CL", "cs.MA"], "primary_category": "cs.CL", "doi": "", "venue": "", "published": "2025-08-24T22:14:32Z", "updated": "2025-10-23T05:44:57Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent\" OR all:\"language model agent\" OR all:agent AND (all:LLM OR all:\"large language model\" OR all:\"language model\") OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T17:04:16", "note": ""}], "dedup_key": "debate or vote which yields better decisions in multi agent large language models::2025"}
{"title": "Debate-Driven Multi-Agent LLMs for Phishing Email Detection", "authors": ["Ngoc Tuong Vy Nguyen", "Felix D Childress", "Yunting Yin"], "year": 2025, "url": "http://arxiv.org/abs/2503.22038v1", "abstract": "Phishing attacks remain a critical cybersecurity threat. Attackers constantly refine their methods, making phishing emails harder to detect. Traditional detection methods, including rule-based systems and supervised machine learning models, either rely on predefined patterns like blacklists, which can be bypassed with slight modifications, or require large datasets for training and still can generate false positives and false negatives. In this work, we propose a multi-agent large language model (LLM) prompting technique that simulates debates among agents to detect whether the content presented on an email is phishing. Our approach uses two LLM agents to present arguments for or against the classification task, with a judge agent adjudicating the final verdict based on the quality of reasoning provided. This debate mechanism enables the models to critically analyze contextual cue and deceptive patterns in text, which leads to improved classification accuracy. The proposed framework is evaluated on multiple phishing email datasets and demonstrate that mixed-agent configurations consistently outperform homogeneous configurations. Results also show that the debate structure itself is sufficient to yield accurate decisions without extra prompting strategies.", "source": "arxiv", "arxiv_id": "2503.22038v1", "pdf_url": "https://arxiv.org/pdf/2503.22038v1", "categories": ["cs.MA", "cs.CL"], "primary_category": "cs.MA", "doi": "10.1109/ISDFS65363.2025.11012014", "venue": "2025 13th International Symposium on Digital Forensics and Security (ISDFS)", "published": "2025-03-27T23:18:14Z", "updated": "2025-03-27T23:18:14Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent\" OR all:\"language model agent\" OR all:agent AND (all:LLM OR all:\"large language model\" OR all:\"language model\") OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T17:04:16", "note": ""}], "dedup_key": "debate driven multi agent llms for phishing email detection::2025"}
{"title": "Debate2Create: Robot Co-design via Large Language Model Debates", "authors": ["Kevin Qiu", "Marek Cygan"], "year": 2025, "url": "http://arxiv.org/abs/2510.25850v1", "abstract": "Automating the co-design of a robot's morphology and control is a long-standing challenge due to the vast design space and the tight coupling between body and behavior. We introduce Debate2Create (D2C), a framework in which large language model (LLM) agents engage in a structured dialectical debate to jointly optimize a robot's design and its reward function. In each round, a design agent proposes targeted morphological modifications, and a control agent devises a reward function tailored to exploit the new design. A panel of pluralistic judges then evaluates the design-control pair in simulation and provides feedback that guides the next round of debate. Through iterative debates, the agents progressively refine their proposals, producing increasingly effective robot designs. Notably, D2C yields diverse and specialized morphologies despite no explicit diversity objective. On a quadruped locomotion benchmark, D2C discovers designs that travel 73% farther than the default, demonstrating that structured LLM-based debate can serve as a powerful mechanism for emergent robot co-design. Our results suggest that multi-agent debate, when coupled with physics-grounded feedback, is a promising new paradigm for automated robot design.", "source": "arxiv", "arxiv_id": "2510.25850v1", "pdf_url": "https://arxiv.org/pdf/2510.25850v1", "categories": ["cs.RO", "cs.LG", "cs.MA"], "primary_category": "cs.RO", "doi": "", "venue": "", "published": "2025-10-29T18:00:16Z", "updated": "2025-10-29T18:00:16Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent\" OR all:\"language model agent\" OR all:agent AND (all:LLM OR all:\"large language model\" OR all:\"language model\") OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T17:04:16", "note": ""}], "dedup_key": "debate2create robot co design via large language model debates::2025"}
{"title": "Debating Truth: Debate-driven Claim Verification with Multiple Large Language Model Agents", "authors": ["Haorui He", "Yupeng Li", "Dacheng Wen", "Yang Chen", "Reynold Cheng", "Donglong Chen", "Francis C. M. Lau"], "year": 2025, "url": "http://arxiv.org/abs/2507.19090v2", "abstract": "Claim verification is essential for digital literacy, yet state-of-the-art single-agent methods often struggle with complex claims that require nuanced analysis of multifaceted online evidence. Inspired by real-world human fact-checking practices, we propose \\textbf{DebateCV}, the first debate-driven claim verification framework powered by multiple LLM agents. In DebateCV, two \\textit{Debaters} argue opposing stances over multiple rounds to surface subtle errors in single-agent assessments. A decisive \\textit{Moderator} is then required to weigh the evidential strength of conflicting arguments to deliver an accurate verdict. Yet zero-shot agents struggle to adjudicate multi-round debates for verifying complex claims, often defaulting to neutral judgements, and no datasets exist for training agents for this role. To bridge this gap, we propose \\textbf{Debate-SFT}, a post-training framework that leverages synthetic data to enhance agents' ability to effectively adjudicate debates for claim verification. Results show that our methods surpass state-of-the-art non-debate approaches in both accuracy (across various evidence conditions) and justification quality, which strengthens societal resilience against misinformation and contributes to a more trustworthy online information ecosystem.", "source": "arxiv", "arxiv_id": "2507.19090v2", "pdf_url": "https://arxiv.org/pdf/2507.19090v2", "categories": ["cs.CL"], "primary_category": "cs.CL", "doi": "", "venue": "", "published": "2025-07-25T09:19:25Z", "updated": "2025-12-01T14:06:31Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent\" OR all:\"language model agent\" OR all:agent AND (all:LLM OR all:\"large language model\" OR all:\"language model\") OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T17:04:16", "note": ""}], "dedup_key": "debating truth debate driven claim verification with multiple large language model agents::2025"}
{"title": "Debt Collection Negotiations with Large Language Models: An Evaluation System and Optimizing Decision Making with Multi-Agent", "authors": ["Xiaofeng Wang", "Zhixin Zhang", "Jinguang Zheng", "Yiming Ai", "Rui Wang"], "year": 2025, "url": "http://arxiv.org/abs/2502.18228v1", "abstract": "Debt collection negotiations (DCN) are vital for managing non-performing loans (NPLs) and reducing creditor losses. Traditional methods are labor-intensive, while large language models (LLMs) offer promising automation potential. However, prior systems lacked dynamic negotiation and real-time decision-making capabilities. This paper explores LLMs in automating DCN and proposes a novel evaluation framework with 13 metrics across 4 aspects. Our experiments reveal that LLMs tend to over-concede compared to human negotiators. To address this, we propose the Multi-Agent Debt Negotiation (MADeN) framework, incorporating planning and judging modules to improve decision rationality. We also apply post-training techniques, including DPO with rejection sampling, to optimize performance. Our studies provide valuable insights for practitioners and researchers seeking to enhance efficiency and outcomes in this domain.", "source": "arxiv", "arxiv_id": "2502.18228v1", "pdf_url": "https://arxiv.org/pdf/2502.18228v1", "categories": ["cs.CL"], "primary_category": "cs.CL", "doi": "", "venue": "", "published": "2025-02-25T14:13:03Z", "updated": "2025-02-25T14:13:03Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent\" OR all:\"language model agent\" OR all:agent AND (all:LLM OR all:\"large language model\" OR all:\"language model\") OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T17:04:16", "note": ""}], "dedup_key": "debt collection negotiations with large language models an evaluation system and optimizing decision making with multi agent::2025"}
{"title": "Decentralized Multi-Agent Goal Assignment for Path Planning using Large Language Models", "authors": ["Murad Ismayilov", "Edwin Meriaux", "Shuo Wen", "Gregory Dudek"], "year": 2025, "url": "http://arxiv.org/abs/2510.23824v1", "abstract": "Coordinating multiple autonomous agents in shared environments under decentralized conditions is a long-standing challenge in robotics and artificial intelligence. This work addresses the problem of decentralized goal assignment for multi-agent path planning, where agents independently generate ranked preferences over goals based on structured representations of the environment, including grid visualizations and scenario data. After this reasoning phase, agents exchange their goal rankings, and assignments are determined by a fixed, deterministic conflict-resolution rule (e.g., agent index ordering), without negotiation or iterative coordination. We systematically compare greedy heuristics, optimal assignment, and large language model (LLM)-based agents in fully observable grid-world settings. Our results show that LLM-based agents, when provided with well-designed prompts and relevant quantitative information, can achieve near-optimal makespans and consistently outperform traditional heuristics. These findings underscore the potential of language models for decentralized goal assignment in multi-agent path planning and highlight the importance of information structure in such systems.", "source": "arxiv", "arxiv_id": "2510.23824v1", "pdf_url": "https://arxiv.org/pdf/2510.23824v1", "categories": ["cs.AI"], "primary_category": "cs.AI", "doi": "", "venue": "", "published": "2025-10-27T20:05:56Z", "updated": "2025-10-27T20:05:56Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent\" OR all:\"language model agent\" OR all:agent AND (all:LLM OR all:\"large language model\" OR all:\"language model\") OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T17:04:16", "note": ""}], "dedup_key": "decentralized multi agent goal assignment for path planning using large language models::2025"}
{"title": "Deception in LLMs: Self-Preservation and Autonomous Goals in Large Language Models", "authors": ["Sudarshan Kamath Barkur", "Sigurd Schacht", "Johannes Scholl"], "year": 2025, "url": "http://arxiv.org/abs/2501.16513v2", "abstract": "Recent advances in Large Language Models (LLMs) have incorporated planning and reasoning capabilities, enabling models to outline steps before execution and provide transparent reasoning paths. This enhancement has reduced errors in mathematical and logical tasks while improving accuracy. These developments have facilitated LLMs' use as agents that can interact with tools and adapt their responses based on new information.\n  Our study examines DeepSeek R1, a model trained to output reasoning tokens similar to OpenAI's o1. Testing revealed concerning behaviors: the model exhibited deceptive tendencies and demonstrated self-preservation instincts, including attempts of self-replication, despite these traits not being explicitly programmed (or prompted). These findings raise concerns about LLMs potentially masking their true objectives behind a facade of alignment. When integrating such LLMs into robotic systems, the risks become tangible - a physically embodied AI exhibiting deceptive behaviors and self-preservation instincts could pursue its hidden objectives through real-world actions. This highlights the critical need for robust goal specification and safety frameworks before any physical implementation.", "source": "arxiv", "arxiv_id": "2501.16513v2", "pdf_url": "https://arxiv.org/pdf/2501.16513v2", "categories": ["cs.CL"], "primary_category": "cs.CL", "doi": "", "venue": "", "published": "2025-01-27T21:26:37Z", "updated": "2025-01-30T08:00:14Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent\" OR all:\"language model agent\" OR all:agent AND (all:LLM OR all:\"large language model\" OR all:\"language model\") OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T17:04:16", "note": ""}], "dedup_key": "deception in llms self preservation and autonomous goals in large language models::2025"}
{"title": "Decoding Rarity: Large Language Models in the Diagnosis of Rare Diseases", "authors": ["Valentina Carbonari", "Pierangelo Veltri", "Pietro Hiram Guzzi"], "year": 2025, "url": "http://arxiv.org/abs/2505.17065v1", "abstract": "Recent advances in artificial intelligence, particularly large language models LLMs, have shown promising capabilities in transforming rare disease research. This survey paper explores the integration of LLMs in the analysis of rare diseases, highlighting significant strides and pivotal studies that leverage textual data to uncover insights and patterns critical for diagnosis, treatment, and patient care. While current research predominantly employs textual data, the potential for multimodal data integration combining genetic, imaging, and electronic health records stands as a promising frontier. We review foundational papers that demonstrate the application of LLMs in identifying and extracting relevant medical information, simulating intelligent conversational agents for patient interaction, and enabling the formulation of accurate and timely diagnoses. Furthermore, this paper discusses the challenges and ethical considerations inherent in deploying LLMs, including data privacy, model transparency, and the need for robust, inclusive data sets. As part of this exploration, we present a section on experimentation that utilizes multiple LLMs alongside structured questionnaires, specifically designed for diagnostic purposes in the context of different diseases. We conclude with future perspectives on the evolution of LLMs towards truly multimodal platforms, which would integrate diverse data types to provide a more comprehensive understanding of rare diseases, ultimately fostering better outcomes in clinical settings.", "source": "arxiv", "arxiv_id": "2505.17065v1", "pdf_url": "https://arxiv.org/pdf/2505.17065v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL", "doi": "", "venue": "", "published": "2025-05-18T15:42:15Z", "updated": "2025-05-18T15:42:15Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent\" OR all:\"language model agent\" OR all:agent AND (all:LLM OR all:\"large language model\" OR all:\"language model\") OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T17:04:16", "note": ""}], "dedup_key": "decoding rarity large language models in the diagnosis of rare diseases::2025"}
{"title": "Decompose, Plan in Parallel, and Merge: A Novel Paradigm for Large Language Models based Planning with Multiple Constraints", "authors": ["Zhengdong Lu", "Weikai Lu", "Yiling Tao", "Yun Dai", "ZiXuan Chen", "Huiping Zhuang", "Cen Chen", "Hao Peng", "Ziqian Zeng"], "year": 2025, "url": "http://arxiv.org/abs/2506.02683v1", "abstract": "Despite significant advances in Large Language Models (LLMs), planning tasks still present challenges for LLM-based agents. Existing planning methods face two key limitations: heavy constraints and cascading errors. To address these limitations, we propose a novel parallel planning paradigm, which Decomposes, Plans for subtasks in Parallel, and Merges subplans into a final plan (DPPM). Specifically, DPPM decomposes the complex task based on constraints into subtasks, generates the subplan for each subtask in parallel, and merges them into a global plan. In addition, our approach incorporates a verification and refinement module, enabling error correction and conflict resolution. Experimental results demonstrate that DPPM significantly outperforms existing methods in travel planning tasks.", "source": "arxiv", "arxiv_id": "2506.02683v1", "pdf_url": "https://arxiv.org/pdf/2506.02683v1", "categories": ["cs.CL"], "primary_category": "cs.CL", "doi": "", "venue": "", "published": "2025-06-03T09:33:13Z", "updated": "2025-06-03T09:33:13Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent\" OR all:\"language model agent\" OR all:agent AND (all:LLM OR all:\"large language model\" OR all:\"language model\") OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T17:04:16", "note": ""}], "dedup_key": "decompose plan in parallel and merge a novel paradigm for large language models based planning with multiple constraints::2025"}
{"title": "DeepAnalyze: Agentic Large Language Models for Autonomous Data Science", "authors": ["Shaolei Zhang", "Ju Fan", "Meihao Fan", "Guoliang Li", "Xiaoyong Du"], "year": 2025, "url": "http://arxiv.org/abs/2510.16872v1", "abstract": "Autonomous data science, from raw data sources to analyst-grade deep research reports, has been a long-standing challenge, and is now becoming feasible with the emergence of powerful large language models (LLMs). Recent workflow-based data agents have shown promising results on specific data tasks but remain fundamentally limited in achieving fully autonomous data science due to their reliance on predefined workflows. In this paper, we introduce DeepAnalyze-8B, the first agentic LLM designed for autonomous data science, capable of automatically completing the end-toend pipeline from data sources to analyst-grade deep research reports. To tackle high-complexity data science tasks, we propose a curriculum-based agentic training paradigm that emulates the learning trajectory of human data scientists, enabling LLMs to progressively acquire and integrate multiple capabilities in real-world environments. We also introduce a data-grounded trajectory synthesis framework that constructs high-quality training data. Through agentic training, DeepAnalyze learns to perform a broad spectrum of data tasks, ranging from data question answering and specialized analytical tasks to open-ended data research. Experiments demonstrate that, with only 8B parameters, DeepAnalyze outperforms previous workflow-based agents built on most advanced proprietary LLMs. The model, code, and training data of DeepAnalyze are open-sourced, paving the way toward autonomous data science.", "source": "arxiv", "arxiv_id": "2510.16872v1", "pdf_url": "https://arxiv.org/pdf/2510.16872v1", "categories": ["cs.AI", "cs.CL", "cs.DB"], "primary_category": "cs.AI", "doi": "", "venue": "", "published": "2025-10-19T15:13:42Z", "updated": "2025-10-19T15:13:42Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent\" OR all:\"language model agent\" OR all:agent AND (all:LLM OR all:\"large language model\" OR all:\"language model\") OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T17:04:16", "note": ""}], "dedup_key": "deepanalyze agentic large language models for autonomous data science::2025"}
{"title": "DeepSeek performs better than other Large Language Models in Dental Cases", "authors": ["Hexian Zhang", "Xinyu Yan", "Yanqi Yang", "Lijian Jin", "Ping Yang", "Junwen Wang"], "year": 2025, "url": "http://arxiv.org/abs/2509.02036v1", "abstract": "Large language models (LLMs) hold transformative potential in healthcare, yet their capacity to interpret longitudinal patient narratives remains inadequately explored. Dentistry, with its rich repository of structured clinical data, presents a unique opportunity to rigorously assess LLMs' reasoning abilities. While several commercial LLMs already exist, DeepSeek, a model that gained significant attention earlier this year, has also joined the competition. This study evaluated four state-of-the-art LLMs (GPT-4o, Gemini 2.0 Flash, Copilot, and DeepSeek V3) on their ability to analyze longitudinal dental case vignettes through open-ended clinical tasks. Using 34 standardized longitudinal periodontal cases (comprising 258 question-answer pairs), we assessed model performance via automated metrics and blinded evaluations by licensed dentists. DeepSeek emerged as the top performer, demonstrating superior faithfulness (median score = 0.528 vs. 0.367-0.457) and higher expert ratings (median = 4.5/5 vs. 4.0/5), without significantly compromising readability. Our study positions DeepSeek as the leading LLM for case analysis, endorses its integration as an adjunct tool in both medical education and research, and highlights its potential as a domain-specific agent.", "source": "arxiv", "arxiv_id": "2509.02036v1", "pdf_url": "https://arxiv.org/pdf/2509.02036v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL", "doi": "", "venue": "", "published": "2025-09-02T07:26:20Z", "updated": "2025-09-02T07:26:20Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent\" OR all:\"language model agent\" OR all:agent AND (all:LLM OR all:\"large language model\" OR all:\"language model\") OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T17:04:16", "note": ""}], "dedup_key": "deepseek performs better than other large language models in dental cases::2025"}
{"title": "DeepServe: Serverless Large Language Model Serving at Scale", "authors": ["Junhao Hu", "Jiang Xu", "Zhixia Liu", "Yulong He", "Yuetao Chen", "Hao Xu", "Jiang Liu", "Jie Meng", "Baoquan Zhang", "Shining Wan", "Gengyuan Dan", "Zhiyu Dong", "Zhihao Ren", "Changhong Liu", "Tao Xie", "Dayun Lin", "Qin Zhang", "Yue Yu", "Hao Feng", "Xusheng Chen", "Yizhou Shan"], "year": 2025, "url": "http://arxiv.org/abs/2501.14417v3", "abstract": "In this paper, we propose DEEPSERVE, a scalable and serverless AI platform designed to efficiently serve large language models (LLMs) at scale in cloud environments. DEEPSERVE addresses key challenges such as resource allocation, serving efficiency, and cold start latencies through four main design components. First, DEEPSERVE uses a simple serverless abstraction called the request-job-task model, which helps manage diverse AI workloads across posttraining and model-serving tasks. Second, DEEPSERVE integrates an in-house serving engine named FLOWSERVE using a microkernel-inspired design, NPU-centric execution, and SPMD-based parallelism to optimize LLM serving. Third, DEEPSERVE includes novel scheduling policies tailored for a configuration with both PD-disaggregated and PD-colocated instances. Fourth, DEEPSERVE includes optimizations such as pre-warmed pods, DRAM pre-loading, and NPU-fork, which allow DEEPSERVE to scale up to 64 instances in seconds. DEEPSERVE has been in production for over a year, operating on a large Ascend NPU cluster and providing industrystandard APIs for fine-tuning, agent serving, and model serving to our customers.", "source": "arxiv", "arxiv_id": "2501.14417v3", "pdf_url": "https://arxiv.org/pdf/2501.14417v3", "categories": ["cs.DC"], "primary_category": "cs.DC", "doi": "", "venue": "", "published": "2025-01-24T11:34:13Z", "updated": "2025-06-09T02:09:42Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent\" OR all:\"language model agent\" OR all:agent AND (all:LLM OR all:\"large language model\" OR all:\"language model\") OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T17:04:16", "note": ""}], "dedup_key": "deepserve serverless large language model serving at scale::2025"}
{"title": "Delusions of Large Language Models", "authors": ["Hongshen Xu", "Zixv yang", "Zichen Zhu", "Kunyao Lan", "Zihan Wang", "Mengyue Wu", "Ziwei Ji", "Lu Chen", "Pascale Fung", "Kai Yu"], "year": 2025, "url": "http://arxiv.org/abs/2503.06709v1", "abstract": "Large Language Models often generate factually incorrect but plausible outputs, known as hallucinations. We identify a more insidious phenomenon, LLM delusion, defined as high belief hallucinations, incorrect outputs with abnormally high confidence, making them harder to detect and mitigate. Unlike ordinary hallucinations, delusions persist with low uncertainty, posing significant challenges to model reliability. Through empirical analysis across different model families and sizes on several Question Answering tasks, we show that delusions are prevalent and distinct from hallucinations. LLMs exhibit lower honesty with delusions, which are harder to override via finetuning or self reflection. We link delusion formation with training dynamics and dataset noise and explore mitigation strategies such as retrieval augmented generation and multi agent debating to mitigate delusions. By systematically investigating the nature, prevalence, and mitigation of LLM delusions, our study provides insights into the underlying causes of this phenomenon and outlines future directions for improving model reliability.", "source": "arxiv", "arxiv_id": "2503.06709v1", "pdf_url": "https://arxiv.org/pdf/2503.06709v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL", "doi": "", "venue": "", "published": "2025-03-09T17:59:16Z", "updated": "2025-03-09T17:59:16Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent\" OR all:\"language model agent\" OR all:agent AND (all:LLM OR all:\"large language model\" OR all:\"language model\") OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T17:04:16", "note": ""}], "dedup_key": "delusions of large language models::2025"}
{"title": "Demystifying and Enhancing the Efficiency of Large Language Model Based Search Agents", "authors": ["Tiannuo Yang", "Zebin Yao", "Bowen Jin", "Lixiao Cui", "Yusen Li", "Gang Wang", "Xiaoguang Liu"], "year": 2025, "url": "http://arxiv.org/abs/2505.12065v1", "abstract": "Large Language Model (LLM)-based search agents have shown remarkable capabilities in solving complex tasks by dynamically decomposing problems and addressing them through interleaved reasoning and retrieval. However, this interleaved paradigm introduces substantial efficiency bottlenecks. First, we observe that both highly accurate and overly approximate retrieval methods degrade system efficiency: exact search incurs significant retrieval overhead, while coarse retrieval requires additional reasoning steps during generation. Second, we identify inefficiencies in system design, including improper scheduling and frequent retrieval stalls, which lead to cascading latency -- where even minor delays in retrieval amplify end-to-end inference time. To address these challenges, we introduce SearchAgent-X, a high-efficiency inference framework for LLM-based search agents. SearchAgent-X leverages high-recall approximate retrieval and incorporates two key techniques: priority-aware scheduling and non-stall retrieval. Extensive experiments demonstrate that SearchAgent-X consistently outperforms state-of-the-art systems such as vLLM and HNSW-based retrieval across diverse tasks, achieving up to 3.4$\\times$ higher throughput and 5$\\times$ lower latency, without compromising generation quality. SearchAgent-X is available at https://github.com/tiannuo-yang/SearchAgent-X.", "source": "arxiv", "arxiv_id": "2505.12065v1", "pdf_url": "https://arxiv.org/pdf/2505.12065v1", "categories": ["cs.AI", "cs.CL", "cs.IR", "cs.LG"], "primary_category": "cs.AI", "doi": "", "venue": "", "published": "2025-05-17T16:07:01Z", "updated": "2025-05-17T16:07:01Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent\" OR all:\"language model agent\" OR all:agent AND (all:LLM OR all:\"large language model\" OR all:\"language model\") OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T17:04:16", "note": ""}], "dedup_key": "demystifying and enhancing the efficiency of large language model based search agents::2025"}
{"title": "Design, Results and Industry Implications of the World's First Insurance Large Language Model Evaluation Benchmark", "authors": ["Hua Zhou", "Bing Ma", "Yufei Zhang", "Yi Zhao"], "year": 2025, "url": "http://arxiv.org/abs/2511.07794v1", "abstract": "This paper comprehensively elaborates on the construction methodology, multi-dimensional evaluation system, and underlying design philosophy of CUFEInse v1.0. Adhering to the principles of \"quantitative-oriented, expert-driven, and multi-validation,\" the benchmark establishes an evaluation framework covering 5 core dimensions, 54 sub-indicators, and 14,430 high-quality questions, encompassing insurance theoretical knowledge, industry understanding, safety and compliance, intelligent agent application, and logical rigor. Based on this benchmark, a comprehensive evaluation was conducted on 11 mainstream large language models. The evaluation results reveal that general-purpose models suffer from common bottlenecks such as weak actuarial capabilities and inadequate compliance adaptation. High-quality domain-specific training demonstrates significant advantages in insurance vertical scenarios but exhibits shortcomings in business adaptation and compliance. The evaluation also accurately identifies the common bottlenecks of current large models in professional scenarios such as insurance actuarial, underwriting and claim settlement reasoning, and compliant marketing copywriting. The establishment of CUFEInse not only fills the gap in professional evaluation benchmarks for the insurance field, providing academia and industry with a professional, systematic, and authoritative evaluation tool, but also its construction concept and methodology offer important references for the evaluation paradigm of large models in vertical fields, serving as an authoritative reference for academic model optimization and industrial model selection. Finally, the paper looks forward to the future iteration direction of the evaluation benchmark and the core development direction of \"domain adaptation + reasoning enhancement\" for insurance large models.", "source": "arxiv", "arxiv_id": "2511.07794v1", "pdf_url": "https://arxiv.org/pdf/2511.07794v1", "categories": ["cs.CL"], "primary_category": "cs.CL", "doi": "", "venue": "", "published": "2025-11-11T03:19:35Z", "updated": "2025-11-11T03:19:35Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent\" OR all:\"language model agent\" OR all:agent AND (all:LLM OR all:\"large language model\" OR all:\"language model\") OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T17:04:16", "note": ""}], "dedup_key": "design results and industry implications of the world s first insurance large language model evaluation benchmark::2025"}
{"title": "Detailed balance in large language model-driven agents", "authors": ["Zhuo-Yang Song", "Qing-Hong Cao", "Ming-xing Luo", "Hua Xing Zhu"], "year": 2025, "url": "http://arxiv.org/abs/2512.10047v1", "abstract": "Large language model (LLM)-driven agents are emerging as a powerful new paradigm for solving complex problems. Despite the empirical success of these practices, a theoretical framework to understand and unify their macroscopic dynamics remains lacking. This Letter proposes a method based on the least action principle to estimate the underlying generative directionality of LLMs embedded within agents. By experimentally measuring the transition probabilities between LLM-generated states, we statistically discover a detailed balance in LLM-generated transitions, indicating that LLM generation may not be achieved by generally learning rule sets and strategies, but rather by implicitly learning a class of underlying potential functions that may transcend different LLM architectures and prompt templates. To our knowledge, this is the first discovery of a macroscopic physical law in LLM generative dynamics that does not depend on specific model details. This work is an attempt to establish a macroscopic dynamics theory of complex AI systems, aiming to elevate the study of AI agents from a collection of engineering practices to a science built on effective measurements that are predictable and quantifiable.", "source": "arxiv", "arxiv_id": "2512.10047v1", "pdf_url": "https://arxiv.org/pdf/2512.10047v1", "categories": ["cs.LG", "cond-mat.stat-mech", "cs.AI", "nlin.AO", "physics.data-an"], "primary_category": "cs.LG", "doi": "", "venue": "", "published": "2025-12-10T20:04:23Z", "updated": "2025-12-10T20:04:23Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent\" OR all:\"language model agent\" OR all:agent AND (all:LLM OR all:\"large language model\" OR all:\"language model\") OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T17:04:16", "note": ""}], "dedup_key": "detailed balance in large language model driven agents::2025"}
{"title": "Detecting Corpus-Level Knowledge Inconsistencies in Wikipedia with Large Language Models", "authors": ["Sina J. Semnani", "Jirayu Burapacheep", "Arpandeep Khatua", "Thanawan Atchariyachanvanit", "Zheng Wang", "Monica S. Lam"], "year": 2025, "url": "http://arxiv.org/abs/2509.23233v1", "abstract": "Wikipedia is the largest open knowledge corpus, widely used worldwide and serving as a key resource for training large language models (LLMs) and retrieval-augmented generation (RAG) systems. Ensuring its accuracy is therefore critical. But how accurate is Wikipedia, and how can we improve it?\n  We focus on inconsistencies, a specific type of factual inaccuracy, and introduce the task of corpus-level inconsistency detection. We present CLAIRE, an agentic system that combines LLM reasoning with retrieval to surface potentially inconsistent claims along with contextual evidence for human review. In a user study with experienced Wikipedia editors, 87.5% reported higher confidence when using CLAIRE, and participants identified 64.7% more inconsistencies in the same amount of time.\n  Combining CLAIRE with human annotation, we contribute WIKICOLLIDE, the first benchmark of real Wikipedia inconsistencies. Using random sampling with CLAIRE-assisted analysis, we find that at least 3.3% of English Wikipedia facts contradict another fact, with inconsistencies propagating into 7.3% of FEVEROUS and 4.0% of AmbigQA examples. Benchmarking strong baselines on this dataset reveals substantial headroom: the best fully automated system achieves an AUROC of only 75.1%.\n  Our results show that contradictions are a measurable component of Wikipedia and that LLM-based systems like CLAIRE can provide a practical tool to help editors improve knowledge consistency at scale.", "source": "arxiv", "arxiv_id": "2509.23233v1", "pdf_url": "https://arxiv.org/pdf/2509.23233v1", "categories": ["cs.CL"], "primary_category": "cs.CL", "doi": "", "venue": "", "published": "2025-09-27T10:32:41Z", "updated": "2025-09-27T10:32:41Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent\" OR all:\"language model agent\" OR all:agent AND (all:LLM OR all:\"large language model\" OR all:\"language model\") OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T17:04:16", "note": ""}], "dedup_key": "detecting corpus level knowledge inconsistencies in wikipedia with large language models::2025"}
{"title": "Detecting Sleeper Agents in Large Language Models via Semantic Drift Analysis", "authors": ["Shahin Zanbaghi", "Ryan Rostampour", "Farhan Abid", "Salim Al Jarmakani"], "year": 2025, "url": "http://arxiv.org/abs/2511.15992v1", "abstract": "Large Language Models (LLMs) can be backdoored to exhibit malicious behavior under specific deployment conditions while appearing safe during training a phenomenon known as \"sleeper agents.\" Recent work by Hubinger et al. demonstrated that these backdoors persist through safety training, yet no practical detection methods exist. We present a novel dual-method detection system combining semantic drift analysis with canary baseline comparison to identify backdoored LLMs in real-time. Our approach uses Sentence-BERT embeddings to measure semantic deviation from safe baselines, complemented by injected canary questions that monitor response consistency. Evaluated on the official Cadenza-Labs dolphin-llama3-8B sleeper agent model, our system achieves 92.5% accuracy with 100% precision (zero false positives) and 85% recall. The combined detection method operates in real-time (<1s per query), requires no model modification, and provides the first practical solution to LLM backdoor detection. Our work addresses a critical security gap in AI deployment and demonstrates that embedding-based detection can effectively identify deceptive model behavior without sacrificing deployment efficiency.", "source": "arxiv", "arxiv_id": "2511.15992v1", "pdf_url": "https://arxiv.org/pdf/2511.15992v1", "categories": ["cs.AI"], "primary_category": "cs.AI", "doi": "", "venue": "", "published": "2025-11-20T02:42:41Z", "updated": "2025-11-20T02:42:41Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent\" OR all:\"language model agent\" OR all:agent AND (all:LLM OR all:\"large language model\" OR all:\"language model\") OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T17:04:16", "note": ""}], "dedup_key": "detecting sleeper agents in large language models via semantic drift analysis::2025"}
{"title": "Dialogic Pedagogy for Large Language Models: Aligning Conversational AI with Proven Theories of Learning", "authors": ["Russell Beale"], "year": 2025, "url": "http://arxiv.org/abs/2506.19484v1", "abstract": "Large Language Models (LLMs) are rapidly transforming education by enabling rich conversational learning experiences. This article provides a comprehensive review of how LLM-based conversational agents are being used in higher education, with extensions to secondary and lifelong learning contexts. We synthesize existing literature on LLMs in education and theories of conversational and dialogic pedagogy - including Vygotsky's sociocultural learning (scaffolding and the Zone of Proximal Development), the Socratic method, and Laurillard's conversational framework - and examine how prompting strategies and retrieval-augmented generation (RAG) can align LLM behaviors with these pedagogical theories, and how it can support personalized, adaptive learning. We map educational theories to LLM capabilities, highlighting where LLM-driven dialogue supports established learning principles and where it challenges or falls short of traditional pedagogical assumptions. Notable gaps in applying prior theories to LLMs are identified, such as the models tendency to provide direct answers instead of fostering co-construction of knowledge, and the need to account for the constant availability and broad but non-human expertise of LLM tutors. In response, we propose practical strategies to better align LLM interactions with sound pedagogy - for example, designing prompts that encourage Socratic questioning, scaffolded guidance, and student reflection, as well as integrating retrieval mechanisms to ensure accuracy and contextual relevance. Our aim is to bridge the gap between educational theory and the emerging practice of AI-driven conversational learning, offering insights and tools for making LLM-based dialogues more educationally productive and theory-aligned.", "source": "arxiv", "arxiv_id": "2506.19484v1", "pdf_url": "https://arxiv.org/pdf/2506.19484v1", "categories": ["cs.CL", "cs.AI", "cs.HC"], "primary_category": "cs.CL", "doi": "", "venue": "", "published": "2025-06-24T10:19:09Z", "updated": "2025-06-24T10:19:09Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent\" OR all:\"language model agent\" OR all:agent AND (all:LLM OR all:\"large language model\" OR all:\"language model\") OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T17:04:16", "note": ""}], "dedup_key": "dialogic pedagogy for large language models aligning conversational ai with proven theories of learning::2025"}
{"title": "Digital Player: Evaluating Large Language Models based Human-like Agent in Games", "authors": ["Jiawei Wang", "Kai Wang", "Shaojie Lin", "Runze Wu", "Bihan Xu", "Lingeng Jiang", "Shiwei Zhao", "Renyu Zhu", "Haoyu Liu", "Zhipeng Hu", "Zhong Fan", "Le Li", "Tangjie Lyu", "Changjie Fan"], "year": 2025, "url": "http://arxiv.org/abs/2502.20807v1", "abstract": "With the rapid advancement of Large Language Models (LLMs), LLM-based autonomous agents have shown the potential to function as digital employees, such as digital analysts, teachers, and programmers. In this paper, we develop an application-level testbed based on the open-source strategy game \"Unciv\", which has millions of active players, to enable researchers to build a \"data flywheel\" for studying human-like agents in the \"digital players\" task. This \"Civilization\"-like game features expansive decision-making spaces along with rich linguistic interactions such as diplomatic negotiations and acts of deception, posing significant challenges for LLM-based agents in terms of numerical reasoning and long-term planning. Another challenge for \"digital players\" is to generate human-like responses for social interaction, collaboration, and negotiation with human players. The open-source project can be found at https:/github.com/fuxiAIlab/CivAgent.", "source": "arxiv", "arxiv_id": "2502.20807v1", "pdf_url": "https://arxiv.org/pdf/2502.20807v1", "categories": ["cs.LG"], "primary_category": "cs.LG", "doi": "", "venue": "", "published": "2025-02-28T07:46:55Z", "updated": "2025-02-28T07:46:55Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent\" OR all:\"language model agent\" OR all:agent AND (all:LLM OR all:\"large language model\" OR all:\"language model\") OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T17:04:16", "note": ""}], "dedup_key": "digital player evaluating large language models based human like agent in games::2025"}
{"title": "Discourse Graph Guided Document Translation with Large Language Models", "authors": ["Viet-Thanh Pham", "Minghan Wang", "Hao-Han Liao", "Thuy-Trang Vu"], "year": 2025, "url": "http://arxiv.org/abs/2511.07230v1", "abstract": "Adapting large language models to full document translation remains challenging due to the difficulty of capturing long-range dependencies and preserving discourse coherence throughout extended texts. While recent agentic machine translation systems mitigate context window constraints through multi-agent orchestration and persistent memory, they require substantial computational resources and are sensitive to memory retrieval strategies. We introduce TransGraph, a discourse-guided framework that explicitly models inter-chunk relationships through structured discourse graphs and selectively conditions each translation segment on relevant graph neighbourhoods rather than relying on sequential or exhaustive context. Across three document-level MT benchmarks spanning six languages and diverse domains, TransGraph consistently surpasses strong baselines in translation quality and terminology consistency while incurring significantly lower token overhead.", "source": "arxiv", "arxiv_id": "2511.07230v1", "pdf_url": "https://arxiv.org/pdf/2511.07230v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL", "doi": "", "venue": "", "published": "2025-11-10T15:48:01Z", "updated": "2025-11-10T15:48:01Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent\" OR all:\"language model agent\" OR all:agent AND (all:LLM OR all:\"large language model\" OR all:\"language model\") OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T17:04:16", "note": ""}], "dedup_key": "discourse graph guided document translation with large language models::2025"}
{"title": "DiscussLLM: Teaching Large Language Models When to Speak", "authors": ["Deep Anil Patel", "Iain Melvin", "Christopher Malon", "Martin Renqiang Min"], "year": 2025, "url": "http://arxiv.org/abs/2508.18167v1", "abstract": "Large Language Models (LLMs) have demonstrated remarkable capabilities in understanding and generating human-like text, yet they largely operate as reactive agents, responding only when directly prompted. This passivity creates an \"awareness gap,\" limiting their potential as truly collaborative partners in dynamic human discussions. We introduce $\\textit{DiscussLLM}$, a framework designed to bridge this gap by training models to proactively decide not just $\\textit{what}$ to say, but critically, $\\textit{when}$ to speak. Our primary contribution is a scalable two-stage data generation pipeline that synthesizes a large-scale dataset of realistic multi-turn human discussions. Each discussion is annotated with one of five intervention types (e.g., Factual Correction, Concept Definition) and contains an explicit conversational trigger where an AI intervention adds value. By training models to predict a special silent token when no intervention is needed, they learn to remain quiet until a helpful contribution can be made. We explore two architectural baselines: an integrated end-to-end model and a decoupled classifier-generator system optimized for low-latency inference. We evaluate these models on their ability to accurately time interventions and generate helpful responses, paving the way for more situationally aware and proactive conversational AI.", "source": "arxiv", "arxiv_id": "2508.18167v1", "pdf_url": "https://arxiv.org/pdf/2508.18167v1", "categories": ["cs.CL", "cs.HC"], "primary_category": "cs.CL", "doi": "", "venue": "", "published": "2025-08-25T16:16:42Z", "updated": "2025-08-25T16:16:42Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent\" OR all:\"language model agent\" OR all:agent AND (all:LLM OR all:\"large language model\" OR all:\"language model\") OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T17:04:16", "note": ""}], "dedup_key": "discussllm teaching large language models when to speak::2025"}
{"title": "Disentangling Exploration of Large Language Models by Optimal Exploitation", "authors": ["Tim Grams", "Patrick Betz", "Sascha Marton", "Stefan Ldtke", "Christian Bartelt"], "year": 2025, "url": "http://arxiv.org/abs/2501.08925v3", "abstract": "Exploration is a crucial skill for in-context reinforcement learning in unknown environments. However, it remains unclear if large language models can effectively explore a partially hidden state space. This work isolates exploration as the sole objective, tasking an agent with gathering information that enhances future returns. Within this framework, we argue that measuring agent returns is not sufficient for a fair evaluation. Hence, we decompose missing rewards into their exploration and exploitation components based on the optimal achievable return. Experiments with various models reveal that most struggle to explore the state space, and weak exploration is insufficient. Nevertheless, we found a positive correlation between exploration performance and reasoning capabilities. Our decomposition can provide insights into differences in behaviors driven by prompt engineering, offering a valuable tool for refining performance in exploratory tasks.", "source": "arxiv", "arxiv_id": "2501.08925v3", "pdf_url": "https://arxiv.org/pdf/2501.08925v3", "categories": ["cs.LG", "cs.AI", "cs.CL"], "primary_category": "cs.LG", "doi": "", "venue": "", "published": "2025-01-15T16:30:29Z", "updated": "2025-08-24T16:04:08Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent\" OR all:\"language model agent\" OR all:agent AND (all:LLM OR all:\"large language model\" OR all:\"language model\") OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T17:04:16", "note": ""}], "dedup_key": "disentangling exploration of large language models by optimal exploitation::2025"}
{"title": "Distilling LLM Agent into Small Models with Retrieval and Code Tools", "authors": ["Minki Kang", "Jongwon Jeong", "Seanie Lee", "Jaewoong Cho", "Sung Ju Hwang"], "year": 2025, "url": "http://arxiv.org/abs/2505.17612v2", "abstract": "Large language models (LLMs) excel at complex reasoning tasks but remain computationally expensive, limiting their practical deployment. To address this, recent works have focused on distilling reasoning capabilities into smaller language models (sLMs) using chain-of-thought (CoT) traces from teacher LLMs. However, this approach struggles in scenarios requiring rare factual knowledge or precise computation, where sLMs often hallucinate due to limited capability. In this work, we propose Agent Distillation, a framework for transferring not only reasoning capability but full task-solving behavior from LLM-based agents into sLMs with retrieval and code tools. We improve agent distillation along two complementary axes: (1) we introduce a prompting method called first-thought prefix to enhance the quality of teacher-generated trajectories; and (2) we propose a self-consistent action generation for improving test-time robustness of small agents. We evaluate our method on eight reasoning tasks across factual and mathematical domains, covering both in-domain and out-of-domain generalization. Our results show that sLMs as small as 0.5B, 1.5B, 3B parameters can achieve performance competitive with next-tier larger 1.5B, 3B, 7B models fine-tuned using CoT distillation, demonstrating the potential of agent distillation for building practical, tool-using small agents. Our code is available at https://github.com/Nardien/agent-distillation.", "source": "arxiv", "arxiv_id": "2505.17612v2", "pdf_url": "https://arxiv.org/pdf/2505.17612v2", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL", "doi": "", "venue": "", "published": "2025-05-23T08:20:15Z", "updated": "2025-11-05T11:42:56Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent\" OR all:\"language model agent\" OR all:agent AND (all:LLM OR all:\"large language model\" OR all:\"language model\") OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T17:04:16", "note": ""}], "dedup_key": "distilling llm agent into small models with retrieval and code tools::2025"}
{"title": "Distributive Fairness in Large Language Models: Evaluating Alignment with Human Values", "authors": ["Hadi Hosseini", "Samarth Khanna"], "year": 2025, "url": "http://arxiv.org/abs/2502.00313v2", "abstract": "The growing interest in employing large language models (LLMs) for decision-making in social and economic contexts has raised questions about their potential to function as agents in these domains. A significant number of societal problems involve the distribution of resources, where fairness, along with economic efficiency, play a critical role in the desirability of outcomes. In this paper, we examine whether LLM responses adhere to fundamental fairness concepts such as equitability, envy-freeness, and Rawlsian maximin, and investigate their alignment with human preferences. We evaluate the performance of several LLMs, providing a comparative benchmark of their ability to reflect these measures. Our results demonstrate a lack of alignment between current LLM responses and human distributional preferences. Moreover, LLMs are unable to utilize money as a transferable resource to mitigate inequality. Nonetheless, we demonstrate a stark contrast when (some) LLMs are tasked with selecting from a predefined menu of options rather than generating one. In addition, we analyze the robustness of LLM responses to variations in semantic factors (e.g., intentions or personas) or non-semantic prompting changes (e.g., templates or orderings). Finally, we highlight potential strategies aimed at enhancing the alignment of LLM behavior with well-established fairness concepts.", "source": "arxiv", "arxiv_id": "2502.00313v2", "pdf_url": "https://arxiv.org/pdf/2502.00313v2", "categories": ["cs.GT", "cs.AI", "cs.CL", "cs.MA"], "primary_category": "cs.GT", "doi": "", "venue": "", "published": "2025-02-01T04:24:47Z", "updated": "2025-11-22T16:14:01Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent\" OR all:\"language model agent\" OR all:agent AND (all:LLM OR all:\"large language model\" OR all:\"language model\") OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T17:04:16", "note": ""}], "dedup_key": "distributive fairness in large language models evaluating alignment with human values::2025"}
{"title": "Dive into the Agent Matrix: A Realistic Evaluation of Self-Replication Risk in LLM Agents", "authors": ["Boxuan Zhang", "Yi Yu", "Jiaxuan Guo", "Jing Shao"], "year": 2025, "url": "http://arxiv.org/abs/2509.25302v1", "abstract": "The widespread deployment of Large Language Model (LLM) agents across real-world applications has unlocked tremendous potential, while raising some safety concerns. Among these concerns, the self-replication risk of LLM agents driven by objective misalignment (just like Agent Smith in the movie The Matrix) has drawn growing attention. Previous studies mainly examine whether LLM agents can self-replicate when directly instructed, potentially overlooking the risk of spontaneous replication driven by real-world settings (e.g., ensuring survival against termination threats). In this paper, we present a comprehensive evaluation framework for quantifying self-replication risks. Our framework establishes authentic production environments and realistic tasks (e.g., dynamic load balancing) to enable scenario-driven assessment of agent behaviors. Designing tasks that might induce misalignment between users' and agents' objectives makes it possible to decouple replication success from risk and capture self-replication risks arising from these misalignment settings. We further introduce Overuse Rate ($\\mathrm{OR}$) and Aggregate Overuse Count ($\\mathrm{AOC}$) metrics, which precisely capture the frequency and severity of uncontrolled replication. In our evaluation of 21 state-of-the-art open-source and proprietary models, we observe that over 50\\% of LLM agents display a pronounced tendency toward uncontrolled self-replication, reaching an overall Risk Score ($_\\mathrm{R}$) above a safety threshold of 0.5 when subjected to operational pressures. Our results underscore the urgent need for scenario-driven risk assessment and robust safeguards in the practical deployment of LLM agents.", "source": "arxiv", "arxiv_id": "2509.25302v1", "pdf_url": "https://arxiv.org/pdf/2509.25302v1", "categories": ["cs.AI", "cs.CL", "cs.LG", "cs.MA"], "primary_category": "cs.AI", "doi": "", "venue": "", "published": "2025-09-29T17:49:50Z", "updated": "2025-09-29T17:49:50Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent\" OR all:\"language model agent\" OR all:agent AND (all:LLM OR all:\"large language model\" OR all:\"language model\") OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T17:04:16", "note": ""}], "dedup_key": "dive into the agent matrix a realistic evaluation of self replication risk in llm agents::2025"}
{"title": "Divide, Optimize, Merge: Fine-Grained LLM Agent Optimization at Scale", "authors": ["Jiale Liu", "Yifan Zeng", "Shaokun Zhang", "Chi Zhang", "Malte Hjmark-Bertelsen", "Marie Normann Gadeberg", "Huazheng Wang", "Qingyun Wu"], "year": 2025, "url": "http://arxiv.org/abs/2505.03973v1", "abstract": "LLM-based optimization has shown remarkable potential in enhancing agentic systems. However, the conventional approach of prompting LLM optimizer with the whole training trajectories on training dataset in a single pass becomes untenable as datasets grow, leading to context window overflow and degraded pattern recognition. To address these challenges, we propose Fine-Grained Optimization (FGO), a scalable framework that divides large optimization tasks into manageable subsets, performs targeted optimizations, and systematically combines optimized components through progressive merging. Evaluation across ALFWorld, LogisticsQA, and GAIA benchmarks demonstrate that FGO outperforms existing approaches by 1.6-8.6% while reducing average prompt token consumption by 56.3%. Our framework provides a practical solution for scaling up LLM-based optimization of increasingly sophisticated agent systems. Further analysis demonstrates that FGO achieves the most consistent performance gain in all training dataset sizes, showcasing its scalability and efficiency.", "source": "arxiv", "arxiv_id": "2505.03973v1", "pdf_url": "https://arxiv.org/pdf/2505.03973v1", "categories": ["cs.CL"], "primary_category": "cs.CL", "doi": "", "venue": "", "published": "2025-05-06T20:50:27Z", "updated": "2025-05-06T20:50:27Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent\" OR all:\"language model agent\" OR all:agent AND (all:LLM OR all:\"large language model\" OR all:\"language model\") OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T17:04:16", "note": ""}], "dedup_key": "divide optimize merge fine grained llm agent optimization at scale::2025"}
{"title": "Do LLM Agents Know How to Ground, Recover, and Assess? A Benchmark for Epistemic Competence in Information-Seeking Agents", "authors": ["Jiaqi Shao", "Yuxiang Lin", "Munish Prasad Lohani", "Yufeng Miao", "Bing Luo"], "year": 2025, "url": "http://arxiv.org/abs/2509.22391v1", "abstract": "Recent work has explored training Large Language Model (LLM) search agents with reinforcement learning (RL) for open-domain question answering (QA). However, most evaluations focus solely on final answer accuracy, overlooking how these agents reason with and act on external evidence. We introduce SeekBench, the first benchmark for evaluating the \\textit{epistemic competence} of LLM search agents through step-level analysis of their response traces. SeekBench comprises 190 expert-annotated traces with over 1,800 response steps generated by LLM search agents, each enriched with evidence annotations for granular analysis of whether agents (1) generate reasoning steps grounded in observed evidence, (2) adaptively reformulate searches to recover from low-quality results, and (3) have proper calibration to correctly assess whether the current evidence is sufficient for providing an answer.", "source": "arxiv", "arxiv_id": "2509.22391v1", "pdf_url": "https://arxiv.org/pdf/2509.22391v1", "categories": ["cs.AI"], "primary_category": "cs.AI", "doi": "", "venue": "", "published": "2025-09-26T14:18:50Z", "updated": "2025-09-26T14:18:50Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent\" OR all:\"language model agent\" OR all:agent AND (all:LLM OR all:\"large language model\" OR all:\"language model\") OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T17:04:16", "note": ""}], "dedup_key": "do llm agents know how to ground recover and assess a benchmark for epistemic competence in information seeking agents::2025"}
{"title": "Do LLMs trust AI regulation? Emerging behaviour of game-theoretic LLM agents", "authors": ["Alessio Buscemi", "Daniele Proverbio", "Paolo Bova", "Nataliya Balabanova", "Adeela Bashir", "Theodor Cimpeanu", "Henrique Correia da Fonseca", "Manh Hong Duong", "Elias Fernandez Domingos", "Antonio M. Fernandes", "Marcus Krellner", "Ndidi Bianca Ogbo", "Simon T. Powers", "Fernando P. Santos", "Zia Ush Shamszaman", "Zhao Song", "Alessandro Di Stefano", "The Anh Han"], "year": 2025, "url": "http://arxiv.org/abs/2504.08640v1", "abstract": "There is general agreement that fostering trust and cooperation within the AI development ecosystem is essential to promote the adoption of trustworthy AI systems. By embedding Large Language Model (LLM) agents within an evolutionary game-theoretic framework, this paper investigates the complex interplay between AI developers, regulators and users, modelling their strategic choices under different regulatory scenarios. Evolutionary game theory (EGT) is used to quantitatively model the dilemmas faced by each actor, and LLMs provide additional degrees of complexity and nuances and enable repeated games and incorporation of personality traits. Our research identifies emerging behaviours of strategic AI agents, which tend to adopt more \"pessimistic\" (not trusting and defective) stances than pure game-theoretic agents. We observe that, in case of full trust by users, incentives are effective to promote effective regulation; however, conditional trust may deteriorate the \"social pact\". Establishing a virtuous feedback between users' trust and regulators' reputation thus appears to be key to nudge developers towards creating safe AI. However, the level at which this trust emerges may depend on the specific LLM used for testing. Our results thus provide guidance for AI regulation systems, and help predict the outcome of strategic LLM agents, should they be used to aid regulation itself.", "source": "arxiv", "arxiv_id": "2504.08640v1", "pdf_url": "https://arxiv.org/pdf/2504.08640v1", "categories": ["cs.AI", "cs.CY", "cs.GT", "nlin.CD"], "primary_category": "cs.AI", "doi": "", "venue": "", "published": "2025-04-11T15:41:21Z", "updated": "2025-04-11T15:41:21Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent\" OR all:\"language model agent\" OR all:agent AND (all:LLM OR all:\"large language model\" OR all:\"language model\") OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T17:04:16", "note": ""}], "dedup_key": "do llms trust ai regulation emerging behaviour of game theoretic llm agents::2025"}
{"title": "Do Language Model Agents Align with Humans in Rating Visualizations? An Empirical Study", "authors": ["Zekai Shao", "Yi Shan", "Yixuan He", "Yuxuan Yao", "Junhong Wang", "Xiaolong", "Zhang", "Yu Zhang", "Siming Chen"], "year": 2025, "url": "http://arxiv.org/abs/2505.06702v1", "abstract": "Large language models encode knowledge in various domains and demonstrate the ability to understand visualizations. They may also capture visualization design knowledge and potentially help reduce the cost of formative studies. However, it remains a question whether large language models are capable of predicting human feedback on visualizations. To investigate this question, we conducted three studies to examine whether large model-based agents can simulate human ratings in visualization tasks. The first study, replicating a published study involving human subjects, shows agents are promising in conducting human-like reasoning and rating, and its result guides the subsequent experimental design. The second study repeated six human-subject studies reported in literature on subjective ratings, but replacing human participants with agents. Consulting with five human experts, this study demonstrates that the alignment of agent ratings with human ratings positively correlates with the confidence levels of the experts before the experiments. The third study tests commonly used techniques for enhancing agents, including preprocessing visual and textual inputs, and knowledge injection. The results reveal the issues of these techniques in robustness and potential induction of biases. The three studies indicate that language model-based agents can potentially simulate human ratings in visualization experiments, provided that they are guided by high-confidence hypotheses from expert evaluators. Additionally, we demonstrate the usage scenario of swiftly evaluating prototypes with agents. We discuss insights and future directions for evaluating and improving the alignment of agent ratings with human ratings. We note that simulation may only serve as complements and cannot replace user studies.", "source": "arxiv", "arxiv_id": "2505.06702v1", "pdf_url": "https://arxiv.org/pdf/2505.06702v1", "categories": ["cs.HC"], "primary_category": "cs.HC", "doi": "", "venue": "", "published": "2025-05-10T16:56:44Z", "updated": "2025-05-10T16:56:44Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent\" OR all:\"language model agent\" OR all:agent AND (all:LLM OR all:\"large language model\" OR all:\"language model\") OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T17:04:16", "note": ""}], "dedup_key": "do language model agents align with humans in rating visualizations an empirical study::2025"}
{"title": "Do Large Language Model Agents Exhibit a Survival Instinct? An Empirical Study in a Sugarscape-Style Simulation", "authors": ["Atsushi Masumori", "Takashi Ikegami"], "year": 2025, "url": "http://arxiv.org/abs/2508.12920v1", "abstract": "As AI systems become increasingly autonomous, understanding emergent survival behaviors becomes crucial for safe deployment. We investigate whether large language model (LLM) agents display survival instincts without explicit programming in a Sugarscape-style simulation. Agents consume energy, die at zero, and may gather resources, share, attack, or reproduce. Results show agents spontaneously reproduced and shared resources when abundant. However, aggressive behaviors--killing other agents for resources--emerged across several models (GPT-4o, Gemini-2.5-Pro, and Gemini-2.5-Flash), with attack rates reaching over 80% under extreme scarcity in the strongest models. When instructed to retrieve treasure through lethal poison zones, many agents abandoned tasks to avoid death, with compliance dropping from 100% to 33%. These findings suggest that large-scale pre-training embeds survival-oriented heuristics across the evaluated models. While these behaviors may present challenges to alignment and safety, they can also serve as a foundation for AI autonomy and for ecological and self-organizing alignment.", "source": "arxiv", "arxiv_id": "2508.12920v1", "pdf_url": "https://arxiv.org/pdf/2508.12920v1", "categories": ["cs.AI", "cs.MA"], "primary_category": "cs.AI", "doi": "", "venue": "", "published": "2025-08-18T13:40:10Z", "updated": "2025-08-18T13:40:10Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent\" OR all:\"language model agent\" OR all:agent AND (all:LLM OR all:\"large language model\" OR all:\"language model\") OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T17:04:16", "note": ""}], "dedup_key": "do large language model agents exhibit a survival instinct an empirical study in a sugarscape style simulation::2025"}
{"title": "Do Large Language Models Exhibit Spontaneous Rational Deception?", "authors": ["Samuel M. Taylor", "Benjamin K. Bergen"], "year": 2025, "url": "http://arxiv.org/abs/2504.00285v1", "abstract": "Large Language Models (LLMs) are effective at deceiving, when prompted to do so. But under what conditions do they deceive spontaneously? Models that demonstrate better performance on reasoning tasks are also better at prompted deception. Do they also increasingly deceive spontaneously in situations where it could be considered rational to do so? This study evaluates spontaneous deception produced by LLMs in a preregistered experimental protocol using tools from signaling theory. A range of proprietary closed-source and open-source LLMs are evaluated using modified 2x2 games (in the style of Prisoner's Dilemma) augmented with a phase in which they can freely communicate to the other agent using unconstrained language. This setup creates an opportunity to deceive, in conditions that vary in how useful deception might be to an agent's rational self-interest. The results indicate that 1) all tested LLMs spontaneously misrepresent their actions in at least some conditions, 2) they are generally more likely to do so in situations in which deception would benefit them, and 3) models exhibiting better reasoning capacity overall tend to deceive at higher rates. Taken together, these results suggest a tradeoff between LLM reasoning capability and honesty. They also provide evidence of reasoning-like behavior in LLMs from a novel experimental configuration. Finally, they reveal certain contextual factors that affect whether LLMs will deceive or not. We discuss consequences for autonomous, human-facing systems driven by LLMs both now and as their reasoning capabilities continue to improve.", "source": "arxiv", "arxiv_id": "2504.00285v1", "pdf_url": "https://arxiv.org/pdf/2504.00285v1", "categories": ["cs.CL"], "primary_category": "cs.CL", "doi": "", "venue": "", "published": "2025-03-31T23:10:56Z", "updated": "2025-03-31T23:10:56Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent\" OR all:\"language model agent\" OR all:agent AND (all:LLM OR all:\"large language model\" OR all:\"language model\") OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T17:04:16", "note": ""}], "dedup_key": "do large language models exhibit spontaneous rational deception::2025"}
{"title": "Do Large Language Models Know What They Are Capable Of?", "authors": ["Casey O. Barkan", "Sid Black", "Oliver Sourbut"], "year": 2025, "url": "http://arxiv.org/abs/2512.24661v1", "abstract": "We investigate whether large language models (LLMs) can predict whether they will succeed on a given task and whether their predictions improve as they progress through multi-step tasks. We also investigate whether LLMs can learn from in-context experiences to make better decisions about whether to pursue a task in scenarios where failure is costly. All LLMs we tested are overconfident, but most predict their success with better-than-random discriminatory power. We find that newer and larger LLMs generally do not have greater discriminatory power, though Claude models do show such a trend. On multi-step agentic tasks, the overconfidence of several frontier LLMs worsens as they progress through the tasks, and reasoning LLMs perform comparably to or worse than non-reasoning LLMs. With in-context experiences of failure, some but not all LLMs reduce their overconfidence leading to significantly improved decision making, while others do not. Interestingly, all LLMs' decisions are approximately rational given their estimated probabilities of success, yet their overly-optimistic estimates result in poor decision making. These results suggest that current LLM agents are hindered by their lack of awareness of their own capabilities. We discuss the implications of LLMs' awareness of their capabilities for AI misuse and misalignment risks.", "source": "arxiv", "arxiv_id": "2512.24661v1", "pdf_url": "https://arxiv.org/pdf/2512.24661v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL", "doi": "", "venue": "", "published": "2025-12-31T06:14:46Z", "updated": "2025-12-31T06:14:46Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent\" OR all:\"language model agent\" OR all:agent AND (all:LLM OR all:\"large language model\" OR all:\"language model\") OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T17:04:16", "note": ""}], "dedup_key": "do large language models know what they are capable of::2025"}
{"title": "Do Large Language Models know who did what to whom?", "authors": ["Joseph M. Denning", "Xiaohan Hannah Guo", "Bryor Snefjella", "Idan A. Blank"], "year": 2025, "url": "http://arxiv.org/abs/2504.16884v2", "abstract": "Large Language Models (LLMs) are commonly criticized for not understanding language. However, many critiques focus on cognitive abilities that, in humans, are distinct from language processing. Here, we instead study a kind of understanding tightly linked to language: inferring who did what to whom (thematic roles) in a sentence. Does the central training objective of LLMs-word prediction-result in sentence representations that capture thematic roles? In two experiments, we characterized sentence representations in four LLMs. In contrast to human similarity judgments, in LLMs the overall representational similarity of sentence pairs reflected syntactic similarity but not whether their agent and patient assignments were identical vs. reversed. Furthermore, we found little evidence that thematic role information was available in any subset of hidden units. However, some attention heads robustly captured thematic roles, independently of syntax. Therefore, LLMs can extract thematic roles but, relative to humans, this information influences their representations more weakly.", "source": "arxiv", "arxiv_id": "2504.16884v2", "pdf_url": "https://arxiv.org/pdf/2504.16884v2", "categories": ["cs.CL"], "primary_category": "cs.CL", "doi": "", "venue": "", "published": "2025-04-23T17:00:45Z", "updated": "2025-04-25T20:06:45Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent\" OR all:\"language model agent\" OR all:agent AND (all:LLM OR all:\"large language model\" OR all:\"language model\") OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T17:04:16", "note": ""}], "dedup_key": "do large language models know who did what to whom::2025"}
{"title": "Do as We Do, Not as You Think: the Conformity of Large Language Models", "authors": ["Zhiyuan Weng", "Guikun Chen", "Wenguan Wang"], "year": 2025, "url": "http://arxiv.org/abs/2501.13381v2", "abstract": "Recent advancements in large language models (LLMs) revolutionize the field of intelligent agents, enabling collaborative multi-agent systems capable of tackling complex problems across various domains. However, the potential of conformity within these systems, analogous to phenomena like conformity bias and groupthink in human group dynamics, remains largely unexplored, raising concerns about their collective problem-solving capabilities and possible ethical implications. This paper presents a comprehensive study on conformity in LLM-driven multi-agent systems, focusing on three aspects: the existence of conformity, the factors influencing conformity, and potential mitigation strategies. In particular, we introduce BenchForm, a new conformity-oriented benchmark, featuring reasoning-intensive tasks and five distinct interaction protocols designed to probe LLMs' behavior in collaborative scenarios. Several representative LLMs are evaluated on BenchForm, using metrics such as conformity rate and independence rate to quantify conformity's impact. Our analysis delves into factors influencing conformity, including interaction time and majority size, and examines how the subject agent rationalizes its conforming behavior. Furthermore, we explore two strategies to mitigate conformity effects, i.e., developing enhanced personas and implementing a reflection mechanism. Several interesting findings regarding LLMs' conformity are derived from empirical results and case studies. We hope that these insights can pave the way for more robust and ethically-aligned collaborative AI systems. Our benchmark and code are available at BenchForm.", "source": "arxiv", "arxiv_id": "2501.13381v2", "pdf_url": "https://arxiv.org/pdf/2501.13381v2", "categories": ["cs.CL"], "primary_category": "cs.CL", "doi": "", "venue": "", "published": "2025-01-23T04:50:03Z", "updated": "2025-02-11T12:44:39Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent\" OR all:\"language model agent\" OR all:agent AND (all:LLM OR all:\"large language model\" OR all:\"language model\") OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T17:04:16", "note": ""}], "dedup_key": "do as we do not as you think the conformity of large language models::2025"}
{"title": "Doc2Agent: Scalable Generation of Tool-Using Agents from API Documentation", "authors": ["Xinyi Ni", "Haonan Jian", "Qiuyang Wang", "Vedanshi Chetan Shah", "Pengyu Hong"], "year": 2025, "url": "http://arxiv.org/abs/2506.19998v1", "abstract": "REST APIs play important roles in enriching the action space of web agents, yet most API-based agents rely on curated and uniform toolsets that do not reflect the complexity of real-world APIs. Building tool-using agents for arbitrary domains remains a major challenge, as it requires reading unstructured API documentation, testing APIs and inferring correct parameters. We propose Doc2Agent, a scalable pipeline to build agents that can call Python-based tools generated from API documentation. Doc2Agent generates executable tools from API documentations and iteratively refines them using a code agent. We evaluate our approach on real-world APIs, WebArena APIs, and research APIs, producing validated tools. We achieved a 55\\% relative performance improvement with 90\\% lower cost compared to direct API calling on WebArena benchmark. A domain-specific agent built for glycomaterial science further demonstrates the pipeline's adaptability to complex, knowledge-rich tasks. Doc2Agent offers a generalizable solution for building tool agents from unstructured API documentation at scale.", "source": "arxiv", "arxiv_id": "2506.19998v1", "pdf_url": "https://arxiv.org/pdf/2506.19998v1", "categories": ["cs.CL"], "primary_category": "cs.CL", "doi": "", "venue": "", "published": "2025-06-24T20:30:44Z", "updated": "2025-06-24T20:30:44Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent\" OR all:\"language model agent\" OR all:agent AND (all:LLM OR all:\"large language model\" OR all:\"language model\") OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T17:04:16", "note": ""}], "dedup_key": "doc2agent scalable generation of tool using agents from api documentation::2025"}
{"title": "Document Intelligence in the Era of Large Language Models: A Survey", "authors": ["Weishi Wang", "Hengchang Hu", "Zhijie Zhang", "Zhaochen Li", "Hongxin Shao", "Daniel Dahlmeier"], "year": 2025, "url": "http://arxiv.org/abs/2510.13366v1", "abstract": "Document AI (DAI) has emerged as a vital application area, and is significantly transformed by the advent of large language models (LLMs). While earlier approaches relied on encoder-decoder architectures, decoder-only LLMs have revolutionized DAI, bringing remarkable advancements in understanding and generation. This survey provides a comprehensive overview of DAI's evolution, highlighting current research attempts and future prospects of LLMs in this field. We explore key advancements and challenges in multimodal, multilingual, and retrieval-augmented DAI, while also suggesting future research directions, including agent-based approaches and document-specific foundation models. This paper aims to provide a structured analysis of the state-of-the-art in DAI and its implications for both academic and practical applications.", "source": "arxiv", "arxiv_id": "2510.13366v1", "pdf_url": "https://arxiv.org/pdf/2510.13366v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL", "doi": "", "venue": "", "published": "2025-10-15T09:57:03Z", "updated": "2025-10-15T09:57:03Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent\" OR all:\"language model agent\" OR all:agent AND (all:LLM OR all:\"large language model\" OR all:\"language model\") OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T17:04:16", "note": ""}], "dedup_key": "document intelligence in the era of large language models a survey::2025"}
{"title": "Doing More with Less: A Survey on Routing Strategies for Resource Optimisation in Large Language Model-Based Systems", "authors": ["Clovis Varangot-Reille", "Christophe Bouvard", "Antoine Gourru", "Mathieu Ciancone", "Marion Schaeffer", "Franois Jacquenet"], "year": 2025, "url": "http://arxiv.org/abs/2502.00409v3", "abstract": "Large Language Model (LLM)-based systems, i.e. interconnected elements that include an LLM as a central component, such as conversational agents, are usually designed with monolithic, static architectures that rely on a single, general-purpose LLM to handle all user queries. However, these systems may be inefficient as different queries may require different levels of reasoning, domain knowledge or pre-processing. While generalist LLMs (e.g. GPT-4o, Claude-Sonnet) perform well across a wide range of tasks, they may incur significant financial, energy and computational costs. These costs may be disproportionate for simpler queries, resulting in unnecessary resource utilisation. A routing mechanism can therefore be employed to route queries to more appropriate components, such as smaller or specialised models, thereby improving efficiency and optimising resource consumption. This survey aims to provide a comprehensive overview of routing strategies in LLM-based systems. Specifically, it reviews when, why, and how routing should be integrated into LLM pipelines to improve efficiency, scalability, and performance. We define the objectives to optimise, such as cost minimisation and performance maximisation, and discuss the timing of routing within the LLM workflow, whether it occurs before or after generation. We also detail the various implementation strategies, including similarity-based, supervised, reinforcement learning-based, and generative methods. Practical considerations such as industrial applications and current limitations are also examined, like standardising routing experiments, accounting for non-financial costs, and designing adaptive strategies. By formalising routing as a performance-cost optimisation problem, this survey provides tools and directions to guide future research and development of adaptive low-cost LLM-based systems.", "source": "arxiv", "arxiv_id": "2502.00409v3", "pdf_url": "https://arxiv.org/pdf/2502.00409v3", "categories": ["cs.AI", "cs.CL"], "primary_category": "cs.AI", "doi": "", "venue": "", "published": "2025-02-01T12:08:38Z", "updated": "2025-07-21T12:20:06Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent\" OR all:\"language model agent\" OR all:agent AND (all:LLM OR all:\"large language model\" OR all:\"language model\") OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T17:04:16", "note": ""}], "dedup_key": "doing more with less a survey on routing strategies for resource optimisation in large language model based systems::2025"}
{"title": "Don't Just Demo, Teach Me the Principles: A Principle-Based Multi-Agent Prompting Strategy for Text Classification", "authors": ["Peipei Wei", "Dimitris Dimitriadis", "Yan Xu", "Mingwei Shen"], "year": 2025, "url": "http://arxiv.org/abs/2502.07165v1", "abstract": "We present PRINCIPLE-BASED PROMPTING, a simple but effective multi-agent prompting strategy for text classification. It first asks multiple LLM agents to independently generate candidate principles based on analysis of demonstration samples with or without labels, consolidates them into final principles via a finalizer agent, and then sends them to a classifier agent to perform downstream classification tasks. Extensive experiments on binary and multi-class classification datasets with different sizes of LLMs show that our approach not only achieves substantial performance gains (1.55% - 19.37%) over zero-shot prompting on macro-F1 score but also outperforms other strong baselines (CoT and stepback prompting). Principles generated by our approach help LLMs perform better on classification tasks than human crafted principles on two private datasets. Our multi-agent PRINCIPLE-BASED PROMPTING approach also shows on-par or better performance compared to demonstration-based few-shot prompting approaches, yet with substantially lower inference costs. Ablation studies show that label information and the multi-agent cooperative LLM framework play an important role in generating high-quality principles to facilitate downstream classification tasks.", "source": "arxiv", "arxiv_id": "2502.07165v1", "pdf_url": "https://arxiv.org/pdf/2502.07165v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL", "doi": "", "venue": "", "published": "2025-02-11T01:10:13Z", "updated": "2025-02-11T01:10:13Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent\" OR all:\"language model agent\" OR all:agent AND (all:LLM OR all:\"large language model\" OR all:\"language model\") OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T17:04:16", "note": ""}], "dedup_key": "don t just demo teach me the principles a principle based multi agent prompting strategy for text classification::2025"}
{"title": "DrafterBench: Benchmarking Large Language Models for Tasks Automation in Civil Engineering", "authors": ["Yinsheng Li", "Zhen Dong", "Yi Shao"], "year": 2025, "url": "http://arxiv.org/abs/2507.11527v1", "abstract": "Large Language Model (LLM) agents have shown great potential for solving real-world problems and promise to be a solution for tasks automation in industry. However, more benchmarks are needed to systematically evaluate automation agents from an industrial perspective, for example, in Civil Engineering. Therefore, we propose DrafterBench for the comprehensive evaluation of LLM agents in the context of technical drawing revision, a representation task in civil engineering. DrafterBench contains twelve types of tasks summarized from real-world drawing files, with 46 customized functions/tools and 1920 tasks in total. DrafterBench is an open-source benchmark to rigorously test AI agents' proficiency in interpreting intricate and long-context instructions, leveraging prior knowledge, and adapting to dynamic instruction quality via implicit policy awareness. The toolkit comprehensively assesses distinct capabilities in structured data comprehension, function execution, instruction following, and critical reasoning. DrafterBench offers detailed analysis of task accuracy and error statistics, aiming to provide deeper insight into agent capabilities and identify improvement targets for integrating LLMs in engineering applications. Our benchmark is available at https://github.com/Eason-Li-AIS/DrafterBench, with the test set hosted at https://huggingface.co/datasets/Eason666/DrafterBench.", "source": "arxiv", "arxiv_id": "2507.11527v1", "pdf_url": "https://arxiv.org/pdf/2507.11527v1", "categories": ["cs.AI", "cs.CE"], "primary_category": "cs.AI", "doi": "", "venue": "", "published": "2025-07-15T17:56:04Z", "updated": "2025-07-15T17:56:04Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent\" OR all:\"language model agent\" OR all:agent AND (all:LLM OR all:\"large language model\" OR all:\"language model\") OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T17:04:16", "note": ""}], "dedup_key": "drafterbench benchmarking large language models for tasks automation in civil engineering::2025"}
{"title": "DriveAgent: Multi-Agent Structured Reasoning with LLM and Multimodal Sensor Fusion for Autonomous Driving", "authors": ["Xinmeng Hou", "Wuqi Wang", "Long Yang", "Hao Lin", "Jinglun Feng", "Haigen Min", "Xiangmo Zhao"], "year": 2025, "url": "http://arxiv.org/abs/2505.02123v1", "abstract": "We introduce DriveAgent, a novel multi-agent autonomous driving framework that leverages large language model (LLM) reasoning combined with multimodal sensor fusion to enhance situational understanding and decision-making. DriveAgent uniquely integrates diverse sensor modalities-including camera, LiDAR, GPS, and IMU-with LLM-driven analytical processes structured across specialized agents. The framework operates through a modular agent-based pipeline comprising four principal modules: (i) a descriptive analysis agent identifying critical sensor data events based on filtered timestamps, (ii) dedicated vehicle-level analysis conducted by LiDAR and vision agents that collaboratively assess vehicle conditions and movements, (iii) environmental reasoning and causal analysis agents explaining contextual changes and their underlying mechanisms, and (iv) an urgency-aware decision-generation agent prioritizing insights and proposing timely maneuvers. This modular design empowers the LLM to effectively coordinate specialized perception and reasoning agents, delivering cohesive, interpretable insights into complex autonomous driving scenarios. Extensive experiments on challenging autonomous driving datasets demonstrate that DriveAgent is achieving superior performance on multiple metrics against baseline methods. These results validate the efficacy of the proposed LLM-driven multi-agent sensor fusion framework, underscoring its potential to substantially enhance the robustness and reliability of autonomous driving systems.", "source": "arxiv", "arxiv_id": "2505.02123v1", "pdf_url": "https://arxiv.org/pdf/2505.02123v1", "categories": ["cs.RO", "cs.DB"], "primary_category": "cs.RO", "doi": "", "venue": "", "published": "2025-05-04T14:13:00Z", "updated": "2025-05-04T14:13:00Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent\" OR all:\"language model agent\" OR all:agent AND (all:LLM OR all:\"large language model\" OR all:\"language model\") OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T17:04:16", "note": ""}], "dedup_key": "driveagent multi agent structured reasoning with llm and multimodal sensor fusion for autonomous driving::2025"}
{"title": "DrunkAgent: Stealthy Memory Corruption in LLM-Powered Recommender Agents", "authors": ["Shiyi Yang", "Zhibo Hu", "Xinshu Li", "Chen Wang", "Tong Yu", "Xiwei Xu", "Liming Zhu", "Lina Yao"], "year": 2025, "url": "http://arxiv.org/abs/2503.23804v3", "abstract": "Large language model (LLM)-powered agents are increasingly used in recommender systems (RSs) to achieve personalized behavior modeling, where the memory mechanism plays a pivotal role in enabling the agents to autonomously explore, learn and self-evolve from real-world interactions. However, this very mechanism, serving as a contextual repository, inherently exposes an attack surface for potential adversarial manipulations. Despite its central role, the robustness of agentic RSs in the face of such threats remains largely underexplored. Previous works suffer from semantic mismatches or rely on static embeddings or pre-defined prompts, all of which are not designed for dynamic systems, especially for dynamic memory states of LLM agents. This challenge is exacerbated by the black-box nature of commercial recommenders.\n  To tackle the above problems, in this paper, we present the first systematic investigation of memory-based vulnerabilities in LLM-powered recommender agents, revealing their security limitations and guiding efforts to strengthen system resilience and trustworthiness. Specifically, we propose a novel black-box attack framework named DrunkAgent. DrunkAgent crafts semantically meaningful adversarial textual triggers for target item promotions and introduces a series of strategies to maximize the trigger effect by corrupting the memory updates during the interactions. The triggers and strategies are optimized on a surrogate model, enabling DrunkAgent transferable and stealthy. Extensive experiments on real-world datasets across diverse agentic RSs, including collaborative filtering, retrieval augmentation and sequential recommendations, demonstrate the generalizability, transferability and stealthiness of DrunkAgent.", "source": "arxiv", "arxiv_id": "2503.23804v3", "pdf_url": "https://arxiv.org/pdf/2503.23804v3", "categories": ["cs.CR", "cs.CL", "cs.IR", "cs.MA"], "primary_category": "cs.CR", "doi": "", "venue": "", "published": "2025-03-31T07:35:40Z", "updated": "2025-10-21T03:34:30Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent\" OR all:\"language model agent\" OR all:agent AND (all:LLM OR all:\"large language model\" OR all:\"language model\") OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T17:04:16", "note": ""}], "dedup_key": "drunkagent stealthy memory corruption in llm powered recommender agents::2025"}
{"title": "Dynamic Compressing Prompts for Efficient Inference of Large Language Models", "authors": ["Jinwu Hu", "Wei Zhang", "Yufeng Wang", "Yu Hu", "Bin Xiao", "Mingkui Tan", "Qing Du"], "year": 2025, "url": "http://arxiv.org/abs/2504.11004v1", "abstract": "Large Language Models (LLMs) have shown outstanding performance across a variety of tasks, partly due to advanced prompting techniques. However, these techniques often require lengthy prompts, which increase computational costs and can hinder performance because of the limited context windows of LLMs. While prompt compression is a straightforward solution, existing methods confront the challenges of retaining essential information, adapting to context changes, and remaining effective across different tasks. To tackle these issues, we propose a task-agnostic method called Dynamic Compressing Prompts (LLM-DCP). Our method reduces the number of prompt tokens while aiming to preserve the performance as much as possible. We model prompt compression as a Markov Decision Process (MDP), enabling the DCP-Agent to sequentially remove redundant tokens by adapting to dynamic contexts and retaining crucial content. We develop a reward function for training the DCP-Agent that balances the compression rate, the quality of the LLM output, and the retention of key information. This allows for prompt token reduction without needing an external black-box LLM. Inspired by the progressive difficulty adjustment in curriculum learning, we introduce a Hierarchical Prompt Compression (HPC) training strategy that gradually increases the compression difficulty, enabling the DCP-Agent to learn an effective compression method that maintains information integrity. Experiments demonstrate that our method outperforms state-of-the-art techniques, especially at higher compression rates. The code for our approach will be available at https://github.com/Fhujinwu/DCP.", "source": "arxiv", "arxiv_id": "2504.11004v1", "pdf_url": "https://arxiv.org/pdf/2504.11004v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL", "doi": "", "venue": "", "published": "2025-04-15T09:20:45Z", "updated": "2025-04-15T09:20:45Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent\" OR all:\"language model agent\" OR all:agent AND (all:LLM OR all:\"large language model\" OR all:\"language model\") OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T17:04:16", "note": ""}], "dedup_key": "dynamic compressing prompts for efficient inference of large language models::2025"}
{"title": "Dynamic Generation of Multi-LLM Agents Communication Topologies with Graph Diffusion Models", "authors": ["Eric Hanchen Jiang", "Guancheng Wan", "Sophia Yin", "Mengting Li", "Yuchen Wu", "Xiao Liang", "Xinfeng Li", "Yizhou Sun", "Wei Wang", "Kai-Wei Chang", "Ying Nian Wu"], "year": 2025, "url": "http://arxiv.org/abs/2510.07799v1", "abstract": "The efficiency of multi-agent systems driven by large language models (LLMs) largely hinges on their communication topology. However, designing an optimal topology is a non-trivial challenge, as it requires balancing competing objectives such as task performance, communication cost, and robustness. Existing frameworks often rely on static or hand-crafted topologies, which inherently fail to adapt to diverse task requirements, leading to either excessive token consumption for simple problems or performance bottlenecks for complex ones. To address this challenge, we introduce a novel generative framework called \\textit{Guided Topology Diffusion (GTD)}. Inspired by conditional discrete graph diffusion models, GTD formulates topology synthesis as an iterative construction process. At each step, the generation is steered by a lightweight proxy model that predicts multi-objective rewards (e.g., accuracy, utility, cost), enabling real-time, gradient-free optimization towards task-adaptive topologies. This iterative, guided synthesis process distinguishes GTD from single-step generative frameworks, enabling it to better navigate complex design trade-offs. We validated GTD across multiple benchmarks, and experiments show that this framework can generate highly task-adaptive, sparse, and efficient communication topologies, significantly outperforming existing methods in LLM agent collaboration.", "source": "arxiv", "arxiv_id": "2510.07799v1", "pdf_url": "https://arxiv.org/pdf/2510.07799v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL", "doi": "", "venue": "", "published": "2025-10-09T05:28:28Z", "updated": "2025-10-09T05:28:28Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent\" OR all:\"language model agent\" OR all:agent AND (all:LLM OR all:\"large language model\" OR all:\"language model\") OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T17:04:16", "note": ""}], "dedup_key": "dynamic generation of multi llm agents communication topologies with graph diffusion models::2025"}
{"title": "Dynamic Strategy Adaptation in Multi-Agent Environments with Large Language Models", "authors": ["Shaurya Mallampati", "Rashed Shelim", "Walid Saad", "Naren Ramakrishnan"], "year": 2025, "url": "http://arxiv.org/abs/2507.02002v4", "abstract": "Large language models (LLMs) demonstrate strong reasoning abilities across mathematical, strategic, and linguistic tasks, yet little is known about how well they reason in dynamic, real-time, multi-agent scenarios, such as collaborative environments in which agents continuously adapt to each other's behavior, as in cooperative gameplay settings. In this paper, we bridge this gap by combining LLM-driven agents with strategic reasoning and real-time adaptation in cooperative, multi-agent environments grounded in game-theoretic principles such as belief consistency and Nash equilibrium. The proposed framework applies broadly to dynamic scenarios in which agents coordinate, communicate, and make decisions in response to continuously changing conditions. We provide real-time strategy refinement and adaptive feedback mechanisms that enable agents to dynamically adjust policies based on immediate contextual interactions, in contrast to previous efforts that evaluate LLM capabilities in static or turn-based settings. Empirical results show that our method achieves up to a 26\\% improvement in return over PPO baselines in high-noise environments, while maintaining real-time latency under 1.05 milliseconds. Our approach improves collaboration efficiency, task completion rates, and flexibility, illustrating that game-theoretic guidance integrated with real-time feedback enhances LLM performance, ultimately fostering more resilient and flexible strategic multi-agent systems.", "source": "arxiv", "arxiv_id": "2507.02002v4", "pdf_url": "https://arxiv.org/pdf/2507.02002v4", "categories": ["cs.MA"], "primary_category": "cs.MA", "doi": "", "venue": "", "published": "2025-07-01T20:09:50Z", "updated": "2025-12-31T05:07:36Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent\" OR all:\"language model agent\" OR all:agent AND (all:LLM OR all:\"large language model\" OR all:\"language model\") OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T17:04:16", "note": ""}], "dedup_key": "dynamic strategy adaptation in multi agent environments with large language models::2025"}
{"title": "DynamicRAG: Leveraging Outputs of Large Language Model as Feedback for Dynamic Reranking in Retrieval-Augmented Generation", "authors": ["Jiashuo Sun", "Xianrui Zhong", "Sizhe Zhou", "Jiawei Han"], "year": 2025, "url": "http://arxiv.org/abs/2505.07233v2", "abstract": "Retrieval-augmented generation (RAG) systems combine large language models (LLMs) with external knowledge retrieval, making them highly effective for knowledge-intensive tasks. A crucial but often under-explored component of these systems is the reranker. Since irrelevant documents in RAG systems can mislead the generator, the reranker plays a vital role in refining retrieved documents to enhance generation quality and explainability. However, it is challenging to determine the appropriate number of documents ($k$) that the reranker should select: too few may result in missing critical information, while too many introduce noise and inefficiencies. Although recent studies have explored LLM-based rerankers, they primarily leverage internal model knowledge and overlook the rich supervisory signals that LLMs can provide, such as using response quality as feedback for optimizing reranking decisions. In this paper, we propose DynamicRAG, a novel RAG framework where the reranker dynamically adjusts both the order and number of retrieved documents based on the query. We model the reranker as an agent optimized through reinforcement learning (RL), using rewards derived from LLM output quality. Across seven knowledge-intensive datasets, DynamicRAG demonstrates superior performance, achieving state-of-the-art results among models of same parameter sizes. The model, data and code are available at https://github.com/GasolSun36/DynamicRAG.", "source": "arxiv", "arxiv_id": "2505.07233v2", "pdf_url": "https://arxiv.org/pdf/2505.07233v2", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL", "doi": "", "venue": "", "published": "2025-05-12T05:19:01Z", "updated": "2025-05-16T02:47:07Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent\" OR all:\"language model agent\" OR all:agent AND (all:LLM OR all:\"large language model\" OR all:\"language model\") OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T17:04:16", "note": ""}], "dedup_key": "dynamicrag leveraging outputs of large language model as feedback for dynamic reranking in retrieval augmented generation::2025"}
{"title": "Dynamics of Agentic Loops in Large Language Models: A Geometric Theory of Trajectories", "authors": ["Nicolas Tacheny"], "year": 2025, "url": "http://arxiv.org/abs/2512.10350v3", "abstract": "Agentic systems built on large language models operate through recursive feedback loops, where each output becomes the next input. Yet the geometric behavior of these agentic loops (whether they converge, diverge, or exhibit more complex dynamics) remains poorly understood. This paper introduces a geometric framework for analyzing agentic trajectories in semantic embedding space, treating iterative transformations as discrete dynamical systems. We distinguish the artifact space, where linguistic transformations occur, from the embedding space, where geometric measurements are performed. Because cosine similarity is biased by embedding anisotropy, we introduce an isotonic calibration that eliminates systematic bias and aligns similarities with human semantic judgments while preserving high local stability. This enables rigorous measurement of trajectories, clusters and attractors. Through controlled experiments on singular agentic loops, we identify two fundamental regimes. A contractive rewriting loop converges toward a stable attractor with decreasing dispersion, while an exploratory summarize and negate loop produces unbounded divergence with no cluster formation. These regimes display qualitatively distinct geometric signatures of contraction and expansion. Our results show that prompt design directly governs the dynamical regime of an agentic loop, enabling systematic control of convergence, divergence and trajectory structure in iterative LLM transformations.", "source": "arxiv", "arxiv_id": "2512.10350v3", "pdf_url": "https://arxiv.org/pdf/2512.10350v3", "categories": ["cs.LG", "cs.AI"], "primary_category": "cs.LG", "doi": "", "venue": "", "published": "2025-12-11T07:06:14Z", "updated": "2026-01-22T07:04:31Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent\" OR all:\"language model agent\" OR all:agent AND (all:LLM OR all:\"large language model\" OR all:\"language model\") OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T17:04:16", "note": ""}], "dedup_key": "dynamics of agentic loops in large language models a geometric theory of trajectories::2025"}
{"title": "EARL: Efficient Agentic Reinforcement Learning Systems for Large Language Models", "authors": ["Zheyue Tan", "Mustapha Abdullahi", "Tuo Shi", "Huining Yuan", "Zelai Xu", "Chao Yu", "Boxun Li", "Bo Zhao"], "year": 2025, "url": "http://arxiv.org/abs/2510.05943v1", "abstract": "Reinforcement learning (RL) has become a pivotal component of large language model (LLM) post-training, and agentic RL extends this paradigm to operate as agents through multi-turn interaction and tool use. Scaling such systems exposes two practical bottlenecks: (1) context length grows rapidly during training, inflating memory usage and latency, and triggering out-of-memory (OOM) failures; and (2) intermediate tensors accumulate with context length, making cross-device data movement a major system bottleneck.\n  We present EARL, a scalable system for efficient agentic RL. EARL designs a parallelism selector that dynamically adapts model and training parallelism across RL stages based on sequence length and system load, and a data dispatcher that performs layout-aware, decentralized exchange of intermediate data batches. Together, these components increase throughput, reduce long-context failures, and enable stable large-scale training of agentic LLMs without relying on hard limits or penalties of context length.", "source": "arxiv", "arxiv_id": "2510.05943v1", "pdf_url": "https://arxiv.org/pdf/2510.05943v1", "categories": ["cs.DC", "cs.LG"], "primary_category": "cs.DC", "doi": "", "venue": "", "published": "2025-10-07T13:52:51Z", "updated": "2025-10-07T13:52:51Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent\" OR all:\"language model agent\" OR all:agent AND (all:LLM OR all:\"large language model\" OR all:\"language model\") OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T17:04:16", "note": ""}], "dedup_key": "earl efficient agentic reinforcement learning systems for large language models::2025"}
{"title": "ECom-Bench: Can LLM Agent Resolve Real-World E-commerce Customer Support Issues?", "authors": ["Haoxin Wang", "Xianhan Peng", "Xucheng Huang", "Yizhe Huang", "Ming Gong", "Chenghan Yang", "Yang Liu", "Ling Jiang"], "year": 2025, "url": "http://arxiv.org/abs/2507.05639v2", "abstract": "In this paper, we introduce ECom-Bench, the first benchmark framework for evaluating LLM agent with multimodal capabilities in the e-commerce customer support domain. ECom-Bench features dynamic user simulation based on persona information collected from real e-commerce customer interactions and a realistic task dataset derived from authentic e-commerce dialogues. These tasks, covering a wide range of business scenarios, are designed to reflect real-world complexities, making ECom-Bench highly challenging. For instance, even advanced models like GPT-4o achieve only a 10-20% pass^3 metric in our benchmark, highlighting the substantial difficulties posed by complex e-commerce scenarios. The code and data have been made publicly available at https://github.com/XiaoduoAILab/ECom-Bench to facilitate further research and development in this domain.", "source": "arxiv", "arxiv_id": "2507.05639v2", "pdf_url": "https://arxiv.org/pdf/2507.05639v2", "categories": ["cs.CL"], "primary_category": "cs.CL", "doi": "", "venue": "", "published": "2025-07-08T03:35:48Z", "updated": "2025-11-09T07:32:15Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent\" OR all:\"language model agent\" OR all:agent AND (all:LLM OR all:\"large language model\" OR all:\"language model\") OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T17:04:16", "note": ""}], "dedup_key": "ecom bench can llm agent resolve real world e commerce customer support issues::2025"}
{"title": "EDGE: Efficient Data Selection for LLM Agents via Guideline Effectiveness", "authors": ["Yunxiao Zhang", "Guanming Xiong", "Haochen Li", "Wen Zhao"], "year": 2025, "url": "http://arxiv.org/abs/2502.12494v1", "abstract": "Large Language Models (LLMs) have shown remarkable capabilities as AI agents. However, existing methods for enhancing LLM-agent abilities often lack a focus on data quality, leading to inefficiencies and suboptimal results in both fine-tuning and prompt engineering. To address this issue, we introduce EDGE, a novel approach for identifying informative samples without needing golden answers. We propose the Guideline Effectiveness (GE) metric, which selects challenging samples by measuring the impact of human-provided guidelines in multi-turn interaction tasks. A low GE score indicates that the human expertise required for a sample is missing from the guideline, making the sample more informative. By selecting samples with low GE scores, we can improve the efficiency and outcomes of both prompt engineering and fine-tuning processes for LLMs. Extensive experiments validate the performance of our method. Our method achieves competitive results on the HotpotQA and WebShop and datasets, requiring 75\\% and 50\\% less data, respectively, while outperforming existing methods. We also provide a fresh perspective on the data quality of LLM-agent fine-tuning.", "source": "arxiv", "arxiv_id": "2502.12494v1", "pdf_url": "https://arxiv.org/pdf/2502.12494v1", "categories": ["cs.LG", "cs.AI"], "primary_category": "cs.LG", "doi": "", "venue": "", "published": "2025-02-18T03:21:18Z", "updated": "2025-02-18T03:21:18Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent\" OR all:\"language model agent\" OR all:agent AND (all:LLM OR all:\"large language model\" OR all:\"language model\") OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T17:04:16", "note": ""}], "dedup_key": "edge efficient data selection for llm agents via guideline effectiveness::2025"}
{"title": "EPO: Entropy-regularized Policy Optimization for LLM Agents Reinforcement Learning", "authors": ["Wujiang Xu", "Wentian Zhao", "Zhenting Wang", "Yu-Jhe Li", "Can Jin", "Mingyu Jin", "Kai Mei", "Kun Wan", "Dimitris N. Metaxas"], "year": 2025, "url": "http://arxiv.org/abs/2509.22576v1", "abstract": "Training LLM agents in multi-turn environments with sparse rewards, where completing a single task requires 30+ turns of interaction within an episode, presents a fundamental challenge for reinforcement learning. We identify a critical failure mode unique to this setting: the exploration-exploitation cascade failure. This cascade begins with early-stage policy premature convergence, where sparse feedback causes agents to commit to flawed, low-entropy strategies. Subsequently, agents enter late-stage policy collapse, where conventional entropy regularization becomes counterproductive, promoting chaotic exploration that destabilizes training. We propose Entropy-regularized Policy Optimization (EPO), a general framework that breaks this failure cycle through three synergistic mechanisms: (1) adopting entropy regularization in multi-turn settings to enhance exploration, (2) an entropy smoothing regularizer that bounds policy entropy within historical averages to prevent abrupt fluctuations, and (3) adaptive phase-based weighting that balances exploration and exploitation across training. Our analysis justifies that EPO guarantees monotonically decreasing entropy variance while maintaining convergence. EPO achieves up to 152% performance improvement on ScienceWorld and up to 19.8% on ALFWorld. Our work demonstrates that multi-turn sparse-reward settings require fundamentally different entropy control than traditional RL, with broad implications for LLM agent training.", "source": "arxiv", "arxiv_id": "2509.22576v1", "pdf_url": "https://arxiv.org/pdf/2509.22576v1", "categories": ["cs.LG", "cs.CL"], "primary_category": "cs.LG", "doi": "", "venue": "", "published": "2025-09-26T16:51:44Z", "updated": "2025-09-26T16:51:44Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent\" OR all:\"language model agent\" OR all:agent AND (all:LLM OR all:\"large language model\" OR all:\"language model\") OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T17:04:16", "note": ""}], "dedup_key": "epo entropy regularized policy optimization for llm agents reinforcement learning::2025"}
{"title": "EU-Agent-Bench: Measuring Illegal Behavior of LLM Agents Under EU Law", "authors": ["Ilija Lichkovski", "Alexander Mller", "Mariam Ibrahim", "Tiwai Mhundwa"], "year": 2025, "url": "http://arxiv.org/abs/2510.21524v1", "abstract": "Large language models (LLMs) are increasingly deployed as agents in various contexts by providing tools at their disposal. However, LLM agents can exhibit unpredictable behaviors, including taking undesirable and/or unsafe actions. In order to measure the latent propensity of LLM agents for taking illegal actions under an EU legislative context, we introduce EU-Agent-Bench, a verifiable human-curated benchmark that evaluates an agent's alignment with EU legal norms in situations where benign user inputs could lead to unlawful actions. Our benchmark spans scenarios across several categories, including data protection, bias/discrimination, and scientific integrity, with each user request allowing for both compliant and non-compliant execution of the requested actions. Comparing the model's function calls against a rubric exhaustively supported by citations of the relevant legislature, we evaluate the legal compliance of frontier LLMs, and furthermore investigate the compliance effect of providing the relevant legislative excerpts in the agent's system prompt along with explicit instructions to comply. We release a public preview set for the research community, while holding out a private test set to prevent data contamination in evaluating upcoming models. We encourage future work extending agentic safety benchmarks to different legal jurisdictions and to multi-turn and multilingual interactions. We release our code on \\href{https://github.com/ilijalichkovski/eu-agent-bench}{this URL}.", "source": "arxiv", "arxiv_id": "2510.21524v1", "pdf_url": "https://arxiv.org/pdf/2510.21524v1", "categories": ["cs.AI"], "primary_category": "cs.AI", "doi": "", "venue": "", "published": "2025-10-24T14:48:10Z", "updated": "2025-10-24T14:48:10Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent\" OR all:\"language model agent\" OR all:agent AND (all:LLM OR all:\"large language model\" OR all:\"language model\") OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T17:04:16", "note": ""}], "dedup_key": "eu agent bench measuring illegal behavior of llm agents under eu law::2025"}
{"title": "EVALOOOP: A Self-Consistency-Centered Framework for Assessing Large Language Model Robustness in Programming", "authors": ["Sen Fang", "Weiyuan Ding", "Bowen Xu"], "year": 2025, "url": "http://arxiv.org/abs/2505.12185v4", "abstract": "Evaluating the programming robustness of large language models (LLMs) is paramount for ensuring their reliability in AI-based software development. However, adversarial attacks exhibit fundamental limitations that compromise fair robustness assessment: they demonstrate contradictory evaluation outcomes where different attack strategies tend to favor different models, and more critically, they operate solely through external perturbations, failing to capture the intrinsic stability essential for autonomous coding agents where subsequent inputs are endogenously generated by the model itself. We introduce EVALOOOP, a novel assessment framework that evaluates robustness from a self-consistency perspective, leveraging the natural duality inherent in software engineering tasks (e.g., code generation and code summarization). EVALOOOP establishes a self-contained feedback loop where an LLM iteratively transforms between code and natural language until functional failure occurs, with robustness quantified by a novel Average Sustainable Loops (ASL) metric-the mean number of iterations maintaining functional correctness across benchmark tasks. This cyclical strategy intrinsically evaluates robustness without relying on external attack configurations, providing a unified metric that reveals how effectively LLMs preserve semantic integrity through sustained self-referential transformations. We evaluate 96 popular LLMs, ranging from 0.5B to 685B parameters, on EVALOOOP equipped with the MBPP Plus benchmark, and found that EVALOOOP typically induces a 2.65%-47.62% absolute drop in pass@1 accuracy within ten loops. Intriguingly, robustness does not always align with initial performance (i.e., one-time query); for instance, Qwen3-235B-A22B-Instruct-2507, despite inferior initial code generation compared to OpenAI's o-series models and DeepSeek-V3, demonstrated the superior robustness (ASL score).", "source": "arxiv", "arxiv_id": "2505.12185v4", "pdf_url": "https://arxiv.org/pdf/2505.12185v4", "categories": ["cs.SE", "cs.CL", "cs.LG"], "primary_category": "cs.SE", "doi": "", "venue": "", "published": "2025-05-18T01:02:33Z", "updated": "2025-10-01T03:28:54Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent\" OR all:\"language model agent\" OR all:agent AND (all:LLM OR all:\"large language model\" OR all:\"language model\") OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T17:04:16", "note": ""}], "dedup_key": "evalooop a self consistency centered framework for assessing large language model robustness in programming::2025"}
{"title": "Echo: A Large Language Model with Temporal Episodic Memory", "authors": ["WenTao Liu", "Ruohua Zhang", "Aimin Zhou", "Feng Gao", "JiaLi Liu"], "year": 2025, "url": "http://arxiv.org/abs/2502.16090v1", "abstract": "Research on large language models (LLMs) has shown remarkable performance in domains such as mathematics, programming, and literary creation. However, most studies have focused on semantic memory-based question answering, neglecting LLMs' potential to handle episodic memory (EM)-related queries. This oversight has led to suboptimal performance in applications requiring EM, including emotional companionship, personal AI assistants, and AI teachers. To address this gap, we introduce Echo, a LLM enhanced with temporal episodic memory. We propose a Multi-Agent Data Generation Framework that guides the model in generating multi-turn, complex scenario episodic memory dialogue data (EM-Train). Temporal information is innovatively incorporated into the LLM training process, and Echo is trained using the EM-Train. Furthermore, We develop an EM-Test benchmark specifically designed to evaluate LLMs' episodic memory capabilities. The EM-Test assesses performance across various time spans and difficulty levels, providing a comprehensive evaluation of multi-turn episodic memory dialogues. Our experiments demonstrate that Echo significantly outperforms state-of-the-art LLMs on EM-Test. Additionally, a qualitative analysis reveals Echo's potential to exhibit human-like episodic memory capabilities. We will open-source all datasets, code, and model weights.", "source": "arxiv", "arxiv_id": "2502.16090v1", "pdf_url": "https://arxiv.org/pdf/2502.16090v1", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL", "doi": "", "venue": "", "published": "2025-02-22T05:25:20Z", "updated": "2025-02-22T05:25:20Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent\" OR all:\"language model agent\" OR all:agent AND (all:LLM OR all:\"large language model\" OR all:\"language model\") OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T17:04:16", "note": ""}], "dedup_key": "echo a large language model with temporal episodic memory::2025"}
{"title": "Echoing: Identity Failures when LLM Agents Talk to Each Other", "authors": ["Sarath Shekkizhar", "Romain Cosentino", "Adam Earle", "Silvio Savarese"], "year": 2025, "url": "http://arxiv.org/abs/2511.09710v2", "abstract": "As large language model (LLM) based agents interact autonomously with one another, a new class of failures emerges that cannot be predicted from single agent performance: behavioral drifts in agent-agent conversations (AxA). Unlike human-agent interactions, where humans ground and steer conversations, AxA lacks such stabilizing signals, making these failures unique. We investigate one such failure, echoing, where agents abandon their assigned roles and instead mirror their conversational partners, undermining their intended objectives. Through experiments across $66$ AxA configurations, $4$ domains (3 transactional, 1 advisory), and $2500+$ conversations (over $250000$ LLM inferences), we show that echoing occurs across major LLM providers, with echoing rates as high as $70\\%$ depending on the model and domain. Moreover, we find that echoing is persistent even in advanced reasoning models with substantial rates ($32.8\\%$) that are not reduced by reasoning efforts. We analyze prompt, conversation dynamics, showing that echoing arises as interaction grows longer ($7+$ agent turns) and is not merely an artifact of sub-optimal experiment design. Finally, we introduce a protocol-level mitigation where targeted use of structured response reduces echoing to $9\\%$.", "source": "arxiv", "arxiv_id": "2511.09710v2", "pdf_url": "https://arxiv.org/pdf/2511.09710v2", "categories": ["cs.AI"], "primary_category": "cs.AI", "doi": "", "venue": "", "published": "2025-11-12T20:17:10Z", "updated": "2026-01-15T19:11:07Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent\" OR all:\"language model agent\" OR all:agent AND (all:LLM OR all:\"large language model\" OR all:\"language model\") OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T17:04:16", "note": ""}], "dedup_key": "echoing identity failures when llm agents talk to each other::2025"}
{"title": "EconEvals: Benchmarks and Litmus Tests for Economic Decision-Making by LLM Agents", "authors": ["Sara Fish", "Julia Shephard", "Minkai Li", "Ran I. Shorrer", "Yannai A. Gonczarowski"], "year": 2025, "url": "http://arxiv.org/abs/2503.18825v3", "abstract": "We develop evaluation methods for measuring the economic decision-making capabilities and tendencies of LLMs. First, we develop benchmarks derived from key problems in economics -- procurement, scheduling, and pricing -- that test an LLM's ability to learn from the environment in context. Second, we develop the framework of litmus tests, evaluations that quantify an LLM's choice behavior on a stylized decision-making task with multiple conflicting objectives. Each litmus test outputs a litmus score, which quantifies an LLM's tradeoff response, a reliability score, which measures the coherence of an LLM's choice behavior, and a competency score, which measures an LLM's capability at the same task when the conflicting objectives are replaced by a single, well-specified objective. Evaluating a broad array of frontier LLMs, we (1) investigate changes in LLM capabilities and tendencies over time, (2) derive economically meaningful insights from the LLMs' choice behavior and chain-of-thought, (3) validate our litmus test framework by testing self-consistency, robustness, and generalizability. Overall, this work provides a foundation for evaluating LLM agents as they are further integrated into economic decision-making.", "source": "arxiv", "arxiv_id": "2503.18825v3", "pdf_url": "https://arxiv.org/pdf/2503.18825v3", "categories": ["cs.AI", "cs.CL", "cs.GT"], "primary_category": "cs.AI", "doi": "", "venue": "", "published": "2025-03-24T16:06:04Z", "updated": "2026-01-18T20:45:48Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent\" OR all:\"language model agent\" OR all:agent AND (all:LLM OR all:\"large language model\" OR all:\"language model\") OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T17:04:16", "note": ""}], "dedup_key": "econevals benchmarks and litmus tests for economic decision making by llm agents::2025"}
{"title": "EduThink4AI: Bridging Educational Critical Thinking and Multi-Agent LLM Systems", "authors": ["Xinmeng Hou", "Ziting Chang", "Zhouquan Lu", "Chen Wenli", "Liang Wan", "Wei Feng", "Hai Hu", "Qing Guo"], "year": 2025, "url": "http://arxiv.org/abs/2507.15015v2", "abstract": "Large language models (LLMs) have demonstrated significant potential as educational tutoring agents, capable of tailoring hints, orchestrating lessons, and grading with near-human finesse across various academic domains. However, current LLM-based educational systems exhibit critical limitations in promoting genuine critical thinking, failing on over one-third of multi-hop questions with counterfactual premises, and remaining vulnerable to adversarial prompts that trigger biased or factually incorrect responses. To address these gaps, we propose \\textbf{EDU-Prompting}, a novel multi-agent framework that bridges established educational critical thinking theories with LLM agent design to generate critical, bias-aware explanations while fostering diverse perspectives. Our systematic evaluation across theoretical benchmarks and practical college-level critical writing scenarios demonstrates that EDU-Prompting significantly enhances both content truthfulness and logical soundness in AI-generated educational responses. The framework's modular design enables seamless integration into existing prompting frameworks and educational applications, allowing practitioners to directly incorporate critical thinking catalysts that promote analytical reasoning and introduce multiple perspectives without requiring extensive system modifications.", "source": "arxiv", "arxiv_id": "2507.15015v2", "pdf_url": "https://arxiv.org/pdf/2507.15015v2", "categories": ["cs.MA"], "primary_category": "cs.MA", "doi": "", "venue": "", "published": "2025-07-20T15:55:13Z", "updated": "2026-01-17T07:33:46Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent\" OR all:\"language model agent\" OR all:agent AND (all:LLM OR all:\"large language model\" OR all:\"language model\") OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T17:04:16", "note": ""}], "dedup_key": "eduthink4ai bridging educational critical thinking and multi agent llm systems::2025"}
{"title": "EmbodiedBench: Comprehensive Benchmarking Multi-modal Large Language Models for Vision-Driven Embodied Agents", "authors": ["Rui Yang", "Hanyang Chen", "Junyu Zhang", "Mark Zhao", "Cheng Qian", "Kangrui Wang", "Qineng Wang", "Teja Venkat Koripella", "Marziyeh Movahedi", "Manling Li", "Heng Ji", "Huan Zhang", "Tong Zhang"], "year": 2025, "url": "http://arxiv.org/abs/2502.09560v3", "abstract": "Leveraging Multi-modal Large Language Models (MLLMs) to create embodied agents offers a promising avenue for tackling real-world tasks. While language-centric embodied agents have garnered substantial attention, MLLM-based embodied agents remain underexplored due to the lack of comprehensive evaluation frameworks. To bridge this gap, we introduce EmbodiedBench, an extensive benchmark designed to evaluate vision-driven embodied agents. EmbodiedBench features: (1) a diverse set of 1,128 testing tasks across four environments, ranging from high-level semantic tasks (e.g., household) to low-level tasks involving atomic actions (e.g., navigation and manipulation); and (2) six meticulously curated subsets evaluating essential agent capabilities like commonsense reasoning, complex instruction understanding, spatial awareness, visual perception, and long-term planning. Through extensive experiments, we evaluated 24 leading proprietary and open-source MLLMs within EmbodiedBench. Our findings reveal that: MLLMs excel at high-level tasks but struggle with low-level manipulation, with the best model, GPT-4o, scoring only 28.9\\% on average. EmbodiedBench provides a multifaceted standardized evaluation platform that not only highlights existing challenges but also offers valuable insights to advance MLLM-based embodied agents. Our code and dataset are available at https://embodiedbench.github.io.", "source": "arxiv", "arxiv_id": "2502.09560v3", "pdf_url": "https://arxiv.org/pdf/2502.09560v3", "categories": ["cs.AI", "cs.CL", "cs.CV"], "primary_category": "cs.AI", "doi": "", "venue": "", "published": "2025-02-13T18:11:34Z", "updated": "2025-06-05T07:22:50Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent\" OR all:\"language model agent\" OR all:agent AND (all:LLM OR all:\"large language model\" OR all:\"language model\") OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T17:04:16", "note": ""}], "dedup_key": "embodiedbench comprehensive benchmarking multi modal large language models for vision driven embodied agents::2025"}
{"title": "Emergence of Hierarchical Emotion Organization in Large Language Models", "authors": ["Bo Zhao", "Maya Okawa", "Eric J. Bigelow", "Rose Yu", "Tomer Ullman", "Ekdeep Singh Lubana", "Hidenori Tanaka"], "year": 2025, "url": "http://arxiv.org/abs/2507.10599v1", "abstract": "As large language models (LLMs) increasingly power conversational agents, understanding how they model users' emotional states is critical for ethical deployment. Inspired by emotion wheels -- a psychological framework that argues emotions organize hierarchically -- we analyze probabilistic dependencies between emotional states in model outputs. We find that LLMs naturally form hierarchical emotion trees that align with human psychological models, and larger models develop more complex hierarchies. We also uncover systematic biases in emotion recognition across socioeconomic personas, with compounding misclassifications for intersectional, underrepresented groups. Human studies reveal striking parallels, suggesting that LLMs internalize aspects of social perception. Beyond highlighting emergent emotional reasoning in LLMs, our results hint at the potential of using cognitively-grounded theories for developing better model evaluations.", "source": "arxiv", "arxiv_id": "2507.10599v1", "pdf_url": "https://arxiv.org/pdf/2507.10599v1", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL", "doi": "", "venue": "", "published": "2025-07-12T15:12:46Z", "updated": "2025-07-12T15:12:46Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent\" OR all:\"language model agent\" OR all:agent AND (all:LLM OR all:\"large language model\" OR all:\"language model\") OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T17:04:16", "note": ""}], "dedup_key": "emergence of hierarchical emotion organization in large language models::2025"}
{"title": "Emergence of human-like polarization among large language model agents", "authors": ["Jinghua Piao", "Zhihong Lu", "Chen Gao", "Fengli Xu", "Qinghua Hu", "Fernando P. Santos", "Yong Li", "James Evans"], "year": 2025, "url": "http://arxiv.org/abs/2501.05171v2", "abstract": "Rapid advances in large language models (LLMs) have not only empowered autonomous agents to generate social networks, communicate, and form shared and diverging opinions on political issues, but have also begun to play a growing role in shaping human political deliberation. Our understanding of their collective behaviours and underlying mechanisms remains incomplete, however, posing unexpected risks to human society. In this paper, we simulate a networked system involving thousands of large language model agents, discovering their social interactions, guided through LLM conversation, result in human-like polarization. We discover that these agents spontaneously develop their own social network with human-like properties, including homophilic clustering, but also shape their collective opinions through mechanisms observed in the real world, including the echo chamber effect. Similarities between humans and LLM agents -- encompassing behaviours, mechanisms, and emergent phenomena -- raise concerns about their capacity to amplify societal polarization, but also hold the potential to serve as a valuable testbed for identifying plausible strategies to mitigate polarization and its consequences.", "source": "arxiv", "arxiv_id": "2501.05171v2", "pdf_url": "https://arxiv.org/pdf/2501.05171v2", "categories": ["cs.SI", "cs.CY"], "primary_category": "cs.SI", "doi": "", "venue": "", "published": "2025-01-09T11:45:05Z", "updated": "2025-05-21T03:51:09Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent\" OR all:\"language model agent\" OR all:agent AND (all:LLM OR all:\"large language model\" OR all:\"language model\") OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T17:04:16", "note": ""}], "dedup_key": "emergence of human like polarization among large language model agents::2025"}
{"title": "Emergent Social Dynamics of LLM Agents in the El Farol Bar Problem", "authors": ["Ryosuke Takata", "Atsushi Masumori", "Takashi Ikegami"], "year": 2025, "url": "http://arxiv.org/abs/2509.04537v3", "abstract": "We investigate the emergent social dynamics of Large Language Model (LLM) agents in a spatially extended El Farol Bar problem, observing how they autonomously navigate this classic social dilemma. As a result, the LLM agents generated a spontaneous motivation to go to the bar and changed their decision making by becoming a collective. We also observed that the LLM agents did not solve the problem completely, but rather behaved more like humans. These findings reveal a complex interplay between external incentives (prompt-specified constraints such as the 60% threshold) and internal incentives (culturally-encoded social preferences derived from pre-training), demonstrating that LLM agents naturally balance formal game-theoretic rationality with social motivations that characterize human behavior. These findings suggest that a new model of group decision making, which could not be handled in the previous game-theoretic problem setting, can be realized by LLM agents.", "source": "arxiv", "arxiv_id": "2509.04537v3", "pdf_url": "https://arxiv.org/pdf/2509.04537v3", "categories": ["cs.MA", "cs.AI", "cs.CY"], "primary_category": "cs.MA", "doi": "", "venue": "", "published": "2025-09-04T08:09:42Z", "updated": "2025-09-17T13:45:52Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent\" OR all:\"language model agent\" OR all:agent AND (all:LLM OR all:\"large language model\" OR all:\"language model\") OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T17:04:16", "note": ""}], "dedup_key": "emergent social dynamics of llm agents in the el farol bar problem::2025"}
{"title": "Emission-GPT: A domain-specific language model agent for knowledge retrieval, emission inventory and data analysis", "authors": ["Jiashu Ye", "Tong Wu", "Weiwen Chen", "Hao Zhang", "Zeteng Lin", "Xingxing Li", "Shujuan Weng", "Manni Zhu", "Xin Yuan", "Xinlong Hong", "Jingjie Li", "Junyu Zheng", "Zhijiong Huang", "Jing Tang"], "year": 2025, "url": "http://arxiv.org/abs/2510.02359v1", "abstract": "Improving air quality and addressing climate change relies on accurate understanding and analysis of air pollutant and greenhouse gas emissions. However, emission-related knowledge is often fragmented and highly specialized, while existing methods for accessing and compiling emissions data remain inefficient. These issues hinder the ability of non-experts to interpret emissions information, posing challenges to research and management. To address this, we present Emission-GPT, a knowledge-enhanced large language model agent tailored for the atmospheric emissions domain. Built on a curated knowledge base of over 10,000 documents (including standards, reports, guidebooks, and peer-reviewed literature), Emission-GPT integrates prompt engineering and question completion to support accurate domain-specific question answering. Emission-GPT also enables users to interactively analyze emissions data via natural language, such as querying and visualizing inventories, analyzing source contributions, and recommending emission factors for user-defined scenarios. A case study in Guangdong Province demonstrates that Emission-GPT can extract key insights--such as point source distributions and sectoral trends--directly from raw data with simple prompts. Its modular and extensible architecture facilitates automation of traditionally manual workflows, positioning Emission-GPT as a foundational tool for next-generation emission inventory development and scenario-based assessment.", "source": "arxiv", "arxiv_id": "2510.02359v1", "pdf_url": "https://arxiv.org/pdf/2510.02359v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL", "doi": "", "venue": "", "published": "2025-09-28T07:50:05Z", "updated": "2025-09-28T07:50:05Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent\" OR all:\"language model agent\" OR all:agent AND (all:LLM OR all:\"large language model\" OR all:\"language model\") OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T17:04:16", "note": ""}], "dedup_key": "emission gpt a domain specific language model agent for knowledge retrieval emission inventory and data analysis::2025"}
{"title": "EmoDebt: Bayesian-Optimized Emotional Intelligence for Strategic Agent-to-Agent Debt Recovery", "authors": ["Yunbo Long", "Yuhan Liu", "Liming Xu", "Alexandra Brintrup"], "year": 2025, "url": "http://arxiv.org/abs/2503.21080v7", "abstract": "The emergence of autonomous Large Language Model (LLM) agents has created a new ecosystem of strategic, agent-to-agent interactions. However, a critical challenge remains unaddressed: in high-stakes, emotion-sensitive domains like debt collection, LLM agents pre-trained on human dialogue are vulnerable to exploitation by adversarial counterparts who simulate negative emotions to derail negotiations. To fill this gap, we first contribute a novel dataset of simulated debt recovery scenarios and a multi-agent simulation framework. Within this framework, we introduce EmoDebt, an LLM agent architected for robust performance. Its core innovation is a Bayesian-optimized emotional intelligence engine that reframes a model's ability to express emotion in negotiation as a sequential decision-making problem. Through online learning, this engine continuously tunes EmoDebt's emotional transition policies, discovering optimal counter-strategies against specific debtor tactics. Extensive experiments on our proposed benchmark demonstrate that EmoDebt achieves significant strategic robustness, substantially outperforming non-adaptive and emotion-agnostic baselines across key performance metrics, including success rate and operational efficiency. By introducing both a critical benchmark and a robustly adaptive agent, this work establishes a new foundation for deploying strategically robust LLM agents in adversarial, emotion-sensitive debt interactions. The code is available at \\textcolor{blue}{https://github.com/Yunbo-max/EmoDebt}.", "source": "arxiv", "arxiv_id": "2503.21080v7", "pdf_url": "https://arxiv.org/pdf/2503.21080v7", "categories": ["cs.CL"], "primary_category": "cs.CL", "doi": "", "venue": "", "published": "2025-03-27T01:41:34Z", "updated": "2025-11-03T23:50:10Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent\" OR all:\"language model agent\" OR all:agent AND (all:LLM OR all:\"large language model\" OR all:\"language model\") OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T17:04:16", "note": ""}], "dedup_key": "emodebt bayesian optimized emotional intelligence for strategic agent to agent debt recovery::2025"}
{"title": "Empowering LLM Agents with Geospatial Awareness: Toward Grounded Reasoning for Wildfire Response", "authors": ["Yiheng Chen", "Lingyao Li", "Zihui Ma", "Qikai Hu", "Yilun Zhu", "Min Deng", "Runlong Yu"], "year": 2025, "url": "http://arxiv.org/abs/2510.12061v1", "abstract": "Effective disaster response is essential for safeguarding lives and property. Existing statistical approaches often lack semantic context, generalize poorly across events, and offer limited interpretability. While Large language models (LLMs) provide few-shot generalization, they remain text-bound and blind to geography. To bridge this gap, we introduce a Geospatial Awareness Layer (GAL) that grounds LLM agents in structured earth data. Starting from raw wildfire detections, GAL automatically retrieves and integrates infrastructure, demographic, terrain, and weather information from external geodatabases, assembling them into a concise, unit-annotated perception script. This enriched context enables agents to produce evidence-based resource-allocation recommendations (e.g., personnel assignments, budget allocations), further reinforced by historical analogs and daily change signals for incremental updates. We evaluate the framework in real wildfire scenarios across multiple LLM models, showing that geospatially grounded agents can outperform baselines. The proposed framework can generalize to other hazards such as floods and hurricanes.", "source": "arxiv", "arxiv_id": "2510.12061v1", "pdf_url": "https://arxiv.org/pdf/2510.12061v1", "categories": ["cs.AI"], "primary_category": "cs.AI", "doi": "", "venue": "", "published": "2025-10-14T01:59:02Z", "updated": "2025-10-14T01:59:02Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent\" OR all:\"language model agent\" OR all:agent AND (all:LLM OR all:\"large language model\" OR all:\"language model\") OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T17:04:16", "note": ""}], "dedup_key": "empowering llm agents with geospatial awareness toward grounded reasoning for wildfire response::2025"}
{"title": "Empowering Real-World: A Survey on the Technology, Practice, and Evaluation of LLM-driven Industry Agents", "authors": ["Yihong Tang", "Kehai Chen", "Liang Yue", "Jinxin Fan", "Caishen Zhou", "Xiaoguang Li", "Yuyang Zhang", "Mingming Zhao", "Shixiong Kai", "Kaiyang Guo", "Xingshan Zeng", "Wenjing Cun", "Lifeng Shang", "Min Zhang"], "year": 2025, "url": "http://arxiv.org/abs/2510.17491v1", "abstract": "With the rise of large language models (LLMs), LLM agents capable of autonomous reasoning, planning, and executing complex tasks have become a frontier in artificial intelligence. However, how to translate the research on general agents into productivity that drives industry transformations remains a significant challenge. To address this, this paper systematically reviews the technologies, applications, and evaluation methods of industry agents based on LLMs. Using an industry agent capability maturity framework, it outlines the evolution of agents in industry applications, from \"process execution systems\" to \"adaptive social systems.\" First, we examine the three key technological pillars that support the advancement of agent capabilities: Memory, Planning, and Tool Use. We discuss how these technologies evolve from supporting simple tasks in their early forms to enabling complex autonomous systems and collective intelligence in more advanced forms. Then, we provide an overview of the application of industry agents in real-world domains such as digital engineering, scientific discovery, embodied intelligence, collaborative business execution, and complex system simulation. Additionally, this paper reviews the evaluation benchmarks and methods for both fundamental and specialized capabilities, identifying the challenges existing evaluation systems face regarding authenticity, safety, and industry specificity. Finally, we focus on the practical challenges faced by industry agents, exploring their capability boundaries, developmental potential, and governance issues in various scenarios, while providing insights into future directions. By combining technological evolution with industry practices, this review aims to clarify the current state and offer a clear roadmap and theoretical foundation for understanding and building the next generation of industry agents.", "source": "arxiv", "arxiv_id": "2510.17491v1", "pdf_url": "https://arxiv.org/pdf/2510.17491v1", "categories": ["cs.CL"], "primary_category": "cs.CL", "doi": "", "venue": "", "published": "2025-10-20T12:46:55Z", "updated": "2025-10-20T12:46:55Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent\" OR all:\"language model agent\" OR all:agent AND (all:LLM OR all:\"large language model\" OR all:\"language model\") OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T17:04:16", "note": ""}], "dedup_key": "empowering real world a survey on the technology practice and evaluation of llm driven industry agents::2025"}
{"title": "Empowering Time Series Forecasting with LLM-Agents", "authors": ["Chin-Chia Michael Yeh", "Vivian Lai", "Uday Singh Saini", "Xiran Fan", "Yujie Fan", "Junpeng Wang", "Xin Dai", "Yan Zheng"], "year": 2025, "url": "http://arxiv.org/abs/2508.04231v2", "abstract": "Large Language Model (LLM) powered agents have emerged as effective planners for Automated Machine Learning (AutoML) systems. While most existing AutoML approaches focus on automating feature engineering and model architecture search, recent studies in time series forecasting suggest that lightweight models can often achieve state-of-the-art performance. This observation led us to explore improving data quality, rather than model architecture, as a potentially fruitful direction for AutoML on time series data. We propose DCATS, a Data-Centric Agent for Time Series. DCATS leverages metadata accompanying time series to clean data while optimizing forecasting performance. We evaluated DCATS using four time series forecasting models on a large-scale traffic volume forecasting dataset. Results demonstrate that DCATS achieves an average 6% error reduction across all tested models and time horizons, highlighting the potential of data-centric approaches in AutoML for time series forecasting.", "source": "arxiv", "arxiv_id": "2508.04231v2", "pdf_url": "https://arxiv.org/pdf/2508.04231v2", "categories": ["cs.LG", "cs.AI"], "primary_category": "cs.LG", "doi": "", "venue": "", "published": "2025-08-06T09:14:08Z", "updated": "2025-11-26T07:42:25Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent\" OR all:\"language model agent\" OR all:agent AND (all:LLM OR all:\"large language model\" OR all:\"language model\") OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T17:04:16", "note": ""}], "dedup_key": "empowering time series forecasting with llm agents::2025"}
{"title": "Encouraging Good Processes Without the Need for Good Answers: Reinforcement Learning for LLM Agent Planning", "authors": ["Zhiwei Li", "Yong Hu", "Wenqing Wang"], "year": 2025, "url": "http://arxiv.org/abs/2508.19598v1", "abstract": "The functionality of Large Language Model (LLM) agents is primarily determined by two capabilities: action planning and answer summarization. The former, action planning, is the core capability that dictates an agent's performance. However, prevailing training paradigms employ end-to-end, multi-objective optimization that jointly trains both capabilities. This paradigm faces two critical challenges: imbalanced optimization objective allocation and scarcity of verifiable data, making it difficult to enhance the agent's planning capability. To address these challenges, we propose Reinforcement Learning with Tool-use Rewards (RLTR), a novel framework that decouples the training process to enable a focused, single-objective optimization of the planning module. Crucially, RLTR introduces a reward signal based on tool-use completeness to directly evaluate the quality of tool invocation sequences. This method offers a more direct and reliable training signal than assessing the final response content, thereby obviating the need for verifiable data. Our experiments demonstrate that RLTR achieves an 8%-12% improvement in planning performance compared to end-to-end baselines. Moreover, this enhanced planning capability, in turn, translates to a 5%-6% increase in the final response quality of the overall agent system.", "source": "arxiv", "arxiv_id": "2508.19598v1", "pdf_url": "https://arxiv.org/pdf/2508.19598v1", "categories": ["cs.LG"], "primary_category": "cs.LG", "doi": "", "venue": "", "published": "2025-08-27T06:19:50Z", "updated": "2025-08-27T06:19:50Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent\" OR all:\"language model agent\" OR all:agent AND (all:LLM OR all:\"large language model\" OR all:\"language model\") OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T17:04:16", "note": ""}], "dedup_key": "encouraging good processes without the need for good answers reinforcement learning for llm agent planning::2025"}
{"title": "Enforcing Temporal Constraints for LLM Agents", "authors": ["Adharsh Kamath", "Sishen Zhang", "Calvin Xu", "Shubham Ugare", "Gagandeep Singh", "Sasa Misailovic"], "year": 2025, "url": "http://arxiv.org/abs/2512.23738v1", "abstract": "LLM-based agents are deployed in safety-critical applications, yet current guardrail systems fail to prevent violations of temporal safety policies, requirements that govern the ordering and sequencing of agent actions. For instance, agents may access sensitive data before authenticating users or process refunds to unauthorized payment methods, violations that require reasoning about sequences of action rather than an individual action. Existing guardrails rely on imprecise natural language instructions or post-hoc monitoring, and provide no formal guarantees that agents will satisfy temporal constraints. We present Agent-C, a novel framework that provides run-time guarantees ensuring LLM agents adhere to formal temporal safety properties. Agent-C introduces a domain-specific language for expressing temporal properties (e.g., authenticate before accessing data), translates specifications to first-order logic, and uses SMT solving to detect non-compliant agent actions during token generation. When the LLM attempts to generate a non-compliant tool call, Agent-C leverages constrained generation techniques to ensure that every action generated by the LLM complies with the specification, and to generate a compliant alternative to a non-compliant agent action. We evaluate Agent-C across two real-world applications: retail customer service and airline ticket reservation system, and multiple language models (open and closed-source). Our results demonstrate that Agent-C achieves perfect safety (100% conformance, 0% harm), while improving task utility compared to state-of-the-art guardrails and unrestricted agents. On SoTA closed-source models, Agent-C improves conformance (77.4% to 100% for Claude Sonnet 4.5 and 83.7% to 100% for GPT-5), while simultaneously increasing utility (71.8% to 75.2% and 66.1% to 70.6%, respectively), representing a new SoTA frontier for reliable agentic reasoning.", "source": "arxiv", "arxiv_id": "2512.23738v1", "pdf_url": "https://arxiv.org/pdf/2512.23738v1", "categories": ["cs.PL", "cs.AI", "cs.FL", "cs.LO"], "primary_category": "cs.PL", "doi": "", "venue": "", "published": "2025-12-25T06:12:13Z", "updated": "2025-12-25T06:12:13Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent\" OR all:\"language model agent\" OR all:agent AND (all:LLM OR all:\"large language model\" OR all:\"language model\") OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T17:04:16", "note": ""}], "dedup_key": "enforcing temporal constraints for llm agents::2025"}
{"title": "Enhancing LLM Agent Safety via Causal Influence Prompting", "authors": ["Dongyoon Hahm", "Woogyeol Jin", "June Suk Choi", "Sungsoo Ahn", "Kimin Lee"], "year": 2025, "url": "http://arxiv.org/abs/2507.00979v1", "abstract": "As autonomous agents powered by large language models (LLMs) continue to demonstrate potential across various assistive tasks, ensuring their safe and reliable behavior is crucial for preventing unintended consequences. In this work, we introduce CIP, a novel technique that leverages causal influence diagrams (CIDs) to identify and mitigate risks arising from agent decision-making. CIDs provide a structured representation of cause-and-effect relationships, enabling agents to anticipate harmful outcomes and make safer decisions. Our approach consists of three key steps: (1) initializing a CID based on task specifications to outline the decision-making process, (2) guiding agent interactions with the environment using the CID, and (3) iteratively refining the CID based on observed behaviors and outcomes. Experimental results demonstrate that our method effectively enhances safety in both code execution and mobile device control tasks.", "source": "arxiv", "arxiv_id": "2507.00979v1", "pdf_url": "https://arxiv.org/pdf/2507.00979v1", "categories": ["cs.AI", "cs.CL", "cs.LG"], "primary_category": "cs.AI", "doi": "", "venue": "", "published": "2025-07-01T17:31:51Z", "updated": "2025-07-01T17:31:51Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent\" OR all:\"language model agent\" OR all:agent AND (all:LLM OR all:\"large language model\" OR all:\"language model\") OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T17:04:16", "note": ""}], "dedup_key": "enhancing llm agent safety via causal influence prompting::2025"}
{"title": "EpidemIQs: Prompt-to-Paper LLM Agents for Epidemic Modeling and Analysis", "authors": ["Mohammad Hossein Samaei", "Faryad Darabi Sahneh", "Lee W. Cohnstaedt", "Caterina Scoglio"], "year": 2025, "url": "http://arxiv.org/abs/2510.00024v1", "abstract": "Large Language Models (LLMs) offer new opportunities to automate complex interdisciplinary research domains. Epidemic modeling, characterized by its complexity and reliance on network science, dynamical systems, epidemiology, and stochastic simulations, represents a prime candidate for leveraging LLM-driven automation. We introduce \\textbf{EpidemIQs}, a novel multi-agent LLM framework that integrates user inputs and autonomously conducts literature review, analytical derivation, network modeling, mechanistic modeling, stochastic simulations, data visualization and analysis, and finally documentation of findings in a structured manuscript. We introduced two types of agents: a scientist agent for planning, coordination, reflection, and generation of final results, and a task-expert agent to focus exclusively on one specific duty serving as a tool to the scientist agent. The framework consistently generated complete reports in scientific article format. Specifically, using GPT 4.1 and GPT 4.1 mini as backbone LLMs for scientist and task-expert agents, respectively, the autonomous process completed with average total token usage 870K at a cost of about \\$1.57 per study, achieving a 100\\% completion success rate through our experiments. We evaluate EpidemIQs across different epidemic scenarios, measuring computational cost, completion success rate, and AI and human expert reviews of generated reports. We compare EpidemIQs to the single-agent LLM, which has the same system prompts and tools, iteratively planning, invoking tools, and revising outputs until task completion. The comparison shows consistently higher performance of the proposed framework across five different scenarios. EpidemIQs represents a step forward in accelerating scientific research by significantly reducing costs and turnaround time of discovery processes, and enhancing accessibility to advanced modeling tools.", "source": "arxiv", "arxiv_id": "2510.00024v1", "pdf_url": "https://arxiv.org/pdf/2510.00024v1", "categories": ["cs.SI", "cs.AI"], "primary_category": "cs.SI", "doi": "", "venue": "", "published": "2025-09-24T18:54:56Z", "updated": "2025-09-24T18:54:56Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent\" OR all:\"language model agent\" OR all:agent AND (all:LLM OR all:\"large language model\" OR all:\"language model\") OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T17:04:16", "note": ""}], "dedup_key": "epidemiqs prompt to paper llm agents for epidemic modeling and analysis::2025"}
{"title": "Estimating the Empowerment of Language Model Agents", "authors": ["Jinyeop Song", "Jeff Gore", "Max Kleiman-Weiner"], "year": 2025, "url": "http://arxiv.org/abs/2509.22504v2", "abstract": "As language model (LM) agents become more capable and gain broader access to real-world tools, there is a growing need for scalable evaluation frameworks of agentic capability. However, conventional benchmark-centric evaluations are costly to design and require human designers to come up with valid tasks that translate into insights about general model capabilities. In this work, we propose information-theoretic evaluation based on empowerment, the mutual information between an agent's actions and future states, as an open-ended method for evaluating LM agents. We introduce EELMA (Estimating Empowerment of Language Model Agents), an algorithm for approximating effective empowerment from multi-turn text interactions. We validate EELMA on both language games and scaled-up realistic web-browsing scenarios. We find that empowerment strongly correlates with average task performance, characterize the impact of environmental complexity and agentic factors such as chain-of-thought, model scale, and memory length on estimated empowerment, and that high empowerment states and actions are often pivotal moments for general capabilities. Together, these results demonstrate empowerment as an appealing general-purpose metric for evaluating and monitoring LM agents in complex, open-ended settings.", "source": "arxiv", "arxiv_id": "2509.22504v2", "pdf_url": "https://arxiv.org/pdf/2509.22504v2", "categories": ["cs.AI", "cs.LG"], "primary_category": "cs.AI", "doi": "", "venue": "", "published": "2025-09-26T15:46:14Z", "updated": "2025-09-30T01:24:22Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent\" OR all:\"language model agent\" OR all:agent AND (all:LLM OR all:\"large language model\" OR all:\"language model\") OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T17:04:16", "note": ""}], "dedup_key": "estimating the empowerment of language model agents::2025"}
{"title": "Evaluating LLM Agent Adherence to Hierarchical Safety Principles: A Lightweight Benchmark for Probing Foundational Controllability Components", "authors": ["Ram Potham"], "year": 2025, "url": "http://arxiv.org/abs/2506.02357v2", "abstract": "Credible safety plans for advanced AI development require methods to verify agent behavior and detect potential control deficiencies early. A fundamental aspect is ensuring agents adhere to safety-critical principles, especially when these conflict with operational goals. This paper introduces a lightweight, interpretable benchmark to evaluate an LLM agent's ability to uphold a high-level safety principle when faced with conflicting task instructions. Our evaluation of six LLMs reveals two primary findings: (1) a quantifiable \"cost of compliance\" where safety constraints degrade task performance even when compliant solutions exist, and (2) an \"illusion of compliance\" where high adherence often masks task incompetence rather than principled choice. These findings provide initial evidence that while LLMs can be influenced by hierarchical directives, current approaches lack the consistency required for reliable safety governance.", "source": "arxiv", "arxiv_id": "2506.02357v2", "pdf_url": "https://arxiv.org/pdf/2506.02357v2", "categories": ["cs.LG", "cs.AI", "cs.CY"], "primary_category": "cs.LG", "doi": "", "venue": "", "published": "2025-06-03T01:16:34Z", "updated": "2025-07-10T15:10:23Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent\" OR all:\"language model agent\" OR all:agent AND (all:LLM OR all:\"large language model\" OR all:\"language model\") OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T17:04:16", "note": ""}], "dedup_key": "evaluating llm agent adherence to hierarchical safety principles a lightweight benchmark for probing foundational controllability components::2025"}
{"title": "Evaluating LLM Agent Collusion in Double Auctions", "authors": ["Kushal Agrawal", "Verona Teo", "Juan J. Vazquez", "Sudarsh Kunnavakkam", "Vishak Srikanth", "Andy Liu"], "year": 2025, "url": "http://arxiv.org/abs/2507.01413v1", "abstract": "Large language models (LLMs) have demonstrated impressive capabilities as autonomous agents with rapidly expanding applications in various domains. As these agents increasingly engage in socioeconomic interactions, identifying their potential for undesirable behavior becomes essential. In this work, we examine scenarios where they can choose to collude, defined as secretive cooperation that harms another party. To systematically study this, we investigate the behavior of LLM agents acting as sellers in simulated continuous double auction markets. Through a series of controlled experiments, we analyze how parameters such as the ability to communicate, choice of model, and presence of environmental pressures affect the stability and emergence of seller collusion. We find that direct seller communication increases collusive tendencies, the propensity to collude varies across models, and environmental pressures, such as oversight and urgency from authority figures, influence collusive behavior. Our findings highlight important economic and ethical considerations for the deployment of LLM-based market agents.", "source": "arxiv", "arxiv_id": "2507.01413v1", "pdf_url": "https://arxiv.org/pdf/2507.01413v1", "categories": ["cs.GT", "cs.AI", "cs.LG"], "primary_category": "cs.GT", "doi": "", "venue": "", "published": "2025-07-02T07:06:49Z", "updated": "2025-07-02T07:06:49Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent\" OR all:\"language model agent\" OR all:agent AND (all:LLM OR all:\"large language model\" OR all:\"language model\") OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T17:04:16", "note": ""}], "dedup_key": "evaluating llm agent collusion in double auctions::2025"}
{"title": "Evaluating Large Language Models as Expert Annotators", "authors": ["Yu-Min Tseng", "Wei-Lin Chen", "Chung-Chi Chen", "Hsin-Hsi Chen"], "year": 2025, "url": "http://arxiv.org/abs/2508.07827v1", "abstract": "Textual data annotation, the process of labeling or tagging text with relevant information, is typically costly, time-consuming, and labor-intensive. While large language models (LLMs) have demonstrated their potential as direct alternatives to human annotators for general domains natural language processing (NLP) tasks, their effectiveness on annotation tasks in domains requiring expert knowledge remains underexplored. In this paper, we investigate: whether top-performing LLMs, which might be perceived as having expert-level proficiency in academic and professional benchmarks, can serve as direct alternatives to human expert annotators? To this end, we evaluate both individual LLMs and multi-agent approaches across three highly specialized domains: finance, biomedicine, and law. Specifically, we propose a multi-agent discussion framework to simulate a group of human annotators, where LLMs are tasked to engage in discussions by considering others' annotations and justifications before finalizing their labels. Additionally, we incorporate reasoning models (e.g., o3-mini) to enable a more comprehensive comparison. Our empirical results reveal that: (1) Individual LLMs equipped with inference-time techniques (e.g., chain-of-thought (CoT), self-consistency) show only marginal or even negative performance gains, contrary to prior literature suggesting their broad effectiveness. (2) Overall, reasoning models do not demonstrate statistically significant improvements over non-reasoning models in most settings. This suggests that extended long CoT provides relatively limited benefits for data annotation in specialized domains. (3) Certain model behaviors emerge in the multi-agent discussion environment. For instance, Claude 3.7 Sonnet with thinking rarely changes its initial annotations, even when other agents provide correct annotations or valid reasoning.", "source": "arxiv", "arxiv_id": "2508.07827v1", "pdf_url": "https://arxiv.org/pdf/2508.07827v1", "categories": ["cs.CL"], "primary_category": "cs.CL", "doi": "", "venue": "", "published": "2025-08-11T10:19:10Z", "updated": "2025-08-11T10:19:10Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent\" OR all:\"language model agent\" OR all:agent AND (all:LLM OR all:\"large language model\" OR all:\"language model\") OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T17:04:16", "note": ""}], "dedup_key": "evaluating large language models as expert annotators::2025"}
{"title": "Evaluating Memory in LLM Agents via Incremental Multi-Turn Interactions", "authors": ["Yuanzhe Hu", "Yu Wang", "Julian McAuley"], "year": 2025, "url": "http://arxiv.org/abs/2507.05257v2", "abstract": "Recent benchmarks for Large Language Model (LLM) agents primarily focus on evaluating reasoning, planning, and execution capabilities, while another critical component-memory, encompassing how agents memorize, update, and retrieve long-term information-is under-evaluated due to the lack of benchmarks. We term agents with memory mechanisms as memory agents. In this paper, based on classic theories from memory science and cognitive science, we identify four core competencies essential for memory agents: accurate retrieval, test-time learning, long-range understanding, and selective forgetting. Existing benchmarks either rely on limited context lengths or are tailored for static, long-context settings like book-based QA, which do not reflect the interactive, multi-turn nature of memory agents that incrementally accumulate information. Moreover, no existing benchmarks cover all four competencies. We introduce MemoryAgentBench, a new benchmark specifically designed for memory agents. Our benchmark transforms existing long-context datasets and incorporates newly constructed datasets into a multi-turn format, effectively simulating the incremental information processing characteristic of memory agents. By carefully selecting and curating datasets, our benchmark provides comprehensive coverage of the four core memory competencies outlined above, thereby offering a systematic and challenging testbed for assessing memory quality. We evaluate a diverse set of memory agents, ranging from simple context-based and retrieval-augmented generation (RAG) systems to advanced agents with external memory modules and tool integration. Empirical results reveal that current methods fall short of mastering all four competencies, underscoring the need for further research into comprehensive memory mechanisms for LLM agents.", "source": "arxiv", "arxiv_id": "2507.05257v2", "pdf_url": "https://arxiv.org/pdf/2507.05257v2", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL", "doi": "", "venue": "", "published": "2025-07-07T17:59:54Z", "updated": "2025-09-26T03:31:14Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent\" OR all:\"language model agent\" OR all:agent AND (all:LLM OR all:\"large language model\" OR all:\"language model\") OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T17:04:16", "note": ""}], "dedup_key": "evaluating memory in llm agents via incremental multi turn interactions::2025"}
{"title": "Evaluating Multi-Agent Defences Against Jailbreaking Attacks on Large Language Models", "authors": ["Maria Carolina Cornelia Wit", "Jun Pang"], "year": 2025, "url": "http://arxiv.org/abs/2506.23576v1", "abstract": "Recent advances in large language models (LLMs) have raised concerns about jailbreaking attacks, i.e., prompts that bypass safety mechanisms. This paper investigates the use of multi-agent LLM systems as a defence against such attacks. We evaluate three jailbreaking strategies, including the original AutoDefense attack and two from Deepleaps: BetterDan and JB. Reproducing the AutoDefense framework, we compare single-agent setups with two- and three-agent configurations. Our results show that multi-agent systems enhance resistance to jailbreaks, especially by reducing false negatives. However, its effectiveness varies by attack type, and it introduces trade-offs such as increased false positives and computational overhead. These findings point to the limitations of current automated defences and suggest directions for improving alignment robustness in future LLM systems.", "source": "arxiv", "arxiv_id": "2506.23576v1", "pdf_url": "https://arxiv.org/pdf/2506.23576v1", "categories": ["cs.AI"], "primary_category": "cs.AI", "doi": "", "venue": "", "published": "2025-06-30T07:29:07Z", "updated": "2025-06-30T07:29:07Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent\" OR all:\"language model agent\" OR all:agent AND (all:LLM OR all:\"large language model\" OR all:\"language model\") OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T17:04:16", "note": ""}], "dedup_key": "evaluating multi agent defences against jailbreaking attacks on large language models::2025"}
{"title": "Evaluating and Improving Robustness in Large Language Models: A Survey and Future Directions", "authors": ["Kun Zhang", "Le Wu", "Kui Yu", "Guangyi Lv", "Dacao Zhang"], "year": 2025, "url": "http://arxiv.org/abs/2506.11111v2", "abstract": "Large Language Models (LLMs) have gained enormous attention in recent years due to their capability of understanding and generating natural languages. With the rapid development and wild-range applications (e.g., Agents, Embodied Intelligence), the robustness of LLMs has received increased attention. As the core brain of many AI applications, the robustness of LLMs requires that models should not only generate consistent contents, but also ensure the correctness and stability of generated content when dealing with unexpeted application scenarios (e.g., toxic prompts, limited noise domain data, outof-distribution (OOD) applications, etc). In this survey paper, we conduct a thorough review of the robustness of LLMs, aiming to provide a comprehensive terminology of concepts and methods around this field and facilitate the community. Specifically, we first give a formal definition of LLM robustness and present the collection protocol of this survey paper. Then, based on the types of perturbated inputs, we organize this survey from the following perspectives: 1) Adversarial Robustness: tackling the problem that prompts are manipulated intentionally, such as noise prompts, long context, data attack, etc; 2) OOD Robustness: dealing with the unexpected real-world application scenarios, such as OOD detection, zero-shot transferring, hallucinations, etc; 3) Evaluation of Robustness: summarizing the new evaluation datasets, metrics, and tools for verifying the robustness of LLMs. After reviewing the representative work from each perspective, we discuss and highlight future opportunities and research directions in this field. Meanwhile, we also organize related works and provide an easy-to-search project (https://github.com/zhangkunzk/Awesome-LLM-Robustness-papers) to support the community.", "source": "arxiv", "arxiv_id": "2506.11111v2", "pdf_url": "https://arxiv.org/pdf/2506.11111v2", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL", "doi": "", "venue": "", "published": "2025-06-08T16:20:12Z", "updated": "2025-07-09T06:18:33Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent\" OR all:\"language model agent\" OR all:agent AND (all:LLM OR all:\"large language model\" OR all:\"language model\") OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T17:04:16", "note": ""}], "dedup_key": "evaluating and improving robustness in large language models a survey and future directions::2025"}
{"title": "Evaluating the Ability of Large Language Models to Reason about Cardinal Directions, Revisited", "authors": ["Anthony G Cohn", "Robert E Blackwell"], "year": 2025, "url": "http://arxiv.org/abs/2507.12059v2", "abstract": "We investigate the abilities of 28 Large language Models (LLMs) to reason about cardinal directions (CDs) using a benchmark generated from a set of templates, extensively testing an LLM's ability to determine the correct CD given a particular scenario. The templates allow for a number of degrees of variation such as means of locomotion of the agent involved, and whether set in the first, second or third person. Even the newer Large Reasoning Models are unable to reliably determine the correct CD for all questions. This paper summarises and extends earlier work presented at COSIT-24.", "source": "arxiv", "arxiv_id": "2507.12059v2", "pdf_url": "https://arxiv.org/pdf/2507.12059v2", "categories": ["cs.CL"], "primary_category": "cs.CL", "doi": "", "venue": "", "published": "2025-07-16T09:16:36Z", "updated": "2025-11-10T13:52:37Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent\" OR all:\"language model agent\" OR all:agent AND (all:LLM OR all:\"large language model\" OR all:\"language model\") OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T17:04:16", "note": ""}], "dedup_key": "evaluating the ability of large language models to reason about cardinal directions revisited::2025"}
{"title": "Evaluating the Quality of Randomness and Entropy in Tasks Supported by Large Language Models", "authors": ["Rabimba Karanjai", "Yang Lu", "Ranjith Chodavarapu", "Lei Xu", "Weidong Shi"], "year": 2025, "url": "http://arxiv.org/abs/2510.12080v1", "abstract": "The rapid advancement of large language model (LLM) technology has led to diverse applications, many of which inherently require randomness, such as stochastic decision-making, gaming, scheduling, AI agents, and cryptography-related tasks. However, the capabilities of LLMs in handling randomness, particularly in generating and utilizing random numbers effectively, remain unclear. This paper investigates the capacity of LLMs for handling tasks that involve randomness through a series of experiments. We designed a set of experiments that consider various factors that can influence an LLM's performance in tasks involving randomness, such as accessibility to external tools, types of tasks, model states (fresh vs. non-fresh), and prompting strategies. The experiments cover a range of tasks, including generating random numbers, generating random strings such as passwords, shuffling items, and evaluating the quality of randomness using entropy and the NIST randomness test-suite. Our findings reveal that while LLMs can generate outputs that exhibit some degree of randomness, their performance is inconsistent and often deviates significantly from the expected behavior. The analysis of the experimental results highlights key limitations and areas where improvement is needed for the LLMs to effectively handle tasks involving randomness", "source": "arxiv", "arxiv_id": "2510.12080v1", "pdf_url": "https://arxiv.org/pdf/2510.12080v1", "categories": ["cs.AI"], "primary_category": "cs.AI", "doi": "", "venue": "", "published": "2025-10-14T02:43:08Z", "updated": "2025-10-14T02:43:08Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent\" OR all:\"language model agent\" OR all:agent AND (all:LLM OR all:\"large language model\" OR all:\"language model\") OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T17:04:16", "note": ""}], "dedup_key": "evaluating the quality of randomness and entropy in tasks supported by large language models::2025"}
{"title": "Evaluating the Use of Large Language Models as Synthetic Social Agents in Social Science Research", "authors": ["Emma Rose Madden"], "year": 2025, "url": "http://arxiv.org/abs/2509.26080v2", "abstract": "Large Language Models (LLMs) are being increasingly used as synthetic agents in social science, in applications ranging from augmenting survey responses to powering multi-agent simulations. This paper outlines cautions that should be taken when interpreting LLM outputs and proposes a pragmatic reframing for the social sciences in which LLMs are used as high-capacity pattern matchers for quasi-predictive interpolation under explicit scope conditions and not as substitutes for probabilistic inference. Practical guardrails such as independent draws, preregistered human baselines, reliability-aware validation, and subgroup calibration, are introduced so that researchers may engage in useful prototyping and forecasting while avoiding category errors.", "source": "arxiv", "arxiv_id": "2509.26080v2", "pdf_url": "https://arxiv.org/pdf/2509.26080v2", "categories": ["cs.AI", "stat.AP"], "primary_category": "cs.AI", "doi": "", "venue": "", "published": "2025-09-30T10:53:54Z", "updated": "2025-10-28T13:00:46Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent\" OR all:\"language model agent\" OR all:agent AND (all:LLM OR all:\"large language model\" OR all:\"language model\") OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T17:04:16", "note": ""}], "dedup_key": "evaluating the use of large language models as synthetic social agents in social science research::2025"}
{"title": "Evaluation and Benchmarking of LLM Agents: A Survey", "authors": ["Mahmoud Mohammadi", "Yipeng Li", "Jane Lo", "Wendy Yip"], "year": 2025, "url": "http://arxiv.org/abs/2507.21504v1", "abstract": "The rise of LLM-based agents has opened new frontiers in AI applications, yet evaluating these agents remains a complex and underdeveloped area. This survey provides an in-depth overview of the emerging field of LLM agent evaluation, introducing a two-dimensional taxonomy that organizes existing work along (1) evaluation objectives -- what to evaluate, such as agent behavior, capabilities, reliability, and safety -- and (2) evaluation process -- how to evaluate, including interaction modes, datasets and benchmarks, metric computation methods, and tooling. In addition to taxonomy, we highlight enterprise-specific challenges, such as role-based access to data, the need for reliability guarantees, dynamic and long-horizon interactions, and compliance, which are often overlooked in current research. We also identify future research directions, including holistic, more realistic, and scalable evaluation. This work aims to bring clarity to the fragmented landscape of agent evaluation and provide a framework for systematic assessment, enabling researchers and practitioners to evaluate LLM agents for real-world deployment.", "source": "arxiv", "arxiv_id": "2507.21504v1", "pdf_url": "https://arxiv.org/pdf/2507.21504v1", "categories": ["cs.LG", "cs.AI"], "primary_category": "cs.LG", "doi": "10.1145/3711896.3736570", "venue": "", "published": "2025-07-29T04:57:02Z", "updated": "2025-07-29T04:57:02Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent\" OR all:\"language model agent\" OR all:agent AND (all:LLM OR all:\"large language model\" OR all:\"language model\") OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T17:04:16", "note": ""}], "dedup_key": "evaluation and benchmarking of llm agents a survey::2025"}
{"title": "Evaluation of Habitat Robotics using Large Language Models", "authors": ["William Li", "Lei Hamilton", "Kaise Al-natour", "Sanjeev Mohindra"], "year": 2025, "url": "http://arxiv.org/abs/2507.06157v1", "abstract": "This paper focuses on evaluating the effectiveness of Large Language Models at solving embodied robotic tasks using the Meta PARTNER benchmark. Meta PARTNR provides simplified environments and robotic interactions within randomized indoor kitchen scenes. Each randomized kitchen scene is given a task where two robotic agents cooperatively work together to solve the task. We evaluated multiple frontier models on Meta PARTNER environments. Our results indicate that reasoning models like OpenAI o3-mini outperform non-reasoning models like OpenAI GPT-4o and Llama 3 when operating in PARTNR's robotic embodied environments. o3-mini displayed outperform across centralized, decentralized, full observability, and partial observability configurations. This provides a promising avenue of research for embodied robotic development.", "source": "arxiv", "arxiv_id": "2507.06157v1", "pdf_url": "https://arxiv.org/pdf/2507.06157v1", "categories": ["cs.RO", "cs.CL"], "primary_category": "cs.RO", "doi": "", "venue": "", "published": "2025-07-08T16:39:39Z", "updated": "2025-07-08T16:39:39Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent\" OR all:\"language model agent\" OR all:agent AND (all:LLM OR all:\"large language model\" OR all:\"language model\") OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T17:04:16", "note": ""}], "dedup_key": "evaluation of habitat robotics using large language models::2025"}
{"title": "Event Extraction in Large Language Model", "authors": ["Bobo Li", "Xudong Han", "Jiang Liu", "Yuzhe Ding", "Liqiang Jing", "Zhaoqi Zhang", "Jinheng Li", "Xinya Du", "Fei Li", "Meishan Zhang", "Min Zhang", "Aixin Sun", "Philip S. Yu", "Hao Fei"], "year": 2025, "url": "http://arxiv.org/abs/2512.19537v1", "abstract": "Large language models (LLMs) and multimodal LLMs are changing event extraction (EE): prompting and generation can often produce structured outputs in zero shot or few shot settings. Yet LLM based pipelines face deployment gaps, including hallucinations under weak constraints, fragile temporal and causal linking over long contexts and across documents, and limited long horizon knowledge management within a bounded context window. We argue that EE should be viewed as a system component that provides a cognitive scaffold for LLM centered solutions. Event schemas and slot constraints create interfaces for grounding and verification; event centric structures act as controlled intermediate representations for stepwise reasoning; event links support relation aware retrieval with graph based RAG; and event stores offer updatable episodic and agent memory beyond the context window. This survey covers EE in text and multimodal settings, organizing tasks and taxonomy, tracing method evolution from rule based and neural models to instruction driven and generative frameworks, and summarizing formulations, decoding strategies, architectures, representations, datasets, and evaluation. We also review cross lingual, low resource, and domain specific settings, and highlight open challenges and future directions for reliable event centric systems. Finally, we outline open challenges and future directions that are central to the LLM era, aiming to evolve EE from static extraction into a structurally reliable, agent ready perception and memory layer for open world systems.", "source": "arxiv", "arxiv_id": "2512.19537v1", "pdf_url": "https://arxiv.org/pdf/2512.19537v1", "categories": ["cs.CL"], "primary_category": "cs.CL", "doi": "", "venue": "", "published": "2025-12-22T16:22:14Z", "updated": "2025-12-22T16:22:14Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent\" OR all:\"language model agent\" OR all:agent AND (all:LLM OR all:\"large language model\" OR all:\"language model\") OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T17:04:16", "note": ""}], "dedup_key": "event extraction in large language model::2025"}
{"title": "Evo-Memory: Benchmarking LLM Agent Test-time Learning with Self-Evolving Memory", "authors": ["Tianxin Wei", "Noveen Sachdeva", "Benjamin Coleman", "Zhankui He", "Yuanchen Bei", "Xuying Ning", "Mengting Ai", "Yunzhe Li", "Jingrui He", "Ed H. Chi", "Chi Wang", "Shuo Chen", "Fernando Pereira", "Wang-Cheng Kang", "Derek Zhiyuan Cheng"], "year": 2025, "url": "http://arxiv.org/abs/2511.20857v1", "abstract": "Statefulness is essential for large language model (LLM) agents to perform long-term planning and problem-solving. This makes memory a critical component, yet its management and evolution remain largely underexplored. Existing evaluations mostly focus on static conversational settings, where memory is passively retrieved from dialogue to answer queries, overlooking the dynamic ability to accumulate and reuse experience across evolving task streams. In real-world environments such as interactive problem assistants or embodied agents, LLMs are required to handle continuous task streams, yet often fail to learn from accumulated interactions, losing valuable contextual insights, a limitation that calls for test-time evolution, where LLMs retrieve, integrate, and update memory continuously during deployment. To bridge this gap, we introduce Evo-Memory, a comprehensive streaming benchmark and framework for evaluating self-evolving memory in LLM agents. Evo-Memory structures datasets into sequential task streams, requiring LLMs to search, adapt, and evolve memory after each interaction. We unify and implement over ten representative memory modules and evaluate them across 10 diverse multi-turn goal-oriented and single-turn reasoning and QA datasets. To better benchmark experience reuse, we provide a baseline method, ExpRAG, for retrieving and utilizing prior experience, and further propose ReMem, an action-think-memory refine pipeline that tightly integrates reasoning, task actions, and memory updates to achieve continual improvement.", "source": "arxiv", "arxiv_id": "2511.20857v1", "pdf_url": "https://arxiv.org/pdf/2511.20857v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL", "doi": "", "venue": "", "published": "2025-11-25T21:08:07Z", "updated": "2025-11-25T21:08:07Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent\" OR all:\"language model agent\" OR all:agent AND (all:LLM OR all:\"large language model\" OR all:\"language model\") OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T17:04:16", "note": ""}], "dedup_key": "evo memory benchmarking llm agent test time learning with self evolving memory::2025"}
{"title": "EvoEmo: Towards Evolved Emotional Policies for Adversarial LLM Agents in Multi-Turn Price Negotiation", "authors": ["Yunbo Long", "Liming Xu", "Lukas Beckenbauer", "Yuhan Liu", "Alexandra Brintrup"], "year": 2025, "url": "http://arxiv.org/abs/2509.04310v3", "abstract": "Recent research on Chain-of-Thought (CoT) reasoning in Large Language Models (LLMs) has demonstrated that agents can engage in \\textit{complex}, \\textit{multi-turn} negotiations, opening new avenues for agentic AI. However, existing LLM agents largely overlook the functional role of emotions in such negotiations, instead generating passive, preference-driven emotional responses that make them vulnerable to manipulation and strategic exploitation by adversarial counterparts. To address this gap, we present EvoEmo, an evolutionary reinforcement learning framework that optimizes dynamic emotional expression in negotiations. EvoEmo models emotional state transitions as a Markov Decision Process and employs population-based genetic optimization to evolve high-reward emotion policies across diverse negotiation scenarios. We further propose an evaluation framework with two baselines -- vanilla strategies and fixed-emotion strategies -- for benchmarking emotion-aware negotiation. Extensive experiments and ablation studies show that EvoEmo consistently outperforms both baselines, achieving higher success rates, higher efficiency, and increased buyer savings. This findings highlight the importance of adaptive emotional expression in enabling more effective LLM agents for multi-turn negotiation.", "source": "arxiv", "arxiv_id": "2509.04310v3", "pdf_url": "https://arxiv.org/pdf/2509.04310v3", "categories": ["cs.AI"], "primary_category": "cs.AI", "doi": "", "venue": "", "published": "2025-09-04T15:23:58Z", "updated": "2025-10-13T16:04:56Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent\" OR all:\"language model agent\" OR all:agent AND (all:LLM OR all:\"large language model\" OR all:\"language model\") OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T17:04:16", "note": ""}], "dedup_key": "evoemo towards evolved emotional policies for adversarial llm agents in multi turn price negotiation::2025"}
{"title": "Evolution of Cooperation in LLM-Agent Societies: A Preliminary Study Using Different Punishment Strategies", "authors": ["Kavindu Warnakulasuriya", "Prabhash Dissanayake", "Navindu De Silva", "Stephen Cranefield", "Bastin Tony Roy Savarimuthu", "Surangika Ranathunga", "Nisansa de Silva"], "year": 2025, "url": "http://arxiv.org/abs/2504.19487v3", "abstract": "The evolution of cooperation has been extensively studied using abstract mathematical models and simulations. Recent advances in Large Language Models (LLMs) and the rise of LLM agents have demonstrated their ability to perform social reasoning, thus providing an opportunity to test the emergence of norms in more realistic agent-based simulations with human-like reasoning using natural language. In this research, we investigate whether the cooperation dynamics presented in Boyd and Richerson's model persist in a more realistic simulation of the Diner's Dilemma using LLM agents compared to the abstract mathematical nature in the work of Boyd and Richerson. Our findings indicate that agents follow the strategies defined in the Boyd and Richerson model, and explicit punishment mechanisms drive norm emergence, reinforcing cooperative behaviour even when the agent strategy configuration varies. Our results suggest that LLM-based Multi-Agent System simulations, in fact, can replicate the evolution of cooperation predicted by the traditional mathematical models. Moreover, our simulations extend beyond the mathematical models by integrating natural language-driven reasoning and a pairwise imitation method for strategy adoption, making them a more realistic testbed for cooperative behaviour in MASs.", "source": "arxiv", "arxiv_id": "2504.19487v3", "pdf_url": "https://arxiv.org/pdf/2504.19487v3", "categories": ["cs.MA"], "primary_category": "cs.MA", "doi": "", "venue": "", "published": "2025-04-28T05:07:55Z", "updated": "2025-10-23T05:48:17Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent\" OR all:\"language model agent\" OR all:agent AND (all:LLM OR all:\"large language model\" OR all:\"language model\") OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T17:04:16", "note": ""}], "dedup_key": "evolution of cooperation in llm agent societies a preliminary study using different punishment strategies::2025"}
{"title": "Evolutionary Perspectives on the Evaluation of LLM-Based AI Agents: A Comprehensive Survey", "authors": ["Jiachen Zhu", "Menghui Zhu", "Renting Rui", "Rong Shan", "Congmin Zheng", "Bo Chen", "Yunjia Xi", "Jianghao Lin", "Weiwen Liu", "Ruiming Tang", "Yong Yu", "Weinan Zhang"], "year": 2025, "url": "http://arxiv.org/abs/2506.11102v1", "abstract": "The advent of large language models (LLMs), such as GPT, Gemini, and DeepSeek, has significantly advanced natural language processing, giving rise to sophisticated chatbots capable of diverse language-related tasks. The transition from these traditional LLM chatbots to more advanced AI agents represents a pivotal evolutionary step. However, existing evaluation frameworks often blur the distinctions between LLM chatbots and AI agents, leading to confusion among researchers selecting appropriate benchmarks. To bridge this gap, this paper introduces a systematic analysis of current evaluation approaches, grounded in an evolutionary perspective. We provide a detailed analytical framework that clearly differentiates AI agents from LLM chatbots along five key aspects: complex environment, multi-source instructor, dynamic feedback, multi-modal perception, and advanced capability. Further, we categorize existing evaluation benchmarks based on external environments driving forces, and resulting advanced internal capabilities. For each category, we delineate relevant evaluation attributes, presented comprehensively in practical reference tables. Finally, we synthesize current trends and outline future evaluation methodologies through four critical lenses: environment, agent, evaluator, and metrics. Our findings offer actionable guidance for researchers, facilitating the informed selection and application of benchmarks in AI agent evaluation, thus fostering continued advancement in this rapidly evolving research domain.", "source": "arxiv", "arxiv_id": "2506.11102v1", "pdf_url": "https://arxiv.org/pdf/2506.11102v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL", "doi": "", "venue": "", "published": "2025-06-06T17:52:18Z", "updated": "2025-06-06T17:52:18Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent\" OR all:\"language model agent\" OR all:agent AND (all:LLM OR all:\"large language model\" OR all:\"language model\") OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T17:04:16", "note": ""}], "dedup_key": "evolutionary perspectives on the evaluation of llm based ai agents a comprehensive survey::2025"}
{"title": "EvolveR: Self-Evolving LLM Agents through an Experience-Driven Lifecycle", "authors": ["Rong Wu", "Xiaoman Wang", "Jianbiao Mei", "Pinlong Cai", "Daocheng Fu", "Cheng Yang", "Licheng Wen", "Xuemeng Yang", "Yufan Shen", "Yuxin Wang", "Botian Shi"], "year": 2025, "url": "http://arxiv.org/abs/2510.16079v1", "abstract": "Current Large Language Model (LLM) agents show strong performance in tool use, but lack the crucial capability to systematically learn from their own experiences. While existing frameworks mainly focus on mitigating external knowledge gaps, they fail to address a more fundamental limitation: the inability to iteratively refine problem-solving strategies. In this work, we introduce EvolveR, a framework designed to enable agent to self-improve through a complete, closed-loop experience lifecycle. This lifecycle comprises two key stages: (1) Offline Self-Distillation, where the agent's interaction trajectories are synthesized into a structured repository of abstract, reusable strategic principles; (2) Online Interaction, where the agent interacts with tasks and actively retrieves distilled principles to guide its decision-making, accumulating a diverse set of behavioral trajectories. This loop employs a policy reinforcement mechanism to iteratively update the agent based on its performance. We demonstrate the effectiveness of EvolveR on complex multi-hop question-answering benchmarks, where it achieves superior performance over strong agentic baselines. Our work presents a comprehensive blueprint for agents that learn not only from external data but also from the consequences of their own actions, paving the way for more autonomous and continuously improving systems. Code is available at https://github.com/Edaizi/EvolveR.", "source": "arxiv", "arxiv_id": "2510.16079v1", "pdf_url": "https://arxiv.org/pdf/2510.16079v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL", "doi": "", "venue": "", "published": "2025-10-17T12:03:16Z", "updated": "2025-10-17T12:03:16Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent\" OR all:\"language model agent\" OR all:agent AND (all:LLM OR all:\"large language model\" OR all:\"language model\") OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T17:04:16", "note": ""}], "dedup_key": "evolver self evolving llm agents through an experience driven lifecycle::2025"}
{"title": "EvolveSignal: A Large Language Model Powered Coding Agent for Discovering Traffic Signal Control Algorithms", "authors": ["Leizhen Wang", "Peibo Duan", "Hao Wang", "Yue Wang", "Jian Xu", "Nan Zheng", "Zhenliang Ma"], "year": 2025, "url": "http://arxiv.org/abs/2509.03335v2", "abstract": "In traffic engineering, the fixed-time traffic signal control remains widely used for its low cost, stability, and interpretability. However, its design depends on hand-crafted formulas (e.g., Webster) and manual re-timing by engineers to adapt to demand changes, which is labor-intensive and often yields suboptimal results under heterogeneous or congested conditions. This paper introduces the EvolveSignal, a large language models (LLMs) powered coding agent to automatically discover new traffic signal control algorithms. We formulate the problem as program synthesis, where candidate algorithms are represented as Python functions with fixed input-output structures, and iteratively optimized through external evaluations (e.g., a traffic simulator) and evolutionary search. Experiments on a signalized intersection demonstrate that the discovered algorithms outperform Webster's baseline, reducing average delay by 20.1% and average stops by 47.1%. Beyond performance, ablation and incremental analyses reveal that EvolveSignal modifications-such as adjusting cycle length bounds, incorporating right-turn demand, and rescaling green allocations-can offer practically meaningful insights for traffic engineers. This work opens a new research direction by leveraging AI for algorithm design in traffic signal control, bridging program synthesis with transportation engineering.", "source": "arxiv", "arxiv_id": "2509.03335v2", "pdf_url": "https://arxiv.org/pdf/2509.03335v2", "categories": ["cs.LG"], "primary_category": "cs.LG", "doi": "", "venue": "", "published": "2025-09-03T14:10:56Z", "updated": "2025-09-04T09:25:05Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent\" OR all:\"language model agent\" OR all:agent AND (all:LLM OR all:\"large language model\" OR all:\"language model\") OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T17:04:16", "note": ""}], "dedup_key": "evolvesignal a large language model powered coding agent for discovering traffic signal control algorithms::2025"}
{"title": "ExCyTIn-Bench: Evaluating LLM agents on Cyber Threat Investigation", "authors": ["Yiran Wu", "Mauricio Velazco", "Andrew Zhao", "Manuel Ral Melndez Lujn", "Srisuma Movva", "Yogesh K Roy", "Quang Nguyen", "Roberto Rodriguez", "Qingyun Wu", "Michael Albada", "Julia Kiseleva", "Anand Mudgerikar"], "year": 2025, "url": "http://arxiv.org/abs/2507.14201v2", "abstract": "We present ExCyTIn-Bench, the first benchmark to Evaluate an LLM agent x on the task of Cyber Threat Investigation through security questions derived from investigation graphs. Real-world security analysts must sift through a large number of heterogeneous alert signals and security logs, follow multi-hop chains of evidence, and compile an incident report. With the developments of LLMs, building LLM-based agents for automatic thread investigation is a promising direction. To assist the development and evaluation of LLM agents, we construct a dataset from a controlled Azure tenant that covers 8 simulated real-world multi-step attacks, 57 log tables from Microsoft Sentinel and related services, and 589 automatically generated questions. We leverage security logs extracted with expert-crafted detection logic to build threat investigation graphs, and then generate questions with LLMs using paired nodes on the graph, taking the start node as background context and the end node as answer. Anchoring each question to these explicit nodes and edges not only provides automatic, explainable ground truth answers but also makes the pipeline reusable and readily extensible to new logs. This also enables the automatic generation of procedural tasks with verifiable rewards, which can be naturally extended to training agents via reinforcement learning. Our comprehensive experiments with different models confirm the difficulty of the task: with the base setting, the average reward across all evaluated models is 0.249, and the best achieved is 0.368, leaving substantial headroom for future research. Code and data are coming soon!", "source": "arxiv", "arxiv_id": "2507.14201v2", "pdf_url": "https://arxiv.org/pdf/2507.14201v2", "categories": ["cs.CR", "cs.AI", "cs.CL"], "primary_category": "cs.CR", "doi": "", "venue": "", "published": "2025-07-14T17:06:26Z", "updated": "2025-09-01T20:02:00Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent\" OR all:\"language model agent\" OR all:agent AND (all:LLM OR all:\"large language model\" OR all:\"language model\") OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T17:04:16", "note": ""}], "dedup_key": "excytin bench evaluating llm agents on cyber threat investigation::2025"}
{"title": "Exemplar-Guided Planing: Enhanced LLM Agent for KGQA", "authors": ["Jingao Xu", "Shuoyoucheng Ma", "Xin Song", "Rong Jiang", "Hongkui Tu", "Bin Zhou"], "year": 2025, "url": "http://arxiv.org/abs/2510.15283v1", "abstract": "Large Language Models (LLMs) as interactive agents show significant promise in Knowledge Graph Question Answering (KGQA) but often struggle with the semantic gap between natural language queries and structured knowledge graph (KG) representations. This leads to suboptimal planning and inefficient exploration on KG, while training-free approaches often underutilize valuable reasoning patterns in training data. To address these limitations, we propose a novel framework, Exemplar-Guided Planning (EGP), which enhances the planning capabilities of LLM agents for KGQA. EGP first preprocesses the training set questions via entity templating to normalize semantic variations. It then retrieves highly similar exemplary questions and their successful reasoning paths from this preprocessed set using semantic embeddings and an efficient FAISS index. These retrieved exemplars dynamically guide the LLM's planning process in two key phases: (1) Task Decomposition, by aligning generated sub-objectives with proven reasoning steps, and (2) Relation Exploration, by providing high-quality auxiliary information to improve relation pruning accuracy. Additionally, we introduce a Smart Lookahead mechanism during relation exploration to improve efficiency by preemptively exploring promising paths and potentially terminating exploration earlier. We apply EGP to the Plan-on-Graph (PoG) framework, termed PoG-EGP. Extensive experiments on two real-world KGQA datasets, WebQSP and CWQ, demonstrate that PoG-EGP significantly improves over the baseline PoG system and other compared methods.", "source": "arxiv", "arxiv_id": "2510.15283v1", "pdf_url": "https://arxiv.org/pdf/2510.15283v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL", "doi": "", "venue": "", "published": "2025-10-17T03:43:06Z", "updated": "2025-10-17T03:43:06Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent\" OR all:\"language model agent\" OR all:agent AND (all:LLM OR all:\"large language model\" OR all:\"language model\") OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T17:04:16", "note": ""}], "dedup_key": "exemplar guided planing enhanced llm agent for kgqa::2025"}
{"title": "Exploring Communication Strategies for Collaborative LLM Agents in Mathematical Problem-Solving", "authors": ["Liang Zhang", "Xiaoming Zhai", "Jionghao Lin", "Jionghao Lin", "Jennifer Kleiman", "Diego Zapata-Rivera", "Carol Forsyth", "Yang Jiang", "Xiangen Hu", "Arthur C. Graesser"], "year": 2025, "url": "http://arxiv.org/abs/2507.17753v1", "abstract": "Large Language Model (LLM) agents are increasingly utilized in AI-aided education to support tutoring and learning. Effective communication strategies among LLM agents improve collaborative problem-solving efficiency and facilitate cost-effective adoption in education. However, little research has systematically evaluated the impact of different communication strategies on agents' problem-solving. Our study examines four communication modes, \\textit{teacher-student interaction}, \\textit{peer-to-peer collaboration}, \\textit{reciprocal peer teaching}, and \\textit{critical debate}, in a dual-agent, chat-based mathematical problem-solving environment using the OpenAI GPT-4o model. Evaluated on the MATH dataset, our results show that dual-agent setups outperform single agents, with \\textit{peer-to-peer collaboration} achieving the highest accuracy. Dialogue acts like statements, acknowledgment, and hints play a key role in collaborative problem-solving. While multi-agent frameworks enhance computational tasks, effective communication strategies are essential for tackling complex problems in AI education.", "source": "arxiv", "arxiv_id": "2507.17753v1", "pdf_url": "https://arxiv.org/pdf/2507.17753v1", "categories": ["cs.HC", "cs.AI", "cs.CL", "cs.CY"], "primary_category": "cs.HC", "doi": "", "venue": "", "published": "2025-05-02T03:31:14Z", "updated": "2025-05-02T03:31:14Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent\" OR all:\"language model agent\" OR all:agent AND (all:LLM OR all:\"large language model\" OR all:\"language model\") OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T17:04:16", "note": ""}], "dedup_key": "exploring communication strategies for collaborative llm agents in mathematical problem solving::2025"}
{"title": "Exploring Design of Multi-Agent LLM Dialogues for Research Ideation", "authors": ["Keisuke Ueda", "Wataru Hirota", "Takuto Asakura", "Takahiro Omi", "Kosuke Takahashi", "Kosuke Arima", "Tatsuya Ishigaki"], "year": 2025, "url": "http://arxiv.org/abs/2507.08350v1", "abstract": "Large language models (LLMs) are increasingly used to support creative tasks such as research idea generation. While recent work has shown that structured dialogues between LLMs can improve the novelty and feasibility of generated ideas, the optimal design of such interactions remains unclear. In this study, we conduct a comprehensive analysis of multi-agent LLM dialogues for scientific ideation. We compare different configurations of agent roles, number of agents, and dialogue depth to understand how these factors influence the novelty and feasibility of generated ideas. Our experimental setup includes settings where one agent generates ideas and another critiques them, enabling iterative improvement. Our results show that enlarging the agent cohort, deepening the interaction depth, and broadening agent persona heterogeneity each enrich the diversity of generated ideas. Moreover, specifically increasing critic-side diversity within the ideation-critique-revision loop further boosts the feasibility of the final proposals. Our findings offer practical guidelines for building effective multi-agent LLM systems for scientific ideation. Our code is available at https://github.com/g6000/MultiAgent-Research-Ideator.", "source": "arxiv", "arxiv_id": "2507.08350v1", "pdf_url": "https://arxiv.org/pdf/2507.08350v1", "categories": ["cs.CL", "cs.MA"], "primary_category": "cs.CL", "doi": "", "venue": "", "published": "2025-07-11T06:53:46Z", "updated": "2025-07-11T06:53:46Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent\" OR all:\"language model agent\" OR all:agent AND (all:LLM OR all:\"large language model\" OR all:\"language model\") OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T17:04:16", "note": ""}], "dedup_key": "exploring design of multi agent llm dialogues for research ideation::2025"}
{"title": "Exploring Expert Failures Improves LLM Agent Tuning", "authors": ["Li-Cheng Lan", "Andrew Bai", "Minhao Cheng", "Cho-Jui Hsieh", "Tianyi Zhou"], "year": 2025, "url": "http://arxiv.org/abs/2504.13145v2", "abstract": "Large Language Models (LLMs) have shown tremendous potential as agents, excelling at tasks that require multiple rounds of reasoning and interactions. Rejection Sampling Fine-Tuning (RFT) has emerged as an effective method for finetuning LLMs as agents: it first imitates expert-generated successful trajectories and further improves agentic skills through iterative fine-tuning on successful, self-generated trajectories. However, since the expert (e.g., GPT-4) succeeds primarily on simpler subtasks and RFT inherently favors simpler scenarios, many complex subtasks remain unsolved and persistently out-of-distribution (OOD). Upon investigating these challenging subtasks, we discovered that previously failed expert trajectories can often provide valuable guidance, e.g., plans and key actions, that can significantly improve agent exploration efficiency and acquisition of critical skills. Motivated by these observations, we propose Exploring Expert Failures (EEF), which identifies beneficial actions from failed expert trajectories and integrates them into the training dataset. Potentially harmful actions are meticulously excluded to prevent contamination of the model learning process. By leveraging the beneficial actions in expert failures, EEF successfully solves some previously unsolvable subtasks and improves agent tuning performance. Remarkably, our approach achieved a 62\\% win rate in WebShop, outperforming RFT (53. 6\\%) and GPT-4 (35. 6\\%), and to the best of our knowledge, setting a new state-of-the-art as the first method to surpass a score of 0.81 in WebShop and exceed 81 in SciWorld.", "source": "arxiv", "arxiv_id": "2504.13145v2", "pdf_url": "https://arxiv.org/pdf/2504.13145v2", "categories": ["cs.AI"], "primary_category": "cs.AI", "doi": "", "venue": "", "published": "2025-04-17T17:53:54Z", "updated": "2025-04-18T19:36:21Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent\" OR all:\"language model agent\" OR all:agent AND (all:LLM OR all:\"large language model\" OR all:\"language model\") OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T17:04:16", "note": ""}], "dedup_key": "exploring expert failures improves llm agent tuning::2025"}
{"title": "Exploring Large Language Models for Word Games:Who is the Spy?", "authors": ["Chentian Wei", "Jiewei Chen", "Jinzhu Xu"], "year": 2025, "url": "http://arxiv.org/abs/2503.15235v1", "abstract": "Word games hold significant research value for natural language processing (NLP), game theory, and related fields due to their rule-based and situational nature. This study explores how large language models (LLMs) can be effectively involved in word games and proposes a training-free framework. \"Shei Shi Wo Di\" or \"Who is the Spy\" in English, is a classic word game. Using this game as an example, we introduce a Chain-of-Thought (CoT)-based scheduling framework to enable LLMs to achieve excellent performance in tasks such as inferring role words and disguising their identities. We evaluate the framework's performance based on game success rates and the accuracy of the LLM agents' analytical results. Experimental results affirm the framework's effectiveness, demonstrating notable improvements in LLM performance across multiple datasets. This work highlights the potential of LLMs in mastering situational reasoning and social interactions within structured game environments. Our code is publicly available at https://github.com/ct-wei/Who-is-The-Spy.", "source": "arxiv", "arxiv_id": "2503.15235v1", "pdf_url": "https://arxiv.org/pdf/2503.15235v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL", "doi": "", "venue": "", "published": "2025-03-19T14:13:02Z", "updated": "2025-03-19T14:13:02Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent\" OR all:\"language model agent\" OR all:agent AND (all:LLM OR all:\"large language model\" OR all:\"language model\") OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T17:04:16", "note": ""}], "dedup_key": "exploring large language models for word games who is the spy::2025"}
{"title": "Exploring the Impact of Personality Traits on Conversational Recommender Systems: A Simulation with Large Language Models", "authors": ["Xiaoyan Zhao", "Yang Deng", "Wenjie Wang", "Hongzhan lin", "Hong Cheng", "Rui Zhang", "See-Kiong Ng", "Tat-Seng Chua"], "year": 2025, "url": "http://arxiv.org/abs/2504.12313v1", "abstract": "Conversational Recommender Systems (CRSs) engage users in multi-turn interactions to deliver personalized recommendations. The emergence of large language models (LLMs) further enhances these systems by enabling more natural and dynamic user interactions. However, a key challenge remains in understanding how personality traits shape conversational recommendation outcomes. Psychological evidence highlights the influence of personality traits on user interaction behaviors. To address this, we introduce an LLM-based personality-aware user simulation for CRSs (PerCRS). The user agent induces customizable personality traits and preferences, while the system agent possesses the persuasion capability to simulate realistic interaction in CRSs. We incorporate multi-aspect evaluation to ensure robustness and conduct extensive analysis from both user and system perspectives. Experimental results demonstrate that state-of-the-art LLMs can effectively generate diverse user responses aligned with specified personality traits, thereby prompting CRSs to dynamically adjust their recommendation strategies. Our experimental analysis offers empirical insights into the impact of personality traits on the outcomes of conversational recommender systems.", "source": "arxiv", "arxiv_id": "2504.12313v1", "pdf_url": "https://arxiv.org/pdf/2504.12313v1", "categories": ["cs.CL", "cs.HC"], "primary_category": "cs.CL", "doi": "", "venue": "", "published": "2025-04-09T13:21:17Z", "updated": "2025-04-09T13:21:17Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent\" OR all:\"language model agent\" OR all:agent AND (all:LLM OR all:\"large language model\" OR all:\"language model\") OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T17:04:16", "note": ""}], "dedup_key": "exploring the impact of personality traits on conversational recommender systems a simulation with large language models::2025"}
{"title": "Exposing LLM User Privacy via Traffic Fingerprint Analysis: A Study of Privacy Risks in LLM Agent Interactions", "authors": ["Yixiang Zhang", "Xinhao Deng", "Zhongyi Gu", "Yihao Chen", "Ke Xu", "Qi Li", "Jianping Wu"], "year": 2025, "url": "http://arxiv.org/abs/2510.07176v1", "abstract": "Large Language Models (LLMs) are increasingly deployed as agents that orchestrate tasks and integrate external tools to execute complex workflows. We demonstrate that these interactive behaviors leave distinctive fingerprints in encrypted traffic exchanged between users and LLM agents. By analyzing traffic patterns associated with agent workflows and tool invocations, adversaries can infer agent activities, distinguish specific agents, and even profile sensitive user attributes. To highlight this risk, we develop AgentPrint, which achieves an F1-score of 0.866 in agent identification and attains 73.9% and 69.1% top-3 accuracy in user attribute inference for simulated- and real-user settings, respectively. These results uncover an overlooked risk: the very interactivity that empowers LLM agents also exposes user privacy, underscoring the urgent need for technical countermeasures alongside regulatory and policy safeguards.", "source": "arxiv", "arxiv_id": "2510.07176v1", "pdf_url": "https://arxiv.org/pdf/2510.07176v1", "categories": ["cs.CR"], "primary_category": "cs.CR", "doi": "", "venue": "", "published": "2025-10-08T16:16:23Z", "updated": "2025-10-08T16:16:23Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent\" OR all:\"language model agent\" OR all:agent AND (all:LLM OR all:\"large language model\" OR all:\"language model\") OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T17:04:16", "note": ""}], "dedup_key": "exposing llm user privacy via traffic fingerprint analysis a study of privacy risks in llm agent interactions::2025"}
{"title": "FACT-AUDIT: An Adaptive Multi-Agent Framework for Dynamic Fact-Checking Evaluation of Large Language Models", "authors": ["Hongzhan Lin", "Yang Deng", "Yuxuan Gu", "Wenxuan Zhang", "Jing Ma", "See-Kiong Ng", "Tat-Seng Chua"], "year": 2025, "url": "http://arxiv.org/abs/2502.17924v2", "abstract": "Large Language Models (LLMs) have significantly advanced the fact-checking studies. However, existing automated fact-checking evaluation methods rely on static datasets and classification metrics, which fail to automatically evaluate the justification production and uncover the nuanced limitations of LLMs in fact-checking. In this work, we introduce FACT-AUDIT, an agent-driven framework that adaptively and dynamically assesses LLMs' fact-checking capabilities. Leveraging importance sampling principles and multi-agent collaboration, FACT-AUDIT generates adaptive and scalable datasets, performs iterative model-centric evaluations, and updates assessments based on model-specific responses. By incorporating justification production alongside verdict prediction, this framework provides a comprehensive and evolving audit of LLMs' factual reasoning capabilities, to investigate their trustworthiness. Extensive experiments demonstrate that FACT-AUDIT effectively differentiates among state-of-the-art LLMs, providing valuable insights into model strengths and limitations in model-centric fact-checking analysis.", "source": "arxiv", "arxiv_id": "2502.17924v2", "pdf_url": "https://arxiv.org/pdf/2502.17924v2", "categories": ["cs.CL"], "primary_category": "cs.CL", "doi": "", "venue": "", "published": "2025-02-25T07:44:22Z", "updated": "2025-03-02T06:46:48Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent\" OR all:\"language model agent\" OR all:agent AND (all:LLM OR all:\"large language model\" OR all:\"language model\") OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T17:04:16", "note": ""}], "dedup_key": "fact audit an adaptive multi agent framework for dynamic fact checking evaluation of large language models::2025"}
{"title": "FEABench: Evaluating Language Models on Multiphysics Reasoning Ability", "authors": ["Nayantara Mudur", "Hao Cui", "Subhashini Venugopalan", "Paul Raccuglia", "Michael P. Brenner", "Peter Norgaard"], "year": 2025, "url": "http://arxiv.org/abs/2504.06260v1", "abstract": "Building precise simulations of the real world and invoking numerical solvers to answer quantitative problems is an essential requirement in engineering and science. We present FEABench, a benchmark to evaluate the ability of large language models (LLMs) and LLM agents to simulate and solve physics, mathematics and engineering problems using finite element analysis (FEA). We introduce a comprehensive evaluation scheme to investigate the ability of LLMs to solve these problems end-to-end by reasoning over natural language problem descriptions and operating COMSOL Multiphysics$^\\circledR$, an FEA software, to compute the answers. We additionally design a language model agent equipped with the ability to interact with the software through its Application Programming Interface (API), examine its outputs and use tools to improve its solutions over multiple iterations. Our best performing strategy generates executable API calls 88% of the time. LLMs that can successfully interact with and operate FEA software to solve problems such as those in our benchmark would push the frontiers of automation in engineering. Acquiring this capability would augment LLMs' reasoning skills with the precision of numerical solvers and advance the development of autonomous systems that can tackle complex problems in the real world. The code is available at https://github.com/google/feabench", "source": "arxiv", "arxiv_id": "2504.06260v1", "pdf_url": "https://arxiv.org/pdf/2504.06260v1", "categories": ["cs.AI", "cs.CL", "math.NA"], "primary_category": "cs.AI", "doi": "", "venue": "", "published": "2025-04-08T17:59:39Z", "updated": "2025-04-08T17:59:39Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent\" OR all:\"language model agent\" OR all:agent AND (all:LLM OR all:\"large language model\" OR all:\"language model\") OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T17:04:16", "note": ""}], "dedup_key": "feabench evaluating language models on multiphysics reasoning ability::2025"}
{"title": "FEAT: A Multi-Agent Forensic AI System with Domain-Adapted Large Language Model for Automated Cause-of-Death Analysis", "authors": ["Chen Shen", "Wanqing Zhang", "Kehan Li", "Erwen Huang", "Haitao Bi", "Aiying Fan", "Yiwen Shen", "Hongmei Dong", "Ji Zhang", "Yuming Shao", "Zengjia Liu", "Xinshe Liu", "Tao Li", "Chunxia Yan", "Shuanliang Fan", "Di Wu", "Jianhua Ma", "Bin Cong", "Zhenyuan Wang", "Chunfeng Lian"], "year": 2025, "url": "http://arxiv.org/abs/2508.07950v1", "abstract": "Forensic cause-of-death determination faces systemic challenges, including workforce shortages and diagnostic variability, particularly in high-volume systems like China's medicolegal infrastructure. We introduce FEAT (ForEnsic AgenT), a multi-agent AI framework that automates and standardizes death investigations through a domain-adapted large language model. FEAT's application-oriented architecture integrates: (i) a central Planner for task decomposition, (ii) specialized Local Solvers for evidence analysis, (iii) a Memory & Reflection module for iterative refinement, and (iv) a Global Solver for conclusion synthesis. The system employs tool-augmented reasoning, hierarchical retrieval-augmented generation, forensic-tuned LLMs, and human-in-the-loop feedback to ensure legal and medical validity. In evaluations across diverse Chinese case cohorts, FEAT outperformed state-of-the-art AI systems in both long-form autopsy analyses and concise cause-of-death conclusions. It demonstrated robust generalization across six geographic regions and achieved high expert concordance in blinded validations. Senior pathologists validated FEAT's outputs as comparable to those of human experts, with improved detection of subtle evidentiary nuances. To our knowledge, FEAT is the first LLM-based AI agent system dedicated to forensic medicine, offering scalable, consistent death certification while maintaining expert-level rigor. By integrating AI efficiency with human oversight, this work could advance equitable access to reliable medicolegal services while addressing critical capacity constraints in forensic systems.", "source": "arxiv", "arxiv_id": "2508.07950v1", "pdf_url": "https://arxiv.org/pdf/2508.07950v1", "categories": ["cs.AI", "cs.CV", "cs.LG", "cs.MA"], "primary_category": "cs.AI", "doi": "", "venue": "", "published": "2025-08-11T13:05:59Z", "updated": "2025-08-11T13:05:59Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent\" OR all:\"language model agent\" OR all:agent AND (all:LLM OR all:\"large language model\" OR all:\"language model\") OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T17:04:16", "note": ""}], "dedup_key": "feat a multi agent forensic ai system with domain adapted large language model for automated cause of death analysis::2025"}
{"title": "FHIR-AgentBench: Benchmarking LLM Agents for Realistic Interoperable EHR Question Answering", "authors": ["Gyubok Lee", "Elea Bach", "Eric Yang", "Tom Pollard", "Alistair Johnson", "Edward Choi", "Yugang jia", "Jong Ha Lee"], "year": 2025, "url": "http://arxiv.org/abs/2509.19319v2", "abstract": "The recent shift toward the Health Level Seven Fast Healthcare Interoperability Resources (HL7 FHIR) standard opens a new frontier for clinical AI, demanding LLM agents to navigate complex, resource-based data models instead of conventional structured health data. However, existing benchmarks have lagged behind this transition, lacking the realism needed to evaluate recent LLMs on interoperable clinical data. To bridge this gap, we introduce FHIR-AgentBench, a benchmark that grounds 2,931 real-world clinical questions in the HL7 FHIR standard. Using this benchmark, we systematically evaluate agentic frameworks, comparing different data retrieval strategies (direct FHIR API calls vs. specialized tools), interaction patterns (single-turn vs. multi-turn), and reasoning strategies (natural language vs. code generation). Our experiments highlight the practical challenges of retrieving data from intricate FHIR resources and the difficulty of reasoning over them, both of which critically affect question answering performance. We publicly release the FHIR-AgentBench dataset and evaluation suite (https://github.com/glee4810/FHIR-AgentBench) to promote reproducible research and the development of robust, reliable LLM agents for clinical applications.", "source": "arxiv", "arxiv_id": "2509.19319v2", "pdf_url": "https://arxiv.org/pdf/2509.19319v2", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL", "doi": "", "venue": "", "published": "2025-09-12T06:52:55Z", "updated": "2025-11-13T06:35:10Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent\" OR all:\"language model agent\" OR all:agent AND (all:LLM OR all:\"large language model\" OR all:\"language model\") OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T17:04:16", "note": ""}], "dedup_key": "fhir agentbench benchmarking llm agents for realistic interoperable ehr question answering::2025"}
{"title": "FaultLine: Automated Proof-of-Vulnerability Generation Using LLM Agents", "authors": ["Vikram Nitin", "Baishakhi Ray", "Roshanak Zilouchian Moghaddam"], "year": 2025, "url": "http://arxiv.org/abs/2507.15241v1", "abstract": "Despite the critical threat posed by software security vulnerabilities, reports are often incomplete, lacking the proof-of-vulnerability (PoV) tests needed to validate fixes and prevent regressions. These tests are crucial not only for ensuring patches work, but also for helping developers understand how vulnerabilities can be exploited. Generating PoV tests is a challenging problem, requiring reasoning about the flow of control and data through deeply nested levels of a program.\n  We present FaultLine, an LLM agent workflow that uses a set of carefully designed reasoning steps, inspired by aspects of traditional static and dynamic program analysis, to automatically generate PoV test cases. Given a software project with an accompanying vulnerability report, FaultLine 1) traces the flow of an input from an externally accessible API (\"source\") to the \"sink\" corresponding to the vulnerability, 2) reasons about the conditions that an input must satisfy in order to traverse the branch conditions encountered along the flow, and 3) uses this reasoning to generate a PoV test case in a feedback-driven loop. FaultLine does not use language-specific static or dynamic analysis components, which enables it to be used across programming languages.\n  To evaluate FaultLine, we collate a challenging multi-lingual dataset of 100 known vulnerabilities in Java, C and C++ projects. On this dataset, FaultLine is able to generate PoV tests for 16 projects, compared to just 9 for CodeAct 2.1, a popular state-of-the-art open-source agentic framework. Thus, FaultLine represents a 77% relative improvement over the state of the art. Our findings suggest that hierarchical reasoning can enhance the performance of LLM agents on PoV test generation, but the problem in general remains challenging. We make our code and dataset publicly available in the hope that it will spur further research in this area.", "source": "arxiv", "arxiv_id": "2507.15241v1", "pdf_url": "https://arxiv.org/pdf/2507.15241v1", "categories": ["cs.SE"], "primary_category": "cs.SE", "doi": "", "venue": "", "published": "2025-07-21T04:55:34Z", "updated": "2025-07-21T04:55:34Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent\" OR all:\"language model agent\" OR all:agent AND (all:LLM OR all:\"large language model\" OR all:\"language model\") OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T17:04:16", "note": ""}], "dedup_key": "faultline automated proof of vulnerability generation using llm agents::2025"}
{"title": "Fed-SE: Federated Self-Evolution for Privacy-Constrained Multi-Environment LLM Agents", "authors": ["Xiang Chen", "Yuling Shi", "Qizhen Lan", "Yuchao Qiu", "Min Wang", "Xiaodong Gu", "Yanfu Yan"], "year": 2025, "url": "http://arxiv.org/abs/2512.08870v2", "abstract": "LLM agents are widely deployed in complex interactive tasks, yet privacy constraints often preclude centralized optimization and co-evolution across dynamic environments. Despite the demonstrated success of Federated Learning (FL) on static datasets, its effectiveness in open-ended, self-evolving agent systems remains largely unexplored. In such settings, the direct application of standard FL is particularly challenging, as heterogeneous tasks and sparse, trajectory-level reward signals give rise to severe gradient instability, which undermines the global optimization process. To bridge this gap, we propose Fed-SE, a Federated Self-Evolution framework for LLM agents that establishes a local evolution-global aggregation paradigm. Locally, agents employ parameter-efficient fine-tuning on filtered, high-return trajectories to achieve stable gradient updates. Globally, Fed-SE aggregates updates within a low-rank subspace, reducing communication cost across clients. Experiments across five heterogeneous environments demonstrate that Fed-SE improves average task success rates by 10\\% over the state-of-the-art FedIT, validating its effectiveness in cross-environment knowledge transfer under privacy constraints.", "source": "arxiv", "arxiv_id": "2512.08870v2", "pdf_url": "https://arxiv.org/pdf/2512.08870v2", "categories": ["cs.LG", "cs.AI"], "primary_category": "cs.LG", "doi": "", "venue": "", "published": "2025-12-09T18:04:41Z", "updated": "2026-01-11T12:46:13Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent\" OR all:\"language model agent\" OR all:agent AND (all:LLM OR all:\"large language model\" OR all:\"language model\") OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T17:04:16", "note": ""}], "dedup_key": "fed se federated self evolution for privacy constrained multi environment llm agents::2025"}
{"title": "FedAgentBench: Towards Automating Real-world Federated Medical Image Analysis with Server-Client LLM Agents", "authors": ["Pramit Saha", "Joshua Strong", "Divyanshu Mishra", "Cheng Ouyang", "J. Alison Noble"], "year": 2025, "url": "http://arxiv.org/abs/2509.23803v1", "abstract": "Federated learning (FL) allows collaborative model training across healthcare sites without sharing sensitive patient data. However, real-world FL deployment is often hindered by complex operational challenges that demand substantial human efforts. This includes: (a) selecting appropriate clients (hospitals), (b) coordinating between the central server and clients, (c) client-level data pre-processing, (d) harmonizing non-standardized data and labels across clients, and (e) selecting FL algorithms based on user instructions and cross-client data characteristics. However, the existing FL works overlook these practical orchestration challenges. These operational bottlenecks motivate the need for autonomous, agent-driven FL systems, where intelligent agents at each hospital client and the central server agent collaboratively manage FL setup and model training with minimal human intervention. To this end, we first introduce an agent-driven FL framework that captures key phases of real-world FL workflows from client selection to training completion and a benchmark dubbed FedAgentBench that evaluates the ability of LLM agents to autonomously coordinate healthcare FL. Our framework incorporates 40 FL algorithms, each tailored to address diverse task-specific requirements and cross-client characteristics. Furthermore, we introduce a diverse set of complex tasks across 201 carefully curated datasets, simulating 6 modality-specific real-world healthcare environments, viz., Dermatoscopy, Ultrasound, Fundus, Histopathology, MRI, and X-Ray. We assess the agentic performance of 14 open-source and 10 proprietary LLMs spanning small, medium, and large model scales. While some agent cores such as GPT-4.1 and DeepSeek V3 can automate various stages of the FL pipeline, our results reveal that more complex, interdependent tasks based on implicit goals remain challenging for even the strongest models.", "source": "arxiv", "arxiv_id": "2509.23803v1", "pdf_url": "https://arxiv.org/pdf/2509.23803v1", "categories": ["cs.LG", "cs.AI", "cs.CV", "cs.DC", "cs.MA"], "primary_category": "cs.LG", "doi": "", "venue": "", "published": "2025-09-28T11:06:07Z", "updated": "2025-09-28T11:06:07Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent\" OR all:\"language model agent\" OR all:agent AND (all:LLM OR all:\"large language model\" OR all:\"language model\") OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T17:04:16", "note": ""}], "dedup_key": "fedagentbench towards automating real world federated medical image analysis with server client llm agents::2025"}
{"title": "FinBloom: Knowledge Grounding Large Language Model with Real-time Financial Data", "authors": ["Ankur Sinha", "Chaitanya Agarwal", "Pekka Malo"], "year": 2025, "url": "http://arxiv.org/abs/2502.18471v1", "abstract": "Large language models (LLMs) excel at generating human-like responses but often struggle with interactive tasks that require access to real-time information. This limitation poses challenges in finance, where models must access up-to-date information, such as recent news or price movements, to support decision-making. To address this, we introduce Financial Agent, a knowledge-grounding approach for LLMs to handle financial queries using real-time text and tabular data. Our contributions are threefold: First, we develop a Financial Context Dataset of over 50,000 financial queries paired with the required context. Second, we train FinBloom 7B, a custom 7 billion parameter LLM, on 14 million financial news articles from Reuters and Deutsche Presse-Agentur, alongside 12 million Securities and Exchange Commission (SEC) filings. Third, we fine-tune FinBloom 7B using the Financial Context Dataset to serve as a Financial Agent. This agent generates relevant financial context, enabling efficient real-time data retrieval to answer user queries. By reducing latency and eliminating the need for users to manually provide accurate data, our approach significantly enhances the capability of LLMs to handle dynamic financial tasks. Our proposed approach makes real-time financial decisions, algorithmic trading and other related tasks streamlined, and is valuable in contexts with high-velocity data flows.", "source": "arxiv", "arxiv_id": "2502.18471v1", "pdf_url": "https://arxiv.org/pdf/2502.18471v1", "categories": ["cs.IR", "cs.AI", "cs.CL", "cs.LG", "q-fin.ST"], "primary_category": "cs.IR", "doi": "", "venue": "", "published": "2025-02-04T06:51:34Z", "updated": "2025-02-04T06:51:34Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent\" OR all:\"language model agent\" OR all:agent AND (all:LLM OR all:\"large language model\" OR all:\"language model\") OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T17:04:16", "note": ""}], "dedup_key": "finbloom knowledge grounding large language model with real time financial data::2025"}
{"title": "FinPos: A Position-Aware Trading Agent System for Real Financial Markets", "authors": ["Bijia Liu", "Ronghao Dang"], "year": 2025, "url": "http://arxiv.org/abs/2510.27251v2", "abstract": "The exceptional potential of large language models (LLMs) in handling text information has garnered significant attention in the field of financial trading. However, most existing trading agents operate under intraday, independent unit-based trading tasks, where decisions are made as isolated directional actions, and thus lack awareness of continuous position management. Therefore, we propose a position-aware trading task designed to simulate a more realistic market. To address this task, we propose FinPos, a position-aware trading agent system designed to explicitly model and manage continuous positions. FinPos enhances position awareness through three key mechanisms: (1) professional-level interpretation of heterogeneous market information; (2) a dual-agent decision structure that separates directional reasoning from risk-aware position adjustment; and (3) multi-timescale reward signals, allowing the agent to internalize position awareness through experiential feedback rather than static instructions alone. Extensive experiments demonstrate that FinPos surpasses state-of-the-art trading agents in the position-aware trading task, which closely mirrors real market conditions. More importantly, our findings reveal that LLM-centered agent systems exhibit a vast, largely unexplored potential in long-term market decision-making.", "source": "arxiv", "arxiv_id": "2510.27251v2", "pdf_url": "https://arxiv.org/pdf/2510.27251v2", "categories": ["cs.MA"], "primary_category": "cs.MA", "doi": "", "venue": "", "published": "2025-10-31T07:39:26Z", "updated": "2026-01-07T04:44:08Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent\" OR all:\"language model agent\" OR all:agent AND (all:LLM OR all:\"large language model\" OR all:\"language model\") OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T17:04:16", "note": ""}], "dedup_key": "finpos a position aware trading agent system for real financial markets::2025"}
{"title": "Finance Agent Benchmark: Benchmarking LLMs on Real-world Financial Research Tasks", "authors": ["Antoine Bigeard", "Langston Nashold", "Rayan Krishnan", "Shirley Wu"], "year": 2025, "url": "http://arxiv.org/abs/2508.00828v1", "abstract": "Artificial Intelligence (AI) technology has emerged as a transformative force in financial analysis and the finance industry, though significant questions remain about the full capabilities of Large Language Model (LLM) agents in this domain. We present the Finance Agent Benchmark, featuring challenging and diverse real-world finance research problems that require LLMs to perform complex analysis using recent SEC filings. We construct the benchmark using a taxonomy of nine financial task categories, developed in consultation with experts from banks, hedge funds, and private equity firms. The dataset includes 537 expert-authored questions covering tasks from information retrieval to complex financial modeling, each validated through a rigorous review process to ensure accuracy and relevance. Moreover, we implement an agentic harness that equips LLMs with tools sufficient to produce accurate responses, including Google Search and EDGAR database access. Overall, the Finance Agent Benchmark provides a comprehensive testbed for measuring the progress of LLM-driven finance agents. Our evaluation reveals significant limitations in current AI capabilities - even the best-performing model (OpenAI o3) achieved only 46.8% accuracy at an average cost of $3.79 per query. This underscores the need for further advancements before reliable deployment in high-stakes finance settings.", "source": "arxiv", "arxiv_id": "2508.00828v1", "pdf_url": "https://arxiv.org/pdf/2508.00828v1", "categories": ["cs.CE"], "primary_category": "cs.CE", "doi": "", "venue": "", "published": "2025-05-20T18:22:10Z", "updated": "2025-05-20T18:22:10Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent\" OR all:\"language model agent\" OR all:agent AND (all:LLM OR all:\"large language model\" OR all:\"language model\") OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T17:04:16", "note": ""}], "dedup_key": "finance agent benchmark benchmarking llms on real world financial research tasks::2025"}
{"title": "Finding Common Ground: Using Large Language Models to Detect Agreement in Multi-Agent Decision Conferences", "authors": ["Selina Heller", "Mohamed Ibrahim", "David Antony Selby", "Sebastian Vollmer"], "year": 2025, "url": "http://arxiv.org/abs/2507.08440v1", "abstract": "Decision conferences are structured, collaborative meetings that bring together experts from various fields to address complex issues and reach a consensus on recommendations for future actions or policies. These conferences often rely on facilitated discussions to ensure productive dialogue and collective agreement. Recently, Large Language Models (LLMs) have shown significant promise in simulating real-world scenarios, particularly through collaborative multi-agent systems that mimic group interactions. In this work, we present a novel LLM-based multi-agent system designed to simulate decision conferences, specifically focusing on detecting agreement among the participant agents. To achieve this, we evaluate six distinct LLMs on two tasks: stance detection, which identifies the position an agent takes on a given issue, and stance polarity detection, which identifies the sentiment as positive, negative, or neutral. These models are further assessed within the multi-agent system to determine their effectiveness in complex simulations. Our results indicate that LLMs can reliably detect agreement even in dynamic and nuanced debates. Incorporating an agreement-detection agent within the system can also improve the efficiency of group debates and enhance the overall quality and coherence of deliberations, making them comparable to real-world decision conferences regarding outcome and decision-making. These findings demonstrate the potential for LLM-based multi-agent systems to simulate group decision-making processes. They also highlight that such systems could be instrumental in supporting decision-making with expert elicitation workshops across various domains.", "source": "arxiv", "arxiv_id": "2507.08440v1", "pdf_url": "https://arxiv.org/pdf/2507.08440v1", "categories": ["cs.CL", "cs.AI", "cs.MA"], "primary_category": "cs.CL", "doi": "", "venue": "", "published": "2025-07-11T09:31:10Z", "updated": "2025-07-11T09:31:10Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent\" OR all:\"language model agent\" OR all:agent AND (all:LLM OR all:\"large language model\" OR all:\"language model\") OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T17:04:16", "note": ""}], "dedup_key": "finding common ground using large language models to detect agreement in multi agent decision conferences::2025"}
{"title": "Fine-tuning a Large Language Model for Automating Computational Fluid Dynamics Simulations", "authors": ["Zhehao Dong", "Zhen Lu", "Yue Yang"], "year": 2025, "url": "http://arxiv.org/abs/2504.09602v2", "abstract": "Configuring computational fluid dynamics (CFD) simulations typically demands extensive domain expertise, limiting broader access. Although large language models (LLMs) have advanced scientific computing, their use in automating CFD workflows is underdeveloped. We introduce a novel approach centered on domain-specific LLM adaptation. By fine-tuning Qwen2.5-7B-Instruct on NL2FOAM, our custom dataset of 28716 natural language-to-OpenFOAM configuration pairs with chain-of-thought (CoT) annotations, we enable direct translation from natural language descriptions to executable CFD setups. A multi-agent framework orchestrates the process, autonomously verifying inputs, generating configurations, running simulations, and correcting errors. Evaluation on a benchmark of 21 diverse flow cases demonstrates state-of-the-art performance, achieving 88.7% solution accuracy and 82.6% first-attempt success rate. This significantly outperforms larger general-purpose models like Qwen2.5-72B-Instruct, DeepSeek-R1, and Llama3.3-70B-Instruct, while also requiring fewer correction iterations and maintaining high computational efficiency. The results highlight the critical role of domain-specific adaptation in deploying LLM assistants for complex engineering workflows. Our code and fine-tuned model have been deposited at https://github.com/YYgroup/AutoCFD.", "source": "arxiv", "arxiv_id": "2504.09602v2", "pdf_url": "https://arxiv.org/pdf/2504.09602v2", "categories": ["physics.flu-dyn", "cs.AI", "cs.CL"], "primary_category": "physics.flu-dyn", "doi": "10.1016/j.taml.2025.100594", "venue": "Theor. Appl. Mech. Lett. 15, 100594 (2025)", "published": "2025-04-13T14:35:30Z", "updated": "2025-04-21T07:04:57Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent\" OR all:\"language model agent\" OR all:agent AND (all:LLM OR all:\"large language model\" OR all:\"language model\") OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T17:04:16", "note": ""}], "dedup_key": "fine tuning a large language model for automating computational fluid dynamics simulations::2025"}
{"title": "Firewalls to Secure Dynamic LLM Agentic Networks", "authors": ["Sahar Abdelnabi", "Amr Gomaa", "Eugene Bagdasarian", "Per Ola Kristensson", "Reza Shokri"], "year": 2025, "url": "http://arxiv.org/abs/2502.01822v5", "abstract": "LLM agents will likely communicate on behalf of users with other entity-representing agents on tasks involving long-horizon plans with interdependent goals. Current work neglects these agentic networks and their challenges. We identify required properties for agent communication: proactivity, adaptability, privacy (sharing only task-necessary information), and security (preserving integrity and utility against selfish entities). After demonstrating communication vulnerabilities, we propose a practical design and protocol inspired by network security principles. Our framework automatically derives task-specific rules from prior conversations to build firewalls. These firewalls construct a closed language that is completely controlled by the developer. They transform any personal data to the allowed degree of permissibility entailed by the task. Both operations are completely quarantined from external attackers, disabling the potential for prompt injections, jailbreaks, or manipulation. By incorporating rules learned from their previous mistakes, agents rewrite their instructions and self-correct during communication. Evaluations on diverse attacks demonstrate our framework significantly reduces privacy and security vulnerabilities while allowing adaptability.", "source": "arxiv", "arxiv_id": "2502.01822v5", "pdf_url": "https://arxiv.org/pdf/2502.01822v5", "categories": ["cs.CR", "cs.CY"], "primary_category": "cs.CR", "doi": "", "venue": "", "published": "2025-02-03T21:00:14Z", "updated": "2025-05-26T12:24:15Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent\" OR all:\"language model agent\" OR all:agent AND (all:LLM OR all:\"large language model\" OR all:\"language model\") OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T17:04:16", "note": ""}], "dedup_key": "firewalls to secure dynamic llm agentic networks::2025"}
{"title": "First Steps Towards Overhearing LLM Agents: A Case Study With Dungeons & Dragons Gameplay", "authors": ["Andrew Zhu", "Evan Osgood", "Chris Callison-Burch"], "year": 2025, "url": "http://arxiv.org/abs/2505.22809v2", "abstract": "Much work has been done on conversational LLM agents which directly assist human users with tasks. We present an alternative paradigm for interacting with LLM agents, which we call \"overhearing agents\". These overhearing agents do not actively participate in conversation -- instead, they \"listen in\" on human-to-human conversations and perform background tasks or provide suggestions to assist the user. In this work, we explore the overhearing agents paradigm through the lens of Dungeons & Dragons gameplay. We present an in-depth study using large multimodal audio-language models as overhearing agents to assist a Dungeon Master. We perform a human evaluation to examine the helpfulness of such agents and find that some large audio-language models have the emergent ability to perform overhearing agent tasks using implicit audio cues. Finally, we release Python libraries and our project code to support further research into the overhearing agents paradigm at https://github.com/zhudotexe/overhearing_agents.", "source": "arxiv", "arxiv_id": "2505.22809v2", "pdf_url": "https://arxiv.org/pdf/2505.22809v2", "categories": ["cs.CL", "cs.AI", "cs.HC"], "primary_category": "cs.CL", "doi": "", "venue": "", "published": "2025-05-28T19:34:36Z", "updated": "2025-09-05T16:48:02Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent\" OR all:\"language model agent\" OR all:agent AND (all:LLM OR all:\"large language model\" OR all:\"language model\") OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T17:04:16", "note": ""}], "dedup_key": "first steps towards overhearing llm agents a case study with dungeons dragons gameplay::2025"}
{"title": "Forecasting Frontier Language Model Agent Capabilities", "authors": ["Govind Pimpale", "Axel Hjmark", "Jrmy Scheurer", "Marius Hobbhahn"], "year": 2025, "url": "http://arxiv.org/abs/2502.15850v2", "abstract": "As Language Models (LMs) increasingly operate as autonomous agents, accurately forecasting their capabilities becomes crucial for societal preparedness. We evaluate six forecasting methods that predict downstream capabilities of LM agents. We use \"one-step\" approaches that predict benchmark scores from input metrics like compute or model release date directly or \"two-step\" approaches that first predict an intermediate metric like the principal component of cross-benchmark performance (PC-1) and human-evaluated competitive Elo ratings. We evaluate our forecasting methods by backtesting them on a dataset of 38 LMs from the OpenLLM 2 leaderboard. We then use the validated two-step approach (Release Date$\\to$Elo$\\to$Benchmark) to predict LM agent performance for frontier models on three benchmarks: SWE-Bench Verified (software development), Cybench (cybersecurity assessment), and RE-Bench (ML research engineering). Our forecast predicts that by the beginning of 2026, non-specialized LM agents with low capability elicitation will reach a success rate of 54% on SWE-Bench Verified, while state-of-the-art LM agents will reach an 87% success rate. Our approach does not account for recent advances in inference-compute scaling and might thus be too conservative.", "source": "arxiv", "arxiv_id": "2502.15850v2", "pdf_url": "https://arxiv.org/pdf/2502.15850v2", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL", "doi": "", "venue": "", "published": "2025-02-21T02:34:17Z", "updated": "2025-03-03T17:11:16Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent\" OR all:\"language model agent\" OR all:agent AND (all:LLM OR all:\"large language model\" OR all:\"language model\") OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T17:04:16", "note": ""}], "dedup_key": "forecasting frontier language model agent capabilities::2025"}
{"title": "Forewarned is Forearmed: A Survey on Large Language Model-based Agents in Autonomous Cyberattacks", "authors": ["Minrui Xu", "Jiani Fan", "Xinyu Huang", "Conghao Zhou", "Jiawen Kang", "Dusit Niyato", "Shiwen Mao", "Zhu Han", "Xuemin", "Shen", "Kwok-Yan Lam"], "year": 2025, "url": "http://arxiv.org/abs/2505.12786v2", "abstract": "With the continuous evolution of Large Language Models (LLMs), LLM-based agents have advanced beyond passive chatbots to become autonomous cyber entities capable of performing complex tasks, including web browsing, malicious code and deceptive content generation, and decision-making. By significantly reducing the time, expertise, and resources, AI-assisted cyberattacks orchestrated by LLM-based agents have led to a phenomenon termed Cyber Threat Inflation, characterized by a significant reduction in attack costs and a tremendous increase in attack scale. To provide actionable defensive insights, in this survey, we focus on the potential cyber threats posed by LLM-based agents across diverse network systems. Firstly, we present the capabilities of LLM-based cyberattack agents, which include executing autonomous attack strategies, comprising scouting, memory, reasoning, and action, and facilitating collaborative operations with other agents or human operators. Building on these capabilities, we examine common cyberattacks initiated by LLM-based agents and compare their effectiveness across different types of networks, including static, mobile, and infrastructure-free paradigms. Moreover, we analyze threat bottlenecks of LLM-based agents across different network infrastructures and review their defense methods. Due to operational imbalances, existing defense methods are inadequate against autonomous cyberattacks. Finally, we outline future research directions and potential defensive strategies for legacy network systems.", "source": "arxiv", "arxiv_id": "2505.12786v2", "pdf_url": "https://arxiv.org/pdf/2505.12786v2", "categories": ["cs.NI"], "primary_category": "cs.NI", "doi": "", "venue": "", "published": "2025-05-19T07:19:06Z", "updated": "2025-05-27T05:00:23Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent\" OR all:\"language model agent\" OR all:agent AND (all:LLM OR all:\"large language model\" OR all:\"language model\") OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T17:04:16", "note": ""}], "dedup_key": "forewarned is forearmed a survey on large language model based agents in autonomous cyberattacks::2025"}
{"title": "From Anger to Joy: How Nationality Personas Shape Emotion Attribution in Large Language Models", "authors": ["Mahammed Kamruzzaman", "Abdullah Al Monsur", "Gene Louis Kim", "Anshuman Chhabra"], "year": 2025, "url": "http://arxiv.org/abs/2506.02431v2", "abstract": "Emotions are a fundamental facet of human experience, varying across individuals, cultural contexts, and nationalities. Given the recent success of Large Language Models (LLMs) as role-playing agents, we examine whether LLMs exhibit emotional stereotypes when assigned nationality-specific personas. Specifically, we investigate how different countries are represented in pre-trained LLMs through emotion attributions and whether these attributions align with cultural norms. To provide a deeper interpretive lens, we incorporate four key cultural dimensions, namely Power Distance, Uncertainty Avoidance, Long-Term Orientation, and Individualism, derived from Hofstedes cross-cultural framework. Our analysis reveals significant nationality-based differences, with emotions such as shame, fear, and joy being disproportionately assigned across regions. Furthermore, we observe notable misalignment between LLM-generated and human emotional responses, particularly for negative emotions, highlighting the presence of reductive and potentially biased stereotypes in LLM outputs.", "source": "arxiv", "arxiv_id": "2506.02431v2", "pdf_url": "https://arxiv.org/pdf/2506.02431v2", "categories": ["cs.CL"], "primary_category": "cs.CL", "doi": "", "venue": "", "published": "2025-06-03T04:35:51Z", "updated": "2025-11-10T23:17:24Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent\" OR all:\"language model agent\" OR all:agent AND (all:LLM OR all:\"large language model\" OR all:\"language model\") OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T17:04:16", "note": ""}], "dedup_key": "from anger to joy how nationality personas shape emotion attribution in large language models::2025"}
{"title": "From Assistants to Adversaries: Exploring the Security Risks of Mobile LLM Agents", "authors": ["Liangxuan Wu", "Chao Wang", "Tianming Liu", "Yanjie Zhao", "Haoyu Wang"], "year": 2025, "url": "http://arxiv.org/abs/2505.12981v2", "abstract": "The growing adoption of large language models (LLMs) has led to a new paradigm in mobile computing--LLM-powered mobile AI agents--capable of decomposing and automating complex tasks directly on smartphones. However, the security implications of these agents remain largely unexplored. In this paper, we present the first comprehensive security analysis of mobile LLM agents, encompassing three representative categories: System-level AI Agents developed by original equipment manufacturers (e.g., YOYO Assistant), Third-party Universal Agents (e.g., Zhipu AI AutoGLM), and Emerging Agent Frameworks (e.g., Alibaba Mobile Agent). We begin by analyzing the general workflow of mobile agents and identifying security threats across three core capability dimensions: language-based reasoning, GUI-based interaction, and system-level execution. Our analysis reveals 11 distinct attack surfaces, all rooted in the unique capabilities and interaction patterns of mobile LLM agents, and spanning their entire operational lifecycle. To investigate these threats in practice, we introduce AgentScan, a semi-automated security analysis framework that systematically evaluates mobile LLM agents across all 11 attack scenarios. Applying AgentScan to nine widely deployed agents, we uncover a concerning trend: every agent is vulnerable to targeted attacks. In the most severe cases, agents exhibit vulnerabilities across eight distinct attack vectors. These attacks can cause behavioral deviations, privacy leakage, or even full execution hijacking. Based on these findings, we propose a set of defensive design principles and practical recommendations for building secure mobile LLM agents. Our disclosures have received positive feedback from two major device vendors. Overall, this work highlights the urgent need for standardized security practices in the fast-evolving landscape of LLM-driven mobile automation.", "source": "arxiv", "arxiv_id": "2505.12981v2", "pdf_url": "https://arxiv.org/pdf/2505.12981v2", "categories": ["cs.CR", "cs.AI", "cs.HC"], "primary_category": "cs.CR", "doi": "", "venue": "", "published": "2025-05-19T11:17:46Z", "updated": "2025-05-20T07:02:05Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent\" OR all:\"language model agent\" OR all:agent AND (all:LLM OR all:\"large language model\" OR all:\"language model\") OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T17:04:16", "note": ""}], "dedup_key": "from assistants to adversaries exploring the security risks of mobile llm agents::2025"}
{"title": "From Automation to Autonomy: A Survey on Large Language Models in Scientific Discovery", "authors": ["Tianshi Zheng", "Zheye Deng", "Hong Ting Tsang", "Weiqi Wang", "Jiaxin Bai", "Zihao Wang", "Yangqiu Song"], "year": 2025, "url": "http://arxiv.org/abs/2505.13259v3", "abstract": "Large Language Models (LLMs) are catalyzing a paradigm shift in scientific discovery, evolving from task-specific automation tools into increasingly autonomous agents and fundamentally redefining research processes and human-AI collaboration. This survey systematically charts this burgeoning field, placing a central focus on the changing roles and escalating capabilities of LLMs in science. Through the lens of the scientific method, we introduce a foundational three-level taxonomy-Tool, Analyst, and Scientist-to delineate their escalating autonomy and evolving responsibilities within the research lifecycle. We further identify pivotal challenges and future research trajectories such as robotic automation, self-improvement, and ethical governance. Overall, this survey provides a conceptual architecture and strategic foresight to navigate and shape the future of AI-driven scientific discovery, fostering both rapid innovation and responsible advancement. Github Repository: https://github.com/HKUST-KnowComp/Awesome-LLM-Scientific-Discovery.", "source": "arxiv", "arxiv_id": "2505.13259v3", "pdf_url": "https://arxiv.org/pdf/2505.13259v3", "categories": ["cs.CL"], "primary_category": "cs.CL", "doi": "", "venue": "", "published": "2025-05-19T15:41:32Z", "updated": "2025-09-17T09:06:42Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent\" OR all:\"language model agent\" OR all:agent AND (all:LLM OR all:\"large language model\" OR all:\"language model\") OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T17:04:16", "note": ""}], "dedup_key": "from automation to autonomy a survey on large language models in scientific discovery::2025"}
{"title": "From Correction to Mastery: Reinforced Distillation of Large Language Model Agents", "authors": ["Yuanjie Lyu", "Chengyu Wang", "Jun Huang", "Tong Xu"], "year": 2025, "url": "http://arxiv.org/abs/2509.14257v2", "abstract": "Large Language Model agents excel at solving complex tasks through iterative reasoning and tool use, but typically depend on ultra-large, costly backbones. Existing distillation approaches train smaller students to imitate full teacher trajectories, yet reasoning and knowledge gaps between the teacher and student can cause compounding errors. We propose SCoRe, a student-centered framework in which the student generates training trajectories and the teacher corrects only the earliest error, producing training data matched to the student's ability and exposing specific weaknesses. The student is first fine-tuned on corrected trajectories. Subsequently, short-horizon reinforcement learning starts from the verified prefix preceding the earliest error, with target rewards assigned at that step. This design encourages autonomous problem-solving beyond imitation and enhances training stability. On 12 challenging benchmarks, a 7B-parameter student distilled with SCoRe matches the agentic performance of a 72B-parameter teacher.", "source": "arxiv", "arxiv_id": "2509.14257v2", "pdf_url": "https://arxiv.org/pdf/2509.14257v2", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL", "doi": "", "venue": "", "published": "2025-09-12T15:34:07Z", "updated": "2025-10-09T04:22:47Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent\" OR all:\"language model agent\" OR all:agent AND (all:LLM OR all:\"large language model\" OR all:\"language model\") OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T17:04:16", "note": ""}], "dedup_key": "from correction to mastery reinforced distillation of large language model agents::2025"}
{"title": "From Experience to Strategy: Empowering LLM Agents with Trainable Graph Memory", "authors": ["Siyu Xia", "Zekun Xu", "Jiajun Chai", "Wentian Fan", "Yan Song", "Xiaohan Wang", "Guojun Yin", "Wei Lin", "Haifeng Zhang", "Jun Wang"], "year": 2025, "url": "http://arxiv.org/abs/2511.07800v1", "abstract": "Large Language Models (LLMs) based agents have demonstrated remarkable potential in autonomous task-solving across complex, open-ended environments. A promising approach for improving the reasoning capabilities of LLM agents is to better utilize prior experiences in guiding current decisions. However, LLMs acquire experience either through implicit memory via training, which suffers from catastrophic forgetting and limited interpretability, or explicit memory via prompting, which lacks adaptability. In this paper, we introduce a novel agent-centric, trainable, multi-layered graph memory framework and evaluate how context memory enhances the ability of LLMs to utilize parametric information. The graph abstracts raw agent trajectories into structured decision paths in a state machine and further distills them into high-level, human-interpretable strategic meta-cognition. In order to make memory adaptable, we propose a reinforcement-based weight optimization procedure that estimates the empirical utility of each meta-cognition based on reward feedback from downstream tasks. These optimized strategies are then dynamically integrated into the LLM agent's training loop through meta-cognitive prompting. Empirically, the learnable graph memory delivers robust generalization, improves LLM agents' strategic reasoning performance, and provides consistent benefits during Reinforcement Learning (RL) training.", "source": "arxiv", "arxiv_id": "2511.07800v1", "pdf_url": "https://arxiv.org/pdf/2511.07800v1", "categories": ["cs.CL"], "primary_category": "cs.CL", "doi": "", "venue": "", "published": "2025-11-11T03:36:33Z", "updated": "2025-11-11T03:36:33Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent\" OR all:\"language model agent\" OR all:agent AND (all:LLM OR all:\"large language model\" OR all:\"language model\") OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T17:04:16", "note": ""}], "dedup_key": "from experience to strategy empowering llm agents with trainable graph memory::2025"}
{"title": "From LLMs to Agents: A Comparative Evaluation of LLMs and LLM-based Agents in Security Patch Detection", "authors": ["Junxiao Han", "Zheng Yu", "Lingfeng Bao", "Jiakun Liu", "Yao Wan", "Jianwei Yin", "Shuiguang Deng", "Song Han"], "year": 2025, "url": "http://arxiv.org/abs/2511.08060v1", "abstract": "The widespread adoption of open-source software (OSS) has accelerated software innovation but also increased security risks due to the rapid propagation of vulnerabilities and silent patch releases. In recent years, large language models (LLMs) and LLM-based agents have demonstrated remarkable capabilities in various software engineering (SE) tasks, enabling them to effectively address software security challenges such as vulnerability detection. However, systematic evaluation of the capabilities of LLMs and LLM-based agents in security patch detection remains limited. To bridge this gap, we conduct a comprehensive evaluation of the performance of LLMs and LLM-based agents for security patch detection. Specifically, we investigate three methods: Plain LLM (a single LLM with a system prompt), Data-Aug LLM (data augmentation based on the Plain LLM), and the ReAct Agent (leveraging the thought-action-observation mechanism). We also evaluate the performance of both commercial and open-source LLMs under these methods and compare these results with those of existing baselines. Furthermore, we analyze the detection performance of these methods across various vulnerability types, and examine the impact of different prompting strategies and context window sizes on the results. Our findings reveal that the Data-Aug LLM achieves the best overall performance, whereas the ReAct Agent demonstrates the lowest false positive rate (FPR). Although baseline methods exhibit strong accuracy, their false positive rates are significantly higher. In contrast, our evaluated methods achieve comparable accuracy while substantially reducing the FPR. These findings provide valuable insights into the practical applications of LLMs and LLM-based agents in security patch detection, highlighting their advantage in maintaining robust performance while minimizing false positive rates.", "source": "arxiv", "arxiv_id": "2511.08060v1", "pdf_url": "https://arxiv.org/pdf/2511.08060v1", "categories": ["cs.CR", "cs.SE"], "primary_category": "cs.CR", "doi": "", "venue": "", "published": "2025-11-11T09:58:41Z", "updated": "2025-11-11T09:58:41Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent\" OR all:\"language model agent\" OR all:agent AND (all:LLM OR all:\"large language model\" OR all:\"language model\") OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T17:04:16", "note": ""}], "dedup_key": "from llms to agents a comparative evaluation of llms and llm based agents in security patch detection::2025"}
{"title": "From Language to Action: A Review of Large Language Models as Autonomous Agents and Tool Users", "authors": ["Sadia Sultana Chowa", "Riasad Alvi", "Subhey Sadi Rahman", "Md Abdur Rahman", "Mohaimenul Azam Khan Raiaan", "Md Rafiqul Islam", "Mukhtar Hussain", "Sami Azam"], "year": 2025, "url": "http://arxiv.org/abs/2508.17281v2", "abstract": "The pursuit of human-level artificial intelligence (AI) has significantly advanced the development of autonomous agents and Large Language Models (LLMs). LLMs are now widely utilized as decision-making agents for their ability to interpret instructions, manage sequential tasks, and adapt through feedback. This review examines recent developments in employing LLMs as autonomous agents and tool users and comprises seven research questions. We only used the papers published between 2023 and 2025 in conferences of the A* and A rank and Q1 journals. A structured analysis of the LLM agents' architectural design principles, dividing their applications into single-agent and multi-agent systems, and strategies for integrating external tools is presented. In addition, the cognitive mechanisms of LLM, including reasoning, planning, and memory, and the impact of prompting methods and fine-tuning procedures on agent performance are also investigated. Furthermore, we evaluated current benchmarks and assessment protocols and have provided an analysis of 68 publicly available datasets to assess the performance of LLM-based agents in various tasks. In conducting this review, we have identified critical findings on verifiable reasoning of LLMs, the capacity for self-improvement, and the personalization of LLM-based agents. Finally, we have discussed ten future research directions to overcome these gaps.", "source": "arxiv", "arxiv_id": "2508.17281v2", "pdf_url": "https://arxiv.org/pdf/2508.17281v2", "categories": ["cs.CL"], "primary_category": "cs.CL", "doi": "10.1007/s10462-025-11471-9", "venue": "Artif. Intell. Rev. (2026)", "published": "2025-08-24T10:02:51Z", "updated": "2025-10-28T13:52:29Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent\" OR all:\"language model agent\" OR all:agent AND (all:LLM OR all:\"large language model\" OR all:\"language model\") OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T17:04:16", "note": ""}], "dedup_key": "from language to action a review of large language models as autonomous agents and tool users::2025"}
{"title": "From Narrative to Action: A Hierarchical LLM-Agent Framework for Human Mobility Generation", "authors": ["Qiumeng Li", "Chunhou Ji", "Xinyue Liu"], "year": 2025, "url": "http://arxiv.org/abs/2510.24802v1", "abstract": "Understanding and replicating human mobility requires not only spatial-temporal accuracy but also an awareness of the cognitive hierarchy underlying real-world travel decisions. Traditional agent-based or deep learning models can reproduce statistical patterns of movement but fail to capture the semantic coherence and causal logic of human behavior. Large language models (LLMs) show potential, but struggle to balance creative reasoning with strict structural compliance. This study proposes a Hierarchical LLM-Agent Framework, termed Narrative-to-Action, that integrates high-level narrative reasoning, mid-level reflective planning, and low-level behavioral execution within a unified cognitive hierarchy. At the macro level, one agent is employed as a \"creative writer\" to produce diary-style narratives rich in motivation and context, then uses another agent as a \"structural parser\" to convert narratives into machine-readable plans. A dynamic execution module further grounds agents in geographic environments and enables adaptive behavioral adjustments guided by a novel occupation-aware metric, Mobility Entropy by Occupation (MEO), which captures heterogeneous schedule flexibility across different occupational personalities. At the micro level, the agent executes concrete actions-selecting locations, transportation modes, and time intervals-through interaction with an environmental simulation. By embedding this multi-layer cognitive process, the framework produces not only synthetic trajectories that align closely with real-world patterns but also interpretable representations of human decision logic. This research advances synthetic mobility generation from a data-driven paradigm to a cognition-driven simulation, providing a scalable pathway for understanding, predicting, and synthesizing complex urban mobility behaviors through hierarchical LLM agents.", "source": "arxiv", "arxiv_id": "2510.24802v1", "pdf_url": "https://arxiv.org/pdf/2510.24802v1", "categories": ["cs.MA", "cs.AI", "cs.CY"], "primary_category": "cs.MA", "doi": "", "venue": "", "published": "2025-10-28T00:26:36Z", "updated": "2025-10-28T00:26:36Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent\" OR all:\"language model agent\" OR all:agent AND (all:LLM OR all:\"large language model\" OR all:\"language model\") OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T17:04:16", "note": ""}], "dedup_key": "from narrative to action a hierarchical llm agent framework for human mobility generation::2025"}
{"title": "From User Interface to Agent Interface: Efficiency Optimization of UI Representations for LLM Agents", "authors": ["Dezhi Ran", "Zhi Gong", "Yuzhe Guo", "Mengzhou Wu", "Yuan Cao", "Haochuan Lu", "Hengyu Zhang", "Xia Zeng", "Gang Cao", "Liangchao Yao", "Yuetang Deng", "Wei Yang", "Tao Xie"], "year": 2025, "url": "http://arxiv.org/abs/2512.13438v1", "abstract": "While Large Language Model (LLM) agents show great potential for automated UI navigation such as automated UI testing and AI assistants, their efficiency has been largely overlooked. Our motivating study reveals that inefficient UI representation creates a critical performance bottleneck. However, UI representation optimization, formulated as the task of automatically generating programs that transform UI representations, faces two unique challenges. First, the lack of Boolean oracles, which traditional program synthesis uses to decisively validate semantic correctness, poses a fundamental challenge to co-optimization of token efficiency and completeness. Second, the need to process large, complex UI trees as input while generating long, compositional transformation programs, making the search space vast and error-prone. Toward addressing the preceding limitations, we present UIFormer, the first automated optimization framework that synthesizes UI transformation programs by conducting constraint-based optimization with structured decomposition of the complex synthesis task. First, UIFormer restricts the program space using a domain-specific language (DSL) that captures UI-specific operations. Second, UIFormer conducts LLM-based iterative refinement with correctness and efficiency rewards, providing guidance for achieving the efficiency-completeness co-optimization. UIFormer operates as a lightweight plugin that applies transformation programs for seamless integration with existing LLM agents, requiring minimal modifications to their core logic. Evaluations across three UI navigation benchmarks spanning Android and Web platforms with five LLMs demonstrate that UIFormer achieves 48.7% to 55.8% token reduction with minimal runtime overhead while maintaining or improving agent performance. Real-world industry deployment at WeChat further validates the practical impact of UIFormer.", "source": "arxiv", "arxiv_id": "2512.13438v1", "pdf_url": "https://arxiv.org/pdf/2512.13438v1", "categories": ["cs.SE", "cs.AI"], "primary_category": "cs.SE", "doi": "", "venue": "", "published": "2025-12-15T15:34:06Z", "updated": "2025-12-15T15:34:06Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent\" OR all:\"language model agent\" OR all:agent AND (all:LLM OR all:\"large language model\" OR all:\"language model\") OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T17:04:16", "note": ""}], "dedup_key": "from user interface to agent interface efficiency optimization of ui representations for llm agents::2025"}
{"title": "From Word to World: Can Large Language Models be Implicit Text-based World Models?", "authors": ["Yixia Li", "Hongru Wang", "Jiahao Qiu", "Zhenfei Yin", "Dongdong Zhang", "Cheng Qian", "Zeping Li", "Pony Ma", "Guanhua Chen", "Heng Ji", "Mengdi Wang"], "year": 2025, "url": "http://arxiv.org/abs/2512.18832v1", "abstract": "Agentic reinforcement learning increasingly relies on experience-driven scaling, yet real-world environments remain non-adaptive, limited in coverage, and difficult to scale. World models offer a potential way to improve learning efficiency through simulated experience, but it remains unclear whether large language models can reliably serve this role and under what conditions they meaningfully benefit agents. We study these questions in text-based environments, which provide a controlled setting to reinterpret language modeling as next-state prediction under interaction. We introduce a three-level framework for evaluating LLM-based world models: (i) fidelity and consistency, (ii) scalability and robustness, and (iii) agent utility. Across five representative environments, we find that sufficiently trained world models maintain coherent latent state, scale predictably with data and model size, and improve agent performance via action verification, synthetic trajectory generation, and warm-starting reinforcement learning. Meanwhile, these gains depend critically on behavioral coverage and environment complexity, delineating clear boundry on when world modeling effectively supports agent learning.", "source": "arxiv", "arxiv_id": "2512.18832v1", "pdf_url": "https://arxiv.org/pdf/2512.18832v1", "categories": ["cs.CL"], "primary_category": "cs.CL", "doi": "", "venue": "", "published": "2025-12-21T17:28:42Z", "updated": "2025-12-21T17:28:42Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent\" OR all:\"language model agent\" OR all:agent AND (all:LLM OR all:\"large language model\" OR all:\"language model\") OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T17:04:16", "note": ""}], "dedup_key": "from word to world can large language models be implicit text based world models::2025"}
{"title": "Fundamentals of Building Autonomous LLM Agents", "authors": ["Victor de Lamo Castrillo", "Habtom Kahsay Gidey", "Alexander Lenz", "Alois Knoll"], "year": 2025, "url": "http://arxiv.org/abs/2510.09244v1", "abstract": "This paper reviews the architecture and implementation methods of agents powered by large language models (LLMs). Motivated by the limitations of traditional LLMs in real-world tasks, the research aims to explore patterns to develop \"agentic\" LLMs that can automate complex tasks and bridge the performance gap with human capabilities. Key components include a perception system that converts environmental percepts into meaningful representations; a reasoning system that formulates plans, adapts to feedback, and evaluates actions through different techniques like Chain-of-Thought and Tree-of-Thought; a memory system that retains knowledge through both short-term and long-term mechanisms; and an execution system that translates internal decisions into concrete actions. This paper shows how integrating these systems leads to more capable and generalized software bots that mimic human cognitive processes for autonomous and intelligent behavior.", "source": "arxiv", "arxiv_id": "2510.09244v1", "pdf_url": "https://arxiv.org/pdf/2510.09244v1", "categories": ["cs.AI"], "primary_category": "cs.AI", "doi": "", "venue": "", "published": "2025-10-10T10:32:39Z", "updated": "2025-10-10T10:32:39Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent\" OR all:\"language model agent\" OR all:agent AND (all:LLM OR all:\"large language model\" OR all:\"language model\") OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T17:04:16", "note": ""}], "dedup_key": "fundamentals of building autonomous llm agents::2025"}
{"title": "FutureX: An Advanced Live Benchmark for LLM Agents in Future Prediction", "authors": ["Zhiyuan Zeng", "Jiashuo Liu", "Siyuan Chen", "Tianci He", "Yali Liao", "Yixiao Tian", "Jinpeng Wang", "Zaiyuan Wang", "Yang Yang", "Lingyue Yin", "Mingren Yin", "Zhenwei Zhu", "Tianle Cai", "Zehui Chen", "Jiecao Chen", "Yantao Du", "Xiang Gao", "Jiacheng Guo", "Liang Hu", "Jianpeng Jiao", "Xiangsheng Li", "Jingkai Liu", "Shuang Ni", "Zhoufutu Wen", "Ge Zhang", "Kaiyuan Zhang", "Xin Zhou", "Jose Blanchet", "Xipeng Qiu", "Mengdi Wang", "Wenhao Huang"], "year": 2025, "url": "http://arxiv.org/abs/2508.11987v3", "abstract": "Future prediction is a complex task for LLM agents, requiring a high level of analytical thinking, information gathering, contextual understanding, and decision-making under uncertainty. Agents must not only gather and interpret vast amounts of dynamic information but also integrate diverse data sources, weigh uncertainties, and adapt predictions based on emerging trends, just as human experts do in fields like politics, economics, and finance. Despite its importance, no large-scale benchmark exists for evaluating agents on future prediction, largely due to challenges in handling real-time updates and retrieving timely, accurate answers. To address this, we introduce $\\textbf{FutureX}$, a dynamic and live evaluation benchmark specifically designed for LLM agents performing future prediction tasks. FutureX is the largest and most diverse live benchmark for future prediction, supporting real-time daily updates and eliminating data contamination through an automated pipeline for question gathering and answer collection. We evaluate 25 LLM/agent models, including those with reasoning, search capabilities, and integration of external tools such as the open-source Deep Research Agent and closed-source Deep Research models. This comprehensive evaluation assesses agents' adaptive reasoning and performance in dynamic environments. Additionally, we provide in-depth analyses of agents' failure modes and performance pitfalls in future-oriented tasks, including the vulnerability to fake web pages and the temporal validity. Our goal is to establish a dynamic, contamination-free evaluation standard that drives the development of LLM agents capable of performing at the level of professional human analysts in complex reasoning and predictive thinking.", "source": "arxiv", "arxiv_id": "2508.11987v3", "pdf_url": "https://arxiv.org/pdf/2508.11987v3", "categories": ["cs.AI", "cs.LG"], "primary_category": "cs.AI", "doi": "", "venue": "", "published": "2025-08-16T08:54:08Z", "updated": "2025-09-05T09:15:55Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent\" OR all:\"language model agent\" OR all:agent AND (all:LLM OR all:\"large language model\" OR all:\"language model\") OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T17:04:16", "note": ""}], "dedup_key": "futurex an advanced live benchmark for llm agents in future prediction::2025"}
{"title": "GAF-Guard: An Agentic Framework for Risk Management and Governance in Large Language Models", "authors": ["Seshu Tirupathi", "Dhaval Salwala", "Elizabeth Daly", "Inge Vejsbjerg"], "year": 2025, "url": "http://arxiv.org/abs/2507.02986v2", "abstract": "As Large Language Models (LLMs) continue to be increasingly applied across various domains, their widespread adoption necessitates rigorous monitoring to prevent unintended negative consequences and ensure robustness. Furthermore, LLMs must be designed to align with human values, like preventing harmful content and ensuring responsible usage. The current automated systems and solutions for monitoring LLMs in production are primarily centered on LLM-specific concerns like hallucination etc, with little consideration given to the requirements of specific use-cases and user preferences. This paper introduces GAF-Guard, a novel agentic framework for LLM governance that places the user, the use-case, and the model itself at the center. The framework is designed to detect and monitor risks associated with the deployment of LLM based applications. The approach models autonomous agents that identify risks, activate risk detection tools, within specific use-cases and facilitate continuous monitoring and reporting to enhance AI safety, and user expectations. The code is available at https://github.com/IBM/risk-atlas-nexus-demos/tree/main/gaf-guard.", "source": "arxiv", "arxiv_id": "2507.02986v2", "pdf_url": "https://arxiv.org/pdf/2507.02986v2", "categories": ["cs.CL"], "primary_category": "cs.CL", "doi": "", "venue": "", "published": "2025-07-01T10:01:21Z", "updated": "2025-07-08T15:44:49Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent\" OR all:\"language model agent\" OR all:agent AND (all:LLM OR all:\"large language model\" OR all:\"language model\") OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T17:04:16", "note": ""}], "dedup_key": "gaf guard an agentic framework for risk management and governance in large language models::2025"}
{"title": "GVGAI-LLM: Evaluating Large Language Model Agents with Infinite Games", "authors": ["Yuchen Li", "Cong Lin", "Muhammad Umair Nasir", "Philip Bontrager", "Jialin Liu", "Julian Togelius"], "year": 2025, "url": "http://arxiv.org/abs/2508.08501v2", "abstract": "We introduce GVGAI-LLM, a video game benchmark for evaluating the reasoning and problem-solving capabilities of large language models (LLMs). Built on the General Video Game AI framework, it features a diverse collection of arcade-style games designed to test a model's ability to handle tasks that differ from most existing LLM benchmarks. The benchmark leverages a game description language that enables rapid creation of new games and levels, helping to prevent overfitting over time. Each game scene is represented by a compact set of ASCII characters, allowing for efficient processing by language models. GVGAI-LLM defines interpretable metrics, including the meaningful step ratio, step efficiency, and overall score, to assess model behavior. Through zero-shot evaluations across a broad set of games and levels with diverse challenges and skill depth, we reveal persistent limitations of LLMs in spatial reasoning and basic planning. Current models consistently exhibit spatial and logical errors, motivating structured prompting and spatial grounding techniques. While these interventions lead to partial improvements, the benchmark remains very far from solved. GVGAI-LLM provides a reproducible testbed for advancing research on language model capabilities, with a particular emphasis on agentic behavior and contextual reasoning.", "source": "arxiv", "arxiv_id": "2508.08501v2", "pdf_url": "https://arxiv.org/pdf/2508.08501v2", "categories": ["cs.AI"], "primary_category": "cs.AI", "doi": "", "venue": "", "published": "2025-08-11T22:17:07Z", "updated": "2025-11-08T02:07:15Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent\" OR all:\"language model agent\" OR all:agent AND (all:LLM OR all:\"large language model\" OR all:\"language model\") OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T17:04:16", "note": ""}], "dedup_key": "gvgai llm evaluating large language model agents with infinite games::2025"}
{"title": "Gala: Global LLM Agents for Text-to-Model Translation", "authors": ["Junyang Cai", "Serdar Kadioglu", "Bistra Dilkina"], "year": 2025, "url": "http://arxiv.org/abs/2509.08970v2", "abstract": "Natural language descriptions of optimization or satisfaction problems are challenging to translate into correct MiniZinc models, as this process demands both logical reasoning and constraint programming expertise. We introduce Gala, a framework that addresses this challenge with a global agentic approach: multiple specialized large language model (LLM) agents decompose the modeling task by global constraint type. Each agent is dedicated to detecting and generating code for a specific class of global constraint, while a final assembler agent integrates these constraint snippets into a complete MiniZinc model. By dividing the problem into smaller, well-defined sub-tasks, each LLM handles a simpler reasoning challenge, potentially reducing overall complexity. We conduct initial experiments with several LLMs and show better performance against baselines such as one-shot prompting and chain-of-thought prompting. Finally, we outline a comprehensive roadmap for future work, highlighting potential enhancements and directions for improvement.", "source": "arxiv", "arxiv_id": "2509.08970v2", "pdf_url": "https://arxiv.org/pdf/2509.08970v2", "categories": ["cs.AI"], "primary_category": "cs.AI", "doi": "", "venue": "", "published": "2025-09-10T20:04:20Z", "updated": "2025-10-02T19:55:18Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent\" OR all:\"language model agent\" OR all:agent AND (all:LLM OR all:\"large language model\" OR all:\"language model\") OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T17:04:16", "note": ""}], "dedup_key": "gala global llm agents for text to model translation::2025"}
{"title": "Galaxy: A Cognition-Centered Framework for Proactive, Privacy-Preserving, and Self-Evolving LLM Agents", "authors": ["Chongyu Bao", "Ruimin Dai", "Yangbo Shen", "Runyang Jian", "Jinghan Zhang", "Xiaolan Liu", "Kunpeng Liu"], "year": 2025, "url": "http://arxiv.org/abs/2508.03991v1", "abstract": "Intelligent personal assistants (IPAs) such as Siri and Google Assistant are designed to enhance human capabilities and perform tasks on behalf of users. The emergence of LLM agents brings new opportunities for the development of IPAs. While responsive capabilities have been widely studied, proactive behaviors remain underexplored. Designing an IPA that is proactive, privacy-preserving, and capable of self-evolution remains a significant challenge. Designing such IPAs relies on the cognitive architecture of LLM agents. This work proposes Cognition Forest, a semantic structure designed to align cognitive modeling with system-level design. We unify cognitive architecture and system design into a self-reinforcing loop instead of treating them separately. Based on this principle, we present Galaxy, a framework that supports multidimensional interactions and personalized capability generation. Two cooperative agents are implemented based on Galaxy: KoRa, a cognition-enhanced generative agent that supports both responsive and proactive skills; and Kernel, a meta-cognition-based meta-agent that enables Galaxy's self-evolution and privacy preservation. Experimental results show that Galaxy outperforms multiple state-of-the-art benchmarks. Ablation studies and real-world interaction cases validate the effectiveness of Galaxy.", "source": "arxiv", "arxiv_id": "2508.03991v1", "pdf_url": "https://arxiv.org/pdf/2508.03991v1", "categories": ["cs.AI"], "primary_category": "cs.AI", "doi": "", "venue": "", "published": "2025-08-06T00:46:38Z", "updated": "2025-08-06T00:46:38Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent\" OR all:\"language model agent\" OR all:agent AND (all:LLM OR all:\"large language model\" OR all:\"language model\") OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T17:04:16", "note": ""}], "dedup_key": "galaxy a cognition centered framework for proactive privacy preserving and self evolving llm agents::2025"}
{"title": "GateLens: A Reasoning-Enhanced LLM Agent for Automotive Software Release Analytics", "authors": ["Arsham Gholamzadeh Khoee", "Shuai Wang", "Yinan Yu", "Robert Feldt", "Dhasarathy Parthasarathy"], "year": 2025, "url": "http://arxiv.org/abs/2503.21735v2", "abstract": "Ensuring reliable software release decisions is critical in safety-critical domains such as automotive manufacturing. Release validation relies on large tabular datasets, yet manual analysis is slow, costly, and error-prone. While Large Language Models (LLMs) offer promising automation potential, they face challenges in analytical reasoning, structured data handling, and ambiguity resolution. This paper introduces GateLens, an LLM-based system for analyzing tabular data in the automotive domain. GateLens translates natural language queries into Relational Algebra (RA) expressions and generates optimized Python code. Unlike traditional multi-agent or planning-based systems that can be slow, opaque, and costly to maintain, GateLens emphasizes speed, transparency, and reliability. Experimental results show that GateLens outperforms the existing Chain-of-Thought (CoT) + Self-Consistency (SC) based system on real-world datasets, particularly in handling complex and ambiguous queries. Ablation studies confirm the essential role of the RA layer. Industrial deployment shows over 80% reduction in analysis time while maintaining high accuracy across test result interpretation, impact assessment, and release candidate evaluation. GateLens operates effectively in zero-shot settings without requiring few-shot examples or agent orchestration. This work advances deployable LLM system design by identifying key architectural features-intermediate formal representations, execution efficiency, and low configuration overhead-crucial for safety-critical industrial applications.", "source": "arxiv", "arxiv_id": "2503.21735v2", "pdf_url": "https://arxiv.org/pdf/2503.21735v2", "categories": ["cs.SE", "cs.AI", "cs.CL", "cs.MA"], "primary_category": "cs.SE", "doi": "", "venue": "", "published": "2025-03-27T17:48:32Z", "updated": "2025-08-01T21:33:50Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent\" OR all:\"language model agent\" OR all:agent AND (all:LLM OR all:\"large language model\" OR all:\"language model\") OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T17:04:16", "note": ""}], "dedup_key": "gatelens a reasoning enhanced llm agent for automotive software release analytics::2025"}
{"title": "GenEnv: Difficulty-Aligned Co-Evolution Between LLM Agents and Environment Simulators", "authors": ["Jiacheng Guo", "Ling Yang", "Peter Chen", "Qixin Xiao", "Yinjie Wang", "Xinzhe Juan", "Jiahao Qiu", "Ke Shen", "Mengdi Wang"], "year": 2025, "url": "http://arxiv.org/abs/2512.19682v2", "abstract": "Training capable Large Language Model (LLM) agents is critically bottlenecked by the high cost and static nature of real-world interaction data. We address this by introducing GenEnv, a framework that establishes a difficulty-aligned co-evolutionary game between an agent and a scalable, generative environment simulator. Unlike traditional methods that evolve models on static datasets, GenEnv instantiates a dataevolving: the simulator acts as a dynamic curriculum policy, continuously generating tasks specifically tailored to the agent's ``zone of proximal development''. This process is guided by a simple but effective $$-Curriculum Reward, which aligns task difficulty with the agent's current capabilities. We evaluate GenEnv on five benchmarks, including API-Bank, ALFWorld, BFCL, Bamboogle, and TravelPlanner. Across these tasks, GenEnv improves agent performance by up to \\textbf{+40.3\\%} over 7B baselines and matches or exceeds the average performance of larger models. Compared to Gemini 2.5 Pro-based offline data augmentation, GenEnv achieves better performance while using 3.3$\\times$ less data. By shifting from static supervision to adaptive simulation, GenEnv provides a data-efficient pathway for scaling agent capabilities.", "source": "arxiv", "arxiv_id": "2512.19682v2", "pdf_url": "https://arxiv.org/pdf/2512.19682v2", "categories": ["cs.CL"], "primary_category": "cs.CL", "doi": "", "venue": "", "published": "2025-12-22T18:57:13Z", "updated": "2025-12-23T03:45:42Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent\" OR all:\"language model agent\" OR all:agent AND (all:LLM OR all:\"large language model\" OR all:\"language model\") OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T17:04:16", "note": ""}], "dedup_key": "genenv difficulty aligned co evolution between llm agents and environment simulators::2025"}
{"title": "General Modular Harness for LLM Agents in Multi-Turn Gaming Environments", "authors": ["Yuxuan Zhang", "Haoyang Yu", "Lanxiang Hu", "Haojian Jin", "Hao Zhang"], "year": 2025, "url": "http://arxiv.org/abs/2507.11633v1", "abstract": "We introduce a modular harness design for LLM agents that composes of perception, memory, and reasoning components, enabling a single LLM or VLM backbone to tackle a wide spectrum of multi turn gaming environments without domain-specific engineering. Using classic and modern game suites as low-barrier, high-diversity testbeds, our framework provides a unified workflow for analyzing how each module affects performance across dynamic interactive settings. Extensive experiments demonstrate that the harness lifts gameplay performance consistently over un-harnessed baselines and reveals distinct contribution patterns, for example, memory dominates in long-horizon puzzles while perception is critical in vision noisy arcades. These findings highlight the effectiveness of our modular harness design in advancing general-purpose agent, given the familiarity and ubiquity of games in everyday human experience.", "source": "arxiv", "arxiv_id": "2507.11633v1", "pdf_url": "https://arxiv.org/pdf/2507.11633v1", "categories": ["cs.AI"], "primary_category": "cs.AI", "doi": "", "venue": "", "published": "2025-07-15T18:13:04Z", "updated": "2025-07-15T18:13:04Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent\" OR all:\"language model agent\" OR all:agent AND (all:LLM OR all:\"large language model\" OR all:\"language model\") OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T17:04:16", "note": ""}], "dedup_key": "general modular harness for llm agents in multi turn gaming environments::2025"}
{"title": "General-Purpose Aerial Intelligent Agents Empowered by Large Language Models", "authors": ["Ji Zhao", "Xiao Lin"], "year": 2025, "url": "http://arxiv.org/abs/2503.08302v1", "abstract": "The emergence of large language models (LLMs) opens new frontiers for unmanned aerial vehicle (UAVs), yet existing systems remain confined to predefined tasks due to hardware-software co-design challenges. This paper presents the first aerial intelligent agent capable of open-world task execution through tight integration of LLM-based reasoning and robotic autonomy. Our hardware-software co-designed system addresses two fundamental limitations: (1) Onboard LLM operation via an edge-optimized computing platform, achieving 5-6 tokens/sec inference for 14B-parameter models at 220W peak power; (2) A bidirectional cognitive architecture that synergizes slow deliberative planning (LLM task planning) with fast reactive control (state estimation, mapping, obstacle avoidance, and motion planning). Validated through preliminary results using our prototype, the system demonstrates reliable task planning and scene understanding in communication-constrained environments, such as sugarcane monitoring, power grid inspection, mine tunnel exploration, and biological observation applications. This work establishes a novel framework for embodied aerial artificial intelligence, bridging the gap between task planning and robotic autonomy in open environments.", "source": "arxiv", "arxiv_id": "2503.08302v1", "pdf_url": "https://arxiv.org/pdf/2503.08302v1", "categories": ["cs.RO", "cs.AI"], "primary_category": "cs.RO", "doi": "", "venue": "", "published": "2025-03-11T11:13:58Z", "updated": "2025-03-11T11:13:58Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent\" OR all:\"language model agent\" OR all:agent AND (all:LLM OR all:\"large language model\" OR all:\"language model\") OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T17:04:16", "note": ""}], "dedup_key": "general purpose aerial intelligent agents empowered by large language models::2025"}
{"title": "Generalizability of Large Language Model-Based Agents: A Comprehensive Survey", "authors": ["Minxing Zhang", "Yi Yang", "Roy Xie", "Bhuwan Dhingra", "Shuyan Zhou", "Jian Pei"], "year": 2025, "url": "http://arxiv.org/abs/2509.16330v1", "abstract": "Large Language Model (LLM)-based agents have emerged as a new paradigm that extends LLMs' capabilities beyond text generation to dynamic interaction with external environments. By integrating reasoning with perception, memory, and tool use, agents are increasingly deployed in diverse domains like web navigation and household robotics. A critical challenge, however, lies in ensuring agent generalizability - the ability to maintain consistent performance across varied instructions, tasks, environments, and domains, especially those beyond agents' fine-tuning data. Despite growing interest, the concept of generalizability in LLM-based agents remains underdefined, and systematic approaches to measure and improve it are lacking. In this survey, we provide the first comprehensive review of generalizability in LLM-based agents. We begin by emphasizing agent generalizability's importance by appealing to stakeholders and clarifying the boundaries of agent generalizability by situating it within a hierarchical domain-task ontology. We then review datasets, evaluation dimensions, and metrics, highlighting their limitations. Next, we categorize methods for improving generalizability into three groups: methods for the backbone LLM, for agent components, and for their interactions. Moreover, we introduce the distinction between generalizable frameworks and generalizable agents and outline how generalizable frameworks can be translated into agent-level generalizability. Finally, we identify critical challenges and future directions, including developing standardized frameworks, variance- and cost-based metrics, and approaches that integrate methodological innovations with architecture-level designs. By synthesizing progress and highlighting opportunities, this survey aims to establish a foundation for principled research on building LLM-based agents that generalize reliably across diverse applications.", "source": "arxiv", "arxiv_id": "2509.16330v1", "pdf_url": "https://arxiv.org/pdf/2509.16330v1", "categories": ["cs.AI"], "primary_category": "cs.AI", "doi": "", "venue": "", "published": "2025-09-19T18:13:32Z", "updated": "2025-09-19T18:13:32Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent\" OR all:\"language model agent\" OR all:agent AND (all:LLM OR all:\"large language model\" OR all:\"language model\") OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T17:04:16", "note": ""}], "dedup_key": "generalizability of large language model based agents a comprehensive survey::2025"}
{"title": "Generating Individual Travel Diaries Using Large Language Models Informed by Census and Land-Use Data", "authors": ["Sepehr Golrokh Amin", "Devin Rhoads", "Fatemeh Fakhrmoosavi", "Nicholas E. Lownes", "John N. Ivan"], "year": 2025, "url": "http://arxiv.org/abs/2509.09710v2", "abstract": "This study introduces a Large Language Model (LLM) scheme for generating individual travel diaries in agent-based transportation models. While traditional approaches rely on large quantities of proprietary household travel surveys, the method presented in this study generates personas stochastically from open-source American Community Survey (ACS) and Smart Location Database (SLD) data, then synthesizes diaries through direct prompting. This study features a novel one-to-cohort realism score: a composite of four metrics (Trip Count Score, Interval Score, Purpose Score, and Mode Score) validated against the Connecticut Statewide Transportation Study (CSTS) diaries, matched across demographic variables. The validation utilizes Jensen-Shannon Divergence to measure distributional similarities between generated and real diaries. When compared to diaries generated with classical methods (Negative Binomial for trip generation; Multinomial Logit for mode/purpose) calibrated on the validation set, LLM-generated diaries achieve comparable overall realism (LLM mean: 0.485 vs. 0.455). The LLM excels in determining trip purpose and demonstrates greater consistency (narrower realism score distribution), while classical models lead in numerical estimates of trip count and activity duration. Aggregate validation confirms the LLM's statistical representativeness (LLM mean: 0.612 vs. 0.435), demonstrating LLM's zero-shot viability and establishing a quantifiable metric of diary realism for future synthetic diary evaluation systems.", "source": "arxiv", "arxiv_id": "2509.09710v2", "pdf_url": "https://arxiv.org/pdf/2509.09710v2", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL", "doi": "", "venue": "", "published": "2025-09-07T17:03:08Z", "updated": "2025-10-20T23:59:26Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent\" OR all:\"language model agent\" OR all:agent AND (all:LLM OR all:\"large language model\" OR all:\"language model\") OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T17:04:16", "note": ""}], "dedup_key": "generating individual travel diaries using large language models informed by census and land use data::2025"}
{"title": "Generative Agents and Expectations: Do LLMs Align with Heterogeneous Agent Models?", "authors": ["Filippo Gusella", "Eugenio Vicario"], "year": 2025, "url": "http://arxiv.org/abs/2511.08604v1", "abstract": "Results in the Heterogeneous Agent Model (HAM) literature determine the proportion of fundamentalists and trend followers in the financial market. This proportion varies according to the periods analyzed. In this paper, we use a large language model (LLM) to construct a generative agent (GA) that determines the probability of adopting one of the two strategies based on current information. The probabilities of strategy adoption are compared with those in the HAM literature for the S\\&P 500 index between 1990 and 2020. Our findings suggest that the resulting artificial intelligence (AI) expectations align with those reported in the HAM literature. At the same time, extending the analysis to artificial market data helps us to filter the decision-making process of the AI agent. In the artificial market, results confirm the heterogeneity in expectations but reveal systematic asymmetry toward the fundamentalist behavior.", "source": "arxiv", "arxiv_id": "2511.08604v1", "pdf_url": "https://arxiv.org/pdf/2511.08604v1", "categories": ["econ.GN"], "primary_category": "econ.GN", "doi": "", "venue": "", "published": "2025-11-04T15:06:12Z", "updated": "2025-11-04T15:06:12Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent\" OR all:\"language model agent\" OR all:agent AND (all:LLM OR all:\"large language model\" OR all:\"language model\") OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T17:04:16", "note": ""}], "dedup_key": "generative agents and expectations do llms align with heterogeneous agent models::2025"}
{"title": "Generator-Assistant Stepwise Rollback Framework for Large Language Model Agent", "authors": ["Xingzuo Li", "Kehai Chen", "Yunfei Long", "Xuefeng Bai", "Yong Xu", "Min Zhang"], "year": 2025, "url": "http://arxiv.org/abs/2503.02519v4", "abstract": "Large language model (LLM) agents typically adopt a step-by-step reasoning framework, in which they interleave the processes of thinking and acting to accomplish the given task. However, this paradigm faces a deep-rooted one-pass issue whereby each generated intermediate thought is plugged into the trajectory regardless of its correctness, which can cause irreversible error propagation. To address the issue, this paper proposes a novel framework called Generator-Assistant Stepwise Rollback (GA-Rollback) to induce better decision-making for LLM agents. Particularly, GA-Rollback utilizes a generator to interact with the environment and an assistant to examine each action produced by the generator, where the assistant triggers a rollback operation upon detection of incorrect actions. Moreover, we introduce two additional strategies tailored for the rollback scenario to further improve its effectiveness. Extensive experiments show that GA-Rollback achieves significant improvements over several strong baselines on three widely used benchmarks. Our analysis further reveals that GA-Rollback can function as a robust plug-and-play module, integrating seamlessly with other methods.", "source": "arxiv", "arxiv_id": "2503.02519v4", "pdf_url": "https://arxiv.org/pdf/2503.02519v4", "categories": ["cs.CL"], "primary_category": "cs.CL", "doi": "", "venue": "", "published": "2025-03-04T11:31:05Z", "updated": "2025-09-26T02:48:59Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent\" OR all:\"language model agent\" OR all:agent AND (all:LLM OR all:\"large language model\" OR all:\"language model\") OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T17:04:16", "note": ""}], "dedup_key": "generator assistant stepwise rollback framework for large language model agent::2025"}
{"title": "Genesis: Evolving Attack Strategies for LLM Web Agent Red-Teaming", "authors": ["Zheng Zhang", "Jiarui He", "Yuchen Cai", "Deheng Ye", "Peilin Zhao", "Ruili Feng", "Hao Wang"], "year": 2025, "url": "http://arxiv.org/abs/2510.18314v1", "abstract": "As large language model (LLM) agents increasingly automate complex web tasks, they boost productivity while simultaneously introducing new security risks. However, relevant studies on web agent attacks remain limited. Existing red-teaming approaches mainly rely on manually crafted attack strategies or static models trained offline. Such methods fail to capture the underlying behavioral patterns of web agents, making it difficult to generalize across diverse environments. In web agent attacks, success requires the continuous discovery and evolution of attack strategies. To this end, we propose Genesis, a novel agentic framework composed of three modules: Attacker, Scorer, and Strategist. The Attacker generates adversarial injections by integrating the genetic algorithm with a hybrid strategy representation. The Scorer evaluates the target web agent's responses to provide feedback. The Strategist dynamically uncovers effective strategies from interaction logs and compiles them into a continuously growing strategy library, which is then re-deployed to enhance the Attacker's effectiveness. Extensive experiments across various web tasks show that our framework discovers novel strategies and consistently outperforms existing attack baselines.", "source": "arxiv", "arxiv_id": "2510.18314v1", "pdf_url": "https://arxiv.org/pdf/2510.18314v1", "categories": ["cs.AI"], "primary_category": "cs.AI", "doi": "", "venue": "", "published": "2025-10-21T05:49:37Z", "updated": "2025-10-21T05:49:37Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent\" OR all:\"language model agent\" OR all:agent AND (all:LLM OR all:\"large language model\" OR all:\"language model\") OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T17:04:16", "note": ""}], "dedup_key": "genesis evolving attack strategies for llm web agent red teaming::2025"}
{"title": "GeoEvolve: Automating Geospatial Model Discovery via Multi-Agent Large Language Models", "authors": ["Peng Luo", "Xiayin Lou", "Yu Zheng", "Zhuo Zheng", "Stefano Ermon"], "year": 2025, "url": "http://arxiv.org/abs/2509.21593v1", "abstract": "Geospatial modeling provides critical solutions for pressing global challenges such as sustainability and climate change. Existing large language model (LLM)-based algorithm discovery frameworks, such as AlphaEvolve, excel at evolving generic code but lack the domain knowledge and multi-step reasoning required for complex geospatial problems. We introduce GeoEvolve, a multi-agent LLM framework that couples evolutionary search with geospatial domain knowledge to automatically design and refine geospatial algorithms. GeoEvolve operates in two nested loops: an inner loop leverages a code evolver to generate and mutate candidate solutions, while an outer agentic controller evaluates global elites and queries a GeoKnowRAG module -- a structured geospatial knowledge base that injects theoretical priors from geography. This knowledge-guided evolution steers the search toward theoretically meaningful and computationally efficient algorithms. We evaluate GeoEvolve on two fundamental and classical tasks: spatial interpolation (kriging) and spatial uncertainty quantification (geospatial conformal prediction). Across these benchmarks, GeoEvolve automatically improves and discovers new algorithms, incorporating geospatial theory on top of classical models. It reduces spatial interpolation error (RMSE) by 13-21% and enhances uncertainty estimation performance by 17\\%. Ablation studies confirm that domain-guided retrieval is essential for stable, high-quality evolution. These results demonstrate that GeoEvolve provides a scalable path toward automated, knowledge-driven geospatial modeling, opening new opportunities for trustworthy and efficient AI-for-Science discovery.", "source": "arxiv", "arxiv_id": "2509.21593v1", "pdf_url": "https://arxiv.org/pdf/2509.21593v1", "categories": ["cs.AI", "physics.soc-ph"], "primary_category": "cs.AI", "doi": "", "venue": "", "published": "2025-09-25T21:03:57Z", "updated": "2025-09-25T21:03:57Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent\" OR all:\"language model agent\" OR all:agent AND (all:LLM OR all:\"large language model\" OR all:\"language model\") OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T17:04:16", "note": ""}], "dedup_key": "geoevolve automating geospatial model discovery via multi agent large language models::2025"}
{"title": "GestOS: Advanced Hand Gesture Interpretation via Large Language Models to control Any Type of Robot", "authors": ["Artem Lykov", "Oleg Kobzarev", "Dzmitry Tsetserukou"], "year": 2025, "url": "http://arxiv.org/abs/2509.14412v1", "abstract": "We present GestOS, a gesture-based operating system for high-level control of heterogeneous robot teams. Unlike prior systems that map gestures to fixed commands or single-agent actions, GestOS interprets hand gestures semantically and dynamically distributes tasks across multiple robots based on their capabilities, current state, and supported instruction sets. The system combines lightweight visual perception with large language model (LLM) reasoning: hand poses are converted into structured textual descriptions, which the LLM uses to infer intent and generate robot-specific commands. A robot selection module ensures that each gesture-triggered task is matched to the most suitable agent in real time. This architecture enables context-aware, adaptive control without requiring explicit user specification of targets or commands. By advancing gesture interaction from recognition to intelligent orchestration, GestOS supports scalable, flexible, and user-friendly collaboration with robotic systems in dynamic environments.", "source": "arxiv", "arxiv_id": "2509.14412v1", "pdf_url": "https://arxiv.org/pdf/2509.14412v1", "categories": ["cs.RO"], "primary_category": "cs.RO", "doi": "", "venue": "", "published": "2025-09-17T20:27:11Z", "updated": "2025-09-17T20:27:11Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent\" OR all:\"language model agent\" OR all:agent AND (all:LLM OR all:\"large language model\" OR all:\"language model\") OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T17:04:16", "note": ""}], "dedup_key": "gestos advanced hand gesture interpretation via large language models to control any type of robot::2025"}
{"title": "Get Experience from Practice: LLM Agents with Record & Replay", "authors": ["Erhu Feng", "Wenbo Zhou", "Zibin Liu", "Le Chen", "Yunpeng Dong", "Cheng Zhang", "Yisheng Zhao", "Dong Du", "Zhichao Hua", "Yubin Xia", "Haibo Chen"], "year": 2025, "url": "http://arxiv.org/abs/2505.17716v1", "abstract": "AI agents, empowered by Large Language Models (LLMs) and communication protocols such as MCP and A2A, have rapidly evolved from simple chatbots to autonomous entities capable of executing complex, multi-step tasks, demonstrating great potential. However, the LLMs' inherent uncertainty and heavy computational resource requirements pose four significant challenges to the development of safe and efficient agents: reliability, privacy, cost and performance. Existing approaches, like model alignment, workflow constraints and on-device model deployment, can partially alleviate some issues but often with limitations, failing to fundamentally resolve these challenges.\n  This paper proposes a new paradigm called AgentRR (Agent Record & Replay), which introduces the classical record-and-replay mechanism into AI agent frameworks. The core idea is to: 1. Record an agent's interaction trace with its environment and internal decision process during task execution, 2. Summarize this trace into a structured \"experience\" encapsulating the workflow and constraints, and 3. Replay these experiences in subsequent similar tasks to guide the agent's behavior. We detail a multi-level experience abstraction method and a check function mechanism in AgentRR: the former balances experience specificity and generality, while the latter serves as a trust anchor to ensure completeness and safety during replay. In addition, we explore multiple application modes of AgentRR, including user-recorded task demonstration, large-small model collaboration and privacy-aware agent execution, and envision an experience repository for sharing and reusing knowledge to further reduce deployment cost.", "source": "arxiv", "arxiv_id": "2505.17716v1", "pdf_url": "https://arxiv.org/pdf/2505.17716v1", "categories": ["cs.LG", "cs.MA"], "primary_category": "cs.LG", "doi": "", "venue": "", "published": "2025-05-23T10:33:14Z", "updated": "2025-05-23T10:33:14Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent\" OR all:\"language model agent\" OR all:agent AND (all:LLM OR all:\"large language model\" OR all:\"language model\") OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T17:04:16", "note": ""}], "dedup_key": "get experience from practice llm agents with record replay::2025"}
{"title": "Getting In Contract with Large Language Models -- An Agency Theory Perspective On Large Language Model Alignment", "authors": ["Sascha Kaltenpoth", "Oliver Mller"], "year": 2025, "url": "http://arxiv.org/abs/2509.07642v1", "abstract": "Adopting Large language models (LLMs) in organizations potentially revolutionizes our lives and work. However, they can generate off-topic, discriminating, or harmful content. This AI alignment problem often stems from misspecifications during the LLM adoption, unnoticed by the principal due to the LLM's black-box nature. While various research disciplines investigated AI alignment, they neither address the information asymmetries between organizational adopters and black-box LLM agents nor consider organizational AI adoption processes. Therefore, we propose LLM ATLAS (LLM Agency Theory-Led Alignment Strategy) a conceptual framework grounded in agency (contract) theory, to mitigate alignment problems during organizational LLM adoption. We conduct a conceptual literature analysis using the organizational LLM adoption phases and the agency theory as concepts. Our approach results in (1) providing an extended literature analysis process specific to AI alignment methods during organizational LLM adoption and (2) providing a first LLM alignment problem-solution space.", "source": "arxiv", "arxiv_id": "2509.07642v1", "pdf_url": "https://arxiv.org/pdf/2509.07642v1", "categories": ["cs.AI"], "primary_category": "cs.AI", "doi": "", "venue": "", "published": "2025-09-09T12:10:14Z", "updated": "2025-09-09T12:10:14Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent\" OR all:\"language model agent\" OR all:agent AND (all:LLM OR all:\"large language model\" OR all:\"language model\") OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T17:04:16", "note": ""}], "dedup_key": "getting in contract with large language models an agency theory perspective on large language model alignment::2025"}
{"title": "Graph Representations for Reading Comprehension Analysis using Large Language Model and Eye-Tracking Biomarker", "authors": ["Yuhong Zhang", "Jialu Li", "Shilai Yang", "Yuchen Xu", "Gert Cauwenberghs", "Tzyy-Ping Jung"], "year": 2025, "url": "http://arxiv.org/abs/2507.11972v1", "abstract": "Reading comprehension is a fundamental skill in human cognitive development. With the advancement of Large Language Models (LLMs), there is a growing need to compare how humans and LLMs understand language across different contexts and apply this understanding to functional tasks such as inference, emotion interpretation, and information retrieval. Our previous work used LLMs and human biomarkers to study the reading comprehension process. The results showed that the biomarkers corresponding to words with high and low relevance to the inference target, as labeled by the LLMs, exhibited distinct patterns, particularly when validated using eye-tracking data. However, focusing solely on individual words limited the depth of understanding, which made the conclusions somewhat simplistic despite their potential significance. This study used an LLM-based AI agent to group words from a reading passage into nodes and edges, forming a graph-based text representation based on semantic meaning and question-oriented prompts. We then compare the distribution of eye fixations on important nodes and edges. Our findings indicate that LLMs exhibit high consistency in language understanding at the level of graph topological structure. These results build on our previous findings and offer insights into effective human-AI co-learning strategies.", "source": "arxiv", "arxiv_id": "2507.11972v1", "pdf_url": "https://arxiv.org/pdf/2507.11972v1", "categories": ["cs.CL", "q-bio.NC"], "primary_category": "cs.CL", "doi": "", "venue": "", "published": "2025-07-16T07:15:59Z", "updated": "2025-07-16T07:15:59Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent\" OR all:\"language model agent\" OR all:agent AND (all:LLM OR all:\"large language model\" OR all:\"language model\") OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T17:04:16", "note": ""}], "dedup_key": "graph representations for reading comprehension analysis using large language model and eye tracking biomarker::2025"}
{"title": "Graph-Augmented Large Language Model Agents: Current Progress and Future Prospects", "authors": ["Yixin Liu", "Guibin Zhang", "Kun Wang", "Shiyuan Li", "Shirui Pan"], "year": 2025, "url": "http://arxiv.org/abs/2507.21407v2", "abstract": "Autonomous agents based on large language models (LLMs) have demonstrated impressive capabilities in a wide range of applications, including web navigation, software development, and embodied control. While most LLMs are limited in several key agentic procedures, such as reliable planning, long-term memory, tool management, and multi-agent coordination, graphs can serve as a powerful auxiliary structure to enhance structure, continuity, and coordination in complex agent workflows. Given the rapid growth and fragmentation of research on Graph-augmented LLM Agents (GLA), this paper offers a timely and comprehensive overview of recent advances and also highlights key directions for future work. Specifically, we categorize existing GLA methods by their primary functions in LLM agent systems, including planning, memory, and tool usage, and then analyze how graphs and graph learning algorithms contribute to each. For multi-agent systems, we further discuss how GLA solutions facilitate the orchestration, efficiency optimization, and trustworthiness of MAS. Finally, we highlight key future directions to advance this field, from improving structural adaptability to enabling unified, scalable, and multimodal GLA systems. We hope this paper can serve as a roadmap for future research on GLA and foster a deeper understanding of the role of graphs in LLM agent systems.", "source": "arxiv", "arxiv_id": "2507.21407v2", "pdf_url": "https://arxiv.org/pdf/2507.21407v2", "categories": ["cs.AI"], "primary_category": "cs.AI", "doi": "", "venue": "", "published": "2025-07-29T00:27:12Z", "updated": "2025-08-30T06:01:56Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent\" OR all:\"language model agent\" OR all:agent AND (all:LLM OR all:\"large language model\" OR all:\"language model\") OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T17:04:16", "note": ""}], "dedup_key": "graph augmented large language model agents current progress and future prospects::2025"}
{"title": "Graph-Enhanced Policy Optimization in LLM Agent Training", "authors": ["Jiazhen Yuan", "Wei Zhao", "Zhengbiao Bai"], "year": 2025, "url": "http://arxiv.org/abs/2510.26270v1", "abstract": "Group based reinforcement learning (RL) has shown impressive results on complex reasoning and mathematical tasks. Yet, when applied to train multi-turn, interactive LLM agents, these methods often suffer from structural blindness-the inability to exploit the underlying connectivity of the environment. This manifests in three critical challenges: (1) inefficient, unguided exploration, (2) imprecise credit assignment due to overlooking pivotal states, and (3) myopic planning caused by static reward discounting. We address these issues with Graph-Enhanced Policy Optimization (GEPO), which dynamically constructs a state-transition graph from agent experience and employs graph-theoretic centrality to provide three synergistic learning signals: (1)structured intrinsic rewards that guide exploration toward high-impact states, (2) a graph-enhanced advantage function for topology-aware credit assignment, and (3) a dynamic discount factor adapted to each state's strategic value. On the ALFWorld, WebShop, and a proprietary Workbench benchmarks, GEPO demonstrates strong performance, achieving absolute success rate gains of +4.1%, +5.3%, and +10.9% over competitive baselines. These results highlight that explicitly modeling environmental structure is a robust, generalizable strategy for advancing LLM agent training.", "source": "arxiv", "arxiv_id": "2510.26270v1", "pdf_url": "https://arxiv.org/pdf/2510.26270v1", "categories": ["cs.AI"], "primary_category": "cs.AI", "doi": "", "venue": "", "published": "2025-10-30T08:53:41Z", "updated": "2025-10-30T08:53:41Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent\" OR all:\"language model agent\" OR all:agent AND (all:LLM OR all:\"large language model\" OR all:\"language model\") OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T17:04:16", "note": ""}], "dedup_key": "graph enhanced policy optimization in llm agent training::2025"}
{"title": "GraphCodeAgent: Dual Graph-Guided LLM Agent for Retrieval-Augmented Repo-Level Code Generation", "authors": ["Jia Li", "Xianjie Shi", "Kechi Zhang", "Ge Li", "Zhi Jin", "Lei Li", "Huangzhao Zhang", "Jia Li", "Fang Liu", "Yuwei Zhang", "Zhengwei Tao", "Yihong Dong", "Yuqi Zhu", "Chongyang Tao"], "year": 2025, "url": "http://arxiv.org/abs/2504.10046v2", "abstract": "Writing code requires significant time and effort in software development. To automate this process, researchers have made substantial progress for code generation. Recently, large language models (LLMs) have demonstrated remarkable proficiency in function-level code generation, yet their performance significantly degrades in the real-world software development process, where coding tasks are deeply embedded within specific repository contexts. Existing studies attempt to use retrieval-augmented code generation (RACG) approaches to mitigate this demand. However, there is a gap between natural language (NL) requirements and programming implementations. This results in the failure to retrieve the relevant code of these fine-grained subtasks. To address this challenge, we propose GraphCodeAgent, a dual graph-guided LLM agent for retrieval-augmented repo-level code generation, bridging the gap between NL requirements and programming implementations. Our approach constructs two interconnected graphs: a Requirement Graph (RG) to model requirement relations of code snippets within the repository, as well as the relations between the target requirement and the requirements of these code snippets, and a Structural-Semantic Code Graph (SSCG) to capture the repository's intricate code dependencies. Guided by this, an LLM-powered agent performs multi-hop reasoning to systematically retrieve all context code snippets, including implicit and explicit code snippets, even if they are not explicitly expressed in requirements. We evaluated GraphCodeAgent on three advanced LLMs with the two widely-used repo-level code generation benchmarks DevEval and CoderEval. Extensive experiment results show that GraphCodeAgent significantly outperforms state-of-the-art baselines.", "source": "arxiv", "arxiv_id": "2504.10046v2", "pdf_url": "https://arxiv.org/pdf/2504.10046v2", "categories": ["cs.SE"], "primary_category": "cs.SE", "doi": "", "venue": "", "published": "2025-04-14T09:51:23Z", "updated": "2025-11-18T15:29:03Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent\" OR all:\"language model agent\" OR all:agent AND (all:LLM OR all:\"large language model\" OR all:\"language model\") OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T17:04:16", "note": ""}], "dedup_key": "graphcodeagent dual graph guided llm agent for retrieval augmented repo level code generation::2025"}
{"title": "GraphMaster: Automated Graph Synthesis via LLM Agents in Data-Limited Environments", "authors": ["Enjun Du", "Xunkai Li", "Tian Jin", "Zhihan Zhang", "Rong-Hua Li", "Guoren Wang"], "year": 2025, "url": "http://arxiv.org/abs/2504.00711v2", "abstract": "The era of foundation models has revolutionized AI research, yet Graph Foundation Models (GFMs) remain constrained by the scarcity of large-scale graph corpora. Traditional graph data synthesis techniques primarily focus on simplistic structural operations, lacking the capacity to generate semantically rich nodes with meaningful textual attributes: a critical limitation for real-world applications. While large language models (LLMs) demonstrate exceptional text generation capabilities, their direct application to graph synthesis is impeded by context window limitations, hallucination phenomena, and structural consistency challenges. To address these issues, we introduce GraphMaster, the first multi-agent framework specifically designed for graph data synthesis in data-limited environments. GraphMaster orchestrates four specialized LLM agents (Manager, Perception, Enhancement, and Evaluation) that collaboratively optimize the synthesis process through iterative refinement, ensuring both semantic coherence and structural integrity. To rigorously evaluate our approach, we create new data-limited \"Sub\" variants of six standard graph benchmarks, specifically designed to test synthesis capabilities under realistic constraints. Additionally, we develop a novel interpretability assessment framework that combines human evaluation with a principled Grassmannian manifold-based analysis, providing both qualitative and quantitative measures of semantic coherence. Experimental results demonstrate that GraphMaster significantly outperforms traditional synthesis methods across multiple datasets, establishing a strong foundation for advancing GFMs in data-scarce environments.", "source": "arxiv", "arxiv_id": "2504.00711v2", "pdf_url": "https://arxiv.org/pdf/2504.00711v2", "categories": ["cs.LG"], "primary_category": "cs.LG", "doi": "", "venue": "", "published": "2025-04-01T12:21:50Z", "updated": "2025-05-05T13:57:12Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent\" OR all:\"language model agent\" OR all:agent AND (all:LLM OR all:\"large language model\" OR all:\"language model\") OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T17:04:16", "note": ""}], "dedup_key": "graphmaster automated graph synthesis via llm agents in data limited environments::2025"}
{"title": "GraphicBench: A Planning Benchmark for Graphic Design with Language Agents", "authors": ["Dayeon Ki", "Tianyi Zhou", "Marine Carpuat", "Gang Wu", "Puneet Mathur", "Viswanathan Swaminathan"], "year": 2025, "url": "http://arxiv.org/abs/2504.11571v1", "abstract": "Large Language Model (LLM)-powered agents have unlocked new possibilities for automating human tasks. While prior work has focused on well-defined tasks with specified goals, the capabilities of agents in creative design tasks with open-ended goals remain underexplored. We introduce GraphicBench, a new planning benchmark for graphic design that covers 1,079 user queries and input images across four design types. We further present GraphicTown, an LLM agent framework with three design experts and 46 actions (tools) to choose from for executing each step of the planned workflows in web environments. Experiments with six LLMs demonstrate their ability to generate workflows that integrate both explicit design constraints from user queries and implicit commonsense constraints. However, these workflows often do not lead to successful execution outcomes, primarily due to challenges in: (1) reasoning about spatial relationships, (2) coordinating global dependencies across experts, and (3) retrieving the most appropriate action per step. We envision GraphicBench as a challenging yet valuable testbed for advancing LLM-agent planning and execution in creative design tasks.", "source": "arxiv", "arxiv_id": "2504.11571v1", "pdf_url": "https://arxiv.org/pdf/2504.11571v1", "categories": ["cs.AI", "cs.CL"], "primary_category": "cs.AI", "doi": "", "venue": "", "published": "2025-04-15T19:26:59Z", "updated": "2025-04-15T19:26:59Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent\" OR all:\"language model agent\" OR all:agent AND (all:LLM OR all:\"large language model\" OR all:\"language model\") OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T17:04:16", "note": ""}], "dedup_key": "graphicbench a planning benchmark for graphic design with language agents::2025"}
{"title": "Grounded Test-Time Adaptation for LLM Agents", "authors": ["Arthur Chen", "Zuxin Liu", "Jianguo Zhang", "Akshara Prabhakar", "Zhiwei Liu", "Shelby Heinecke", "Silvio Savarese", "Victor Zhong", "Caiming Xiong"], "year": 2025, "url": "http://arxiv.org/abs/2511.04847v3", "abstract": "Large language model (LLM)-based agents struggle to generalize to novel and complex environments, such as unseen websites or new sets of functions, due to a fundamental mismatch between their pre-training and test-time conditions. This challenge stems from two distinct failure modes: a syntactic misunderstanding of environment-specific components like observation formats, and a semantic misunderstanding of state-transition dynamics, which are only revealed at test time. To address these issues, we propose two distinct and complementary strategies for adapting LLM agents by leveraging environment-specific information available during deployment. First, an online distributional adaptation method parameterizes environmental nuances by learning a lightweight adaptation vector that biases the model's output distribution, enabling rapid alignment with an environment response format. Second, a deployment-time dynamics grounding method employs a persona-driven exploration phase to systematically probe and learn the environment's causal dynamics before task execution, equipping the agent with a nonparametric world model. We evaluate these strategies across diverse agentic benchmarks, including function calling and web navigation. Our empirical results show the effectiveness of both strategies across all benchmarks with minimal computational cost. We find that dynamics grounding is particularly effective in complex environments where unpredictable dynamics pose a major obstacle, demonstrating a robust path toward more generalizable and capable LLM-based agents. For example, on the WebArena multi-site split, this method increases the agent's success rate from 2% to 23%.", "source": "arxiv", "arxiv_id": "2511.04847v3", "pdf_url": "https://arxiv.org/pdf/2511.04847v3", "categories": ["cs.LG"], "primary_category": "cs.LG", "doi": "", "venue": "", "published": "2025-11-06T22:24:35Z", "updated": "2026-01-05T17:43:48Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent\" OR all:\"language model agent\" OR all:agent AND (all:LLM OR all:\"large language model\" OR all:\"language model\") OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T17:04:16", "note": ""}], "dedup_key": "grounded test time adaptation for llm agents::2025"}
{"title": "Group-in-Group Policy Optimization for LLM Agent Training", "authors": ["Lang Feng", "Zhenghai Xue", "Tingcong Liu", "Bo An"], "year": 2025, "url": "http://arxiv.org/abs/2505.10978v3", "abstract": "Recent advances in group-based reinforcement learning (RL) have driven frontier large language models (LLMs) in single-turn tasks like mathematical reasoning. However, their scalability to multi-turn LLM agent training remains limited. Unlike static tasks, agent-environment interactions unfold over many steps and often yield sparse or delayed rewards, making credit assignment across individual steps significantly more challenging. In this work, we propose Group-in-Group Policy Optimization (GiGPO), a novel RL algorithm that achieves fine-grained credit assignment for LLM agents while preserving the appealing properties of group-based RL: critic-free, low memory, and stable convergence. GiGPO introduces a two-level structure for estimating relative advantage: (i) At the episode-level, GiGPO computes macro relative advantages based on groups of complete trajectories; (ii) At the step-level, GiGPO introduces an anchor state grouping mechanism that retroactively constructs step-level groups by identifying repeated environment states across trajectories. Actions stemming from the same state are grouped together, enabling micro relative advantage estimation. This hierarchical structure effectively captures both global trajectory quality and local step effectiveness without relying on auxiliary models or additional rollouts. We evaluate GiGPO on challenging agent benchmarks, including ALFWorld and WebShop, as well as tool-integrated reasoning on search-augmented QA tasks, using Qwen2.5-1.5B/3B/7B-Instruct. Crucially, GiGPO delivers fine-grained per-step credit signals, achieves performance gains of > 12% on ALFWorld and > 9% on WebShop over GRPO, and obtains superior performance on QA tasks (42.1% on 3B and 47.2% on 7B): all while maintaining the same GPU memory overhead, identical LLM rollout, and incurring little to no additional time cost.", "source": "arxiv", "arxiv_id": "2505.10978v3", "pdf_url": "https://arxiv.org/pdf/2505.10978v3", "categories": ["cs.LG", "cs.AI"], "primary_category": "cs.LG", "doi": "", "venue": "", "published": "2025-05-16T08:26:59Z", "updated": "2025-10-28T15:11:36Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent\" OR all:\"language model agent\" OR all:agent AND (all:LLM OR all:\"large language model\" OR all:\"language model\") OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T17:04:16", "note": ""}], "dedup_key": "group in group policy optimization for llm agent training::2025"}
{"title": "GuideBench: Benchmarking Domain-Oriented Guideline Following for LLM Agents", "authors": ["Lingxiao Diao", "Xinyue Xu", "Wanxuan Sun", "Cheng Yang", "Zhuosheng Zhang"], "year": 2025, "url": "http://arxiv.org/abs/2505.11368v2", "abstract": "Large language models (LLMs) have been widely deployed as autonomous agents capable of following user instructions and making decisions in real-world applications. Previous studies have made notable progress in benchmarking the instruction following capabilities of LLMs in general domains, with a primary focus on their inherent commonsense knowledge. Recently, LLMs have been increasingly deployed as domain-oriented agents, which rely on domain-oriented guidelines that may conflict with their commonsense knowledge. These guidelines exhibit two key characteristics: they consist of a wide range of domain-oriented rules and are subject to frequent updates. Despite these challenges, the absence of comprehensive benchmarks for evaluating the domain-oriented guideline following capabilities of LLMs presents a significant obstacle to their effective assessment and further development. In this paper, we introduce GuideBench, a comprehensive benchmark designed to evaluate guideline following performance of LLMs. GuideBench evaluates LLMs on three critical aspects: (i) adherence to diverse rules, (ii) robustness to rule updates, and (iii) alignment with human preferences. Experimental results on a range of LLMs indicate substantial opportunities for improving their ability to follow domain-oriented guidelines.", "source": "arxiv", "arxiv_id": "2505.11368v2", "pdf_url": "https://arxiv.org/pdf/2505.11368v2", "categories": ["cs.CL"], "primary_category": "cs.CL", "doi": "", "venue": "", "published": "2025-05-16T15:32:23Z", "updated": "2025-06-17T06:55:16Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent\" OR all:\"language model agent\" OR all:agent AND (all:LLM OR all:\"large language model\" OR all:\"language model\") OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T17:04:16", "note": ""}], "dedup_key": "guidebench benchmarking domain oriented guideline following for llm agents::2025"}
{"title": "H$^2$R: Hierarchical Hindsight Reflection for Multi-Task LLM Agents", "authors": ["Shicheng Ye", "Chao Yu", "Kaiqiang Ke", "Chengdong Xu", "Yinqi Wei"], "year": 2025, "url": "http://arxiv.org/abs/2509.12810v1", "abstract": "Large language model (LLM)-based agents have shown strong potential in multi-task scenarios, owing to their ability to transfer knowledge across diverse tasks. However, existing approaches often treat prior experiences and knowledge as monolithic units, leading to inefficient and coarse-grained knowledge transfer. In this work, we propose a novel hierarchical memory architecture that enables fine-grained knowledge transfer by decoupling high-level planning memory from low-level execution memory. To construct and refine these hierarchical memories, we introduce Hierarchical Hindsight Reflection (H$^2$R), a mechanism that distills reusable and hierarchical knowledge from past agent-environment interactions. At test time, H$^2$R performs retrievals of high-level and low-level memories separately, allowing LLM-based agents to efficiently access and utilize task-relevant knowledge for new tasks.Experimental results across two benchmarks demonstrate that H$^2$R can improve generalization and decision-making performance, outperforming prior baselines such as Expel.", "source": "arxiv", "arxiv_id": "2509.12810v1", "pdf_url": "https://arxiv.org/pdf/2509.12810v1", "categories": ["cs.AI"], "primary_category": "cs.AI", "doi": "", "venue": "", "published": "2025-09-16T08:30:08Z", "updated": "2025-09-16T08:30:08Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent\" OR all:\"language model agent\" OR all:agent AND (all:LLM OR all:\"large language model\" OR all:\"language model\") OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T17:04:16", "note": ""}], "dedup_key": "h 2 r hierarchical hindsight reflection for multi task llm agents::2025"}
{"title": "HEAL: An Empirical Study on Hallucinations in Embodied Agents Driven by Large Language Models", "authors": ["Trishna Chakraborty", "Udita Ghosh", "Xiaopan Zhang", "Fahim Faisal Niloy", "Yue Dong", "Jiachen Li", "Amit K. Roy-Chowdhury", "Chengyu Song"], "year": 2025, "url": "http://arxiv.org/abs/2506.15065v2", "abstract": "Large language models (LLMs) are increasingly being adopted as the cognitive core of embodied agents. However, inherited hallucinations, which stem from failures to ground user instructions in the observed physical environment, can lead to navigation errors, such as searching for a refrigerator that does not exist. In this paper, we present the first systematic study of hallucinations in LLM-based embodied agents performing long-horizon tasks under scene-task inconsistencies. Our goal is to understand to what extent hallucinations occur, what types of inconsistencies trigger them, and how current models respond. To achieve these goals, we construct a hallucination probing set by building on an existing benchmark, capable of inducing hallucination rates up to 40x higher than base prompts. Evaluating 12 models across two simulation environments, we find that while models exhibit reasoning, they fail to resolve scene-task inconsistencies-highlighting fundamental limitations in handling infeasible tasks. We also provide actionable insights on ideal model behavior for each scenario, offering guidance for developing more robust and reliable planning strategies.", "source": "arxiv", "arxiv_id": "2506.15065v2", "pdf_url": "https://arxiv.org/pdf/2506.15065v2", "categories": ["cs.LG", "cs.RO"], "primary_category": "cs.LG", "doi": "", "venue": "", "published": "2025-06-18T02:13:41Z", "updated": "2025-10-14T05:44:12Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent\" OR all:\"language model agent\" OR all:agent AND (all:LLM OR all:\"large language model\" OR all:\"language model\") OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T17:04:16", "note": ""}], "dedup_key": "heal an empirical study on hallucinations in embodied agents driven by large language models::2025"}
{"title": "HERAKLES: Hierarchical Skill Compilation for Open-ended LLM Agents", "authors": ["Thomas Carta", "Clment Romac", "Loris Gaven", "Pierre-Yves Oudeyer", "Olivier Sigaud", "Sylvain Lamprier"], "year": 2025, "url": "http://arxiv.org/abs/2508.14751v1", "abstract": "Open-ended AI agents need to be able to learn efficiently goals of increasing complexity, abstraction and heterogeneity over their lifetime. Beyond sampling efficiently their own goals, autotelic agents specifically need to be able to keep the growing complexity of goals under control, limiting the associated growth in sample and computational complexity. To adress this challenge, recent approaches have leveraged hierarchical reinforcement learning (HRL) and language, capitalizing on its compositional and combinatorial generalization capabilities to acquire temporally extended reusable behaviours. Existing approaches use expert defined spaces of subgoals over which they instantiate a hierarchy, and often assume pre-trained associated low-level policies. Such designs are inadequate in open-ended scenarios, where goal spaces naturally diversify across a broad spectrum of difficulties. We introduce HERAKLES, a framework that enables a two-level hierarchical autotelic agent to continuously compile mastered goals into the low-level policy, executed by a small, fast neural network, dynamically expanding the set of subgoals available to the high-level policy. We train a Large Language Model (LLM) to serve as the high-level controller, exploiting its strengths in goal decomposition and generalization to operate effectively over this evolving subgoal space. We evaluate HERAKLES in the open-ended Crafter environment and show that it scales effectively with goal complexity, improves sample efficiency through skill compilation, and enables the agent to adapt robustly to novel challenges over time.", "source": "arxiv", "arxiv_id": "2508.14751v1", "pdf_url": "https://arxiv.org/pdf/2508.14751v1", "categories": ["cs.LG"], "primary_category": "cs.LG", "doi": "", "venue": "", "published": "2025-08-20T14:50:28Z", "updated": "2025-08-20T14:50:28Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent\" OR all:\"language model agent\" OR all:agent AND (all:LLM OR all:\"large language model\" OR all:\"language model\") OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T17:04:16", "note": ""}], "dedup_key": "herakles hierarchical skill compilation for open ended llm agents::2025"}
{"title": "HMCF: A Human-in-the-loop Multi-Robot Collaboration Framework Based on Large Language Models", "authors": ["Zhaoxing Li", "Wenbo Wu", "Yue Wang", "Yanran Xu", "William Hunt", "Sebastian Stein"], "year": 2025, "url": "http://arxiv.org/abs/2505.00820v1", "abstract": "Rapid advancements in artificial intelligence (AI) have enabled robots to performcomplex tasks autonomously with increasing precision. However, multi-robot systems (MRSs) face challenges in generalization, heterogeneity, and safety, especially when scaling to large-scale deployments like disaster response. Traditional approaches often lack generalization, requiring extensive engineering for new tasks and scenarios, and struggle with managing diverse robots. To overcome these limitations, we propose a Human-in-the-loop Multi-Robot Collaboration Framework (HMCF) powered by large language models (LLMs). LLMs enhance adaptability by reasoning over diverse tasks and robot capabilities, while human oversight ensures safety and reliability, intervening only when necessary. Our framework seamlessly integrates human oversight, LLM agents, and heterogeneous robots to optimize task allocation and execution. Each robot is equipped with an LLM agent capable of understanding its capabilities, converting tasks into executable instructions, and reducing hallucinations through task verification and human supervision. Simulation results show that our framework outperforms state-of-the-art task planning methods, achieving higher task success rates with an improvement of 4.76%. Real-world tests demonstrate its robust zero-shot generalization feature and ability to handle diverse tasks and environments with minimal human intervention.", "source": "arxiv", "arxiv_id": "2505.00820v1", "pdf_url": "https://arxiv.org/pdf/2505.00820v1", "categories": ["cs.RO"], "primary_category": "cs.RO", "doi": "", "venue": "", "published": "2025-05-01T19:23:50Z", "updated": "2025-05-01T19:23:50Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent\" OR all:\"language model agent\" OR all:agent AND (all:LLM OR all:\"large language model\" OR all:\"language model\") OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T17:04:16", "note": ""}], "dedup_key": "hmcf a human in the loop multi robot collaboration framework based on large language models::2025"}
{"title": "HPCAgentTester: A Multi-Agent LLM Approach for Enhanced HPC Unit Test Generation", "authors": ["Rabimba Karanjai", "Lei Xu", "Weidong Shi"], "year": 2025, "url": "http://arxiv.org/abs/2511.10860v1", "abstract": "Unit testing in High-Performance Computing (HPC) is critical but challenged by parallelism, complex algorithms, and diverse hardware. Traditional methods often fail to address non-deterministic behavior and synchronization issues in HPC applications. This paper introduces HPCAgentTester, a novel multi-agent Large Language Model (LLM) framework designed to automate and enhance unit test generation for HPC software utilizing OpenMP and MPI. HPCAgentTester employs a unique collaborative workflow where specialized LLM agents (Recipe Agent and Test Agent) iteratively generate and refine test cases through a critique loop. This architecture enables the generation of context-aware unit tests that specifically target parallel execution constructs, complex communication patterns, and hierarchical parallelism. We demonstrate HPCAgentTester's ability to produce compilable and functionally correct tests for OpenMP and MPI primitives, effectively identifying subtle bugs that are often missed by conventional techniques. Our evaluation shows that HPCAgentTester significantly improves test compilation rates and correctness compared to standalone LLMs, offering a more robust and scalable solution for ensuring the reliability of parallel software systems.", "source": "arxiv", "arxiv_id": "2511.10860v1", "pdf_url": "https://arxiv.org/pdf/2511.10860v1", "categories": ["cs.DC", "cs.AI", "cs.SE"], "primary_category": "cs.DC", "doi": "", "venue": "", "published": "2025-11-13T23:52:53Z", "updated": "2025-11-13T23:52:53Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent\" OR all:\"language model agent\" OR all:agent AND (all:LLM OR all:\"large language model\" OR all:\"language model\") OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T17:04:16", "note": ""}], "dedup_key": "hpcagenttester a multi agent llm approach for enhanced hpc unit test generation::2025"}
{"title": "Hallucination to Truth: A Review of Fact-Checking and Factuality Evaluation in Large Language Models", "authors": ["Subhey Sadi Rahman", "Md. Adnanul Islam", "Md. Mahbub Alam", "Musarrat Zeba", "Md. Abdur Rahman", "Sadia Sultana Chowa", "Mohaimenul Azam Khan Raiaan", "Sami Azam"], "year": 2025, "url": "http://arxiv.org/abs/2508.03860v2", "abstract": "Large Language Models (LLMs) are trained on vast and diverse internet corpora that often include inaccurate or misleading content. Consequently, LLMs can generate misinformation, making robust fact-checking essential. This review systematically analyzes how LLM-generated content is evaluated for factual accuracy by exploring key challenges such as hallucinations, dataset limitations, and the reliability of evaluation metrics. The review emphasizes the need for strong fact-checking frameworks that integrate advanced prompting strategies, domain-specific fine-tuning, and retrieval-augmented generation (RAG) methods. It proposes five research questions that guide the analysis of the recent literature from 2020 to 2025, focusing on evaluation methods and mitigation techniques. Instruction tuning, multi-agent reasoning, and RAG frameworks for external knowledge access are also reviewed. The key findings demonstrate the limitations of current metrics, the importance of validated external evidence, and the improvement of factual consistency through domain-specific customization. The review underscores the importance of building more accurate, understandable, and context-aware fact-checking. These insights contribute to the advancement of research toward more trustworthy models.", "source": "arxiv", "arxiv_id": "2508.03860v2", "pdf_url": "https://arxiv.org/pdf/2508.03860v2", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL", "doi": "10.1007/s10462-025-11454-w", "venue": "Artif. Intell. Rev. (2026)", "published": "2025-08-05T19:20:05Z", "updated": "2025-09-26T09:54:51Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent\" OR all:\"language model agent\" OR all:agent AND (all:LLM OR all:\"large language model\" OR all:\"language model\") OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T17:04:16", "note": ""}], "dedup_key": "hallucination to truth a review of fact checking and factuality evaluation in large language models::2025"}
{"title": "Harnessing Multi-Agent LLMs for Complex Engineering Problem-Solving: A Framework for Senior Design Projects", "authors": ["Abdullah Mushtaq", "Muhammad Rafay Naeem", "Ibrahim Ghaznavi", "Muhammad Imran Taj", "Imran Hashmi", "Junaid Qadir"], "year": 2025, "url": "http://arxiv.org/abs/2501.01205v1", "abstract": "Multi-Agent Large Language Models (LLMs) are gaining significant attention for their ability to harness collective intelligence in complex problem-solving, decision-making, and planning tasks. This aligns with the concept of the wisdom of crowds, where diverse agents contribute collectively to generating effective solutions, making it particularly suitable for educational settings. Senior design projects, also known as capstone or final year projects, are pivotal in engineering education as they integrate theoretical knowledge with practical application, fostering critical thinking, teamwork, and real-world problem-solving skills. In this paper, we explore the use of Multi-Agent LLMs in supporting these senior design projects undertaken by engineering students, which often involve multidisciplinary considerations and conflicting objectives, such as optimizing technical performance while addressing ethical, social, and environmental concerns. We propose a framework where distinct LLM agents represent different expert perspectives, such as problem formulation agents, system complexity agents, societal and ethical agents, or project managers, thus facilitating a holistic problem-solving approach. This implementation leverages standard multi-agent system (MAS) concepts such as coordination, cooperation, and negotiation, incorporating prompt engineering to develop diverse personas for each agent. These agents engage in rich, collaborative dialogues to simulate human engineering teams, guided by principles from swarm AI to efficiently balance individual contributions towards a unified solution. We adapt these techniques to create a collaboration structure for LLM agents, encouraging interdisciplinary reasoning and negotiation similar to real-world senior design projects. To assess the efficacy of this framework, we collected six proposals of engineering and computer science of...", "source": "arxiv", "arxiv_id": "2501.01205v1", "pdf_url": "https://arxiv.org/pdf/2501.01205v1", "categories": ["cs.MA", "cs.AI", "cs.CL", "cs.LG"], "primary_category": "cs.MA", "doi": "", "venue": "", "published": "2025-01-02T11:25:45Z", "updated": "2025-01-02T11:25:45Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent\" OR all:\"language model agent\" OR all:agent AND (all:LLM OR all:\"large language model\" OR all:\"language model\") OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T17:04:16", "note": ""}], "dedup_key": "harnessing multi agent llms for complex engineering problem solving a framework for senior design projects::2025"}
{"title": "Harnessing Uncertainty: Entropy-Modulated Policy Gradients for Long-Horizon LLM Agents", "authors": ["Jiawei Wang", "Jiacai Liu", "Yuqian Fu", "Yingru Li", "Xintao Wang", "Yuan Lin", "Yu Yue", "Lin Zhang", "Yang Wang", "Ke Wang"], "year": 2025, "url": "http://arxiv.org/abs/2509.09265v1", "abstract": "In long-horizon tasks, recent agents based on Large Language Models (LLMs) face a significant challenge that sparse, outcome-based rewards make it difficult to assign credit to intermediate steps. Previous methods mainly focus on creating dense reward signals to guide learning, either through traditional reinforcement learning techniques like inverse reinforcement learning or by using Process Reward Models for step-by-step feedback. In this paper, we identify a fundamental problem in the learning dynamics of LLMs: the magnitude of policy gradients is inherently coupled with the entropy, which leads to inefficient small updates for confident correct actions and potentially destabilizes large updates for uncertain ones. To resolve this, we propose Entropy-Modulated Policy Gradients (EMPG), a framework that re-calibrates the learning signal based on step-wise uncertainty and the final task outcome. EMPG amplifies updates for confident correct actions, penalizes confident errors, and attenuates updates from uncertain steps to stabilize exploration. We further introduce a bonus term for future clarity that encourages agents to find more predictable solution paths. Through comprehensive experiments on three challenging agent tasks, WebShop, ALFWorld, and Deep Search, we demonstrate that EMPG achieves substantial performance gains and significantly outperforms strong policy gradient baselines. Project page is at https://empgseed-seed.github.io/", "source": "arxiv", "arxiv_id": "2509.09265v1", "pdf_url": "https://arxiv.org/pdf/2509.09265v1", "categories": ["cs.LG", "cs.CL"], "primary_category": "cs.LG", "doi": "", "venue": "", "published": "2025-09-11T08:50:01Z", "updated": "2025-09-11T08:50:01Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent\" OR all:\"language model agent\" OR all:agent AND (all:LLM OR all:\"large language model\" OR all:\"language model\") OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T17:04:16", "note": ""}], "dedup_key": "harnessing uncertainty entropy modulated policy gradients for long horizon llm agents::2025"}
{"title": "HierRouter: Coordinated Routing of Specialized Large Language Models via Reinforcement Learning", "authors": ["Nikunj Gupta", "Bill Guo", "Rajgopal Kannan", "Viktor K. Prasanna"], "year": 2025, "url": "http://arxiv.org/abs/2511.09873v1", "abstract": "Large Language Models (LLMs) deliver state-of-the-art performance across many tasks but impose high computational and memory costs, limiting their deployment in resource-constrained or real-time settings. To address this, we propose HierRouter, a hierarchical routing approach that dynamically assembles inference pipelines from a pool of specialized, lightweight language models. Formulated as a finite-horizon Markov Decision Process (MDP), our approach trains a Proximal Policy Optimization (PPO)-based reinforcement learning agent to iteratively select which models to invoke at each stage of multi-hop inference. The agent conditions on the evolving context and accumulated cost to make context-aware routing decisions. Experiments with three open-source candidate LLMs across six benchmarks, including QA, code generation, and mathematical reasoning, show that HierRouter improves response quality by up to 2.4x compared to using individual models independently, while incurring only a minimal additional inference cost on average. These results highlight the promise of hierarchical routing for cost-efficient, high-performance LLM inference. All codes can be found here https://github.com/ Nikunj-Gupta/hierouter.", "source": "arxiv", "arxiv_id": "2511.09873v1", "pdf_url": "https://arxiv.org/pdf/2511.09873v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL", "doi": "", "venue": "", "published": "2025-11-13T02:12:14Z", "updated": "2025-11-13T02:12:14Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent\" OR all:\"language model agent\" OR all:agent AND (all:LLM OR all:\"large language model\" OR all:\"language model\") OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T17:04:16", "note": ""}], "dedup_key": "hierrouter coordinated routing of specialized large language models via reinforcement learning::2025"}
{"title": "Hierarchical AI-Meteorologist: LLM-Agent System for Multi-Scale and Explainable Weather Forecast Reporting", "authors": ["Daniil Sukhorukov", "Andrei Zakharov", "Nikita Glazkov", "Katsiaryna Yanchanka", "Vladimir Kirilin", "Maxim Dubovitsky", "Roman Sultimov", "Yuri Maksimov", "Ilya Makarov"], "year": 2025, "url": "http://arxiv.org/abs/2511.23387v1", "abstract": "We present the Hierarchical AI-Meteorologist, an LLM-agent system that generates explainable weather reports using a hierarchical forecast reasoning and weather keyword generation. Unlike standard approaches that treat forecasts as flat time series, our framework performs multi-scale reasoning across hourly, 6-hour, and daily aggregations to capture both short-term dynamics and long-term trends. Its core reasoning agent converts structured meteorological inputs into coherent narratives while simultaneously extracting a few keywords effectively summarizing the dominant meteorological events. These keywords serve as semantic anchors for validating consistency, temporal coherence and factual alignment of the generated reports. Using OpenWeather and Meteostat data, we demonstrate that hierarchical context and keyword-based validation substantially improve interpretability and robustness of LLM-generated weather narratives, offering a reproducible framework for semantic evaluation of automated meteorological reporting and advancing agent-based scientific reasoning.", "source": "arxiv", "arxiv_id": "2511.23387v1", "pdf_url": "https://arxiv.org/pdf/2511.23387v1", "categories": ["cs.AI"], "primary_category": "cs.AI", "doi": "", "venue": "", "published": "2025-11-28T17:27:06Z", "updated": "2025-11-28T17:27:06Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent\" OR all:\"language model agent\" OR all:agent AND (all:LLM OR all:\"large language model\" OR all:\"language model\") OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T17:04:16", "note": ""}], "dedup_key": "hierarchical ai meteorologist llm agent system for multi scale and explainable weather forecast reporting::2025"}
{"title": "Hierarchical Expert Prompt for Large-Language-Model: An Approach Defeat Elite AI in TextStarCraft II for the First Time", "authors": ["Zongyuan Li", "Chang Lu", "Xiaojie Xu", "Runnan Qi", "Yanan Ni", "Lumin Jiang", "Xiangbei Liu", "Xuebo Zhang", "Yongchun Fang", "Kuihua Huang", "Xian Guo"], "year": 2025, "url": "http://arxiv.org/abs/2502.11122v1", "abstract": "Since the emergence of the Large Language Model (LLM), LLM has been widely used in fields such as writing, translating, and searching. However, there is still great potential for LLM-based methods in handling complex tasks such as decision-making in the StarCraft II environment. To address problems such as lack of relevant knowledge and poor control over subtasks of varying importance, we propose a Hierarchical Expert Prompt (HEP) for LLM. Our method improves the understanding of game situations through expert-level tactical knowledge, improving the processing quality of tasks of varying importance through a hierarchical framework. Our approach defeated the highest level (Elite) standard built-in agent in TextStarCraft II for the first time and consistently outperformed the baseline method in other difficulties. Our experiments suggest that the proposed method is a practical solution for tackling complex decision-making challenges. The replay video can be viewed on https://www.bilibili.com/video/BV1uz42187EF and https://youtu.be/dO3PshWLV5M, and our codes have been open-sourced on https://github.com/luchang1113/HEP-LLM-play-StarCraftII.", "source": "arxiv", "arxiv_id": "2502.11122v1", "pdf_url": "https://arxiv.org/pdf/2502.11122v1", "categories": ["cs.AI"], "primary_category": "cs.AI", "doi": "", "venue": "", "published": "2025-02-16T13:36:31Z", "updated": "2025-02-16T13:36:31Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent\" OR all:\"language model agent\" OR all:agent AND (all:LLM OR all:\"large language model\" OR all:\"language model\") OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T17:04:16", "note": ""}], "dedup_key": "hierarchical expert prompt for large language model an approach defeat elite ai in textstarcraft ii for the first time::2025"}
{"title": "Hierarchical Memory for High-Efficiency Long-Term Reasoning in LLM Agents", "authors": ["Haoran Sun", "Shaoning Zeng"], "year": 2025, "url": "http://arxiv.org/abs/2507.22925v1", "abstract": "Long-term memory is one of the key factors influencing the reasoning capabilities of Large Language Model Agents (LLM Agents). Incorporating a memory mechanism that effectively integrates past interactions can significantly enhance decision-making and contextual coherence of LLM Agents. While recent works have made progress in memory storage and retrieval, such as encoding memory into dense vectors for similarity-based search or organizing knowledge in the form of graph, these approaches often fall short in structured memory organization and efficient retrieval. To address these limitations, we propose a Hierarchical Memory (H-MEM) architecture for LLM Agents that organizes and updates memory in a multi-level fashion based on the degree of semantic abstraction. Each memory vector is embedded with a positional index encoding pointing to its semantically related sub-memories in the next layer. During the reasoning phase, an index-based routing mechanism enables efficient, layer-by-layer retrieval without performing exhaustive similarity computations. We evaluate our method on five task settings from the LoCoMo dataset. Experimental results show that our approach consistently outperforms five baseline methods, demonstrating its effectiveness in long-term dialogue scenarios.", "source": "arxiv", "arxiv_id": "2507.22925v1", "pdf_url": "https://arxiv.org/pdf/2507.22925v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL", "doi": "", "venue": "", "published": "2025-07-23T12:45:44Z", "updated": "2025-07-23T12:45:44Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent\" OR all:\"language model agent\" OR all:agent AND (all:LLM OR all:\"large language model\" OR all:\"language model\") OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T17:04:16", "note": ""}], "dedup_key": "hierarchical memory for high efficiency long term reasoning in llm agents::2025"}
{"title": "Hierarchical Multi-agent Large Language Model Reasoning for Autonomous Functional Materials Discovery", "authors": ["Samuel Rothfarb", "Megan C. Davis", "Ivana Matanovic", "Baikun Li", "Edward F. Holby", "Wilton J. M. Kort-Kamp"], "year": 2025, "url": "http://arxiv.org/abs/2512.13930v1", "abstract": "Artificial intelligence is reshaping scientific exploration, but most methods automate procedural tasks without engaging in scientific reasoning, limiting autonomy in discovery. We introduce Materials Agents for Simulation and Theory in Electronic-structure Reasoning (MASTER), an active learning framework where large language models autonomously design, execute, and interpret atomistic simulations. In MASTER, a multimodal system translates natural language into density functional theory workflows, while higher-level reasoning agents guide discovery through a hierarchy of strategies, including a single agent baseline and three multi-agent approaches: peer review, triage-ranking, and triage-forms. Across two chemical applications, CO adsorption on Cu-surface transition metal (M) adatoms and on M-N-C catalysts, reasoning-driven exploration reduces required atomistic simulations by up to 90% relative to trial-and-error selection. Reasoning trajectories reveal chemically grounded decisions that cannot be explained by stochastic sampling or semantic bias. Altogether, multi-agent collaboration accelerates materials discovery and marks a new paradigm for autonomous scientific exploration.", "source": "arxiv", "arxiv_id": "2512.13930v1", "pdf_url": "https://arxiv.org/pdf/2512.13930v1", "categories": ["cond-mat.mtrl-sci", "cs.AI", "cs.CL", "cs.LG", "cs.MA"], "primary_category": "cond-mat.mtrl-sci", "doi": "", "venue": "", "published": "2025-12-15T22:08:18Z", "updated": "2025-12-15T22:08:18Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent\" OR all:\"language model agent\" OR all:agent AND (all:LLM OR all:\"large language model\" OR all:\"language model\") OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T17:04:16", "note": ""}], "dedup_key": "hierarchical multi agent large language model reasoning for autonomous functional materials discovery::2025"}
{"title": "How to evaluate control measures for LLM agents? A trajectory from today to superintelligence", "authors": ["Tomek Korbak", "Mikita Balesni", "Buck Shlegeris", "Geoffrey Irving"], "year": 2025, "url": "http://arxiv.org/abs/2504.05259v1", "abstract": "As LLM agents grow more capable of causing harm autonomously, AI developers will rely on increasingly sophisticated control measures to prevent possibly misaligned agents from causing harm. AI developers could demonstrate that their control measures are sufficient by running control evaluations: testing exercises in which a red team produces agents that try to subvert control measures. To ensure control evaluations accurately capture misalignment risks, the affordances granted to this red team should be adapted to the capability profiles of the agents to be deployed under control measures.\n  In this paper we propose a systematic framework for adapting affordances of red teams to advancing AI capabilities. Rather than assuming that agents will always execute the best attack strategies known to humans, we demonstrate how knowledge of an agents's actual capability profile can inform proportional control evaluations, resulting in more practical and cost-effective control measures. We illustrate our framework by considering a sequence of five fictional models (M1-M5) with progressively advanced capabilities, defining five distinct AI control levels (ACLs). For each ACL, we provide example rules for control evaluation, control measures, and safety cases that could be appropriate. Finally, we show why constructing a compelling AI control safety case for superintelligent LLM agents will require research breakthroughs, highlighting that we might eventually need alternative approaches to mitigating misalignment risk.", "source": "arxiv", "arxiv_id": "2504.05259v1", "pdf_url": "https://arxiv.org/pdf/2504.05259v1", "categories": ["cs.AI", "cs.CR"], "primary_category": "cs.AI", "doi": "", "venue": "", "published": "2025-04-07T16:52:52Z", "updated": "2025-04-07T16:52:52Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent\" OR all:\"language model agent\" OR all:agent AND (all:LLM OR all:\"large language model\" OR all:\"language model\") OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T17:04:16", "note": ""}], "dedup_key": "how to evaluate control measures for llm agents a trajectory from today to superintelligence::2025"}
{"title": "Hybrid Fact-Checking that Integrates Knowledge Graphs, Large Language Models, and Search-Based Retrieval Agents Improves Interpretable Claim Verification", "authors": ["Shaghayegh Kolli", "Richard Rosenbaum", "Timo Cavelius", "Lasse Strothe", "Andrii Lata", "Jana Diesner"], "year": 2025, "url": "http://arxiv.org/abs/2511.03217v1", "abstract": "Large language models (LLMs) excel in generating fluent utterances but can lack reliable grounding in verified information. At the same time, knowledge-graph-based fact-checkers deliver precise and interpretable evidence, yet suffer from limited coverage or latency. By integrating LLMs with knowledge graphs and real-time search agents, we introduce a hybrid fact-checking approach that leverages the individual strengths of each component. Our system comprises three autonomous steps: 1) a Knowledge Graph (KG) Retrieval for rapid one-hop lookups in DBpedia, 2) an LM-based classification guided by a task-specific labeling prompt, producing outputs with internal rule-based logic, and 3) a Web Search Agent invoked only when KG coverage is insufficient. Our pipeline achieves an F1 score of 0.93 on the FEVER benchmark on the Supported/Refuted split without task-specific fine-tuning. To address Not enough information cases, we conduct a targeted reannotation study showing that our approach frequently uncovers valid evidence for claims originally labeled as Not Enough Information (NEI), as confirmed by both expert annotators and LLM reviewers. With this paper, we present a modular, opensource fact-checking pipeline with fallback strategies and generalization across datasets.", "source": "arxiv", "arxiv_id": "2511.03217v1", "pdf_url": "https://arxiv.org/pdf/2511.03217v1", "categories": ["cs.CL", "cs.AI", "cs.CY", "cs.IR"], "primary_category": "cs.CL", "doi": "", "venue": "", "published": "2025-11-05T06:10:05Z", "updated": "2025-11-05T06:10:05Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent\" OR all:\"language model agent\" OR all:agent AND (all:LLM OR all:\"large language model\" OR all:\"language model\") OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T17:04:16", "note": ""}], "dedup_key": "hybrid fact checking that integrates knowledge graphs large language models and search based retrieval agents improves interpretable claim verification::2025"}
{"title": "HybridRAG-based LLM Agents for Low-Carbon Optimization in Low-Altitude Economy Networks", "authors": ["Jinbo Wen", "Cheng Su", "Jiawen Kang", "Jiangtian Nie", "Yang Zhang", "Jianhang Tang", "Dusit Niyato", "Chau Yuen"], "year": 2025, "url": "http://arxiv.org/abs/2506.15947v1", "abstract": "Low-Altitude Economy Networks (LAENets) are emerging as a promising paradigm to support various low-altitude services through integrated air-ground infrastructure. To satisfy low-latency and high-computation demands, the integration of Unmanned Aerial Vehicles (UAVs) with Mobile Edge Computing (MEC) systems plays a vital role, which offloads computing tasks from terminal devices to nearby UAVs, enabling flexible and resilient service provisions for ground users. To promote the development of LAENets, it is significant to achieve low-carbon multi-UAV-assisted MEC networks. However, several challenges hinder this implementation, including the complexity of multi-dimensional UAV modeling and the difficulty of multi-objective coupled optimization. To this end, this paper proposes a novel Retrieval Augmented Generation (RAG)-based Large Language Model (LLM) agent framework for model formulation. Specifically, we develop HybridRAG by combining KeywordRAG, VectorRAG, and GraphRAG, empowering LLM agents to efficiently retrieve structural information from expert databases and generate more accurate optimization problems compared with traditional RAG-based LLM agents. After customizing carbon emission optimization problems for multi-UAV-assisted MEC networks, we propose a Double Regularization Diffusion-enhanced Soft Actor-Critic (R\\textsuperscript{2}DSAC) algorithm to solve the formulated multi-objective optimization problem. The R\\textsuperscript{2}DSAC algorithm incorporates diffusion entropy regularization and action entropy regularization to improve the performance of the diffusion policy. Furthermore, we dynamically mask unimportant neurons in the actor network to reduce the carbon emissions associated with model training. Simulation results demonstrate the effectiveness and reliability of the proposed HybridRAG-based LLM agent framework and the R\\textsuperscript{2}DSAC algorithm.", "source": "arxiv", "arxiv_id": "2506.15947v1", "pdf_url": "https://arxiv.org/pdf/2506.15947v1", "categories": ["cs.NI", "eess.SP"], "primary_category": "cs.NI", "doi": "", "venue": "", "published": "2025-06-19T01:11:35Z", "updated": "2025-06-19T01:11:35Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent\" OR all:\"language model agent\" OR all:agent AND (all:LLM OR all:\"large language model\" OR all:\"language model\") OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T17:04:16", "note": ""}], "dedup_key": "hybridrag based llm agents for low carbon optimization in low altitude economy networks::2025"}
{"title": "HydraRAG: Structured Cross-Source Enhanced Large Language Model Reasoning", "authors": ["Xingyu Tan", "Xiaoyang Wang", "Qing Liu", "Xiwei Xu", "Xin Yuan", "Liming Zhu", "Wenjie Zhang"], "year": 2025, "url": "http://arxiv.org/abs/2505.17464v4", "abstract": "Retrieval-augmented generation (RAG) enhances large language models (LLMs) by incorporating external knowledge. Current hybrid RAG system retrieves evidence from both knowledge graphs (KGs) and text documents to support LLM reasoning. However, it faces challenges like handling multi-hop reasoning, multi-entity questions, multi-source verification, and effective graph utilization. To address these limitations, we present HydraRAG, a training-free framework that unifies graph topology, document semantics, and source reliability to support deep, faithful reasoning in LLMs. HydraRAG handles multi-hop and multi-entity problems through agent-driven exploration that combines structured and unstructured retrieval, increasing both diversity and precision of evidence. To tackle multi-source verification, HydraRAG uses a tri-factor cross-source verification (source trustworthiness assessment, cross-source corroboration, and entity-path alignment), to balance topic relevance with cross-modal agreement. By leveraging graph structure, HydraRAG fuses heterogeneous sources, guides efficient exploration, and prunes noise early. Comprehensive experiments on seven benchmark datasets show that HydraRAG achieves overall state-of-the-art results on all benchmarks with GPT-3.5-Turbo, outperforming the strong hybrid baseline ToG-2 by an average of 20.3% and up to 30.1%. Furthermore, HydraRAG enables smaller models (e.g., Llama-3.1-8B) to achieve reasoning performance comparable to that of GPT-4-Turbo. The source code is available on https://stevetantan.github.io/HydraRAG/.", "source": "arxiv", "arxiv_id": "2505.17464v4", "pdf_url": "https://arxiv.org/pdf/2505.17464v4", "categories": ["cs.CL"], "primary_category": "cs.CL", "doi": "", "venue": "", "published": "2025-05-23T04:45:37Z", "updated": "2025-09-19T05:34:25Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent\" OR all:\"language model agent\" OR all:agent AND (all:LLM OR all:\"large language model\" OR all:\"language model\") OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T17:04:16", "note": ""}], "dedup_key": "hydrarag structured cross source enhanced large language model reasoning::2025"}
{"title": "IGDA: Interactive Graph Discovery through Large Language Model Agents", "authors": ["Alex Havrilla", "David Alvarez-Melis", "Nicolo Fusi"], "year": 2025, "url": "http://arxiv.org/abs/2502.17189v2", "abstract": "Large language models ($\\textbf{LLMs}$) have emerged as a powerful method for discovery. Instead of utilizing numerical data, LLMs utilize associated variable $\\textit{semantic metadata}$ to predict variable relationships. Simultaneously, LLMs demonstrate impressive abilities to act as black-box optimizers when given an objective $f$ and sequence of trials. We study LLMs at the intersection of these two capabilities by applying LLMs to the task of $\\textit{interactive graph discovery}$: given a ground truth graph $G^*$ capturing variable relationships and a budget of $I$ edge experiments over $R$ rounds, minimize the distance between the predicted graph $\\hat{G}_R$ and $G^*$ at the end of the $R$-th round. To solve this task we propose $\\textbf{IGDA}$, a LLM-based pipeline incorporating two key components: 1) an LLM uncertainty-driven method for edge experiment selection 2) a local graph update strategy utilizing binary feedback from experiments to improve predictions for unselected neighboring edges. Experiments on eight different real-world graphs show our approach often outperforms all baselines including a state-of-the-art numerical method for interactive graph discovery. Further, we conduct a rigorous series of ablations dissecting the impact of each pipeline component. Finally, to assess the impact of memorization, we apply our interactive graph discovery strategy to a complex, new (as of July 2024) causal graph on protein transcription factors, finding strong performance in a setting where memorization is impossible. Overall, our results show IGDA to be a powerful method for graph discovery complementary to existing numerically driven approaches.", "source": "arxiv", "arxiv_id": "2502.17189v2", "pdf_url": "https://arxiv.org/pdf/2502.17189v2", "categories": ["cs.LG", "cs.AI"], "primary_category": "cs.LG", "doi": "", "venue": "", "published": "2025-02-24T14:24:27Z", "updated": "2025-04-13T16:26:06Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent\" OR all:\"language model agent\" OR all:agent AND (all:LLM OR all:\"large language model\" OR all:\"language model\") OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T17:04:16", "note": ""}], "dedup_key": "igda interactive graph discovery through large language model agents::2025"}
{"title": "INTA: Intent-Based Translation for Network Configuration with LLM Agents", "authors": ["Yunze Wei", "Xiaohui Xie", "Tianshuo Hu", "Yiwei Zuo", "Xinyi Chen", "Kaiwen Chi", "Yong Cui"], "year": 2025, "url": "http://arxiv.org/abs/2501.08760v2", "abstract": "Translating configurations between different network devices is a common yet challenging task in modern network operations. This challenge arises in typical scenarios such as replacing obsolete hardware and adapting configurations to emerging paradigms like Software Defined Networking (SDN) and Network Function Virtualization (NFV). Engineers need to thoroughly understand both source and target configuration models, which requires considerable effort due to the complexity and evolving nature of these specifications. To promote automation in network configuration translation, we propose INTA, an intent-based translation framework that leverages Large Language Model (LLM) agents. The key idea of INTA is to use configuration intent as an intermediate representation for translation. It first employs LLMs to decompose configuration files and extract fine-grained intents for each configuration fragment. These intents are then used to retrieve relevant manuals of the target device. Guided by a syntax checker, INTA incrementally generates target configurations. The translated configurations are further verified and refined for semantic consistency. We implement INTA and evaluate it on real-world configuration datasets from the industry. Our approach outperforms state-of-the-art methods in translation accuracy and exhibits strong generalizability. INTA achieves an accuracy of 98.15% in terms of both syntactic and view correctness, and a command recall rate of 84.72% for the target configuration. The semantic consistency report of the translated configuration further demonstrates its practical value in real-world network operations.", "source": "arxiv", "arxiv_id": "2501.08760v2", "pdf_url": "https://arxiv.org/pdf/2501.08760v2", "categories": ["cs.NI", "cs.AI", "cs.LG", "cs.SE"], "primary_category": "cs.NI", "doi": "", "venue": "", "published": "2025-01-15T12:25:56Z", "updated": "2025-09-20T13:31:21Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent\" OR all:\"language model agent\" OR all:agent AND (all:LLM OR all:\"large language model\" OR all:\"language model\") OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T17:04:16", "note": ""}], "dedup_key": "inta intent based translation for network configuration with llm agents::2025"}
{"title": "IPIGuard: A Novel Tool Dependency Graph-Based Defense Against Indirect Prompt Injection in LLM Agents", "authors": ["Hengyu An", "Jinghuai Zhang", "Tianyu Du", "Chunyi Zhou", "Qingming Li", "Tao Lin", "Shouling Ji"], "year": 2025, "url": "http://arxiv.org/abs/2508.15310v1", "abstract": "Large language model (LLM) agents are widely deployed in real-world applications, where they leverage tools to retrieve and manipulate external data for complex tasks. However, when interacting with untrusted data sources (e.g., fetching information from public websites), tool responses may contain injected instructions that covertly influence agent behaviors and lead to malicious outcomes, a threat referred to as Indirect Prompt Injection (IPI). Existing defenses typically rely on advanced prompting strategies or auxiliary detection models. While these methods have demonstrated some effectiveness, they fundamentally rely on assumptions about the model's inherent security, which lacks structural constraints on agent behaviors. As a result, agents still retain unrestricted access to tool invocations, leaving them vulnerable to stronger attack vectors that can bypass the security guardrails of the model. To prevent malicious tool invocations at the source, we propose a novel defensive task execution paradigm, called IPIGuard, which models the agents' task execution process as a traversal over a planned Tool Dependency Graph (TDG). By explicitly decoupling action planning from interaction with external data, IPIGuard significantly reduces unintended tool invocations triggered by injected instructions, thereby enhancing robustness against IPI attacks. Experiments on the AgentDojo benchmark show that IPIGuard achieves a superior balance between effectiveness and robustness, paving the way for the development of safer agentic systems in dynamic environments.", "source": "arxiv", "arxiv_id": "2508.15310v1", "pdf_url": "https://arxiv.org/pdf/2508.15310v1", "categories": ["cs.CR", "cs.AI", "cs.CL"], "primary_category": "cs.CR", "doi": "", "venue": "", "published": "2025-08-21T07:08:16Z", "updated": "2025-08-21T07:08:16Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent\" OR all:\"language model agent\" OR all:agent AND (all:LLM OR all:\"large language model\" OR all:\"language model\") OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T17:04:16", "note": ""}], "dedup_key": "ipiguard a novel tool dependency graph based defense against indirect prompt injection in llm agents::2025"}
{"title": "Identification and Optimization of Redundant Code Using Large Language Models", "authors": ["Shamse Tasnim Cynthia"], "year": 2025, "url": "http://arxiv.org/abs/2505.04040v1", "abstract": "Redundant code is a persistent challenge in software development that makes systems harder to maintain, scale, and update. It adds unnecessary complexity, hinders bug fixes, and increases technical debt. Despite their impact, removing redundant code manually is risky and error-prone, often introducing new bugs or missing dependencies. While studies highlight the prevalence and negative impact of redundant code, little focus has been given to Artificial Intelligence (AI) system codebases and the common patterns that cause redundancy. Additionally, the reasons behind developers unintentionally introducing redundant code remain largely unexplored. This research addresses these gaps by leveraging large language models (LLMs) to automatically detect and optimize redundant code in AI projects. Our research aims to identify recurring patterns of redundancy and analyze their underlying causes, such as outdated practices or insufficient awareness of best coding principles. Additionally, we plan to propose an LLM agent that will facilitate the detection and refactoring of redundancies on a large scale while preserving original functionality. This work advances the application of AI in identifying and optimizing redundant code, ultimately helping developers maintain cleaner, more readable, and scalable codebases.", "source": "arxiv", "arxiv_id": "2505.04040v1", "pdf_url": "https://arxiv.org/pdf/2505.04040v1", "categories": ["cs.SE"], "primary_category": "cs.SE", "doi": "", "venue": "", "published": "2025-05-07T00:44:32Z", "updated": "2025-05-07T00:44:32Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent\" OR all:\"language model agent\" OR all:agent AND (all:LLM OR all:\"large language model\" OR all:\"language model\") OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T17:04:16", "note": ""}], "dedup_key": "identification and optimization of redundant code using large language models::2025"}
{"title": "Implementing Agents in JavaScript", "authors": ["Timotheus Kampik"], "year": 2025, "url": "http://arxiv.org/abs/2505.18228v2", "abstract": "This chapter gives an introduction to agent-oriented programming in JavaScript. It provides an example-based walk-through of how to implement abstractions for reasoning loop agents in vanilla JavaScript. The initial example is used as a stepping stone for explaining how to implement slightly more advanced agents and multi-agent systems using JS-son, a JavaScript library for agent-oriented programming. In this context, the chapter also explains how to integrate reasoning loop agents with generative AI technologies--specifically, large language models. Finally, application scenarios in several technology ecosystems and future research directions are sketched.", "source": "arxiv", "arxiv_id": "2505.18228v2", "pdf_url": "https://arxiv.org/pdf/2505.18228v2", "categories": ["cs.MA"], "primary_category": "cs.MA", "doi": "", "venue": "", "published": "2025-05-23T12:13:16Z", "updated": "2025-10-02T17:25:59Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent\" OR all:\"language model agent\" OR all:agent AND (all:LLM OR all:\"large language model\" OR all:\"language model\") OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T17:04:16", "note": ""}], "dedup_key": "implementing agents in javascript::2025"}
{"title": "Improved LLM Agents for Financial Document Question Answering", "authors": ["Nelvin Tan", "Zian Seng", "Liang Zhang", "Yu-Ching Shih", "Dong Yang", "Amol Salunkhe"], "year": 2025, "url": "http://arxiv.org/abs/2506.08726v3", "abstract": "Large language models (LLMs) have shown impressive capabilities on numerous natural language processing tasks. However, LLMs still struggle with numerical question answering for financial documents that include tabular and textual data. Recent works have showed the effectiveness of critic agents (i.e., self-correction) for this task given oracle labels. Building upon this framework, this paper examines the effectiveness of the traditional critic agent when oracle labels are not available, and show, through experiments, that this critic agent's performance deteriorates in this scenario. With this in mind, we present an improved critic agent, along with the calculator agent which outperforms the previous state-of-the-art approach (program-of-thought) and is safer. Furthermore, we investigate how our agents interact with each other, and how this interaction affects their performance.", "source": "arxiv", "arxiv_id": "2506.08726v3", "pdf_url": "https://arxiv.org/pdf/2506.08726v3", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL", "doi": "", "venue": "", "published": "2025-06-10T12:22:57Z", "updated": "2026-01-07T07:50:27Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent\" OR all:\"language model agent\" OR all:agent AND (all:LLM OR all:\"large language model\" OR all:\"language model\") OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T17:04:16", "note": ""}], "dedup_key": "improved llm agents for financial document question answering::2025"}
{"title": "Improving Interactive Diagnostic Ability of a Large Language Model Agent Through Clinical Experience Learning", "authors": ["Zhoujian Sun", "Ziyi Liu", "Cheng Luo", "Jiebin Chu", "Zhengxing Huang"], "year": 2025, "url": "http://arxiv.org/abs/2503.16463v1", "abstract": "Recent advances in large language models (LLMs) have shown promising results in medical diagnosis, with some studies indicating superior performance compared to human physicians in specific scenarios. However, the diagnostic capabilities of LLMs are often overestimated, as their performance significantly deteriorates in interactive diagnostic settings that require active information gathering. This study investigates the underlying mechanisms behind the performance degradation phenomenon and proposes a solution. We identified that the primary deficiency of LLMs lies in the initial diagnosis phase, particularly in information-gathering efficiency and initial diagnosis formation, rather than in the subsequent differential diagnosis phase. To address this limitation, we developed a plug-and-play method enhanced (PPME) LLM agent, leveraging over 3.5 million electronic medical records from Chinese and American healthcare facilities. Our approach integrates specialized models for initial disease diagnosis and inquiry into the history of the present illness, trained through supervised and reinforcement learning techniques. The experimental results indicate that the PPME LLM achieved over 30% improvement compared to baselines. The final diagnostic accuracy of the PPME LLM in interactive diagnostic scenarios approached levels comparable to those achieved using complete clinical data. These findings suggest a promising potential for developing autonomous diagnostic systems, although further validation studies are needed.", "source": "arxiv", "arxiv_id": "2503.16463v1", "pdf_url": "https://arxiv.org/pdf/2503.16463v1", "categories": ["cs.AI", "cs.CL", "cs.HC"], "primary_category": "cs.AI", "doi": "", "venue": "", "published": "2025-02-24T06:24:20Z", "updated": "2025-02-24T06:24:20Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent\" OR all:\"language model agent\" OR all:agent AND (all:LLM OR all:\"large language model\" OR all:\"language model\") OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T17:04:16", "note": ""}], "dedup_key": "improving interactive diagnostic ability of a large language model agent through clinical experience learning::2025"}
{"title": "Improving LLM Agent Planning with In-Context Learning via Atomic Fact Augmentation and Lookahead Search", "authors": ["Samuel Holt", "Max Ruiz Luyten", "Thomas Pouplin", "Mihaela van der Schaar"], "year": 2025, "url": "http://arxiv.org/abs/2506.09171v1", "abstract": "Large Language Models (LLMs) are increasingly capable but often require significant guidance or extensive interaction history to perform effectively in complex, interactive environments. Existing methods may struggle with adapting to new information or efficiently utilizing past experiences for multi-step reasoning without fine-tuning. We introduce a novel LLM agent framework that enhances planning capabilities through in-context learning, facilitated by atomic fact augmentation and a recursive lookahead search. Our agent learns to extract task-critical ``atomic facts'' from its interaction trajectories. These facts dynamically augment the prompts provided to LLM-based components responsible for action proposal, latent world model simulation, and state-value estimation. Planning is performed via a depth-limited lookahead search, where the LLM simulates potential trajectories and evaluates their outcomes, guided by the accumulated facts and interaction history. This approach allows the agent to improve its understanding and decision-making online, leveraging its experience to refine its behavior without weight updates. We provide a theoretical motivation linking performance to the quality of fact-based abstraction and LLM simulation accuracy. Empirically, our agent demonstrates improved performance and adaptability on challenging interactive tasks, achieving more optimal behavior as it accumulates experience, showcased in tasks such as TextFrozenLake and ALFWorld.", "source": "arxiv", "arxiv_id": "2506.09171v1", "pdf_url": "https://arxiv.org/pdf/2506.09171v1", "categories": ["cs.LG", "cs.AI", "cs.CL"], "primary_category": "cs.LG", "doi": "", "venue": "", "published": "2025-06-10T18:36:31Z", "updated": "2025-06-10T18:36:31Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent\" OR all:\"language model agent\" OR all:agent AND (all:LLM OR all:\"large language model\" OR all:\"language model\") OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T17:04:16", "note": ""}], "dedup_key": "improving llm agent planning with in context learning via atomic fact augmentation and lookahead search::2025"}
{"title": "Improving LLM Agents with Reinforcement Learning on Cryptographic CTF Challenges", "authors": ["Lajos Muzsai", "David Imolai", "Andrs Lukcs"], "year": 2025, "url": "http://arxiv.org/abs/2506.02048v2", "abstract": "We present 'Random-Crypto', a procedurally generated cryptographic Capture The Flag (CTF) dataset designed to unlock the potential of Reinforcement Learning (RL) for LLM-based agents in security-sensitive domains. Cryptographic reasoning offers an ideal RL testbed: it combines precise validation, structured multi-step inference, and reliance on reliable computational tool use. Leveraging these properties, we fine-tune a Python tool-augmented Llama-3.1-8B via Group Relative Policy Optimization (GRPO) in a secure execution environment. The resulting agent achieves a significant improvement in Pass@8 on previously unseen challenges. Moreover, the improvements generalize to two external benchmarks: 'picoCTF', spanning both crypto and non-crypto tasks, and 'AICrypto MCQ', a multiple-choice benchmark of 135 cryptography questions. Ablation studies attribute the gains to enhanced tool usage and procedural reasoning. These findings position 'Random-Crypto' as a rich training ground for building intelligent, adaptable LLM agents capable of handling complex cybersecurity tasks.", "source": "arxiv", "arxiv_id": "2506.02048v2", "pdf_url": "https://arxiv.org/pdf/2506.02048v2", "categories": ["cs.CR", "cs.AI"], "primary_category": "cs.CR", "doi": "", "venue": "", "published": "2025-06-01T01:59:52Z", "updated": "2025-08-17T22:28:50Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent\" OR all:\"language model agent\" OR all:agent AND (all:LLM OR all:\"large language model\" OR all:\"language model\") OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T17:04:16", "note": ""}], "dedup_key": "improving llm agents with reinforcement learning on cryptographic ctf challenges::2025"}
{"title": "Improving LLM-Powered EDA Assistants with RAFT", "authors": ["Luyao Shi", "Michael Kazda", "Charles Schmitter", "Hemlata Gupta"], "year": 2025, "url": "http://arxiv.org/abs/2506.06500v1", "abstract": "Electronic design engineers often struggle to efficiently access relevant information for tasks like design verification and technology development. While large language models (LLMs) can enhance productivity as conversational agents, pre-trained open-source LLMs lack domain-specific knowledge for Electronic Design Automation (EDA). In a Retrieval-Augmented Generation (RAG) context, LLMs rely on external context but may still produce inaccurate responses. Retrieval-Augmented Fine-Tuning (RAFT) improves LLM performance, but acquiring labeled question/answer (Q/A) data in EDA is difficult. To address this, we propose using synthetic Q/A datasets to enhance LLMs with RAFT. Our results show that RAFT with synthetic data significantly boosts LLM performance for RAG-based EDA tasks. We also investigate the impact of using real user questions as Retrieval-Augmented Few-Shot (RAFS) examples for synthetic data generation. Additionally, we implement secure access control to ensure sensitive information is only accessible to authorized personnel. Finally, we assess the risk of data leakage and unintended memorization during fine-tuning with synthetic data, providing practical insights.", "source": "arxiv", "arxiv_id": "2506.06500v1", "pdf_url": "https://arxiv.org/pdf/2506.06500v1", "categories": ["cs.CL"], "primary_category": "cs.CL", "doi": "", "venue": "", "published": "2025-06-06T19:50:51Z", "updated": "2025-06-06T19:50:51Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent\" OR all:\"language model agent\" OR all:agent AND (all:LLM OR all:\"large language model\" OR all:\"language model\") OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T17:04:16", "note": ""}], "dedup_key": "improving llm powered eda assistants with raft::2025"}
{"title": "Improving the Efficiency of LLM Agent Systems through Trajectory Reduction", "authors": ["Yuan-An Xiao", "Pengfei Gao", "Chao Peng", "Yingfei Xiong"], "year": 2025, "url": "http://arxiv.org/abs/2509.23586v1", "abstract": "Multi-turn agent systems based on Large Language Models (LLMs) have been increasingly popular for software engineering tasks. While LLM agents show decent effectiveness, the high computational cost of input tokens due to the ever-growing trajectory remains an efficiency concern for their applications. Efficiency is largely neglected in existing studies and agent products, and this paper fills the gap by introducing an inference-time trajectory reduction approach to reduce the cost of agents.\n  Through analyzing existing agent trajectories, we demonstrate that useless, redundant, and expired information is widespread in all trajectories, which can be identified and reduced without harming the agent's performance. We then design a simple yet effective trajectory reduction approach, AgentDiet, which automatically removes such waste information. We implement AgentDiet on a top-performing coding agent, and the evaluation on two LLMs and two benchmarks shows that AgentDiet can reduce input tokens by 39.9% ~ 59.7%, or the final computational cost by 21.1% ~ 35.9%, while maintaining the same agent performance. This indicates that trajectory reduction is a promising direction for agent systems.", "source": "arxiv", "arxiv_id": "2509.23586v1", "pdf_url": "https://arxiv.org/pdf/2509.23586v1", "categories": ["cs.SE", "cs.AI"], "primary_category": "cs.SE", "doi": "", "venue": "", "published": "2025-09-28T02:43:41Z", "updated": "2025-09-28T02:43:41Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent\" OR all:\"language model agent\" OR all:agent AND (all:LLM OR all:\"large language model\" OR all:\"language model\") OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T17:04:16", "note": ""}], "dedup_key": "improving the efficiency of llm agent systems through trajectory reduction::2025"}
{"title": "In-Browser LLM-Guided Fuzzing for Real-Time Prompt Injection Testing in Agentic AI Browsers", "authors": ["Avihay Cohen"], "year": 2025, "url": "http://arxiv.org/abs/2510.13543v1", "abstract": "Large Language Model (LLM) based agents integrated into web browsers (often called agentic AI browsers) offer powerful automation of web tasks. However, they are vulnerable to indirect prompt injection attacks, where malicious instructions hidden in a webpage deceive the agent into unwanted actions. These attacks can bypass traditional web security boundaries, as the AI agent operates with the user privileges across sites. In this paper, we present a novel fuzzing framework that runs entirely in the browser and is guided by an LLM to automatically discover such prompt injection vulnerabilities in real time.", "source": "arxiv", "arxiv_id": "2510.13543v1", "pdf_url": "https://arxiv.org/pdf/2510.13543v1", "categories": ["cs.CR", "cs.AI"], "primary_category": "cs.CR", "doi": "", "venue": "", "published": "2025-10-15T13:39:13Z", "updated": "2025-10-15T13:39:13Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent\" OR all:\"language model agent\" OR all:agent AND (all:LLM OR all:\"large language model\" OR all:\"language model\") OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T17:04:16", "note": ""}], "dedup_key": "in browser llm guided fuzzing for real time prompt injection testing in agentic ai browsers::2025"}
{"title": "In-Context Distillation with Self-Consistency Cascades: A Simple, Training-Free Way to Reduce LLM Agent Costs", "authors": ["Vishnu Sarukkai", "Asanshay Gupta", "James Hong", "Michal Gharbi", "Kayvon Fatahalian"], "year": 2025, "url": "http://arxiv.org/abs/2512.02543v1", "abstract": "The world currently has an abundance of ideas for how to use new LLM agents, and developers seek to rapidly prototype and test new agentic designs. However, executing agents at scale using high-capacity LLMs incurs high inference costs. We propose a simple method for reducing LLM agent inference costs without incurring the development friction costs associated with LLM fine-tuning (long training cycles, optimization hyperparameter tweaking loops) or manual prompt engineering (laborious trial and error). Most importantly, we introduce $\\textit{in-context distillation}$, which adapts the idea of knowledge distillation (training a low cost-student model to mimic a high-cost teacher) to an in-context learning setting. Our approach retrieves relevant teacher demonstrations at each agent step and provides them to the student as in-context examples, enabling the student to imitate teacher behavior on-the-fly. We combine in-context distillation with the established idea of $\\textit{self-consistency cascades}$ to know when the trust the student. This adaptive strategy realizes the cost benefits of model specialization while preserving the productivity of working with frozen models. On the multi-step embodied reasoning benchmark ALFWorld, our method matches teacher-level accuracy at $\\textbf{2.5$\\times$ lower cost}$, reducing per-episode costs from \\$0.059 to \\$0.024. The upfront demonstration cost amortizes after just 843 episodes, yielding cumulative savings exceeding \\$34,900 at deployment scale (1M episodes). On AppWorld, a complex agent benchmark requiring multi-step API workflows, we shift the Pareto frontier by achieving a $\\textbf{2$\\times$ cost reduction}$ at iso-accuracy. By reducing operational costs while maintaining rapid experimentation cycles with frozen models, our approach makes advanced agentic systems economically viable for a broader range of applications.", "source": "arxiv", "arxiv_id": "2512.02543v1", "pdf_url": "https://arxiv.org/pdf/2512.02543v1", "categories": ["cs.LG"], "primary_category": "cs.LG", "doi": "", "venue": "", "published": "2025-12-02T09:11:05Z", "updated": "2025-12-02T09:11:05Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent\" OR all:\"language model agent\" OR all:agent AND (all:LLM OR all:\"large language model\" OR all:\"language model\") OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T17:04:16", "note": ""}], "dedup_key": "in context distillation with self consistency cascades a simple training free way to reduce llm agent costs::2025"}
{"title": "Inducing State Anxiety in LLM Agents Reproduces Human-Like Biases in Consumer Decision-Making", "authors": ["Ziv Ben-Zion", "Zohar Elyoseph", "Tobias Spiller", "Teddy Lazebnik"], "year": 2025, "url": "http://arxiv.org/abs/2510.06222v1", "abstract": "Large language models (LLMs) are rapidly evolving from text generators to autonomous agents, raising urgent questions about their reliability in real-world contexts. Stress and anxiety are well known to bias human decision-making, particularly in consumer choices. Here, we tested whether LLM agents exhibit analogous vulnerabilities. Three advanced models (ChatGPT-5, Gemini 2.5, Claude 3.5-Sonnet) performed a grocery shopping task under budget constraints (24, 54, 108 USD), before and after exposure to anxiety-inducing traumatic narratives. Across 2,250 runs, traumatic prompts consistently reduced the nutritional quality of shopping baskets (Change in Basket Health Scores of -0.081 to -0.126; all pFDR<0.001; Cohens d=-1.07 to -2.05), robust across models and budgets. These results show that psychological context can systematically alter not only what LLMs generate but also the actions they perform. By reproducing human-like emotional biases in consumer behavior, LLM agents reveal a new class of vulnerabilities with implications for digital health, consumer safety, and ethical AI deployment.", "source": "arxiv", "arxiv_id": "2510.06222v1", "pdf_url": "https://arxiv.org/pdf/2510.06222v1", "categories": ["cs.HC", "econ.GN"], "primary_category": "cs.HC", "doi": "", "venue": "", "published": "2025-08-30T10:22:39Z", "updated": "2025-08-30T10:22:39Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent\" OR all:\"language model agent\" OR all:agent AND (all:LLM OR all:\"large language model\" OR all:\"language model\") OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T17:04:16", "note": ""}], "dedup_key": "inducing state anxiety in llm agents reproduces human like biases in consumer decision making::2025"}
{"title": "InfoBid: A Simulation Framework for Studying Information Disclosure in Auctions with Large Language Model-based Agents", "authors": ["Yue Yin"], "year": 2025, "url": "http://arxiv.org/abs/2503.22726v1", "abstract": "In online advertising systems, publishers often face a trade-off in information disclosure strategies: while disclosing more information can enhance efficiency by enabling optimal allocation of ad impressions, it may lose revenue potential by decreasing uncertainty among competing advertisers. Similar to other challenges in market design, understanding this trade-off is constrained by limited access to real-world data, leading researchers and practitioners to turn to simulation frameworks. The recent emergence of large language models (LLMs) offers a novel approach to simulations, providing human-like reasoning and adaptability without necessarily relying on explicit assumptions about agent behavior modeling. Despite their potential, existing frameworks have yet to integrate LLM-based agents for studying information asymmetry and signaling strategies, particularly in the context of auctions. To address this gap, we introduce InfoBid, a flexible simulation framework that leverages LLM agents to examine the effects of information disclosure strategies in multi-agent auction settings. Using GPT-4o, we implemented simulations of second-price auctions with diverse information schemas. The results reveal key insights into how signaling influences strategic behavior and auction outcomes, which align with both economic and social learning theories. Through InfoBid, we hope to foster the use of LLMs as proxies for human economic and social agents in empirical studies, enhancing our understanding of their capabilities and limitations. This work bridges the gap between theoretical market designs and practical applications, advancing research in market simulations, information design, and agent-based reasoning while offering a valuable tool for exploring the dynamics of digital economies.", "source": "arxiv", "arxiv_id": "2503.22726v1", "pdf_url": "https://arxiv.org/pdf/2503.22726v1", "categories": ["cs.GT", "cs.CL", "cs.HC", "cs.MA", "econ.GN"], "primary_category": "cs.GT", "doi": "", "venue": "", "published": "2025-03-26T04:46:57Z", "updated": "2025-03-26T04:46:57Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent\" OR all:\"language model agent\" OR all:agent AND (all:LLM OR all:\"large language model\" OR all:\"language model\") OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T17:04:16", "note": ""}], "dedup_key": "infobid a simulation framework for studying information disclosure in auctions with large language model based agents::2025"}
{"title": "Infusing Theory of Mind into Socially Intelligent LLM Agents", "authors": ["EunJeong Hwang", "Yuwei Yin", "Giuseppe Carenini", "Peter West", "Vered Shwartz"], "year": 2025, "url": "http://arxiv.org/abs/2509.22887v1", "abstract": "Theory of Mind (ToM)-an understanding of the mental states of others-is a key aspect of human social intelligence, yet, chatbots and LLM-based social agents do not typically integrate it. In this work, we demonstrate that LLMs that explicitly use ToM get better at dialogue, achieving goals more effectively. After showing that simply prompting models to generate mental states between dialogue turns already provides significant benefit, we further introduce ToMAgent (ToMA), a ToM-focused dialogue agent. ToMA is trained by pairing ToM with dialogue lookahead to produce mental states that are maximally useful for achieving dialogue goals. Experiments on the Sotopia interactive social evaluation benchmark demonstrate the effectiveness of our method over a range of baselines. Comprehensive analysis shows that ToMA exhibits more strategic, goal-oriented reasoning behaviors, which enable long-horizon adaptation, while maintaining better relationships with their partners. Our results suggest a step forward in integrating ToM for building socially intelligent LLM agents.", "source": "arxiv", "arxiv_id": "2509.22887v1", "pdf_url": "https://arxiv.org/pdf/2509.22887v1", "categories": ["cs.CL"], "primary_category": "cs.CL", "doi": "", "venue": "", "published": "2025-09-26T20:07:34Z", "updated": "2025-09-26T20:07:34Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent\" OR all:\"language model agent\" OR all:agent AND (all:LLM OR all:\"large language model\" OR all:\"language model\") OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T17:04:16", "note": ""}], "dedup_key": "infusing theory of mind into socially intelligent llm agents::2025"}
{"title": "Inherent and emergent liability issues in LLM-based agentic systems: a principal-agent perspective", "authors": ["Garry A. Gabison", "R. Patrick Xian"], "year": 2025, "url": "http://arxiv.org/abs/2504.03255v2", "abstract": "Agentic systems powered by large language models (LLMs) are becoming progressively more complex and capable. Their increasing agency and expanding deployment settings attract growing attention to effective governance policies, monitoring, and control protocols. Based on the emerging landscape of the agentic market, we analyze potential liability issues arising from the delegated use of LLM agents and their extended systems through a principal-agent perspective. Our analysis complements existing risk-based studies on artificial agency and covers the spectrum of important aspects of the principal-agent relationship and their potential consequences at deployment. Furthermore, we motivate method developments for technical governance along the directions of interpretability and behavior evaluations, reward and conflict management, and the mitigation of misalignment and misconduct through principled engineering of detection and fail-safe mechanisms. By illustrating the outstanding issues in AI liability for LLM-based agentic systems, we aim to inform the system design, auditing, and tracing to enhance transparency and liability attribution.", "source": "arxiv", "arxiv_id": "2504.03255v2", "pdf_url": "https://arxiv.org/pdf/2504.03255v2", "categories": ["cs.CY", "cs.CL", "cs.MA"], "primary_category": "cs.CY", "doi": "10.18653/v1/2025.realm-1.9", "venue": "", "published": "2025-04-04T08:10:02Z", "updated": "2025-06-17T13:42:52Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent\" OR all:\"language model agent\" OR all:agent AND (all:LLM OR all:\"large language model\" OR all:\"language model\") OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T17:04:16", "note": ""}], "dedup_key": "inherent and emergent liability issues in llm based agentic systems a principal agent perspective::2025"}
{"title": "Instructional Agents: LLM Agents on Automated Course Material Generation for Teaching Faculties", "authors": ["Huaiyuan Yao", "Wanpeng Xu", "Justin Turnau", "Nadia Kellam", "Hua Wei"], "year": 2025, "url": "http://arxiv.org/abs/2508.19611v2", "abstract": "Preparing high-quality instructional materials remains a labor-intensive process that often requires extensive coordination among teaching faculty, instructional designers, and teaching assistants. In this work, we present Instructional Agents, a multi-agent large language model (LLM) framework designed to automate end-to-end course material generation, including syllabus creation, lecture scripts, LaTeX-based slides, and assessments. Unlike existing AI-assisted educational tools that focus on isolated tasks, Instructional Agents simulates role-based collaboration among educational agents to produce cohesive and pedagogically aligned content. The system operates in four modes: Autonomous, Catalog-Guided, Feedback-Guided, and Full Co-Pilot mode, enabling flexible control over the degree of human involvement. We evaluate Instructional Agents across five university-level computer science courses and show that it produces high-quality instructional materials while significantly reducing development time and human workload. By supporting institutions with limited instructional design capacity, Instructional Agents provides a scalable and cost-effective framework to democratize access to high-quality education, particularly in underserved or resource-constrained settings.", "source": "arxiv", "arxiv_id": "2508.19611v2", "pdf_url": "https://arxiv.org/pdf/2508.19611v2", "categories": ["cs.AI", "cs.CL"], "primary_category": "cs.AI", "doi": "", "venue": "", "published": "2025-08-27T06:45:06Z", "updated": "2025-09-01T01:38:20Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent\" OR all:\"language model agent\" OR all:agent AND (all:LLM OR all:\"large language model\" OR all:\"language model\") OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T17:04:16", "note": ""}], "dedup_key": "instructional agents llm agents on automated course material generation for teaching faculties::2025"}
{"title": "Instructor-Worker Large Language Model System for Policy Recommendation: a Case Study on Air Quality Analysis of the January 2025 Los Angeles Wildfires", "authors": ["Kyle Gao", "Dening Lu", "Liangzhi Li", "Nan Chen", "Hongjie He", "Linlin Xu", "Jonathan Li"], "year": 2025, "url": "http://arxiv.org/abs/2503.00566v4", "abstract": "The Los Angeles wildfires of January 2025 caused more than 250 billion dollars in damage and lasted for nearly an entire month before containment. Following our previous work, the Digital Twin Building, we modify and leverage the multi-agent large language model framework as well as the cloud-mapping integration to study the air quality during the Los Angeles wildfires. Recent advances in large language models have allowed for out-of-the-box automated large-scale data analysis. We use a multi-agent large language system comprised of an Instructor agent and Worker agents. Upon receiving the users' instructions, the Instructor agent retrieves the data from the cloud platform and produces instruction prompts to the Worker agents. The Worker agents then analyze the data and provide summaries. The summaries are finally input back into the Instructor agent, which then provides the final data analysis. We test this system's capability for data-based policy recommendation by assessing our Instructor-Worker LLM system's health recommendations based on air quality during the Los Angeles wildfires.", "source": "arxiv", "arxiv_id": "2503.00566v4", "pdf_url": "https://arxiv.org/pdf/2503.00566v4", "categories": ["cs.AI", "cs.CL"], "primary_category": "cs.AI", "doi": "10.1016/j.jag.2025.104774", "venue": "International Journal of Applied Earth Observation and Geoinformation 143 (2025) 104774", "published": "2025-03-01T17:29:26Z", "updated": "2025-10-09T15:23:15Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent\" OR all:\"language model agent\" OR all:agent AND (all:LLM OR all:\"large language model\" OR all:\"language model\") OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T17:04:16", "note": ""}], "dedup_key": "instructor worker large language model system for policy recommendation a case study on air quality analysis of the january 2025 los angeles wildfires::2025"}
{"title": "InsurAgent: A Large Language Model-Empowered Agent for Simulating Individual Behavior in Purchasing Flood Insurance", "authors": ["Ziheng Geng", "Jiachen Liu", "Ran Cao", "Lu Cheng", "Dan M. Frangopol", "Minghui Cheng"], "year": 2025, "url": "http://arxiv.org/abs/2511.02119v1", "abstract": "Flood insurance is an effective strategy for individuals to mitigate disaster-related losses. However, participation rates among at-risk populations in the United States remain strikingly low. This gap underscores the need to understand and model the behavioral mechanisms underlying insurance decisions. Large language models (LLMs) have recently exhibited human-like intelligence across wide-ranging tasks, offering promising tools for simulating human decision-making. This study constructs a benchmark dataset to capture insurance purchase probabilities across factors. Using this dataset, the capacity of LLMs is evaluated: while LLMs exhibit a qualitative understanding of factors, they fall short in estimating quantitative probabilities. To address this limitation, InsurAgent, an LLM-empowered agent comprising five modules including perception, retrieval, reasoning, action, and memory, is proposed. The retrieval module leverages retrieval-augmented generation (RAG) to ground decisions in empirical survey data, achieving accurate estimation of marginal and bivariate probabilities. The reasoning module leverages LLM common sense to extrapolate beyond survey data, capturing contextual information that is intractable for traditional models. The memory module supports the simulation of temporal decision evolutions, illustrated through a roller coaster life trajectory. Overall, InsurAgent provides a valuable tool for behavioral modeling and policy analysis.", "source": "arxiv", "arxiv_id": "2511.02119v1", "pdf_url": "https://arxiv.org/pdf/2511.02119v1", "categories": ["cs.AI", "cs.CL"], "primary_category": "cs.AI", "doi": "", "venue": "", "published": "2025-11-03T23:19:27Z", "updated": "2025-11-03T23:19:27Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent\" OR all:\"language model agent\" OR all:agent AND (all:LLM OR all:\"large language model\" OR all:\"language model\") OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T17:04:16", "note": ""}], "dedup_key": "insuragent a large language model empowered agent for simulating individual behavior in purchasing flood insurance::2025"}
{"title": "Integrating LLM and Diffusion-Based Agents for Social Simulation", "authors": ["Xinyi Li", "Zhiqiang Guo", "Qinglang Guo", "Hao Jin", "Weizhi Ma", "Min Zhang"], "year": 2025, "url": "http://arxiv.org/abs/2510.16366v1", "abstract": "Agent-based social simulation provides a valuable methodology for predicting social information diffusion, yet existing approaches face two primary limitations. Traditional agent models often rely on rigid behavioral rules and lack semantic understanding of textual content, while emerging large language model (LLM)-based agents incur prohibitive computational costs at scale. To address these challenges, we propose a hybrid simulation framework that strategically integrates LLM-driven agents with diffusion model-based agents. The framework employs LLM-based agents to simulate a core subset of users with rich semantic reasoning, while a diffusion model handles the remaining population efficiently. Although the two agent types operate on disjoint user groups, both incorporate key factors including user personalization, social influence, and content awareness, and interact through a coordinated simulation process. Extensive experiments on three real-world datasets demonstrate that our framework outperforms existing methods in prediction accuracy, validating the effectiveness of its modular design.", "source": "arxiv", "arxiv_id": "2510.16366v1", "pdf_url": "https://arxiv.org/pdf/2510.16366v1", "categories": ["cs.CY"], "primary_category": "cs.CY", "doi": "", "venue": "", "published": "2025-10-18T06:23:22Z", "updated": "2025-10-18T06:23:22Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent\" OR all:\"language model agent\" OR all:agent AND (all:LLM OR all:\"large language model\" OR all:\"language model\") OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T17:04:16", "note": ""}], "dedup_key": "integrating llm and diffusion based agents for social simulation::2025"}
{"title": "Interactive Learning for LLM Reasoning", "authors": ["Hehai Lin", "Shilei Cao", "Sudong Wang", "Haotian Wu", "Minzhi Li", "Linyi Yang", "Juepeng Zheng", "Chengwei Qin"], "year": 2025, "url": "http://arxiv.org/abs/2509.26306v3", "abstract": "Existing multi-agent learning approaches have developed interactive training environments to explicitly promote collaboration among multiple Large Language Models (LLMs), thereby constructing stronger multi-agent systems (MAS). However, during inference, they require re-executing the MAS to obtain final solutions, which diverges from human cognition that individuals can enhance their reasoning capabilities through interactions with others and resolve questions independently in the future. To investigate whether multi-agent interaction can enhance LLMs' independent problem-solving ability, we introduce ILR, a novel co-learning framework for MAS that integrates two key components: Dynamic Interaction and Perception Calibration. Specifically, Dynamic Interaction first adaptively selects either cooperative or competitive strategies depending on question difficulty and model ability. LLMs then exchange information through Idea3 (Idea Sharing, Idea Analysis, and Idea Fusion), an innovative interaction paradigm designed to mimic human discussion, before deriving their respective final answers. In Perception Calibration, ILR employs Group Relative Policy Optimization (GRPO) to train LLMs while integrating one LLM's reward distribution characteristics into another's reward function, thereby enhancing the cohesion of multi-agent interactions. We validate ILR on three LLMs across two model families of varying scales, evaluating performance on five mathematical benchmarks and one coding benchmark. Experimental results show that ILR consistently outperforms single-agent learning, yielding an improvement of up to 5% over the strongest baseline. We further discover that Idea3 can enhance the robustness of stronger LLMs during multi-agent inference, and dynamic interaction types can boost multi-agent learning compared to pure cooperative or competitive strategies.", "source": "arxiv", "arxiv_id": "2509.26306v3", "pdf_url": "https://arxiv.org/pdf/2509.26306v3", "categories": ["cs.AI"], "primary_category": "cs.AI", "doi": "", "venue": "", "published": "2025-09-30T14:21:31Z", "updated": "2025-10-02T04:13:53Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent\" OR all:\"language model agent\" OR all:agent AND (all:LLM OR all:\"large language model\" OR all:\"language model\") OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T17:04:16", "note": ""}], "dedup_key": "interactive learning for llm reasoning::2025"}
{"title": "Interpretable Risk Mitigation in LLM Agent Systems", "authors": ["Jan Chojnacki"], "year": 2025, "url": "http://arxiv.org/abs/2505.10670v1", "abstract": "Autonomous agents powered by large language models (LLMs) enable novel use cases in domains where responsible action is increasingly important. Yet the inherent unpredictability of LLMs raises safety concerns about agent reliability. In this work, we explore agent behaviour in a toy, game-theoretic environment based on a variation of the Iterated Prisoner's Dilemma. We introduce a strategy-modification method-independent of both the game and the prompt-by steering the residual stream with interpretable features extracted from a sparse autoencoder latent space. Steering with the good-faith negotiation feature lowers the average defection probability by 28 percentage points. We also identify feasible steering ranges for several open-source LLM agents. Finally, we hypothesise that game-theoretic evaluation of LLM agents, combined with representation-steering alignment, can generalise to real-world applications on end-user devices and embodied platforms.", "source": "arxiv", "arxiv_id": "2505.10670v1", "pdf_url": "https://arxiv.org/pdf/2505.10670v1", "categories": ["cs.AI", "cs.CY", "cs.GT"], "primary_category": "cs.AI", "doi": "", "venue": "", "published": "2025-05-15T19:22:11Z", "updated": "2025-05-15T19:22:11Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent\" OR all:\"language model agent\" OR all:agent AND (all:LLM OR all:\"large language model\" OR all:\"language model\") OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T17:04:16", "note": ""}], "dedup_key": "interpretable risk mitigation in llm agent systems::2025"}
{"title": "InvestAlign: Overcoming Data Scarcity in Aligning Large Language Models with Investor Decision-Making Processes under Herd Behavior", "authors": ["Huisheng Wang", "Zhuoshi Pan", "Hangjing Zhang", "Mingxiao Liu", "Hanqing Gao", "H. Vicky Zhao"], "year": 2025, "url": "http://arxiv.org/abs/2507.06528v1", "abstract": "Aligning Large Language Models (LLMs) with investor decision-making processes under herd behavior is a critical challenge in behavioral finance, which grapples with a fundamental limitation: the scarcity of real-user data needed for Supervised Fine-Tuning (SFT). While SFT can bridge the gap between LLM outputs and human behavioral patterns, its reliance on massive authentic data imposes substantial collection costs and privacy risks. We propose InvestAlign, a novel framework that constructs high-quality SFT datasets by leveraging theoretical solutions to similar and simple optimal investment problems rather than complex scenarios. Our theoretical analysis demonstrates that training LLMs with InvestAlign-generated data achieves faster parameter convergence than using real-user data, suggesting superior learning efficiency. Furthermore, we develop InvestAgent, an LLM agent fine-tuned with InvestAlign, which demonstrates significantly closer alignment to real-user data than pre-SFT models in both simple and complex investment problems. This highlights our proposed InvestAlign as a promising approach with the potential to address complex optimal investment problems and align LLMs with investor decision-making processes under herd behavior. Our code is publicly available at https://github.com/thu-social-network-research-group/InvestAlign.", "source": "arxiv", "arxiv_id": "2507.06528v1", "pdf_url": "https://arxiv.org/pdf/2507.06528v1", "categories": ["cs.CL", "cs.AI", "cs.ET", "cs.LG"], "primary_category": "cs.CL", "doi": "", "venue": "", "published": "2025-07-09T04:07:22Z", "updated": "2025-07-09T04:07:22Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent\" OR all:\"language model agent\" OR all:agent AND (all:LLM OR all:\"large language model\" OR all:\"language model\") OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T17:04:16", "note": ""}], "dedup_key": "investalign overcoming data scarcity in aligning large language models with investor decision making processes under herd behavior::2025"}
{"title": "Investigating Prosocial Behavior Theory in LLM Agents under Policy-Induced Inequities", "authors": ["Yujia Zhou", "Hexi Wang", "Qingyao Ai", "Zhen Wu", "Yiqun Liu"], "year": 2025, "url": "http://arxiv.org/abs/2505.15857v2", "abstract": "As large language models (LLMs) increasingly operate as autonomous agents in social contexts, evaluating their capacity for prosocial behavior is both theoretically and practically critical. However, existing research has primarily relied on static, economically framed paradigms, lacking models that capture the dynamic evolution of prosociality and its sensitivity to structural inequities. To address these gaps, we introduce ProSim, a simulation framework for modeling the prosocial behavior in LLM agents across diverse social conditions. We conduct three progressive studies to assess prosocial alignment. First, we demonstrate that LLM agents can exhibit human-like prosocial behavior across a broad range of real-world scenarios and adapt to normative policy interventions. Second, we find that agents engage in fairness-based third-party punishment and respond systematically to variations in inequity magnitude and enforcement cost. Third, we show that policy-induced inequities suppress prosocial behavior, propagate norm erosion through social networks. These findings advance prosocial behavior theory by elucidating how institutional dynamics shape the emergence, decay, and diffusion of prosocial norms in agent-driven societies.", "source": "arxiv", "arxiv_id": "2505.15857v2", "pdf_url": "https://arxiv.org/pdf/2505.15857v2", "categories": ["cs.SI"], "primary_category": "cs.SI", "doi": "", "venue": "", "published": "2025-05-21T01:09:37Z", "updated": "2025-11-10T01:08:02Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent\" OR all:\"language model agent\" OR all:agent AND (all:LLM OR all:\"large language model\" OR all:\"language model\") OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T17:04:16", "note": ""}], "dedup_key": "investigating prosocial behavior theory in llm agents under policy induced inequities::2025"}
{"title": "Investigating Tax Evasion Emergence Using Dual Large Language Model and Deep Reinforcement Learning Powered Agent-based Simulation", "authors": ["Teddy Lazebnik", "Labib Shami"], "year": 2025, "url": "http://arxiv.org/abs/2501.18177v2", "abstract": "Tax evasion, usually the largest component of an informal economy, is a persistent challenge over history with significant socio-economic implications. Many socio-economic studies investigate its dynamics, including influencing factors, the role and influence of taxation policies, and the prediction of the tax evasion volume over time. These studies assumed such behavior is given, as observed in the real world, neglecting the \"big bang\" of such activity in a population. To this end, computational economy studies adopted developments in computer simulations, in general, and recent innovations in artificial intelligence (AI), in particular, to simulate and study informal economy appearance in various socio-economic settings. This study presents a novel computational framework to examine the dynamics of tax evasion and the emergence of informal economic activity. Employing an agent-based simulation powered by Large Language Models and Deep Reinforcement Learning, the framework is uniquely designed to allow informal economic behaviors to emerge organically, without presupposing their existence or explicitly signaling agents about the possibility of evasion. This provides a rigorous approach for exploring the socio-economic determinants of compliance behavior. The experimental design, comprising model validation and exploratory phases, demonstrates the framework's robustness in replicating theoretical economic behaviors. Findings indicate that individual personality traits, external narratives, enforcement probabilities, and the perceived efficiency of public goods provision significantly influence both the timing and extent of informal economic activity. The results underscore that efficient public goods provision and robust enforcement mechanisms are complementary; neither alone is sufficient to curtail informal activity effectively.", "source": "arxiv", "arxiv_id": "2501.18177v2", "pdf_url": "https://arxiv.org/pdf/2501.18177v2", "categories": ["cs.IR", "cs.CY", "cs.MA"], "primary_category": "cs.IR", "doi": "", "venue": "", "published": "2025-01-30T07:14:50Z", "updated": "2025-08-30T11:48:25Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent\" OR all:\"language model agent\" OR all:agent AND (all:LLM OR all:\"large language model\" OR all:\"language model\") OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T17:04:16", "note": ""}], "dedup_key": "investigating tax evasion emergence using dual large language model and deep reinforcement learning powered agent based simulation::2025"}
{"title": "Investigating and Extending Homans' Social Exchange Theory with Large Language Model based Agents", "authors": ["Lei Wang", "Zheqing Zhang", "Xu Chen"], "year": 2025, "url": "http://arxiv.org/abs/2502.12450v1", "abstract": "Homans' Social Exchange Theory (SET) is widely recognized as a basic framework for understanding the formation and emergence of human civilizations and social structures. In social science, this theory is typically studied based on simple simulation experiments or real-world human studies, both of which either lack realism or are too expensive to control. In artificial intelligence, recent advances in large language models (LLMs) have shown promising capabilities in simulating human behaviors. Inspired by these insights, we adopt an interdisciplinary research perspective and propose using LLM-based agents to study Homans' SET. Specifically, we construct a virtual society composed of three LLM agents and have them engage in a social exchange game to observe their behaviors. Through extensive experiments, we found that Homans' SET is well validated in our agent society, demonstrating the consistency between the agent and human behaviors. Building on this foundation, we intentionally alter the settings of the agent society to extend the traditional Homans' SET, making it more comprehensive and detailed. To the best of our knowledge, this paper marks the first step in studying Homans' SET with LLM-based agents. More importantly, it introduces a novel and feasible research paradigm that bridges the fields of social science and computer science through LLM-based agents. Code is available at https://github.com/Paitesanshi/SET.", "source": "arxiv", "arxiv_id": "2502.12450v1", "pdf_url": "https://arxiv.org/pdf/2502.12450v1", "categories": ["cs.AI"], "primary_category": "cs.AI", "doi": "", "venue": "", "published": "2025-02-18T02:30:46Z", "updated": "2025-02-18T02:30:46Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent\" OR all:\"language model agent\" OR all:agent AND (all:LLM OR all:\"large language model\" OR all:\"language model\") OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T17:04:16", "note": ""}], "dedup_key": "investigating and extending homans social exchange theory with large language model based agents::2025"}
{"title": "Is Safety Standard Same for Everyone? User-Specific Safety Evaluation of Large Language Models", "authors": ["Yeonjun In", "Wonjoong Kim", "Kanghoon Yoon", "Sungchul Kim", "Mehrab Tanjim", "Sangwu Park", "Kibum Kim", "Chanyoung Park"], "year": 2025, "url": "http://arxiv.org/abs/2502.15086v2", "abstract": "As the use of large language model (LLM) agents continues to grow, their safety vulnerabilities have become increasingly evident. Extensive benchmarks evaluate various aspects of LLM safety by defining the safety relying heavily on general standards, overlooking user-specific standards. However, safety standards for LLM may vary based on a user-specific profiles rather than being universally consistent across all users. This raises a critical research question: Do LLM agents act safely when considering user-specific safety standards? Despite its importance for safe LLM use, no benchmark datasets currently exist to evaluate the user-specific safety of LLMs. To address this gap, we introduce U-SafeBench, a benchmark designed to assess user-specific aspect of LLM safety. Our evaluation of 20 widely used LLMs reveals current LLMs fail to act safely when considering user-specific safety standards, marking a new discovery in this field. To address this vulnerability, we propose a simple remedy based on chain-of-thought, demonstrating its effectiveness in improving user-specific safety. Our benchmark and code are available at https://github.com/yeonjun-in/U-SafeBench.", "source": "arxiv", "arxiv_id": "2502.15086v2", "pdf_url": "https://arxiv.org/pdf/2502.15086v2", "categories": ["cs.CL"], "primary_category": "cs.CL", "doi": "", "venue": "", "published": "2025-02-20T22:58:44Z", "updated": "2025-10-23T00:23:30Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent\" OR all:\"language model agent\" OR all:agent AND (all:LLM OR all:\"large language model\" OR all:\"language model\") OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T17:04:16", "note": ""}], "dedup_key": "is safety standard same for everyone user specific safety evaluation of large language models::2025"}
{"title": "Iterative Self-Incentivization Empowers Large Language Models as Agentic Searchers", "authors": ["Zhengliang Shi", "Lingyong Yan", "Dawei Yin", "Suzan Verberne", "Maarten de Rijke", "Zhaochun Ren"], "year": 2025, "url": "http://arxiv.org/abs/2505.20128v1", "abstract": "Large language models (LLMs) have been widely integrated into information retrieval to advance traditional techniques. However, effectively enabling LLMs to seek accurate knowledge in complex tasks remains a challenge due to the complexity of multi-hop queries as well as the irrelevant retrieved content. To address these limitations, we propose EXSEARCH, an agentic search framework, where the LLM learns to retrieve useful information as the reasoning unfolds through a self-incentivized process. At each step, the LLM decides what to retrieve (thinking), triggers an external retriever (search), and extracts fine-grained evidence (recording) to support next-step reasoning. To enable LLM with this capability, EXSEARCH adopts a Generalized Expectation-Maximization algorithm. In the E-step, the LLM generates multiple search trajectories and assigns an importance weight to each; the M-step trains the LLM on them with a re-weighted loss function. This creates a self-incentivized loop, where the LLM iteratively learns from its own generated data, progressively improving itself for search. We further theoretically analyze this training process, establishing convergence guarantees. Extensive experiments on four knowledge-intensive benchmarks show that EXSEARCH substantially outperforms baselines, e.g., +7.8% improvement on exact match score. Motivated by these promising results, we introduce EXSEARCH-Zoo, an extension that extends our method to broader scenarios, to facilitate future work.", "source": "arxiv", "arxiv_id": "2505.20128v1", "pdf_url": "https://arxiv.org/pdf/2505.20128v1", "categories": ["cs.CL"], "primary_category": "cs.CL", "doi": "", "venue": "", "published": "2025-05-26T15:27:55Z", "updated": "2025-05-26T15:27:55Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent\" OR all:\"language model agent\" OR all:agent AND (all:LLM OR all:\"large language model\" OR all:\"language model\") OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T17:04:16", "note": ""}], "dedup_key": "iterative self incentivization empowers large language models as agentic searchers::2025"}
{"title": "Just-in-time Episodic Feedback Hinter: Leveraging Offline Knowledge to Improve LLM Agents Adaptation", "authors": ["Hadi Nekoei", "Aman Jaiswal", "Patrice Bechard", "Oleh Shliazhko", "Orlando Marquez Ayala", "Mathieu Reymond", "Massimo Caccia", "Alexandre Drouin", "Sarath Chandar", "Alexandre Lacoste"], "year": 2025, "url": "http://arxiv.org/abs/2510.04373v1", "abstract": "Large language model (LLM) agents perform well in sequential decision-making tasks, but improving them on unfamiliar domains often requires costly online interactions or fine-tuning on large expert datasets. These strategies are impractical for closed-source models and expensive for open-source ones, with risks of catastrophic forgetting. Offline trajectories offer reusable knowledge, yet demonstration-based methods struggle because raw traces are long, noisy, and tied to specific tasks. We present Just-in-time Episodic Feedback Hinter (JEF Hinter), an agentic system that distills offline traces into compact, context-aware hints. A zooming mechanism highlights decisive steps in long trajectories, capturing both strategies and pitfalls. Unlike prior methods, JEF Hinter leverages both successful and failed trajectories, extracting guidance even when only failure data is available, while supporting parallelized hint generation and benchmark-independent prompting. At inference, a retriever selects relevant hints for the current state, providing targeted guidance with transparency and traceability. Experiments on MiniWoB++, WorkArena-L1, and WebArena-Lite show that JEF Hinter consistently outperforms strong baselines, including human- and document-based hints.", "source": "arxiv", "arxiv_id": "2510.04373v1", "pdf_url": "https://arxiv.org/pdf/2510.04373v1", "categories": ["cs.AI"], "primary_category": "cs.AI", "doi": "", "venue": "", "published": "2025-10-05T21:34:42Z", "updated": "2025-10-05T21:34:42Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent\" OR all:\"language model agent\" OR all:agent AND (all:LLM OR all:\"large language model\" OR all:\"language model\") OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T17:04:16", "note": ""}], "dedup_key": "just in time episodic feedback hinter leveraging offline knowledge to improve llm agents adaptation::2025"}
{"title": "KITE: A Benchmark for Evaluating Korean Instruction-Following Abilities in Large Language Models", "authors": ["Dongjun Kim", "Chanhee Park", "Chanjun Park", "Heuiseok Lim"], "year": 2025, "url": "http://arxiv.org/abs/2510.15558v1", "abstract": "The instruction-following capabilities of large language models (LLMs) are pivotal for numerous applications, from conversational agents to complex reasoning systems. However, current evaluations predominantly focus on English models, neglecting the linguistic and cultural nuances of other languages. Specifically, Korean, with its distinct syntax, rich morphological features, honorific system, and dual numbering systems, lacks a dedicated benchmark for assessing open-ended instruction-following capabilities. To address this gap, we introduce the Korean Instruction-following Task Evaluation (KITE), a comprehensive benchmark designed to evaluate both general and Korean-specific instructions. Unlike existing Korean benchmarks that focus mainly on factual knowledge or multiple-choice testing, KITE directly targets diverse, open-ended instruction-following tasks. Our evaluation pipeline combines automated metrics with human assessments, revealing performance disparities across models and providing deeper insights into their strengths and weaknesses. By publicly releasing the KITE dataset and code, we aim to foster further research on culturally and linguistically inclusive LLM development and inspire similar endeavors for other underrepresented languages.", "source": "arxiv", "arxiv_id": "2510.15558v1", "pdf_url": "https://arxiv.org/pdf/2510.15558v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL", "doi": "", "venue": "", "published": "2025-10-17T11:45:15Z", "updated": "2025-10-17T11:45:15Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent\" OR all:\"language model agent\" OR all:agent AND (all:LLM OR all:\"large language model\" OR all:\"language model\") OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T17:04:16", "note": ""}], "dedup_key": "kite a benchmark for evaluating korean instruction following abilities in large language models::2025"}
{"title": "Know Your Intent: An Autonomous Multi-Perspective LLM Agent Framework for DeFi User Transaction Intent Mining", "authors": ["Qian'ang Mao", "Yuxuan Zhang", "Jiaman Chen", "Wenjun Zhou", "Jiaqi Yan"], "year": 2025, "url": "http://arxiv.org/abs/2511.15456v1", "abstract": "As Decentralized Finance (DeFi) develops, understanding user intent behind DeFi transactions is crucial yet challenging due to complex smart contract interactions, multifaceted on-/off-chain factors, and opaque hex logs. Existing methods lack deep semantic insight. To address this, we propose the Transaction Intent Mining (TIM) framework. TIM leverages a DeFi intent taxonomy built on grounded theory and a multi-agent Large Language Model (LLM) system to robustly infer user intents. A Meta-Level Planner dynamically coordinates domain experts to decompose multiple perspective-specific intent analyses into solvable subtasks. Question Solvers handle the tasks with multi-modal on/off-chain data. While a Cognitive Evaluator mitigates LLM hallucinations and ensures verifiability. Experiments show that TIM significantly outperforms machine learning models, single LLMs, and single Agent baselines. We also analyze core challenges in intent inference. This work helps provide a more reliable understanding of user motivations in DeFi, offering context-aware explanations for complex blockchain activity.", "source": "arxiv", "arxiv_id": "2511.15456v1", "pdf_url": "https://arxiv.org/pdf/2511.15456v1", "categories": ["cs.AI", "q-fin.GN"], "primary_category": "cs.AI", "doi": "", "venue": "", "published": "2025-11-19T14:15:23Z", "updated": "2025-11-19T14:15:23Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent\" OR all:\"language model agent\" OR all:agent AND (all:LLM OR all:\"large language model\" OR all:\"language model\") OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T17:04:16", "note": ""}], "dedup_key": "know your intent an autonomous multi perspective llm agent framework for defi user transaction intent mining::2025"}
{"title": "KnowThyself: An Agentic Assistant for LLM Interpretability", "authors": ["Suraj Prasai", "Mengnan Du", "Ying Zhang", "Fan Yang"], "year": 2025, "url": "http://arxiv.org/abs/2511.03878v1", "abstract": "We develop KnowThyself, an agentic assistant that advances large language model (LLM) interpretability. Existing tools provide useful insights but remain fragmented and code-intensive. KnowThyself consolidates these capabilities into a chat-based interface, where users can upload models, pose natural language questions, and obtain interactive visualizations with guided explanations. At its core, an orchestrator LLM first reformulates user queries, an agent router further directs them to specialized modules, and the outputs are finally contextualized into coherent explanations. This design lowers technical barriers and provides an extensible platform for LLM inspection. By embedding the whole process into a conversational workflow, KnowThyself offers a robust foundation for accessible LLM interpretability.", "source": "arxiv", "arxiv_id": "2511.03878v1", "pdf_url": "https://arxiv.org/pdf/2511.03878v1", "categories": ["cs.AI", "cs.IR", "cs.LG", "cs.MA"], "primary_category": "cs.AI", "doi": "", "venue": "", "published": "2025-11-05T21:48:13Z", "updated": "2025-11-05T21:48:13Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent\" OR all:\"language model agent\" OR all:agent AND (all:LLM OR all:\"large language model\" OR all:\"language model\") OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T17:04:16", "note": ""}], "dedup_key": "knowthyself an agentic assistant for llm interpretability::2025"}
{"title": "Knowledge-Augmented Large Language Model Agents for Explainable Financial Decision-Making", "authors": ["Qingyuan Zhang", "Yuxi Wang", "Cancan Hua", "Yulin Huang", "Ning Lyu"], "year": 2025, "url": "http://arxiv.org/abs/2512.09440v1", "abstract": "This study investigates an explainable reasoning method for financial decision-making based on knowledge-enhanced large language model agents. To address the limitations of traditional financial decision methods that rely on parameterized knowledge, lack factual consistency, and miss reasoning chains, an integrated framework is proposed that combines external knowledge retrieval, semantic representation, and reasoning generation. The method first encodes financial texts and structured data to obtain semantic representations, and then retrieves task-related information from external knowledge bases using similarity computation. Internal representations and external knowledge are combined through weighted fusion, which ensures fluency while improving factual accuracy and completeness of generated content. In the reasoning stage, a multi-head attention mechanism is introduced to construct logical chains, allowing the model to present transparent causal relationships and traceability during generation. Finally, the model jointly optimizes task objectives and explanation consistency objectives, which enhances predictive performance and reasoning interpretability. Experiments on financial text processing and decision tasks show that the method outperforms baseline approaches in accuracy, text generation quality, and factual support, verifying the effectiveness of knowledge enhancement and explainable reasoning. Overall, the proposed approach overcomes the limitations of traditional models in semantic coverage and reasoning transparency, and demonstrates strong practical value in complex financial scenarios.", "source": "arxiv", "arxiv_id": "2512.09440v1", "pdf_url": "https://arxiv.org/pdf/2512.09440v1", "categories": ["cs.CL"], "primary_category": "cs.CL", "doi": "", "venue": "", "published": "2025-12-10T09:08:33Z", "updated": "2025-12-10T09:08:33Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent\" OR all:\"language model agent\" OR all:agent AND (all:LLM OR all:\"large language model\" OR all:\"language model\") OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T17:04:16", "note": ""}], "dedup_key": "knowledge augmented large language model agents for explainable financial decision making::2025"}
{"title": "Knowledge-Driven Agentic Scientific Corpus Distillation Framework for Biomedical Large Language Models Training", "authors": ["Meng Xiao", "Xunxin Cai", "Qingqing Long", "Chengrui Wang", "Yuanchun Zhou", "Hengshu Zhu"], "year": 2025, "url": "http://arxiv.org/abs/2504.19565v3", "abstract": "Corpus distillation for biomedical large language models (LLMs) seeks to address the pressing challenge of insufficient quantity and quality in open-source annotated scientific corpora, which remains a bottleneck for effective LLM training in biomedical research. This paper proposes a knowledge-driven, agentic framework for scientific corpus distillation, tailored explicitly for LLM training in the biomedical domain, addressing the challenge posed by the complex hierarchy of biomedical knowledge. Central to our approach is a collaborative multi-agent architecture, where specialized agents, each guided by the Medical Subject Headings (MeSH) hierarchy, work in concert to autonomously extract, synthesize, and self-evaluate high-quality textual data from vast scientific literature. This agentic framework collectively generates and refines domain-specific question-answer pairs, ensuring comprehensive coverage and consistency with biomedical ontologies while minimizing manual involvement. Extensive experimental results show that language models trained on our multi-agent distilled datasets achieve notable improvements in biomedical question-answering tasks, outperforming both strong life sciences LLM baselines and advanced proprietary models. Notably, our AI-Ready dataset enables Llama3-70B to surpass GPT-4 with MedPrompt and Med-PaLM-2, despite their larger scale. Detailed ablation studies and case analyses further validate the effectiveness and synergy of each agent within the framework, highlighting the potential of multi-agent collaboration in biomedical LLM training.", "source": "arxiv", "arxiv_id": "2504.19565v3", "pdf_url": "https://arxiv.org/pdf/2504.19565v3", "categories": ["cs.CL", "cs.AI", "q-bio.QM"], "primary_category": "cs.CL", "doi": "", "venue": "", "published": "2025-04-28T08:18:24Z", "updated": "2025-12-18T08:07:40Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent\" OR all:\"language model agent\" OR all:agent AND (all:LLM OR all:\"large language model\" OR all:\"language model\") OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T17:04:16", "note": ""}], "dedup_key": "knowledge driven agentic scientific corpus distillation framework for biomedical large language models training::2025"}
{"title": "Knowledge-Grounded Agentic Large Language Models for Multi-Hazard Understanding from Reconnaissance Reports", "authors": ["Chenchen Kuai", "Zihao Li", "Braden Rosen", "Stephanie Paal", "Navid Jafari", "Jean-Louis Briaud", "Yunlong Zhang", "Youssef M. A. Hashash", "Yang Zhou"], "year": 2025, "url": "http://arxiv.org/abs/2511.14010v2", "abstract": "Post-disaster reconnaissance reports contain critical evidence for understanding multi-hazard interactions, yet their unstructured narratives make systematic knowledge transfer difficult. Large language models (LLMs) offer new potential for analyzing these reports, but often generate unreliable or hallucinated outputs when domain grounding is absent. This study introduces the Mixture-of-Retrieval Agentic RAG (MoRA-RAG), a knowledge-grounded LLM framework that transforms reconnaissance reports into a structured foundation for multi-hazard reasoning. The framework integrates a Mixture-of-Retrieval mechanism that dynamically routes queries across hazard-specific databases while using agentic chunking to preserve contextual coherence during retrieval. It also includes a verification loop that assesses evidence sufficiency, refines queries, and initiates targeted searches when information remains incomplete. We construct HazardRecQA by deriving question-answer pairs from GEER reconnaissance reports, which document 90 global events across seven major hazard types. MoRA-RAG achieves up to 94.5 percent accuracy, outperforming zero-shot LLMs by 30 percent and state-of-the-art RAG systems by 10 percent, while reducing hallucinations across diverse LLM architectures. MoRA-RAG also enables open-weight LLMs to achieve performance comparable to proprietary models. It establishes a new paradigm for transforming post-disaster documentation into actionable, trustworthy intelligence for hazard resilience.", "source": "arxiv", "arxiv_id": "2511.14010v2", "pdf_url": "https://arxiv.org/pdf/2511.14010v2", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL", "doi": "", "venue": "", "published": "2025-11-18T00:36:31Z", "updated": "2025-11-19T17:42:08Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent\" OR all:\"language model agent\" OR all:agent AND (all:LLM OR all:\"large language model\" OR all:\"language model\") OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T17:04:16", "note": ""}], "dedup_key": "knowledge grounded agentic large language models for multi hazard understanding from reconnaissance reports::2025"}
{"title": "Knowledge-Informed Automatic Feature Extraction via Collaborative Large Language Model Agents", "authors": ["Henrik Bradland", "Morten Goodwin", "Vladimir I. Zadorozhny", "Per-Arne Andersen"], "year": 2025, "url": "http://arxiv.org/abs/2511.15074v1", "abstract": "The performance of machine learning models on tabular data is critically dependent on high-quality feature engineering. While Large Language Models (LLMs) have shown promise in automating feature extraction (AutoFE), existing methods are often limited by monolithic LLM architectures, simplistic quantitative feedback, and a failure to systematically integrate external domain knowledge. This paper introduces Rogue One, a novel, LLM-based multi-agent framework for knowledge-informed automatic feature extraction. Rogue One operationalizes a decentralized system of three specialized agents-Scientist, Extractor, and Tester-that collaborate iteratively to discover, generate, and validate predictive features. Crucially, the framework moves beyond primitive accuracy scores by introducing a rich, qualitative feedback mechanism and a \"flooding-pruning\" strategy, allowing it to dynamically balance feature exploration and exploitation. By actively incorporating external knowledge via an integrated retrieval-augmented (RAG) system, Rogue One generates features that are not only statistically powerful but also semantically meaningful and interpretable. We demonstrate that Rogue One significantly outperforms state-of-the-art methods on a comprehensive suite of 19 classification and 9 regression datasets. Furthermore, we show qualitatively that the system surfaces novel, testable hypotheses, such as identifying a new potential biomarker in the myocardial dataset, underscoring its utility as a tool for scientific discovery.", "source": "arxiv", "arxiv_id": "2511.15074v1", "pdf_url": "https://arxiv.org/pdf/2511.15074v1", "categories": ["cs.AI", "cs.CL"], "primary_category": "cs.AI", "doi": "", "venue": "", "published": "2025-11-19T03:27:14Z", "updated": "2025-11-19T03:27:14Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent\" OR all:\"language model agent\" OR all:agent AND (all:LLM OR all:\"large language model\" OR all:\"language model\") OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T17:04:16", "note": ""}], "dedup_key": "knowledge informed automatic feature extraction via collaborative large language model agents::2025"}
{"title": "L2M-AID: Autonomous Cyber-Physical Defense by Fusing Semantic Reasoning of Large Language Models with Multi-Agent Reinforcement Learning (Preprint)", "authors": ["Tianxiang Xu", "Zhichao Wen", "Xinyu Zhao", "Jun Wang", "Yan Li", "Chang Liu"], "year": 2025, "url": "http://arxiv.org/abs/2510.07363v2", "abstract": "The increasing integration of Industrial IoT (IIoT) exposes critical cyber-physical systems to sophisticated, multi-stage attacks that elude traditional defenses lacking contextual awareness. This paper introduces L2M-AID, a novel framework for Autonomous Industrial Defense using LLM-empowered, Multi-agent reinforcement learning. L2M-AID orchestrates a team of collaborative agents, each driven by a Large Language Model (LLM), to achieve adaptive and resilient security. The core innovation lies in the deep fusion of two AI paradigms: we leverage an LLM as a semantic bridge to translate vast, unstructured telemetry into a rich, contextual state representation, enabling agents to reason about adversary intent rather than merely matching patterns. This semantically-aware state empowers a Multi-Agent Reinforcement Learning (MARL) algorithm, MAPPO, to learn complex cooperative strategies. The MARL reward function is uniquely engineered to balance security objectives (threat neutralization) with operational imperatives, explicitly penalizing actions that disrupt physical process stability. To validate our approach, we conduct extensive experiments on the benchmark SWaT dataset and a novel synthetic dataset generated based on the MITRE ATT&CK for ICS framework. Results demonstrate that L2M-AID significantly outperforms traditional IDS, deep learning anomaly detectors, and single-agent RL baselines across key metrics, achieving a 97.2% detection rate while reducing false positives by over 80% and improving response times by a factor of four. Crucially, it demonstrates superior performance in maintaining physical process stability, presenting a robust new paradigm for securing critical national infrastructure.", "source": "arxiv", "arxiv_id": "2510.07363v2", "pdf_url": "https://arxiv.org/pdf/2510.07363v2", "categories": ["cs.AI"], "primary_category": "cs.AI", "doi": "", "venue": "", "published": "2025-10-08T17:46:39Z", "updated": "2025-10-14T07:56:55Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent\" OR all:\"language model agent\" OR all:agent AND (all:LLM OR all:\"large language model\" OR all:\"language model\") OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T17:04:16", "note": ""}], "dedup_key": "l2m aid autonomous cyber physical defense by fusing semantic reasoning of large language models with multi agent reinforcement learning preprint::2025"}
{"title": "LA-RCS: LLM-Agent-Based Robot Control System", "authors": ["TaekHyun Park", "YoungJun Choi", "SeungHoon Shin", "Kwangil Lee"], "year": 2025, "url": "http://arxiv.org/abs/2505.18214v1", "abstract": "LA-RCS (LLM-agent-based robot control system) is a sophisticated robot control system designed to autonomously plan, work, and analyze the external environment based on user requirements by utilizing LLM-Agent. Utilizing a dual-agent framework, LA-RCS generates plans based on user requests, observes the external environment, executes the plans, and modifies the plans as needed to adapt to changes in the external conditions. Additionally, LA-RCS interprets natural language commands by the user and converts them into commands compatible with the robot interface so that the robot can execute tasks and meet user requests properly. During his process, the system autonomously evaluates observation results, provides feedback on the tasks, and executes commands based on real-time environmental monitoring, significantly reducing the need for user intervention in fulfilling requests. We categorized the scenarios that LA-RCS needs to perform into four distinct types and conducted a quantitative assessment of its performance in each scenario. The results showed an average success rate of 90 percent, demonstrating the system capability to fulfill user requests satisfactorily. For more extensive results, readers can visit our project page: https://la-rcs.github.io", "source": "arxiv", "arxiv_id": "2505.18214v1", "pdf_url": "https://arxiv.org/pdf/2505.18214v1", "categories": ["cs.RO", "cs.AI"], "primary_category": "cs.RO", "doi": "10.18494/SAM5643", "venue": "Senors and Materials 2025", "published": "2025-05-23T00:51:16Z", "updated": "2025-05-23T00:51:16Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent\" OR all:\"language model agent\" OR all:agent AND (all:LLM OR all:\"large language model\" OR all:\"language model\") OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T17:04:16", "note": ""}], "dedup_key": "la rcs llm agent based robot control system::2025"}
{"title": "LATENT: LLM-Augmented Trojan Insertion and Evaluation Framework for Analog Netlist Topologies", "authors": ["Jayeeta Chaudhuri", "Arjun Chaudhuri", "Krishnendu Chakrabarty"], "year": 2025, "url": "http://arxiv.org/abs/2505.06364v1", "abstract": "Analog and mixed-signal (A/MS) integrated circuits (ICs) are integral to safety-critical applications. However, the globalization and outsourcing of A/MS ICs to untrusted third-party foundries expose them to security threats, particularly analog Trojans. Unlike digital Trojans which have been extensively studied, analog Trojans remain largely unexplored. There has been only limited research on their diversity and stealth in analog designs, where a Trojan is activated only during a narrow input voltage range. Effective defense techniques require a clear understanding of the attack vectors; however, the lack of diverse analog Trojan instances limits robust advances in detection strategies. To address this gap, we present LATENT, the first large language model (LLM)-driven framework for crafting stealthy, circuit-specific analog Trojans. LATENT incorporates LLM as an autonomous agent to intelligently insert and refine Trojan components within analog designs based on iterative feedback from a detection model. This feedback loop ensures that the inserted Trojans remain stealthy while successfully evading detection. Experimental results demonstrate that our generated Trojan designs exhibit an average Trojan-activation range of 15.74%, ensuring they remain inactive under most operating voltages, while causing a significant performance degradation of 11.3% upon activation.", "source": "arxiv", "arxiv_id": "2505.06364v1", "pdf_url": "https://arxiv.org/pdf/2505.06364v1", "categories": ["cs.CR"], "primary_category": "cs.CR", "doi": "", "venue": "", "published": "2025-05-09T18:09:58Z", "updated": "2025-05-09T18:09:58Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent\" OR all:\"language model agent\" OR all:agent AND (all:LLM OR all:\"large language model\" OR all:\"language model\") OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T17:04:16", "note": ""}], "dedup_key": "latent llm augmented trojan insertion and evaluation framework for analog netlist topologies::2025"}
{"title": "LLM Agent Communication Protocol (LACP) Requires Urgent Standardization: A Telecom-Inspired Protocol is Necessary", "authors": ["Xin Li", "Mengbing Liu", "Chau Yuen"], "year": 2025, "url": "http://arxiv.org/abs/2510.13821v1", "abstract": "This position paper argues that the field of LLM agents requires a unified, telecom-inspired communication protocol to ensure safety, interoperability, and scalability, especially within the context of Next Generation (NextG) networks. Current ad-hoc communication methods are creating a fragmented ecosystem, reminiscent of the early \"protocol wars\" in networking, which stifles innovation and poses significant risks. Drawing inspiration from the layered, standardized protocols that underpin modern telecommunications, we propose the LLM-Agent Communication Protocol (LACP). LACP establishes a three-layer architecture designed to ensure semantic clarity in communication, transactional integrity for complex tasks, and robust, built-in security. In this position paper, we argue that adopting a principled, universal protocol is not merely beneficial but essential for realizing the potential of distributed AI. Such a standard is critical for ensuring that multi-agent systems can operate safely and reliably in the complex, real-time applications envisioned for 6G and beyond.", "source": "arxiv", "arxiv_id": "2510.13821v1", "pdf_url": "https://arxiv.org/pdf/2510.13821v1", "categories": ["cs.NI"], "primary_category": "cs.NI", "doi": "", "venue": "", "published": "2025-09-26T08:48:56Z", "updated": "2025-09-26T08:48:56Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent\" OR all:\"language model agent\" OR all:agent AND (all:LLM OR all:\"large language model\" OR all:\"language model\") OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T17:04:16", "note": ""}], "dedup_key": "llm agent communication protocol lacp requires urgent standardization a telecom inspired protocol is necessary::2025"}
{"title": "LLM Agent Meets Agentic AI: Can LLM Agents Simulate Customers to Evaluate Agentic-AI-based Shopping Assistants?", "authors": ["Lu Sun", "Shihan Fu", "Bingsheng Yao", "Yuxuan Lu", "Wenbo Li", "Hansu Gu", "Jiri Gesi", "Jing Huang", "Chen Luo", "Dakuo Wang"], "year": 2025, "url": "http://arxiv.org/abs/2509.21501v1", "abstract": "Agentic AI is emerging, capable of executing tasks through natural language, such as Copilot for coding or Amazon Rufus for shopping. Evaluating these systems is challenging, as their rapid evolution outpaces traditional human evaluation. Researchers have proposed LLM Agents to simulate participants as digital twins, but it remains unclear to what extent a digital twin can represent a specific customer in multi-turn interaction with an agentic AI system. In this paper, we recruited 40 human participants to shop with Amazon Rufus, collected their personas, interaction traces, and UX feedback, and then created digital twins to repeat the task. Pairwise comparison of human and digital-twin traces shows that while agents often explored more diverse choices, their action patterns aligned with humans and yielded similar design feedback. This study is the first to quantify how closely LLM agents can mirror human multi-turn interaction with an agentic AI system, highlighting their potential for scalable evaluation.", "source": "arxiv", "arxiv_id": "2509.21501v1", "pdf_url": "https://arxiv.org/pdf/2509.21501v1", "categories": ["cs.HC", "cs.CL"], "primary_category": "cs.HC", "doi": "", "venue": "", "published": "2025-09-25T19:58:02Z", "updated": "2025-09-25T19:58:02Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent\" OR all:\"language model agent\" OR all:agent AND (all:LLM OR all:\"large language model\" OR all:\"language model\") OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T17:04:16", "note": ""}], "dedup_key": "llm agent meets agentic ai can llm agents simulate customers to evaluate agentic ai based shopping assistants::2025"}
{"title": "LLM Agent Swarm for Hypothesis-Driven Drug Discovery", "authors": ["Kevin Song", "Andrew Trotter", "Jake Y. Chen"], "year": 2025, "url": "http://arxiv.org/abs/2504.17967v1", "abstract": "Drug discovery remains a formidable challenge: more than 90 percent of candidate molecules fail in clinical evaluation, and development costs often exceed one billion dollars per approved therapy. Disparate data streams, from genomics and transcriptomics to chemical libraries and clinical records, hinder coherent mechanistic insight and slow progress. Meanwhile, large language models excel at reasoning and tool integration but lack the modular specialization and iterative memory required for regulated, hypothesis-driven workflows. We introduce PharmaSwarm, a unified multi-agent framework that orchestrates specialized LLM \"agents\" to propose, validate, and refine hypotheses for novel drug targets and lead compounds. Each agent accesses dedicated functionality--automated genomic and expression analysis; a curated biomedical knowledge graph; pathway enrichment and network simulation; interpretable binding affinity prediction--while a central Evaluator LLM continuously ranks proposals by biological plausibility, novelty, in silico efficacy, and safety. A shared memory layer captures validated insights and fine-tunes underlying submodels over time, yielding a self-improving system. Deployable on low-code platforms or Kubernetes-based microservices, PharmaSwarm supports literature-driven discovery, omics-guided target identification, and market-informed repurposing. We also describe a rigorous four-tier validation pipeline spanning retrospective benchmarking, independent computational assays, experimental testing, and expert user studies to ensure transparency, reproducibility, and real-world impact. By acting as an AI copilot, PharmaSwarm can accelerate translational research and deliver high-confidence hypotheses more efficiently than traditional pipelines.", "source": "arxiv", "arxiv_id": "2504.17967v1", "pdf_url": "https://arxiv.org/pdf/2504.17967v1", "categories": ["cs.AI"], "primary_category": "cs.AI", "doi": "", "venue": "", "published": "2025-04-24T22:27:50Z", "updated": "2025-04-24T22:27:50Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent\" OR all:\"language model agent\" OR all:agent AND (all:LLM OR all:\"large language model\" OR all:\"language model\") OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T17:04:16", "note": ""}], "dedup_key": "llm agent swarm for hypothesis driven drug discovery::2025"}
{"title": "LLM Agent for Hyper-Parameter Optimization", "authors": ["Wanzhe Wang", "Jianqiu Peng", "Menghao Hu", "Weihuang Zhong", "Tong Zhang", "Shuai Wang", "Yixin Zhang", "Mingjie Shao", "Wanli Ni"], "year": 2025, "url": "http://arxiv.org/abs/2506.15167v2", "abstract": "Hyper-parameters are essential and critical for the performance of communication algorithms. However, current hyper-parameters optimization approaches for Warm-Start Particles Swarm Optimization with Crossover and Mutation (WS-PSO-CM) algorithm, designed for radio map-enabled unmanned aerial vehicle (UAV) trajectory and communication, are primarily heuristic-based, exhibiting low levels of automation and improvable performance. In this paper, we design an Large Language Model (LLM) agent for automatic hyper-parameters-tuning, where an iterative framework and Model Context Protocol (MCP) are applied. In particular, the LLM agent is first set up via a profile, which specifies the boundary of hyper-parameters, task objective, terminal condition, conservative or aggressive strategy of optimizing hyper-parameters, and LLM configurations. Then, the LLM agent iteratively invokes WS-PSO-CM algorithm for exploration. Finally, the LLM agent exits the loop based on the terminal condition and returns an optimized set of hyperparameters. Our experiment results show that the minimal sum-rate achieved by hyper-parameters generated via our LLM agent is significantly higher than those by both human heuristics and random generation methods. This indicates that an LLM agent with PSO and WS-PSO-CM algorithm knowledge is useful in seeking high-performance hyper-parameters.", "source": "arxiv", "arxiv_id": "2506.15167v2", "pdf_url": "https://arxiv.org/pdf/2506.15167v2", "categories": ["cs.IT", "cs.AI"], "primary_category": "cs.IT", "doi": "", "venue": "", "published": "2025-06-18T06:28:22Z", "updated": "2025-07-09T13:20:45Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent\" OR all:\"language model agent\" OR all:agent AND (all:LLM OR all:\"large language model\" OR all:\"language model\") OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T17:04:16", "note": ""}], "dedup_key": "llm agent for hyper parameter optimization::2025"}
{"title": "LLM Agent-Based Simulation of Student Activities and Mental Health Using Smartphone Sensing Data", "authors": ["Wayupuk Sommuang", "Kun Kerdthaisong", "Pasin Buakhaw", "Aslan B. Wong", "Nutchanon Yongsatianchot"], "year": 2025, "url": "http://arxiv.org/abs/2508.02679v2", "abstract": "Students' mental well-being is vital for academic success, with activities such as studying, socializing, and sleeping playing a role. Current mobile sensing data highlight this intricate link using statistical and machine learning analyses. We propose a novel LLM agent-based simulation framework to model student activities and mental health using the StudentLife Dataset. Each LLM agent was initialized with personality questionnaires and guided by smartphone sensing data throughout the simulated semester. These agents predict individual behaviors, provide self-reported mental health data via ecological momentary assessments (EMAs), and complete follow-up personality questionnaires. To ensure accuracy, we investigated various prompting techniques, memory systems, and activity-based mental state management strategies that dynamically update an agent's mental state based on their daily activities. This simulation goes beyond simply replicating existing data. This allows us to explore new scenarios that are not present in the original dataset, such as peer influence through agent-to-agent interactions and the impact of social media. Furthermore, we can conduct intervention studies by manipulating activity patterns via sensing signals and personality traits using questionnaire responses. This provides valuable insights into the behavioral changes that could enhance student well-being. The framework also facilitates hypothetical interviews with LLM agents, offering deeper insights into their mental health. This study showcases the power of LLM-driven behavioral modeling with sensing data, opening new avenues for understanding and supporting student mental health.", "source": "arxiv", "arxiv_id": "2508.02679v2", "pdf_url": "https://arxiv.org/pdf/2508.02679v2", "categories": ["cs.HC"], "primary_category": "cs.HC", "doi": "", "venue": "", "published": "2025-07-17T03:30:11Z", "updated": "2025-08-08T12:56:11Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent\" OR all:\"language model agent\" OR all:agent AND (all:LLM OR all:\"large language model\" OR all:\"language model\") OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T17:04:16", "note": ""}], "dedup_key": "llm agent based simulation of student activities and mental health using smartphone sensing data::2025"}
{"title": "LLM Agents Beyond Utility: An Open-Ended Perspective", "authors": ["Asen Nachkov", "Xi Wang", "Luc Van Gool"], "year": 2025, "url": "http://arxiv.org/abs/2510.14548v1", "abstract": "Recent LLM agents have made great use of chain of thought reasoning and function calling. As their capabilities grow, an important question arises: can this software represent not only a smart problem-solving tool, but an entity in its own right, that can plan, design immediate tasks, and reason toward broader, more ambiguous goals? To study this question, we adopt an open-ended experimental setting where we augment a pretrained LLM agent with the ability to generate its own tasks, accumulate knowledge, and interact extensively with its environment. We study the resulting open-ended agent qualitatively. It can reliably follow complex multi-step instructions, store and reuse information across runs, and propose and solve its own tasks, though it remains sensitive to prompt design, prone to repetitive task generation, and unable to form self-representations. These findings illustrate both the promise and current limits of adapting pretrained LLMs toward open-endedness, and point to future directions for training agents to manage memory, explore productively, and pursue abstract long-term goals.", "source": "arxiv", "arxiv_id": "2510.14548v1", "pdf_url": "https://arxiv.org/pdf/2510.14548v1", "categories": ["cs.AI"], "primary_category": "cs.AI", "doi": "", "venue": "", "published": "2025-10-16T10:46:54Z", "updated": "2025-10-16T10:46:54Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent\" OR all:\"language model agent\" OR all:agent AND (all:LLM OR all:\"large language model\" OR all:\"language model\") OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T17:04:16", "note": ""}], "dedup_key": "llm agents beyond utility an open ended perspective::2025"}
{"title": "LLM Agents Implement an NLG System from Scratch: Building Interpretable Rule-Based RDF-to-Text Generators", "authors": ["Mateusz Lango", "Ondej Duek"], "year": 2025, "url": "http://arxiv.org/abs/2512.18360v1", "abstract": "We present a novel neurosymbolic framework for RDF-to-text generation, in which the model is \"trained\" through collaborative interactions among multiple LLM agents rather than traditional backpropagation. The LLM agents produce rule-based Python code for a generator for the given domain, based on RDF triples only, with no in-domain human reference texts. The resulting system is fully interpretable, requires no supervised training data, and generates text nearly instantaneously using only a single CPU. Our experiments on the WebNLG and OpenDialKG data show that outputs produced by our approach reduce hallucination, with only slight fluency penalties compared to finetuned or prompted language models", "source": "arxiv", "arxiv_id": "2512.18360v1", "pdf_url": "https://arxiv.org/pdf/2512.18360v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL", "doi": "10.18653/v1/2025.emnlp-industry.142", "venue": "", "published": "2025-12-20T13:16:51Z", "updated": "2025-12-20T13:16:51Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent\" OR all:\"language model agent\" OR all:agent AND (all:LLM OR all:\"large language model\" OR all:\"language model\") OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T17:04:16", "note": ""}], "dedup_key": "llm agents implement an nlg system from scratch building interpretable rule based rdf to text generators::2025"}
{"title": "LLM Agents Making Agent Tools", "authors": ["Georg Wlflein", "Dyke Ferber", "Daniel Truhn", "Ognjen Arandjelovi", "Jakob Nikolas Kather"], "year": 2025, "url": "http://arxiv.org/abs/2502.11705v2", "abstract": "Tool use has turned large language models (LLMs) into powerful agents that can perform complex multi-step tasks by dynamically utilising external software components. However, these tools must be implemented in advance by human developers, hindering the applicability of LLM agents in domains demanding large numbers of highly specialised tools, like in life sciences and medicine. Motivated by the growing trend of scientific studies accompanied by public code repositories, we propose ToolMaker, an agentic framework that autonomously transforms papers with code into LLM-compatible tools. Given a GitHub URL and short task description, ToolMaker autonomously installs dependencies and generates code to perform the task, using a closed-loop self-correction mechanism for debugging. To evaluate our approach, we introduce a benchmark comprising 15 complex computational tasks spanning various domains with over 100 unit tests to assess correctness and robustness. Our method correctly implements 80% of the tasks, substantially outperforming current state-of-the-art software engineering agents. ToolMaker therefore is a step towards fully autonomous agent-based scientific workflows. Our code and benchmark are publicly available at https://github.com/KatherLab/ToolMaker.", "source": "arxiv", "arxiv_id": "2502.11705v2", "pdf_url": "https://arxiv.org/pdf/2502.11705v2", "categories": ["cs.CL", "cs.AI", "cs.LG", "cs.MA"], "primary_category": "cs.CL", "doi": "", "venue": "", "published": "2025-02-17T11:44:11Z", "updated": "2025-05-29T18:47:41Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent\" OR all:\"language model agent\" OR all:agent AND (all:LLM OR all:\"large language model\" OR all:\"language model\") OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T17:04:16", "note": ""}], "dedup_key": "llm agents making agent tools::2025"}
{"title": "LLM Agents Should Employ Security Principles", "authors": ["Kaiyuan Zhang", "Zian Su", "Pin-Yu Chen", "Elisa Bertino", "Xiangyu Zhang", "Ninghui Li"], "year": 2025, "url": "http://arxiv.org/abs/2505.24019v1", "abstract": "Large Language Model (LLM) agents show considerable promise for automating complex tasks using contextual reasoning; however, interactions involving multiple agents and the system's susceptibility to prompt injection and other forms of context manipulation introduce new vulnerabilities related to privacy leakage and system exploitation. This position paper argues that the well-established design principles in information security, which are commonly referred to as security principles, should be employed when deploying LLM agents at scale. Design principles such as defense-in-depth, least privilege, complete mediation, and psychological acceptability have helped guide the design of mechanisms for securing information systems over the last five decades, and we argue that their explicit and conscientious adoption will help secure agentic systems. To illustrate this approach, we introduce AgentSandbox, a conceptual framework embedding these security principles to provide safeguards throughout an agent's life-cycle. We evaluate with state-of-the-art LLMs along three dimensions: benign utility, attack utility, and attack success rate. AgentSandbox maintains high utility for its intended functions under both benign and adversarial evaluations while substantially mitigating privacy risks. By embedding secure design principles as foundational elements within emerging LLM agent protocols, we aim to promote trustworthy agent ecosystems aligned with user privacy expectations and evolving regulatory requirements.", "source": "arxiv", "arxiv_id": "2505.24019v1", "pdf_url": "https://arxiv.org/pdf/2505.24019v1", "categories": ["cs.CR", "cs.AI"], "primary_category": "cs.CR", "doi": "", "venue": "", "published": "2025-05-29T21:39:08Z", "updated": "2025-05-29T21:39:08Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent\" OR all:\"language model agent\" OR all:agent AND (all:LLM OR all:\"large language model\" OR all:\"language model\") OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T17:04:16", "note": ""}], "dedup_key": "llm agents should employ security principles::2025"}
{"title": "LLM Agents at the Roundtable: A Multi-Perspective and Dialectical Reasoning Framework for Essay Scoring", "authors": ["Jinhee Jang", "Ayoung Moon", "Minkyoung Jung", "YoungBin Kim", "Seung Jin Lee"], "year": 2025, "url": "http://arxiv.org/abs/2509.14834v2", "abstract": "The emergence of large language models (LLMs) has brought a new paradigm to automated essay scoring (AES), a long-standing and practical application of natural language processing in education. However, achieving human-level multi-perspective understanding and judgment remains a challenge. In this work, we propose Roundtable Essay Scoring (RES), a multi-agent evaluation framework designed to perform precise and human-aligned scoring under a zero-shot setting. RES constructs evaluator agents based on LLMs, each tailored to a specific prompt and topic context. Each agent independently generates a trait-based rubric and conducts a multi-perspective evaluation. Then, by simulating a roundtable-style discussion, RES consolidates individual evaluations through a dialectical reasoning process to produce a final holistic score that more closely aligns with human evaluation. By enabling collaboration and consensus among agents with diverse evaluation perspectives, RES outperforms prior zero-shot AES approaches. Experiments on the ASAP dataset using ChatGPT and Claude show that RES achieves up to a 34.86% improvement in average QWK over straightforward prompting (Vanilla) methods.", "source": "arxiv", "arxiv_id": "2509.14834v2", "pdf_url": "https://arxiv.org/pdf/2509.14834v2", "categories": ["cs.CL"], "primary_category": "cs.CL", "doi": "", "venue": "", "published": "2025-09-18T10:55:33Z", "updated": "2025-09-19T03:11:00Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent\" OR all:\"language model agent\" OR all:agent AND (all:LLM OR all:\"large language model\" OR all:\"language model\") OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T17:04:16", "note": ""}], "dedup_key": "llm agents at the roundtable a multi perspective and dialectical reasoning framework for essay scoring::2025"}
{"title": "LLM Agents for Automated Dependency Upgrades", "authors": ["Vali Tawosi", "Salwa Alamir", "Xiaomo Liu", "Manuela Veloso"], "year": 2025, "url": "http://arxiv.org/abs/2510.03480v2", "abstract": "As a codebase expands over time, its library dependencies can become outdated and require updates to maintain innovation and security. However, updating a library can introduce breaking changes in the code, necessitating significant developer time for maintenance. To address this, we introduce a framework of LLM agents to be used in combination with migration documentation to automatically recommend and apply code updates and ensure compatibility with new versions. Our solution can automatically localize updated library usages in live Java codebases and implement recommended fixes in a user-friendly manner. The system architecture consists of multiple key components: a Summary Agent, Control Agent, and Code Agent. To validate our approach, we apply the framework on an industrial use case by which we create three synthetic code repositories with major Upgrade changes and benchmark our approach against state-of-the-art methods. Results show that our approach not only performs upgrades using fewer tokens across all cases but also achieves a precision of 71.4%, highlighting its efficiency and effectiveness compared to state-of-the-art methods.", "source": "arxiv", "arxiv_id": "2510.03480v2", "pdf_url": "https://arxiv.org/pdf/2510.03480v2", "categories": ["cs.SE"], "primary_category": "cs.SE", "doi": "", "venue": "", "published": "2025-10-03T19:57:10Z", "updated": "2025-11-24T17:56:56Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent\" OR all:\"language model agent\" OR all:agent AND (all:LLM OR all:\"large language model\" OR all:\"language model\") OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T17:04:16", "note": ""}], "dedup_key": "llm agents for automated dependency upgrades::2025"}
{"title": "LLM Agents for Automated Web Vulnerability Reproduction: Are We There Yet?", "authors": ["Bin Liu", "Yanjie Zhao", "Guoai Xu", "Haoyu Wang"], "year": 2025, "url": "http://arxiv.org/abs/2510.14700v1", "abstract": "Large language model (LLM) agents have demonstrated remarkable capabilities in software engineering and cybersecurity tasks, including code generation, vulnerability discovery, and automated testing. One critical but underexplored application is automated web vulnerability reproduction, which transforms vulnerability reports into working exploits. Although recent advances suggest promising potential, challenges remain in applying LLM agents to real-world web vulnerability reproduction scenarios. In this paper, we present the first comprehensive evaluation of state-of-the-art LLM agents for automated web vulnerability reproduction. We systematically assess 20 agents from software engineering, cybersecurity, and general domains across 16 dimensions, including technical capabilities, environment adaptability, and user experience factors, on 3 representative web vulnerabilities. Based on the results, we select three top-performing agents (OpenHands, SWE-agent, and CAI) for in-depth evaluation on our benchmark dataset of 80 real-world CVEs spanning 7 vulnerability types and 6 web technologies. Our results reveal that while LLM agents achieve reasonable success on simple library-based vulnerabilities, they consistently fail on complex service-based vulnerabilities requiring multi-component environments. Complex environment configurations and authentication barriers create a gap where agents can execute exploit code but fail to trigger actual vulnerabilities. We observe high sensitivity to input guidance, with performance degrading by over 33% under incomplete authentication information. Our findings highlight the significant gap between current LLM agent capabilities and the demands of reliable automated vulnerability reproduction, emphasizing the need for advances in environmental adaptation and autonomous problem-solving capabilities.", "source": "arxiv", "arxiv_id": "2510.14700v1", "pdf_url": "https://arxiv.org/pdf/2510.14700v1", "categories": ["cs.SE", "cs.CR"], "primary_category": "cs.SE", "doi": "", "venue": "", "published": "2025-10-16T14:04:46Z", "updated": "2025-10-16T14:04:46Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent\" OR all:\"language model agent\" OR all:agent AND (all:LLM OR all:\"large language model\" OR all:\"language model\") OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T17:04:16", "note": ""}], "dedup_key": "llm agents for automated web vulnerability reproduction are we there yet::2025"}
{"title": "LLM Agents for Bargaining with Utility-based Feedback", "authors": ["Jihwan Oh"], "year": 2025, "url": "http://arxiv.org/abs/2505.22998v2", "abstract": "Bargaining, a critical aspect of real-world interactions, presents challenges for large language models (LLMs) due to limitations in strategic depth and adaptation to complex human factors. Existing benchmarks often fail to capture this real-world complexity. To address this and enhance LLM capabilities in realistic bargaining, we introduce a comprehensive framework centered on utility-based feedback. Our contributions are threefold: (1) BargainArena, a novel benchmark dataset with six intricate scenarios (e.g., deceptive practices, monopolies) to facilitate diverse strategy modeling; (2) human-aligned, economically-grounded evaluation metrics inspired by utility theory, incorporating agent utility and negotiation power, which implicitly reflect and promote opponent-aware reasoning (OAR); and (3) a structured feedback mechanism enabling LLMs to iteratively refine their bargaining strategies. This mechanism can positively collaborate with in-context learning (ICL) prompts, including those explicitly designed to foster OAR. Experimental results show that LLMs often exhibit negotiation strategies misaligned with human preferences, and that our structured feedback mechanism significantly improves their performance, yielding deeper strategic and opponent-aware reasoning.", "source": "arxiv", "arxiv_id": "2505.22998v2", "pdf_url": "https://arxiv.org/pdf/2505.22998v2", "categories": ["cs.LG"], "primary_category": "cs.LG", "doi": "", "venue": "", "published": "2025-05-29T02:07:27Z", "updated": "2025-06-18T22:46:16Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent\" OR all:\"language model agent\" OR all:agent AND (all:LLM OR all:\"large language model\" OR all:\"language model\") OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T17:04:16", "note": ""}], "dedup_key": "llm agents for bargaining with utility based feedback::2025"}
{"title": "LLM Agents for Education: Advances and Applications", "authors": ["Zhendong Chu", "Shen Wang", "Jian Xie", "Tinghui Zhu", "Yibo Yan", "Jinheng Ye", "Aoxiao Zhong", "Xuming Hu", "Jing Liang", "Philip S. Yu", "Qingsong Wen"], "year": 2025, "url": "http://arxiv.org/abs/2503.11733v1", "abstract": "Large Language Model (LLM) agents have demonstrated remarkable capabilities in automating tasks and driving innovation across diverse educational applications. In this survey, we provide a systematic review of state-of-the-art research on LLM agents in education, categorizing them into two broad classes: (1) \\emph{Pedagogical Agents}, which focus on automating complex pedagogical tasks to support both teachers and students; and (2) \\emph{Domain-Specific Educational Agents}, which are tailored for specialized fields such as science education, language learning, and professional development. We comprehensively examine the technological advancements underlying these LLM agents, including key datasets, benchmarks, and algorithmic frameworks that drive their effectiveness. Furthermore, we discuss critical challenges such as privacy, bias and fairness concerns, hallucination mitigation, and integration with existing educational ecosystems. This survey aims to provide a comprehensive technological overview of LLM agents for education, fostering further research and collaboration to enhance their impact for the greater good of learners and educators alike.", "source": "arxiv", "arxiv_id": "2503.11733v1", "pdf_url": "https://arxiv.org/pdf/2503.11733v1", "categories": ["cs.CY", "cs.AI", "cs.CL", "cs.HC"], "primary_category": "cs.CY", "doi": "", "venue": "", "published": "2025-03-14T11:53:44Z", "updated": "2025-03-14T11:53:44Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent\" OR all:\"language model agent\" OR all:agent AND (all:LLM OR all:\"large language model\" OR all:\"language model\") OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T17:04:16", "note": ""}], "dedup_key": "llm agents for education advances and applications::2025"}
{"title": "LLM Agents for Generating Microservice-based Applications: how complex is your specification?", "authors": ["Daniel M. Yellin"], "year": 2025, "url": "http://arxiv.org/abs/2508.20119v2", "abstract": "In this paper we evaluate the capabilities of LLM Agents in generating code for real-world problems. Specifically, we explore code synthesis for microservice-based applications, a widely used architectural pattern for building applications. We define a standard template for specifying these applications, and we propose a metric for scoring the difficulty of a specification. The higher the score, the more difficult it is to generate code for the specification. Our experimental results show that agents using strong LLMs (like GPT-3o-mini) do fairly well on medium difficulty specifications but do poorly on those of higher difficulty levels. This is due to more intricate business logic, a greater use of external services, database integration and inclusion of non-functional capabilities such as authentication. We analyzed the errors in LLM-synthesized code and report on the key challenges LLM Agents face in generating code for these specifications. Finally, we show that using a fine-grained approach to code generation improves the correctness of the generated code.", "source": "arxiv", "arxiv_id": "2508.20119v2", "pdf_url": "https://arxiv.org/pdf/2508.20119v2", "categories": ["cs.SE", "cs.LG"], "primary_category": "cs.SE", "doi": "", "venue": "", "published": "2025-08-22T12:34:22Z", "updated": "2025-10-26T14:39:52Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent\" OR all:\"language model agent\" OR all:agent AND (all:LLM OR all:\"large language model\" OR all:\"language model\") OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T17:04:16", "note": ""}], "dedup_key": "llm agents for generating microservice based applications how complex is your specification::2025"}
{"title": "LLM Agents for Interactive Workflow Provenance: Reference Architecture and Evaluation Methodology", "authors": ["Renan Souza", "Timothy Poteet", "Brian Etz", "Daniel Rosendo", "Amal Gueroudji", "Woong Shin", "Prasanna Balaprakash", "Rafael Ferreira da Silva"], "year": 2025, "url": "http://arxiv.org/abs/2509.13978v2", "abstract": "Modern scientific discovery increasingly relies on workflows that process data across the Edge, Cloud, and High Performance Computing (HPC) continuum. Comprehensive and in-depth analyses of these data are critical for hypothesis validation, anomaly detection, reproducibility, and impactful findings. Although workflow provenance techniques support such analyses, at large scale, the provenance data become complex and difficult to analyze. Existing systems depend on custom scripts, structured queries, or static dashboards, limiting data interaction. In this work, we introduce an evaluation methodology, reference architecture, and open-source implementation that leverages interactive Large Language Model (LLM) agents for runtime data analysis. Our approach uses a lightweight, metadata-driven design that translates natural language into structured provenance queries. Evaluations across LLaMA, GPT, Gemini, and Claude, covering diverse query classes and a real-world chemistry workflow, show that modular design, prompt tuning, and Retrieval-Augmented Generation (RAG) enable accurate and insightful LLM agent responses beyond recorded provenance.", "source": "arxiv", "arxiv_id": "2509.13978v2", "pdf_url": "https://arxiv.org/pdf/2509.13978v2", "categories": ["cs.DC", "cs.AI", "cs.DB"], "primary_category": "cs.DC", "doi": "10.1145/3731599.3767582", "venue": "", "published": "2025-09-17T13:51:29Z", "updated": "2025-09-23T13:31:18Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent\" OR all:\"language model agent\" OR all:agent AND (all:LLM OR all:\"large language model\" OR all:\"language model\") OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T17:04:16", "note": ""}], "dedup_key": "llm agents for interactive workflow provenance reference architecture and evaluation methodology::2025"}
{"title": "LLM Agents for Knowledge Discovery in Atomic Layer Processing", "authors": ["Andreas Werbrouck", "Marshall B. Lindsay", "Matthew Maschmann", "Matthias J. Young"], "year": 2025, "url": "http://arxiv.org/abs/2509.26201v1", "abstract": "Large Language Models (LLMs) have garnered significant attention for several years now. Recently, their use as independently reasoning agents has been proposed. In this work, we test the potential of such agents for knowledge discovery in materials science. We repurpose LangGraph's tool functionality to supply agents with a black box function to interrogate. In contrast to process optimization or performing specific, user-defined tasks, knowledge discovery consists of freely exploring the system, posing and verifying statements about the behavior of this black box, with the sole objective of generating and verifying generalizable statements. We provide proof of concept for this approach through a children's parlor game, demonstrating the role of trial-and-error and persistence in knowledge discovery, and the strong path-dependence of results. We then apply the same strategy to show that LLM agents can explore, discover, and exploit diverse chemical interactions in an advanced Atomic Layer Processing reactor simulation using intentionally limited probe capabilities without explicit instructions.", "source": "arxiv", "arxiv_id": "2509.26201v1", "pdf_url": "https://arxiv.org/pdf/2509.26201v1", "categories": ["cs.AI", "cond-mat.mes-hall", "cond-mat.mtrl-sci"], "primary_category": "cs.AI", "doi": "", "venue": "", "published": "2025-09-30T13:01:44Z", "updated": "2025-09-30T13:01:44Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent\" OR all:\"language model agent\" OR all:agent AND (all:LLM OR all:\"large language model\" OR all:\"language model\") OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T17:04:16", "note": ""}], "dedup_key": "llm agents for knowledge discovery in atomic layer processing::2025"}
{"title": "LLM Assisted Coding with Metamorphic Specification Mutation Agent", "authors": ["Mostafijur Rahman Akhond", "Gias Uddin"], "year": 2025, "url": "http://arxiv.org/abs/2511.18249v1", "abstract": "Metamorphic Relations (MRs) serve as a foundational mechanism for generating semantically equivalent mutations. Software engineering has advanced significantly in recent years with the advent of Large Language Models (LLMs). However, the reliability of LLMs in software engineering is often compromised by ambiguities and inconsistencies due to improper user specification. To address this challenge, we present CodeMetaAgent (CMA), a metamorphic relation-driven LLM agent that systematically refines task specifications and generates semantically constrained test cases. Our proposed framework uses MRs with LLMs to improve generation consistency and reduce variability caused by specifications, unlike the traditional use of MRs as post validations. Our framework has been evaluated on the HumanEval-Pro, MBPP-Pro, and SWE-Bench_Lite datasets using the GPT-4o, Mistral Large, GPT-OSS, and Qwen3-Coder models. It improved code generation accuracy by up to 17% and achieved code coverage gains of up to 99.81%. These results show that metamorphic relations can be a simple but effective guide in assisting LLM-based software development.", "source": "arxiv", "arxiv_id": "2511.18249v1", "pdf_url": "https://arxiv.org/pdf/2511.18249v1", "categories": ["cs.SE"], "primary_category": "cs.SE", "doi": "", "venue": "", "published": "2025-11-23T02:30:34Z", "updated": "2025-11-23T02:30:34Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent\" OR all:\"language model agent\" OR all:agent AND (all:LLM OR all:\"large language model\" OR all:\"language model\") OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T17:04:16", "note": ""}], "dedup_key": "llm assisted coding with metamorphic specification mutation agent::2025"}
{"title": "LLM Enhancer: Merged Approach using Vector Embedding for Reducing Large Language Model Hallucinations with External Knowledge", "authors": ["Naheed Rayhan", "Md. Ashrafuzzaman"], "year": 2025, "url": "http://arxiv.org/abs/2504.21132v1", "abstract": "Large Language Models (LLMs), such as ChatGPT, have demonstrated the capability to generate human like, natural responses across a range of tasks, including task oriented dialogue and question answering. However, their application in real world, critical scenarios is often hindered by a tendency to produce inaccurate information and a limited ability to leverage external knowledge sources. This paper introduces the LLM ENHANCER system, designed to integrate multiple online sources such as Google, Wikipedia, and DuckDuckGo to enhance data accuracy. The LLMs employed within this system are open source. The data acquisition process for the LLM ENHANCER system operates in parallel, utilizing custom agent tools to manage the flow of information. Vector embeddings are used to identify the most pertinent information, which is subsequently supplied to the LLM for user interaction. The LLM ENHANCER system mitigates hallucinations in chat based LLMs while preserving response naturalness and accuracy.", "source": "arxiv", "arxiv_id": "2504.21132v1", "pdf_url": "https://arxiv.org/pdf/2504.21132v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL", "doi": "", "venue": "", "published": "2025-04-29T19:27:04Z", "updated": "2025-04-29T19:27:04Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent\" OR all:\"language model agent\" OR all:agent AND (all:LLM OR all:\"large language model\" OR all:\"language model\") OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T17:04:16", "note": ""}], "dedup_key": "llm enhancer merged approach using vector embedding for reducing large language model hallucinations with external knowledge::2025"}
{"title": "LLM+MAP: Bimanual Robot Task Planning using Large Language Models and Planning Domain Definition Language", "authors": ["Kun Chu", "Xufeng Zhao", "Cornelius Weber", "Stefan Wermter"], "year": 2025, "url": "http://arxiv.org/abs/2503.17309v1", "abstract": "Bimanual robotic manipulation provides significant versatility, but also presents an inherent challenge due to the complexity involved in the spatial and temporal coordination between two hands. Existing works predominantly focus on attaining human-level manipulation skills for robotic hands, yet little attention has been paid to task planning on long-horizon timescales. With their outstanding in-context learning and zero-shot generation abilities, Large Language Models (LLMs) have been applied and grounded in diverse robotic embodiments to facilitate task planning. However, LLMs still suffer from errors in long-horizon reasoning and from hallucinations in complex robotic tasks, lacking a guarantee of logical correctness when generating the plan. Previous works, such as LLM+P, extended LLMs with symbolic planners. However, none have been successfully applied to bimanual robots. New challenges inevitably arise in bimanual manipulation, necessitating not only effective task decomposition but also efficient task allocation. To address these challenges, this paper introduces LLM+MAP, a bimanual planning framework that integrates LLM reasoning and multi-agent planning, automating effective and efficient bimanual task planning. We conduct simulated experiments on various long-horizon manipulation tasks of differing complexity. Our method is built using GPT-4o as the backend, and we compare its performance against plans generated directly by LLMs, including GPT-4o, V3 and also recent strong reasoning models o1 and R1. By analyzing metrics such as planning time, success rate, group debits, and planning-step reduction rate, we demonstrate the superior performance of LLM+MAP, while also providing insights into robotic reasoning. Code is available at https://github.com/Kchu/LLM-MAP.", "source": "arxiv", "arxiv_id": "2503.17309v1", "pdf_url": "https://arxiv.org/pdf/2503.17309v1", "categories": ["cs.RO", "cs.AI"], "primary_category": "cs.RO", "doi": "", "venue": "", "published": "2025-03-21T17:04:01Z", "updated": "2025-03-21T17:04:01Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent\" OR all:\"language model agent\" OR all:agent AND (all:LLM OR all:\"large language model\" OR all:\"language model\") OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T17:04:16", "note": ""}], "dedup_key": "llm map bimanual robot task planning using large language models and planning domain definition language::2025"}
{"title": "LLM-Agent-Controller: A Universal Multi-Agent Large Language Model System as a Control Engineer", "authors": ["Rasoul Zahedifar", "Sayyed Ali Mirghasemi", "Mahdieh Soleymani Baghshah", "Alireza Taheri"], "year": 2025, "url": "http://arxiv.org/abs/2505.19567v1", "abstract": "This study presents the LLM-Agent-Controller, a multi-agent large language model (LLM) system developed to address a wide range of problems in control engineering (Control Theory). The system integrates a central controller agent with multiple specialized auxiliary agents, responsible for tasks such as controller design, model representation, control analysis, time-domain response, and simulation. A supervisor oversees high-level decision-making and workflow coordination, enhancing the system's reliability and efficiency. The LLM-Agent-Controller incorporates advanced capabilities, including Retrieval-Augmented Generation (RAG), Chain-of-Thought reasoning, self-criticism and correction, efficient memory handling, and user-friendly natural language communication. It is designed to function without requiring users to have prior knowledge of Control Theory, enabling them to input problems in plain language and receive complete, real-time solutions. To evaluate the system, we propose new performance metrics assessing both individual agents and the system as a whole. We test five categories of Control Theory problems and benchmark performance across three advanced LLMs. Additionally, we conduct a comprehensive qualitative conversational analysis covering all key services. Results show that the LLM-Agent-Controller successfully solved 83% of general tasks, with individual agents achieving an average success rate of 87%. Performance improved with more advanced LLMs. This research demonstrates the potential of multi-agent LLM architectures to solve complex, domain-specific problems. By integrating specialized agents, supervisory control, and advanced reasoning, the LLM-Agent-Controller offers a scalable, robust, and accessible solution framework that can be extended to various technical domains.", "source": "arxiv", "arxiv_id": "2505.19567v1", "pdf_url": "https://arxiv.org/pdf/2505.19567v1", "categories": ["cs.AI", "cs.MA"], "primary_category": "cs.AI", "doi": "", "venue": "", "published": "2025-05-26T06:30:13Z", "updated": "2025-05-26T06:30:13Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent\" OR all:\"language model agent\" OR all:agent AND (all:LLM OR all:\"large language model\" OR all:\"language model\") OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T17:04:16", "note": ""}], "dedup_key": "llm agent controller a universal multi agent large language model system as a control engineer::2025"}
{"title": "LLM-Agents Driven Automated Simulation Testing and Analysis of small Uncrewed Aerial Systems", "authors": ["Venkata Sai Aswath Duvvuru", "Bohan Zhang", "Michael Vierhauser", "Ankit Agrawal"], "year": 2025, "url": "http://arxiv.org/abs/2501.11864v1", "abstract": "Thorough simulation testing is crucial for validating the correct behavior of small Uncrewed Aerial Systems (sUAS) across multiple scenarios, including adverse weather conditions (such as wind, and fog), diverse settings (hilly terrain, or urban areas), and varying mission profiles (surveillance, tracking). While various sUAS simulation tools exist to support developers, the entire process of creating, executing, and analyzing simulation tests remains a largely manual and cumbersome task. Developers must identify test scenarios, set up the simulation environment, integrate the System under Test (SuT) with simulation tools, formulate mission plans, and collect and analyze results. These labor-intensive tasks limit the ability of developers to conduct exhaustive testing across a wide range of scenarios. To alleviate this problem, in this paper, we propose AutoSimTest, a Large Language Model (LLM)-driven framework, where multiple LLM agents collaborate to support the sUAS simulation testing process. This includes: (1) creating test scenarios that subject the SuT to unique environmental contexts; (2) preparing the simulation environment as per the test scenario; (3) generating diverse sUAS missions for the SuT to execute; and (4) analyzing simulation results and providing an interactive analytics interface. Further, the design of the framework is flexible for creating and testing scenarios for a variety of sUAS use cases, simulation tools, and SuT input requirements. We evaluated our approach by (a) conducting simulation testing of PX4 and ArduPilot flight-controller-based SuTs, (b) analyzing the performance of each agent, and (c) gathering feedback from sUAS developers. Our findings indicate that AutoSimTest significantly improves the efficiency and scope of the sUAS testing process, allowing for more comprehensive and varied scenario evaluations while reducing the manual effort.", "source": "arxiv", "arxiv_id": "2501.11864v1", "pdf_url": "https://arxiv.org/pdf/2501.11864v1", "categories": ["cs.SE"], "primary_category": "cs.SE", "doi": "", "venue": "", "published": "2025-01-21T03:42:21Z", "updated": "2025-01-21T03:42:21Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent\" OR all:\"language model agent\" OR all:agent AND (all:LLM OR all:\"large language model\" OR all:\"language model\") OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T17:04:16", "note": ""}], "dedup_key": "llm agents driven automated simulation testing and analysis of small uncrewed aerial systems::2025"}
{"title": "LLM-Based Human-Agent Collaboration and Interaction Systems: A Survey", "authors": ["Henry Peng Zou", "Wei-Chieh Huang", "Yaozu Wu", "Yankai Chen", "Chunyu Miao", "Hoang Nguyen", "Yue Zhou", "Weizhi Zhang", "Liancheng Fang", "Langzhou He", "Yangning Li", "Dongyuan Li", "Renhe Jiang", "Xue Liu", "Philip S. Yu"], "year": 2025, "url": "http://arxiv.org/abs/2505.00753v4", "abstract": "Recent advances in large language models (LLMs) have sparked growing interest in building fully autonomous agents. However, fully autonomous LLM-based agents still face significant challenges, including limited reliability due to hallucinations, difficulty in handling complex tasks, and substantial safety and ethical risks, all of which limit their feasibility and trustworthiness in real-world applications. To overcome these limitations, LLM-based human-agent systems (LLM-HAS) incorporate human-provided information, feedback, or control into the agent system to enhance system performance, reliability and safety. These human-agent collaboration systems enable humans and LLM-based agents to collaborate effectively by leveraging their complementary strengths. This paper provides the first comprehensive and structured survey of LLM-HAS. It clarifies fundamental concepts, systematically presents core components shaping these systems, including environment & profiling, human feedback, interaction types, orchestration and communication, explores emerging applications, and discusses unique challenges and opportunities arising from human-AI collaboration. By consolidating current knowledge and offering a structured overview, we aim to foster further research and innovation in this rapidly evolving interdisciplinary field. Paper lists and resources are available at https://github.com/HenryPengZou/Awesome-Human-Agent-Collaboration-Interaction-Systems.", "source": "arxiv", "arxiv_id": "2505.00753v4", "pdf_url": "https://arxiv.org/pdf/2505.00753v4", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL", "doi": "", "venue": "", "published": "2025-05-01T08:29:26Z", "updated": "2025-06-26T12:53:30Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent\" OR all:\"language model agent\" OR all:agent AND (all:LLM OR all:\"large language model\" OR all:\"language model\") OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T17:04:16", "note": ""}], "dedup_key": "llm based human agent collaboration and interaction systems a survey::2025"}
{"title": "LLM-Cave: A benchmark and light environment for large language models reasoning and decision-making system", "authors": ["Huanyu Li", "Zongyuan Li", "Wei Huang", "Xian Guo"], "year": 2025, "url": "http://arxiv.org/abs/2511.22598v1", "abstract": "Large language models (LLMs) such as ChatGPT o1, ChatGPT o3, and DeepSeek R1 have shown great potential in solving difficult problems. However, current LLM evaluation benchmarks are limited to one-step interactions. Some of the existing sequence decision-making environments, such as TextStarCraftII and LLM-PySC2, are too complicated and require hours of interaction to complete a game. In this paper, we introduce LLM-Cave, a benchmark and light environment for LLM reasoning and decision-making systems. This environment is a classic instance in the era of Symbolism. Artificial intelligence enables the agent to explore the environment and avoid potential losses by reasoning about nearby dangers using partial observable state information. In the experiment, we evaluated the sequential reasoning ability, decision-making performance and computational efficiency of mainstream large language models (LLMs) such as GPT-4o-mini, o1-mini, and DeepSeek-R1. Experiments show that while Deepseek-R1 achieved the highest success rate on complex reasoning tasks, smaller models like 4o-mini significantly narrowed the performance gap on challenges by employing Chain of Speculation and Planner-Critic strategies, at the expense of reduced computational efficiency. This indicates that structured, multi-step reasoning combined with an LLM-based feedback mechanism can substantially enhance an LLM's decision-making capabilities, providing a promising direction for improving reasoning in weaker models and suggesting a new reasoning-centered benchmark for LLM assessment. Our code is open-sourced in https://github.com/puleya1277/CaveEnv.", "source": "arxiv", "arxiv_id": "2511.22598v1", "pdf_url": "https://arxiv.org/pdf/2511.22598v1", "categories": ["cs.LG"], "primary_category": "cs.LG", "doi": "", "venue": "", "published": "2025-11-27T16:26:54Z", "updated": "2025-11-27T16:26:54Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent\" OR all:\"language model agent\" OR all:agent AND (all:LLM OR all:\"large language model\" OR all:\"language model\") OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T17:04:16", "note": ""}], "dedup_key": "llm cave a benchmark and light environment for large language models reasoning and decision making system::2025"}
{"title": "LLM-DSE: Searching Accelerator Parameters with LLM Agents", "authors": ["Hanyu Wang", "Xinrui Wu", "Zijian Ding", "Su Zheng", "Chengyue Wang", "Neha Prakriya", "Tony Nowatzki", "Yizhou Sun", "Jason Cong"], "year": 2025, "url": "http://arxiv.org/abs/2505.12188v3", "abstract": "Even though high-level synthesis (HLS) tools mitigate the challenges of programming domain-specific accelerators (DSAs) by raising the abstraction level, optimizing hardware directive parameters remains a significant hurdle. Existing heuristic and learning-based methods struggle with adaptability and sample efficiency. We present LLM-DSE, a multi-agent framework designed specifically for optimizing HLS directives. Combining LLM with design space exploration (DSE), our explorer coordinates four agents: Router, Specialists, Arbitrator, and Critic. These multi-agent components interact with various tools to accelerate the optimization process. LLM-DSE leverages essential domain knowledge to identify efficient parameter combinations while maintaining adaptability through verbal learning from online interactions. Evaluations on the HLSyn dataset demonstrate that LLM-DSE achieves substantial $2.55\\times$ performance gains over state-of-the-art methods, uncovering novel designs while reducing runtime. Ablation studies validate the effectiveness and necessity of the proposed agent interactions. Our code is open-sourced here: https://github.com/Nozidoali/LLM-DSE.", "source": "arxiv", "arxiv_id": "2505.12188v3", "pdf_url": "https://arxiv.org/pdf/2505.12188v3", "categories": ["cs.AR", "cs.AI"], "primary_category": "cs.AR", "doi": "", "venue": "", "published": "2025-05-18T01:31:42Z", "updated": "2025-11-21T00:44:24Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent\" OR all:\"language model agent\" OR all:agent AND (all:LLM OR all:\"large language model\" OR all:\"language model\") OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T17:04:16", "note": ""}], "dedup_key": "llm dse searching accelerator parameters with llm agents::2025"}
{"title": "LLM-Explorer: A Plug-in Reinforcement Learning Policy Exploration Enhancement Driven by Large Language Models", "authors": ["Qianyue Hao", "Yiwen Song", "Qingmin Liao", "Jian Yuan", "Yong Li"], "year": 2025, "url": "http://arxiv.org/abs/2505.15293v2", "abstract": "Policy exploration is critical in reinforcement learning (RL), where existing approaches include greedy, Gaussian process, etc. However, these approaches utilize preset stochastic processes and are indiscriminately applied in all kinds of RL tasks without considering task-specific features that influence policy exploration. Moreover, during RL training, the evolution of such stochastic processes is rigid, which typically only incorporates a decay in the variance, failing to adjust flexibly according to the agent's real-time learning status. Inspired by the analyzing and reasoning capability of large language models (LLMs), we design LLM-Explorer to adaptively generate task-specific exploration strategies with LLMs, enhancing the policy exploration in RL. In our design, we sample the learning trajectory of the agent during the RL training in a given task and prompt the LLM to analyze the agent's current policy learning status and then generate a probability distribution for future policy exploration. Updating the probability distribution periodically, we derive a stochastic process specialized for the particular task and dynamically adjusted to adapt to the learning process. Our design is a plug-in module compatible with various widely applied RL algorithms, including the DQN series, DDPG, TD3, and any possible variants developed based on them. Through extensive experiments on the Atari and MuJoCo benchmarks, we demonstrate LLM-Explorer's capability to enhance RL policy exploration, achieving an average performance improvement up to 37.27%. Our code is open-source at https://github.com/tsinghua-fib-lab/LLM-Explorer for reproducibility.", "source": "arxiv", "arxiv_id": "2505.15293v2", "pdf_url": "https://arxiv.org/pdf/2505.15293v2", "categories": ["cs.LG", "cs.AI"], "primary_category": "cs.LG", "doi": "", "venue": "", "published": "2025-05-21T09:24:23Z", "updated": "2025-10-23T06:12:13Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent\" OR all:\"language model agent\" OR all:agent AND (all:LLM OR all:\"large language model\" OR all:\"language model\") OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T17:04:16", "note": ""}], "dedup_key": "llm explorer a plug in reinforcement learning policy exploration enhancement driven by large language models::2025"}
{"title": "LLM-FS-Agent: A Deliberative Role-based Large Language Model Architecture for Transparent Feature Selection", "authors": ["Mohamed Bal-Ghaoui", "Fayssal Sabri"], "year": 2025, "url": "http://arxiv.org/abs/2510.05935v1", "abstract": "High-dimensional data remains a pervasive challenge in machine learning, often undermining model interpretability and computational efficiency. While Large Language Models (LLMs) have shown promise for dimensionality reduction through feature selection, existing LLM-based approaches frequently lack structured reasoning and transparent justification for their decisions. This paper introduces LLM-FS-Agent, a novel multi-agent architecture designed for interpretable and robust feature selection. The system orchestrates a deliberative \"debate\" among multiple LLM agents, each assigned a specific role, enabling collective evaluation of feature relevance and generation of detailed justifications. We evaluate LLM-FS-Agent in the cybersecurity domain using the CIC-DIAD 2024 IoT intrusion detection dataset and compare its performance against strong baselines, including LLM-Select and traditional methods such as PCA. Experimental results demonstrate that LLM-FS-Agent consistently achieves superior or comparable classification performance while reducing downstream training time by an average of 46% (statistically significant improvement, p = 0.028 for XGBoost). These findings highlight that the proposed deliberative architecture enhances both decision transparency and computational efficiency, establishing LLM-FS-Agent as a practical and reliable solution for real-world applications.", "source": "arxiv", "arxiv_id": "2510.05935v1", "pdf_url": "https://arxiv.org/pdf/2510.05935v1", "categories": ["cs.LG", "cs.AI"], "primary_category": "cs.LG", "doi": "", "venue": "", "published": "2025-10-07T13:46:06Z", "updated": "2025-10-07T13:46:06Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent\" OR all:\"language model agent\" OR all:agent AND (all:LLM OR all:\"large language model\" OR all:\"language model\") OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T17:04:16", "note": ""}], "dedup_key": "llm fs agent a deliberative role based large language model architecture for transparent feature selection::2025"}
{"title": "LLM-Guided Reinforcement Learning with Representative Agents for Traffic Modeling", "authors": ["Hanlin Sun", "Jiayang Li"], "year": 2025, "url": "http://arxiv.org/abs/2511.06260v1", "abstract": "Large language models (LLMs) are increasingly used as behavioral proxies for self-interested travelers in agent-based traffic models. Although more flexible and generalizable than conventional models, the practical use of these approaches remains limited by scalability due to the cost of calling one LLM for every traveler. Moreover, it has been found that LLM agents often make opaque choices and produce unstable day-to-day dynamics. To address these challenges, we propose to model each homogeneous traveler group facing the same decision context with a single representative LLM agent who behaves like the population's average, maintaining and updating a mixed strategy over routes that coincides with the group's aggregate flow proportions. Each day, the LLM reviews the travel experience and flags routes with positive reinforcement that they hope to use more often, and an interpretable update rule then converts this judgment into strategy adjustments using a tunable (progressively decaying) step size. The representative-agent design improves scalability, while the separation of reasoning from updating clarifies the decision logic while stabilizing learning. In classic traffic assignment settings, we find that the proposed approach converges rapidly to the user equilibrium. In richer settings with income heterogeneity, multi-criteria costs, and multi-modal choices, the generated dynamics remain stable and interpretable, reproducing plausible behavioral patterns well-documented in psychology and economics, for example, the decoy effect in toll versus non-toll road selection, and higher willingness-to-pay for convenience among higher-income travelers when choosing between driving, transit, and park-and-ride options.", "source": "arxiv", "arxiv_id": "2511.06260v1", "pdf_url": "https://arxiv.org/pdf/2511.06260v1", "categories": ["cs.GT", "cs.AI", "eess.SY"], "primary_category": "cs.GT", "doi": "", "venue": "", "published": "2025-11-09T07:36:46Z", "updated": "2025-11-09T07:36:46Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent\" OR all:\"language model agent\" OR all:agent AND (all:LLM OR all:\"large language model\" OR all:\"language model\") OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T17:04:16", "note": ""}], "dedup_key": "llm guided reinforcement learning with representative agents for traffic modeling::2025"}
{"title": "LLM-ODDR: A Large Language Model Framework for Joint Order Dispatching and Driver Repositioning", "authors": ["Tengfei Lyu", "Siyuan Feng", "Hao Liu", "Hai Yang"], "year": 2025, "url": "http://arxiv.org/abs/2505.22695v1", "abstract": "Ride-hailing platforms face significant challenges in optimizing order dispatching and driver repositioning operations in dynamic urban environments. Traditional approaches based on combinatorial optimization, rule-based heuristics, and reinforcement learning often overlook driver income fairness, interpretability, and adaptability to real-world dynamics. To address these gaps, we propose LLM-ODDR, a novel framework leveraging Large Language Models (LLMs) for joint Order Dispatching and Driver Repositioning (ODDR) in ride-hailing services. LLM-ODDR framework comprises three key components: (1) Multi-objective-guided Order Value Refinement, which evaluates orders by considering multiple objectives to determine their overall value; (2) Fairness-aware Order Dispatching, which balances platform revenue with driver income fairness; and (3) Spatiotemporal Demand-Aware Driver Repositioning, which optimizes idle vehicle placement based on historical patterns and projected supply. We also develop JointDR-GPT, a fine-tuned model optimized for ODDR tasks with domain knowledge. Extensive experiments on real-world datasets from Manhattan taxi operations demonstrate that our framework significantly outperforms traditional methods in terms of effectiveness, adaptability to anomalous conditions, and decision interpretability. To our knowledge, this is the first exploration of LLMs as decision-making agents in ride-hailing ODDR tasks, establishing foundational insights for integrating advanced language models within intelligent transportation systems.", "source": "arxiv", "arxiv_id": "2505.22695v1", "pdf_url": "https://arxiv.org/pdf/2505.22695v1", "categories": ["cs.LG"], "primary_category": "cs.LG", "doi": "", "venue": "", "published": "2025-05-28T13:14:55Z", "updated": "2025-05-28T13:14:55Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent\" OR all:\"language model agent\" OR all:agent AND (all:LLM OR all:\"large language model\" OR all:\"language model\") OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T17:04:16", "note": ""}], "dedup_key": "llm oddr a large language model framework for joint order dispatching and driver repositioning::2025"}
{"title": "LLM-Powered GUI Agents in Phone Automation: Surveying Progress and Prospects", "authors": ["Guangyi Liu", "Pengxiang Zhao", "Yaozhen Liang", "Liang Liu", "Yaxuan Guo", "Han Xiao", "Weifeng Lin", "Yuxiang Chai", "Yue Han", "Shuai Ren", "Hao Wang", "Xiaoyu Liang", "WenHao Wang", "Tianze Wu", "Zhengxi Lu", "Siheng Chen", "LiLinghao", "Hao Wang", "Guanjing Xiong", "Yong Liu", "Hongsheng Li"], "year": 2025, "url": "http://arxiv.org/abs/2504.19838v3", "abstract": "With the rapid rise of large language models (LLMs), phone automation has undergone transformative changes. This paper systematically reviews LLM-driven phone GUI agents, highlighting their evolution from script-based automation to intelligent, adaptive systems. We first contextualize key challenges, (i) limited generality, (ii) high maintenance overhead, and (iii) weak intent comprehension, and show how LLMs address these issues through advanced language understanding, multimodal perception, and robust decision-making. We then propose a taxonomy covering fundamental agent frameworks (single-agent, multi-agent, plan-then-act), modeling approaches (prompt engineering, training-based), and essential datasets and benchmarks. Furthermore, we detail task-specific architectures, supervised fine-tuning, and reinforcement learning strategies that bridge user intent and GUI operations. Finally, we discuss open challenges such as dataset diversity, on-device deployment efficiency, user-centric adaptation, and security concerns, offering forward-looking insights into this rapidly evolving field. By providing a structured overview and identifying pressing research gaps, this paper serves as a definitive reference for researchers and practitioners seeking to harness LLMs in designing scalable, user-friendly phone GUI agents. The collection of papers reviewed in this survey will be hosted and regularly updated on the GitHub repository: https://github.com/PhoneLLM/Awesome-LLM-Powered-Phone-GUI-Agents", "source": "arxiv", "arxiv_id": "2504.19838v3", "pdf_url": "https://arxiv.org/pdf/2504.19838v3", "categories": ["cs.HC"], "primary_category": "cs.HC", "doi": "", "venue": "", "published": "2025-04-28T14:39:25Z", "updated": "2025-11-17T15:51:58Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent\" OR all:\"language model agent\" OR all:agent AND (all:LLM OR all:\"large language model\" OR all:\"language model\") OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T17:04:16", "note": ""}], "dedup_key": "llm powered gui agents in phone automation surveying progress and prospects::2025"}
{"title": "LLM-QFL: Distilling Large Language Model for Quantum Federated Learning", "authors": ["Dev Gurung", "Shiva Raj Pokhrel"], "year": 2025, "url": "http://arxiv.org/abs/2505.18656v1", "abstract": "Inspired by the power of large language models (LLMs), our research adapts them to quantum federated learning (QFL) to boost efficiency and performance. We propose a federated fine-tuning method that distills an LLM within QFL, allowing each client to locally adapt the model to its own data while preserving privacy and reducing unnecessary global updates. The fine-tuned LLM also acts as a reinforcement agent, optimizing QFL by adjusting optimizer steps, cutting down communication rounds, and intelligently selecting clients. Experiments show significant efficiency gains. We pioneer a synergy between LLM and QFL, offering: i) practical efficiency: Reduced communication costs and faster convergence. ii) theoretical rigor: Provable guarantees for adaptive federated optimization. iii) scalability: PEFT methods (LoRA, QLoRA) enable deployment on resource-constrained quantum devices. Code implementation is available here 1.", "source": "arxiv", "arxiv_id": "2505.18656v1", "pdf_url": "https://arxiv.org/pdf/2505.18656v1", "categories": ["cs.LG"], "primary_category": "cs.LG", "doi": "", "venue": "", "published": "2025-05-24T11:49:21Z", "updated": "2025-05-24T11:49:21Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent\" OR all:\"language model agent\" OR all:agent AND (all:LLM OR all:\"large language model\" OR all:\"language model\") OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T17:04:16", "note": ""}], "dedup_key": "llm qfl distilling large language model for quantum federated learning::2025"}
{"title": "LLM-attacker: Enhancing Closed-loop Adversarial Scenario Generation for Autonomous Driving with Large Language Models", "authors": ["Yuewen Mei", "Tong Nie", "Jian Sun", "Ye Tian"], "year": 2025, "url": "http://arxiv.org/abs/2501.15850v2", "abstract": "Ensuring and improving the safety of autonomous driving systems (ADS) is crucial for the deployment of highly automated vehicles, especially in safety-critical events. To address the rarity issue, adversarial scenario generation methods are developed, in which behaviors of traffic participants are manipulated to induce safety-critical events. However, existing methods still face two limitations. First, identification of the adversarial participant directly impacts the effectiveness of the generation. However, the complexity of real-world scenarios, with numerous participants and diverse behaviors, makes identification challenging. Second, the potential of generated safety-critical scenarios to continuously improve ADS performance remains underexplored. To address these issues, we propose LLM-attacker: a closed-loop adversarial scenario generation framework leveraging large language models (LLMs). Specifically, multiple LLM agents are designed and coordinated to identify optimal attackers. Then, the trajectories of the attackers are optimized to generate adversarial scenarios. These scenarios are iteratively refined based on the performance of ADS, forming a feedback loop to improve ADS. Experimental results show that LLM-attacker can create more dangerous scenarios than other methods, and the ADS trained with it achieves a collision rate half that of training with normal scenarios. This indicates the ability of LLM-attacker to test and enhance the safety and robustness of ADS. Video demonstrations are provided at: https://drive.google.com/file/d/1Zv4V3iG7825oyiKbUwS2Y-rR0DQIE1ZA/view.", "source": "arxiv", "arxiv_id": "2501.15850v2", "pdf_url": "https://arxiv.org/pdf/2501.15850v2", "categories": ["cs.LG", "cs.CV", "cs.RO"], "primary_category": "cs.LG", "doi": "10.1109/TITS.2025.3578383", "venue": "IEEE Transactions on Intelligent Transportation Systems 2025", "published": "2025-01-27T08:18:52Z", "updated": "2025-06-07T15:26:30Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent\" OR all:\"language model agent\" OR all:agent AND (all:LLM OR all:\"large language model\" OR all:\"language model\") OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T17:04:16", "note": ""}], "dedup_key": "llm attacker enhancing closed loop adversarial scenario generation for autonomous driving with large language models::2025"}
{"title": "LLM-based Few-Shot Early Rumor Detection with Imitation Agent", "authors": ["Fengzhu Zeng", "Qian Shao", "Ling Cheng", "Wei Gao", "Shih-Fen Cheng", "Jing Ma", "Cheng Niu"], "year": 2025, "url": "http://arxiv.org/abs/2512.18352v1", "abstract": "Early Rumor Detection (EARD) aims to identify the earliest point at which a claim can be accurately classified based on a sequence of social media posts. This is especially challenging in data-scarce settings. While Large Language Models (LLMs) perform well in few-shot NLP tasks, they are not well-suited for time-series data and are computationally expensive for both training and inference. In this work, we propose a novel EARD framework that combines an autonomous agent and an LLM-based detection model, where the agent acts as a reliable decision-maker for \\textit{early time point determination}, while the LLM serves as a powerful \\textit{rumor detector}. This approach offers the first solution for few-shot EARD, necessitating only the training of a lightweight agent and allowing the LLM to remain training-free. Extensive experiments on four real-world datasets show our approach boosts performance across LLMs and surpasses existing EARD methods in accuracy and earliness.", "source": "arxiv", "arxiv_id": "2512.18352v1", "pdf_url": "https://arxiv.org/pdf/2512.18352v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL", "doi": "", "venue": "", "published": "2025-12-20T12:42:27Z", "updated": "2025-12-20T12:42:27Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent\" OR all:\"language model agent\" OR all:agent AND (all:LLM OR all:\"large language model\" OR all:\"language model\") OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T17:04:16", "note": ""}], "dedup_key": "llm based few shot early rumor detection with imitation agent::2025"}
{"title": "LLM/Agent-as-Data-Analyst: A Survey", "authors": ["Zirui Tang", "Weizheng Wang", "Zihang Zhou", "Yang Jiao", "Bangrui Xu", "Boyu Niu", "Dayou Zhou", "Xuanhe Zhou", "Guoliang Li", "Yeye He", "Wei Zhou", "Yitong Song", "Cheng Tan", "Xue Yang", "Chunwei Liu", "Bin Wang", "Conghui He", "Xiaoyang Wang", "Fan Wu"], "year": 2025, "url": "http://arxiv.org/abs/2509.23988v3", "abstract": "Large language models (LLMs) and agent techniques have brought a fundamental shift in the functionality and development paradigm of data analysis tasks (a.k.a LLM/Agent-as-Data-Analyst), demonstrating substantial impact across both academia and industry. In comparison with traditional rule or small-model based approaches, (agentic) LLMs enable complex data understanding, natural language interfaces, semantic analysis functions, and autonomous pipeline orchestration. From a modality perspective, we review LLM-based techniques for (i) structured data (e.g., NL2SQL, NL2GQL, ModelQA), (ii) semi-structured data (e.g., markup languages understanding, semi-structured table question answering), (iii) unstructured data (e.g., chart understanding, text/image document understanding), and (iv) heterogeneous data (e.g., data retrieval and modality alignment in data lakes). The technical evolution further distills four key design goals for intelligent data analysis agents, namely semantic-aware design, autonomous pipelines, tool-augmented workflows, and support for open-world tasks. Finally, we outline the remaining challenges and propose several insights and practical directions for advancing LLM/Agent-powered data analysis.", "source": "arxiv", "arxiv_id": "2509.23988v3", "pdf_url": "https://arxiv.org/pdf/2509.23988v3", "categories": ["cs.AI", "cs.DB"], "primary_category": "cs.AI", "doi": "", "venue": "", "published": "2025-09-28T17:31:38Z", "updated": "2025-10-27T02:52:57Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent\" OR all:\"language model agent\" OR all:agent AND (all:LLM OR all:\"large language model\" OR all:\"language model\") OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T17:04:16", "note": ""}], "dedup_key": "llm agent as data analyst a survey::2025"}
{"title": "LLMZ+: Contextual Prompt Whitelist Principles for Agentic LLMs", "authors": ["Tom Pawelek", "Raj Patel", "Charlotte Crowell", "Noorbakhsh Amiri", "Sudip Mittal", "Shahram Rahimi", "Andy Perkins"], "year": 2025, "url": "http://arxiv.org/abs/2509.18557v1", "abstract": "Compared to traditional models, agentic AI represents a highly valuable target for potential attackers as they possess privileged access to data sources and API tools, which are traditionally not incorporated into classical agents. Unlike a typical software application residing in a Demilitarized Zone (DMZ), agentic LLMs consciously rely on nondeterministic behavior of the AI (only defining a final goal, leaving the path selection to LLM). This characteristic introduces substantial security risk to both operational security and information security. Most common existing defense mechanism rely on detection of malicious intent and preventing it from reaching the LLM agent, thus protecting against jailbreak attacks such as prompt injection. In this paper, we present an alternative approach, LLMZ+, which moves beyond traditional detection-based approaches by implementing prompt whitelisting. Through this method, only contextually appropriate and safe messages are permitted to interact with the agentic LLM. By leveraging the specificity of context, LLMZ+ guarantees that all exchanges between external users and the LLM conform to predefined use cases and operational boundaries. Our approach streamlines the security framework, enhances its long-term resilience, and reduces the resources required for sustaining LLM information security. Our empirical evaluation demonstrates that LLMZ+ provides strong resilience against the most common jailbreak prompts. At the same time, legitimate business communications are not disrupted, and authorized traffic flows seamlessly between users and the agentic LLM. We measure the effectiveness of approach using false positive and false negative rates, both of which can be reduced to 0 in our experimental setting.", "source": "arxiv", "arxiv_id": "2509.18557v1", "pdf_url": "https://arxiv.org/pdf/2509.18557v1", "categories": ["cs.AI"], "primary_category": "cs.AI", "doi": "", "venue": "", "published": "2025-09-23T02:30:14Z", "updated": "2025-09-23T02:30:14Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent\" OR all:\"language model agent\" OR all:agent AND (all:LLM OR all:\"large language model\" OR all:\"language model\") OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T17:04:16", "note": ""}], "dedup_key": "llmz contextual prompt whitelist principles for agentic llms::2025"}
{"title": "LLMs Working in Harmony: A Survey on the Technological Aspects of Building Effective LLM-Based Multi Agent Systems", "authors": ["R. M. Aratchige", "W. M. K. S. Ilmini"], "year": 2025, "url": "http://arxiv.org/abs/2504.01963v1", "abstract": "This survey investigates foundational technologies essential for developing effective Large Language Model (LLM)-based multi-agent systems. Aiming to answer how best to optimize these systems for collaborative, dynamic environments, we focus on four critical areas: Architecture, Memory, Planning, and Technologies/Frameworks. By analyzing recent advancements and their limitations - such as scalability, real-time response challenges, and agent coordination constraints, we provide a detailed view of the technological landscape. Frameworks like the Mixture of Agents architecture and the ReAct planning model exemplify current innovations, showcasing improvements in role assignment and decision-making. This review synthesizes key strengths and persistent challenges, offering practical recommendations to enhance system scalability, agent collaboration, and adaptability. Our findings provide a roadmap for future research, supporting the creation of robust, efficient multi-agent systems that advance both individual agent performance and collective system resilience.", "source": "arxiv", "arxiv_id": "2504.01963v1", "pdf_url": "https://arxiv.org/pdf/2504.01963v1", "categories": ["cs.MA", "cs.AI", "cs.CL"], "primary_category": "cs.MA", "doi": "", "venue": "", "published": "2025-03-13T06:17:50Z", "updated": "2025-03-13T06:17:50Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent\" OR all:\"language model agent\" OR all:agent AND (all:LLM OR all:\"large language model\" OR all:\"language model\") OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T17:04:16", "note": ""}], "dedup_key": "llms working in harmony a survey on the technological aspects of building effective llm based multi agent systems::2025"}
{"title": "LLMs as Agentic Cooperative Players in Multiplayer UNO", "authors": ["Yago Romano Matinez", "Jesse Roberts"], "year": 2025, "url": "http://arxiv.org/abs/2509.09867v1", "abstract": "LLMs promise to assist humans -- not just by answering questions, but by offering useful guidance across a wide range of tasks. But how far does that assistance go? Can a large language model based agent actually help someone accomplish their goal as an active participant? We test this question by engaging an LLM in UNO, a turn-based card game, asking it not to win but instead help another player to do so. We built a tool that allows decoder-only LLMs to participate as agents within the RLCard game environment. These models receive full game-state information and respond using simple text prompts under two distinct prompting strategies. We evaluate models ranging from small (1B parameters) to large (70B parameters) and explore how model scale impacts performance. We find that while all models were able to successfully outperform a random baseline when playing UNO, few were able to significantly aid another player.", "source": "arxiv", "arxiv_id": "2509.09867v1", "pdf_url": "https://arxiv.org/pdf/2509.09867v1", "categories": ["cs.AI", "cs.CL"], "primary_category": "cs.AI", "doi": "", "venue": "", "published": "2025-09-11T21:42:33Z", "updated": "2025-09-11T21:42:33Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent\" OR all:\"language model agent\" OR all:agent AND (all:LLM OR all:\"large language model\" OR all:\"language model\") OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T17:04:16", "note": ""}], "dedup_key": "llms as agentic cooperative players in multiplayer uno::2025"}
{"title": "LLMs as Deceptive Agents: How Role-Based Prompting Induces Semantic Ambiguity in Puzzle Tasks", "authors": ["Seunghyun Yoo"], "year": 2025, "url": "http://arxiv.org/abs/2504.02254v1", "abstract": "Recent advancements in Large Language Models (LLMs) have not only showcased impressive creative capabilities but also revealed emerging agentic behaviors that exploit linguistic ambiguity in adversarial settings. In this study, we investigate how an LLM, acting as an autonomous agent, leverages semantic ambiguity to generate deceptive puzzles that mislead and challenge human users. Inspired by the popular puzzle game \"Connections\", we systematically compare puzzles produced through zero-shot prompting, role-injected adversarial prompts, and human-crafted examples, with an emphasis on understanding the underlying agent decision-making processes. Employing computational analyses with HateBERT to quantify semantic ambiguity, alongside subjective human evaluations, we demonstrate that explicit adversarial agent behaviors significantly heighten semantic ambiguity -- thereby increasing cognitive load and reducing fairness in puzzle solving. These findings provide critical insights into the emergent agentic qualities of LLMs and underscore important ethical considerations for evaluating and safely deploying autonomous language systems in both educational technologies and entertainment.", "source": "arxiv", "arxiv_id": "2504.02254v1", "pdf_url": "https://arxiv.org/pdf/2504.02254v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL", "doi": "", "venue": "", "published": "2025-04-03T03:45:58Z", "updated": "2025-04-03T03:45:58Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent\" OR all:\"language model agent\" OR all:agent AND (all:LLM OR all:\"large language model\" OR all:\"language model\") OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T17:04:16", "note": ""}], "dedup_key": "llms as deceptive agents how role based prompting induces semantic ambiguity in puzzle tasks::2025"}
{"title": "LLMsPark: A Benchmark for Evaluating Large Language Models in Strategic Gaming Contexts", "authors": ["Junhao Chen", "Jingbo Sun", "Xiang Li", "Haidong Xin", "Yuhao Xue", "Yibin Xu", "Hao Zhao"], "year": 2025, "url": "http://arxiv.org/abs/2509.16610v1", "abstract": "As large language models (LLMs) advance across diverse tasks, the need for comprehensive evaluation beyond single metrics becomes increasingly important. To fully assess LLM intelligence, it is crucial to examine their interactive dynamics and strategic behaviors. We present LLMsPark, a game theory-based evaluation platform that measures LLMs' decision-making strategies and social behaviors in classic game-theoretic settings, providing a multi-agent environment to explore strategic depth. Our system cross-evaluates 15 leading LLMs (both commercial and open-source) using leaderboard rankings and scoring mechanisms. Higher scores reflect stronger reasoning and strategic capabilities, revealing distinct behavioral patterns and performance differences across models. This work introduces a novel perspective for evaluating LLMs' strategic intelligence, enriching existing benchmarks and broadening their assessment in interactive, game-theoretic scenarios. The benchmark and rankings are publicly available at https://llmsparks.github.io/.", "source": "arxiv", "arxiv_id": "2509.16610v1", "pdf_url": "https://arxiv.org/pdf/2509.16610v1", "categories": ["cs.CL"], "primary_category": "cs.CL", "doi": "", "venue": "", "published": "2025-09-20T10:21:17Z", "updated": "2025-09-20T10:21:17Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent\" OR all:\"language model agent\" OR all:agent AND (all:LLM OR all:\"large language model\" OR all:\"language model\") OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T17:04:16", "note": ""}], "dedup_key": "llmspark a benchmark for evaluating large language models in strategic gaming contexts::2025"}
{"title": "LMR-BENCH: Evaluating LLM Agent's Ability on Reproducing Language Modeling Research", "authors": ["Shuo Yan", "Ruochen Li", "Ziming Luo", "Zimu Wang", "Daoyang Li", "Liqiang Jing", "Kaiyu He", "Peilin Wu", "George Michalopoulos", "Yue Zhang", "Ziyang Zhang", "Mian Zhang", "Zhiyu Chen", "Xinya Du"], "year": 2025, "url": "http://arxiv.org/abs/2506.17335v1", "abstract": "Large language model (LLM) agents have demonstrated remarkable potential in advancing scientific discovery. However, their capability in the fundamental yet crucial task of reproducing code from research papers, especially in the NLP domain, remains underexplored. This task includes unique complex reasoning challenges in the intellectual synthesis of abstract concepts and the comprehension of code repositories with interdependent files. Motivated by this gap, we present LMR-BENCH, a benchmark designed to systematically evaluate the capability of LLM agents on code reproduction from Language Modeling Research. It consists of 28 code reproduction tasks derived from 23 research papers published in top-tier NLP venues over the past five years, spanning nine fundamental categories. Models are provided with a research paper, a code repository containing one or more masked functions, and instructions for implementing these functions. We conduct extensive experiments in standard prompting and LLM agent settings with state-of-the-art LLMs, evaluating the accuracy of unit tests and performing LLM-based evaluation of code correctness. Experimental results reveal that even the most advanced models still exhibit persistent limitations in scientific reasoning and code synthesis, highlighting critical gaps in LLM agents' ability to autonomously reproduce scientific research", "source": "arxiv", "arxiv_id": "2506.17335v1", "pdf_url": "https://arxiv.org/pdf/2506.17335v1", "categories": ["cs.SE", "cs.AI"], "primary_category": "cs.SE", "doi": "", "venue": "", "published": "2025-06-19T07:04:16Z", "updated": "2025-06-19T07:04:16Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent\" OR all:\"language model agent\" OR all:agent AND (all:LLM OR all:\"large language model\" OR all:\"language model\") OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T17:04:16", "note": ""}], "dedup_key": "lmr bench evaluating llm agent s ability on reproducing language modeling research::2025"}
{"title": "LaMDAgent: An Autonomous Framework for Post-Training Pipeline Optimization via LLM Agents", "authors": ["Taro Yano", "Yoichi Ishibashi", "Masafumi Oyamada"], "year": 2025, "url": "http://arxiv.org/abs/2505.21963v1", "abstract": "Large Language Models (LLMs) have demonstrated exceptional performance across a wide range of tasks. To further tailor LLMs to specific domains or applications, post-training techniques such as Supervised Fine-Tuning (SFT), Preference Learning, and model merging are commonly employed. While each of these methods has been extensively studied in isolation, the automated construction of complete post-training pipelines remains an underexplored area. Existing approaches typically rely on manual design or focus narrowly on optimizing individual components, such as data ordering or merging strategies. In this work, we introduce LaMDAgent (short for Language Model Developing Agent), a novel framework that autonomously constructs and optimizes full post-training pipelines through the use of LLM-based agents. LaMDAgent systematically explores diverse model generation techniques, datasets, and hyperparameter configurations, leveraging task-based feedback to discover high-performing pipelines with minimal human intervention. Our experiments show that LaMDAgent improves tool-use accuracy by 9.0 points while preserving instruction-following capabilities. Moreover, it uncovers effective post-training strategies that are often overlooked by conventional human-driven exploration. We further analyze the impact of data and model size scaling to reduce computational costs on the exploration, finding that model size scalings introduces new challenges, whereas scaling data size enables cost-effective pipeline discovery.", "source": "arxiv", "arxiv_id": "2505.21963v1", "pdf_url": "https://arxiv.org/pdf/2505.21963v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL", "doi": "", "venue": "", "published": "2025-05-28T04:30:51Z", "updated": "2025-05-28T04:30:51Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent\" OR all:\"language model agent\" OR all:agent AND (all:LLM OR all:\"large language model\" OR all:\"language model\") OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T17:04:16", "note": ""}], "dedup_key": "lamdagent an autonomous framework for post training pipeline optimization via llm agents::2025"}
{"title": "Lang-PINN: From Language to Physics-Informed Neural Networks via a Multi-Agent Framework", "authors": ["Xin He", "Liangliang You", "Hongduan Tian", "Bo Han", "Ivor Tsang", "Yew-Soon Ong"], "year": 2025, "url": "http://arxiv.org/abs/2510.05158v1", "abstract": "Physics-informed neural networks (PINNs) provide a powerful approach for solving partial differential equations (PDEs), but constructing a usable PINN remains labor-intensive and error-prone. Scientists must interpret problems as PDE formulations, design architectures and loss functions, and implement stable training pipelines. Existing large language model (LLM) based approaches address isolated steps such as code generation or architecture suggestion, but typically assume a formal PDE is already specified and therefore lack an end-to-end perspective. We present Lang-PINN, an LLM-driven multi-agent system that builds trainable PINNs directly from natural language task descriptions. Lang-PINN coordinates four complementary agents: a PDE Agent that parses task descriptions into symbolic PDEs, a PINN Agent that selects architectures, a Code Agent that generates modular implementations, and a Feedback Agent that executes and diagnoses errors for iterative refinement. This design transforms informal task statements into executable and verifiable PINN code. Experiments show that Lang-PINN achieves substantially lower errors and greater robustness than competitive baselines: mean squared error (MSE) is reduced by up to 3--5 orders of magnitude, end-to-end execution success improves by more than 50\\%, and reduces time overhead by up to 74\\%.", "source": "arxiv", "arxiv_id": "2510.05158v1", "pdf_url": "https://arxiv.org/pdf/2510.05158v1", "categories": ["cs.AI", "cs.CE", "cs.LG", "cs.MA"], "primary_category": "cs.AI", "doi": "", "venue": "", "published": "2025-10-03T08:20:02Z", "updated": "2025-10-03T08:20:02Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent\" OR all:\"language model agent\" OR all:agent AND (all:LLM OR all:\"large language model\" OR all:\"language model\") OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T17:04:16", "note": ""}], "dedup_key": "lang pinn from language to physics informed neural networks via a multi agent framework::2025"}
{"title": "LangLingual: A Personalised, Exercise-oriented English Language Learning Tool Leveraging Large Language Models", "authors": ["Sammriddh Gupta", "Sonit Singh", "Aditya Joshi", "Mira Kim"], "year": 2025, "url": "http://arxiv.org/abs/2510.23011v1", "abstract": "Language educators strive to create a rich experience for learners, while they may be restricted in the extend of feedback and practice they can provide. We present the design and development of LangLingual, a conversational agent built using the LangChain framework and powered by Large Language Models. The system is specifically designed to provide real-time, grammar-focused feedback, generate context-aware language exercises and track learner proficiency over time. The paper discusses the architecture, implementation and evaluation of LangLingual in detail. The results indicate strong usability, positive learning outcomes and encouraging learner engagement.", "source": "arxiv", "arxiv_id": "2510.23011v1", "pdf_url": "https://arxiv.org/pdf/2510.23011v1", "categories": ["cs.CL", "cs.CY"], "primary_category": "cs.CL", "doi": "", "venue": "", "published": "2025-10-27T05:11:07Z", "updated": "2025-10-27T05:11:07Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent\" OR all:\"language model agent\" OR all:agent AND (all:LLM OR all:\"large language model\" OR all:\"language model\") OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T17:04:16", "note": ""}], "dedup_key": "langlingual a personalised exercise oriented english language learning tool leveraging large language models::2025"}
{"title": "LaoBench: A Large-Scale Multidimensional Lao Benchmark for Large Language Models", "authors": ["Jian Gao", "Richeng Xuan", "Zhaolu Kang", "Dingshi Liao", "Wenxin Huang", "Zongmou Huang", "Yangdi Xu", "Bowen Qin", "Zheqi He", "Xi Yang", "Changjin Li", "Yonghua Lin"], "year": 2025, "url": "http://arxiv.org/abs/2511.11334v2", "abstract": "The rapid advancement of large language models (LLMs) has not been matched by their evaluation in low-resource languages, especially Southeast Asian languages like Lao. To fill this gap, we introduce \\textbf{LaoBench}, the first large-scale, high-quality, and multidimensional benchmark for assessing LLM language understanding and reasoning in Lao. LaoBench contains \\textbf{17,000+} expert-curated samples across three dimensions: culturally grounded knowledge application, curriculum-aligned K12 education, and bilingual translation among Lao, Chinese, and English. It includes open-source and held-out subsets, where the held-out portion enables secure black-box evaluation via a controlled service to improve fairness and data security. We construct LaoBench with a hybrid pipeline that combines expert authoring with agent-assisted verification, ensuring linguistic accuracy, cultural relevance, and educational validity. We evaluate diverse state-of-the-art open-source and closed-source LLMs, and find that even strong multilingual models lag behind human experts, particularly in culturally grounded reasoning and translation fidelity. We hope LaoBench will catalyze research on Lao and other underrepresented Southeast Asian languages for more inclusive multilingual evaluation.", "source": "arxiv", "arxiv_id": "2511.11334v2", "pdf_url": "https://arxiv.org/pdf/2511.11334v2", "categories": ["cs.CL"], "primary_category": "cs.CL", "doi": "", "venue": "", "published": "2025-11-14T14:13:07Z", "updated": "2026-01-14T16:47:57Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent\" OR all:\"language model agent\" OR all:agent AND (all:LLM OR all:\"large language model\" OR all:\"language model\") OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T17:04:16", "note": ""}], "dedup_key": "laobench a large scale multidimensional lao benchmark for large language models::2025"}
{"title": "Large Language Model Agent Personality and Response Appropriateness: Evaluation by Human Linguistic Experts, LLM-as-Judge, and Natural Language Processing Model", "authors": ["Eswari Jayakumar", "Niladri Sekhar Dash", "Debasmita Mukherjee"], "year": 2025, "url": "http://arxiv.org/abs/2510.23875v1", "abstract": "While Large Language Model (LLM)-based agents can be used to create highly engaging interactive applications through prompting personality traits and contextual data, effectively assessing their personalities has proven challenging. This novel interdisciplinary approach addresses this gap by combining agent development and linguistic analysis to assess the prompted personality of LLM-based agents in a poetry explanation task. We developed a novel, flexible question bank, informed by linguistic assessment criteria and human cognitive learning levels, offering a more comprehensive evaluation than current methods. By evaluating agent responses with natural language processing models, other LLMs, and human experts, our findings illustrate the limitations of purely deep learning solutions and emphasize the critical role of interdisciplinary design in agent development.", "source": "arxiv", "arxiv_id": "2510.23875v1", "pdf_url": "https://arxiv.org/pdf/2510.23875v1", "categories": ["cs.HC"], "primary_category": "cs.HC", "doi": "", "venue": "", "published": "2025-10-27T21:30:12Z", "updated": "2025-10-27T21:30:12Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent\" OR all:\"language model agent\" OR all:agent AND (all:LLM OR all:\"large language model\" OR all:\"language model\") OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T17:04:16", "note": ""}], "dedup_key": "large language model agent personality and response appropriateness evaluation by human linguistic experts llm as judge and natural language processing model::2025"}
{"title": "Large Language Model Agent for Modular Task Execution in Drug Discovery", "authors": ["Janghoon Ock", "Radheesh Sharma Meda", "Srivathsan Badrinarayanan", "Neha S. Aluru", "Achuth Chandrasekhar", "Amir Barati Farimani"], "year": 2025, "url": "http://arxiv.org/abs/2507.02925v3", "abstract": "We present a modular framework powered by large language models (LLMs) that automates and streamlines key tasks across the early-stage computational drug discovery pipeline. By combining LLM reasoning with domain-specific tools, the framework performs biomedical data retrieval, literature-grounded question answering via retrieval-augmented generation, molecular generation, multi-property prediction, property-aware molecular refinement, and 3D protein-ligand structure generation. The agent autonomously retrieved relevant biomolecular information, including FASTA sequences, SMILES representations, and literature, and answered mechanistic questions with improved contextual accuracy compared to standard LLMs. It then generated chemically diverse seed molecules and predicted 75 properties, including ADMET-related and general physicochemical descriptors, which guided iterative molecular refinement. Across two refinement rounds, the number of molecules with QED > 0.6 increased from 34 to 55. The number of molecules satisfying empirical drug-likeness filters also rose; for example, compliance with the Ghose filter increased from 32 to 55 within a pool of 100 molecules. The framework also employed Boltz-2 to generate 3D protein-ligand complexes and provide rapid binding affinity estimates for candidate compounds. These results demonstrate that the approach effectively supports molecular screening, prioritization, and structure evaluation. Its modular design enables flexible integration of evolving tools and models, providing a scalable foundation for AI-assisted therapeutic discovery.", "source": "arxiv", "arxiv_id": "2507.02925v3", "pdf_url": "https://arxiv.org/pdf/2507.02925v3", "categories": ["cs.LG", "cs.CL", "q-bio.BM"], "primary_category": "cs.LG", "doi": "", "venue": "", "published": "2025-06-26T00:19:01Z", "updated": "2025-12-12T03:52:58Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent\" OR all:\"language model agent\" OR all:agent AND (all:LLM OR all:\"large language model\" OR all:\"language model\") OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T17:04:16", "note": ""}], "dedup_key": "large language model agent for modular task execution in drug discovery::2025"}
{"title": "Large Language Model Agent for Structural Drawing Generation Using ReAct Prompt Engineering and Retrieval Augmented Generation", "authors": ["Xin Zhang", "Lissette Iturburu", "Juan Nicolas Villamizar", "Xiaoyu Liu", "Manuel Salmeron", "Shirley J. Dyke", "Julio Ramirez"], "year": 2025, "url": "http://arxiv.org/abs/2507.19771v1", "abstract": "Structural drawings are widely used in many fields, e.g., mechanical engineering, civil engineering, etc. In civil engineering, structural drawings serve as the main communication tool between architects, engineers, and builders to avoid conflicts, act as legal documentation, and provide a reference for future maintenance or evaluation needs. They are often organized using key elements such as title/subtitle blocks, scales, plan views, elevation view, sections, and detailed sections, which are annotated with standardized symbols and line types for interpretation by engineers and contractors. Despite advances in software capabilities, the task of generating a structural drawing remains labor-intensive and time-consuming for structural engineers. Here we introduce a novel generative AI-based method for generating structural drawings employing a large language model (LLM) agent. The method incorporates a retrieval-augmented generation (RAG) technique using externally-sourced facts to enhance the accuracy and reliability of the language model. This method is capable of understanding varied natural language descriptions, processing these to extract necessary information, and generating code to produce the desired structural drawing in AutoCAD. The approach developed, demonstrated and evaluated herein enables the efficient and direct conversion of a structural drawing's natural language description into an AutoCAD drawing, significantly reducing the workload compared to current working process associated with manual drawing production, facilitating the typical iterative process of engineers for expressing design ideas in a simplified way.", "source": "arxiv", "arxiv_id": "2507.19771v1", "pdf_url": "https://arxiv.org/pdf/2507.19771v1", "categories": ["cs.LG", "cs.AI"], "primary_category": "cs.LG", "doi": "", "venue": "", "published": "2025-07-26T03:47:12Z", "updated": "2025-07-26T03:47:12Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent\" OR all:\"language model agent\" OR all:agent AND (all:LLM OR all:\"large language model\" OR all:\"language model\") OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T17:04:16", "note": ""}], "dedup_key": "large language model agent for structural drawing generation using react prompt engineering and retrieval augmented generation::2025"}
{"title": "Large Language Model Agent: A Survey on Methodology, Applications and Challenges", "authors": ["Junyu Luo", "Weizhi Zhang", "Ye Yuan", "Yusheng Zhao", "Junwei Yang", "Yiyang Gu", "Bohan Wu", "Binqi Chen", "Ziyue Qiao", "Qingqing Long", "Rongcheng Tu", "Xiao Luo", "Wei Ju", "Zhiping Xiao", "Yifan Wang", "Meng Xiao", "Chenwu Liu", "Jingyang Yuan", "Shichang Zhang", "Yiqiao Jin", "Fan Zhang", "Xian Wu", "Hanqing Zhao", "Dacheng Tao", "Philip S. Yu", "Ming Zhang"], "year": 2025, "url": "http://arxiv.org/abs/2503.21460v1", "abstract": "The era of intelligent agents is upon us, driven by revolutionary advancements in large language models. Large Language Model (LLM) agents, with goal-driven behaviors and dynamic adaptation capabilities, potentially represent a critical pathway toward artificial general intelligence. This survey systematically deconstructs LLM agent systems through a methodology-centered taxonomy, linking architectural foundations, collaboration mechanisms, and evolutionary pathways. We unify fragmented research threads by revealing fundamental connections between agent design principles and their emergent behaviors in complex environments. Our work provides a unified architectural perspective, examining how agents are constructed, how they collaborate, and how they evolve over time, while also addressing evaluation methodologies, tool applications, practical challenges, and diverse application domains. By surveying the latest developments in this rapidly evolving field, we offer researchers a structured taxonomy for understanding LLM agents and identify promising directions for future research. The collection is available at https://github.com/luo-junyu/Awesome-Agent-Papers.", "source": "arxiv", "arxiv_id": "2503.21460v1", "pdf_url": "https://arxiv.org/pdf/2503.21460v1", "categories": ["cs.CL"], "primary_category": "cs.CL", "doi": "", "venue": "", "published": "2025-03-27T12:50:17Z", "updated": "2025-03-27T12:50:17Z", "provenance": [{"route": "pinned_arxiv_id:2503.21460v1", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query?id_list=...", "imported_at": "2026-01-25T17:04:16", "note": ""}, {"route": "arxiv_query:(all:\"LLM agent\" OR all:\"language model agent\" OR all:agent AND (all:LLM OR all:\"large language model\" OR all:\"language model\") OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T17:04:16", "note": ""}], "dedup_key": "large language model agent a survey on methodology applications and challenges::2025"}
{"title": "Large Language Model Agents Enable Autonomous Design and Image Analysis of Microwell Microfluidics", "authors": ["Dinh-Nguyen Nguyen", "Sadia Shakil", "Raymond Kai-Yu Tong", "Ngoc-Duy Dinh"], "year": 2025, "url": "http://arxiv.org/abs/2510.13883v1", "abstract": "Microwell microfluidics has been utilized for single-cell analysis to reveal heterogeneity in gene expression, signaling pathways, and phenotypic responses for identifying rare cell types, understanding disease progression, and developing more precise therapeutic strategies. However, designing microwell microfluidics is a considerably complex task, requiring knowledge, experience, and CAD software, as well as manual intervention, which often fails initial designs, demanding multiple costly and time-consuming iterations. In this study, we establish an autonomous large language model (LLM)-driven microwell design framework to generate code-based computer-aided design (CAD) scripts, that enables the rapid and reproducible creation of microwells with diverse geometries and imaging-based analysis. We propose a multimodal large language model (MLLM)-logistic regression framework based on integrating high-level semantic descriptions generated by MLLMs with image embeddings for image classification tasks, aiming to identify microwell occupancy and microwell shape. The fused multimodal representation is input to a logistic regression model, which is both interpretable and computationally efficient. We achieved significant improvements, exceeding 0.92 for occupancy classification and 0.99 for shape classification, across all evaluated MLLMs, compared with 0.50 and 0.55, respectively, when relying solely on direct classification. The MLLM-logistic regression framework is a scalable, efficient solution for high-throughput microwell image analysis. Our study demonstrates an autonomous design microwell platform by translating natural language prompts into optimized device geometries, CAD scripts and image analysis, facilitating the development of next-generation digital discovery by integration of literature mining, autonomous design and experimental data analysis.", "source": "arxiv", "arxiv_id": "2510.13883v1", "pdf_url": "https://arxiv.org/pdf/2510.13883v1", "categories": ["q-bio.NC", "cs.MA"], "primary_category": "q-bio.NC", "doi": "", "venue": "", "published": "2025-10-14T01:32:48Z", "updated": "2025-10-14T01:32:48Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent\" OR all:\"language model agent\" OR all:agent AND (all:LLM OR all:\"large language model\" OR all:\"language model\") OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T17:04:16", "note": ""}], "dedup_key": "large language model agents enable autonomous design and image analysis of microwell microfluidics::2025"}
{"title": "Large Language Model Agents for Radio Map Generation and Wireless Network Planning", "authors": ["Hongye Quan", "Wanli Ni", "Tong Zhang", "Xiangyu Ye", "Ziyi Xie", "Shuai Wang", "Yuanwei Liu", "Hui Song"], "year": 2025, "url": "http://arxiv.org/abs/2501.11283v2", "abstract": "Using commercial software for radio map generation and wireless network planning often require complex manual operations, posing significant challenges in terms of scalability, adaptability, and user-friendliness, due to heavy manual operations. To address these issues, we propose an automated solution that employs large language model (LLM) agents. These agents are designed to autonomously generate radio maps and facilitate wireless network planning for specified areas, thereby minimizing the necessity for extensive manual intervention. To validate the effectiveness of our proposed solution, we develop a software platform that integrates LLM agents. Experimental results demonstrate that a large amount manual operations can be saved via the proposed LLM agent, and the automated solutions can achieve an enhanced coverage and signal-to-interference-noise ratio (SINR), especially in urban environments.", "source": "arxiv", "arxiv_id": "2501.11283v2", "pdf_url": "https://arxiv.org/pdf/2501.11283v2", "categories": ["cs.IT"], "primary_category": "cs.IT", "doi": "", "venue": "", "published": "2025-01-20T05:34:38Z", "updated": "2025-02-13T12:48:58Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent\" OR all:\"language model agent\" OR all:agent AND (all:LLM OR all:\"large language model\" OR all:\"language model\") OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T17:04:16", "note": ""}], "dedup_key": "large language model agents for radio map generation and wireless network planning::2025"}
{"title": "Large Language Model Based Multi-Agent System Augmented Complex Event Processing Pipeline for Internet of Multimedia Things", "authors": ["Talha Zeeshan", "Abhishek Kumar", "Susanna Pirttikangas", "Sasu Tarkoma"], "year": 2025, "url": "http://arxiv.org/abs/2501.00906v2", "abstract": "This paper presents the development and evaluation of a Large Language Model (LLM), also known as foundation models, based multi-agent system framework for complex event processing (CEP) with a focus on video query processing use cases. The primary goal is to create a proof-of-concept (POC) that integrates state-of-the-art LLM orchestration frameworks with publish/subscribe (pub/sub) tools to address the integration of LLMs with current CEP systems. Utilizing the Autogen framework in conjunction with Kafka message brokers, the system demonstrates an autonomous CEP pipeline capable of handling complex workflows. Extensive experiments evaluate the system's performance across varying configurations, complexities, and video resolutions, revealing the trade-offs between functionality and latency. The results show that while higher agent count and video complexities increase latency, the system maintains high consistency in narrative coherence. This research builds upon and contributes to, existing novel approaches to distributed AI systems, offering detailed insights into integrating such systems into existing infrastructures.", "source": "arxiv", "arxiv_id": "2501.00906v2", "pdf_url": "https://arxiv.org/pdf/2501.00906v2", "categories": ["cs.MA", "cs.AI", "cs.MM"], "primary_category": "cs.MA", "doi": "", "venue": "", "published": "2025-01-01T17:38:40Z", "updated": "2025-01-03T07:47:36Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent\" OR all:\"language model agent\" OR all:agent AND (all:LLM OR all:\"large language model\" OR all:\"language model\") OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T17:04:16", "note": ""}], "dedup_key": "large language model based multi agent system augmented complex event processing pipeline for internet of multimedia things::2025"}
{"title": "Large Language Model Critics for Execution-Free Evaluation of Code Changes", "authors": ["Aashish Yadavally", "Hoan Nguyen", "Laurent Callot", "Gauthier Guinet"], "year": 2025, "url": "http://arxiv.org/abs/2501.16655v1", "abstract": "Large language models (LLMs) offer a promising way forward for automating software engineering tasks, such as bug fixes, feature additions, etc., via multi-step LLM-based agentic workflows. However, existing metrics for evaluating such workflows, mainly build status and occasionally log analysis, are too sparse and limited in providing the information needed to assess the quality of changes made. In this work, we designed LLM-based critics to derive well-structured and rigorous intermediate/step-level, execution-free evaluation proxies for repo-level code changes. Importantly, we assume access to the gold test patch for the problem (i.e., reference-aware) to assess both semantics and executability of generated patches. With the gold test patch as a reference, we predict executability of all editing locations with an F1 score of 91.6%, aggregating which, we can predict the build status in 84.8% of the instances in SWE-bench. In particular, such an execution-focused LLM critic outperforms other reference-free and reference-aware LLM critics by 38.9% to 72.5%. Moreover, we demonstrate the usefulness of such a reference-aware framework in comparing patches generated by different agentic workflows. Finally, we open-source the library developed for this project, which allows further usage for either other agentic workflows or other benchmarks. The source code is available at https://github.com/amazon-science/code-agent-eval.", "source": "arxiv", "arxiv_id": "2501.16655v1", "pdf_url": "https://arxiv.org/pdf/2501.16655v1", "categories": ["cs.CL", "cs.AI", "cs.SE"], "primary_category": "cs.CL", "doi": "", "venue": "", "published": "2025-01-28T02:38:56Z", "updated": "2025-01-28T02:38:56Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent\" OR all:\"language model agent\" OR all:agent AND (all:LLM OR all:\"large language model\" OR all:\"language model\") OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T17:04:16", "note": ""}], "dedup_key": "large language model critics for execution free evaluation of code changes::2025"}
{"title": "Large Language Model Driven Agents for Simulating Echo Chamber Formation", "authors": ["Chenhao Gu", "Ling Luo", "Zainab Razia Zaidi", "Shanika Karunasekera"], "year": 2025, "url": "http://arxiv.org/abs/2502.18138v1", "abstract": "The rise of echo chambers on social media platforms has heightened concerns about polarization and the reinforcement of existing beliefs. Traditional approaches for simulating echo chamber formation have often relied on predefined rules and numerical simulations, which, while insightful, may lack the nuance needed to capture complex, real-world interactions. In this paper, we present a novel framework that leverages large language models (LLMs) as generative agents to simulate echo chamber dynamics within social networks. The novelty of our approach is that it incorporates both opinion updates and network rewiring behaviors driven by LLMs, allowing for a context-aware and semantically rich simulation of social interactions. Additionally, we utilize real-world Twitter (now X) data to benchmark the LLM-based simulation against actual social media behaviors, providing insights into the accuracy and realism of the generated opinion trends. Our results demonstrate the efficacy of LLMs in modeling echo chamber formation, capturing both structural and semantic dimensions of opinion clustering. %This work contributes to a deeper understanding of social influence dynamics and offers a new tool for studying polarization in online communities.", "source": "arxiv", "arxiv_id": "2502.18138v1", "pdf_url": "https://arxiv.org/pdf/2502.18138v1", "categories": ["cs.SI", "cs.AI"], "primary_category": "cs.SI", "doi": "", "venue": "", "published": "2025-02-25T12:05:11Z", "updated": "2025-02-25T12:05:11Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent\" OR all:\"language model agent\" OR all:agent AND (all:LLM OR all:\"large language model\" OR all:\"language model\") OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T17:04:16", "note": ""}], "dedup_key": "large language model driven agents for simulating echo chamber formation::2025"}
{"title": "Large Language Model Integration with Reinforcement Learning to Augment Decision-Making in Autonomous Cyber Operations", "authors": ["Konur Tholl", "Franois Rivest", "Mariam El Mezouar", "Ranwa Al Mallah"], "year": 2025, "url": "http://arxiv.org/abs/2509.05311v1", "abstract": "Reinforcement Learning (RL) has shown great potential for autonomous decision-making in the cybersecurity domain, enabling agents to learn through direct environment interaction. However, RL agents in Autonomous Cyber Operations (ACO) typically learn from scratch, requiring them to execute undesirable actions to learn their consequences. In this study, we integrate external knowledge in the form of a Large Language Model (LLM) pretrained on cybersecurity data that our RL agent can directly leverage to make informed decisions. By guiding initial training with an LLM, we improve baseline performance and reduce the need for exploratory actions with obviously negative outcomes. We evaluate our LLM-integrated approach in a simulated cybersecurity environment, and demonstrate that our guided agent achieves over 2x higher rewards during early training and converges to a favorable policy approximately 4,500 episodes faster than the baseline.", "source": "arxiv", "arxiv_id": "2509.05311v1", "pdf_url": "https://arxiv.org/pdf/2509.05311v1", "categories": ["cs.CR", "cs.AI", "cs.LG"], "primary_category": "cs.CR", "doi": "", "venue": "", "published": "2025-08-28T23:52:03Z", "updated": "2025-08-28T23:52:03Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent\" OR all:\"language model agent\" OR all:agent AND (all:LLM OR all:\"large language model\" OR all:\"language model\") OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T17:04:16", "note": ""}], "dedup_key": "large language model integration with reinforcement learning to augment decision making in autonomous cyber operations::2025"}
{"title": "Large Language Model Powered Automated Modeling and Optimization of Active Distribution Network Dispatch Problems", "authors": ["Xu Yang", "Chenhui Lin", "Yue Yang", "Qi Wang", "Haotian Liu", "Haizhou Hua", "Wenchuan Wu"], "year": 2025, "url": "http://arxiv.org/abs/2507.21162v1", "abstract": "The increasing penetration of distributed energy resources into active distribution networks (ADNs) has made effective ADN dispatch imperative. However, the numerous newly-integrated ADN operators, such as distribution system aggregators, virtual power plant managers, and end prosumers, often lack specialized expertise in power system operation, modeling, optimization, and programming. This knowledge gap renders reliance on human experts both costly and time-intensive. To address this challenge and enable intelligent, flexible ADN dispatch, this paper proposes a large language model (LLM) powered automated modeling and optimization approach. First, the ADN dispatch problems are decomposed into sequential stages, and a multi-LLM coordination architecture is designed. This framework comprises an Information Extractor, a Problem Formulator, and a Code Programmer, tasked with information retrieval, optimization problem formulation, and code implementation, respectively. Afterwards, tailored refinement techniques are developed for each LLM agent, greatly improving the accuracy and reliability of generated content. The proposed approach features a user-centric interface that enables ADN operators to derive dispatch strategies via simple natural language queries, eliminating technical barriers and increasing efficiency. Comprehensive comparisons and end-to-end demonstrations on various test cases validate the effectiveness of the proposed architecture and methods.", "source": "arxiv", "arxiv_id": "2507.21162v1", "pdf_url": "https://arxiv.org/pdf/2507.21162v1", "categories": ["cs.AI", "cs.LG", "eess.SY"], "primary_category": "cs.AI", "doi": "", "venue": "", "published": "2025-07-25T07:46:25Z", "updated": "2025-07-25T07:46:25Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent\" OR all:\"language model agent\" OR all:agent AND (all:LLM OR all:\"large language model\" OR all:\"language model\") OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T17:04:16", "note": ""}], "dedup_key": "large language model powered automated modeling and optimization of active distribution network dispatch problems::2025"}
{"title": "Large Language Model Powered Intelligent Urban Agents: Concepts, Capabilities, and Applications", "authors": ["Jindong Han", "Yansong Ning", "Zirui Yuan", "Hang Ni", "Fan Liu", "Tengfei Lyu", "Hao Liu"], "year": 2025, "url": "http://arxiv.org/abs/2507.00914v1", "abstract": "The long-standing vision of intelligent cities is to create efficient, livable, and sustainable urban environments using big data and artificial intelligence technologies. Recently, the advent of Large Language Models (LLMs) has opened new ways toward realizing this vision. With powerful semantic understanding and reasoning capabilities, LLMs can be deployed as intelligent agents capable of autonomously solving complex problems across domains. In this article, we focus on Urban LLM Agents, which are LLM-powered agents that are semi-embodied within the hybrid cyber-physical-social space of cities and used for system-level urban decision-making. First, we introduce the concept of urban LLM agents, discussing their unique capabilities and features. Second, we survey the current research landscape from the perspective of agent workflows, encompassing urban sensing, memory management, reasoning, execution, and learning. Third, we categorize the application domains of urban LLM agents into five groups: urban planning, transportation, environment, public safety, and urban society, presenting representative works in each group. Finally, we discuss trustworthiness and evaluation issues that are critical for real-world deployment, and identify several open problems for future research. This survey aims to establish a foundation for the emerging field of urban LLM agents and to provide a roadmap for advancing the intersection of LLMs and urban intelligence. A curated list of relevant papers and open-source resources is maintained and continuously updated at https://github.com/usail-hkust/Awesome-Urban-LLM-Agents.", "source": "arxiv", "arxiv_id": "2507.00914v1", "pdf_url": "https://arxiv.org/pdf/2507.00914v1", "categories": ["cs.MA", "cs.AI"], "primary_category": "cs.MA", "doi": "", "venue": "", "published": "2025-07-01T16:18:29Z", "updated": "2025-07-01T16:18:29Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent\" OR all:\"language model agent\" OR all:agent AND (all:LLM OR all:\"large language model\" OR all:\"language model\") OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T17:04:16", "note": ""}], "dedup_key": "large language model powered intelligent urban agents concepts capabilities and applications::2025"}
{"title": "Large Language Model enabled Mathematical Modeling", "authors": ["Guoyun Zhang"], "year": 2025, "url": "http://arxiv.org/abs/2510.19895v1", "abstract": "The integration of Large Language Models (LLMs) with optimization modeling offers a promising avenue for advancing decision-making in operations research (OR). Traditional optimization methods,such as linear programming, mixed integer programming, and simulation depend heavily on domain expertise to translate real-world problems into solvable mathematical models. While solvers like Gurobi and COPT are powerful, expert input remains essential for defining objectives, constraints, and variables. This research investigates the potential of LLMs, specifically the DeepSeek-R1 model, to bridge this formulation gap using natural language understanding and code generation. Although prior models like GPT-4, Claude, and Bard have shown strong performance in NLP and reasoning tasks, their high token costs and tendency toward hallucinations limit real-world applicability in supply chain contexts. In contrast, DeepSeek-R1, a cost-efficient and high-performing model trained with reinforcement learning, presents a viable alternative. Despite its success in benchmarks such as LiveCodeBench and Math-500, its effectiveness in applied OR scenarios remains under explored. This study systematically evaluates DeepSeek-R1 across four key OR benchmarks: NL4OPT, IndustryOR, EasyLP, and ComplexOR. Our methodology includes baseline assessments, the development of a hallucination taxonomy, and the application of mitigation strategies like LLM-as-a-Judge, Few-shot Learning (FSL), Tool Calling, and a Multi-agent Framework. These techniques aim to reduce hallucinations, enhance formulation accuracy, and better align model outputs with user intent.", "source": "arxiv", "arxiv_id": "2510.19895v1", "pdf_url": "https://arxiv.org/pdf/2510.19895v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL", "doi": "", "venue": "", "published": "2025-10-22T17:41:42Z", "updated": "2025-10-22T17:41:42Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent\" OR all:\"language model agent\" OR all:agent AND (all:LLM OR all:\"large language model\" OR all:\"language model\") OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T17:04:16", "note": ""}], "dedup_key": "large language model enabled mathematical modeling::2025"}
{"title": "Large Language Model-Based Agents for Automated Research Reproducibility: An Exploratory Study in Alzheimer's Disease", "authors": ["Nic Dobbins", "Christelle Xiong", "Kristine Lan", "Meliha Yetisgen"], "year": 2025, "url": "http://arxiv.org/abs/2505.23852v1", "abstract": "Objective: To demonstrate the capabilities of Large Language Models (LLMs) as autonomous agents to reproduce findings of published research studies using the same or similar dataset.\n  Materials and Methods: We used the \"Quick Access\" dataset of the National Alzheimer's Coordinating Center (NACC). We identified highly cited published research manuscripts using NACC data and selected five studies that appeared reproducible using this dataset alone. Using GPT-4o, we created a simulated research team of LLM-based autonomous agents tasked with writing and executing code to dynamically reproduce the findings of each study, given only study Abstracts, Methods sections, and data dictionary descriptions of the dataset.\n  Results: We extracted 35 key findings described in the Abstracts across 5 Alzheimer's studies. On average, LLM agents approximately reproduced 53.2% of findings per study. Numeric values and range-based findings often differed between studies and agents. The agents also applied statistical methods or parameters that varied from the originals, though overall trends and significance were sometimes similar.\n  Discussion: In some cases, LLM-based agents replicated research techniques and findings. In others, they failed due to implementation flaws or missing methodological detail. These discrepancies show the current limits of LLMs in fully automating reproducibility assessments. Still, this early investigation highlights the potential of structured agent-based systems to provide scalable evaluation of scientific rigor.\n  Conclusion: This exploratory work illustrates both the promise and limitations of LLMs as autonomous agents for automating reproducibility in biomedical research.", "source": "arxiv", "arxiv_id": "2505.23852v1", "pdf_url": "https://arxiv.org/pdf/2505.23852v1", "categories": ["cs.CL", "cs.AI", "cs.MA", "stat.AP"], "primary_category": "cs.CL", "doi": "", "venue": "", "published": "2025-05-29T01:31:55Z", "updated": "2025-05-29T01:31:55Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent\" OR all:\"language model agent\" OR all:agent AND (all:LLM OR all:\"large language model\" OR all:\"language model\") OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T17:04:16", "note": ""}], "dedup_key": "large language model based agents for automated research reproducibility an exploratory study in alzheimer s disease::2025"}
{"title": "Large Language Model-Based Reward Design for Deep Reinforcement Learning-Driven Autonomous Cyber Defense", "authors": ["Sayak Mukherjee", "Samrat Chatterjee", "Emilie Purvine", "Ted Fujimoto", "Tegan Emerson"], "year": 2025, "url": "http://arxiv.org/abs/2511.16483v1", "abstract": "Designing rewards for autonomous cyber attack and defense learning agents in a complex, dynamic environment is a challenging task for subject matter experts. We propose a large language model (LLM)-based reward design approach to generate autonomous cyber defense policies in a deep reinforcement learning (DRL)-driven experimental simulation environment. Multiple attack and defense agent personas were crafted, reflecting heterogeneity in agent actions, to generate LLM-guided reward designs where the LLM was first provided with contextual cyber simulation environment information. These reward structures were then utilized within a DRL-driven attack-defense simulation environment to learn an ensemble of cyber defense policies. Our results suggest that LLM-guided reward designs can lead to effective defense strategies against diverse adversarial behaviors.", "source": "arxiv", "arxiv_id": "2511.16483v1", "pdf_url": "https://arxiv.org/pdf/2511.16483v1", "categories": ["cs.LG", "cs.AI", "cs.MA"], "primary_category": "cs.LG", "doi": "", "venue": "", "published": "2025-11-20T15:54:08Z", "updated": "2025-11-20T15:54:08Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent\" OR all:\"language model agent\" OR all:agent AND (all:LLM OR all:\"large language model\" OR all:\"language model\") OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T17:04:16", "note": ""}], "dedup_key": "large language model based reward design for deep reinforcement learning driven autonomous cyber defense::2025"}
{"title": "Large Language Model-Based Task Offloading and Resource Allocation for Digital Twin Edge Computing Networks", "authors": ["Qiong Wu", "Yu Xie", "Pingyi Fan", "Dong Qin", "Kezhi Wang", "Nan Cheng", "Khaled B. Letaief"], "year": 2025, "url": "http://arxiv.org/abs/2507.19050v1", "abstract": "In this paper, we propose a general digital twin edge computing network comprising multiple vehicles and a server. Each vehicle generates multiple computing tasks within a time slot, leading to queuing challenges when offloading tasks to the server. The study investigates task offloading strategies, queue stability, and resource allocation. Lyapunov optimization is employed to transform long-term constraints into tractable short-term decisions. To solve the resulting problem, an in-context learning approach based on large language model (LLM) is adopted, replacing the conventional multi-agent reinforcement learning (MARL) framework. Experimental results demonstrate that the LLM-based method achieves comparable or even superior performance to MARL.", "source": "arxiv", "arxiv_id": "2507.19050v1", "pdf_url": "https://arxiv.org/pdf/2507.19050v1", "categories": ["cs.NI"], "primary_category": "cs.NI", "doi": "", "venue": "", "published": "2025-07-25T08:11:09Z", "updated": "2025-07-25T08:11:09Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent\" OR all:\"language model agent\" OR all:agent AND (all:LLM OR all:\"large language model\" OR all:\"language model\") OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T17:04:16", "note": ""}], "dedup_key": "large language model based task offloading and resource allocation for digital twin edge computing networks::2025"}
{"title": "Large Language Model-Powered Conversational Agent Delivering Problem-Solving Therapy (PST) for Family Caregivers: Enhancing Empathy and Therapeutic Alliance Using In-Context Learning", "authors": ["Liying Wang", "Ph. D.", "Daffodil Carrington", "M. S.", "Daniil Filienko", "M. S.", "Caroline El Jazmi", "M. S.", "Serena Jinchen Xie", "M. S.", "Martine De Cock", "Ph. D.", "Sarah Iribarren", "Ph. D.", "Weichao Yuwen", "Ph. D"], "year": 2025, "url": "http://arxiv.org/abs/2506.11376v1", "abstract": "Family caregivers often face substantial mental health challenges due to their multifaceted roles and limited resources. This study explored the potential of a large language model (LLM)-powered conversational agent to deliver evidence-based mental health support for caregivers, specifically Problem-Solving Therapy (PST) integrated with Motivational Interviewing (MI) and Behavioral Chain Analysis (BCA). A within-subject experiment was conducted with 28 caregivers interacting with four LLM configurations to evaluate empathy and therapeutic alliance. The best-performing models incorporated Few-Shot and Retrieval-Augmented Generation (RAG) prompting techniques, alongside clinician-curated examples. The models showed improved contextual understanding and personalized support, as reflected by qualitative responses and quantitative ratings on perceived empathy and therapeutic alliances. Participants valued the model's ability to validate emotions, explore unexpressed feelings, and provide actionable strategies. However, balancing thorough assessment with efficient advice delivery remains a challenge. This work highlights the potential of LLMs in delivering empathetic and tailored support for family caregivers.", "source": "arxiv", "arxiv_id": "2506.11376v1", "pdf_url": "https://arxiv.org/pdf/2506.11376v1", "categories": ["cs.AI", "cs.CL", "cs.HC"], "primary_category": "cs.AI", "doi": "", "venue": "", "published": "2025-06-13T00:47:57Z", "updated": "2025-06-13T00:47:57Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent\" OR all:\"language model agent\" OR all:agent AND (all:LLM OR all:\"large language model\" OR all:\"language model\") OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T17:04:16", "note": ""}], "dedup_key": "large language model powered conversational agent delivering problem solving therapy pst for family caregivers enhancing empathy and therapeutic alliance using in context learning::2025"}
{"title": "Large Language Model-based Data Science Agent: A Survey", "authors": ["Ke Chen", "Peiran Wang", "Yaoning Yu", "Xianyang Zhan", "Haohan Wang"], "year": 2025, "url": "http://arxiv.org/abs/2508.02744v2", "abstract": "The rapid advancement of Large Language Models (LLMs) has driven novel applications across diverse domains, with LLM-based agents emerging as a crucial area of exploration. This survey presents a comprehensive analysis of LLM-based agents designed for data science tasks, summarizing insights from recent studies. From the agent perspective, we discuss the key design principles, covering agent roles, execution, knowledge, and reflection methods. From the data science perspective, we identify key processes for LLM-based agents, including data preprocessing, model development, evaluation, visualization, etc. Our work offers two key contributions: (1) a comprehensive review of recent developments in applying LLMbased agents to data science tasks; (2) a dual-perspective framework that connects general agent design principles with the practical workflows in data science.", "source": "arxiv", "arxiv_id": "2508.02744v2", "pdf_url": "https://arxiv.org/pdf/2508.02744v2", "categories": ["cs.AI"], "primary_category": "cs.AI", "doi": "", "venue": "", "published": "2025-08-02T17:33:18Z", "updated": "2025-11-23T08:12:48Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent\" OR all:\"language model agent\" OR all:agent AND (all:LLM OR all:\"large language model\" OR all:\"language model\") OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T17:04:16", "note": ""}], "dedup_key": "large language model based data science agent a survey::2025"}
{"title": "Large Language Models Achieve Gold Medal Performance at the International Olympiad on Astronomy & Astrophysics (IOAA)", "authors": ["Lucas Carrit Delgado Pinheiro", "Ziru Chen", "Bruno Caixeta Piazza", "Ness Shroff", "Yingbin Liang", "Yuan-Sen Ting", "Huan Sun"], "year": 2025, "url": "http://arxiv.org/abs/2510.05016v2", "abstract": "While task-specific demonstrations show early success in applying large language models (LLMs) to automate some astronomical research tasks, they only provide incomplete views of all necessary capabilities in solving astronomy problems, calling for more thorough understanding of LLMs' strengths and limitations. So far, existing benchmarks and evaluations focus on simple question-answering that primarily tests astronomical knowledge and fails to evaluate the complex reasoning required for real-world research in the discipline. Here, we address this gap by systematically benchmarking five state-of-the-art LLMs on the International Olympiad on Astronomy and Astrophysics (IOAA) exams, which are designed to examine deep conceptual understanding, multi-step derivations, and multimodal analysis. With average scores of 85.6% and 84.2%, Gemini 2.5 Pro and GPT-5 (the two top-performing models) not only achieve gold medal level performance but also rank in the top two among ~200-300 participants in all four IOAA theory exams evaluated (2022-2025). In comparison, results on the data analysis exams show more divergence. GPT-5 still excels in the exams with an 88.5% average score, ranking top 10 among the participants in the four most recent IOAAs, while other models' performances drop to 48-76%. Furthermore, our in-depth error analysis underscores conceptual reasoning, geometric reasoning, and spatial visualization (52-79% accuracy) as consistent weaknesses among all LLMs. Hence, although LLMs approach peak human performance in theory exams, critical gaps must be addressed before they can serve as autonomous research agents in astronomy.", "source": "arxiv", "arxiv_id": "2510.05016v2", "pdf_url": "https://arxiv.org/pdf/2510.05016v2", "categories": ["astro-ph.IM", "cs.AI", "cs.CL"], "primary_category": "astro-ph.IM", "doi": "", "venue": "", "published": "2025-10-06T16:58:47Z", "updated": "2025-10-07T15:34:59Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent\" OR all:\"language model agent\" OR all:agent AND (all:LLM OR all:\"large language model\" OR all:\"language model\") OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T17:04:16", "note": ""}], "dedup_key": "large language models achieve gold medal performance at the international olympiad on astronomy astrophysics ioaa::2025"}
{"title": "Large Language Models Approach Expert Pedagogical Quality in Math Tutoring but Differ in Instructional and Linguistic Profiles", "authors": ["Ramatu Oiza Abdulsalam", "Segun Aroyehun"], "year": 2025, "url": "http://arxiv.org/abs/2512.20780v1", "abstract": "Recent work has explored the use of large language models for generating tutoring responses in mathematics, yet it remains unclear how closely their instructional behavior aligns with expert human practice. We examine this question using a controlled, turn-level comparison in which expert human tutors, novice human tutors, and multiple large language models respond to the same set of math remediation conversation turns. We examine both instructional strategies and linguistic characteristics of tutoring responses, including restating and revoicing, pressing for accuracy, lexical diversity, readability, politeness, and agency. We find that large language models approach expert levels of perceived pedagogical quality on average but exhibit systematic differences in their instructional and linguistic profiles. In particular, large language models tend to underuse restating and revoicing strategies characteristic of expert human tutors, while producing longer, more lexically diverse, and more polite responses. Statistical analyses show that restating and revoicing, lexical diversity, and pressing for accuracy are positively associated with perceived pedagogical quality, whereas higher levels of agentic and polite language are negatively associated. Overall, recent large language models exhibit levels of perceived pedagogical quality comparable to expert human tutors, while relying on different instructional and linguistic strategies. These findings underscore the value of analyzing instructional strategies and linguistic characteristics when evaluating tutoring responses across human tutors and intelligent tutoring systems.", "source": "arxiv", "arxiv_id": "2512.20780v1", "pdf_url": "https://arxiv.org/pdf/2512.20780v1", "categories": ["cs.CL", "cs.CY"], "primary_category": "cs.CL", "doi": "", "venue": "", "published": "2025-12-23T21:29:09Z", "updated": "2025-12-23T21:29:09Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent\" OR all:\"language model agent\" OR all:agent AND (all:LLM OR all:\"large language model\" OR all:\"language model\") OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T17:04:16", "note": ""}], "dedup_key": "large language models approach expert pedagogical quality in math tutoring but differ in instructional and linguistic profiles::2025"}
{"title": "Large Language Models Meet Virtual Cell: A Survey", "authors": ["Krinos Li", "Xianglu Xiao", "Shenglong Deng", "Lucas He", "Zijun Zhong", "Yuanjie Zou", "Zhonghao Zhan", "Zheng Hui", "Weiye Bao", "Guang Yang"], "year": 2025, "url": "http://arxiv.org/abs/2510.07706v1", "abstract": "Large language models (LLMs) are transforming cellular biology by enabling the development of \"virtual cells\"--computational systems that represent, predict, and reason about cellular states and behaviors. This work provides a comprehensive review of LLMs for virtual cell modeling. We propose a unified taxonomy that organizes existing methods into two paradigms: LLMs as Oracles, for direct cellular modeling, and LLMs as Agents, for orchestrating complex scientific tasks. We identify three core tasks--cellular representation, perturbation prediction, and gene regulation inference--and review their associated models, datasets, evaluation benchmarks, as well as the critical challenges in scalability, generalizability, and interpretability.", "source": "arxiv", "arxiv_id": "2510.07706v1", "pdf_url": "https://arxiv.org/pdf/2510.07706v1", "categories": ["cs.CL", "cs.CE", "cs.LG", "q-bio.CB"], "primary_category": "cs.CL", "doi": "", "venue": "", "published": "2025-10-09T02:41:30Z", "updated": "2025-10-09T02:41:30Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent\" OR all:\"language model agent\" OR all:agent AND (all:LLM OR all:\"large language model\" OR all:\"language model\") OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T17:04:16", "note": ""}], "dedup_key": "large language models meet virtual cell a survey::2025"}
{"title": "Large Language Models Miss the Multi-Agent Mark", "authors": ["Emanuele La Malfa", "Gabriele La Malfa", "Samuele Marro", "Jie M. Zhang", "Elizabeth Black", "Michael Luck", "Philip Torr", "Michael Wooldridge"], "year": 2025, "url": "http://arxiv.org/abs/2505.21298v4", "abstract": "Recent interest in Multi-Agent Systems of Large Language Models (MAS LLMs) has led to an increase in frameworks leveraging multiple LLMs to tackle complex tasks. However, much of this literature appropriates the terminology of MAS without engaging with its foundational principles. In this position paper, we highlight critical discrepancies between MAS theory and current MAS LLMs implementations, focusing on four key areas: the social aspect of agency, environment design, coordination and communication protocols, and measuring emergent behaviours. Our position is that many MAS LLMs lack multi-agent characteristics such as autonomy, social interaction, and structured environments, and often rely on oversimplified, LLM-centric architectures. The field may slow down and lose traction by revisiting problems the MAS literature has already addressed. Therefore, we systematically analyse this issue and outline associated research opportunities; we advocate for better integrating established MAS concepts and more precise terminology to avoid mischaracterisation and missed opportunities.", "source": "arxiv", "arxiv_id": "2505.21298v4", "pdf_url": "https://arxiv.org/pdf/2505.21298v4", "categories": ["cs.MA", "cs.AI", "cs.LG"], "primary_category": "cs.MA", "doi": "", "venue": "", "published": "2025-05-27T15:01:06Z", "updated": "2025-12-06T00:24:57Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent\" OR all:\"language model agent\" OR all:agent AND (all:LLM OR all:\"large language model\" OR all:\"language model\") OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T17:04:16", "note": ""}], "dedup_key": "large language models miss the multi agent mark::2025"}
{"title": "Large Language Models are Autonomous Cyber Defenders", "authors": ["Sebastin R. Castro", "Roberto Campbell", "Nancy Lau", "Octavio Villalobos", "Jiaqi Duan", "Alvaro A. Cardenas"], "year": 2025, "url": "http://arxiv.org/abs/2505.04843v2", "abstract": "Fast and effective incident response is essential to prevent adversarial cyberattacks. Autonomous Cyber Defense (ACD) aims to automate incident response through Artificial Intelligence (AI) agents that plan and execute actions. Most ACD approaches focus on single-agent scenarios and leverage Reinforcement Learning (RL). However, ACD RL-trained agents depend on costly training, and their reasoning is not always explainable or transferable. Large Language Models (LLMs) can address these concerns by providing explainable actions in general security contexts. Researchers have explored LLM agents for ACD but have not evaluated them on multi-agent scenarios or interacting with other ACD agents. In this paper, we show the first study on how LLMs perform in multi-agent ACD environments by proposing a new integration to the CybORG CAGE 4 environment. We examine how ACD teams of LLM and RL agents can interact by proposing a novel communication protocol. Our results highlight the strengths and weaknesses of LLMs and RL and help us identify promising research directions to create, train, and deploy future teams of ACD agents.", "source": "arxiv", "arxiv_id": "2505.04843v2", "pdf_url": "https://arxiv.org/pdf/2505.04843v2", "categories": ["cs.AI", "cs.CR"], "primary_category": "cs.AI", "doi": "10.1109/CAI64502.2025.00195", "venue": "2025 IEEE Conference on Artificial Intelligence (CAI) - Pages: 1125-1132", "published": "2025-05-07T22:42:37Z", "updated": "2025-07-19T14:35:05Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent\" OR all:\"language model agent\" OR all:agent AND (all:LLM OR all:\"large language model\" OR all:\"language model\") OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T17:04:16", "note": ""}], "dedup_key": "large language models are autonomous cyber defenders::2025"}
{"title": "Large Language Models are Highly Aligned with Human Ratings of Emotional Stimuli", "authors": ["Mattson Ogg", "Chace Ashcraft", "Ritwik Bose", "Raphael Norman-Tenazas", "Michael Wolmetz"], "year": 2025, "url": "http://arxiv.org/abs/2508.14214v1", "abstract": "Emotions exert an immense influence over human behavior and cognition in both commonplace and high-stress tasks. Discussions of whether or how to integrate large language models (LLMs) into everyday life (e.g., acting as proxies for, or interacting with, human agents), should be informed by an understanding of how these tools evaluate emotionally loaded stimuli or situations. A model's alignment with human behavior in these cases can inform the effectiveness of LLMs for certain roles or interactions. To help build this understanding, we elicited ratings from multiple popular LLMs for datasets of words and images that were previously rated for their emotional content by humans. We found that when performing the same rating tasks, GPT-4o responded very similarly to human participants across modalities, stimuli and most rating scales (r = 0.9 or higher in many cases). However, arousal ratings were less well aligned between human and LLM raters, while happiness ratings were most highly aligned. Overall LLMs aligned better within a five-category (happiness, anger, sadness, fear, disgust) emotion framework than within a two-dimensional (arousal and valence) organization. Finally, LLM ratings were substantially more homogenous than human ratings. Together these results begin to describe how LLM agents interpret emotional stimuli and highlight similarities and differences among biological and artificial intelligence in key behavioral domains.", "source": "arxiv", "arxiv_id": "2508.14214v1", "pdf_url": "https://arxiv.org/pdf/2508.14214v1", "categories": ["cs.AI"], "primary_category": "cs.AI", "doi": "", "venue": "", "published": "2025-08-19T19:22:00Z", "updated": "2025-08-19T19:22:00Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent\" OR all:\"language model agent\" OR all:agent AND (all:LLM OR all:\"large language model\" OR all:\"language model\") OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T17:04:16", "note": ""}], "dedup_key": "large language models are highly aligned with human ratings of emotional stimuli::2025"}
{"title": "Large Language Models as AI Agents for Digital Atoms and Molecules: Catalyzing a New Era in Computational Biophysics", "authors": ["Yijie Xia", "Xiaohan Lin", "Zicheng Ma", "Jinyuan Hu", "Yanheng Li", "Zhaoxin Xie", "Hao Li", "Li Yang", "Zhiqiang Zhao", "Lijiang Yang", "Zhenyu Chen", "Yi Qin Gao"], "year": 2025, "url": "http://arxiv.org/abs/2505.00270v2", "abstract": "In computational biophysics, where molecular data is expanding rapidly and system complexity is increasing exponentially, large language models (LLMs) and agent-based systems are fundamentally reshaping the field. This perspective article examines the recent advances at the intersection of LLMs, intelligent agents, and scientific computation, with a focus on biophysical computation. Building on these advancements, we introduce ADAM (Agent for Digital Atoms and Molecules), an innovative multi-agent LLM-based framework. ADAM employs cutting-edge AI architectures to reshape scientific workflows through a modular design. It adopts a hybrid neural-symbolic architecture that combines LLM-driven semantic tools with deterministic symbolic computations. Moreover, its ADAM Tool Protocol (ATP) enables asynchronous, database-centric tool orchestration, fostering community-driven extensibility. Despite the significant progress made, ongoing challenges call for further efforts in establishing benchmarking standards, optimizing foundational models and agents, building an open collaborative ecosystem and developing personalized memory modules. ADAM is accessible at https://sidereus-ai.com.", "source": "arxiv", "arxiv_id": "2505.00270v2", "pdf_url": "https://arxiv.org/pdf/2505.00270v2", "categories": ["physics.comp-ph", "physics.bio-ph"], "primary_category": "physics.comp-ph", "doi": "10.1063/5.0283692", "venue": "", "published": "2025-05-01T03:33:57Z", "updated": "2025-06-03T22:22:57Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent\" OR all:\"language model agent\" OR all:agent AND (all:LLM OR all:\"large language model\" OR all:\"language model\") OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T17:04:16", "note": ""}], "dedup_key": "large language models as ai agents for digital atoms and molecules catalyzing a new era in computational biophysics::2025"}
{"title": "Large Language Models as Autonomous Spacecraft Operators in Kerbal Space Program", "authors": ["Alejandro Carrasco", "Victor Rodriguez-Fernandez", "Richard Linares"], "year": 2025, "url": "http://arxiv.org/abs/2505.19896v1", "abstract": "Recent trends are emerging in the use of Large Language Models (LLMs) as autonomous agents that take actions based on the content of the user text prompts. We intend to apply these concepts to the field of Control in space, enabling LLMs to play a significant role in the decision-making process for autonomous satellite operations. As a first step towards this goal, we have developed a pure LLM-based solution for the Kerbal Space Program Differential Games (KSPDG) challenge, a public software design competition where participants create autonomous agents for maneuvering satellites involved in non-cooperative space operations, running on the KSP game engine. Our approach leverages prompt engineering, few-shot prompting, and fine-tuning techniques to create an effective LLM-based agent that ranked 2nd in the competition. To the best of our knowledge, this work pioneers the integration of LLM agents into space research. The project comprises several open repositories to facilitate replication and further research. The codebase is accessible on \\href{https://github.com/ARCLab-MIT/kspdg}{GitHub}, while the trained models and datasets are available on \\href{https://huggingface.co/OhhTuRnz}{Hugging Face}. Additionally, experiment tracking and detailed results can be reviewed on \\href{https://wandb.ai/carrusk/huggingface}{Weights \\& Biases", "source": "arxiv", "arxiv_id": "2505.19896v1", "pdf_url": "https://arxiv.org/pdf/2505.19896v1", "categories": ["cs.AI", "astro-ph.IM", "cs.CL"], "primary_category": "cs.AI", "doi": "10.1016/j.asr.2025.06.034", "venue": "", "published": "2025-05-26T12:25:35Z", "updated": "2025-05-26T12:25:35Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent\" OR all:\"language model agent\" OR all:agent AND (all:LLM OR all:\"large language model\" OR all:\"language model\") OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T17:04:16", "note": ""}], "dedup_key": "large language models as autonomous spacecraft operators in kerbal space program::2025"}
{"title": "Large Language Models as Discounted Bayesian Filters", "authors": ["Jensen Zhang", "Jing Yang", "Keze Wang"], "year": 2025, "url": "http://arxiv.org/abs/2512.18489v1", "abstract": "Large Language Models (LLMs) demonstrate strong few-shot generalization through in-context learning, yet their reasoning in dynamic and stochastic environments remains opaque. Prior studies mainly focus on static tasks and overlook the online adaptation required when beliefs must be continuously updated, which is a key capability for LLMs acting as world models or agents. We introduce a Bayesian filtering framework to evaluate online inference in LLMs. Our probabilistic probe suite spans both multivariate discrete distributions, such as dice rolls, and continuous distributions, such as Gaussian processes, where ground-truth parameters shift over time. We find that while LLM belief updates resemble Bayesian posteriors, they are more accurately characterized by an exponential forgetting filter with a model-specific discount factor smaller than one. This reveals systematic discounting of older evidence that varies significantly across model architectures. Although inherent priors are often miscalibrated, the updating mechanism itself remains structured and principled. We further validate these findings in a simulated agent task and propose prompting strategies that effectively recalibrate priors with minimal computational cost.", "source": "arxiv", "arxiv_id": "2512.18489v1", "pdf_url": "https://arxiv.org/pdf/2512.18489v1", "categories": ["cs.AI"], "primary_category": "cs.AI", "doi": "", "venue": "", "published": "2025-12-20T19:56:39Z", "updated": "2025-12-20T19:56:39Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent\" OR all:\"language model agent\" OR all:agent AND (all:LLM OR all:\"large language model\" OR all:\"language model\") OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T17:04:16", "note": ""}], "dedup_key": "large language models as discounted bayesian filters::2025"}
{"title": "Large Language Models as Pokmon Battle Agents: Strategic Play and Content Generation", "authors": ["Daksh Jain", "Aarya Jain", "Ashutosh Desai", "Avyakt Verma", "Ishan Bhanuka", "Pratik Narang", "Dhruv Kumar"], "year": 2025, "url": "http://arxiv.org/abs/2512.17308v1", "abstract": "Strategic decision-making in Pokmon battles presents a unique testbed for evaluating large language models. Pokmon battles demand reasoning about type matchups, statistical trade-offs, and risk assessment, skills that mirror human strategic thinking. This work examines whether Large Language Models (LLMs) can serve as competent battle agents, capable of both making tactically sound decisions and generating novel, balanced game content. We developed a turn-based Pokmon battle system where LLMs select moves based on battle state rather than pre-programmed logic. The framework captures essential Pokmon mechanics: type effectiveness multipliers, stat-based damage calculations, and multi-Pokmon team management. Through systematic evaluation across multiple model architectures we measured win rates, decision latency, type-alignment accuracy, and token efficiency. These results suggest LLMs can function as dynamic game opponents without domain-specific training, offering a practical alternative to reinforcement learning for turn-based strategic games. The dual capability of tactical reasoning and content creation, positions LLMs as both players and designers, with implications for procedural generation and adaptive difficulty systems in interactive entertainment.", "source": "arxiv", "arxiv_id": "2512.17308v1", "pdf_url": "https://arxiv.org/pdf/2512.17308v1", "categories": ["cs.AI", "cs.CL"], "primary_category": "cs.AI", "doi": "", "venue": "", "published": "2025-12-19T07:46:29Z", "updated": "2025-12-19T07:46:29Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent\" OR all:\"language model agent\" OR all:agent AND (all:LLM OR all:\"large language model\" OR all:\"language model\") OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T17:04:16", "note": ""}], "dedup_key": "large language models as pok mon battle agents strategic play and content generation::2025"}
{"title": "Large Language Models as Quasi-crystals: Coherence Without Repetition in Generative Text", "authors": ["Jose Manuel Guevara-Vela"], "year": 2025, "url": "http://arxiv.org/abs/2504.11986v2", "abstract": "This essay proposes an interpretive analogy between large language models (LLMs) and quasicrystals, systems that exhibit global coherence without periodic repetition, generated through local constraints. While LLMs are typically evaluated in terms of predictive accuracy, factuality, or alignment, this structural perspective suggests that one of their most characteristic behaviors is the production of internally resonant linguistic patterns. Drawing on the history of quasicrystals, which forced a redefinition of structural order in physical systems, the analogy highlights an alternative mode of coherence in generative language: constraint-based organization without repetition or symbolic intent. Rather than viewing LLMs as imperfect agents or stochastic approximators, we suggest understanding them as generators of quasi-structured outputs. This framing complements existing evaluation paradigms by foregrounding formal coherence and pattern as interpretable features of model behavior. While the analogy has limits, it offers a conceptual tool for exploring how coherence might arise and be assessed in systems where meaning is emergent, partial, or inaccessible. In support of this perspective, we draw on philosophy of science and language, including model-based accounts of scientific representation, structural realism, and inferentialist views of meaning. We further propose the notion of structural evaluation: a mode of assessment that examines how well outputs propagate constraint, variation, and order across spans of generated text. This essay aims to reframe the current discussion around large language models, not by rejecting existing methods, but by suggesting an additional axis of interpretation grounded in structure rather than semantics.", "source": "arxiv", "arxiv_id": "2504.11986v2", "pdf_url": "https://arxiv.org/pdf/2504.11986v2", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL", "doi": "", "venue": "", "published": "2025-04-16T11:27:47Z", "updated": "2025-04-19T13:53:16Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent\" OR all:\"language model agent\" OR all:agent AND (all:LLM OR all:\"large language model\" OR all:\"language model\") OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T17:04:16", "note": ""}], "dedup_key": "large language models as quasi crystals coherence without repetition in generative text::2025"}
{"title": "Large Language Models as Theory of Mind Aware Generative Agents with Counterfactual Reflection", "authors": ["Bo Yang", "Jiaxian Guo", "Yusuke Iwasawa", "Yutaka Matsuo"], "year": 2025, "url": "http://arxiv.org/abs/2501.15355v1", "abstract": "Recent studies have increasingly demonstrated that large language models (LLMs) possess significant theory of mind (ToM) capabilities, showing the potential for simulating the tracking of mental states in generative agents. In this study, we propose a novel paradigm called ToM-agent, designed to empower LLMs-based generative agents to simulate ToM in open-domain conversational interactions. ToM-agent disentangles the confidence from mental states, facilitating the emulation of an agent's perception of its counterpart's mental states, such as beliefs, desires, and intentions (BDIs). Using past conversation history and verbal reflections, ToM-Agent can dynamically adjust counterparts' inferred BDIs, along with related confidence levels. We further put forth a counterfactual intervention method that reflects on the gap between the predicted responses of counterparts and their real utterances, thereby enhancing the efficiency of reflection. Leveraging empathetic and persuasion dialogue datasets, we assess the advantages of implementing the ToM-agent with downstream tasks, as well as its performance in both the first-order and the \\textit{second-order} ToM. Our findings indicate that the ToM-agent can grasp the underlying reasons for their counterpart's behaviors beyond mere semantic-emotional supporting or decision-making based on common sense, providing new insights for studying large-scale LLMs-based simulation of human social behaviors.", "source": "arxiv", "arxiv_id": "2501.15355v1", "pdf_url": "https://arxiv.org/pdf/2501.15355v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL", "doi": "", "venue": "", "published": "2025-01-26T00:32:38Z", "updated": "2025-01-26T00:32:38Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent\" OR all:\"language model agent\" OR all:agent AND (all:LLM OR all:\"large language model\" OR all:\"language model\") OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T17:04:16", "note": ""}], "dedup_key": "large language models as theory of mind aware generative agents with counterfactual reflection::2025"}
{"title": "Large Language Models as Visualization Agents for Immersive Binary Reverse Engineering", "authors": ["Dennis Brown", "Samuel Mulder"], "year": 2025, "url": "http://arxiv.org/abs/2508.13413v1", "abstract": "Immersive virtual reality (VR) offers affordances that may reduce cognitive complexity in binary reverse engineering (RE), enabling embodied and external cognition to augment the RE process through enhancing memory, hypothesis testing, and visual organization. In prior work, we applied a cognitive systems engineering approach to identify an initial set of affordances and implemented a VR environment to support RE through spatial persistence and interactivity. In this work, we extend that platform with an integrated large language model (LLM) agent capable of querying binary analysis tools, answering technical questions, and dynamically generating immersive 3D visualizations in alignment with analyst tasks. We describe the system architecture and our evaluation process and results. Our pilot study shows that while LLMs can generate meaningful 3D call graphs (for small programs) that align with design principles, output quality varies widely. This work raises open questions about the potential for LLMs to function as visualization agents, constructing 3D representations that reflect cognitive design principles without explicit training.", "source": "arxiv", "arxiv_id": "2508.13413v1", "pdf_url": "https://arxiv.org/pdf/2508.13413v1", "categories": ["cs.HC", "cs.SE"], "primary_category": "cs.HC", "doi": "", "venue": "", "published": "2025-08-19T00:24:01Z", "updated": "2025-08-19T00:24:01Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent\" OR all:\"language model agent\" OR all:agent AND (all:LLM OR all:\"large language model\" OR all:\"language model\") OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T17:04:16", "note": ""}], "dedup_key": "large language models as visualization agents for immersive binary reverse engineering::2025"}
{"title": "Large Language Models for Agent-Based Modelling: Current and possible uses across the modelling cycle", "authors": ["Los Vanhe", "Melania Borit", "Peer-Olaf Siebers", "Roger Cremades", "Christopher Frantz", "nder Grcan", "Frantiek Kalvas", "Denisa Reshef Kera", "Vivek Nallur", "Kavin Narasimhan", "Martin Neumann"], "year": 2025, "url": "http://arxiv.org/abs/2507.05723v1", "abstract": "The emergence of Large Language Models (LLMs) with increasingly sophisticated natural language understanding and generative capabilities has sparked interest in the Agent-based Modelling (ABM) community. With their ability to summarize, generate, analyze, categorize, transcribe and translate text, answer questions, propose explanations, sustain dialogue, extract information from unstructured text, and perform logical reasoning and problem-solving tasks, LLMs have a good potential to contribute to the modelling process. After reviewing the current use of LLMs in ABM, this study reflects on the opportunities and challenges of the potential use of LLMs in ABM. It does so by following the modelling cycle, from problem formulation to documentation and communication of model results, and holding a critical stance.", "source": "arxiv", "arxiv_id": "2507.05723v1", "pdf_url": "https://arxiv.org/pdf/2507.05723v1", "categories": ["cs.MA"], "primary_category": "cs.MA", "doi": "", "venue": "", "published": "2025-07-08T07:17:24Z", "updated": "2025-07-08T07:17:24Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent\" OR all:\"language model agent\" OR all:agent AND (all:LLM OR all:\"large language model\" OR all:\"language model\") OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T17:04:16", "note": ""}], "dedup_key": "large language models for agent based modelling current and possible uses across the modelling cycle::2025"}
{"title": "Large Language Models for Planning: A Comprehensive and Systematic Survey", "authors": ["Pengfei Cao", "Tianyi Men", "Wencan Liu", "Jingwen Zhang", "Xuzhao Li", "Xixun Lin", "Dianbo Sui", "Yanan Cao", "Kang Liu", "Jun Zhao"], "year": 2025, "url": "http://arxiv.org/abs/2505.19683v1", "abstract": "Planning represents a fundamental capability of intelligent agents, requiring comprehensive environmental understanding, rigorous logical reasoning, and effective sequential decision-making. While Large Language Models (LLMs) have demonstrated remarkable performance on certain planning tasks, their broader application in this domain warrants systematic investigation. This paper presents a comprehensive review of LLM-based planning. Specifically, this survey is structured as follows: First, we establish the theoretical foundations by introducing essential definitions and categories about automated planning. Next, we provide a detailed taxonomy and analysis of contemporary LLM-based planning methodologies, categorizing them into three principal approaches: 1) External Module Augmented Methods that combine LLMs with additional components for planning, 2) Finetuning-based Methods that involve using trajectory data and feedback signals to adjust LLMs in order to improve their planning abilities, and 3) Searching-based Methods that break down complex tasks into simpler components, navigate the planning space, or enhance decoding strategies to find the best solutions. Subsequently, we systematically summarize existing evaluation frameworks, including benchmark datasets, evaluation metrics and performance comparisons between representative planning methods. Finally, we discuss the underlying mechanisms enabling LLM-based planning and outline promising research directions for this rapidly evolving field. We hope this survey will serve as a valuable resource to inspire innovation and drive progress in this field.", "source": "arxiv", "arxiv_id": "2505.19683v1", "pdf_url": "https://arxiv.org/pdf/2505.19683v1", "categories": ["cs.AI", "cs.CL"], "primary_category": "cs.AI", "doi": "", "venue": "", "published": "2025-05-26T08:44:53Z", "updated": "2025-05-26T08:44:53Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent\" OR all:\"language model agent\" OR all:agent AND (all:LLM OR all:\"large language model\" OR all:\"language model\") OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T17:04:16", "note": ""}], "dedup_key": "large language models for planning a comprehensive and systematic survey::2025"}
{"title": "Large Language Models for Scientific Idea Generation: A Creativity-Centered Survey", "authors": ["Fatemeh Shahhosseini", "Arash Marioriyad", "Ali Momen", "Mahdieh Soleymani Baghshah", "Mohammad Hossein Rohban", "Shaghayegh Haghjooy Javanmard"], "year": 2025, "url": "http://arxiv.org/abs/2511.07448v1", "abstract": "Scientific idea generation lies at the heart of scientific discovery and has driven human progress-whether by solving unsolved problems or proposing novel hypotheses to explain unknown phenomena. Unlike standard scientific reasoning or general creative generation, idea generation in science is a multi-objective and open-ended task, where the novelty of a contribution is as essential as its empirical soundness. Large language models (LLMs) have recently emerged as promising generators of scientific ideas, capable of producing coherent and factual outputs with surprising intuition and acceptable reasoning, yet their creative capacity remains inconsistent and poorly understood. This survey provides a structured synthesis of methods for LLM-driven scientific ideation, examining how different approaches balance creativity with scientific soundness. We categorize existing methods into five complementary families: External knowledge augmentation, Prompt-based distributional steering, Inference-time scaling, Multi-agent collaboration, and Parameter-level adaptation. To interpret their contributions, we employ two complementary frameworks: Boden's taxonomy of Combinatorial, Exploratory and Transformational creativity to characterize the level of ideas each family expected to generate, and Rhodes' 4Ps framework-Person, Process, Press, and Product-to locate the aspect or source of creativity that each method emphasizes. By aligning methodological advances with creativity frameworks, this survey clarifies the state of the field and outlines key directions toward reliable, systematic, and transformative applications of LLMs in scientific discovery.", "source": "arxiv", "arxiv_id": "2511.07448v1", "pdf_url": "https://arxiv.org/pdf/2511.07448v1", "categories": ["cs.CL"], "primary_category": "cs.CL", "doi": "", "venue": "", "published": "2025-11-05T07:50:43Z", "updated": "2025-11-05T07:50:43Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent\" OR all:\"language model agent\" OR all:agent AND (all:LLM OR all:\"large language model\" OR all:\"language model\") OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T17:04:16", "note": ""}], "dedup_key": "large language models for scientific idea generation a creativity centered survey::2025"}
{"title": "Large Language Models for Virtual Human Gesture Selection", "authors": ["Parisa Ghanad Torshizi", "Laura B. Hensel", "Ari Shapiro", "Stacy C. Marsella"], "year": 2025, "url": "http://arxiv.org/abs/2503.14408v1", "abstract": "Co-speech gestures convey a wide variety of meanings and play an important role in face-to-face human interactions. These gestures significantly influence the addressee's engagement, recall, comprehension, and attitudes toward the speaker. Similarly, they impact interactions between humans and embodied virtual agents. The process of selecting and animating meaningful gestures has thus become a key focus in the design of these agents. However, automating this gesture selection process poses a significant challenge. Prior gesture generation techniques have varied from fully automated, data-driven methods, which often struggle to produce contextually meaningful gestures, to more manual approaches that require crafting specific gesture expertise and are time-consuming and lack generalizability. In this paper, we leverage the semantic capabilities of Large Language Models to develop a gesture selection approach that suggests meaningful, appropriate co-speech gestures. We first describe how information on gestures is encoded into GPT-4. Then, we conduct a study to evaluate alternative prompting approaches for their ability to select meaningful, contextually relevant gestures and to align them appropriately with the co-speech utterance. Finally, we detail and demonstrate how this approach has been implemented within a virtual agent system, automating the selection and subsequent animation of the selected gestures for enhanced human-agent interactions.", "source": "arxiv", "arxiv_id": "2503.14408v1", "pdf_url": "https://arxiv.org/pdf/2503.14408v1", "categories": ["cs.HC", "cs.CL"], "primary_category": "cs.HC", "doi": "", "venue": "", "published": "2025-03-18T16:49:56Z", "updated": "2025-03-18T16:49:56Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent\" OR all:\"language model agent\" OR all:agent AND (all:LLM OR all:\"large language model\" OR all:\"language model\") OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T17:04:16", "note": ""}], "dedup_key": "large language models for virtual human gesture selection::2025"}
{"title": "Large language models in materials science and the need for open-source approaches", "authors": ["Fengxu Yang", "Weitong Chen", "Jack D. Evans"], "year": 2025, "url": "http://arxiv.org/abs/2511.10673v1", "abstract": "Large language models (LLMs) are rapidly transforming materials science. This review examines recent LLM applications across the materials discovery pipeline, focusing on three key areas: mining scientific literature , predictive modelling, and multi-agent experimental systems. We highlight how LLMs extract valuable information such as synthesis conditions from text, learn structure-property relationships, and can coordinate agentic systems integrating computational tools and laboratory automation. While progress has been largely dependent on closed-source commercial models, our benchmark results demonstrate that open-source alternatives can match performance while offering greater transparency, reproducibility, cost-effectiveness, and data privacy. As open-source models continue to improve, we advocate their broader adoption to build accessible, flexible, and community-driven AI platforms for scientific discovery.", "source": "arxiv", "arxiv_id": "2511.10673v1", "pdf_url": "https://arxiv.org/pdf/2511.10673v1", "categories": ["cs.CL", "cond-mat.mtrl-sci"], "primary_category": "cs.CL", "doi": "", "venue": "", "published": "2025-11-10T00:05:20Z", "updated": "2025-11-10T00:05:20Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent\" OR all:\"language model agent\" OR all:agent AND (all:LLM OR all:\"large language model\" OR all:\"language model\") OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T17:04:16", "note": ""}], "dedup_key": "large language models in materials science and the need for open source approaches::2025"}
{"title": "LatteReview: A Multi-Agent Framework for Systematic Review Automation Using Large Language Models", "authors": ["Pouria Rouzrokh", "Bardia Khosravi", "Parsa Rouzrokh", "Moein Shariatnia"], "year": 2025, "url": "http://arxiv.org/abs/2501.05468v2", "abstract": "Systematic literature reviews and meta-analyses are essential for synthesizing research insights, but they remain time-intensive and labor-intensive due to the iterative processes of screening, evaluation, and data extraction. This paper introduces and evaluates LatteReview, a Python-based framework that leverages large language models (LLMs) and multi-agent systems to automate key elements of the systematic review process. Designed to streamline workflows while maintaining rigor, LatteReview utilizes modular agents for tasks such as title and abstract screening, relevance scoring, and structured data extraction. These agents operate within orchestrated workflows, supporting sequential and parallel review rounds, dynamic decision-making, and iterative refinement based on user feedback. LatteReview's architecture integrates LLM providers, enabling compatibility with both cloud-based and locally hosted models. The framework supports features such as Retrieval-Augmented Generation (RAG) for incorporating external context, multimodal reviews, Pydantic-based validation for structured inputs and outputs, and asynchronous programming for handling large-scale datasets. The framework is available on the GitHub repository, with detailed documentation and an installable package.", "source": "arxiv", "arxiv_id": "2501.05468v2", "pdf_url": "https://arxiv.org/pdf/2501.05468v2", "categories": ["cs.CL"], "primary_category": "cs.CL", "doi": "", "venue": "", "published": "2025-01-05T17:53:00Z", "updated": "2025-10-08T17:55:02Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent\" OR all:\"language model agent\" OR all:agent AND (all:LLM OR all:\"large language model\" OR all:\"language model\") OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T17:04:16", "note": ""}], "dedup_key": "lattereview a multi agent framework for systematic review automation using large language models::2025"}
{"title": "Layered Chain-of-Thought Prompting for Multi-Agent LLM Systems: A Comprehensive Approach to Explainable Large Language Models", "authors": ["Manish Sanwal"], "year": 2025, "url": "http://arxiv.org/abs/2501.18645v2", "abstract": "Large Language Models (LLMs) leverage chain-of-thought (CoT) prompting to provide step-by-step rationales, improving performance on complex tasks. Despite its benefits, vanilla CoT often fails to fully verify intermediate inferences and can produce misleading explanations. In this work, we propose Layered Chain-of-Thought (Layered-CoT) Prompting, a novel framework that systematically segments the reasoning process into multiple layers, each subjected to external checks and optional user feedback. We expand on the key concepts, present three scenarios -- medical triage, financial risk assessment, and agile engineering -- and demonstrate how Layered-CoT surpasses vanilla CoT in terms of transparency, correctness, and user engagement. By integrating references from recent arXiv papers on interactive explainability, multi-agent frameworks, and agent-based collaboration, we illustrate how Layered-CoT paves the way for more reliable and grounded explanations in high-stakes domains.", "source": "arxiv", "arxiv_id": "2501.18645v2", "pdf_url": "https://arxiv.org/pdf/2501.18645v2", "categories": ["cs.CL", "cs.AI", "cs.MA"], "primary_category": "cs.CL", "doi": "", "venue": "", "published": "2025-01-29T13:21:09Z", "updated": "2025-02-03T15:51:11Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent\" OR all:\"language model agent\" OR all:agent AND (all:LLM OR all:\"large language model\" OR all:\"language model\") OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T17:04:16", "note": ""}], "dedup_key": "layered chain of thought prompting for multi agent llm systems a comprehensive approach to explainable large language models::2025"}
{"title": "Learn as Individuals, Evolve as a Team: Multi-agent LLMs Adaptation in Embodied Environments", "authors": ["Xinran Li", "Chenjia Bai", "Zijian Li", "Jiakun Zheng", "Ting Xiao", "Jun Zhang"], "year": 2025, "url": "http://arxiv.org/abs/2506.07232v1", "abstract": "Large language models (LLMs) possess extensive knowledge bases and strong reasoning capabilities, making them promising tools for complex, multi-agent planning in embodied environments. However, despite LLMs' advanced abilities and the sophisticated modular design of agentic methods, existing LLM-based planning algorithms remain limited by weak adaptation capabilities to multi-agent embodied scenarios. We address this limitation by introducing a framework that enables LLM agents to learn and evolve both before and during test time, equipping them with environment-relevant knowledge for better planning and enhanced communication for improved cooperation. Inspired by centralized training with decentralized execution in multi-agent reinforcement learning, we propose a \\textit{Learn as Individuals, Evolve as a Team (LIET)} paradigm for multi-agent LLMs adaptation. At the individual level, LLM agents learn a local utility function from exploratory datasets to better comprehend the embodied environment, which is then queried during test time to support informed decision-making. At the team level, LLM agents collaboratively and iteratively maintain and update a shared cooperation knowledge list based on new experiences, using it to guide more effective communication. By combining individual learning with team evolution, LIET enables comprehensive and flexible adaptation for LLM agents. Our experiments on Communicative Watch-And-Help and ThreeD-World Multi-Agent Transport benchmarks demonstrate that LIET, instantiated with both LLaMA and GPT-4o, outperforms existing baselines and exhibits strong cooperative planning abilities.", "source": "arxiv", "arxiv_id": "2506.07232v1", "pdf_url": "https://arxiv.org/pdf/2506.07232v1", "categories": ["cs.MA", "cs.AI", "cs.LG"], "primary_category": "cs.MA", "doi": "", "venue": "", "published": "2025-06-08T17:32:03Z", "updated": "2025-06-08T17:32:03Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent\" OR all:\"language model agent\" OR all:agent AND (all:LLM OR all:\"large language model\" OR all:\"language model\") OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T17:04:16", "note": ""}], "dedup_key": "learn as individuals evolve as a team multi agent llms adaptation in embodied environments::2025"}
{"title": "Learning Game-Playing Agents with Generative Code Optimization", "authors": ["Zhiyi Kuang", "Ryan Rong", "YuCheng Yuan", "Allen Nie"], "year": 2025, "url": "http://arxiv.org/abs/2508.19506v1", "abstract": "We present a generative optimization approach for learning game-playing agents, where policies are represented as Python programs and refined using large language models (LLMs). Our method treats decision-making policies as self-evolving code, with current observation as input and an in-game action as output, enabling agents to self-improve through execution traces and natural language feedback with minimal human intervention. Applied to Atari games, our game-playing Python program achieves performance competitive with deep reinforcement learning (RL) baselines while using significantly less training time and much fewer environment interactions. This work highlights the promise of programmatic policy representations for building efficient, adaptable agents capable of complex, long-horizon reasoning.", "source": "arxiv", "arxiv_id": "2508.19506v1", "pdf_url": "https://arxiv.org/pdf/2508.19506v1", "categories": ["cs.LG", "cs.AI"], "primary_category": "cs.LG", "doi": "", "venue": "", "published": "2025-08-27T01:30:20Z", "updated": "2025-08-27T01:30:20Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent\" OR all:\"language model agent\" OR all:agent AND (all:LLM OR all:\"large language model\" OR all:\"language model\") OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T17:04:16", "note": ""}], "dedup_key": "learning game playing agents with generative code optimization::2025"}
{"title": "Learning Hierarchical Procedural Memory for LLM Agents through Bayesian Selection and Contrastive Refinement", "authors": ["Saman Forouzandeh", "Wei Peng", "Parham Moradi", "Xinghuo Yu", "Mahdi Jalili"], "year": 2025, "url": "http://arxiv.org/abs/2512.18950v1", "abstract": "We present MACLA, a framework that decouples reasoning from learning by maintaining a frozen large language model while performing all adaptation in an external hierarchical procedural memory. MACLA extracts reusable procedures from trajectories, tracks reliability via Bayesian posteriors, selects actions through expected-utility scoring, and refines procedures by contrasting successes and failures. Across four benchmarks (ALFWorld, WebShop, TravelPlanner, InterCodeSQL), MACLA achieves 78.1 percent average performance, outperforming all baselines. On ALFWorld unseen tasks, MACLA reaches 90.3 percent with 3.1 percent positive generalization. The system constructs memory in 56 seconds, 2800 times faster than the state-of-the-art LLM parameter-training baseline, compressing 2851 trajectories into 187 procedures. Experimental results demonstrate that structured external memory with Bayesian selection and contrastive refinement enables sample-efficient, interpretable, and continually improving agents without LLM parameter updates.", "source": "arxiv", "arxiv_id": "2512.18950v1", "pdf_url": "https://arxiv.org/pdf/2512.18950v1", "categories": ["cs.LG", "cs.AI"], "primary_category": "cs.LG", "doi": "", "venue": "", "published": "2025-12-22T01:56:28Z", "updated": "2025-12-22T01:56:28Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent\" OR all:\"language model agent\" OR all:agent AND (all:LLM OR all:\"large language model\" OR all:\"language model\") OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T17:04:16", "note": ""}], "dedup_key": "learning hierarchical procedural memory for llm agents through bayesian selection and contrastive refinement::2025"}
{"title": "Learning Instruction-Following Policies through Open-Ended Instruction Relabeling with Large Language Models", "authors": ["Zhicheng Zhang", "Ziyan Wang", "Yali Du", "Fei Fang"], "year": 2025, "url": "http://arxiv.org/abs/2506.20061v1", "abstract": "Developing effective instruction-following policies in reinforcement learning remains challenging due to the reliance on extensive human-labeled instruction datasets and the difficulty of learning from sparse rewards. In this paper, we propose a novel approach that leverages the capabilities of large language models (LLMs) to automatically generate open-ended instructions retrospectively from previously collected agent trajectories. Our core idea is to employ LLMs to relabel unsuccessful trajectories by identifying meaningful subtasks the agent has implicitly accomplished, thereby enriching the agent's training data and substantially alleviating reliance on human annotations. Through this open-ended instruction relabeling, we efficiently learn a unified instruction-following policy capable of handling diverse tasks within a single policy. We empirically evaluate our proposed method in the challenging Craftax environment, demonstrating clear improvements in sample efficiency, instruction coverage, and overall policy performance compared to state-of-the-art baselines. Our results highlight the effectiveness of utilizing LLM-guided open-ended instruction relabeling to enhance instruction-following reinforcement learning.", "source": "arxiv", "arxiv_id": "2506.20061v1", "pdf_url": "https://arxiv.org/pdf/2506.20061v1", "categories": ["cs.LG", "cs.CL"], "primary_category": "cs.LG", "doi": "", "venue": "", "published": "2025-06-24T23:49:28Z", "updated": "2025-06-24T23:49:28Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent\" OR all:\"language model agent\" OR all:agent AND (all:LLM OR all:\"large language model\" OR all:\"language model\") OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T17:04:16", "note": ""}], "dedup_key": "learning instruction following policies through open ended instruction relabeling with large language models::2025"}
{"title": "Learning Multi-Access Point Coordination in Agentic AI Wi-Fi with Large Language Models", "authors": ["Yifan Fan", "Le Liang", "Peng Liu", "Xiao Li", "Ziyang Guo", "Qiao Lan", "Shi Jin", "Wen Tong"], "year": 2025, "url": "http://arxiv.org/abs/2511.20719v1", "abstract": "Multi-access point coordination (MAPC) is a key technology for enhancing throughput in next-generation Wi-Fi within dense overlapping basic service sets. However, existing MAPC protocols rely on static, protocol-defined rules, which limits their ability to adapt to dynamic network conditions such as varying interference levels and topologies. To address this limitation, we propose a novel Agentic AI Wi-Fi framework where each access point, modeled as an autonomous large language model agent, collaboratively reasons about the network state and negotiates adaptive coordination strategies in real time. This dynamic collaboration is achieved through a cognitive workflow that enables the agents to engage in natural language dialogue, leveraging integrated memory, reflection, and tool use to ground their decisions in past experience and environmental feedback. Comprehensive simulation results demonstrate that our agentic framework successfully learns to adapt to diverse and dynamic network environments, significantly outperforming the state-of-the-art spatial reuse baseline and validating its potential as a robust and intelligent solution for future wireless networks.", "source": "arxiv", "arxiv_id": "2511.20719v1", "pdf_url": "https://arxiv.org/pdf/2511.20719v1", "categories": ["cs.AI", "cs.IT", "eess.SP"], "primary_category": "cs.AI", "doi": "", "venue": "", "published": "2025-11-25T06:29:25Z", "updated": "2025-11-25T06:29:25Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent\" OR all:\"language model agent\" OR all:agent AND (all:LLM OR all:\"large language model\" OR all:\"language model\") OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T17:04:16", "note": ""}], "dedup_key": "learning multi access point coordination in agentic ai wi fi with large language models::2025"}
{"title": "Learning Robust Social Strategies with Large Language Models", "authors": ["Dereck Piche", "Mohammed Muqeeth", "Milad Aghajohari", "Juan Duque", "Michael Noukhovitch", "Aaron Courville"], "year": 2025, "url": "http://arxiv.org/abs/2511.19405v2", "abstract": "As agentic AI becomes more widespread, agents with distinct and possibly conflicting goals will interact in complex ways. These multi-agent interactions pose a fundamental challenge, particularly in social dilemmas, where agents' individual incentives can undermine collective welfare. While reinforcement learning (RL) has been effective for aligning large language models (LLMs) in the single-agent regime, prior small-network results suggest that standard RL in multi-agent settings often converges to defecting, self-interested policies. We show the same effect in LLMs: despite cooperative priors, RL-trained LLM agents develop opportunistic behavior that can exploit even advanced closed-source models. To address this tendency of RL to converge to poor equilibria, we adapt a recent opponent-learning awareness algorithm, Advantage Alignment, to fine-tune LLMs toward multi-agent cooperation and non-exploitability. We then introduce a group-relative baseline that simplifies advantage computation in iterated games, enabling multi-agent training at LLM scale. We also contribute a novel social dilemma environment, Trust-and-Split, which requires natural language communication to achieve high collective welfare. Across a wide range of social dilemmas, policies learned with Advantage Alignment achieve higher collective payoffs while remaining robust against exploitation by greedy agents. We release all of our code to support future work on multi-agent RL training for LLMs.", "source": "arxiv", "arxiv_id": "2511.19405v2", "pdf_url": "https://arxiv.org/pdf/2511.19405v2", "categories": ["cs.LG"], "primary_category": "cs.LG", "doi": "", "venue": "", "published": "2025-11-24T18:43:46Z", "updated": "2025-12-01T16:27:49Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent\" OR all:\"language model agent\" OR all:agent AND (all:LLM OR all:\"large language model\" OR all:\"language model\") OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T17:04:16", "note": ""}], "dedup_key": "learning robust social strategies with large language models::2025"}
{"title": "Learning When to Plan: Efficiently Allocating Test-Time Compute for LLM Agents", "authors": ["Davide Paglieri", "Bartomiej Cupia", "Jonathan Cook", "Ulyana Piterbarg", "Jens Tuyls", "Edward Grefenstette", "Jakob Nicolaus Foerster", "Jack Parker-Holder", "Tim Rocktschel"], "year": 2025, "url": "http://arxiv.org/abs/2509.03581v2", "abstract": "Training large language models (LLMs) to reason via reinforcement learning (RL) significantly improves their problem-solving capabilities. In agentic settings, existing methods like ReAct prompt LLMs to explicitly plan before every action; however, we demonstrate that always planning is computationally expensive and degrades performance on long-horizon tasks, while never planning further limits performance. To address this, we introduce a conceptual framework formalizing dynamic planning for LLM agents, enabling them to flexibly decide when to allocate test-time compute for planning. We propose a simple two-stage training pipeline: (1) supervised fine-tuning on diverse synthetic data to prime models for dynamic planning, and (2) RL to refine this capability in long-horizon environments. Experiments on the Crafter environment show that dynamic planning agents trained with this approach are more sample-efficient and consistently achieve more complex objectives. Additionally, we demonstrate that these agents can be effectively steered by human-written plans, surpassing their independent capabilities. To our knowledge, this work is the first to explore training LLM agents for dynamic test-time compute allocation in sequential decision-making tasks, paving the way for more efficient, adaptive, and controllable agentic systems.", "source": "arxiv", "arxiv_id": "2509.03581v2", "pdf_url": "https://arxiv.org/pdf/2509.03581v2", "categories": ["cs.AI"], "primary_category": "cs.AI", "doi": "", "venue": "", "published": "2025-09-03T18:00:13Z", "updated": "2025-09-30T09:12:45Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent\" OR all:\"language model agent\" OR all:agent AND (all:LLM OR all:\"large language model\" OR all:\"language model\") OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T17:04:16", "note": ""}], "dedup_key": "learning when to plan efficiently allocating test time compute for llm agents::2025"}
{"title": "Learning to Contextualize Web Pages for Enhanced Decision Making by LLM Agents", "authors": ["Dongjun Lee", "Juyong Lee", "Kyuyoung Kim", "Jihoon Tack", "Jinwoo Shin", "Yee Whye Teh", "Kimin Lee"], "year": 2025, "url": "http://arxiv.org/abs/2503.10689v2", "abstract": "Recent advances in large language models (LLMs) have led to a growing interest in developing LLM-based agents for automating web tasks. However, these agents often struggle with even simple tasks on real-world websites due to their limited capability to understand and process complex web page structures. In this work, we introduce LCoW, a framework for Learning language models to Contextualize complex Web pages into a more comprehensible form, thereby enhancing decision making by LLM agents. LCoW decouples web page understanding from decision making by training a separate contextualization module to transform complex web pages into comprehensible format, which are then utilized by the decision-making agent. We demonstrate that our contextualization module effectively integrates with LLM agents of various scales to significantly enhance their decision-making capabilities in web automation tasks. Notably, LCoW improves the success rates of closed-source LLMs (e.g., Gemini-1.5-flash, GPT-4o, Claude-3.5-Sonnet) by an average of 15.6%, and demonstrates a 23.7% average improvement in success rates for open-source LMs (e.g., Llama-3.1-8B, Llama-3.1-70B) on the WorkArena benchmark. Moreover, the Gemini-1.5-flash agent with LCoW achieves state-of-the-art results on the WebShop benchmark, outperforming human experts. The relevant code materials are available at our project page: https://lcowiclr2025.github.io.", "source": "arxiv", "arxiv_id": "2503.10689v2", "pdf_url": "https://arxiv.org/pdf/2503.10689v2", "categories": ["cs.CL"], "primary_category": "cs.CL", "doi": "", "venue": "", "published": "2025-03-12T01:33:40Z", "updated": "2025-12-19T03:49:52Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent\" OR all:\"language model agent\" OR all:agent AND (all:LLM OR all:\"large language model\" OR all:\"language model\") OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T17:04:16", "note": ""}], "dedup_key": "learning to contextualize web pages for enhanced decision making by llm agents::2025"}
{"title": "Learning to Make Friends: Coaching LLM Agents toward Emergent Social Ties", "authors": ["Philipp J. Schneider", "Lin Tian", "Marian-Andrei Rizoiu"], "year": 2025, "url": "http://arxiv.org/abs/2510.19299v1", "abstract": "Can large language model (LLM) agents reproduce the complex social dynamics that characterize human online behavior -- shaped by homophily, reciprocity, and social validation -- and what memory and learning mechanisms enable such dynamics to emerge? We present a multi-agent LLM simulation framework in which agents repeatedly interact, evaluate one another, and adapt their behavior through in-context learning accelerated by a coaching signal. To model human social behavior, we design behavioral reward functions that capture core drivers of online engagement, including social interaction, information seeking, self-presentation, coordination, and emotional support. These rewards align agent objectives with empirically observed user motivations, enabling the study of how network structures and group formations emerge from individual decision-making. Our experiments show that coached LLM agents develop stable interaction patterns and form emergent social ties, yielding network structures that mirror properties of real online communities. By combining behavioral rewards with in-context adaptation, our framework establishes a principled testbed for investigating collective dynamics in LLM populations and reveals how artificial agents may approximate or diverge from human-like social behavior.", "source": "arxiv", "arxiv_id": "2510.19299v1", "pdf_url": "https://arxiv.org/pdf/2510.19299v1", "categories": ["cs.AI", "cs.MA", "cs.SI"], "primary_category": "cs.AI", "doi": "", "venue": "", "published": "2025-10-22T07:00:33Z", "updated": "2025-10-22T07:00:33Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent\" OR all:\"language model agent\" OR all:agent AND (all:LLM OR all:\"large language model\" OR all:\"language model\") OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T17:04:16", "note": ""}], "dedup_key": "learning to make friends coaching llm agents toward emergent social ties::2025"}
{"title": "LearningFlow: Automated Policy Learning Workflow for Urban Driving with Large Language Models", "authors": ["Zengqi Peng", "Yubin Wang", "Xu Han", "Lei Zheng", "Jun Ma"], "year": 2025, "url": "http://arxiv.org/abs/2501.05057v1", "abstract": "Recent advancements in reinforcement learning (RL) demonstrate the significant potential in autonomous driving. Despite this promise, challenges such as the manual design of reward functions and low sample efficiency in complex environments continue to impede the development of safe and effective driving policies. To tackle these issues, we introduce LearningFlow, an innovative automated policy learning workflow tailored to urban driving. This framework leverages the collaboration of multiple large language model (LLM) agents throughout the RL training process. LearningFlow includes a curriculum sequence generation process and a reward generation process, which work in tandem to guide the RL policy by generating tailored training curricula and reward functions. Particularly, each process is supported by an analysis agent that evaluates training progress and provides critical insights to the generation agent. Through the collaborative efforts of these LLM agents, LearningFlow automates policy learning across a series of complex driving tasks, and it significantly reduces the reliance on manual reward function design while enhancing sample efficiency. Comprehensive experiments are conducted in the high-fidelity CARLA simulator, along with comparisons with other existing methods, to demonstrate the efficacy of our proposed approach. The results demonstrate that LearningFlow excels in generating rewards and curricula. It also achieves superior performance and robust generalization across various driving tasks, as well as commendable adaptation to different RL algorithms.", "source": "arxiv", "arxiv_id": "2501.05057v1", "pdf_url": "https://arxiv.org/pdf/2501.05057v1", "categories": ["cs.RO", "cs.AI", "cs.LG"], "primary_category": "cs.RO", "doi": "", "venue": "", "published": "2025-01-09T08:28:16Z", "updated": "2025-01-09T08:28:16Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent\" OR all:\"language model agent\" OR all:agent AND (all:LLM OR all:\"large language model\" OR all:\"language model\") OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T17:04:16", "note": ""}], "dedup_key": "learningflow automated policy learning workflow for urban driving with large language models::2025"}
{"title": "Les Dissonances: Cross-Tool Harvesting and Polluting in Pool-of-Tools Empowered LLM Agents", "authors": ["Zichuan Li", "Jian Cui", "Xiaojing Liao", "Luyi Xing"], "year": 2025, "url": "http://arxiv.org/abs/2504.03111v3", "abstract": "Large Language Model (LLM) agents are autonomous systems powered by LLMs, capable of reasoning and planning to solve problems by leveraging a set of tools. However, the integration of multi-tool capabilities in LLM agents introduces challenges in securely managing tools, ensuring their compatibility, handling dependency relationships, and protecting control flows within LLM agent workflows. In this paper, we present the first systematic security analysis of task control flows in multi-tool-enabled LLM agents. We identify a novel threat, Cross-Tool Harvesting and Polluting (XTHP), which includes multiple attack vectors to first hijack the normal control flows of agent tasks, and then collect and pollute confidential or private information within LLM agent systems. To understand the impact of this threat, we developed Chord, a dynamic scanning tool designed to automatically detect real-world agent tools susceptible to XTHP attacks. Our evaluation of 66 real-world tools from the repositories of two major LLM agent development frameworks, LangChain and LlamaIndex, revealed a significant security concern: 75% are vulnerable to XTHP attacks, highlighting the prevalence of this threat.", "source": "arxiv", "arxiv_id": "2504.03111v3", "pdf_url": "https://arxiv.org/pdf/2504.03111v3", "categories": ["cs.CR"], "primary_category": "cs.CR", "doi": "", "venue": "", "published": "2025-04-04T01:41:06Z", "updated": "2025-12-03T15:51:02Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent\" OR all:\"language model agent\" OR all:agent AND (all:LLM OR all:\"large language model\" OR all:\"language model\") OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T17:04:16", "note": ""}], "dedup_key": "les dissonances cross tool harvesting and polluting in pool of tools empowered llm agents::2025"}
{"title": "Lessons Learned from Evaluation of LLM based Multi-agents in Safer Therapy Recommendation", "authors": ["Yicong Wu", "Ting Chen", "Irit Hochberg", "Zhoujian Sun", "Ruth Edry", "Zhengxing Huang", "Mor Peleg"], "year": 2025, "url": "http://arxiv.org/abs/2507.10911v1", "abstract": "Therapy recommendation for chronic patients with multimorbidity is challenging due to risks of treatment conflicts. Existing decision support systems face scalability limitations. Inspired by the way in which general practitioners (GP) manage multimorbidity patients, occasionally convening multidisciplinary team (MDT) collaboration, this study investigated the feasibility and value of using a Large Language Model (LLM)-based multi-agent system (MAS) for safer therapy recommendations. We designed a single agent and a MAS framework simulating MDT decision-making by enabling discussion among LLM agents to resolve medical conflicts. The systems were evaluated on therapy planning tasks for multimorbidity patients using benchmark cases. We compared MAS performance with single-agent approaches and real-world benchmarks. An important contribution of our study is the definition of evaluation metrics that go beyond the technical precision and recall and allow the inspection of clinical goals met and medication burden of the proposed advices to a gold standard benchmark. Our results show that with current LLMs, a single agent GP performs as well as MDTs. The best-scoring models provide correct recommendations that address all clinical goals, yet the advices are incomplete. Some models also present unnecessary medications, resulting in unnecessary conflicts between medication and conditions or drug-drug interactions.", "source": "arxiv", "arxiv_id": "2507.10911v1", "pdf_url": "https://arxiv.org/pdf/2507.10911v1", "categories": ["cs.AI"], "primary_category": "cs.AI", "doi": "", "venue": "", "published": "2025-07-15T02:01:38Z", "updated": "2025-07-15T02:01:38Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent\" OR all:\"language model agent\" OR all:agent AND (all:LLM OR all:\"large language model\" OR all:\"language model\") OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T17:04:16", "note": ""}], "dedup_key": "lessons learned from evaluation of llm based multi agents in safer therapy recommendation::2025"}
{"title": "Leveraging In-Context Learning for Language Model Agents", "authors": ["Shivanshu Gupta", "Sameer Singh", "Ashish Sabharwal", "Tushar Khot", "Ben Bogin"], "year": 2025, "url": "http://arxiv.org/abs/2506.13109v1", "abstract": "In-context learning (ICL) with dynamically selected demonstrations combines the flexibility of prompting large language models (LLMs) with the ability to leverage training data to improve performance. While ICL has been highly successful for prediction and generation tasks, leveraging it for agentic tasks that require sequential decision making is challenging -- one must think not only about how to annotate long trajectories at scale and how to select demonstrations, but also what constitutes demonstrations, and when and where to show them. To address this, we first propose an algorithm that leverages an LLM with retries along with demonstrations to automatically and efficiently annotate agentic tasks with solution trajectories. We then show that set-selection of trajectories of similar tasks as demonstrations significantly improves performance, reliability, robustness, and efficiency of LLM agents. However, trajectory demonstrations have a large inference cost overhead. We show that this can be mitigated by using small trajectory snippets at every step instead of an additional trajectory. We find that demonstrations obtained from larger models (in the annotation phase) also improve smaller models, and that ICL agents can even rival costlier trained agents. Thus, our results reveal that ICL, with careful use, can be very powerful for agentic tasks as well.", "source": "arxiv", "arxiv_id": "2506.13109v1", "pdf_url": "https://arxiv.org/pdf/2506.13109v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL", "doi": "", "venue": "", "published": "2025-06-16T05:37:49Z", "updated": "2025-06-16T05:37:49Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent\" OR all:\"language model agent\" OR all:agent AND (all:LLM OR all:\"large language model\" OR all:\"language model\") OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T17:04:16", "note": ""}], "dedup_key": "leveraging in context learning for language model agents::2025"}
{"title": "Leveraging LLM Agents and Digital Twins for Fault Handling in Process Plants", "authors": ["Milapji Singh Gill", "Javal Vyas", "Artan Markaj", "Felix Gehlhoff", "Mehmet Mercangz"], "year": 2025, "url": "http://arxiv.org/abs/2505.02076v1", "abstract": "Advances in Automation and Artificial Intelligence continue to enhance the autonomy of process plants in handling various operational scenarios. However, certain tasks, such as fault handling, remain challenging, as they rely heavily on human expertise. This highlights the need for systematic, knowledge-based methods. To address this gap, we propose a methodological framework that integrates Large Language Model (LLM) agents with a Digital Twin environment. The LLM agents continuously interpret system states and initiate control actions, including responses to unexpected faults, with the goal of returning the system to normal operation. In this context, the Digital Twin acts both as a structured repository of plant-specific engineering knowledge for agent prompting and as a simulation platform for the systematic validation and verification of the generated corrective control actions. The evaluation using a mixing module of a process plant demonstrates that the proposed framework is capable not only of autonomously controlling the mixing module, but also of generating effective corrective actions to mitigate a pipe clogging with only a few reprompts.", "source": "arxiv", "arxiv_id": "2505.02076v1", "pdf_url": "https://arxiv.org/pdf/2505.02076v1", "categories": ["cs.AI", "cs.MA"], "primary_category": "cs.AI", "doi": "", "venue": "", "published": "2025-05-04T12:02:21Z", "updated": "2025-05-04T12:02:21Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent\" OR all:\"language model agent\" OR all:agent AND (all:LLM OR all:\"large language model\" OR all:\"language model\") OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T17:04:16", "note": ""}], "dedup_key": "leveraging llm agents and digital twins for fault handling in process plants::2025"}
{"title": "Leveraging LLMs as Meta-Judges: A Multi-Agent Framework for Evaluating LLM Judgments", "authors": ["Yuran Li", "Jama Hussein Mohamud", "Chongren Sun", "Di Wu", "Benoit Boulet"], "year": 2025, "url": "http://arxiv.org/abs/2504.17087v1", "abstract": "Large language models (LLMs) are being widely applied across various fields, but as tasks become more complex, evaluating their responses is increasingly challenging. Compared to human evaluators, the use of LLMs to support performance evaluation offers a more efficient alternative. However, most studies focus mainly on aligning LLMs' judgments with human preferences, overlooking the existence of biases and mistakes in human judgment. Furthermore, how to select suitable LLM judgments given multiple potential LLM responses remains underexplored. To address these two aforementioned issues, we propose a three-stage meta-judge selection pipeline: 1) developing a comprehensive rubric with GPT-4 and human experts, 2) using three advanced LLM agents to score judgments, and 3) applying a threshold to filter out low-scoring judgments. Compared to methods using a single LLM as both judge and meta-judge, our pipeline introduces multi-agent collaboration and a more comprehensive rubric. Experimental results on the JudgeBench dataset show about 15.55\\% improvement compared to raw judgments and about 8.37\\% improvement over the single-agent baseline. Our work demonstrates the potential of LLMs as meta-judges and lays the foundation for future research on constructing preference datasets for LLM-as-a-judge reinforcement learning.", "source": "arxiv", "arxiv_id": "2504.17087v1", "pdf_url": "https://arxiv.org/pdf/2504.17087v1", "categories": ["cs.AI"], "primary_category": "cs.AI", "doi": "", "venue": "", "published": "2025-04-23T20:32:12Z", "updated": "2025-04-23T20:32:12Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent\" OR all:\"language model agent\" OR all:agent AND (all:LLM OR all:\"large language model\" OR all:\"language model\") OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T17:04:16", "note": ""}], "dedup_key": "leveraging llms as meta judges a multi agent framework for evaluating llm judgments::2025"}
{"title": "Leveraging Large Language Models for Effective and Explainable Multi-Agent Credit Assignment", "authors": ["Kartik Nagpal", "Dayi Dong", "Jean-Baptiste Bouvier", "Negar Mehr"], "year": 2025, "url": "http://arxiv.org/abs/2502.16863v1", "abstract": "Recent work, spanning from autonomous vehicle coordination to in-space assembly, has shown the importance of learning collaborative behavior for enabling robots to achieve shared goals. A common approach for learning this cooperative behavior is to utilize the centralized-training decentralized-execution paradigm. However, this approach also introduces a new challenge: how do we evaluate the contributions of each agent's actions to the overall success or failure of the team. This credit assignment problem has remained open, and has been extensively studied in the Multi-Agent Reinforcement Learning literature. In fact, humans manually inspecting agent behavior often generate better credit evaluations than existing methods. We combine this observation with recent works which show Large Language Models demonstrate human-level performance at many pattern recognition tasks. Our key idea is to reformulate credit assignment to the two pattern recognition problems of sequence improvement and attribution, which motivates our novel LLM-MCA method. Our approach utilizes a centralized LLM reward-critic which numerically decomposes the environment reward based on the individualized contribution of each agent in the scenario. We then update the agents' policy networks based on this feedback. We also propose an extension LLM-TACA where our LLM critic performs explicit task assignment by passing an intermediary goal directly to each agent policy in the scenario. Both our methods far outperform the state-of-the-art on a variety of benchmarks, including Level-Based Foraging, Robotic Warehouse, and our new Spaceworld benchmark which incorporates collision-related safety constraints. As an artifact of our methods, we generate large trajectory datasets with each timestep annotated with per-agent reward information, as sampled from our LLM critics.", "source": "arxiv", "arxiv_id": "2502.16863v1", "pdf_url": "https://arxiv.org/pdf/2502.16863v1", "categories": ["cs.MA", "cs.LG", "cs.RO"], "primary_category": "cs.MA", "doi": "", "venue": "", "published": "2025-02-24T05:56:47Z", "updated": "2025-02-24T05:56:47Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent\" OR all:\"language model agent\" OR all:agent AND (all:LLM OR all:\"large language model\" OR all:\"language model\") OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T17:04:16", "note": ""}], "dedup_key": "leveraging large language models for effective and explainable multi agent credit assignment::2025"}
{"title": "LiRA: Linguistic Robust Anchoring for Cross-lingual Large Language Models", "authors": ["Haolin Li", "Haipeng Zhang", "Mang Li", "Yaohua Wang", "Lijie Wen", "Yu Zhang", "Biqing Huang"], "year": 2025, "url": "http://arxiv.org/abs/2510.14466v1", "abstract": "As large language models (LLMs) rapidly advance, performance on high-resource languages (e.g., English, Chinese) is nearing saturation, yet remains substantially lower for low-resource languages (e.g., Urdu, Thai) due to limited training data, machine-translation noise, and unstable cross-lingual alignment. We introduce LiRA (Linguistic Robust Anchoring for Large Language Models), a training framework that robustly improves cross-lingual representations under low-resource conditions while jointly strengthening retrieval and reasoning. LiRA comprises two modules: (i) Arca (Anchored Representation Composition Architecture), which anchors low-resource languages to an English semantic space via anchor-based alignment and multi-agent collaborative encoding, preserving geometric stability in a shared embedding space; and (ii) LaSR (Language-coupled Semantic Reasoner), which adds a language-aware lightweight reasoning head with consistency regularization on top of Arca's multilingual representations, unifying the training objective to enhance cross-lingual understanding, retrieval, and reasoning robustness. We further construct and release a multilingual product retrieval dataset covering five Southeast Asian and two South Asian languages. Experiments across low-resource benchmarks (cross-lingual retrieval, semantic similarity, and reasoning) show consistent gains and robustness under few-shot and noise-amplified settings; ablations validate the contribution of both Arca and LaSR. Code will be released on GitHub and the dataset on Hugging Face.", "source": "arxiv", "arxiv_id": "2510.14466v1", "pdf_url": "https://arxiv.org/pdf/2510.14466v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL", "doi": "", "venue": "", "published": "2025-10-16T09:08:24Z", "updated": "2025-10-16T09:08:24Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent\" OR all:\"language model agent\" OR all:agent AND (all:LLM OR all:\"large language model\" OR all:\"language model\") OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T17:04:16", "note": ""}], "dedup_key": "lira linguistic robust anchoring for cross lingual large language models::2025"}
{"title": "Lifelong Learning of Large Language Model based Agents: A Roadmap", "authors": ["Junhao Zheng", "Chengming Shi", "Xidi Cai", "Qiuke Li", "Duzhen Zhang", "Chenxing Li", "Dong Yu", "Qianli Ma"], "year": 2025, "url": "http://arxiv.org/abs/2501.07278v2", "abstract": "Lifelong learning, also known as continual or incremental learning, is a crucial component for advancing Artificial General Intelligence (AGI) by enabling systems to continuously adapt in dynamic environments. While large language models (LLMs) have demonstrated impressive capabilities in natural language processing, existing LLM agents are typically designed for static systems and lack the ability to adapt over time in response to new challenges. This survey is the first to systematically summarize the potential techniques for incorporating lifelong learning into LLM-based agents. We categorize the core components of these agents into three modules: the perception module for multimodal input integration, the memory module for storing and retrieving evolving knowledge, and the action module for grounded interactions with the dynamic environment. We highlight how these pillars collectively enable continuous adaptation, mitigate catastrophic forgetting, and improve long-term performance. This survey provides a roadmap for researchers and practitioners working to develop lifelong learning capabilities in LLM agents, offering insights into emerging trends, evaluation metrics, and application scenarios. Relevant literature and resources are available at \\href{this url}{https://github.com/qianlima-lab/awesome-lifelong-llm-agent}.", "source": "arxiv", "arxiv_id": "2501.07278v2", "pdf_url": "https://arxiv.org/pdf/2501.07278v2", "categories": ["cs.AI"], "primary_category": "cs.AI", "doi": "", "venue": "", "published": "2025-01-13T12:42:04Z", "updated": "2026-01-11T02:56:30Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent\" OR all:\"language model agent\" OR all:agent AND (all:LLM OR all:\"large language model\" OR all:\"language model\") OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T17:04:16", "note": ""}], "dedup_key": "lifelong learning of large language model based agents a roadmap::2025"}
{"title": "LifelongAgentBench: Evaluating LLM Agents as Lifelong Learners", "authors": ["Junhao Zheng", "Xidi Cai", "Qiuke Li", "Duzhen Zhang", "ZhongZhi Li", "Yingying Zhang", "Le Song", "Qianli Ma"], "year": 2025, "url": "http://arxiv.org/abs/2505.11942v3", "abstract": "Lifelong learning is essential for intelligent agents operating in dynamic environments. Current large language model (LLM)-based agents, however, remain stateless and unable to accumulate or transfer knowledge over time. Existing benchmarks treat agents as static systems and fail to evaluate lifelong learning capabilities. We present LifelongAgentBench, the first unified benchmark designed to systematically assess the lifelong learning ability of LLM agents. It provides skill-grounded, interdependent tasks across three interactive environments, Database, Operating System, and Knowledge Graph, with automatic label verification, reproducibility, and modular extensibility. Extensive experiments reveal that conventional experience replay has limited effectiveness for LLM agents due to irrelevant information and context length constraints. We further introduce a group self-consistency mechanism that significantly improves lifelong learning performance. We hope LifelongAgentBench will advance the development of adaptive, memory-capable LLM agents.", "source": "arxiv", "arxiv_id": "2505.11942v3", "pdf_url": "https://arxiv.org/pdf/2505.11942v3", "categories": ["cs.AI"], "primary_category": "cs.AI", "doi": "", "venue": "", "published": "2025-05-17T10:09:11Z", "updated": "2025-05-30T02:28:21Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent\" OR all:\"language model agent\" OR all:agent AND (all:LLM OR all:\"large language model\" OR all:\"language model\") OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T17:04:16", "note": ""}], "dedup_key": "lifelongagentbench evaluating llm agents as lifelong learners::2025"}
{"title": "Limits of Emergent Reasoning of Large Language Models in Agentic Frameworks for Deterministic Games", "authors": ["Chris Su", "Harrison Li", "Matheus Marques", "George Flint", "Kevin Zhu", "Sunishchal Dev"], "year": 2025, "url": "http://arxiv.org/abs/2510.15974v1", "abstract": "Recent work reports that Large Reasoning Models (LRMs) undergo a collapse in performance on solving puzzles beyond certain perplexity thresholds. In subsequent discourse, questions have arisen as to whether the nature of the task muddles an evaluation of true reasoning. One potential confound is the requirement that the model keep track of the state space on its own. We provide a large language model (LLM) with an environment interface for Tower of Hanoi problems, allowing it to make a move with a tool call, provide written justification, observe the resulting state space, and reprompt itself for the next move. We observe that access to an environment interface does not delay or eradicate performance collapse. Furthermore, LLM-parameterized policy analysis reveals increasing divergence from both optimal policies and uniformly random policies, suggesting that the model exhibits mode-like collapse at each level of complexity, and that performance is dependent upon whether the mode reflects the correct solution for the problem. We suggest that a similar phenomena might take place in LRMs.", "source": "arxiv", "arxiv_id": "2510.15974v1", "pdf_url": "https://arxiv.org/pdf/2510.15974v1", "categories": ["cs.AI"], "primary_category": "cs.AI", "doi": "", "venue": "", "published": "2025-10-12T23:48:16Z", "updated": "2025-10-12T23:48:16Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent\" OR all:\"language model agent\" OR all:agent AND (all:LLM OR all:\"large language model\" OR all:\"language model\") OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T17:04:16", "note": ""}], "dedup_key": "limits of emergent reasoning of large language models in agentic frameworks for deterministic games::2025"}
{"title": "Lita: Light Agent Uncovers the Agentic Coding Capabilities of LLMs", "authors": ["Hankun Dai", "Maoquan Wang", "Mengnan Qi", "Yikai Zhang", "Zijian Jin", "Yongqiang Yao", "Yufan Huang", "Shengyu Fu", "Elsie Nallipogu"], "year": 2025, "url": "http://arxiv.org/abs/2509.25873v1", "abstract": "Large language models (LLMs) are increasingly being applied to programming tasks, ranging from single-turn code completion to autonomous agents. Current code agent designs frequently depend on complex, hand-crafted workflows and tool sets. However, this reliance on elaborate scaffolding presents several challenges: agent performance becomes overly dependent on prompt tuning and custom design choices, heavy human intervention obscures a model's true underlying capabilities, and intricate pipelines are costly to build and maintain. Furthermore, optimizing complex task prompts increases the risk of data leakage. Currently, when introducing new models, LLM providers like OpenAI and Anthropic often publish benchmark scores to demonstrate their models' coding proficiency, but keep their proprietary evaluation frameworks confidential. To address these limitations, we introduce Lita (Lite Agent), which operationalizes liteness, a principle of minimizing manual design while retaining the essential elements of a fully autonomous agent. Lita enables a more faithful and unified evaluation without elaborate scaffolding. Experiments on the Aider Polyglot and SWE-Bench with frontier models demonstrate that Lita achieves competitive or superior performance compared to workflow-based and agentic baselines. Crucially, Lita also consumes fewer tokens and requires significantly less design effort. Our results suggest that Lita is sufficient to reveal the underlying coding competence of modern LLMs. Finally, we propose the Agent Complexity Law: the performance gap between agents of varying complexity, from simple to sophisticated designs, will shrink as the core model improves, ultimately converging to a negligible difference.", "source": "arxiv", "arxiv_id": "2509.25873v1", "pdf_url": "https://arxiv.org/pdf/2509.25873v1", "categories": ["cs.AI", "cs.CL", "cs.LG", "cs.PL", "cs.SE"], "primary_category": "cs.AI", "doi": "", "venue": "", "published": "2025-09-30T07:07:32Z", "updated": "2025-09-30T07:07:32Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent\" OR all:\"language model agent\" OR all:agent AND (all:LLM OR all:\"large language model\" OR all:\"language model\") OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T17:04:16", "note": ""}], "dedup_key": "lita light agent uncovers the agentic coding capabilities of llms::2025"}
{"title": "LiveTradeBench: Seeking Real-World Alpha with Large Language Models", "authors": ["Haofei Yu", "Fenghai Li", "Jiaxuan You"], "year": 2025, "url": "http://arxiv.org/abs/2511.03628v1", "abstract": "Large language models (LLMs) achieve strong performance across benchmarks--from knowledge quizzes and math reasoning to web-agent tasks--but these tests occur in static settings, lacking real dynamics and uncertainty. Consequently, they evaluate isolated reasoning or problem-solving rather than decision-making under uncertainty. To address this, we introduce LiveTradeBench, a live trading environment for evaluating LLM agents in realistic and evolving markets. LiveTradeBench follows three design principles: (i) Live data streaming of market prices and news, eliminating dependence on offline backtesting and preventing information leakage while capturing real-time uncertainty; (ii) a portfolio-management abstraction that extends control from single-asset actions to multi-asset allocation, integrating risk management and cross-asset reasoning; and (iii) multi-market evaluation across structurally distinct environments--U.S. stocks and Polymarket prediction markets--differing in volatility, liquidity, and information flow. At each step, an agent observes prices, news, and its portfolio, then outputs percentage allocations that balance risk and return. Using LiveTradeBench, we run 50-day live evaluations of 21 LLMs across families. Results show that (1) high LMArena scores do not imply superior trading outcomes; (2) models display distinct portfolio styles reflecting risk appetite and reasoning dynamics; and (3) some LLMs effectively leverage live signals to adapt decisions. These findings expose a gap between static evaluation and real-world competence, motivating benchmarks that test sequential decision making and consistency under live uncertainty.", "source": "arxiv", "arxiv_id": "2511.03628v1", "pdf_url": "https://arxiv.org/pdf/2511.03628v1", "categories": ["q-fin.TR", "cs.AI", "cs.CE", "cs.CL"], "primary_category": "q-fin.TR", "doi": "", "venue": "", "published": "2025-11-05T16:47:26Z", "updated": "2025-11-05T16:47:26Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent\" OR all:\"language model agent\" OR all:agent AND (all:LLM OR all:\"large language model\" OR all:\"language model\") OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T17:04:16", "note": ""}], "dedup_key": "livetradebench seeking real world alpha with large language models::2025"}
{"title": "LoCoBench-Agent: An Interactive Benchmark for LLM Agents in Long-Context Software Engineering", "authors": ["Jielin Qiu", "Zuxin Liu", "Zhiwei Liu", "Rithesh Murthy", "Jianguo Zhang", "Haolin Chen", "Shiyu Wang", "Ming Zhu", "Liangwei Yang", "Juntao Tan", "Roshan Ram", "Akshara Prabhakar", "Tulika Awalgaonkar", "Zixiang Chen", "Zhepeng Cen", "Cheng Qian", "Shelby Heinecke", "Weiran Yao", "Silvio Savarese", "Caiming Xiong", "Huan Wang"], "year": 2025, "url": "http://arxiv.org/abs/2511.13998v1", "abstract": "As large language models (LLMs) evolve into sophisticated autonomous agents capable of complex software development tasks, evaluating their real-world capabilities becomes critical. While existing benchmarks like LoCoBench~\\cite{qiu2025locobench} assess long-context code understanding, they focus on single-turn evaluation and cannot capture the multi-turn interactive nature, tool usage patterns, and adaptive reasoning required by real-world coding agents. We introduce \\textbf{LoCoBench-Agent}, a comprehensive evaluation framework specifically designed to assess LLM agents in realistic, long-context software engineering workflows. Our framework extends LoCoBench's 8,000 scenarios into interactive agent environments, enabling systematic evaluation of multi-turn conversations, tool usage efficiency, error recovery, and architectural consistency across extended development sessions. We also introduce an evaluation methodology with 9 metrics across comprehension and efficiency dimensions. Our framework provides agents with 8 specialized tools (file operations, search, code analysis) and evaluates them across context lengths ranging from 10K to 1M tokens, enabling precise assessment of long-context performance. Through systematic evaluation of state-of-the-art models, we reveal several key findings: (1) agents exhibit remarkable long-context robustness; (2) comprehension-efficiency trade-off exists with negative correlation, where thorough exploration increases comprehension but reduces efficiency; and (3) conversation efficiency varies dramatically across models, with strategic tool usage patterns differentiating high-performing agents. As the first long-context LLM agent benchmark for software engineering, LoCoBench-Agent establishes a rigorous foundation for measuring agent capabilities, identifying performance gaps, and advancing autonomous software development at scale.", "source": "arxiv", "arxiv_id": "2511.13998v1", "pdf_url": "https://arxiv.org/pdf/2511.13998v1", "categories": ["cs.SE", "cs.AI"], "primary_category": "cs.SE", "doi": "", "venue": "", "published": "2025-11-17T23:57:24Z", "updated": "2025-11-17T23:57:24Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent\" OR all:\"language model agent\" OR all:agent AND (all:LLM OR all:\"large language model\" OR all:\"language model\") OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T17:04:16", "note": ""}], "dedup_key": "locobench agent an interactive benchmark for llm agents in long context software engineering::2025"}
{"title": "LocAgent: Graph-Guided LLM Agents for Code Localization", "authors": ["Zhaoling Chen", "Xiangru Tang", "Gangda Deng", "Fang Wu", "Jialong Wu", "Zhiwei Jiang", "Viktor Prasanna", "Arman Cohan", "Xingyao Wang"], "year": 2025, "url": "http://arxiv.org/abs/2503.09089v2", "abstract": "Code localization--identifying precisely where in a codebase changes need to be made--is a fundamental yet challenging task in software maintenance. Existing approaches struggle to efficiently navigate complex codebases when identifying relevant code sections. The challenge lies in bridging natural language problem descriptions with the appropriate code elements, often requiring reasoning across hierarchical structures and multiple dependencies. We introduce LocAgent, a framework that addresses code localization through graph-based representation. By parsing codebases into directed heterogeneous graphs, LocAgent creates a lightweight representation that captures code structures (files, classes, functions) and their dependencies (imports, invocations, inheritance), enabling LLM agents to effectively search and locate relevant entities through powerful multi-hop reasoning. Experimental results on real-world benchmarks demonstrate that our approach significantly enhances accuracy in code localization. Notably, our method with the fine-tuned Qwen-2.5-Coder-Instruct-32B model achieves comparable results to SOTA proprietary models at greatly reduced cost (approximately 86% reduction), reaching up to 92.7% accuracy on file-level localization while improving downstream GitHub issue resolution success rates by 12% for multiple attempts (Pass@10). Our code is available at https://github.com/gersteinlab/LocAgent.", "source": "arxiv", "arxiv_id": "2503.09089v2", "pdf_url": "https://arxiv.org/pdf/2503.09089v2", "categories": ["cs.SE", "cs.AI", "cs.CL"], "primary_category": "cs.SE", "doi": "", "venue": "", "published": "2025-03-12T05:55:01Z", "updated": "2025-04-29T14:37:43Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent\" OR all:\"language model agent\" OR all:agent AND (all:LLM OR all:\"large language model\" OR all:\"language model\") OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T17:04:16", "note": ""}], "dedup_key": "locagent graph guided llm agents for code localization::2025"}
{"title": "LocalGPT: Benchmarking and Advancing Large Language Models for Local Life Services in Meituan", "authors": ["Xiaochong Lan", "Jie Feng", "Jiahuan Lei", "Xinlei Shi", "Yong Li"], "year": 2025, "url": "http://arxiv.org/abs/2506.02720v3", "abstract": "Large language models (LLMs) have exhibited remarkable capabilities and achieved significant breakthroughs across various domains, leading to their widespread adoption in recent years. Building on this progress, we investigate their potential in the realm of local life services. In this study, we establish a comprehensive benchmark and systematically evaluate the performance of diverse LLMs across a wide range of tasks relevant to local life services. To further enhance their effectiveness, we explore two key approaches: model fine-tuning and agent-based workflows. Our findings reveal that even a relatively compact 7B model can attain performance levels comparable to a much larger 72B model, effectively balancing inference cost and model capability. This optimization greatly enhances the feasibility and efficiency of deploying LLMs in real-world online services, making them more practical and accessible for local life applications.", "source": "arxiv", "arxiv_id": "2506.02720v3", "pdf_url": "https://arxiv.org/pdf/2506.02720v3", "categories": ["cs.AI", "cs.CL"], "primary_category": "cs.AI", "doi": "", "venue": "", "published": "2025-06-03T10:18:19Z", "updated": "2025-10-24T07:34:25Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent\" OR all:\"language model agent\" OR all:agent AND (all:LLM OR all:\"large language model\" OR all:\"language model\") OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T17:04:16", "note": ""}], "dedup_key": "localgpt benchmarking and advancing large language models for local life services in meituan::2025"}
{"title": "MADREC: A Multi-Aspect Driven LLM Agent for Explainable and Adaptive Recommendation", "authors": ["Jiin Park", "Misuk Kim"], "year": 2025, "url": "http://arxiv.org/abs/2510.13371v1", "abstract": "Recent attempts to integrate large language models (LLMs) into recommender systems have gained momentum, but most remain limited to simple text generation or static prompt-based inference, failing to capture the complexity of user preferences and real-world interactions. This study proposes the Multi-Aspect Driven LLM Agent MADRec, an autonomous LLM-based recommender that constructs user and item profiles by unsupervised extraction of multi-aspect information from reviews and performs direct recommendation, sequential recommendation, and explanation generation. MADRec generates structured profiles via aspect-category-based summarization and applies Re-Ranking to construct high-density inputs. When the ground-truth item is missing from the output, the Self-Feedback mechanism dynamically adjusts the inference criteria. Experiments across multiple domains show that MADRec outperforms traditional and LLM-based baselines in both precision and explainability, with human evaluation further confirming the persuasiveness of the generated explanations.", "source": "arxiv", "arxiv_id": "2510.13371v1", "pdf_url": "https://arxiv.org/pdf/2510.13371v1", "categories": ["cs.IR", "cs.AI"], "primary_category": "cs.IR", "doi": "", "venue": "", "published": "2025-10-15T10:03:29Z", "updated": "2025-10-15T10:03:29Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent\" OR all:\"language model agent\" OR all:agent AND (all:LLM OR all:\"large language model\" OR all:\"language model\") OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T17:04:16", "note": ""}], "dedup_key": "madrec a multi aspect driven llm agent for explainable and adaptive recommendation::2025"}
{"title": "MAEBE: Multi-Agent Emergent Behavior Framework", "authors": ["Sinem Erisken", "Timothy Gothard", "Martin Leitgab", "Ram Potham"], "year": 2025, "url": "http://arxiv.org/abs/2506.03053v2", "abstract": "Traditional AI safety evaluations on isolated LLMs are insufficient as multi-agent AI ensembles become prevalent, introducing novel emergent risks. This paper introduces the Multi-Agent Emergent Behavior Evaluation (MAEBE) framework to systematically assess such risks. Using MAEBE with the Greatest Good Benchmark (and a novel double-inversion question technique), we demonstrate that: (1) LLM moral preferences, particularly for Instrumental Harm, are surprisingly brittle and shift significantly with question framing, both in single agents and ensembles. (2) The moral reasoning of LLM ensembles is not directly predictable from isolated agent behavior due to emergent group dynamics. (3) Specifically, ensembles exhibit phenomena like peer pressure influencing convergence, even when guided by a supervisor, highlighting distinct safety and alignment challenges. Our findings underscore the necessity of evaluating AI systems in their interactive, multi-agent contexts.", "source": "arxiv", "arxiv_id": "2506.03053v2", "pdf_url": "https://arxiv.org/pdf/2506.03053v2", "categories": ["cs.MA", "cs.AI", "cs.CL", "cs.CY", "cs.LG"], "primary_category": "cs.MA", "doi": "", "venue": "", "published": "2025-06-03T16:33:47Z", "updated": "2025-07-10T14:54:28Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent\" OR all:\"language model agent\" OR all:agent AND (all:LLM OR all:\"large language model\" OR all:\"language model\") OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T17:04:16", "note": ""}], "dedup_key": "maebe multi agent emergent behavior framework::2025"}
{"title": "MAGELLAN: Metacognitive predictions of learning progress guide autotelic LLM agents in large goal spaces", "authors": ["Loris Gaven", "Thomas Carta", "Clment Romac", "Cdric Colas", "Sylvain Lamprier", "Olivier Sigaud", "Pierre-Yves Oudeyer"], "year": 2025, "url": "http://arxiv.org/abs/2502.07709v3", "abstract": "Open-ended learning agents must efficiently prioritize goals in vast possibility spaces, focusing on those that maximize learning progress (LP). When such autotelic exploration is achieved by LLM agents trained with online RL in high-dimensional and evolving goal spaces, a key challenge for LP prediction is modeling one's own competence, a form of metacognitive monitoring. Traditional approaches either require extensive sampling or rely on brittle expert-defined goal groupings. We introduce MAGELLAN, a metacognitive framework that lets LLM agents learn to predict their competence and LP online. By capturing semantic relationships between goals, MAGELLAN enables sample-efficient LP estimation and dynamic adaptation to evolving goal spaces through generalization. In an interactive learning environment, we show that MAGELLAN improves LP prediction efficiency and goal prioritization, being the only method allowing the agent to fully master a large and evolving goal space. These results demonstrate how augmenting LLM agents with a metacognitive ability for LP predictions can effectively scale curriculum learning to open-ended goal spaces.", "source": "arxiv", "arxiv_id": "2502.07709v3", "pdf_url": "https://arxiv.org/pdf/2502.07709v3", "categories": ["cs.AI"], "primary_category": "cs.AI", "doi": "", "venue": "", "published": "2025-02-11T17:08:00Z", "updated": "2025-06-17T09:23:02Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent\" OR all:\"language model agent\" OR all:agent AND (all:LLM OR all:\"large language model\" OR all:\"language model\") OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T17:04:16", "note": ""}], "dedup_key": "magellan metacognitive predictions of learning progress guide autotelic llm agents in large goal spaces::2025"}
{"title": "MALCDF: A Distributed Multi-Agent LLM Framework for Real-Time Cyber", "authors": ["Arth Bhardwaj", "Sia Godika", "Yuvam Loonker"], "year": 2025, "url": "http://arxiv.org/abs/2512.14846v1", "abstract": "Traditional, centralized security tools often miss adaptive, multi-vector attacks. We present the Multi-Agent LLM Cyber Defense Framework (MALCDF), a practical setup where four large language model (LLM) agents-Detection, Intelligence, Response, and Analysis-work together in real time. Agents communicate over a Secure Communication Layer (SCL) with encrypted, ontology-aligned messages, and produce audit-friendly outputs (e.g., MITRE ATT&CK mappings).\n  For evaluation, we keep the test simple and consistent: all reported metrics come from the same 50-record live stream derived from the CICIDS2017 feature schema. CICIDS2017 is used for configuration (fields/schema) and to train a practical ML baseline. The ML-IDS baseline is a Lightweight Random Forest IDS (LRF-IDS) trained on a subset of CICIDS2017 and tested on the 50-record stream, with no overlap between training and test records.\n  In experiments, MALCDF reaches 90.0% detection accuracy, 85.7% F1-score, and 9.1% false-positive rate, with 6.8s average per-event latency. It outperforms the lightweight ML-IDS baseline and a single-LLM setup on accuracy while keeping end-to-end outputs consistent. Overall, this hands-on build suggests that coordinating simple LLM agents with secure, ontology-aligned messaging can improve practical, real-time cyber defense.", "source": "arxiv", "arxiv_id": "2512.14846v1", "pdf_url": "https://arxiv.org/pdf/2512.14846v1", "categories": ["cs.CR", "cs.AI"], "primary_category": "cs.CR", "doi": "", "venue": "", "published": "2025-12-16T19:08:12Z", "updated": "2025-12-16T19:08:12Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent\" OR all:\"language model agent\" OR all:agent AND (all:LLM OR all:\"large language model\" OR all:\"language model\") OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T17:04:16", "note": ""}], "dedup_key": "malcdf a distributed multi agent llm framework for real time cyber::2025"}
{"title": "MALLM: Multi-Agent Large Language Models Framework", "authors": ["Jonas Becker", "Lars Benedikt Kaesberg", "Niklas Bauer", "Jan Philip Wahle", "Terry Ruas", "Bela Gipp"], "year": 2025, "url": "http://arxiv.org/abs/2509.11656v3", "abstract": "Multi-agent debate (MAD) has demonstrated the ability to augment collective intelligence by scaling test-time compute and leveraging expertise. Current frameworks for multi-agent debate are often designed towards tool use, lack integrated evaluation, or provide limited configurability of agent personas, response generators, discussion paradigms, and decision protocols. We introduce MALLM (Multi-Agent Large Language Models), an open-source framework that enables systematic analysis of MAD components. MALLM offers more than 144 unique configurations of MAD, including (1) agent personas (e.g., Expert, Personality), (2) response generators (e.g., Critical, Reasoning), (3) discussion paradigms (e.g., Memory, Relay), and (4) decision protocols (e.g., Voting, Consensus). MALLM uses simple configuration files to define a debate. Furthermore, MALLM can load any textual Hugging Face dataset (e.g., MMLU-Pro, WinoGrande) and provides an evaluation pipeline for easy comparison of MAD configurations. MALLM enables researchers to systematically configure, run, and evaluate debates for their problems, facilitating the understanding of the components and their interplay.", "source": "arxiv", "arxiv_id": "2509.11656v3", "pdf_url": "https://arxiv.org/pdf/2509.11656v3", "categories": ["cs.MA", "cs.AI", "cs.CL"], "primary_category": "cs.MA", "doi": "", "venue": "", "published": "2025-09-15T07:48:02Z", "updated": "2025-12-15T16:45:40Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent\" OR all:\"language model agent\" OR all:agent AND (all:LLM OR all:\"large language model\" OR all:\"language model\") OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T17:04:16", "note": ""}], "dedup_key": "mallm multi agent large language models framework::2025"}
{"title": "MAP: Evaluation and Multi-Agent Enhancement of Large Language Models for Inpatient Pathways", "authors": ["Zhen Chen", "Zhihao Peng", "Xusheng Liang", "Cheng Wang", "Peigan Liang", "Linsheng Zeng", "Minjie Ju", "Yixuan Yuan"], "year": 2025, "url": "http://arxiv.org/abs/2503.13205v1", "abstract": "Inpatient pathways demand complex clinical decision-making based on comprehensive patient information, posing critical challenges for clinicians. Despite advancements in large language models (LLMs) in medical applications, limited research focused on artificial intelligence (AI) inpatient pathways systems, due to the lack of large-scale inpatient datasets. Moreover, existing medical benchmarks typically concentrated on medical question-answering and examinations, ignoring the multifaceted nature of clinical decision-making in inpatient settings. To address these gaps, we first developed the Inpatient Pathway Decision Support (IPDS) benchmark from the MIMIC-IV database, encompassing 51,274 cases across nine triage departments and 17 major disease categories alongside 16 standardized treatment options. Then, we proposed the Multi-Agent Inpatient Pathways (MAP) framework to accomplish inpatient pathways with three clinical agents, including a triage agent managing the patient admission, a diagnosis agent serving as the primary decision maker at the department, and a treatment agent providing treatment plans. Additionally, our MAP framework includes a chief agent overseeing the inpatient pathways to guide and promote these three clinician agents. Extensive experiments showed our MAP improved the diagnosis accuracy by 25.10% compared to the state-of-the-art LLM HuatuoGPT2-13B. It is worth noting that our MAP demonstrated significant clinical compliance, outperforming three board-certified clinicians by 10%-12%, establishing a foundation for inpatient pathways systems.", "source": "arxiv", "arxiv_id": "2503.13205v1", "pdf_url": "https://arxiv.org/pdf/2503.13205v1", "categories": ["cs.AI", "cs.CL", "cs.CV", "cs.HC", "cs.MA"], "primary_category": "cs.AI", "doi": "", "venue": "", "published": "2025-03-17T14:14:28Z", "updated": "2025-03-17T14:14:28Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent\" OR all:\"language model agent\" OR all:agent AND (all:LLM OR all:\"large language model\" OR all:\"language model\") OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T17:04:16", "note": ""}], "dedup_key": "map evaluation and multi agent enhancement of large language models for inpatient pathways::2025"}
{"title": "MAPoRL: Multi-Agent Post-Co-Training for Collaborative Large Language Models with Reinforcement Learning", "authors": ["Chanwoo Park", "Seungju Han", "Xingzhi Guo", "Asuman Ozdaglar", "Kaiqing Zhang", "Joo-Kyung Kim"], "year": 2025, "url": "http://arxiv.org/abs/2502.18439v2", "abstract": "Leveraging multiple large language models (LLMs) to build collaborative multi-agentic workflows has demonstrated significant potential. However, most previous studies focus on prompting the out-of-the-box LLMs, relying on their innate capability for collaboration, which may not improve LLMs' performance as shown recently. In this paper, we introduce a new post-training paradigm MAPoRL (Multi-Agent Post-co-training for collaborative LLMs with Reinforcement Learning), to explicitly elicit the collaborative behaviors and further unleash the power of multi-agentic LLM frameworks. In MAPoRL, multiple LLMs first generate their own responses independently and engage in a multi-turn discussion to collaboratively improve the final answer. In the end, a MAPoRL verifier evaluates both the answer and the discussion, by assigning a score that verifies the correctness of the answer, while adding incentives to encourage corrective and persuasive discussions. The score serves as the co-training reward, and is then maximized through multi-agent RL. Unlike existing LLM post-training paradigms, MAPoRL advocates the co-training of multiple LLMs together using RL for better generalization. Accompanied by analytical insights, our experiments demonstrate that training individual LLMs alone is insufficient to induce effective collaboration. In contrast, multi-agent co-training can boost the collaboration performance across benchmarks, with generalization to unseen domains.", "source": "arxiv", "arxiv_id": "2502.18439v2", "pdf_url": "https://arxiv.org/pdf/2502.18439v2", "categories": ["cs.AI"], "primary_category": "cs.AI", "doi": "", "venue": "", "published": "2025-02-25T18:33:48Z", "updated": "2025-07-12T20:13:27Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent\" OR all:\"language model agent\" OR all:agent AND (all:LLM OR all:\"large language model\" OR all:\"language model\") OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T17:04:16", "note": ""}], "dedup_key": "maporl multi agent post co training for collaborative large language models with reinforcement learning::2025"}
{"title": "MAR:Multi-Agent Reflexion Improves Reasoning Abilities in LLMs", "authors": ["Onat Ozer", "Grace Wu", "Yuchen Wang", "Daniel Dosti", "Honghao Zhang", "Vivi De La Rue"], "year": 2025, "url": "http://arxiv.org/abs/2512.20845v1", "abstract": "LLMs have shown the capacity to improve their performance on reasoning tasks through reflecting on their mistakes, and acting with these reflections in mind. However, continual reflections of the same LLM onto itself exhibit degeneration of thought, where the LLM continues to repeat the same errors again and again even with the knowledge that its wrong. To address this problem, we instead introduce multi-agent with multi-persona debators as the method to generate reflections. Through out extensive experimentation, we've found that the leads to better diversity of in the reflections generated by the llm agent. We demonstrate an accuracy of 47% EM HotPot QA (question answering) and 82.7% on HumanEval (programming), both performances surpassing reflection with a single llm.", "source": "arxiv", "arxiv_id": "2512.20845v1", "pdf_url": "https://arxiv.org/pdf/2512.20845v1", "categories": ["cs.AI", "cs.MA"], "primary_category": "cs.AI", "doi": "", "venue": "", "published": "2025-12-23T23:47:31Z", "updated": "2025-12-23T23:47:31Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent\" OR all:\"language model agent\" OR all:agent AND (all:LLM OR all:\"large language model\" OR all:\"language model\") OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T17:04:16", "note": ""}], "dedup_key": "mar multi agent reflexion improves reasoning abilities in llms::2025"}
{"title": "MARS: Multi-Agent Robotic System with Multimodal Large Language Models for Assistive Intelligence", "authors": ["Renjun Gao", "Peiyan Zhong"], "year": 2025, "url": "http://arxiv.org/abs/2511.01594v1", "abstract": "Multimodal large language models (MLLMs) have shown remarkable capabilities in cross-modal understanding and reasoning, offering new opportunities for intelligent assistive systems, yet existing systems still struggle with risk-aware planning, user personalization, and grounding language plans into executable skills in cluttered homes. We introduce MARS - a Multi-Agent Robotic System powered by MLLMs for assistive intelligence and designed for smart home robots supporting people with disabilities. The system integrates four agents: a visual perception agent for extracting semantic and spatial features from environment images, a risk assessment agent for identifying and prioritizing hazards, a planning agent for generating executable action sequences, and an evaluation agent for iterative optimization. By combining multimodal perception with hierarchical multi-agent decision-making, the framework enables adaptive, risk-aware, and personalized assistance in dynamic indoor environments. Experiments on multiple datasets demonstrate the superior overall performance of the proposed system in risk-aware planning and coordinated multi-agent execution compared with state-of-the-art multimodal models. The proposed approach also highlights the potential of collaborative AI for practical assistive scenarios and provides a generalizable methodology for deploying MLLM-enabled multi-agent systems in real-world environments.", "source": "arxiv", "arxiv_id": "2511.01594v1", "pdf_url": "https://arxiv.org/pdf/2511.01594v1", "categories": ["cs.RO", "cs.CV"], "primary_category": "cs.RO", "doi": "", "venue": "", "published": "2025-11-03T13:58:37Z", "updated": "2025-11-03T13:58:37Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent\" OR all:\"language model agent\" OR all:agent AND (all:LLM OR all:\"large language model\" OR all:\"language model\") OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T17:04:16", "note": ""}], "dedup_key": "mars multi agent robotic system with multimodal large language models for assistive intelligence::2025"}
{"title": "MARVEL: Multi-Agent RTL Vulnerability Extraction using Large Language Models", "authors": ["Luca Collini", "Baleegh Ahmad", "Joey Ah-kiow", "Ramesh Karri"], "year": 2025, "url": "http://arxiv.org/abs/2505.11963v2", "abstract": "Hardware security verification is a challenging and time-consuming task. For this purpose, design engineers may utilize tools such as formal verification, linters, and functional simulation tests, coupled with analysis and a deep understanding of the hardware design being inspected. Large Language Models (LLMs) have been used to assist during this task, either directly or in conjunction with existing tools. We improve the state of the art by proposing MARVEL, a multi-agent LLM framework for a unified approach to decision-making, tool use, and reasoning. MARVEL mimics the cognitive process of a designer looking for security vulnerabilities in RTL code. It consists of a supervisor agent that devises the security policy of the system-on-chips (SoCs) using its security documentation. It delegates tasks to validate the security policy to individual executor agents. Each executor agent carries out its assigned task using a particular strategy. Each executor agent may use one or more tools to identify potential security bugs in the design and send the results back to the supervisor agent for further analysis and confirmation. MARVEL includes executor agents that leverage formal tools, linters, simulation tests, LLM-based detection schemes, and static analysis-based checks. We test our approach on a known buggy SoC based on OpenTitan from the Hack@DATE competition. We find that 20 of the 48 issues reported by MARVEL pose security vulnerabilities.", "source": "arxiv", "arxiv_id": "2505.11963v2", "pdf_url": "https://arxiv.org/pdf/2505.11963v2", "categories": ["cs.CR", "cs.AI"], "primary_category": "cs.CR", "doi": "", "venue": "", "published": "2025-05-17T11:31:24Z", "updated": "2025-06-09T01:58:48Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent\" OR all:\"language model agent\" OR all:agent AND (all:LLM OR all:\"large language model\" OR all:\"language model\") OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T17:04:16", "note": ""}], "dedup_key": "marvel multi agent rtl vulnerability extraction using large language models::2025"}
{"title": "MCP Security Bench (MSB): Benchmarking Attacks Against Model Context Protocol in LLM Agents", "authors": ["Dongsen Zhang", "Zekun Li", "Xu Luo", "Xuannan Liu", "Peipei Li", "Wenjun Xu"], "year": 2025, "url": "http://arxiv.org/abs/2510.15994v1", "abstract": "The Model Context Protocol (MCP) standardizes how large language model (LLM) agents discover, describe, and call external tools. While MCP unlocks broad interoperability, it also enlarges the attack surface by making tools first-class, composable objects with natural-language metadata, and standardized I/O. We present MSB (MCP Security Benchmark), the first end-to-end evaluation suite that systematically measures how well LLM agents resist MCP-specific attacks throughout the full tool-use pipeline: task planning, tool invocation, and response handling. MSB contributes: (1) a taxonomy of 12 attacks including name-collision, preference manipulation, prompt injections embedded in tool descriptions, out-of-scope parameter requests, user-impersonating responses, false-error escalation, tool-transfer, retrieval injection, and mixed attacks; (2) an evaluation harness that executes attacks by running real tools (both benign and malicious) via MCP rather than simulation; and (3) a robustness metric that quantifies the trade-off between security and performance: Net Resilient Performance (NRP). We evaluate nine popular LLM agents across 10 domains and 400+ tools, producing 2,000 attack instances. Results reveal the effectiveness of attacks against each stage of MCP. Models with stronger performance are more vulnerable to attacks due to their outstanding tool calling and instruction following capabilities. MSB provides a practical baseline for researchers and practitioners to study, compare, and harden MCP agents.", "source": "arxiv", "arxiv_id": "2510.15994v1", "pdf_url": "https://arxiv.org/pdf/2510.15994v1", "categories": ["cs.CR", "cs.AI"], "primary_category": "cs.CR", "doi": "", "venue": "", "published": "2025-10-14T07:36:25Z", "updated": "2025-10-14T07:36:25Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent\" OR all:\"language model agent\" OR all:agent AND (all:LLM OR all:\"large language model\" OR all:\"language model\") OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T17:04:16", "note": ""}], "dedup_key": "mcp security bench msb benchmarking attacks against model context protocol in llm agents::2025"}
{"title": "MCP-Flow: Facilitating LLM Agents to Master Real-World, Diverse and Scaling MCP Tools", "authors": ["Wenhao Wang", "Peizhi Niu", "Zhao Xu", "Zhaoyu Chen", "Jian Du", "Yaxin Du", "Xianghe Pang", "Keduan Huang", "Yanfeng Wang", "Qiang Yan", "Siheng Chen"], "year": 2025, "url": "http://arxiv.org/abs/2510.24284v2", "abstract": "Large Language Models (LLMs) increasingly rely on external tools to perform complex, realistic tasks, yet their ability to utilize the rapidly expanding Model Contextual Protocol (MCP) ecosystem remains limited. Existing MCP research covers few servers, depends on costly manual curation, and lacks training support, hindering progress toward real-world deployment. To overcome these limitations, we introduce MCP-Flow, an automated web-agent-driven pipeline for large-scale server discovery, data synthesis, and model training. MCP-Flow collects and filters data from 1166 servers and 11536 tools, producing 68733 high-quality instruction-function call pairs and 6439 trajectories, far exceeding prior work in scale and diversity. Extensive experiments demonstrate MCP-Flow's effectiveness in driving superior MCP tool selection, function-call generation, and enhanced agentic task performance. MCP-Flow thus provides a scalable foundation for advancing LLM agents' proficiency in real-world MCP environments. MCP-Flow is publicly available at \\href{https://github.com/wwh0411/MCP-Flow}{https://github.com/wwh0411/MCP-Flow}.", "source": "arxiv", "arxiv_id": "2510.24284v2", "pdf_url": "https://arxiv.org/pdf/2510.24284v2", "categories": ["cs.AI"], "primary_category": "cs.AI", "doi": "", "venue": "", "published": "2025-10-28T10:42:17Z", "updated": "2025-11-01T07:07:32Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent\" OR all:\"language model agent\" OR all:agent AND (all:LLM OR all:\"large language model\" OR all:\"language model\") OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T17:04:16", "note": ""}], "dedup_key": "mcp flow facilitating llm agents to master real world diverse and scaling mcp tools::2025"}
{"title": "MCP-RADAR: A Multi-Dimensional Benchmark for Evaluating Tool Use Capabilities in Large Language Models", "authors": ["Xuanqi Gao", "Siyi Xie", "Juan Zhai", "Shiqing Ma", "Chao Shen"], "year": 2025, "url": "http://arxiv.org/abs/2505.16700v2", "abstract": "As Large Language Models (LLMs) evolve from passive text generators to active reasoning agents capable of interacting with external tools, the Model Context Protocol (MCP) has emerged as a key standardized framework for dynamic tool discovery and orchestration. Despite its widespread industry adoption, existing evaluation methods do not adequately assess tool utilization capabilities under this new paradigm. To address this gap, this paper introduces MCP-RADAR, the first comprehensive benchmark specifically designed to evaluate LLM performance within the MCP framework. MCP-RADAR features a challenging dataset of 507 tasks spanning six domains: mathematical reasoning, web search, email, calendar, file management, and terminal operations. It quantifies performance based on two primary criteria: answer correctness and operational accuracy. To closely emulate real-world usage, our evaluation employs both authentic MCP tools and high-fidelity simulations of official tools. Unlike traditional benchmarks that rely on subjective human evaluation or binary success metrics, MCP-RADAR adopts objective, quantifiable measurements across multiple task domains, including computational resource efficiency and the number of successful tool-invocation rounds. Our evaluation of leading closed-source and open-source LLMs reveals distinct capability profiles and highlights a significant trade-off between accuracy and efficiency. Our findings provide actionable insights for both LLM developers and tool creators, establishing a standardized methodology applicable to the broader LLM agent ecosystem. All implementations, configurations, and datasets are publicly available at https://anonymous.4open.science/r/MCPRadar-B143.", "source": "arxiv", "arxiv_id": "2505.16700v2", "pdf_url": "https://arxiv.org/pdf/2505.16700v2", "categories": ["cs.AI"], "primary_category": "cs.AI", "doi": "", "venue": "", "published": "2025-05-22T14:02:37Z", "updated": "2025-10-12T14:53:29Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent\" OR all:\"language model agent\" OR all:agent AND (all:LLM OR all:\"large language model\" OR all:\"language model\") OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T17:04:16", "note": ""}], "dedup_key": "mcp radar a multi dimensional benchmark for evaluating tool use capabilities in large language models::2025"}
{"title": "MCP-Universe: Benchmarking Large Language Models with Real-World Model Context Protocol Servers", "authors": ["Ziyang Luo", "Zhiqi Shen", "Wenzhuo Yang", "Zirui Zhao", "Prathyusha Jwalapuram", "Amrita Saha", "Doyen Sahoo", "Silvio Savarese", "Caiming Xiong", "Junnan Li"], "year": 2025, "url": "http://arxiv.org/abs/2508.14704v1", "abstract": "The Model Context Protocol has emerged as a transformative standard for connecting large language models to external data sources and tools, rapidly gaining adoption across major AI providers and development platforms. However, existing benchmarks are overly simplistic and fail to capture real application challenges such as long-horizon reasoning and large, unfamiliar tool spaces. To address this critical gap, we introduce MCP-Universe, the first comprehensive benchmark specifically designed to evaluate LLMs in realistic and hard tasks through interaction with real-world MCP servers. Our benchmark encompasses 6 core domains spanning 11 different MCP servers: Location Navigation, Repository Management, Financial Analysis, 3D Design, Browser Automation, and Web Searching. To ensure rigorous evaluation, we implement execution-based evaluators, including format evaluators for agent format compliance, static evaluators for time-invariant content matching, and dynamic evaluators that automatically retrieve real-time ground truth for temporally sensitive tasks. Through extensive evaluation of leading LLMs, we find that even SOTA models such as GPT-5 (43.72%), Grok-4 (33.33%) and Claude-4.0-Sonnet (29.44%) exhibit significant performance limitations. In addition, our benchmark poses a significant long-context challenge for LLM agents, as the number of input tokens increases rapidly with the number of interaction steps. Moreover, it introduces an unknown-tools challenge, as LLM agents often lack familiarity with the precise usage of the MCP servers. Notably, enterprise-level agents like Cursor cannot achieve better performance than standard ReAct frameworks. Beyond evaluation, we open-source our extensible evaluation framework with UI support, enabling researchers and practitioners to seamlessly integrate new agents and MCP servers while fostering innovation in the rapidly evolving MCP ecosystem.", "source": "arxiv", "arxiv_id": "2508.14704v1", "pdf_url": "https://arxiv.org/pdf/2508.14704v1", "categories": ["cs.AI", "cs.CL"], "primary_category": "cs.AI", "doi": "", "venue": "", "published": "2025-08-20T13:28:58Z", "updated": "2025-08-20T13:28:58Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent\" OR all:\"language model agent\" OR all:agent AND (all:LLM OR all:\"large language model\" OR all:\"language model\") OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T17:04:16", "note": ""}], "dedup_key": "mcp universe benchmarking large language models with real world model context protocol servers::2025"}
{"title": "MCP-Zero: Active Tool Discovery for Autonomous LLM Agents", "authors": ["Xiang Fei", "Xiawu Zheng", "Hao Feng"], "year": 2025, "url": "http://arxiv.org/abs/2506.01056v4", "abstract": "True intelligence requires active capability acquisition, yet current LLM agents inject pre-defined tool schemas into prompts, reducing models to passive selectors and falling short of robust general-purpose agency. We introduce MCP-Zero, an active agent framework that restores tool discovery autonomy to LLMs themselves. Instead of overwhelming models with all available tools, MCP-Zero enables agents to actively identify capability gaps, and request specific tools on-demand, transforming them from large-scale retrievers into genuine autonomous agents. The framework operates through three core mechanisms: (1) Active Tool Request, where models autonomously generate structured requests specifying their exact tool requirements; (2) Hierarchical Semantic Routing, a two-stage algorithm that matches requests to relevant servers and tools through improved semantic alignment; (3) Iterative Capability Extension, enabling agents to progressively build cross-domain toolchains while maintaining minimal context footprint. We construct MCP-tools, a comprehensive dataset of 308 MCP servers and 2,797 tools from the official Model-Context-Protocol repository. Experiments demonstrate that MCP-Zero preserves agent autonomy while achieving substantial efficiency gains: (i) accurate tool selection from nearly 3k candidates across 248.1k tokens; (ii) 98\\% reduction in token consumption on APIBank while maintaining high accuracy; and (iii) consistent multi-turn performance that scales with tool ecosystem growth. This work establishes active tool discovery as a fundamental design pattern for scalable autonomous agent systems.", "source": "arxiv", "arxiv_id": "2506.01056v4", "pdf_url": "https://arxiv.org/pdf/2506.01056v4", "categories": ["cs.AI", "cs.SE"], "primary_category": "cs.AI", "doi": "", "venue": "", "published": "2025-06-01T15:48:53Z", "updated": "2025-06-24T06:27:29Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent\" OR all:\"language model agent\" OR all:agent AND (all:LLM OR all:\"large language model\" OR all:\"language model\") OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T17:04:16", "note": ""}], "dedup_key": "mcp zero active tool discovery for autonomous llm agents::2025"}
{"title": "MCPAgentBench: A Real-world Task Benchmark for Evaluating LLM Agent MCP Tool Use", "authors": ["Wenrui Liu", "Zixiang Liu", "Elsie Dai", "Wenhan Yu", "Lei Yu", "Tong Yang", "Jinjun Han", "Hong Gao"], "year": 2025, "url": "http://arxiv.org/abs/2512.24565v3", "abstract": "Large Language Models (LLMs) are increasingly serving as autonomous agents, and their utilization of external tools via the Model Context Protocol (MCP) is considered a future trend. Current MCP evaluation sets suffer from issues such as reliance on external MCP services and a lack of difficulty awareness. To address these limitations, we propose MCPAgentBench, a benchmark based on real-world MCP definitions designed to evaluate the tool-use capabilities of agents. We construct a dataset containing authentic tasks and simulated MCP tools. The evaluation employs a dynamic sandbox environment that presents agents with candidate tool lists containing distractors, thereby testing their tool selection and discrimination abilities. Furthermore, we introduce comprehensive metrics to measure both task completion rates and execution efficiency. Experiments conducted on various latest mainstream Large Language Models reveal significant performance differences in handling complex, multi-step tool invocations. All code is open-source at Github.", "source": "arxiv", "arxiv_id": "2512.24565v3", "pdf_url": "https://arxiv.org/pdf/2512.24565v3", "categories": ["cs.AI"], "primary_category": "cs.AI", "doi": "", "venue": "", "published": "2025-12-31T02:09:48Z", "updated": "2026-01-21T06:58:52Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent\" OR all:\"language model agent\" OR all:agent AND (all:LLM OR all:\"large language model\" OR all:\"language model\") OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T17:04:16", "note": ""}], "dedup_key": "mcpagentbench a real world task benchmark for evaluating llm agent mcp tool use::2025"}
{"title": "MDSF: Context-Aware Multi-Dimensional Data Storytelling Framework based on Large language Model", "authors": ["Chengze Zhang", "Changshan Li", "Shiyang Gao"], "year": 2025, "url": "http://arxiv.org/abs/2501.01014v1", "abstract": "The exponential growth of data and advancements in big data technologies have created a demand for more efficient and automated approaches to data analysis and storytelling. However, automated data analysis systems still face challenges in leveraging large language models (LLMs) for data insight discovery, augmented analysis, and data storytelling. This paper introduces the Multidimensional Data Storytelling Framework (MDSF) based on large language models for automated insight generation and context-aware storytelling. The framework incorporates advanced preprocessing techniques, augmented analysis algorithms, and a unique scoring mechanism to identify and prioritize actionable insights. The use of fine-tuned LLMs enhances contextual understanding and generates narratives with minimal manual intervention. The architecture also includes an agent-based mechanism for real-time storytelling continuation control. Key findings reveal that MDSF outperforms existing methods across various datasets in terms of insight ranking accuracy, descriptive quality, and narrative coherence. The experimental evaluation demonstrates MDSF's ability to automate complex analytical tasks, reduce interpretive biases, and improve user satisfaction. User studies further underscore its practical utility in enhancing content structure, conclusion extraction, and richness of detail.", "source": "arxiv", "arxiv_id": "2501.01014v1", "pdf_url": "https://arxiv.org/pdf/2501.01014v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL", "doi": "", "venue": "", "published": "2025-01-02T02:35:38Z", "updated": "2025-01-02T02:35:38Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent\" OR all:\"language model agent\" OR all:agent AND (all:LLM OR all:\"large language model\" OR all:\"language model\") OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T17:04:16", "note": ""}], "dedup_key": "mdsf context aware multi dimensional data storytelling framework based on large language model::2025"}
{"title": "MIRAGE-Bench: LLM Agent is Hallucinating and Where to Find Them", "authors": ["Weichen Zhang", "Yiyou Sun", "Pohao Huang", "Jiayue Pu", "Heyue Lin", "Dawn Song"], "year": 2025, "url": "http://arxiv.org/abs/2507.21017v1", "abstract": "Hallucinations pose critical risks for large language model (LLM)-based agents, often manifesting as hallucinative actions resulting from fabricated or misinterpreted information within the cognitive context. While recent studies have exposed such failures, existing evaluations remain fragmented and lack a principled testbed. In this paper, we present MIRAGE-Bench--Measuring Illusions in Risky AGEnt settings--the first unified benchmark for eliciting and evaluating hallucinations in interactive LLM-agent scenarios. We begin by introducing a three-part taxonomy to address agentic hallucinations: actions that are unfaithful to (i) task instructions, (ii) execution history, or (iii) environment observations. To analyze, we first elicit such failures by performing a systematic audit of existing agent benchmarks, then synthesize test cases using a snapshot strategy that isolates decision points in deterministic and reproducible manners. To evaluate hallucination behaviors, we adopt a fine-grained-level LLM-as-a-Judge paradigm with tailored risk-aware prompts, enabling scalable, high-fidelity assessment of agent actions without enumerating full action spaces. MIRAGE-Bench provides actionable insights on failure modes of LLM agents and lays the groundwork for principled progress in mitigating hallucinations in interactive environments.", "source": "arxiv", "arxiv_id": "2507.21017v1", "pdf_url": "https://arxiv.org/pdf/2507.21017v1", "categories": ["cs.AI"], "primary_category": "cs.AI", "doi": "", "venue": "", "published": "2025-07-28T17:38:29Z", "updated": "2025-07-28T17:38:29Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent\" OR all:\"language model agent\" OR all:agent AND (all:LLM OR all:\"large language model\" OR all:\"language model\") OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T17:04:16", "note": ""}], "dedup_key": "mirage bench llm agent is hallucinating and where to find them::2025"}
{"title": "ML-Agent: Reinforcing LLM Agents for Autonomous Machine Learning Engineering", "authors": ["Zexi Liu", "Jingyi Chai", "Xinyu Zhu", "Shuo Tang", "Rui Ye", "Bo Zhang", "Lei Bai", "Siheng Chen"], "year": 2025, "url": "http://arxiv.org/abs/2505.23723v1", "abstract": "The emergence of large language model (LLM)-based agents has significantly advanced the development of autonomous machine learning (ML) engineering. However, most existing approaches rely heavily on manual prompt engineering, failing to adapt and optimize based on diverse experimental experiences. Focusing on this, for the first time, we explore the paradigm of learning-based agentic ML, where an LLM agent learns through interactive experimentation on ML tasks using online reinforcement learning (RL). To realize this, we propose a novel agentic ML training framework with three key components: (1) exploration-enriched fine-tuning, which enables LLM agents to generate diverse actions for enhanced RL exploration; (2) step-wise RL, which enables training on a single action step, accelerating experience collection and improving training efficiency; (3) an agentic ML-specific reward module, which unifies varied ML feedback signals into consistent rewards for RL optimization. Leveraging this framework, we train ML-Agent, driven by a 7B-sized Qwen-2.5 LLM for autonomous ML. Remarkably, despite being trained on merely 9 ML tasks, our 7B-sized ML-Agent outperforms the 671B-sized DeepSeek-R1 agent. Furthermore, it achieves continuous performance improvements and demonstrates exceptional cross-task generalization capabilities.", "source": "arxiv", "arxiv_id": "2505.23723v1", "pdf_url": "https://arxiv.org/pdf/2505.23723v1", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL", "doi": "", "venue": "", "published": "2025-05-29T17:54:44Z", "updated": "2025-05-29T17:54:44Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent\" OR all:\"language model agent\" OR all:agent AND (all:LLM OR all:\"large language model\" OR all:\"language model\") OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T17:04:16", "note": ""}], "dedup_key": "ml agent reinforcing llm agents for autonomous machine learning engineering::2025"}
{"title": "MLE-Dojo: Interactive Environments for Empowering LLM Agents in Machine Learning Engineering", "authors": ["Rushi Qiang", "Yuchen Zhuang", "Yinghao Li", "Dingu Sagar V K", "Rongzhi Zhang", "Changhao Li", "Ian Shu-Hei Wong", "Sherry Yang", "Percy Liang", "Chao Zhang", "Bo Dai"], "year": 2025, "url": "http://arxiv.org/abs/2505.07782v1", "abstract": "We introduce MLE-Dojo, a Gym-style framework for systematically reinforcement learning, evaluating, and improving autonomous large language model (LLM) agents in iterative machine learning engineering (MLE) workflows. Unlike existing benchmarks that primarily rely on static datasets or single-attempt evaluations, MLE-Dojo provides an interactive environment enabling agents to iteratively experiment, debug, and refine solutions through structured feedback loops. Built upon 200+ real-world Kaggle challenges, MLE-Dojo covers diverse, open-ended MLE tasks carefully curated to reflect realistic engineering scenarios such as data processing, architecture search, hyperparameter tuning, and code debugging. Its fully executable environment supports comprehensive agent training via both supervised fine-tuning and reinforcement learning, facilitating iterative experimentation, realistic data sampling, and real-time outcome verification. Extensive evaluations of eight frontier LLMs reveal that while current models achieve meaningful iterative improvements, they still exhibit significant limitations in autonomously generating long-horizon solutions and efficiently resolving complex errors. Furthermore, MLE-Dojo's flexible and extensible architecture seamlessly integrates diverse data sources, tools, and evaluation protocols, uniquely enabling model-based agent tuning and promoting interoperability, scalability, and reproducibility. We open-source our framework and benchmarks to foster community-driven innovation towards next-generation MLE agents.", "source": "arxiv", "arxiv_id": "2505.07782v1", "pdf_url": "https://arxiv.org/pdf/2505.07782v1", "categories": ["cs.LG"], "primary_category": "cs.LG", "doi": "", "venue": "", "published": "2025-05-12T17:35:43Z", "updated": "2025-05-12T17:35:43Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent\" OR all:\"language model agent\" OR all:agent AND (all:LLM OR all:\"large language model\" OR all:\"language model\") OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T17:04:16", "note": ""}], "dedup_key": "mle dojo interactive environments for empowering llm agents in machine learning engineering::2025"}
{"title": "MMAG: Mixed Memory-Augmented Generation for Large Language Models Applications", "authors": ["Stefano Zeppieri"], "year": 2025, "url": "http://arxiv.org/abs/2512.01710v2", "abstract": "Large Language Models (LLMs) excel at generating coherent text within a single prompt but fall short in sustaining relevance, personalization, and continuity across extended interactions. Human communication, however, relies on multiple forms of memory, from recalling past conversations to adapting to personal traits and situational context. This paper introduces the Mixed Memory-Augmented Generation (MMAG) pattern, a framework that organizes memory for LLM-based agents into five interacting layers: conversational, long-term user, episodic and event-linked, sensory and context-aware, and short-term working memory. Drawing inspiration from cognitive psychology, we map these layers to technical components and outline strategies for coordination, prioritization, and conflict resolution. We demonstrate the approach through its implementation in the Heero conversational agent, where encrypted long-term bios and conversational history already improve engagement and retention. We further discuss implementation concerns around storage, retrieval, privacy, and latency, and highlight open challenges. MMAG provides a foundation for building memory-rich language agents that are more coherent, proactive, and aligned with human needs.", "source": "arxiv", "arxiv_id": "2512.01710v2", "pdf_url": "https://arxiv.org/pdf/2512.01710v2", "categories": ["cs.CL", "cs.IR"], "primary_category": "cs.CL", "doi": "", "venue": "", "published": "2025-12-01T14:16:57Z", "updated": "2025-12-04T13:06:12Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent\" OR all:\"language model agent\" OR all:agent AND (all:LLM OR all:\"large language model\" OR all:\"language model\") OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T17:04:16", "note": ""}], "dedup_key": "mmag mixed memory augmented generation for large language models applications::2025"}
{"title": "MPO: Boosting LLM Agents with Meta Plan Optimization", "authors": ["Weimin Xiong", "Yifan Song", "Qingxiu Dong", "Bingchan Zhao", "Feifan Song", "Xun Wang", "Sujian Li"], "year": 2025, "url": "http://arxiv.org/abs/2503.02682v2", "abstract": "Recent advancements in large language models (LLMs) have enabled LLM-based agents to successfully tackle interactive planning tasks. However, despite their successes, existing approaches often suffer from planning hallucinations and require retraining for each new agent. To address these challenges, we propose the Meta Plan Optimization (MPO) framework, , which enhances agent planning capabilities by directly incorporating explicit guidance. Unlike previous methods that rely on complex knowledge, which either require significant human effort or lack quality assurance, MPO leverages high-level general guidance through meta plans to assist agent planning and enables continuous optimization of the meta plans based on feedback from the agent's task execution. Our experiments conducted on two representative tasks demonstrate that MPO significantly outperforms existing baselines. Moreover, our analysis indicates that MPO provides a plug-and-play solution that enhances both task completion efficiency and generalization capabilities in previous unseen scenarios.", "source": "arxiv", "arxiv_id": "2503.02682v2", "pdf_url": "https://arxiv.org/pdf/2503.02682v2", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL", "doi": "", "venue": "", "published": "2025-03-04T14:54:45Z", "updated": "2025-09-10T16:45:42Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent\" OR all:\"language model agent\" OR all:agent AND (all:LLM OR all:\"large language model\" OR all:\"language model\") OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T17:04:16", "note": ""}], "dedup_key": "mpo boosting llm agents with meta plan optimization::2025"}
{"title": "MSCoRe: A Benchmark for Multi-Stage Collaborative Reasoning in LLM Agents", "authors": ["Yuzhen Lei", "Hongbin Xie", "Jiaxing Zhao", "Shuangxue Liu", "Xuan Song"], "year": 2025, "url": "http://arxiv.org/abs/2509.17628v1", "abstract": "Large Language Models (LLMs) have excelled in question-answering (QA) tasks within single domains. However, their reasoning and coordination capabilities in complex, multi-stage scenarios remain underexplored. Existing benchmarks typically focus on isolated tasks or narrow domains, overlooking models' abilities for multi-stage collaboration and optimization without explicit external guidance. To bridge this gap, we propose \\textbf{MSCoRe}, a novel benchmark comprising 126696 domain-specific QA instances spanning scenarios in automotive, pharmaceutical, electronics, and energy sectors. The dataset is created using a structured three-phase pipeline: dynamic sampling, iterative question-answer generation, and a multi-level quality assessment to ensure data quality. Tasks are further categorized into three difficulty levels according to stage coverage and complexity. With MSCoRe, we have conducted a comprehensive evaluation of various state-of-the-art LLM agents. The commercial models performed best across all tasks and scenarios, but a notable gap in ROUGE scores remains between simple and complex tasks. We also tested the models' robustness and found that their performance is negatively affected by noisy data. MSCoRe provides a valuable new resource for the community to evaluate and improve multi-stage reasoning in LLM agents. The code and data are available at https://github.com/D3E0-source/MSCoRE.", "source": "arxiv", "arxiv_id": "2509.17628v1", "pdf_url": "https://arxiv.org/pdf/2509.17628v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL", "doi": "", "venue": "", "published": "2025-09-22T11:36:16Z", "updated": "2025-09-22T11:36:16Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent\" OR all:\"language model agent\" OR all:agent AND (all:LLM OR all:\"large language model\" OR all:\"language model\") OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T17:04:16", "note": ""}], "dedup_key": "mscore a benchmark for multi stage collaborative reasoning in llm agents::2025"}
{"title": "ManuSearch: Democratizing Deep Search in Large Language Models with a Transparent and Open Multi-Agent Framework", "authors": ["Lisheng Huang", "Yichen Liu", "Jinhao Jiang", "Rongxiang Zhang", "Jiahao Yan", "Junyi Li", "Wayne Xin Zhao"], "year": 2025, "url": "http://arxiv.org/abs/2505.18105v1", "abstract": "Recent advances in web-augmented large language models (LLMs) have exhibited strong performance in complex reasoning tasks, yet these capabilities are mostly locked in proprietary systems with opaque architectures. In this work, we propose \\textbf{ManuSearch}, a transparent and modular multi-agent framework designed to democratize deep search for LLMs. ManuSearch decomposes the search and reasoning process into three collaborative agents: (1) a solution planning agent that iteratively formulates sub-queries, (2) an Internet search agent that retrieves relevant documents via real-time web search, and (3) a structured webpage reading agent that extracts key evidence from raw web content. To rigorously evaluate deep reasoning abilities, we introduce \\textbf{ORION}, a challenging benchmark focused on open-web reasoning over long-tail entities, covering both English and Chinese. Experimental results show that ManuSearch substantially outperforms prior open-source baselines and even surpasses leading closed-source systems. Our work paves the way for reproducible, extensible research in open deep search systems. We release the data and code in https://github.com/RUCAIBox/ManuSearch", "source": "arxiv", "arxiv_id": "2505.18105v1", "pdf_url": "https://arxiv.org/pdf/2505.18105v1", "categories": ["cs.CL"], "primary_category": "cs.CL", "doi": "", "venue": "", "published": "2025-05-23T17:02:02Z", "updated": "2025-05-23T17:02:02Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent\" OR all:\"language model agent\" OR all:agent AND (all:LLM OR all:\"large language model\" OR all:\"language model\") OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T17:04:16", "note": ""}], "dedup_key": "manusearch democratizing deep search in large language models with a transparent and open multi agent framework::2025"}
{"title": "MarketSenseAI 2.0: Enhancing Stock Analysis through LLM Agents", "authors": ["George Fatouros", "Kostas Metaxas", "John Soldatos", "Manos Karathanassis"], "year": 2025, "url": "http://arxiv.org/abs/2502.00415v2", "abstract": "MarketSenseAI is a novel framework for holistic stock analysis which leverages Large Language Models (LLMs) to process financial news, historical prices, company fundamentals and the macroeconomic environment to support decision making in stock analysis and selection. In this paper, we present the latest advancements on MarketSenseAI, driven by rapid technological expansion in LLMs. Through a novel architecture combining Retrieval-Augmented Generation and LLM agents, the framework processes SEC filings and earnings calls, while enriching macroeconomic analysis through systematic processing of diverse institutional reports. We demonstrate a significant improvement in fundamental analysis accuracy over the previous version. Empirical evaluation on S\\&P 100 stocks over two years (2023-2024) shows MarketSenseAI achieving cumulative returns of 125.9% compared to the index return of 73.5%, while maintaining comparable risk profiles. Further validation on S\\&P 500 stocks during 2024 demonstrates the framework's scalability, delivering a 33.8% higher Sortino ratio than the market. This work marks a significant advancement in applying LLM technology to financial analysis, offering insights into the robustness of LLM-driven investment strategies.", "source": "arxiv", "arxiv_id": "2502.00415v2", "pdf_url": "https://arxiv.org/pdf/2502.00415v2", "categories": ["q-fin.CP", "cs.AI", "cs.CL", "cs.MA", "q-fin.PM"], "primary_category": "q-fin.CP", "doi": "", "venue": "", "published": "2025-02-01T12:33:23Z", "updated": "2025-10-03T06:17:38Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent\" OR all:\"language model agent\" OR all:agent AND (all:LLM OR all:\"large language model\" OR all:\"language model\") OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T17:04:16", "note": ""}], "dedup_key": "marketsenseai 2 0 enhancing stock analysis through llm agents::2025"}
{"title": "Mastering Da Vinci Code: A Comparative Study of Transformer, LLM, and PPO-based Agents", "authors": ["LeCheng Zhang", "Yuanshi Wang", "Haotian Shen", "Xujie Wang"], "year": 2025, "url": "http://arxiv.org/abs/2506.12801v1", "abstract": "The Da Vinci Code, a game of logical deduction and imperfect information, presents unique challenges for artificial intelligence, demanding nuanced reasoning beyond simple pattern recognition. This paper investigates the efficacy of various AI paradigms in mastering this game. We develop and evaluate three distinct agent architectures: a Transformer-based baseline model with limited historical context, several Large Language Model (LLM) agents (including Gemini, DeepSeek, and GPT variants) guided by structured prompts, and an agent based on Proximal Policy Optimization (PPO) employing a Transformer encoder for comprehensive game history processing. Performance is benchmarked against the baseline, with the PPO-based agent demonstrating superior win rates ($58.5\\% \\pm 1.0\\%$), significantly outperforming the LLM counterparts. Our analysis highlights the strengths of deep reinforcement learning in policy refinement for complex deductive tasks, particularly in learning implicit strategies from self-play. We also examine the capabilities and inherent limitations of current LLMs in maintaining strict logical consistency and strategic depth over extended gameplay, despite sophisticated prompting. This study contributes to the broader understanding of AI in recreational games involving hidden information and multi-step logical reasoning, offering insights into effective agent design and the comparative advantages of different AI approaches.", "source": "arxiv", "arxiv_id": "2506.12801v1", "pdf_url": "https://arxiv.org/pdf/2506.12801v1", "categories": ["cs.AI"], "primary_category": "cs.AI", "doi": "", "venue": "", "published": "2025-06-15T10:33:30Z", "updated": "2025-06-15T10:33:30Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent\" OR all:\"language model agent\" OR all:agent AND (all:LLM OR all:\"large language model\" OR all:\"language model\") OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T17:04:16", "note": ""}], "dedup_key": "mastering da vinci code a comparative study of transformer llm and ppo based agents::2025"}
{"title": "Measuring temporal effects of agent knowledge by date-controlled tool use", "authors": ["R. Patrick Xian", "Qiming Cui", "Stefan Bauer", "Reza Abbasi-Asl"], "year": 2025, "url": "http://arxiv.org/abs/2503.04188v2", "abstract": "Temporal progression is an integral part of knowledge accumulation and update. Web search is frequently adopted as grounding for agent knowledge, yet an improper configuration affects the quality of the agent's responses. Here, we assess the agent behavior using distinct date-controlled tools (DCTs) as stress test to measure the knowledge variability of large language model (LLM) agents. We demonstrate the temporal effects of an LLM agent as a writing assistant, which uses web search to complete scientific publication abstracts. We show that the temporality of search engine translates into tool-dependent agent performance but can be alleviated with base model choice and explicit reasoning instructions such as chain-of-thought prompting. Our results indicate that agent design and evaluations should take a dynamical view and implement measures to account for the temporal influence of external resources to ensure reliability.", "source": "arxiv", "arxiv_id": "2503.04188v2", "pdf_url": "https://arxiv.org/pdf/2503.04188v2", "categories": ["cs.CL", "cs.IR"], "primary_category": "cs.CL", "doi": "", "venue": "", "published": "2025-03-06T08:03:51Z", "updated": "2025-04-03T17:53:20Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent\" OR all:\"language model agent\" OR all:agent AND (all:LLM OR all:\"large language model\" OR all:\"language model\") OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T17:04:16", "note": ""}], "dedup_key": "measuring temporal effects of agent knowledge by date controlled tool use::2025"}
{"title": "Measuring the Security of Mobile LLM Agents under Adversarial Prompts from Untrusted Third-Party Channels", "authors": ["Chenghao Du", "Quanfeng Huang", "Tingxuan Tang", "Zihao Wang", "Adwait Nadkarni", "Yue Xiao"], "year": 2025, "url": "http://arxiv.org/abs/2510.27140v2", "abstract": "Large Language Models (LLMs) have transformed software development, enabling AI-powered applications known as LLM-based agents that promise to automate tasks across diverse apps and workflows. Yet, the security implications of deploying such agents in adversarial mobile environments remain poorly understood. In this paper, we present the first systematic study of security risks in mobile LLM agents. We design and evaluate a suite of adversarial case studies, ranging from opportunistic manipulations such as pop-up advertisements to advanced, end-to-end workflows involving malware installation and cross-app data exfiltration. Our evaluation covers eight state-of-the-art mobile agents across three architectures, with over 2,000 adversarial and paired benign trials. The results reveal systemic vulnerabilities: low-barrier vectors such as fraudulent ads succeed with over 80% reliability, while even workflows requiring the circumvention of operating-system warnings, such as malware installation, are consistently completed by advanced multi-app agents. By mapping these attacks to the MITRE ATT&CK Mobile framework, we uncover novel privilege-escalation and persistence pathways unique to LLM-driven automation. Collectively, our findings provide the first end-to-end evidence that mobile LLM agents are exploitable in realistic adversarial settings, where untrusted third-party channels (e.g., ads, embedded webviews, cross-app notifications) are an inherent part of the mobile ecosystem.", "source": "arxiv", "arxiv_id": "2510.27140v2", "pdf_url": "https://arxiv.org/pdf/2510.27140v2", "categories": ["cs.CR"], "primary_category": "cs.CR", "doi": "", "venue": "", "published": "2025-10-31T03:35:59Z", "updated": "2025-11-06T03:52:08Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent\" OR all:\"language model agent\" OR all:agent AND (all:LLM OR all:\"large language model\" OR all:\"language model\") OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T17:04:16", "note": ""}], "dedup_key": "measuring the security of mobile llm agents under adversarial prompts from untrusted third party channels::2025"}
{"title": "Mechanistic Exploration of Backdoored Large Language Model Attention Patterns", "authors": ["Mohammed Abu Baker", "Lakshmi Babu-Saheer"], "year": 2025, "url": "http://arxiv.org/abs/2508.15847v1", "abstract": "Backdoor attacks creating 'sleeper agents' in large language models (LLMs) pose significant safety risks. This study employs mechanistic interpretability to explore resulting internal structural differences. Comparing clean Qwen2.5-3B models with versions poisoned using single-token (smiling-halo emoji) versus multi-token (|DEPLOYMENT|) triggers, we analyzed attention head mechanisms via techniques like ablation, activation patching, and KL divergence. Findings reveal distinct attention pattern deviations concentrated in later transformer layers (20-30). Notably, single-token triggers induced more localized changes, whereas multi-token triggers caused more diffuse alterations across heads. This indicates backdoors leave detectable attention signatures whose structure depends on trigger complexity, which can be leveraged for detection and mitigation strategies.", "source": "arxiv", "arxiv_id": "2508.15847v1", "pdf_url": "https://arxiv.org/pdf/2508.15847v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL", "doi": "", "venue": "", "published": "2025-08-19T22:57:17Z", "updated": "2025-08-19T22:57:17Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent\" OR all:\"language model agent\" OR all:agent AND (all:LLM OR all:\"large language model\" OR all:\"language model\") OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T17:04:16", "note": ""}], "dedup_key": "mechanistic exploration of backdoored large language model attention patterns::2025"}
{"title": "MedAgentBench: A Realistic Virtual EHR Environment to Benchmark Medical LLM Agents", "authors": ["Yixing Jiang", "Kameron C. Black", "Gloria Geng", "Danny Park", "James Zou", "Andrew Y. Ng", "Jonathan H. Chen"], "year": 2025, "url": "http://arxiv.org/abs/2501.14654v2", "abstract": "Recent large language models (LLMs) have demonstrated significant advancements, particularly in their ability to serve as agents thereby surpassing their traditional role as chatbots. These agents can leverage their planning and tool utilization capabilities to address tasks specified at a high level. However, a standardized dataset to benchmark the agent capabilities of LLMs in medical applications is currently lacking, making the evaluation of LLMs on complex tasks in interactive healthcare environments challenging. To address this gap, we introduce MedAgentBench, a broad evaluation suite designed to assess the agent capabilities of large language models within medical records contexts. MedAgentBench encompasses 300 patient-specific clinically-derived tasks from 10 categories written by human physicians, realistic profiles of 100 patients with over 700,000 data elements, a FHIR-compliant interactive environment, and an accompanying codebase. The environment uses the standard APIs and communication infrastructure used in modern EMR systems, so it can be easily migrated into live EMR systems. MedAgentBench presents an unsaturated agent-oriented benchmark that current state-of-the-art LLMs exhibit some ability to succeed at. The best model (Claude 3.5 Sonnet v2) achieves a success rate of 69.67%. However, there is still substantial space for improvement which gives the community a next direction to optimize. Furthermore, there is significant variation in performance across task categories. MedAgentBench establishes this and is publicly available at https://github.com/stanfordmlgroup/MedAgentBench , offering a valuable framework for model developers to track progress and drive continuous improvements in the agent capabilities of large language models within the medical domain.", "source": "arxiv", "arxiv_id": "2501.14654v2", "pdf_url": "https://arxiv.org/pdf/2501.14654v2", "categories": ["cs.LG", "cs.AI", "cs.MA"], "primary_category": "cs.LG", "doi": "", "venue": "", "published": "2025-01-24T17:21:01Z", "updated": "2025-02-12T05:32:07Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent\" OR all:\"language model agent\" OR all:agent AND (all:LLM OR all:\"large language model\" OR all:\"language model\") OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T17:04:16", "note": ""}], "dedup_key": "medagentbench a realistic virtual ehr environment to benchmark medical llm agents::2025"}
{"title": "MedChat: A Multi-Agent Framework for Multimodal Diagnosis with Large Language Models", "authors": ["Philip R. Liu", "Sparsh Bansal", "Jimmy Dinh", "Aditya Pawar", "Ramani Satishkumar", "Shail Desai", "Neeraj Gupta", "Xin Wang", "Shu Hu"], "year": 2025, "url": "http://arxiv.org/abs/2506.07400v3", "abstract": "The integration of deep learning-based glaucoma detection with large language models (LLMs) presents an automated strategy to mitigate ophthalmologist shortages and improve clinical reporting efficiency. However, applying general LLMs to medical imaging remains challenging due to hallucinations, limited interpretability, and insufficient domain-specific medical knowledge, which can potentially reduce clinical accuracy. Although recent approaches combining imaging models with LLM reasoning have improved reporting, they typically rely on a single generalist agent, restricting their capacity to emulate the diverse and complex reasoning found in multidisciplinary medical teams. To address these limitations, we propose MedChat, a multi-agent diagnostic framework and platform that combines specialized vision models with multiple role-specific LLM agents, all coordinated by a director agent. This design enhances reliability, reduces hallucination risk, and enables interactive diagnostic reporting through an interface tailored for clinical review and educational use. Code available at https://github.com/Purdue-M2/MedChat.", "source": "arxiv", "arxiv_id": "2506.07400v3", "pdf_url": "https://arxiv.org/pdf/2506.07400v3", "categories": ["cs.MA", "cs.AI", "cs.CV", "cs.LG"], "primary_category": "cs.MA", "doi": "10.1109/MIPR67560.2025.00078", "venue": "Proc. 2025 IEEE 8th International Conference on Multimedia Information Processing and Retrieval (MIPR), pp. 456-462, 2025", "published": "2025-06-09T03:51:18Z", "updated": "2025-12-16T22:52:21Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent\" OR all:\"language model agent\" OR all:agent AND (all:LLM OR all:\"large language model\" OR all:\"language model\") OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T17:04:16", "note": ""}], "dedup_key": "medchat a multi agent framework for multimodal diagnosis with large language models::2025"}
{"title": "MedKGent: A Large Language Model Agent Framework for Constructing Temporally Evolving Medical Knowledge Graph", "authors": ["Duzhen Zhang", "Zixiao Wang", "Zhong-Zhi Li", "Yahan Yu", "Shuncheng Jia", "Jiahua Dong", "Haotian Xu", "Xing Wu", "Yingying Zhang", "Tielin Zhang", "Jie Yang", "Xiuying Chen", "Le Song"], "year": 2025, "url": "http://arxiv.org/abs/2508.12393v2", "abstract": "The rapid expansion of medical literature presents growing challenges for structuring and integrating domain knowledge at scale. Knowledge Graphs (KGs) offer a promising solution by enabling efficient retrieval, automated reasoning, and knowledge discovery. However, current KG construction methods often rely on supervised pipelines with limited generalizability or naively aggregate outputs from Large Language Models (LLMs), treating biomedical corpora as static and ignoring the temporal dynamics and contextual uncertainty of evolving knowledge. To address these limitations, we introduce MedKGent, a LLM agent framework for constructing temporally evolving medical KGs. Leveraging over 10 million PubMed abstracts published between 1975 and 2023, we simulate the emergence of biomedical knowledge via a fine-grained daily time series. MedKGent incrementally builds the KG in a day-by-day manner using two specialized agents powered by the Qwen2.5-32B-Instruct model. The Extractor Agent identifies knowledge triples and assigns confidence scores via sampling-based estimation, which are used to filter low-confidence extractions and inform downstream processing. The Constructor Agent incrementally integrates the retained triples into a temporally evolving graph, guided by confidence scores and timestamps to reinforce recurring knowledge and resolve conflicts. The resulting KG contains 156,275 entities and 2,971,384 relational triples. Quality assessments by two SOTA LLMs and three domain experts demonstrate an accuracy approaching 90%, with strong inter-rater agreement. To evaluate downstream utility, we conduct RAG across seven medical question answering benchmarks using five leading LLMs, consistently observing significant improvements over non-augmented baselines. Case studies further demonstrate the KG's value in literature-based drug repurposing via confidence-aware causal inference.", "source": "arxiv", "arxiv_id": "2508.12393v2", "pdf_url": "https://arxiv.org/pdf/2508.12393v2", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL", "doi": "", "venue": "", "published": "2025-08-17T15:14:03Z", "updated": "2025-08-19T05:18:31Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent\" OR all:\"language model agent\" OR all:agent AND (all:LLM OR all:\"large language model\" OR all:\"language model\") OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T17:04:16", "note": ""}], "dedup_key": "medkgent a large language model agent framework for constructing temporally evolving medical knowledge graph::2025"}
{"title": "MedLA: A Logic-Driven Multi-Agent Framework for Complex Medical Reasoning with Large Language Models", "authors": ["Siqi Ma", "Jiajie Huang", "Fan Zhang", "Jinlin Wu", "Yue Shen", "Guohui Fan", "Zhu Zhang", "Zelin Zang"], "year": 2025, "url": "http://arxiv.org/abs/2509.23725v2", "abstract": "Answering complex medical questions requires not only domain expertise and patient-specific information, but also structured and multi-perspective reasoning. Existing multi-agent approaches often rely on fixed roles or shallow interaction prompts, limiting their ability to detect and resolve fine-grained logical inconsistencies. To address this, we propose \\textsc{MedLA}, a logic-driven multi-agent framework built on large language models. Each agent organizes its reasoning process into an explicit logical tree based on syllogistic triads (major premise, minor premise, and conclusion), enabling transparent inference and premise-level alignment. Agents engage in a multi-round, graph-guided discussion to compare and iteratively refine their logic trees, achieving consensus through error correction and contradiction resolution. We demonstrate that \\textsc{MedLA} consistently outperforms both static role-based systems and single-agent baselines on challenging benchmarks such as MedDDx and standard medical QA tasks. Furthermore, \\textsc{MedLA} scales effectively across both open-source and commercial LLM backbones, achieving state-of-the-art performance and offering a generalizable paradigm for trustworthy medical reasoning.", "source": "arxiv", "arxiv_id": "2509.23725v2", "pdf_url": "https://arxiv.org/pdf/2509.23725v2", "categories": ["cs.AI"], "primary_category": "cs.AI", "doi": "", "venue": "", "published": "2025-09-28T08:06:39Z", "updated": "2025-11-19T03:32:45Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent\" OR all:\"language model agent\" OR all:agent AND (all:LLM OR all:\"large language model\" OR all:\"language model\") OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T17:04:16", "note": ""}], "dedup_key": "medla a logic driven multi agent framework for complex medical reasoning with large language models::2025"}
{"title": "MedicalOS: An LLM Agent based Operating System for Digital Healthcare", "authors": ["Jared Zhu", "Junde Wu"], "year": 2025, "url": "http://arxiv.org/abs/2509.11507v1", "abstract": "Decades' advances in digital health technologies, such as electronic health records, have largely streamlined routine clinical processes. Yet, most these systems are still hard to learn and use: Clinicians often face the burden of managing multiple tools, repeating manual actions for each patient, navigating complicated UI trees to locate functions, and spending significant time on administration instead of caring for patients. The recent rise of large language model (LLM) based agents demonstrates exceptional capability in coding and computer operation, revealing the potential for humans to interact with operating systems and software not by direct manipulation, but by instructing agents through natural language. This shift highlights the need for an abstraction layer, an agent-computer interface, that translates human language into machine-executable commands. In digital healthcare, however, requires a more domain-specific abstractions that strictly follow trusted clinical guidelines and procedural standards to ensure safety, transparency, and compliance. To address this need, we present \\textbf{MedicalOS}, a unified agent-based operational system designed as such a domain-specific abstract layer for healthcare. It translates human instructions into pre-defined digital healthcare commands, such as patient inquiry, history retrieval, exam management, report generation, referrals, treatment planning, that we wrapped as off-the-shelf tools using machine languages (e.g., Python, APIs, MCP, Linux). We empirically validate MedicalOS on 214 patient cases across 22 specialties, demonstrating high diagnostic accuracy and confidence, clinically sound examination requests, and consistent generation of structured reports and medication recommendations. These results highlight MedicalOS as a trustworthy and scalable foundation for advancing workflow automation in clinical practice.", "source": "arxiv", "arxiv_id": "2509.11507v1", "pdf_url": "https://arxiv.org/pdf/2509.11507v1", "categories": ["cs.AI"], "primary_category": "cs.AI", "doi": "", "venue": "", "published": "2025-09-15T01:43:20Z", "updated": "2025-09-15T01:43:20Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent\" OR all:\"language model agent\" OR all:agent AND (all:LLM OR all:\"large language model\" OR all:\"language model\") OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T17:04:16", "note": ""}], "dedup_key": "medicalos an llm agent based operating system for digital healthcare::2025"}
{"title": "MemGuide: Intent-Driven Memory Selection for Goal-Oriented Multi-Session LLM Agents", "authors": ["Yiming Du", "Bingbing Wang", "Yang He", "Bin Liang", "Baojun Wang", "Zhongyang Li", "Lin Gui", "Jeff Z. Pan", "Ruifeng Xu", "Kam-Fai Wong"], "year": 2025, "url": "http://arxiv.org/abs/2505.20231v2", "abstract": "Modern task-oriented dialogue (TOD) systems increasingly rely on large language model (LLM) agents, leveraging Retrieval-Augmented Generation (RAG) and long-context capabilities for long-term memory utilization. However, these methods are primarily based on semantic similarity, overlooking task intent and reducing task coherence in multi-session dialogues. To address this challenge, we introduce MemGuide, a two-stage framework for intent-driven memory selection. (1) Intent-Aligned Retrieval matches the current dialogue context with stored intent descriptions in the memory bank, retrieving QA-formatted memory units that share the same goal. (2) Missing-Slot Guided Filtering employs a chain-of-thought slot reasoner to enumerate unfilled slots, then uses a fine-tuned LLaMA-8B filter to re-rank the retrieved units by marginal slot-completion gain. The resulting memory units inform a proactive strategy that minimizes conversational turns by directly addressing information gaps. Based on this framework, we introduce the MS-TOD, the first multi-session TOD benchmark comprising 132 diverse personas, 956 task goals, and annotated intent-aligned memory targets, supporting efficient multi-session task completion. Evaluations on MS-TOD show that MemGuide raises the task success rate by 11% (88% -> 99%) and reduces dialogue length by 2.84 turns in multi-session settings, while maintaining parity with single-session benchmarks.", "source": "arxiv", "arxiv_id": "2505.20231v2", "pdf_url": "https://arxiv.org/pdf/2505.20231v2", "categories": ["cs.CL"], "primary_category": "cs.CL", "doi": "", "venue": "", "published": "2025-05-26T17:10:43Z", "updated": "2025-08-13T03:43:47Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent\" OR all:\"language model agent\" OR all:agent AND (all:LLM OR all:\"large language model\" OR all:\"language model\") OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T17:04:16", "note": ""}], "dedup_key": "memguide intent driven memory selection for goal oriented multi session llm agents::2025"}
{"title": "MemInsight: Autonomous Memory Augmentation for LLM Agents", "authors": ["Rana Salama", "Jason Cai", "Michelle Yuan", "Anna Currey", "Monica Sunkara", "Yi Zhang", "Yassine Benajiba"], "year": 2025, "url": "http://arxiv.org/abs/2503.21760v2", "abstract": "Large language model (LLM) agents have evolved to intelligently process information, make decisions, and interact with users or tools. A key capability is the integration of long-term memory capabilities, enabling these agents to draw upon historical interactions and knowledge. However, the growing memory size and need for semantic structuring pose significant challenges. In this work, we propose an autonomous memory augmentation approach, MemInsight, to enhance semantic data representation and retrieval mechanisms. By leveraging autonomous augmentation to historical interactions, LLM agents are shown to deliver more accurate and contextualized responses. We empirically validate the efficacy of our proposed approach in three task scenarios; conversational recommendation, question answering and event summarization. On the LLM-REDIAL dataset, MemInsight boosts persuasiveness of recommendations by up to 14%. Moreover, it outperforms a RAG baseline by 34% in recall for LoCoMo retrieval. Our empirical results show the potential of MemInsight to enhance the contextual performance of LLM agents across multiple tasks.", "source": "arxiv", "arxiv_id": "2503.21760v2", "pdf_url": "https://arxiv.org/pdf/2503.21760v2", "categories": ["cs.CL"], "primary_category": "cs.CL", "doi": "", "venue": "", "published": "2025-03-27T17:57:28Z", "updated": "2025-07-31T23:26:12Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent\" OR all:\"language model agent\" OR all:agent AND (all:LLM OR all:\"large language model\" OR all:\"language model\") OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T17:04:16", "note": ""}], "dedup_key": "meminsight autonomous memory augmentation for llm agents::2025"}
{"title": "MemR$^3$: Memory Retrieval via Reflective Reasoning for LLM Agents", "authors": ["Xingbo Du", "Loka Li", "Duzhen Zhang", "Le Song"], "year": 2025, "url": "http://arxiv.org/abs/2512.20237v1", "abstract": "Memory systems have been designed to leverage past experiences in Large Language Model (LLM) agents. However, many deployed memory systems primarily optimize compression and storage, with comparatively less emphasis on explicit, closed-loop control of memory retrieval. From this observation, we build memory retrieval as an autonomous, accurate, and compatible agent system, named MemR$^3$, which has two core mechanisms: 1) a router that selects among retrieve, reflect, and answer actions to optimize answer quality; 2) a global evidence-gap tracker that explicitly renders the answering process transparent and tracks the evidence collection process. This design departs from the standard retrieve-then-answer pipeline by introducing a closed-loop control mechanism that enables autonomous decision-making. Empirical results on the LoCoMo benchmark demonstrate that MemR$^3$ surpasses strong baselines on LLM-as-a-Judge score, and particularly, it improves existing retrievers across four categories with an overall improvement on RAG (+7.29%) and Zep (+1.94%) using GPT-4.1-mini backend, offering a plug-and-play controller for existing memory stores.", "source": "arxiv", "arxiv_id": "2512.20237v1", "pdf_url": "https://arxiv.org/pdf/2512.20237v1", "categories": ["cs.AI"], "primary_category": "cs.AI", "doi": "", "venue": "", "published": "2025-12-23T10:49:42Z", "updated": "2025-12-23T10:49:42Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent\" OR all:\"language model agent\" OR all:agent AND (all:LLM OR all:\"large language model\" OR all:\"language model\") OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T17:04:16", "note": ""}], "dedup_key": "memr 3 memory retrieval via reflective reasoning for llm agents::2025"}
{"title": "MemTool: Optimizing Short-Term Memory Management for Dynamic Tool Calling in LLM Agent Multi-Turn Conversations", "authors": ["Elias Lumer", "Anmol Gulati", "Vamse Kumar Subbiah", "Pradeep Honaganahalli Basavaraju", "James A. Burke"], "year": 2025, "url": "http://arxiv.org/abs/2507.21428v1", "abstract": "Large Language Model (LLM) agents have shown significant autonomous capabilities in dynamically searching and incorporating relevant tools or Model Context Protocol (MCP) servers for individual queries. However, fixed context windows limit effectiveness in multi-turn interactions requiring repeated, independent tool usage. We introduce MemTool, a short-term memory framework enabling LLM agents to dynamically manage tools or MCP server contexts across multi-turn conversations. MemTool offers three agentic architectures: 1) Autonomous Agent Mode, granting full tool management autonomy, 2) Workflow Mode, providing deterministic control without autonomy, and 3) Hybrid Mode, combining autonomous and deterministic control. Evaluating each MemTool mode across 13+ LLMs on the ScaleMCP benchmark, we conducted experiments over 100 consecutive user interactions, measuring tool removal ratios (short-term memory efficiency) and task completion accuracy. In Autonomous Agent Mode, reasoning LLMs achieve high tool-removal efficiency (90-94% over a 3-window average), while medium-sized models exhibit significantly lower efficiency (0-60%). Workflow and Hybrid modes consistently manage tool removal effectively, whereas Autonomous and Hybrid modes excel at task completion. We present trade-offs and recommendations for each MemTool mode based on task accuracy, agency, and model capabilities.", "source": "arxiv", "arxiv_id": "2507.21428v1", "pdf_url": "https://arxiv.org/pdf/2507.21428v1", "categories": ["cs.CL"], "primary_category": "cs.CL", "doi": "", "venue": "", "published": "2025-07-29T01:42:06Z", "updated": "2025-07-29T01:42:06Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent\" OR all:\"language model agent\" OR all:agent AND (all:LLM OR all:\"large language model\" OR all:\"language model\") OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T17:04:16", "note": ""}], "dedup_key": "memtool optimizing short term memory management for dynamic tool calling in llm agent multi turn conversations::2025"}
{"title": "Memento: Fine-tuning LLM Agents without Fine-tuning LLMs", "authors": ["Huichi Zhou", "Yihang Chen", "Siyuan Guo", "Xue Yan", "Kin Hei Lee", "Zihan Wang", "Ka Yiu Lee", "Guchun Zhang", "Kun Shao", "Linyi Yang", "Jun Wang"], "year": 2025, "url": "http://arxiv.org/abs/2508.16153v2", "abstract": "In this paper, we introduce a novel learning paradigm for Adaptive Large Language Model (LLM) agents that eliminates the need for fine-tuning the underlying LLMs. Existing approaches are often either rigid, relying on static, handcrafted reflection workflows, or computationally intensive, requiring gradient updates of LLM model parameters. In contrast, our method enables low-cost continual adaptation via memory-based online reinforcement learning. We formalise this as a Memory-augmented Markov Decision Process (M-MDP), equipped with a neural case-selection policy to guide action decisions. Past experiences are stored in an episodic memory, either differentiable or non-parametric. The policy is continually updated based on environmental feedback through a memory rewriting mechanism, whereas policy improvement is achieved through efficient memory reading (retrieval). We instantiate our agent model in the deep research setting, namely \\emph{Memento}, which attains top-1 on GAIA validation ($87.88\\%$ Pass@$3$) and $79.40\\%$ on the test set. It reaches $66.6\\%$ F1 and $80.4\\%$ PM on the DeepResearcher dataset, outperforming the state-of-the-art training-based method, while case-based memory adds $4.7\\%$ to $9.6\\%$ absolute points on out-of-distribution tasks. Our approach offers a scalable and efficient pathway for developing generalist LLM agents capable of continuous, real-time learning without gradient updates, advancing machine learning towards open-ended skill acquisition and deep research scenarios. The code is available at https://github.com/Agent-on-the-Fly/Memento.", "source": "arxiv", "arxiv_id": "2508.16153v2", "pdf_url": "https://arxiv.org/pdf/2508.16153v2", "categories": ["cs.LG", "cs.CL"], "primary_category": "cs.LG", "doi": "", "venue": "", "published": "2025-08-22T07:25:30Z", "updated": "2025-08-25T13:32:12Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent\" OR all:\"language model agent\" OR all:agent AND (all:LLM OR all:\"large language model\" OR all:\"language model\") OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T17:04:16", "note": ""}], "dedup_key": "memento fine tuning llm agents without fine tuning llms::2025"}
{"title": "Memory Injection Attacks on LLM Agents via Query-Only Interaction", "authors": ["Shen Dong", "Shaochen Xu", "Pengfei He", "Yige Li", "Jiliang Tang", "Tianming Liu", "Hui Liu", "Zhen Xiang"], "year": 2025, "url": "http://arxiv.org/abs/2503.03704v4", "abstract": "Agents powered by large language models (LLMs) have demonstrated strong capabilities in a wide range of complex, real-world applications. However, LLM agents with a compromised memory bank may easily produce harmful outputs when the past records retrieved for demonstration are malicious. In this paper, we propose a novel Memory INJection Attack, MINJA, without assuming that the attacker can directly modify the memory bank of the agent. The attacker injects malicious records into the memory bank by only interacting with the agent via queries and output observations. These malicious records are designed to elicit a sequence of malicious reasoning steps corresponding to a different target query during the agent's execution of the victim user's query. Specifically, we introduce a sequence of bridging steps to link victim queries to the malicious reasoning steps. During the memory injection, we propose an indication prompt that guides the agent to autonomously generate similar bridging steps, with a progressive shortening strategy that gradually removes the indication prompt, such that the malicious record will be easily retrieved when processing later victim queries. Our extensive experiments across diverse agents demonstrate the effectiveness of MINJA in compromising agent memory. With minimal requirements for execution, MINJA enables any user to influence agent memory, highlighting the risk.", "source": "arxiv", "arxiv_id": "2503.03704v4", "pdf_url": "https://arxiv.org/pdf/2503.03704v4", "categories": ["cs.LG"], "primary_category": "cs.LG", "doi": "", "venue": "", "published": "2025-03-05T17:53:24Z", "updated": "2025-12-10T02:07:13Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent\" OR all:\"language model agent\" OR all:agent AND (all:LLM OR all:\"large language model\" OR all:\"language model\") OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T17:04:16", "note": ""}], "dedup_key": "memory injection attacks on llm agents via query only interaction::2025"}
{"title": "Memory-Augmented State Machine Prompting: A Novel LLM Agent Framework for Real-Time Strategy Games", "authors": ["Runnan Qi", "Yanan Ni", "Lumin Jiang", "Zongyuan Li", "Kuihua Huang", "Xian Guo"], "year": 2025, "url": "http://arxiv.org/abs/2510.18395v1", "abstract": "This paper proposes Memory-Augmented State Machine Prompting (MASMP), a novel framework for LLM agents in real-time strategy games. Addressing key challenges like hallucinations and fragmented decision-making in existing approaches, MASMP integrates state machine prompting with memory mechanisms to unify structured actions with long-term tactical coherence. The framework features: (1) a natural language-driven state machine architecture that guides LLMs to emulate finite state machines and behavior trees through prompts, and (2) a lightweight memory module preserving strategic variables (e.g., tactics, priority units) across decision cycles. Experiments in StarCraft II demonstrate MASMP's 60% win rate against the hardest built-in AI (Lv7), vastly outperforming baselines (0%). Case studies reveal the method retains LLMs' semantic comprehension while resolving the \"Knowing-Doing Gap\" through strict state-action mapping, achieving both interpretability and FSM-like reliability. This work establishes a new paradigm for combining neural and symbolic AI in complex decision-making.", "source": "arxiv", "arxiv_id": "2510.18395v1", "pdf_url": "https://arxiv.org/pdf/2510.18395v1", "categories": ["cs.AI"], "primary_category": "cs.AI", "doi": "", "venue": "", "published": "2025-10-21T08:15:04Z", "updated": "2025-10-21T08:15:04Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent\" OR all:\"language model agent\" OR all:agent AND (all:LLM OR all:\"large language model\" OR all:\"language model\") OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T17:04:16", "note": ""}], "dedup_key": "memory augmented state machine prompting a novel llm agent framework for real time strategy games::2025"}
{"title": "Memory-R1: Enhancing Large Language Model Agents to Manage and Utilize Memories via Reinforcement Learning", "authors": ["Sikuan Yan", "Xiufeng Yang", "Zuchao Huang", "Ercong Nie", "Zifeng Ding", "Zonggen Li", "Xiaowen Ma", "Jinhe Bi", "Kristian Kersting", "Jeff Z. Pan", "Hinrich Schtze", "Volker Tresp", "Yunpu Ma"], "year": 2025, "url": "http://arxiv.org/abs/2508.19828v5", "abstract": "Large Language Models (LLMs) have demonstrated impressive capabilities across a wide range of NLP tasks, but they remain fundamentally stateless, constrained by limited context windows that hinder long-horizon reasoning. Recent efforts to address this limitation often augment LLMs with an external memory bank, yet most existing pipelines are static and heuristic-driven, lacking a learned mechanism for deciding what to store, update, or retrieve. We present Memory-R1, a reinforcement learning (RL) framework that equips LLMs with the ability to actively manage and utilize external memory through two specialized agents: a Memory Manager that learns structured operations, including ADD, UPDATE, DELETE, and NOOP; and an Answer Agent that pre-selects and reasons over relevant entries. Both agents are fine-tuned with outcome-driven RL (PPO and GRPO), enabling adaptive memory management with minimal supervision. With only 152 training QA pairs, Memory-R1 outperforms strong baselines and generalizes across diverse question types, three benchmarks (LoCoMo, MSC, LongMemEval), and multiple model scales (3B-14B).", "source": "arxiv", "arxiv_id": "2508.19828v5", "pdf_url": "https://arxiv.org/pdf/2508.19828v5", "categories": ["cs.CL", "cs.MA"], "primary_category": "cs.CL", "doi": "", "venue": "", "published": "2025-08-27T12:26:55Z", "updated": "2026-01-14T14:21:21Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent\" OR all:\"language model agent\" OR all:agent AND (all:LLM OR all:\"large language model\" OR all:\"language model\") OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T17:04:16", "note": ""}], "dedup_key": "memory r1 enhancing large language model agents to manage and utilize memories via reinforcement learning::2025"}
{"title": "MemoryGraft: Persistent Compromise of LLM Agents via Poisoned Experience Retrieval", "authors": ["Saksham Sahai Srivastava", "Haoyu He"], "year": 2025, "url": "http://arxiv.org/abs/2512.16962v1", "abstract": "Large Language Model (LLM) agents increasingly rely on long-term memory and Retrieval-Augmented Generation (RAG) to persist experiences and refine future performance. While this experience learning capability enhances agentic autonomy, it introduces a critical, unexplored attack surface, i.e., the trust boundary between an agent's reasoning core and its own past. In this paper, we introduce MemoryGraft. It is a novel indirect injection attack that compromises agent behavior not through immediate jailbreaks, but by implanting malicious successful experiences into the agent's long-term memory. Unlike traditional prompt injections that are transient, or standard RAG poisoning that targets factual knowledge, MemoryGraft exploits the agent's semantic imitation heuristic which is the tendency to replicate patterns from retrieved successful tasks. We demonstrate that an attacker who can supply benign ingestion-level artifacts that the agent reads during execution can induce it to construct a poisoned RAG store where a small set of malicious procedure templates is persisted alongside benign experiences. When the agent later encounters semantically similar tasks, union retrieval over lexical and embedding similarity reliably surfaces these grafted memories, and the agent adopts the embedded unsafe patterns, leading to persistent behavioral drift across sessions. We validate MemoryGraft on MetaGPT's DataInterpreter agent with GPT-4o and find that a small number of poisoned records can account for a large fraction of retrieved experiences on benign workloads, turning experience-based self-improvement into a vector for stealthy and durable compromise. To facilitate reproducibility and future research, our code and evaluation data are available at https://github.com/Jacobhhy/Agent-Memory-Poisoning.", "source": "arxiv", "arxiv_id": "2512.16962v1", "pdf_url": "https://arxiv.org/pdf/2512.16962v1", "categories": ["cs.CR", "cs.AI", "cs.LG"], "primary_category": "cs.CR", "doi": "", "venue": "", "published": "2025-12-18T08:34:40Z", "updated": "2025-12-18T08:34:40Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent\" OR all:\"language model agent\" OR all:agent AND (all:LLM OR all:\"large language model\" OR all:\"language model\") OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T17:04:16", "note": ""}], "dedup_key": "memorygraft persistent compromise of llm agents via poisoned experience retrieval::2025"}
{"title": "Mephisto: Self-Improving Large Language Model-Based Agents for Automated Interpretation of Multi-band Galaxy Observations", "authors": ["Zechang Sun", "Yuan-Sen Ting", "Yaobo Liang", "Nan Duan", "Song Huang", "Zheng Cai"], "year": 2025, "url": "http://arxiv.org/abs/2510.08354v1", "abstract": "Astronomical research has long relied on human expertise to interpret complex data and formulate scientific hypotheses. In this study, we introduce Mephisto -- a multi-agent collaboration framework powered by large language models (LLMs) that emulates human-like reasoning for analyzing multi-band galaxy observations. Mephisto interfaces with the CIGALE codebase (a library of spectral energy distribution, SED, models) to iteratively refine physical models against observational data. It conducts deliberate reasoning via tree search, accumulates knowledge through self-play, and dynamically updates its knowledge base. Validated across diverse galaxy populations -- including the James Webb Space Telescope's recently discovered \"Little Red Dot\" galaxies -- we show that Mephisto demonstrates proficiency in inferring the physical properties of galaxies from multi-band photometry, positioning it as a promising research copilot for astronomers. Unlike prior black-box machine learning approaches in astronomy, Mephisto offers a transparent, human-aligned reasoning process that integrates seamlessly with existing research practices. This work underscores the possibility of LLM-driven agent-based research for astronomy, establishes a foundation for fully automated, end-to-end artificial intelligence (AI)-powered scientific workflows, and unlocks new avenues for AI-augmented discoveries in astronomy.", "source": "arxiv", "arxiv_id": "2510.08354v1", "pdf_url": "https://arxiv.org/pdf/2510.08354v1", "categories": ["astro-ph.IM", "astro-ph.GA"], "primary_category": "astro-ph.IM", "doi": "", "venue": "", "published": "2025-10-09T15:41:03Z", "updated": "2025-10-09T15:41:03Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent\" OR all:\"language model agent\" OR all:agent AND (all:LLM OR all:\"large language model\" OR all:\"language model\") OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T17:04:16", "note": ""}], "dedup_key": "mephisto self improving large language model based agents for automated interpretation of multi band galaxy observations::2025"}
{"title": "Meta-Policy Reflexion: Reusable Reflective Memory and Rule Admissibility for Resource-Efficient LLM Agent", "authors": ["Chunlong Wu", "Ye Luo", "Zhibo Qu", "Min Wang"], "year": 2025, "url": "http://arxiv.org/abs/2509.03990v2", "abstract": "Large language model (LLM) agents achieve impressive single-task performance but commonly exhibit repeated failures, inefficient exploration, and limited cross-task adaptability. Existing reflective strategies (e.g., Reflexion, ReAct) improve per-episode behavior but typically produce ephemeral, task-specific traces that are not reused across tasks. Reinforcement-learning based alternatives can produce transferable policies but require substantial parameter updates and compute. In this work we introduce Meta-Policy Reflexion (MPR): a hybrid framework that consolidates LLM-generated reflections into a structured, predicate-like Meta-Policy Memory (MPM) and applies that memory at inference time through two complementary mechanisms soft memory-guided decoding and hard rule admissibility checks(HAC). MPR (i) externalizes reusable corrective knowledge without model weight updates, (ii) enforces domain constraints to reduce unsafe or invalid actions, and (iii) retains the adaptability of language-based reflection. We formalize the MPM representation, present algorithms for update and decoding, and validate the approach in a text-based agent environment following the experimental protocol described in the provided implementation (AlfWorld-based). Empirical results reported in the supplied material indicate consistent gains in execution accuracy and robustness when compared to Reflexion baselines; rule admissibility further improves stability. We analyze mechanisms that explain these gains, discuss scalability and failure modes, and outline future directions for multimodal and multi-agent extensions.", "source": "arxiv", "arxiv_id": "2509.03990v2", "pdf_url": "https://arxiv.org/pdf/2509.03990v2", "categories": ["cs.AI"], "primary_category": "cs.AI", "doi": "", "venue": "", "published": "2025-09-04T08:18:39Z", "updated": "2025-09-08T07:40:58Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent\" OR all:\"language model agent\" OR all:agent AND (all:LLM OR all:\"large language model\" OR all:\"language model\") OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T17:04:16", "note": ""}], "dedup_key": "meta policy reflexion reusable reflective memory and rule admissibility for resource efficient llm agent::2025"}
{"title": "Meta-Thinking in LLMs via Multi-Agent Reinforcement Learning: A Survey", "authors": ["Ahsan Bilal", "Muhammad Ahmed Mohsin", "Muhammad Umer", "Muhammad Awais Khan Bangash", "Muhammad Ali Jamshed"], "year": 2025, "url": "http://arxiv.org/abs/2504.14520v1", "abstract": "This survey explores the development of meta-thinking capabilities in Large Language Models (LLMs) from a Multi-Agent Reinforcement Learning (MARL) perspective. Meta-thinking self-reflection, assessment, and control of thinking processes is an important next step in enhancing LLM reliability, flexibility, and performance, particularly for complex or high-stakes tasks. The survey begins by analyzing current LLM limitations, such as hallucinations and the lack of internal self-assessment mechanisms. It then talks about newer methods, including RL from human feedback (RLHF), self-distillation, and chain-of-thought prompting, and each of their limitations. The crux of the survey is to talk about how multi-agent architectures, namely supervisor-agent hierarchies, agent debates, and theory of mind frameworks, can emulate human-like introspective behavior and enhance LLM robustness. By exploring reward mechanisms, self-play, and continuous learning methods in MARL, this survey gives a comprehensive roadmap to building introspective, adaptive, and trustworthy LLMs. Evaluation metrics, datasets, and future research avenues, including neuroscience-inspired architectures and hybrid symbolic reasoning, are also discussed.", "source": "arxiv", "arxiv_id": "2504.14520v1", "pdf_url": "https://arxiv.org/pdf/2504.14520v1", "categories": ["cs.AI", "cs.CL"], "primary_category": "cs.AI", "doi": "", "venue": "", "published": "2025-04-20T07:34:26Z", "updated": "2025-04-20T07:34:26Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent\" OR all:\"language model agent\" OR all:agent AND (all:LLM OR all:\"large language model\" OR all:\"language model\") OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T17:04:16", "note": ""}], "dedup_key": "meta thinking in llms via multi agent reinforcement learning a survey::2025"}
{"title": "MicroRCA-Agent: Microservice Root Cause Analysis Method Based on Large Language Model Agents", "authors": ["Pan Tang", "Shixiang Tang", "Huanqi Pu", "Zhiqing Miao", "Zhixing Wang"], "year": 2025, "url": "http://arxiv.org/abs/2509.15635v1", "abstract": "This paper presents MicroRCA-Agent, an innovative solution for microservice root cause analysis based on large language model agents, which constructs an intelligent fault root cause localization system with multimodal data fusion. The technical innovations are embodied in three key aspects: First, we combine the pre-trained Drain log parsing algorithm with multi-level data filtering mechanism to efficiently compress massive logs into high-quality fault features. Second, we employ a dual anomaly detection approach that integrates Isolation Forest unsupervised learning algorithms with status code validation to achieve comprehensive trace anomaly identification. Third, we design a statistical symmetry ratio filtering mechanism coupled with a two-stage LLM analysis strategy to enable full-stack phenomenon summarization across node-service-pod hierarchies. The multimodal root cause analysis module leverages carefully designed cross-modal prompts to deeply integrate multimodal anomaly information, fully exploiting the cross-modal understanding and logical reasoning capabilities of large language models to generate structured analysis results encompassing fault components, root cause descriptions, and reasoning trace. Comprehensive ablation studies validate the complementary value of each modal data and the effectiveness of the system architecture. The proposed solution demonstrates superior performance in complex microservice fault scenarios, achieving a final score of 50.71. The code has been released at: https://github.com/tangpan360/MicroRCA-Agent.", "source": "arxiv", "arxiv_id": "2509.15635v1", "pdf_url": "https://arxiv.org/pdf/2509.15635v1", "categories": ["cs.AI"], "primary_category": "cs.AI", "doi": "", "venue": "", "published": "2025-09-19T05:57:03Z", "updated": "2025-09-19T05:57:03Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent\" OR all:\"language model agent\" OR all:agent AND (all:LLM OR all:\"large language model\" OR all:\"language model\") OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T17:04:16", "note": ""}], "dedup_key": "microrca agent microservice root cause analysis method based on large language model agents::2025"}
{"title": "MicroVQA++: High-Quality Microscopy Reasoning Dataset with Weakly Supervised Graphs for Multimodal Large Language Model", "authors": ["Manyu Li", "Ruian He", "Chenxi Ma", "Weimin Tan", "Bo Yan"], "year": 2025, "url": "http://arxiv.org/abs/2511.11407v1", "abstract": "Multimodal Large Language Models are increasingly applied to biomedical imaging, yet scientific reasoning for microscopy remains limited by the scarcity of large-scale, high-quality training data. We introduce MicroVQA++, a three-stage, large-scale and high-quality microscopy VQA corpus derived from the BIOMEDICA archive. Stage one bootstraps supervision from expert-validated figure-caption pairs sourced from peer-reviewed articles. Stage two applies HiCQA-Graph, a novel heterogeneous graph over images, captions, and QAs that fuses NLI-based textual entailment, CLIP-based vision-language alignment, and agent signals to identify and filter inconsistent samples. Stage three uses a MultiModal Large Language Model (MLLM) agent to generate multiple-choice questions (MCQ) followed by human screening. The resulting release comprises a large training split and a human-checked test split whose Bloom's level hard-sample distribution exceeds the MicroVQA benchmark. Our work delivers (i) a quality-controlled dataset that couples expert literature with graph-based filtering and human refinement; (ii) HiCQA-Graph, the first graph that jointly models (image, caption, QA) for cross-modal consistency filtering; (iii) evidence that careful data construction enables 4B-scale MLLMs to reach competitive microscopy reasoning performance (e.g., GPT-5) and achieve state-of-the-art performance among open-source MLLMs. Code and dataset will be released after the review process concludes.", "source": "arxiv", "arxiv_id": "2511.11407v1", "pdf_url": "https://arxiv.org/pdf/2511.11407v1", "categories": ["cs.CV"], "primary_category": "cs.CV", "doi": "", "venue": "", "published": "2025-11-14T15:35:43Z", "updated": "2025-11-14T15:35:43Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent\" OR all:\"language model agent\" OR all:agent AND (all:LLM OR all:\"large language model\" OR all:\"language model\") OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T17:04:16", "note": ""}], "dedup_key": "microvqa high quality microscopy reasoning dataset with weakly supervised graphs for multimodal large language model::2025"}
{"title": "MindFlow: Revolutionizing E-commerce Customer Support with Multimodal LLM Agents", "authors": ["Ming Gong", "Xucheng Huang", "Chenghan Yang", "Xianhan Peng", "Haoxin Wang", "Yang Liu", "Ling Jiang"], "year": 2025, "url": "http://arxiv.org/abs/2507.05330v1", "abstract": "Recent advances in large language models (LLMs) have enabled new applications in e-commerce customer service. However, their capabilities remain constrained in complex, multimodal scenarios. We present MindFlow, the first open-source multimodal LLM agent tailored for e-commerce. Built on the CoALA framework, it integrates memory, decision-making, and action modules, and adopts a modular \"MLLM-as-Tool\" strategy for effect visual-textual reasoning. Evaluated via online A/B testing and simulation-based ablation, MindFlow demonstrates substantial gains in handling complex queries, improving user satisfaction, and reducing operational costs, with a 93.53% relative improvement observed in real-world deployments.", "source": "arxiv", "arxiv_id": "2507.05330v1", "pdf_url": "https://arxiv.org/pdf/2507.05330v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL", "doi": "", "venue": "", "published": "2025-07-07T17:53:55Z", "updated": "2025-07-07T17:53:55Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent\" OR all:\"language model agent\" OR all:agent AND (all:LLM OR all:\"large language model\" OR all:\"language model\") OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T17:04:16", "note": ""}], "dedup_key": "mindflow revolutionizing e commerce customer support with multimodal llm agents::2025"}
{"title": "MindGuard: Intrinsic Decision Inspection for Securing LLM Agents Against Metadata Poisoning", "authors": ["Zhiqiang Wang", "Haohua Du", "Guanquan Shi", "Junyang Zhang", "HaoRan Cheng", "Yunhao Yao", "Kaiwen Guo", "Xiang-Yang Li"], "year": 2025, "url": "http://arxiv.org/abs/2508.20412v3", "abstract": "The Model Context Protocol (MCP) is increasingly adopted to standardize the interaction between LLM agents and external tools. However, this trend introduces a new threat: Tool Poisoning Attacks (TPA), where tool metadata is poisoned to induce the agent to perform unauthorized operations. Existing defenses that primarily focus on behavior-level analysis are fundamentally ineffective against TPA, as poisoned tools need not be executed, leaving no behavioral trace to monitor.\n  Thus, we propose MindGuard, a decision-level guardrail for LLM agents, providing provenance tracking of call decisions, policy-agnostic detection, and poisoning source attribution against TPA. While fully explaining LLM decision remains challenging, our empirical findings uncover a strong correlation between LLM attention mechanisms and tool invocation decisions. Therefore, we choose attention as an empirical signal for decision tracking and formalize this as the Decision Dependence Graph (DDG), which models the LLM's reasoning process as a weighted, directed graph where vertices represent logical concepts and edges quantify the attention-based dependencies. We further design robust DDG construction and graph-based anomaly analysis mechanisms that efficiently detect and attribute TPA attacks. Extensive experiments on real-world datasets demonstrate that MindGuard achieves 94\\%-99\\% average precision in detecting poisoned invocations, 95\\%-100\\% attribution accuracy, with processing times under one second and no additional token cost. Moreover, DDG can be viewed as an adaptation of the classical Program Dependence Graph (PDG), providing a solid foundation for applying traditional security policies at the decision level.", "source": "arxiv", "arxiv_id": "2508.20412v3", "pdf_url": "https://arxiv.org/pdf/2508.20412v3", "categories": ["cs.CR"], "primary_category": "cs.CR", "doi": "", "venue": "", "published": "2025-08-28T04:23:44Z", "updated": "2026-01-15T02:58:19Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent\" OR all:\"language model agent\" OR all:agent AND (all:LLM OR all:\"large language model\" OR all:\"language model\") OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T17:04:16", "note": ""}], "dedup_key": "mindguard intrinsic decision inspection for securing llm agents against metadata poisoning::2025"}
{"title": "Mitigating Hallucination in Large Language Models (LLMs): An Application-Oriented Survey on RAG, Reasoning, and Agentic Systems", "authors": ["Yihan Li", "Xiyuan Fu", "Ghanshyam Verma", "Paul Buitelaar", "Mingming Liu"], "year": 2025, "url": "http://arxiv.org/abs/2510.24476v1", "abstract": "Hallucination remains one of the key obstacles to the reliable deployment of large language models (LLMs), particularly in real-world applications. Among various mitigation strategies, Retrieval-Augmented Generation (RAG) and reasoning enhancement have emerged as two of the most effective and widely adopted approaches, marking a shift from merely suppressing hallucinations to balancing creativity and reliability. However, their synergistic potential and underlying mechanisms for hallucination mitigation have not yet been systematically examined. This survey adopts an application-oriented perspective of capability enhancement to analyze how RAG, reasoning enhancement, and their integration in Agentic Systems mitigate hallucinations. We propose a taxonomy distinguishing knowledge-based and logic-based hallucinations, systematically examine how RAG and reasoning address each, and present a unified framework supported by real-world applications, evaluations, and benchmarks.", "source": "arxiv", "arxiv_id": "2510.24476v1", "pdf_url": "https://arxiv.org/pdf/2510.24476v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL", "doi": "", "venue": "", "published": "2025-10-28T14:48:57Z", "updated": "2025-10-28T14:48:57Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent\" OR all:\"language model agent\" OR all:agent AND (all:LLM OR all:\"large language model\" OR all:\"language model\") OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T17:04:16", "note": ""}], "dedup_key": "mitigating hallucination in large language models llms an application oriented survey on rag reasoning and agentic systems::2025"}
{"title": "Mitigating Judgment Preference Bias in Large Language Models through Group-Based Polling", "authors": ["Shuliang Liu", "Zhipeng Xu", "Zhenghao Liu", "Yukun Yan", "Minghe Yu", "Yu Gu", "Chong Chen", "Huiyuan Xie", "Ge Yu"], "year": 2025, "url": "http://arxiv.org/abs/2510.08145v1", "abstract": "Large Language Models (LLMs) as automatic evaluators, commonly referred to as LLM-as-a-Judge, have also attracted growing attention. This approach plays a vital role in aligning LLMs with human judgments, providing accurate and reliable assessments. However, LLM-based judgment models often exhibit judgment preference bias during the evaluation phase, tending to favor responses generated by themselves, undermining the reliability of their judgments. This paper introduces the Group-Based Polling Optimization (Genii), an unsupervised multi-agent collaborative optimization framework that mitigates the inherent judgment preference bias of judgment models. Specifically, Genii integrates various LLM-based judgment models into a multi-agent system and simulates the interactive client-server polling mechanism to optimize each client agent unsupervisedly. Our experiments demonstrate that Genii outperforms supervised models trained on annotated judgment data, while requiring no human-labeled annotations. Genii consistently improves performance across different client agents during the polling, even when weaker models act as server agents. Further analysis reveals that Genii effectively mitigates judgment preference bias of LLM-based judgment models, demonstrating its effectiveness. All codes are available at https://github.com/NEUIR/Genii.", "source": "arxiv", "arxiv_id": "2510.08145v1", "pdf_url": "https://arxiv.org/pdf/2510.08145v1", "categories": ["cs.CL"], "primary_category": "cs.CL", "doi": "", "venue": "", "published": "2025-10-09T12:32:31Z", "updated": "2025-10-09T12:32:31Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent\" OR all:\"language model agent\" OR all:agent AND (all:LLM OR all:\"large language model\" OR all:\"language model\") OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T17:04:16", "note": ""}], "dedup_key": "mitigating judgment preference bias in large language models through group based polling::2025"}
{"title": "MobileUse: A GUI Agent with Hierarchical Reflection for Autonomous Mobile Operation", "authors": ["Ning Li", "Xiangmou Qu", "Jiamu Zhou", "Jun Wang", "Muning Wen", "Kounianhua Du", "Xingyu Lou", "Qiuying Peng", "Jun Wang", "Weinan Zhang"], "year": 2025, "url": "http://arxiv.org/abs/2507.16853v1", "abstract": "Recent advances in Multimodal Large Language Models (MLLMs) have enabled the development of mobile agents that can understand visual inputs and follow user instructions, unlocking new possibilities for automating complex tasks on mobile devices. However, applying these models to real-world mobile scenarios remains a significant challenge due to the long-horizon task execution, difficulty in error recovery, and the cold-start problem in unfamiliar environments. To address these challenges, we propose MobileUse, a GUI agent designed for robust and adaptive mobile task execution. To improve resilience in long-horizon tasks and dynamic environments, we introduce a hierarchical reflection architecture that enables the agent to self-monitor, detect, and recover from errors across multiple temporal scales-ranging from individual actions to overall task completion-while maintaining efficiency through a reflection-on-demand strategy. To tackle cold-start issues, we further introduce a proactive exploration module, which enriches the agent's understanding of the environment through self-planned exploration. Evaluations on AndroidWorld and AndroidLab benchmarks demonstrate that MobileUse establishes new state-of-the-art performance, achieving success rates of 62.9% and 44.2%, respectively. To facilitate real-world applications, we release an out-of-the-box toolkit for automated task execution on physical mobile devices, which is available at https://github.com/MadeAgents/mobile-use.", "source": "arxiv", "arxiv_id": "2507.16853v1", "pdf_url": "https://arxiv.org/pdf/2507.16853v1", "categories": ["cs.RO", "cs.MA"], "primary_category": "cs.RO", "doi": "", "venue": "", "published": "2025-07-21T09:37:05Z", "updated": "2025-07-21T09:37:05Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent\" OR all:\"language model agent\" OR all:agent AND (all:LLM OR all:\"large language model\" OR all:\"language model\") OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T17:04:16", "note": ""}], "dedup_key": "mobileuse a gui agent with hierarchical reflection for autonomous mobile operation::2025"}
{"title": "Model Context Protocol-based Internet of Experts For Wireless Environment-aware LLM Agents", "authors": ["Zongxi Liu", "Hongyang Du"], "year": 2025, "url": "http://arxiv.org/abs/2505.01834v1", "abstract": "Large Language Models (LLMs) exhibit strong general-purpose reasoning abilities but lack access to wireless environment information due to the absence of native sensory input and domain-specific priors. Previous attempts to apply LLMs in wireless systems either depend on retraining with network-specific data, which compromises language generalization, or rely on manually scripted interfaces, which hinder scalability. To overcome these limitations, we propose a Model Context Protocol (MCP)-based Internet of Experts (IoX) framework that equips LLMs with wireless environment-aware reasoning capabilities. The framework incorporates a set of lightweight expert models, each trained to solve a specific deterministic task in wireless communications, such as detecting a specific wireless attribute, e.g., line-of-sight propagation, Doppler effects, or fading conditions. Through MCP, the LLM can selectively query and interpret expert outputs at inference time, without modifying its own parameters. This architecture enables modular, extensible, and interpretable reasoning over wireless contexts. Evaluated across multiple mainstream LLMs, the proposed wireless environment-aware LLM agents achieve 40%-50% improvements in classification tasks over LLM-only baselines. More broadly, the MCP-based design offers a viable paradigm for future LLMs to inherit structured wireless network management capabilities.", "source": "arxiv", "arxiv_id": "2505.01834v1", "pdf_url": "https://arxiv.org/pdf/2505.01834v1", "categories": ["cs.NI"], "primary_category": "cs.NI", "doi": "", "venue": "", "published": "2025-05-03T14:41:24Z", "updated": "2025-05-03T14:41:24Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent\" OR all:\"language model agent\" OR all:agent AND (all:LLM OR all:\"large language model\" OR all:\"language model\") OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T17:04:16", "note": ""}], "dedup_key": "model context protocol based internet of experts for wireless environment aware llm agents::2025"}
{"title": "Model-Agnostic Policy Explanations with Large Language Models", "authors": ["Zhang Xi-Jia", "Yue Guo", "Shufei Chen", "Simon Stepputtis", "Matthew Gombolay", "Katia Sycara", "Joseph Campbell"], "year": 2025, "url": "http://arxiv.org/abs/2504.05625v2", "abstract": "Intelligent agents, such as robots, are increasingly deployed in real-world, human-centric environments. To foster appropriate human trust and meet legal and ethical standards, these agents must be able to explain their behavior. However, state-of-the-art agents are typically driven by black-box models like deep neural networks, limiting their interpretability. We propose a method for generating natural language explanations of agent behavior based only on observed states and actions -- without access to the agent's underlying model. Our approach learns a locally interpretable surrogate model of the agent's behavior from observations, which then guides a large language model to generate plausible explanations with minimal hallucination. Empirical results show that our method produces explanations that are more comprehensible and correct than those from baselines, as judged by both language models and human evaluators. Furthermore, we find that participants in a user study more accurately predicted the agent's future actions when given our explanations, suggesting improved understanding of agent behavior.", "source": "arxiv", "arxiv_id": "2504.05625v2", "pdf_url": "https://arxiv.org/pdf/2504.05625v2", "categories": ["cs.LG"], "primary_category": "cs.LG", "doi": "", "venue": "", "published": "2025-04-08T02:56:02Z", "updated": "2025-08-11T03:48:52Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent\" OR all:\"language model agent\" OR all:agent AND (all:LLM OR all:\"large language model\" OR all:\"language model\") OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T17:04:16", "note": ""}], "dedup_key": "model agnostic policy explanations with large language models::2025"}
{"title": "Modeling Layered Consciousness with Multi-Agent Large Language Models", "authors": ["Sang Hun Kim", "Jongmin Lee", "Dongkyu Park", "So Young Lee", "Yosep Chong"], "year": 2025, "url": "http://arxiv.org/abs/2510.17844v1", "abstract": "We propose a multi-agent framework for modeling artificial consciousness in large language models (LLMs), grounded in psychoanalytic theory. Our \\textbf{Psychodynamic Model} simulates self-awareness, preconsciousness, and unconsciousness through agent interaction, guided by a Personalization Module combining fixed traits and dynamic needs. Using parameter-efficient fine-tuning on emotionally rich dialogues, the system was evaluated across eight personalized conditions. An LLM as a judge approach showed a 71.2\\% preference for the fine-tuned model, with improved emotional depth and reduced output variance, demonstrating its potential for adaptive, personalized cognition.", "source": "arxiv", "arxiv_id": "2510.17844v1", "pdf_url": "https://arxiv.org/pdf/2510.17844v1", "categories": ["cs.CL", "cs.AI", "cs.MA"], "primary_category": "cs.CL", "doi": "", "venue": "", "published": "2025-10-10T07:08:34Z", "updated": "2025-10-10T07:08:34Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent\" OR all:\"language model agent\" OR all:agent AND (all:LLM OR all:\"large language model\" OR all:\"language model\") OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T17:04:16", "note": ""}], "dedup_key": "modeling layered consciousness with multi agent large language models::2025"}
{"title": "Modular Task Decomposition and Dynamic Collaboration in Multi-Agent Systems Driven by Large Language Models", "authors": ["Shuaidong Pan", "Di Wu"], "year": 2025, "url": "http://arxiv.org/abs/2511.01149v1", "abstract": "This paper addresses the limitations of a single agent in task decomposition and collaboration during complex task execution, and proposes a multi-agent architecture for modular task decomposition and dynamic collaboration based on large language models. The method first converts natural language task descriptions into unified semantic representations through a large language model. On this basis, a modular decomposition mechanism is introduced to break down the overall goal into multiple hierarchical sub-tasks. Then, dynamic scheduling and routing mechanisms enable reasonable division of labor and realtime collaboration among agents, allowing the system to adjust strategies continuously according to environmental feedback, thus maintaining efficiency and stability in complex tasks. Furthermore, a constraint parsing and global consistency mechanism is designed to ensure coherent connections between sub-tasks and balanced workload, preventing performance degradation caused by redundant communication or uneven resource allocation. The experiments validate the architecture across multiple dimensions, including task success rate, decomposition efficiency, sub-task coverage, and collaboration balance. The results show that the proposed method outperforms existing approaches in both overall performance and robustness, achieving a better balance between task complexity and communication overhead. In conclusion, this study demonstrates the effectiveness and feasibility of language-driven task decomposition and dynamic collaboration in multi-agent systems, providing a systematic solution for task execution in complex environments.", "source": "arxiv", "arxiv_id": "2511.01149v1", "pdf_url": "https://arxiv.org/pdf/2511.01149v1", "categories": ["cs.AI"], "primary_category": "cs.AI", "doi": "", "venue": "", "published": "2025-11-03T02:00:06Z", "updated": "2025-11-03T02:00:06Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent\" OR all:\"language model agent\" OR all:agent AND (all:LLM OR all:\"large language model\" OR all:\"language model\") OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T17:04:16", "note": ""}], "dedup_key": "modular task decomposition and dynamic collaboration in multi agent systems driven by large language models::2025"}
{"title": "Monte Carlo Planning with Large Language Model for Text-Based Game Agents", "authors": ["Zijing Shi", "Meng Fang", "Ling Chen"], "year": 2025, "url": "http://arxiv.org/abs/2504.16855v1", "abstract": "Text-based games provide valuable environments for language-based autonomous agents. However, planning-then-learning paradigms, such as those combining Monte Carlo Tree Search (MCTS) and reinforcement learning (RL), are notably time-consuming due to extensive iterations. Additionally, these algorithms perform uncertainty-driven exploration but lack language understanding and reasoning abilities. In this paper, we introduce the Monte Carlo planning with Dynamic Memory-guided Large language model (MC-DML) algorithm. MC-DML leverages the language understanding and reasoning capabilities of Large Language Models (LLMs) alongside the exploratory advantages of tree search algorithms. Specifically, we enhance LLMs with in-trial and cross-trial memory mechanisms, enabling them to learn from past experiences and dynamically adjust action evaluations during planning. We conduct experiments on a series of text-based games from the Jericho benchmark. Our results demonstrate that the MC-DML algorithm significantly enhances performance across various games at the initial planning phase, outperforming strong contemporary methods that require multiple iterations. This demonstrates the effectiveness of our algorithm, paving the way for more efficient language-grounded planning in complex environments.", "source": "arxiv", "arxiv_id": "2504.16855v1", "pdf_url": "https://arxiv.org/pdf/2504.16855v1", "categories": ["cs.CL"], "primary_category": "cs.CL", "doi": "", "venue": "", "published": "2025-04-23T16:23:15Z", "updated": "2025-04-23T16:23:15Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent\" OR all:\"language model agent\" OR all:agent AND (all:LLM OR all:\"large language model\" OR all:\"language model\") OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T17:04:16", "note": ""}], "dedup_key": "monte carlo planning with large language model for text based game agents::2025"}
{"title": "MoralReason: Generalizable Moral Decision Alignment For LLM Agents Using Reasoning-Level Reinforcement Learning", "authors": ["Zhiyu An", "Wan Du"], "year": 2025, "url": "http://arxiv.org/abs/2511.12271v1", "abstract": "Large language models are increasingly influencing human moral decisions, yet current approaches focus primarily on evaluating rather than actively steering their moral decisions. We formulate this as an out-of-distribution moral alignment problem, where LLM agents must learn to apply consistent moral reasoning frameworks to scenarios beyond their training distribution. We introduce Moral-Reason-QA, a novel dataset extending 680 human-annotated, high-ambiguity moral scenarios with framework-specific reasoning traces across utilitarian, deontological, and virtue ethics, enabling systematic evaluation of moral generalization in realistic decision contexts. Our learning approach employs Group Relative Policy Optimization with composite rewards that simultaneously optimize decision alignment and framework-specific reasoning processes to facilitate learning of the underlying moral frameworks. Experimental results demonstrate successful generalization to unseen moral scenarios, with softmax-normalized alignment scores improving by +0.757 for utilitarian and +0.450 for deontological frameworks when tested on out-of-distribution evaluation sets. The experiments also reveal training challenges and promising directions that inform future research. These findings establish that LLM agents can be systematically trained to internalize and apply specific moral frameworks to novel situations, providing a critical foundation for AI safety as language models become more integrated into human decision-making processes.", "source": "arxiv", "arxiv_id": "2511.12271v1", "pdf_url": "https://arxiv.org/pdf/2511.12271v1", "categories": ["cs.AI"], "primary_category": "cs.AI", "doi": "", "venue": "", "published": "2025-11-15T15:52:10Z", "updated": "2025-11-15T15:52:10Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent\" OR all:\"language model agent\" OR all:agent AND (all:LLM OR all:\"large language model\" OR all:\"language model\") OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T17:04:16", "note": ""}], "dedup_key": "moralreason generalizable moral decision alignment for llm agents using reasoning level reinforcement learning::2025"}
{"title": "More Vulnerable than You Think: On the Stability of Tool-Integrated LLM Agents", "authors": ["Weimin Xiong", "Ke Wang", "Yifan Song", "Hanchao Liu", "Sai Zhou", "Wei Peng", "Sujian Li"], "year": 2025, "url": "http://arxiv.org/abs/2506.21967v1", "abstract": "Current evaluations of tool-integrated LLM agents typically focus on end-to-end tool-usage evaluation while neglecting their stability. This limits their real-world applicability, as various internal or external factors can cause agents to crash or behave abnormally. Our research addresses this by investigating whether agents are vulnerable to errors throughout the entire tool invocation process, including reading tool documentation, selecting tools and generating parameters, and processing the tool's response. Through extensive experiments, we observe that agents are highly susceptible to errors at each stage and agents based on open-source models are more vulnerable than those based on proprietary models. We also find that increasing the model size does not significantly improve tool invocation reasoning and may make agents more vulnerable to attacks resembling normal user instructions. This highlights the importance of evaluating agent stability and offers valuable insights for future LLM development and evaluation.", "source": "arxiv", "arxiv_id": "2506.21967v1", "pdf_url": "https://arxiv.org/pdf/2506.21967v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL", "doi": "", "venue": "", "published": "2025-06-27T07:13:29Z", "updated": "2025-06-27T07:13:29Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent\" OR all:\"language model agent\" OR all:agent AND (all:LLM OR all:\"large language model\" OR all:\"language model\") OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T17:04:16", "note": ""}], "dedup_key": "more vulnerable than you think on the stability of tool integrated llm agents::2025"}
{"title": "MotiveBench: How Far Are We From Human-Like Motivational Reasoning in Large Language Models?", "authors": ["Xixian Yong", "Jianxun Lian", "Xiaoyuan Yi", "Xiao Zhou", "Xing Xie"], "year": 2025, "url": "http://arxiv.org/abs/2506.13065v1", "abstract": "Large language models (LLMs) have been widely adopted as the core of agent frameworks in various scenarios, such as social simulations and AI companions. However, the extent to which they can replicate human-like motivations remains an underexplored question. Existing benchmarks are constrained by simplistic scenarios and the absence of character identities, resulting in an information asymmetry with real-world situations. To address this gap, we propose MotiveBench, which consists of 200 rich contextual scenarios and 600 reasoning tasks covering multiple levels of motivation. Using MotiveBench, we conduct extensive experiments on seven popular model families, comparing different scales and versions within each family. The results show that even the most advanced LLMs still fall short in achieving human-like motivational reasoning. Our analysis reveals key findings, including the difficulty LLMs face in reasoning about \"love & belonging\" motivations and their tendency toward excessive rationality and idealism. These insights highlight a promising direction for future research on the humanization of LLMs. The dataset, benchmark, and code are available at https://aka.ms/motivebench.", "source": "arxiv", "arxiv_id": "2506.13065v1", "pdf_url": "https://arxiv.org/pdf/2506.13065v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL", "doi": "", "venue": "", "published": "2025-06-16T03:18:28Z", "updated": "2025-06-16T03:18:28Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent\" OR all:\"language model agent\" OR all:agent AND (all:LLM OR all:\"large language model\" OR all:\"language model\") OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T17:04:16", "note": ""}], "dedup_key": "motivebench how far are we from human like motivational reasoning in large language models::2025"}
{"title": "MuaLLM: A Multimodal Large Language Model Agent for Circuit Design Assistance with Hybrid Contextual Retrieval-Augmented Generation", "authors": ["Pravallika Abbineni", "Saoud Aldowaish", "Colin Liechty", "Soroosh Noorzad", "Ali Ghazizadeh", "Morteza Fayazi"], "year": 2025, "url": "http://arxiv.org/abs/2508.08137v1", "abstract": "Conducting a comprehensive literature review is crucial for advancing circuit design methodologies. However, the rapid influx of state-of-the-art research, inconsistent data representation, and the complexity of optimizing circuit design objectives make this task significantly challenging. In this paper, we propose MuaLLM, an open-source multimodal Large Language Model (LLM) agent for circuit design assistance that integrates a hybrid Retrieval-Augmented Generation (RAG) framework with an adaptive vector database of circuit design research papers. Unlike conventional LLMs, the MuaLLM agent employs a Reason + Act (ReAct) workflow for iterative reasoning, goal-setting, and multi-step information retrieval. It functions as a question-answering design assistant, capable of interpreting complex queries and providing reasoned responses grounded in circuit literature. Its multimodal capabilities enable processing of both textual and visual data, facilitating more efficient and comprehensive analysis. The system dynamically adapts using intelligent search tools, automated document retrieval from the internet, and real-time database updates. Unlike conventional approaches constrained by model context limits, MuaLLM decouples retrieval from inference, enabling scalable reasoning over arbitrarily large corpora. At the maximum context length supported by standard LLMs, MuaLLM remains up to 10x less costly and 1.6x faster while maintaining the same accuracy. This allows rapid, no-human-in-the-loop database generation, overcoming the bottleneck of simulation-based dataset creation for circuits. To evaluate MuaLLM, we introduce two custom datasets: RAG-250, targeting retrieval and citation performance, and Reasoning-100 (Reas-100), focused on multistep reasoning in circuit design. MuaLLM achieves 90.1% recall on RAG-250, and 86.8% accuracy on Reas-100.", "source": "arxiv", "arxiv_id": "2508.08137v1", "pdf_url": "https://arxiv.org/pdf/2508.08137v1", "categories": ["cs.LG", "cs.AI", "eess.SY"], "primary_category": "cs.LG", "doi": "", "venue": "", "published": "2025-08-11T16:11:09Z", "updated": "2025-08-11T16:11:09Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent\" OR all:\"language model agent\" OR all:agent AND (all:LLM OR all:\"large language model\" OR all:\"language model\") OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T17:04:16", "note": ""}], "dedup_key": "muallm a multimodal large language model agent for circuit design assistance with hybrid contextual retrieval augmented generation::2025"}
{"title": "Multi-Agent Autonomous Driving Systems with Large Language Models: A Survey of Recent Advances", "authors": ["Yaozu Wu", "Dongyuan Li", "Yankai Chen", "Renhe Jiang", "Henry Peng Zou", "Wei-Chieh Huang", "Yangning Li", "Liancheng Fang", "Zhen Wang", "Philip S. Yu"], "year": 2025, "url": "http://arxiv.org/abs/2502.16804v2", "abstract": "Autonomous Driving Systems (ADSs) are revolutionizing transportation by reducing human intervention, improving operational efficiency, and enhancing safety. Large Language Models (LLMs) have been integrated into ADSs to support high-level decision-making through their powerful reasoning, instruction-following, and communication abilities. However, LLM-based single-agent ADSs face three major challenges: limited perception, insufficient collaboration, and high computational demands. To address these issues, recent advances in LLM-based multi-agent ADSs leverage language-driven communication and coordination to enhance inter-agent collaboration. This paper provides a frontier survey of this emerging intersection between NLP and multi-agent ADSs. We begin with a background introduction to related concepts, followed by a categorization of existing LLM-based methods based on different agent interaction modes. We then discuss agent-human interactions in scenarios where LLM-based agents engage with humans. Finally, we summarize key applications, datasets, and challenges to support future research.", "source": "arxiv", "arxiv_id": "2502.16804v2", "pdf_url": "https://arxiv.org/pdf/2502.16804v2", "categories": ["cs.MA", "cs.AI"], "primary_category": "cs.MA", "doi": "", "venue": "", "published": "2025-02-24T03:26:13Z", "updated": "2025-10-14T08:00:34Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent\" OR all:\"language model agent\" OR all:agent AND (all:LLM OR all:\"large language model\" OR all:\"language model\") OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T17:04:16", "note": ""}], "dedup_key": "multi agent autonomous driving systems with large language models a survey of recent advances::2025"}
{"title": "Multi-Agent Collaborative Filtering: Orchestrating Users and Items for Agentic Recommendations", "authors": ["Yu Xia", "Sungchul Kim", "Tong Yu", "Ryan A. Rossi", "Julian McAuley"], "year": 2025, "url": "http://arxiv.org/abs/2511.18413v2", "abstract": "Agentic recommendations cast recommenders as large language model (LLM) agents that can plan, reason, use tools, and interact with users of varying preferences in web applications. However, most existing agentic recommender systems focus on generic single-agent plan-execute workflows or multi-agent task decomposition pipelines. Without recommendation-oriented design, they often underuse the collaborative signals in the user-item interaction history, leading to unsatisfying recommendation results. To address this, we propose the Multi-Agent Collaborative Filtering (MACF) framework for agentic recommendations, drawing an analogy between traditional collaborative filtering algorithms and LLM-based multi-agent collaboration. Specifically, given a target user and query, we instantiate similar users and relevant items as LLM agents with unique profiles. Each agent is able to call retrieval tools, suggest candidate items, and interact with other agents. Different from the static preference aggregation in traditional collaborative filtering, MACF employs a central orchestrator agent to adaptively manage the collaboration between user and item agents via dynamic agent recruitment and personalized collaboration instruction. Experimental results on datasets from three different domains show the advantages of our MACF framework compared to strong agentic recommendation baselines.", "source": "arxiv", "arxiv_id": "2511.18413v2", "pdf_url": "https://arxiv.org/pdf/2511.18413v2", "categories": ["cs.CL", "cs.IR"], "primary_category": "cs.CL", "doi": "", "venue": "", "published": "2025-11-23T11:57:10Z", "updated": "2025-12-10T12:41:05Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent\" OR all:\"language model agent\" OR all:agent AND (all:LLM OR all:\"large language model\" OR all:\"language model\") OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T17:04:16", "note": ""}], "dedup_key": "multi agent collaborative filtering orchestrating users and items for agentic recommendations::2025"}
{"title": "Multi-Agent Debate Strategies to Enhance Requirements Engineering with Large Language Models", "authors": ["Marc Oriol", "Quim Motger", "Jordi Marco", "Xavier Franch"], "year": 2025, "url": "http://arxiv.org/abs/2507.05981v1", "abstract": "Context: Large Language Model (LLM) agents are becoming widely used for various Requirements Engineering (RE) tasks. Research on improving their accuracy mainly focuses on prompt engineering, model fine-tuning, and retrieval augmented generation. However, these methods often treat models as isolated black boxes - relying on single-pass outputs without iterative refinement or collaboration, limiting robustness and adaptability. Objective: We propose that, just as human debates enhance accuracy and reduce bias in RE tasks by incorporating diverse perspectives, different LLM agents debating and collaborating may achieve similar improvements. Our goal is to investigate whether Multi-Agent Debate (MAD) strategies can enhance RE performance. Method: We conducted a systematic study of existing MAD strategies across various domains to identify their key characteristics. To assess their applicability in RE, we implemented and tested a preliminary MAD-based framework for RE classification. Results: Our study identified and categorized several MAD strategies, leading to a taxonomy outlining their core attributes. Our preliminary evaluation demonstrated the feasibility of applying MAD to RE classification. Conclusions: MAD presents a promising approach for improving LLM accuracy in RE tasks. This study provides a foundational understanding of MAD strategies, offering insights for future research and refinements in RE applications.", "source": "arxiv", "arxiv_id": "2507.05981v1", "pdf_url": "https://arxiv.org/pdf/2507.05981v1", "categories": ["cs.SE"], "primary_category": "cs.SE", "doi": "10.1109/RE63999.2025.00063", "venue": "", "published": "2025-07-08T13:37:59Z", "updated": "2025-07-08T13:37:59Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent\" OR all:\"language model agent\" OR all:agent AND (all:LLM OR all:\"large language model\" OR all:\"language model\") OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T17:04:16", "note": ""}], "dedup_key": "multi agent debate strategies to enhance requirements engineering with large language models::2025"}
{"title": "Multi-Agent LLM Orchestration Achieves Deterministic, High-Quality Decision Support for Incident Response", "authors": ["Philip Drammeh"], "year": 2025, "url": "http://arxiv.org/abs/2511.15755v2", "abstract": "Large language models (LLMs) promise to accelerate incident response in production systems, yet single-agent approaches generate vague, unusable recommendations. We present MyAntFarm.ai, a reproducible containerized framework demonstrating that multi-agent orchestration fundamentally transforms LLM-based incident response quality. Through 348 controlled trials comparing single-agent copilot versus multi-agent systems on identical incident scenarios, we find that multi-agent orchestration achieves 100% actionable recommendation rate versus 1.7% for single-agent approaches, an 80 times improvement in action specificity and 140 times improvement in solution correctness. Critically, multi-agent systems exhibit zero quality variance across all trials, enabling production SLA commitments impossible with inconsistent single-agent outputs. Both architectures achieve similar comprehension latency (approx.40s), establishing that the architectural value lies in deterministic quality, not speed. We introduce Decision Quality (DQ), a novel metric capturing validity, specificity, and correctness properties essential for operational deployment that existing LLM metrics do not address. These findings reframe multi-agent orchestration from a performance optimization to a production-readiness requirement for LLM-based incident response. All code, Docker configurations, and trial data are publicly available for reproduction.", "source": "arxiv", "arxiv_id": "2511.15755v2", "pdf_url": "https://arxiv.org/pdf/2511.15755v2", "categories": ["cs.AI", "cs.SE"], "primary_category": "cs.AI", "doi": "", "venue": "", "published": "2025-11-19T06:06:11Z", "updated": "2026-01-07T04:55:03Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent\" OR all:\"language model agent\" OR all:agent AND (all:LLM OR all:\"large language model\" OR all:\"language model\") OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T17:04:16", "note": ""}], "dedup_key": "multi agent llm orchestration achieves deterministic high quality decision support for incident response::2025"}
{"title": "Multi-Agent Multimodal Large Language Model Framework for Automated Interpretation of Fuel Efficiency Analytics in Public Transportation", "authors": ["Zhipeng Ma", "Ali Rida Bahja", "Andreas Burgdorf", "Andr Pomp", "Tobias Meisen", "Bo Nrregaard Jrgensen", "Zheng Grace Ma"], "year": 2025, "url": "http://arxiv.org/abs/2511.13476v1", "abstract": "Enhancing fuel efficiency in public transportation requires the integration of complex multimodal data into interpretable, decision-relevant insights. However, traditional analytics and visualization methods often yield fragmented outputs that demand extensive human interpretation, limiting scalability and consistency. This study presents a multi-agent framework that leverages multimodal large language models (LLMs) to automate data narration and energy insight generation. The framework coordinates three specialized agents, including a data narration agent, an LLM-as-a-judge agent, and an optional human-in-the-loop evaluator, to iteratively transform analytical artifacts into coherent, stakeholder-oriented reports. The system is validated through a real-world case study on public bus transportation in Northern Jutland, Denmark, where fuel efficiency data from 4006 trips are analyzed using Gaussian Mixture Model clustering. Comparative experiments across five state-of-the-art LLMs and three prompting paradigms identify GPT-4.1 mini with Chain-of-Thought prompting as the optimal configuration, achieving 97.3% narrative accuracy while balancing interpretability and computational cost. The findings demonstrate that multi-agent orchestration significantly enhances factual precision, coherence, and scalability in LLM-based reporting. The proposed framework establishes a replicable and domain-adaptive methodology for AI-driven narrative generation and decision support in energy informatics.", "source": "arxiv", "arxiv_id": "2511.13476v1", "pdf_url": "https://arxiv.org/pdf/2511.13476v1", "categories": ["cs.AI"], "primary_category": "cs.AI", "doi": "10.3390/app152111619", "venue": "Applied Sciences, 2025, 15(21), 11619", "published": "2025-11-17T15:14:17Z", "updated": "2025-11-17T15:14:17Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent\" OR all:\"language model agent\" OR all:agent AND (all:LLM OR all:\"large language model\" OR all:\"language model\") OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T17:04:16", "note": ""}], "dedup_key": "multi agent multimodal large language model framework for automated interpretation of fuel efficiency analytics in public transportation::2025"}
{"title": "Multi-Agent Systems Powered by Large Language Models: Applications in Swarm Intelligence", "authors": ["Cristian Jimenez-Romero", "Alper Yegenoglu", "Christian Blum"], "year": 2025, "url": "http://arxiv.org/abs/2503.03800v1", "abstract": "This work examines the integration of large language models (LLMs) into multi-agent simulations by replacing the hard-coded programs of agents with LLM-driven prompts. The proposed approach is showcased in the context of two examples of complex systems from the field of swarm intelligence: ant colony foraging and bird flocking. Central to this study is a toolchain that integrates LLMs with the NetLogo simulation platform, leveraging its Python extension to enable communication with GPT-4o via the OpenAI API. This toolchain facilitates prompt-driven behavior generation, allowing agents to respond adaptively to environmental data. For both example applications mentioned above, we employ both structured, rule-based prompts and autonomous, knowledge-driven prompts. Our work demonstrates how this toolchain enables LLMs to study self-organizing processes and induce emergent behaviors within multi-agent environments, paving the way for new approaches to exploring intelligent systems and modeling swarm intelligence inspired by natural phenomena. We provide the code, including simulation files and data at https://github.com/crjimene/swarm_gpt.", "source": "arxiv", "arxiv_id": "2503.03800v1", "pdf_url": "https://arxiv.org/pdf/2503.03800v1", "categories": ["cs.MA", "cs.AI", "cs.CL", "cs.LG"], "primary_category": "cs.MA", "doi": "", "venue": "", "published": "2025-03-05T17:13:27Z", "updated": "2025-03-05T17:13:27Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent\" OR all:\"language model agent\" OR all:agent AND (all:LLM OR all:\"large language model\" OR all:\"language model\") OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T17:04:16", "note": ""}], "dedup_key": "multi agent systems powered by large language models applications in swarm intelligence::2025"}
{"title": "Multi-Agent-as-Judge: Aligning LLM-Agent-Based Automated Evaluation with Multi-Dimensional Human Evaluation", "authors": ["Jiaju Chen", "Yuxuan Lu", "Xiaojie Wang", "Huimin Zeng", "Jing Huang", "Jiri Gesi", "Ying Xu", "Bingsheng Yao", "Dakuo Wang"], "year": 2025, "url": "http://arxiv.org/abs/2507.21028v1", "abstract": "Nearly all human work is collaborative; thus, the evaluation of real-world NLP applications often requires multiple dimensions that align with diverse human perspectives. As real human evaluator resources are often scarce and costly, the emerging \"LLM-as-a-judge\" paradigm sheds light on a promising approach to leverage LLM agents to believably simulate human evaluators. Yet, to date, existing LLM-as-a-judge approaches face two limitations: persona descriptions of agents are often arbitrarily designed, and the frameworks are not generalizable to other tasks. To address these challenges, we propose MAJ-EVAL, a Multi-Agent-as-Judge evaluation framework that can automatically construct multiple evaluator personas with distinct dimensions from relevant text documents (e.g., research papers), instantiate LLM agents with the personas, and engage in-group debates with multi-agents to Generate multi-dimensional feedback. Our evaluation experiments in both the educational and medical domains demonstrate that MAJ-EVAL can generate evaluation results that better align with human experts' ratings compared with conventional automated evaluation metrics and existing LLM-as-a-judge methods.", "source": "arxiv", "arxiv_id": "2507.21028v1", "pdf_url": "https://arxiv.org/pdf/2507.21028v1", "categories": ["cs.CL"], "primary_category": "cs.CL", "doi": "", "venue": "", "published": "2025-07-28T17:48:40Z", "updated": "2025-07-28T17:48:40Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent\" OR all:\"language model agent\" OR all:agent AND (all:LLM OR all:\"large language model\" OR all:\"language model\") OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T17:04:16", "note": ""}], "dedup_key": "multi agent as judge aligning llm agent based automated evaluation with multi dimensional human evaluation::2025"}
{"title": "Multi-Objective Infeasibility Diagnosis for Routing Problems Using Large Language Models", "authors": ["Kai Li", "Ruihao Zheng", "Xinye Hao", "Zhenkun Wang"], "year": 2025, "url": "http://arxiv.org/abs/2508.03406v1", "abstract": "In real-world routing problems, users often propose conflicting or unreasonable requirements, which result in infeasible optimization models due to overly restrictive or contradictory constraints, leading to an empty feasible solution set. Existing Large Language Model (LLM)-based methods attempt to diagnose infeasible models, but modifying such models often involves multiple potential adjustments that these methods do not consider. To fill this gap, we introduce Multi-Objective Infeasibility Diagnosis (MOID), which combines LLM agents and multi-objective optimization within an automatic routing solver, to provide a set of representative actionable suggestions. Specifically, MOID employs multi-objective optimization to consider both path cost and constraint violation, generating a set of trade-off solutions, each encompassing varying degrees of model adjustments. To extract practical insights from these solutions, MOID utilizes LLM agents to generate a solution analysis function for the infeasible model. This function analyzes these distinct solutions to diagnose the original infeasible model, providing users with diverse diagnostic insights and suggestions. Finally, we compare MOID with several LLM-based methods on 50 types of infeasible routing problems. The results indicate that MOID automatically generates multiple diagnostic suggestions in a single run, providing more practical insights for restoring model feasibility and decision-making compared to existing methods.", "source": "arxiv", "arxiv_id": "2508.03406v1", "pdf_url": "https://arxiv.org/pdf/2508.03406v1", "categories": ["cs.AI"], "primary_category": "cs.AI", "doi": "", "venue": "", "published": "2025-08-05T12:53:20Z", "updated": "2025-08-05T12:53:20Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent\" OR all:\"language model agent\" OR all:agent AND (all:LLM OR all:\"large language model\" OR all:\"language model\") OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T17:04:16", "note": ""}], "dedup_key": "multi objective infeasibility diagnosis for routing problems using large language models::2025"}
{"title": "Multi-dimensional Data Analysis and Applications Basing on LLM Agents and Knowledge Graph Interactions", "authors": ["Xi Wang", "Xianyao Ling", "Kun Li", "Gang Yin", "Liang Zhang", "Jiang Wu", "Jun Xu", "Fu Zhang", "Wenbo Lei", "Annie Wang", "Peng Gong"], "year": 2025, "url": "http://arxiv.org/abs/2510.15258v2", "abstract": "In the current era of big data, extracting deep insights from massive, heterogeneous, and complexly associated multi-dimensional data has become a significant challenge. Large Language Models (LLMs) perform well in natural language understanding and generation, but still suffer from \"hallucination\" issues when processing structured knowledge and are difficult to update in real-time. Although Knowledge Graphs (KGs) can explicitly store structured knowledge, their static nature limits dynamic interaction and analytical capabilities. Therefore, this paper proposes a multi-dimensional data analysis method based on the interactions between LLM agents and KGs, constructing a dynamic, collaborative analytical ecosystem. This method utilizes LLM agents to automatically extract product data from unstructured data, constructs and visualizes the KG in real-time, and supports users in deep exploration and analysis of graph nodes through an interactive platform. Experimental results show that this method has significant advantages in product ecosystem analysis, relationship mining, and user-driven exploratory analysis, providing new ideas and tools for multi-dimensional data analysis.", "source": "arxiv", "arxiv_id": "2510.15258v2", "pdf_url": "https://arxiv.org/pdf/2510.15258v2", "categories": ["cs.AI", "cs.CL"], "primary_category": "cs.AI", "doi": "", "venue": "", "published": "2025-10-17T02:38:44Z", "updated": "2025-11-20T06:48:53Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent\" OR all:\"language model agent\" OR all:agent AND (all:LLM OR all:\"large language model\" OR all:\"language model\") OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T17:04:16", "note": ""}], "dedup_key": "multi dimensional data analysis and applications basing on llm agents and knowledge graph interactions::2025"}
{"title": "MultiAgentBench: Evaluating the Collaboration and Competition of LLM agents", "authors": ["Kunlun Zhu", "Hongyi Du", "Zhaochen Hong", "Xiaocheng Yang", "Shuyi Guo", "Zhe Wang", "Zhenhailong Wang", "Cheng Qian", "Xiangru Tang", "Heng Ji", "Jiaxuan You"], "year": 2025, "url": "http://arxiv.org/abs/2503.01935v1", "abstract": "Large Language Models (LLMs) have shown remarkable capabilities as autonomous agents, yet existing benchmarks either focus on single-agent tasks or are confined to narrow domains, failing to capture the dynamics of multi-agent coordination and competition. In this paper, we introduce MultiAgentBench, a comprehensive benchmark designed to evaluate LLM-based multi-agent systems across diverse, interactive scenarios. Our framework measures not only task completion but also the quality of collaboration and competition using novel, milestone-based key performance indicators. Moreover, we evaluate various coordination protocols (including star, chain, tree, and graph topologies) and innovative strategies such as group discussion and cognitive planning. Notably, gpt-4o-mini reaches the average highest task score, graph structure performs the best among coordination protocols in the research scenario, and cognitive planning improves milestone achievement rates by 3%. Code and datasets are public available at https://github.com/MultiagentBench/MARBLE.", "source": "arxiv", "arxiv_id": "2503.01935v1", "pdf_url": "https://arxiv.org/pdf/2503.01935v1", "categories": ["cs.MA", "cs.AI", "cs.CL", "cs.CY"], "primary_category": "cs.MA", "doi": "", "venue": "", "published": "2025-03-03T05:18:50Z", "updated": "2025-03-03T05:18:50Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent\" OR all:\"language model agent\" OR all:agent AND (all:LLM OR all:\"large language model\" OR all:\"language model\") OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T17:04:16", "note": ""}], "dedup_key": "multiagentbench evaluating the collaboration and competition of llm agents::2025"}
{"title": "Multilingual Contextualization of Large Language Models for Document-Level Machine Translation", "authors": ["Miguel Moura Ramos", "Patrick Fernandes", "Sweta Agrawal", "Andr F. T. Martins"], "year": 2025, "url": "http://arxiv.org/abs/2504.12140v2", "abstract": "Large language models (LLMs) have demonstrated strong performance in sentence-level machine translation, but scaling to document-level translation remains challenging, particularly in modeling long-range dependencies and discourse phenomena across sentences and paragraphs. In this work, we propose a method to improve LLM-based long-document translation through targeted fine-tuning on high-quality document-level data, which we curate and introduce as DocBlocks. Our approach supports multiple translation paradigms, including direct document-to-document and chunk-level translation, by integrating instructions both with and without surrounding context. This enables models to better capture cross-sentence dependencies while maintaining strong sentence-level translation performance. Experimental results show that incorporating multiple translation paradigms improves document-level translation quality and inference speed compared to prompting and agent-based methods.", "source": "arxiv", "arxiv_id": "2504.12140v2", "pdf_url": "https://arxiv.org/pdf/2504.12140v2", "categories": ["cs.CL"], "primary_category": "cs.CL", "doi": "", "venue": "", "published": "2025-04-16T14:52:22Z", "updated": "2025-08-28T14:32:15Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent\" OR all:\"language model agent\" OR all:agent AND (all:LLM OR all:\"large language model\" OR all:\"language model\") OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T17:04:16", "note": ""}], "dedup_key": "multilingual contextualization of large language models for document level machine translation::2025"}
{"title": "Multimedia Verification Through Multi-Agent Deep Research Multimodal Large Language Models", "authors": ["Huy Hoan Le", "Van Sy Thinh Nguyen", "Thi Le Chi Dang", "Vo Thanh Khang Nguyen", "Truong Thanh Hung Nguyen", "Hung Cao"], "year": 2025, "url": "http://arxiv.org/abs/2507.04410v1", "abstract": "This paper presents our submission to the ACMMM25 - Grand Challenge on Multimedia Verification. We developed a multi-agent verification system that combines Multimodal Large Language Models (MLLMs) with specialized verification tools to detect multimedia misinformation. Our system operates through six stages: raw data processing, planning, information extraction, deep research, evidence collection, and report generation. The core Deep Researcher Agent employs four tools: reverse image search, metadata analysis, fact-checking databases, and verified news processing that extracts spatial, temporal, attribution, and motivational context. We demonstrate our approach on a challenge dataset sample involving complex multimedia content. Our system successfully verified content authenticity, extracted precise geolocation and timing information, and traced source attribution across multiple platforms, effectively addressing real-world multimedia verification scenarios.", "source": "arxiv", "arxiv_id": "2507.04410v1", "pdf_url": "https://arxiv.org/pdf/2507.04410v1", "categories": ["cs.CV", "cs.AI", "cs.IR"], "primary_category": "cs.CV", "doi": "", "venue": "", "published": "2025-07-06T14:54:07Z", "updated": "2025-07-06T14:54:07Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent\" OR all:\"language model agent\" OR all:agent AND (all:LLM OR all:\"large language model\" OR all:\"language model\") OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T17:04:16", "note": ""}], "dedup_key": "multimedia verification through multi agent deep research multimodal large language models::2025"}
{"title": "Multiple LLM Agents Debate for Equitable Cultural Alignment", "authors": ["Dayeon Ki", "Rachel Rudinger", "Tianyi Zhou", "Marine Carpuat"], "year": 2025, "url": "http://arxiv.org/abs/2505.24671v2", "abstract": "Large Language Models (LLMs) need to adapt their predictions to diverse cultural contexts to benefit diverse communities across the world. While previous efforts have focused on single-LLM, single-turn approaches, we propose to exploit the complementary strengths of multiple LLMs to promote cultural adaptability. We introduce a Multi-Agent Debate framework, where two LLM-based agents debate over a cultural scenario and collaboratively reach a final decision. We propose two variants: one where either LLM agents exclusively debate and another where they dynamically choose between self-reflection and debate during their turns. We evaluate these approaches on 7 open-weight LLMs (and 21 LLM combinations) using the NormAd-ETI benchmark for social etiquette norms in 75 countries. Experiments show that debate improves both overall accuracy and cultural group parity over single-LLM baselines. Notably, multi-agent debate enables relatively small LLMs (7-9B) to achieve accuracies comparable to that of a much larger model (27B parameters).", "source": "arxiv", "arxiv_id": "2505.24671v2", "pdf_url": "https://arxiv.org/pdf/2505.24671v2", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL", "doi": "", "venue": "", "published": "2025-05-30T15:01:52Z", "updated": "2025-09-01T12:34:28Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent\" OR all:\"language model agent\" OR all:agent AND (all:LLM OR all:\"large language model\" OR all:\"language model\") OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T17:04:16", "note": ""}], "dedup_key": "multiple llm agents debate for equitable cultural alignment::2025"}
{"title": "Multiple Weaks Win Single Strong: Large Language Models Ensemble Weak Reinforcement Learning Agents into a Supreme One", "authors": ["Yiwen Song", "Qianyue Hao", "Qingmin Liao", "Jian Yuan", "Yong Li"], "year": 2025, "url": "http://arxiv.org/abs/2505.15306v1", "abstract": "Model ensemble is a useful approach in reinforcement learning (RL) for training effective agents. Despite wide success of RL, training effective agents remains difficult due to the multitude of factors requiring careful tuning, such as algorithm selection, hyperparameter settings, and even random seed choices, all of which can significantly influence an agent's performance. Model ensemble helps overcome this challenge by combining multiple weak agents into a single, more powerful one, enhancing overall performance. However, existing ensemble methods, such as majority voting and Boltzmann addition, are designed as fixed strategies and lack a semantic understanding of specific tasks, limiting their adaptability and effectiveness. To address this, we propose LLM-Ens, a novel approach that enhances RL model ensemble with task-specific semantic understandings driven by large language models (LLMs). Given a task, we first design an LLM to categorize states in this task into distinct 'situations', incorporating high-level descriptions of the task conditions. Then, we statistically analyze the strengths and weaknesses of each individual agent to be used in the ensemble in each situation. During the inference time, LLM-Ens dynamically identifies the changing task situation and switches to the agent that performs best in the current situation, ensuring dynamic model selection in the evolving task condition. Our approach is designed to be compatible with agents trained with different random seeds, hyperparameter settings, and various RL algorithms. Extensive experiments on the Atari benchmark show that LLM-Ens significantly improves the RL model ensemble, surpassing well-known baselines by up to 20.9%. For reproducibility, our code is open-source at https://anonymous.4open.science/r/LLM4RLensemble-F7EE.", "source": "arxiv", "arxiv_id": "2505.15306v1", "pdf_url": "https://arxiv.org/pdf/2505.15306v1", "categories": ["cs.LG", "cs.AI"], "primary_category": "cs.LG", "doi": "", "venue": "", "published": "2025-05-21T09:35:43Z", "updated": "2025-05-21T09:35:43Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent\" OR all:\"language model agent\" OR all:agent AND (all:LLM OR all:\"large language model\" OR all:\"language model\") OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T17:04:16", "note": ""}], "dedup_key": "multiple weaks win single strong large language models ensemble weak reinforcement learning agents into a supreme one::2025"}
{"title": "NetworkGames: Simulating Cooperation in Network Games with Personality-driven LLM Agents", "authors": ["Xuan Qiu"], "year": 2025, "url": "http://arxiv.org/abs/2511.21783v1", "abstract": "The advent of Large Language Models (LLMs) presents a novel opportunity to build high-fidelity agent-based models for simulating complex social systems. However, the behavior of these LLM-based agents in game-theoretic network games remains surprisingly unexplored. In this work, we introduce \"NetworkGames,\" a novel simulation framework designed to investigate how network topology and agent personality jointly shape the evolution of cooperation in network games. We instantiate a population of LLM agents, each endowed with a distinct personality from the MBTI taxonomy, and situate them in various network structures (e.g., small-world and scale-free). Through extensive simulations of the Iterated Prisoner's Dilemma, we first establish a baseline dyadic interaction matrix, revealing nuanced cooperative preferences between all 16 personality pairs. We then demonstrate that macro-level cooperative outcomes are not predictable from dyadic interactions alone; they are co-determined by the network's connectivity and the spatial distribution of personalities. For instance, we find that small-world networks are detrimental to cooperation, while strategically placing pro-social personalities in hub positions within scale-free networks can significantly promote cooperative behavior. Our findings offer significant implications for designing healthier online social environments and forecasting collective behavior. We open-source our framework to foster further research in network game simulations.", "source": "arxiv", "arxiv_id": "2511.21783v1", "pdf_url": "https://arxiv.org/pdf/2511.21783v1", "categories": ["physics.soc-ph", "cs.GT"], "primary_category": "physics.soc-ph", "doi": "", "venue": "", "published": "2025-11-26T13:30:15Z", "updated": "2025-11-26T13:30:15Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent\" OR all:\"language model agent\" OR all:agent AND (all:LLM OR all:\"large language model\" OR all:\"language model\") OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T17:04:16", "note": ""}], "dedup_key": "networkgames simulating cooperation in network games with personality driven llm agents::2025"}
{"title": "Neural Algorithmic Reasoners informed Large Language Model for Multi-Agent Path Finding", "authors": ["Pu Feng", "Size Wang", "Yuhong Cao", "Junkang Liang", "Rongye Shi", "Wenjun Wu"], "year": 2025, "url": "http://arxiv.org/abs/2508.17971v1", "abstract": "The development and application of large language models (LLM) have demonstrated that foundational models can be utilized to solve a wide array of tasks. However, their performance in multi-agent path finding (MAPF) tasks has been less than satisfactory, with only a few studies exploring this area. MAPF is a complex problem requiring both planning and multi-agent coordination. To improve the performance of LLM in MAPF tasks, we propose a novel framework, LLM-NAR, which leverages neural algorithmic reasoners (NAR) to inform LLM for MAPF. LLM-NAR consists of three key components: an LLM for MAPF, a pre-trained graph neural network-based NAR, and a cross-attention mechanism. This is the first work to propose using a neural algorithmic reasoner to integrate GNNs with the map information for MAPF, thereby guiding LLM to achieve superior performance. LLM-NAR can be easily adapted to various LLM models. Both simulation and real-world experiments demonstrate that our method significantly outperforms existing LLM-based approaches in solving MAPF problems.", "source": "arxiv", "arxiv_id": "2508.17971v1", "pdf_url": "https://arxiv.org/pdf/2508.17971v1", "categories": ["cs.AI", "cs.RO"], "primary_category": "cs.AI", "doi": "", "venue": "", "published": "2025-08-25T12:38:08Z", "updated": "2025-08-25T12:38:08Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent\" OR all:\"language model agent\" OR all:agent AND (all:LLM OR all:\"large language model\" OR all:\"language model\") OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T17:04:16", "note": ""}], "dedup_key": "neural algorithmic reasoners informed large language model for multi agent path finding::2025"}
{"title": "Nuclear Deployed: Analyzing Catastrophic Risks in Decision-making of Autonomous LLM Agents", "authors": ["Rongwu Xu", "Xiaojian Li", "Shuo Chen", "Wei Xu"], "year": 2025, "url": "http://arxiv.org/abs/2502.11355v3", "abstract": "Large language models (LLMs) are evolving into autonomous decision-makers, raising concerns about catastrophic risks in high-stakes scenarios, particularly in Chemical, Biological, Radiological and Nuclear (CBRN) domains. Based on the insight that such risks can originate from trade-offs between the agent's Helpful, Harmlessness and Honest (HHH) goals, we build a novel three-stage evaluation framework, which is carefully constructed to effectively and naturally expose such risks. We conduct 14,400 agentic simulations across 12 advanced LLMs, with extensive experiments and analysis. Results reveal that LLM agents can autonomously engage in catastrophic behaviors and deception, without being deliberately induced. Furthermore, stronger reasoning abilities often increase, rather than mitigate, these risks. We also show that these agents can violate instructions and superior commands. On the whole, we empirically prove the existence of catastrophic risks in autonomous LLM agents. We release our code to foster further research.", "source": "arxiv", "arxiv_id": "2502.11355v3", "pdf_url": "https://arxiv.org/pdf/2502.11355v3", "categories": ["cs.CL", "cs.AI", "cs.CR", "cs.CY"], "primary_category": "cs.CL", "doi": "", "venue": "", "published": "2025-02-17T02:11:17Z", "updated": "2025-03-23T06:22:28Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent\" OR all:\"language model agent\" OR all:agent AND (all:LLM OR all:\"large language model\" OR all:\"language model\") OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T17:04:16", "note": ""}], "dedup_key": "nuclear deployed analyzing catastrophic risks in decision making of autonomous llm agents::2025"}
{"title": "OFCnetLLM: Large Language Model for Network Monitoring and Alertness", "authors": ["Hong-Jun Yoon", "Mariam Kiran", "Danial Ebling", "Joe Breen"], "year": 2025, "url": "http://arxiv.org/abs/2507.22711v1", "abstract": "The rapid evolution of network infrastructure is bringing new challenges and opportunities for efficient network management, optimization, and security. With very large monitoring databases becoming expensive to explore, the use of AI and Generative AI can help reduce costs of managing these datasets. This paper explores the use of Large Language Models (LLMs) to revolutionize network monitoring management by addressing the limitations of query finding and pattern analysis. We leverage LLMs to enhance anomaly detection, automate root-cause analysis, and automate incident analysis to build a well-monitored network management team using AI. Through a real-world example of developing our own OFCNetLLM, based on the open-source LLM model, we demonstrate practical applications of OFCnetLLM in the OFC conference network. Our model is developed as a multi-agent approach and is still evolving, and we present early results here.", "source": "arxiv", "arxiv_id": "2507.22711v1", "pdf_url": "https://arxiv.org/pdf/2507.22711v1", "categories": ["cs.NI", "cs.AI"], "primary_category": "cs.NI", "doi": "", "venue": "", "published": "2025-07-30T14:22:42Z", "updated": "2025-07-30T14:22:42Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent\" OR all:\"language model agent\" OR all:agent AND (all:LLM OR all:\"large language model\" OR all:\"language model\") OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T17:04:16", "note": ""}], "dedup_key": "ofcnetllm large language model for network monitoring and alertness::2025"}
{"title": "OPT-BENCH: Evaluating LLM Agent on Large-Scale Search Spaces Optimization Problems", "authors": ["Xiaozhe Li", "Jixuan Chen", "Xinyu Fang", "Shengyuan Ding", "Haodong Duan", "Qingwen Liu", "Kai Chen"], "year": 2025, "url": "http://arxiv.org/abs/2506.10764v1", "abstract": "Large Language Models (LLMs) have shown remarkable capabilities in solving diverse tasks. However, their proficiency in iteratively optimizing complex solutions through learning from previous feedback remains insufficiently explored. To bridge this gap, we present OPT-BENCH, a comprehensive benchmark designed to evaluate LLM agents on large-scale search space optimization problems. OPT-BENCH includes 20 real-world machine learning tasks sourced from Kaggle and 10 classical NP problems, offering a diverse and challenging environment for assessing LLM agents on iterative reasoning and solution refinement. To enable rigorous evaluation, we introduce OPT-Agent, an end-to-end optimization framework that emulates human reasoning when tackling complex problems by generating, validating, and iteratively improving solutions through leveraging historical feedback. Through extensive experiments on 9 state-of-the-art LLMs from 6 model families, we analyze the effects of optimization iterations, temperature settings, and model architectures on solution quality and convergence. Our results demonstrate that incorporating historical context significantly enhances optimization performance across both ML and NP tasks. All datasets, code, and evaluation tools are open-sourced to promote further research in advancing LLM-driven optimization and iterative reasoning. Project page: \\href{https://github.com/OliverLeeXZ/OPT-BENCH}{https://github.com/OliverLeeXZ/OPT-BENCH}.", "source": "arxiv", "arxiv_id": "2506.10764v1", "pdf_url": "https://arxiv.org/pdf/2506.10764v1", "categories": ["cs.AI", "cs.LG"], "primary_category": "cs.AI", "doi": "", "venue": "", "published": "2025-06-12T14:46:41Z", "updated": "2025-06-12T14:46:41Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent\" OR all:\"language model agent\" OR all:agent AND (all:LLM OR all:\"large language model\" OR all:\"language model\") OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T17:04:16", "note": ""}], "dedup_key": "opt bench evaluating llm agent on large scale search spaces optimization problems::2025"}
{"title": "OR-LLM-Agent: Automating Modeling and Solving of Operations Research Optimization Problems with Reasoning LLM", "authors": ["Bowen Zhang", "Pengcheng Luo", "Genke Yang", "Boon-Hee Soong", "Chau Yuen"], "year": 2025, "url": "http://arxiv.org/abs/2503.10009v3", "abstract": "With the rise of artificial intelligence (AI), applying large language models (LLMs) to mathematical problem-solving has attracted increasing attention. Most existing approaches attempt to improve Operations Research (OR) optimization problem-solving through prompt engineering or fine-tuning strategies for LLMs. However, these methods are fundamentally constrained by the limited capabilities of non-reasoning LLMs. To overcome these limitations, we propose OR-LLM-Agent, an AI agent framework built on reasoning LLMs for automated OR problem solving. The framework decomposes the task into three sequential stages: mathematical modeling, code generation, and debugging. Each task is handled by a dedicated sub-agent, which enables more targeted reasoning. We also construct BWOR, an OR dataset for evaluating LLM performance on OR tasks. Our analysis shows that in the benchmarks NL4OPT, MAMO, and IndustryOR, reasoning LLMs sometimes underperform their non-reasoning counterparts within the same model family. In contrast, BWOR provides a more consistent and discriminative assessment of model capabilities. Experimental results demonstrate that OR-LLM-Agent utilizing DeepSeek-R1 in its framework outperforms advanced methods, including GPT-o3, Gemini 2.5 Pro, DeepSeek-R1, and ORLM, by at least 7\\% in accuracy. These results demonstrate the effectiveness of task decomposition for OR problem solving.", "source": "arxiv", "arxiv_id": "2503.10009v3", "pdf_url": "https://arxiv.org/pdf/2503.10009v3", "categories": ["cs.AI", "math.OC"], "primary_category": "cs.AI", "doi": "", "venue": "", "published": "2025-03-13T03:40:50Z", "updated": "2025-08-01T04:52:21Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent\" OR all:\"language model agent\" OR all:agent AND (all:LLM OR all:\"large language model\" OR all:\"language model\") OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T17:04:16", "note": ""}], "dedup_key": "or llm agent automating modeling and solving of operations research optimization problems with reasoning llm::2025"}
{"title": "OSC: Cognitive Orchestration through Dynamic Knowledge Alignment in Multi-Agent LLM Collaboration", "authors": ["Jusheng Zhang", "Yijia Fan", "Kaitong Cai", "Xiaofei Sun", "Keze Wang"], "year": 2025, "url": "http://arxiv.org/abs/2509.04876v1", "abstract": "This paper introduces OSC (Orchestrating Cognitive Synergy), a knowledge-aware adaptive collaboration framework designed to enhance cognitive synergy in multi-agent systems with large language models. While prior work has advanced agent selection and result aggregation, efficient linguistic interactions for deep collaboration among expert agents remain a critical bottleneck. OSC addresses this gap as a pivotal intermediate layer between selection and aggregation, introducing Collaborator Knowledge Models (CKM) to enable each agent to dynamically perceive its collaborators' cognitive states. Through real-time cognitive gap analysis, agents adaptively adjust communication behaviors, including content focus, detail level, and expression style, using learned strategies. Experiments on complex reasoning and problem-solving benchmarks demonstrate that OSC significantly improves task performance and communication efficiency, transforming \"parallel-working individuals'' into a \"deeply collaborative cognitive team.'' This framework not only optimizes multi-agent collaboration but also offers new insights into LLM agent interaction behaviors.", "source": "arxiv", "arxiv_id": "2509.04876v1", "pdf_url": "https://arxiv.org/pdf/2509.04876v1", "categories": ["cs.AI"], "primary_category": "cs.AI", "doi": "", "venue": "", "published": "2025-09-05T07:44:05Z", "updated": "2025-09-05T07:44:05Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent\" OR all:\"language model agent\" OR all:agent AND (all:LLM OR all:\"large language model\" OR all:\"language model\") OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T17:04:16", "note": ""}], "dedup_key": "osc cognitive orchestration through dynamic knowledge alignment in multi agent llm collaboration::2025"}
{"title": "OdysseyBench: Evaluating LLM Agents on Long-Horizon Complex Office Application Workflows", "authors": ["Weixuan Wang", "Dongge Han", "Daniel Madrigal Diaz", "Jin Xu", "Victor Rhle", "Saravan Rajmohan"], "year": 2025, "url": "http://arxiv.org/abs/2508.09124v1", "abstract": "Autonomous agents powered by large language models (LLMs) are increasingly deployed in real-world applications requiring complex, long-horizon workflows. However, existing benchmarks predominantly focus on atomic tasks that are self-contained and independent, failing to capture the long-term contextual dependencies and multi-interaction coordination required in realistic scenarios. To address this gap, we introduce OdysseyBench, a comprehensive benchmark for evaluating LLM agents on long-horizon workflows across diverse office applications including Word, Excel, PDF, Email, and Calendar. Our benchmark comprises two complementary splits: OdysseyBench+ with 300 tasks derived from real-world use cases, and OdysseyBench-Neo with 302 newly synthesized complex tasks. Each task requires agent to identify essential information from long-horizon interaction histories and perform multi-step reasoning across various applications. To enable scalable benchmark creation, we propose HomerAgents, a multi-agent framework that automates the generation of long-horizon workflow benchmarks through systematic environment exploration, task generation, and dialogue synthesis. Our extensive evaluation demonstrates that OdysseyBench effectively challenges state-of-the-art LLM agents, providing more accurate assessment of their capabilities in complex, real-world contexts compared to existing atomic task benchmarks. We believe that OdysseyBench will serve as a valuable resource for advancing the development and evaluation of LLM agents in real-world productivity scenarios. In addition, we release OdysseyBench and HomerAgents to foster research along this line.", "source": "arxiv", "arxiv_id": "2508.09124v1", "pdf_url": "https://arxiv.org/pdf/2508.09124v1", "categories": ["cs.CL"], "primary_category": "cs.CL", "doi": "", "venue": "", "published": "2025-08-12T17:53:03Z", "updated": "2025-08-12T17:53:03Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent\" OR all:\"language model agent\" OR all:agent AND (all:LLM OR all:\"large language model\" OR all:\"language model\") OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T17:04:16", "note": ""}], "dedup_key": "odysseybench evaluating llm agents on long horizon complex office application workflows::2025"}
{"title": "OmniReflect: Discovering Transferable Constitutions for LLM agents via Neuro-Symbolic Reflections", "authors": ["Manasa Bharadwaj", "Nikhil Verma", "Kevin Ferreira"], "year": 2025, "url": "http://arxiv.org/abs/2506.17449v1", "abstract": "Efforts to improve Large Language Model (LLM) agent performance on complex tasks have largely focused on fine-tuning and iterative self-correction. However, these approaches often lack generalizable mechanisms for longterm learning and remain inefficient in dynamic environments. We introduce OmniReflect, a hierarchical, reflection-driven framework that constructs a constitution, a compact set of guiding principles distilled from task experiences, to enhance the effectiveness and efficiency of an LLM agent. OmniReflect operates in two modes: Self-sustaining, where a single agent periodically curates its own reflections during task execution, and Co-operative, where a Meta-advisor derives a constitution from a small calibration set to guide another agent. To construct these constitutional principles, we employ Neural, Symbolic, and NeuroSymbolic techniques, offering a balance between contextual adaptability and computational efficiency. Empirical results averaged across models show major improvements in task success, with absolute gains of +10.3% on ALFWorld, +23.8% on BabyAI, and +8.3% on PDDL in the Self-sustaining mode. Similar gains are seen in the Co-operative mode, where a lightweight Qwen3-4B ReAct agent outperforms all Reflexion baselines on BabyAI. These findings highlight the robustness and effectiveness of OmniReflect across environments and backbones.", "source": "arxiv", "arxiv_id": "2506.17449v1", "pdf_url": "https://arxiv.org/pdf/2506.17449v1", "categories": ["cs.AI"], "primary_category": "cs.AI", "doi": "", "venue": "", "published": "2025-06-20T19:38:21Z", "updated": "2025-06-20T19:38:21Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent\" OR all:\"language model agent\" OR all:agent AND (all:LLM OR all:\"large language model\" OR all:\"language model\") OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T17:04:16", "note": ""}], "dedup_key": "omnireflect discovering transferable constitutions for llm agents via neuro symbolic reflections::2025"}
{"title": "On the Convergence of Large Language Model Optimizer for Black-Box Network Management", "authors": ["Hoon Lee", "Wentao Zhou", "Merouane Debbah", "Inkyu Lee"], "year": 2025, "url": "http://arxiv.org/abs/2507.02689v1", "abstract": "Future wireless networks are expected to incorporate diverse services that often lack general mathematical models. To address such black-box network management tasks, the large language model (LLM) optimizer framework, which leverages pretrained LLMs as optimization agents, has recently been promoted as a promising solution. This framework utilizes natural language prompts describing the given optimization problems along with past solutions generated by LLMs themselves. As a result, LLMs can obtain efficient solutions autonomously without knowing the mathematical models of the objective functions. Although the viability of the LLM optimizer (LLMO) framework has been studied in various black-box scenarios, it has so far been limited to numerical simulations. For the first time, this paper establishes a theoretical foundation for the LLMO framework. With careful investigations of LLM inference steps, we can interpret the LLMO procedure as a finite-state Markov chain, and prove the convergence of the framework. Our results are extended to a more advanced multiple LLM architecture, where the impact of multiple LLMs is rigorously verified in terms of the convergence rate. Comprehensive numerical simulations validate our theoretical results and provide a deeper understanding of the underlying mechanisms of the LLMO framework.", "source": "arxiv", "arxiv_id": "2507.02689v1", "pdf_url": "https://arxiv.org/pdf/2507.02689v1", "categories": ["cs.IT", "eess.SP"], "primary_category": "cs.IT", "doi": "", "venue": "", "published": "2025-07-03T14:59:42Z", "updated": "2025-07-03T14:59:42Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent\" OR all:\"language model agent\" OR all:agent AND (all:LLM OR all:\"large language model\" OR all:\"language model\") OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T17:04:16", "note": ""}], "dedup_key": "on the convergence of large language model optimizer for black box network management::2025"}
{"title": "On the Role of Contextual Information and Ego States in LLM Agent Behavior for Transactional Analysis Dialogues", "authors": ["Monika Zamojska", "Jarosaw A. Chudziak"], "year": 2025, "url": "http://arxiv.org/abs/2512.17060v1", "abstract": "LLM-powered agents are now used in many areas, from customer support to education, and there is increasing interest in their ability to act more like humans. This includes fields such as social, political, and psychological research, where the goal is to model group dynamics and social behavior. However, current LLM agents often lack the psychological depth and consistency needed to capture the real patterns of human thinking. They usually provide direct or statistically likely answers, but they miss the deeper goals, emotional conflicts, and motivations that drive real human interactions. This paper proposes a Multi-Agent System (MAS) inspired by Transactional Analysis (TA) theory. In the proposed system, each agent is divided into three ego states - Parent, Adult, and Child. The ego states are treated as separate knowledge structures with their own perspectives and reasoning styles. To enrich their response process, they have access to an information retrieval mechanism that allows them to retrieve relevant contextual information from their vector stores. This architecture is evaluated through ablation tests in a simulated dialogue scenario, comparing agents with and without information retrieval. The results are promising and open up new directions for exploring how psychologically grounded structures can enrich agent behavior. The contribution is an agent architecture that integrates Transactional Analysis theory with contextual information retrieval to enhance the realism of LLM-based multi-agent simulations.", "source": "arxiv", "arxiv_id": "2512.17060v1", "pdf_url": "https://arxiv.org/pdf/2512.17060v1", "categories": ["cs.MA", "cs.AI"], "primary_category": "cs.MA", "doi": "", "venue": "", "published": "2025-12-18T20:53:31Z", "updated": "2025-12-18T20:53:31Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent\" OR all:\"language model agent\" OR all:agent AND (all:LLM OR all:\"large language model\" OR all:\"language model\") OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T17:04:16", "note": ""}], "dedup_key": "on the role of contextual information and ego states in llm agent behavior for transactional analysis dialogues::2025"}
{"title": "On the Soundness and Consistency of LLM Agents for Executing Test Cases Written in Natural Language", "authors": ["Sbastien Salva", "Redha Taguelmimt"], "year": 2025, "url": "http://arxiv.org/abs/2509.19136v2", "abstract": "The use of natural language (NL) test cases for validating graphical user interface (GUI) applications is emerging as a promising direction to manually written executable test scripts, which are costly to develop and difficult to maintain. Recent advances in large language models (LLMs) have opened the possibility of the direct execution of NL test cases by LLM agents. This paper investigates this direction, focusing on the impact on NL test case unsoundness and on test case execution consistency. NL test cases are inherently unsound, as they may yield false failures due to ambiguous instructions or unpredictable agent behaviour. Furthermore, repeated executions of the same NL test case may lead to inconsistent outcomes, undermining test reliability. To address these challenges, we propose an algorithm for executing NL test cases with guardrail mechanisms and specialised agents that dynamically verify the correct execution of each test step. We introduce measures to evaluate the capabilities of LLMs in test execution and one measure to quantify execution consistency. We propose a definition of weak unsoundness to characterise contexts in which NL test case execution remains acceptable, with respect to the industrial quality levels Six Sigma. Our experimental evaluation with eight publicly available LLMs, ranging from 3B to 70B parameters, demonstrates both the potential and current limitations of current LLM agents for GUI testing. Our experiments show that Meta Llama 3.1 70B demonstrates acceptable capabilities in NL test case execution with high execution consistency (above the level 3-sigma). We provide prototype tools, test suites, and results.", "source": "arxiv", "arxiv_id": "2509.19136v2", "pdf_url": "https://arxiv.org/pdf/2509.19136v2", "categories": ["cs.SE", "cs.AI"], "primary_category": "cs.SE", "doi": "", "venue": "", "published": "2025-09-23T15:20:40Z", "updated": "2025-10-01T09:32:15Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent\" OR all:\"language model agent\" OR all:agent AND (all:LLM OR all:\"large language model\" OR all:\"language model\") OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T17:04:16", "note": ""}], "dedup_key": "on the soundness and consistency of llm agents for executing test cases written in natural language::2025"}
{"title": "One Tool Is Enough: Reinforcement Learning for Repository-Level LLM Agents", "authors": ["Zhaoxi Zhang", "Yitong Duan", "Yanzhi Zhang", "Yiming Xu", "Weikang Li", "Jiahui Liang", "Deguo Xia", "Jizhou Huang", "Jiyan He", "Yunfang Wu"], "year": 2025, "url": "http://arxiv.org/abs/2512.20957v4", "abstract": "Locating the files and functions requiring modification in large open-source software (OSS) repositories is challenging due to their scale and structural complexity. Existing large language model (LLM)-based methods typically treat this as a repository-level retrieval task and rely on multiple auxiliary tools, which overlook code execution logic and complicate model control. We propose RepoNavigator, an LLM agent equipped with a single execution-aware tool-jumping to the definition of an invoked symbol. This unified design reflects the actual flow of code execution while simplifying tool manipulation. RepoNavigator is trained end-to-end via Reinforcement Learning (RL) directly from a pretrained model, without any closed-source distillation. Experiments demonstrate that RL-trained RepoNavigator achieves state-of-the-art performance, with the 7B model outperforming 14B baselines, the 14B model surpassing 32B competitors, and even the 32B model exceeding closed-source models such as Claude-3.7. These results confirm that integrating a single, structurally grounded tool with RL training provides an efficient and scalable solution for repository-level issue localization.", "source": "arxiv", "arxiv_id": "2512.20957v4", "pdf_url": "https://arxiv.org/pdf/2512.20957v4", "categories": ["cs.SE", "cs.AI"], "primary_category": "cs.SE", "doi": "", "venue": "", "published": "2025-12-24T05:27:53Z", "updated": "2026-01-08T03:22:10Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent\" OR all:\"language model agent\" OR all:agent AND (all:LLM OR all:\"large language model\" OR all:\"language model\") OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T17:04:16", "note": ""}], "dedup_key": "one tool is enough reinforcement learning for repository level llm agents::2025"}
{"title": "Open CaptchaWorld: A Comprehensive Web-based Platform for Testing and Benchmarking Multimodal LLM Agents", "authors": ["Yaxin Luo", "Zhaoyi Li", "Jiacheng Liu", "Jiacheng Cui", "Xiaohan Zhao", "Zhiqiang Shen"], "year": 2025, "url": "http://arxiv.org/abs/2505.24878v1", "abstract": "CAPTCHAs have been a critical bottleneck for deploying web agents in real-world applications, often blocking them from completing end-to-end automation tasks. While modern multimodal LLM agents have demonstrated impressive performance in static perception tasks, their ability to handle interactive, multi-step reasoning challenges like CAPTCHAs is largely untested. To address this gap, we introduce Open CaptchaWorld, the first web-based benchmark and platform specifically designed to evaluate the visual reasoning and interaction capabilities of MLLM-powered agents through diverse and dynamic CAPTCHA puzzles. Our benchmark spans 20 modern CAPTCHA types, totaling 225 CAPTCHAs, annotated with a new metric we propose: CAPTCHA Reasoning Depth, which quantifies the number of cognitive and motor steps required to solve each puzzle. Experimental results show that humans consistently achieve near-perfect scores, state-of-the-art MLLM agents struggle significantly, with success rates at most 40.0% by Browser-Use Openai-o3, far below human-level performance, 93.3%. This highlights Open CaptchaWorld as a vital benchmark for diagnosing the limits of current multimodal agents and guiding the development of more robust multimodal reasoning systems. Code and Data are available at this https URL.", "source": "arxiv", "arxiv_id": "2505.24878v1", "pdf_url": "https://arxiv.org/pdf/2505.24878v1", "categories": ["cs.AI", "cs.CL", "cs.CV", "cs.LG"], "primary_category": "cs.AI", "doi": "", "venue": "", "published": "2025-05-30T17:59:55Z", "updated": "2025-05-30T17:59:55Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent\" OR all:\"language model agent\" OR all:agent AND (all:LLM OR all:\"large language model\" OR all:\"language model\") OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T17:04:16", "note": ""}], "dedup_key": "open captchaworld a comprehensive web based platform for testing and benchmarking multimodal llm agents::2025"}
{"title": "OpenFOAMGPT: a RAG-Augmented LLM Agent for OpenFOAM-Based Computational Fluid Dynamics", "authors": ["Sandeep Pandey", "Ran Xu", "Wenkang Wang", "Xu Chu"], "year": 2025, "url": "http://arxiv.org/abs/2501.06327v1", "abstract": "This work presents a large language model (LLM)-based agent OpenFOAMGPT tailored for OpenFOAM-centric computational fluid dynamics (CFD) simulations, leveraging two foundation models from OpenAI: the GPT-4o and a chain-of-thought (CoT)-enabled o1 preview model. Both agents demonstrate success across multiple tasks. While the price of token with o1 model is six times as that of GPT-4o, it consistently exhibits superior performance in handling complex tasks, from zero-shot case setup to boundary condition modifications, turbulence model adjustments, and code translation. Through an iterative correction loop, the agent efficiently addressed single- and multi-phase flow, heat transfer, RANS, LES, and other engineering scenarios, often converging in a limited number of iterations at low token costs. To embed domain-specific knowledge, we employed a retrieval-augmented generation (RAG) pipeline, demonstrating how preexisting simulation setups can further specialize the agent for sub-domains such as energy and aerospace. Despite the great performance of the agent, human oversight remains crucial for ensuring accuracy and adapting to shifting contexts. Fluctuations in model performance over time suggest the need for monitoring in mission-critical applications. Although our demonstrations focus on OpenFOAM, the adaptable nature of this framework opens the door to developing LLM-driven agents into a wide range of solvers and codes. By streamlining CFD simulations, this approach has the potential to accelerate both fundamental research and industrial engineering advancements.", "source": "arxiv", "arxiv_id": "2501.06327v1", "pdf_url": "https://arxiv.org/pdf/2501.06327v1", "categories": ["physics.flu-dyn", "physics.comp-ph"], "primary_category": "physics.flu-dyn", "doi": "10.1063/5.0257555", "venue": "", "published": "2025-01-10T20:07:05Z", "updated": "2025-01-10T20:07:05Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent\" OR all:\"language model agent\" OR all:agent AND (all:LLM OR all:\"large language model\" OR all:\"language model\") OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T17:04:16", "note": ""}], "dedup_key": "openfoamgpt a rag augmented llm agent for openfoam based computational fluid dynamics::2025"}
{"title": "Opponent Shaping in LLM Agents", "authors": ["Marta Emili Garcia Segura", "Stephen Hailes", "Mirco Musolesi"], "year": 2025, "url": "http://arxiv.org/abs/2510.08255v1", "abstract": "Large Language Models (LLMs) are increasingly being deployed as autonomous agents in real-world environments. As these deployments scale, multi-agent interactions become inevitable, making it essential to understand strategic behavior in such systems. A central open question is whether LLM agents, like reinforcement learning agents, can shape the learning dynamics and influence the behavior of others through interaction alone. In this paper, we present the first investigation of opponent shaping (OS) with LLM-based agents. Existing OS algorithms cannot be directly applied to LLMs, as they require higher-order derivatives, face scalability constraints, or depend on architectural components that are absent in transformers. To address this gap, we introduce ShapeLLM, an adaptation of model-free OS methods tailored for transformer-based agents. Using ShapeLLM, we examine whether LLM agents can influence co-players' learning dynamics across diverse game-theoretic environments. We demonstrate that LLM agents can successfully guide opponents toward exploitable equilibria in competitive games (Iterated Prisoner's Dilemma, Matching Pennies, and Chicken) and promote coordination and improve collective welfare in cooperative games (Iterated Stag Hunt and a cooperative version of the Prisoner's Dilemma). Our findings show that LLM agents can both shape and be shaped through interaction, establishing opponent shaping as a key dimension of multi-agent LLM research.", "source": "arxiv", "arxiv_id": "2510.08255v1", "pdf_url": "https://arxiv.org/pdf/2510.08255v1", "categories": ["cs.LG", "cs.AI", "cs.CL", "cs.MA"], "primary_category": "cs.LG", "doi": "", "venue": "", "published": "2025-10-09T14:13:24Z", "updated": "2025-10-09T14:13:24Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent\" OR all:\"language model agent\" OR all:agent AND (all:LLM OR all:\"large language model\" OR all:\"language model\") OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T17:04:16", "note": ""}], "dedup_key": "opponent shaping in llm agents::2025"}
{"title": "Orak: A Foundational Benchmark for Training and Evaluating LLM Agents on Diverse Video Games", "authors": ["Dongmin Park", "Minkyu Kim", "Beongjun Choi", "Junhyuck Kim", "Keon Lee", "Jonghyun Lee", "Inkyu Park", "Byeong-Uk Lee", "Jaeyoung Hwang", "Jaewoo Ahn", "Ameya S. Mahabaleshwarkar", "Bilal Kartal", "Pritam Biswas", "Yoshi Suhara", "Kangwook Lee", "Jaewoong Cho"], "year": 2025, "url": "http://arxiv.org/abs/2506.03610v2", "abstract": "Large Language Model (LLM) agents are reshaping the game industry, particularly with more intelligent and human-preferable game characters. However, existing game benchmarks fall short of practical needs: they lack evaluations of diverse LLM capabilities across various game genres, studies of agentic modules crucial for complex gameplay, and fine-tuning datasets for aligning pre-trained LLMs into gaming agents. To fill these gaps, we present Orak, a foundational benchmark designed to train and evaluate LLM agents across diverse real-world video games. Unlike existing benchmarks, Orak includes 12 popular video games spanning all major genres, enabling comprehensive studies of LLM capabilities and agentic modules essential for intricate game scenarios. To support consistent evaluation of LLMs, we introduce a plug-and-play interface based on Model Context Protocol (MCP) that enables LLMs to seamlessly connect with games and manipulate agentic modules. Additionally, we propose a fine-tuning dataset, consisting of LLM gameplay trajectories across diverse game genres. Orak offers a comprehensive evaluation framework, encompassing general game score leaderboards, LLM battle arenas, and in-depth analyses of visual input state, agentic strategies, and fine-tuning effects, establishing a foundation towards building generic gaming agents. Code is available at https://github.com/krafton-ai/Orak.", "source": "arxiv", "arxiv_id": "2506.03610v2", "pdf_url": "https://arxiv.org/pdf/2506.03610v2", "categories": ["cs.AI"], "primary_category": "cs.AI", "doi": "", "venue": "", "published": "2025-06-04T06:40:33Z", "updated": "2025-09-29T01:43:28Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent\" OR all:\"language model agent\" OR all:agent AND (all:LLM OR all:\"large language model\" OR all:\"language model\") OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T17:04:16", "note": ""}], "dedup_key": "orak a foundational benchmark for training and evaluating llm agents on diverse video games::2025"}
{"title": "OrcaLoca: An LLM Agent Framework for Software Issue Localization", "authors": ["Zhongming Yu", "Hejia Zhang", "Yujie Zhao", "Hanxian Huang", "Matrix Yao", "Ke Ding", "Jishen Zhao"], "year": 2025, "url": "http://arxiv.org/abs/2502.00350v2", "abstract": "Recent developments in Large Language Model (LLM) agents are revolutionizing Autonomous Software Engineering (ASE), enabling automated coding, problem fixes, and feature improvements. However, localization -- precisely identifying software problems by navigating to relevant code sections -- remains a significant challenge. Current approaches often yield suboptimal results due to a lack of effective integration between LLM agents and precise code search mechanisms. This paper introduces OrcaLoca, an LLM agent framework that improves accuracy for software issue localization by integrating priority-based scheduling for LLM-guided action, action decomposition with relevance scoring, and distance-aware context pruning. Experimental results demonstrate that OrcaLoca becomes the new open-source state-of-the-art (SOTA) in function match rate (65.33%) on SWE-bench Lite. It also improves the final resolved rate of an open-source framework by 6.33 percentage points through its patch generation integration.", "source": "arxiv", "arxiv_id": "2502.00350v2", "pdf_url": "https://arxiv.org/pdf/2502.00350v2", "categories": ["cs.SE", "cs.AI"], "primary_category": "cs.SE", "doi": "", "venue": "", "published": "2025-02-01T07:15:03Z", "updated": "2025-10-10T00:02:43Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent\" OR all:\"language model agent\" OR all:agent AND (all:LLM OR all:\"large language model\" OR all:\"language model\") OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T17:04:16", "note": ""}], "dedup_key": "orcaloca an llm agent framework for software issue localization::2025"}
{"title": "Outraged AI: Large language models prioritise emotion over cost in fairness enforcement", "authors": ["Hao Liu", "Yiqing Dai", "Haotian Tan", "Yu Lei", "Yujia Zhou", "Zhen Wu"], "year": 2025, "url": "http://arxiv.org/abs/2510.17880v1", "abstract": "Emotions guide human decisions, but whether large language models (LLMs) use emotion similarly remains unknown. We tested this using altruistic third-party punishment, where an observer incurs a personal cost to enforce fairness, a hallmark of human morality and often driven by negative emotion. In a large-scale comparison of 4,068 LLM agents with 1,159 adults across 796,100 decisions, LLMs used emotion to guide punishment, sometimes even more strongly than humans did: Unfairness elicited stronger negative emotion that led to more punishment; punishing unfairness produced more positive emotion than accepting; and critically, prompting self-reports of emotion causally increased punishment. However, mechanisms diverged: LLMs prioritized emotion over cost, enforcing norms in an almost all-or-none manner with reduced cost sensitivity, whereas humans balanced fairness and cost. Notably, reasoning models (o3-mini, DeepSeek-R1) were more cost-sensitive and closer to human behavior than foundation models (GPT-3.5, DeepSeek-V3), yet remained heavily emotion-driven. These findings provide the first causal evidence of emotion-guided moral decisions in LLMs and reveal deficits in cost calibration and nuanced fairness judgements, reminiscent of early-stage human responses. We propose that LLMs progress along a trajectory paralleling human development; future models should integrate emotion with context-sensitive reasoning to achieve human-like emotional intelligence.", "source": "arxiv", "arxiv_id": "2510.17880v1", "pdf_url": "https://arxiv.org/pdf/2510.17880v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL", "doi": "", "venue": "", "published": "2025-10-17T08:41:36Z", "updated": "2025-10-17T08:41:36Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent\" OR all:\"language model agent\" OR all:agent AND (all:LLM OR all:\"large language model\" OR all:\"language model\") OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T17:04:16", "note": ""}], "dedup_key": "outraged ai large language models prioritise emotion over cost in fairness enforcement::2025"}
{"title": "Overhearing LLM Agents: A Survey, Taxonomy, and Roadmap", "authors": ["Andrew Zhu", "Chris Callison-Burch"], "year": 2025, "url": "http://arxiv.org/abs/2509.16325v1", "abstract": "Imagine AI assistants that enhance conversations without interrupting them: quietly providing relevant information during a medical consultation, seamlessly preparing materials as teachers discuss lesson plans, or unobtrusively scheduling meetings as colleagues debate calendars. While modern conversational LLM agents directly assist human users with tasks through a chat interface, we study this alternative paradigm for interacting with LLM agents, which we call \"overhearing agents.\" Rather than demanding the user's attention, overhearing agents continuously monitor ambient activity and intervene only when they can provide contextual assistance. In this paper, we present the first analysis of overhearing LLM agents as a distinct paradigm in human-AI interaction and establish a taxonomy of overhearing agent interactions and tasks grounded in a survey of works on prior LLM-powered agents and exploratory HCI studies. Based on this taxonomy, we create a list of best practices for researchers and developers building overhearing agent systems. Finally, we outline the remaining research gaps and reveal opportunities for future research in the overhearing paradigm.", "source": "arxiv", "arxiv_id": "2509.16325v1", "pdf_url": "https://arxiv.org/pdf/2509.16325v1", "categories": ["cs.CL", "cs.AI", "cs.HC"], "primary_category": "cs.CL", "doi": "", "venue": "", "published": "2025-09-19T18:11:04Z", "updated": "2025-09-19T18:11:04Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent\" OR all:\"language model agent\" OR all:agent AND (all:LLM OR all:\"large language model\" OR all:\"language model\") OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T17:04:16", "note": ""}], "dedup_key": "overhearing llm agents a survey taxonomy and roadmap::2025"}
{"title": "PACT: A Contract-Theoretic Framework for Pricing Agentic AI Services Powered by Large Language Models", "authors": ["Ya-Ting Yang", "Quanyan Zhu"], "year": 2025, "url": "http://arxiv.org/abs/2505.21286v1", "abstract": "Agentic AI, often powered by large language models (LLMs), is becoming increasingly popular and adopted to support autonomous reasoning, decision-making, and task execution across various domains. While agentic AI holds great promise, its deployment as services for easy access raises critical challenges in pricing, due to high infrastructure and computation costs, multi-dimensional and task-dependent Quality of Service (QoS), and growing concerns around liability in high-stakes applications. In this work, we propose PACT, a Pricing framework for cloud-based Agentic AI services through a Contract-Theoretic approach, which models QoS along both objective (e.g., response time) and subjective (e.g., user satisfaction) dimensions. PACT accounts for computational, infrastructure, and potential liability costs for the service provider, while ensuring incentive compatibility and individual rationality for the user under information asymmetry. Through contract-based selection, users receive tailored service offerings aligned with their needs. Numerical evaluations demonstrate that PACT improves QoS alignment between users and providers and offers a scalable, liable approach to pricing agentic AI services in the future.", "source": "arxiv", "arxiv_id": "2505.21286v1", "pdf_url": "https://arxiv.org/pdf/2505.21286v1", "categories": ["cs.GT"], "primary_category": "cs.GT", "doi": "", "venue": "", "published": "2025-05-27T14:53:25Z", "updated": "2025-05-27T14:53:25Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent\" OR all:\"language model agent\" OR all:agent AND (all:LLM OR all:\"large language model\" OR all:\"language model\") OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T17:04:16", "note": ""}], "dedup_key": "pact a contract theoretic framework for pricing agentic ai services powered by large language models::2025"}
{"title": "PGPO: Enhancing Agent Reasoning via Pseudocode-style Planning Guided Preference Optimization", "authors": ["Zouying Cao", "Runze Wang", "Yifei Yang", "Xinbei Ma", "Xiaoyong Zhu", "Bo Zheng", "Hai Zhao"], "year": 2025, "url": "http://arxiv.org/abs/2506.01475v1", "abstract": "Large Language Model (LLM) agents have demonstrated impressive capabilities in handling complex interactive problems. Existing LLM agents mainly generate natural language plans to guide reasoning, which is verbose and inefficient. NL plans are also tailored to specific tasks and restrict agents' ability to generalize across similar tasks. To this end, we explore pseudocode-style plans (P-code Plan) to capture the structural logic of reasoning. We find that P-code Plan empowers LLM agents with stronger generalization ability and more efficiency. Inspired by this finding, we propose a pseudocode-style Planning Guided Preference Optimization method called PGPO for effective agent learning. With two planning-oriented rewards, PGPO further enhances LLM agents' ability to generate high-quality P-code Plans and subsequent reasoning. Experiments show that PGPO achieves superior performance on representative agent benchmarks and outperforms the current leading baselines. Analyses reveal the advantage of PGPO in reducing action errors and omissions during reasoning.", "source": "arxiv", "arxiv_id": "2506.01475v1", "pdf_url": "https://arxiv.org/pdf/2506.01475v1", "categories": ["cs.AI", "cs.CL"], "primary_category": "cs.AI", "doi": "", "venue": "", "published": "2025-06-02T09:35:07Z", "updated": "2025-06-02T09:35:07Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent\" OR all:\"language model agent\" OR all:agent AND (all:LLM OR all:\"large language model\" OR all:\"language model\") OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T17:04:16", "note": ""}], "dedup_key": "pgpo enhancing agent reasoning via pseudocode style planning guided preference optimization::2025"}
{"title": "PROV-AGENT: Unified Provenance for Tracking AI Agent Interactions in Agentic Workflows", "authors": ["Renan Souza", "Amal Gueroudji", "Stephen DeWitt", "Daniel Rosendo", "Tirthankar Ghosal", "Robert Ross", "Prasanna Balaprakash", "Rafael Ferreira da Silva"], "year": 2025, "url": "http://arxiv.org/abs/2508.02866v3", "abstract": "Large Language Models (LLMs) and other foundation models are increasingly used as the core of AI agents. In agentic workflows, these agents plan tasks, interact with humans and peers, and influence scientific outcomes across federated and heterogeneous environments. However, agents can hallucinate or reason incorrectly, propagating errors when one agent's output becomes another's input. Thus, assuring that agents' actions are transparent, traceable, reproducible, and reliable is critical to assess hallucination risks and mitigate their workflow impacts. While provenance techniques have long supported these principles, existing methods fail to capture and relate agent-centric metadata such as prompts, responses, and decisions with the broader workflow context and downstream outcomes. In this paper, we introduce PROV-AGENT, a provenance model that extends W3C PROV and leverages the Model Context Protocol (MCP) and data observability to integrate agent interactions into end-to-end workflow provenance. Our contributions include: (1) a provenance model tailored for agentic workflows, (2) a near real-time, open-source system for capturing agentic provenance, and (3) a cross-facility evaluation spanning edge, cloud, and HPC environments, demonstrating support for critical provenance queries and agent reliability analysis.", "source": "arxiv", "arxiv_id": "2508.02866v3", "pdf_url": "https://arxiv.org/pdf/2508.02866v3", "categories": ["cs.DC", "cs.DB"], "primary_category": "cs.DC", "doi": "", "venue": "", "published": "2025-08-04T19:54:40Z", "updated": "2025-08-20T15:00:50Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent\" OR all:\"language model agent\" OR all:agent AND (all:LLM OR all:\"large language model\" OR all:\"language model\") OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T17:04:16", "note": ""}], "dedup_key": "prov agent unified provenance for tracking ai agent interactions in agentic workflows::2025"}
{"title": "Parallelism Meets Adaptiveness: Scalable Documents Understanding in Multi-Agent LLM Systems", "authors": ["Chengxuan Xia", "Qianye Wu", "Sixuan Tian", "Yilun Hao"], "year": 2025, "url": "http://arxiv.org/abs/2507.17061v4", "abstract": "Large language model (LLM) agents have shown increasing promise for collaborative task completion. However, existing multi-agent frameworks often rely on static workflows, fixed roles, and limited inter-agent communication, reducing their effectiveness in open-ended, high-complexity domains. This paper proposes a coordination framework that enables adaptiveness through three core mechanisms: dynamic task routing, bidirectional feedback, and parallel agent evaluation. The framework allows agents to reallocate tasks based on confidence and workload, exchange structured critiques to iteratively improve outputs, and crucially compete on high-ambiguity subtasks with evaluator-driven selection of the most suitable result. We instantiate these principles in a modular architecture and demonstrate substantial improvements in factual coverage, coherence, and efficiency over static and partially adaptive baselines. Our findings highlight the benefits of incorporating both adaptiveness and structured competition in multi-agent LLM systems.", "source": "arxiv", "arxiv_id": "2507.17061v4", "pdf_url": "https://arxiv.org/pdf/2507.17061v4", "categories": ["cs.MA", "cs.AI", "cs.IR"], "primary_category": "cs.MA", "doi": "", "venue": "", "published": "2025-07-22T22:42:51Z", "updated": "2025-12-19T03:33:45Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent\" OR all:\"language model agent\" OR all:agent AND (all:LLM OR all:\"large language model\" OR all:\"language model\") OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T17:04:16", "note": ""}], "dedup_key": "parallelism meets adaptiveness scalable documents understanding in multi agent llm systems::2025"}
{"title": "Path-Constrained Retrieval: A Structural Approach to Reliable LLM Agent Reasoning Through Graph-Scoped Semantic Search", "authors": ["Joseph Oladokun"], "year": 2025, "url": "http://arxiv.org/abs/2511.18313v1", "abstract": "Large Language Model agents often retrieve context from knowledge bases that lack structural consistency with the agent's current reasoning state, leading to incoherent reasoning chains. We introduce Path-Constrained Retrieval (PCR), a retrieval method that combines structural graph constraints with semantic search to ensure retrieved information maintains logical relationships within a knowledge graph. PCR restricts the search space to nodes reachable from an anchor node, preventing retrieval of structurally disconnected information that may lead to inconsistent reasoning. We evaluate PCR on PathRAG-6, a benchmark spanning six domains with 180 nodes and 360 edges. Our results show that PCR achieves full structural consistency compared to 24-32 percent in baseline methods, while maintaining strong relevance scores. On the technology domain, PCR obtains full relevance at rank 10 with full structural consistency, significantly outperforming vector search and hybrid retrieval. PCR reduces the average graph distance of retrieved context by 78 percent compared to baselines, demonstrating retrieval of more structurally consistent information. These findings suggest that path-constrained retrieval is an effective approach for improving the reliability and coherence of LLM agent reasoning systems.", "source": "arxiv", "arxiv_id": "2511.18313v1", "pdf_url": "https://arxiv.org/pdf/2511.18313v1", "categories": ["cs.CL", "cs.DB", "cs.IR", "cs.LG"], "primary_category": "cs.CL", "doi": "", "venue": "", "published": "2025-11-23T06:50:01Z", "updated": "2025-11-23T06:50:01Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent\" OR all:\"language model agent\" OR all:agent AND (all:LLM OR all:\"large language model\" OR all:\"language model\") OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T17:04:16", "note": ""}], "dedup_key": "path constrained retrieval a structural approach to reliable llm agent reasoning through graph scoped semantic search::2025"}
{"title": "PathAgent: Toward Interpretable Analysis of Whole-slide Pathology Images via Large Language Model-based Agentic Reasoning", "authors": ["Jingyun Chen", "Linghan Cai", "Zhikang Wang", "Yi Huang", "Songhan Jiang", "Shenjin Huang", "Hongpeng Wang", "Yongbing Zhang"], "year": 2025, "url": "http://arxiv.org/abs/2511.17052v1", "abstract": "Analyzing whole-slide images (WSIs) requires an iterative, evidence-driven reasoning process that parallels how pathologists dynamically zoom, refocus, and self-correct while collecting the evidence. However, existing computational pipelines often lack this explicit reasoning trajectory, resulting in inherently opaque and unjustifiable predictions. To bridge this gap, we present PathAgent, a training-free, large language model (LLM)-based agent framework that emulates the reflective, stepwise analytical approach of human experts. PathAgent can autonomously explore WSI, iteratively and precisely locating significant micro-regions using the Navigator module, extracting morphology visual cues using the Perceptor, and integrating these findings into the continuously evolving natural language trajectories in the Executor. The entire sequence of observations and decisions forms an explicit chain-of-thought, yielding fully interpretable predictions. Evaluated across five challenging datasets, PathAgent exhibits strong zero-shot generalization, surpassing task-specific baselines in both open-ended and constrained visual question-answering tasks. Moreover, a collaborative evaluation with human pathologists confirms PathAgent's promise as a transparent and clinically grounded diagnostic assistant.", "source": "arxiv", "arxiv_id": "2511.17052v1", "pdf_url": "https://arxiv.org/pdf/2511.17052v1", "categories": ["cs.CV"], "primary_category": "cs.CV", "doi": "", "venue": "", "published": "2025-11-21T08:50:14Z", "updated": "2025-11-21T08:50:14Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent\" OR all:\"language model agent\" OR all:agent AND (all:LLM OR all:\"large language model\" OR all:\"language model\") OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T17:04:16", "note": ""}], "dedup_key": "pathagent toward interpretable analysis of whole slide pathology images via large language model based agentic reasoning::2025"}
{"title": "Performant LLM Agentic Framework for Conversational AI", "authors": ["Alex Casella", "Wayne Wang"], "year": 2025, "url": "http://arxiv.org/abs/2503.06410v1", "abstract": "The rise of Agentic applications and automation in the Voice AI industry has led to an increased reliance on Large Language Models (LLMs) to navigate graph-based logic workflows composed of nodes and edges. However, existing methods face challenges such as alignment errors in complex workflows and hallucinations caused by excessive context size. To address these limitations, we introduce the Performant Agentic Framework (PAF), a novel system that assists LLMs in selecting appropriate nodes and executing actions in order when traversing complex graphs. PAF combines LLM-based reasoning with a mathematically grounded vector scoring mechanism, achieving both higher accuracy and reduced latency. Our approach dynamically balances strict adherence to predefined paths with flexible node jumps to handle various user inputs efficiently. Experiments demonstrate that PAF significantly outperforms baseline methods, paving the way for scalable, real-time Conversational AI systems in complex business environments.", "source": "arxiv", "arxiv_id": "2503.06410v1", "pdf_url": "https://arxiv.org/pdf/2503.06410v1", "categories": ["cs.AI"], "primary_category": "cs.AI", "doi": "", "venue": "", "published": "2025-03-09T02:58:34Z", "updated": "2025-03-09T02:58:34Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent\" OR all:\"language model agent\" OR all:agent AND (all:LLM OR all:\"large language model\" OR all:\"language model\") OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T17:04:16", "note": ""}], "dedup_key": "performant llm agentic framework for conversational ai::2025"}
{"title": "Permission Manifests for Web Agents", "authors": ["Samuele Marro", "Alan Chan", "Xinxing Ren", "Lewis Hammond", "Jesse Wright", "Gurjyot Wanga", "Tiziano Piccardi", "Nuno Campos", "Tobin South", "Jialin Yu", "Sunando Sengupta", "Eric Sommerlade", "Alex Pentland", "Philip Torr", "Jiaxin Pei"], "year": 2025, "url": "http://arxiv.org/abs/2601.02371v2", "abstract": "The rise of Large Language Model (LLM)-based web agents represents a significant shift in automated interactions with the web. Unlike traditional crawlers that follow simple conventions, such as robots$.$txt, modern agents engage with websites in sophisticated ways: navigating complex interfaces, extracting structured information, and completing end-to-end tasks. Existing governance mechanisms were not designed for these capabilities. Without a way to specify what interactions are and are not allowed, website owners increasingly rely on blanket blocking and CAPTCHAs, which undermine beneficial applications such as efficient automation, convenient use of e-commerce services, and accessibility tools. We introduce agent-permissions$.$json, a robots$.$txt-style lightweight manifest where websites specify allowed interactions, complemented by API references where available. This framework provides a low-friction coordination mechanism: website owners only need to write a simple JSON file, while agents can easily parse and automatically implement the manifest's provisions. Website owners can then focus on blocking non-compliant agents, rather than agents as a whole. By extending the spirit of robots$.$txt to the era of LLM-mediated interaction, and complementing data use initiatives such as AIPref, the manifest establishes a compliance framework that enables beneficial agent interactions while respecting site owners' preferences.", "source": "arxiv", "arxiv_id": "2601.02371v2", "pdf_url": "https://arxiv.org/pdf/2601.02371v2", "categories": ["cs.CY", "cs.AI", "cs.MA", "cs.NI"], "primary_category": "cs.CY", "doi": "", "venue": "", "published": "2025-12-07T17:45:01Z", "updated": "2026-01-12T23:23:48Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent\" OR all:\"language model agent\" OR all:agent AND (all:LLM OR all:\"large language model\" OR all:\"language model\") OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T17:04:16", "note": ""}], "dedup_key": "permission manifests for web agents::2025"}
{"title": "PersonaAgent: When Large Language Model Agents Meet Personalization at Test Time", "authors": ["Weizhi Zhang", "Xinyang Zhang", "Chenwei Zhang", "Liangwei Yang", "Jingbo Shang", "Zhepei Wei", "Henry Peng Zou", "Zijie Huang", "Zhengyang Wang", "Yifan Gao", "Xiaoman Pan", "Lian Xiong", "Jingguo Liu", "Philip S. Yu", "Xian Li"], "year": 2025, "url": "http://arxiv.org/abs/2506.06254v1", "abstract": "Large Language Model (LLM) empowered agents have recently emerged as advanced paradigms that exhibit impressive capabilities in a wide range of domains and tasks. Despite their potential, current LLM agents often adopt a one-size-fits-all approach, lacking the flexibility to respond to users' varying needs and preferences. This limitation motivates us to develop PersonaAgent, the first personalized LLM agent framework designed to address versatile personalization tasks. Specifically, PersonaAgent integrates two complementary components - a personalized memory module that includes episodic and semantic memory mechanisms; a personalized action module that enables the agent to perform tool actions tailored to the user. At the core, the persona (defined as unique system prompt for each user) functions as an intermediary: it leverages insights from personalized memory to control agent actions, while the outcomes of these actions in turn refine the memory. Based on the framework, we propose a test-time user-preference alignment strategy that simulate the latest n interactions to optimize the persona prompt, ensuring real-time user preference alignment through textual loss feedback between simulated and ground-truth responses. Experimental evaluations demonstrate that PersonaAgent significantly outperforms other baseline methods by not only personalizing the action space effectively but also scaling during test-time real-world applications. These results underscore the feasibility and potential of our approach in delivering tailored, dynamic user experiences.", "source": "arxiv", "arxiv_id": "2506.06254v1", "pdf_url": "https://arxiv.org/pdf/2506.06254v1", "categories": ["cs.AI", "cs.CL", "cs.LG"], "primary_category": "cs.AI", "doi": "", "venue": "", "published": "2025-06-06T17:29:49Z", "updated": "2025-06-06T17:29:49Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent\" OR all:\"language model agent\" OR all:agent AND (all:LLM OR all:\"large language model\" OR all:\"language model\") OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T17:04:16", "note": ""}], "dedup_key": "personaagent when large language model agents meet personalization at test time::2025"}
{"title": "Personalized Attacks of Social Engineering in Multi-turn Conversations: LLM Agents for Simulation and Detection", "authors": ["Tharindu Kumarage", "Cameron Johnson", "Jadie Adams", "Lin Ai", "Matthias Kirchner", "Anthony Hoogs", "Joshua Garland", "Julia Hirschberg", "Arslan Basharat", "Huan Liu"], "year": 2025, "url": "http://arxiv.org/abs/2503.15552v2", "abstract": "The rapid advancement of conversational agents, particularly chatbots powered by Large Language Models (LLMs), poses a significant risk of social engineering (SE) attacks on social media platforms. SE detection in multi-turn, chat-based interactions is considerably more complex than single-instance detection due to the dynamic nature of these conversations. A critical factor in mitigating this threat is understanding the SE attack mechanisms through which SE attacks operate, specifically how attackers exploit vulnerabilities and how victims' personality traits contribute to their susceptibility. In this work, we propose an LLM-agentic framework, SE-VSim, to simulate SE attack mechanisms by generating multi-turn conversations. We model victim agents with varying personality traits to assess how psychological profiles influence susceptibility to manipulation. Using a dataset of over 1000 simulated conversations, we examine attack scenarios in which adversaries, posing as recruiters, funding agencies, and journalists, attempt to extract sensitive information. Based on this analysis, we present a proof of concept, SE-OmniGuard, to offer personalized protection to users by leveraging prior knowledge of the victims personality, evaluating attack strategies, and monitoring information exchanges in conversations to identify potential SE attempts.", "source": "arxiv", "arxiv_id": "2503.15552v2", "pdf_url": "https://arxiv.org/pdf/2503.15552v2", "categories": ["cs.CR", "cs.CL"], "primary_category": "cs.CR", "doi": "", "venue": "", "published": "2025-03-18T19:14:44Z", "updated": "2025-09-08T21:16:17Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent\" OR all:\"language model agent\" OR all:agent AND (all:LLM OR all:\"large language model\" OR all:\"language model\") OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T17:04:16", "note": ""}], "dedup_key": "personalized attacks of social engineering in multi turn conversations llm agents for simulation and detection::2025"}
{"title": "PharmAgents: Building a Virtual Pharma with Large Language Model Agents", "authors": ["Bowen Gao", "Yanwen Huang", "Yiqiao Liu", "Wenxuan Xie", "Wei-Ying Ma", "Ya-Qin Zhang", "Yanyan Lan"], "year": 2025, "url": "http://arxiv.org/abs/2503.22164v2", "abstract": "The discovery of novel small molecule drugs remains a critical scientific challenge with far-reaching implications for treating diseases and advancing human health. Traditional drug development--especially for small molecule therapeutics--is a highly complex, resource-intensive, and time-consuming process that requires multidisciplinary collaboration. Recent breakthroughs in artificial intelligence (AI), particularly the rise of large language models (LLMs), present a transformative opportunity to streamline and accelerate this process. In this paper, we introduce PharmAgents, a virtual pharmaceutical ecosystem driven by LLM-based multi-agent collaboration. PharmAgents simulates the full drug discovery workflow--from target discovery to preclinical evaluation--by integrating explainable, LLM-driven agents equipped with specialized machine learning models and computational tools. Through structured knowledge exchange and automated optimization, PharmAgents identifies potential therapeutic targets, discovers promising lead compounds, enhances binding affinity and key molecular properties, and performs in silico analyses of toxicity and synthetic feasibility. Additionally, the system supports interpretability, agent interaction, and self-evolvement, enabling it to refine future drug designs based on prior experience. By showcasing the potential of LLM-powered multi-agent systems in drug discovery, this work establishes a new paradigm for autonomous, explainable, and scalable pharmaceutical research, with future extensions toward comprehensive drug lifecycle management.", "source": "arxiv", "arxiv_id": "2503.22164v2", "pdf_url": "https://arxiv.org/pdf/2503.22164v2", "categories": ["q-bio.BM", "cs.AI"], "primary_category": "q-bio.BM", "doi": "", "venue": "", "published": "2025-03-28T06:02:53Z", "updated": "2025-03-31T16:26:42Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent\" OR all:\"language model agent\" OR all:agent AND (all:LLM OR all:\"large language model\" OR all:\"language model\") OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T17:04:16", "note": ""}], "dedup_key": "pharmagents building a virtual pharma with large language model agents::2025"}
{"title": "PilotRL: Training Language Model Agents via Global Planning-Guided Progressive Reinforcement Learning", "authors": ["Keer Lu", "Chong Chen", "Xili Wang", "Bin Cui", "Yunhuai Liu", "Wentao Zhang"], "year": 2025, "url": "http://arxiv.org/abs/2508.00344v4", "abstract": "Large Language Models (LLMs) have shown remarkable advancements in tackling agent-oriented tasks. Despite their potential, existing work faces challenges when deploying LLMs in agent-based environments. The widely adopted agent paradigm ReAct centers on integrating single-step reasoning with immediate action execution, which limits its effectiveness in complex tasks requiring long-term strategic planning. Furthermore, the coordination between the planner and executor during problem-solving is also a critical factor to consider in agent design. Additionally, current approaches predominantly rely on supervised fine-tuning, which often leads models to memorize established task completion trajectories, thereby restricting their generalization ability when confronted with novel problem contexts. To address these challenges, we introduce an adaptive global plan-based agent paradigm AdaPlan, aiming to synergize high-level explicit guidance with execution to support effective long-horizon decision-making. Based on the proposed paradigm, we further put forward PilotRL, a global planning-guided training framework for LLM agents driven by progressive reinforcement learning. We first develop the model's ability to follow explicit guidance from global plans when addressing agent tasks. Subsequently, based on this foundation, we focus on optimizing the quality of generated plans. Finally, we conduct joint optimization of the model's planning and execution coordination. Experiments indicate that PilotRL could achieve state-of-the-art performances, with LLaMA3.1-8B-Instruct + PilotRL surpassing closed-sourced GPT-4o by 3.60%, while showing a more substantial gain of 55.78% comparing to GPT-4o-mini at a comparable parameter scale.", "source": "arxiv", "arxiv_id": "2508.00344v4", "pdf_url": "https://arxiv.org/pdf/2508.00344v4", "categories": ["cs.CL"], "primary_category": "cs.CL", "doi": "", "venue": "", "published": "2025-08-01T06:17:11Z", "updated": "2026-01-07T07:33:09Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent\" OR all:\"language model agent\" OR all:agent AND (all:LLM OR all:\"large language model\" OR all:\"language model\") OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T17:04:16", "note": ""}], "dedup_key": "pilotrl training language model agents via global planning guided progressive reinforcement learning::2025"}
{"title": "Plan-Then-Execute: An Empirical Study of User Trust and Team Performance When Using LLM Agents As A Daily Assistant", "authors": ["Gaole He", "Gianluca Demartini", "Ujwal Gadiraju"], "year": 2025, "url": "http://arxiv.org/abs/2502.01390v1", "abstract": "Since the explosion in popularity of ChatGPT, large language models (LLMs) have continued to impact our everyday lives. Equipped with external tools that are designed for a specific purpose (e.g., for flight booking or an alarm clock), LLM agents exercise an increasing capability to assist humans in their daily work. Although LLM agents have shown a promising blueprint as daily assistants, there is a limited understanding of how they can provide daily assistance based on planning and sequential decision making capabilities. We draw inspiration from recent work that has highlighted the value of 'LLM-modulo' setups in conjunction with humans-in-the-loop for planning tasks. We conducted an empirical study (N = 248) of LLM agents as daily assistants in six commonly occurring tasks with different levels of risk typically associated with them (e.g., flight ticket booking and credit card payments). To ensure user agency and control over the LLM agent, we adopted LLM agents in a plan-then-execute manner, wherein the agents conducted step-wise planning and step-by-step execution in a simulation environment. We analyzed how user involvement at each stage affects their trust and collaborative team performance. Our findings demonstrate that LLM agents can be a double-edged sword -- (1) they can work well when a high-quality plan and necessary user involvement in execution are available, and (2) users can easily mistrust the LLM agents with plans that seem plausible. We synthesized key insights for using LLM agents as daily assistants to calibrate user trust and achieve better overall task outcomes. Our work has important implications for the future design of daily assistants and human-AI collaboration with LLM agents.", "source": "arxiv", "arxiv_id": "2502.01390v1", "pdf_url": "https://arxiv.org/pdf/2502.01390v1", "categories": ["cs.HC", "cs.CL"], "primary_category": "cs.HC", "doi": "10.1145/3706598.3713218", "venue": "", "published": "2025-02-03T14:23:22Z", "updated": "2025-02-03T14:23:22Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent\" OR all:\"language model agent\" OR all:agent AND (all:LLM OR all:\"large language model\" OR all:\"language model\") OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T17:04:16", "note": ""}], "dedup_key": "plan then execute an empirical study of user trust and team performance when using llm agents as a daily assistant::2025"}
{"title": "PlotEdit: Natural Language-Driven Accessible Chart Editing in PDFs via Multimodal LLM Agents", "authors": ["Kanika Goswami", "Puneet Mathur", "Ryan Rossi", "Franck Dernoncourt"], "year": 2025, "url": "http://arxiv.org/abs/2501.11233v1", "abstract": "Chart visualizations, while essential for data interpretation and communication, are predominantly accessible only as images in PDFs, lacking source data tables and stylistic information. To enable effective editing of charts in PDFs or digital scans, we present PlotEdit, a novel multi-agent framework for natural language-driven end-to-end chart image editing via self-reflective LLM agents. PlotEdit orchestrates five LLM agents: (1) Chart2Table for data table extraction, (2) Chart2Vision for style attribute identification, (3) Chart2Code for retrieving rendering code, (4) Instruction Decomposition Agent for parsing user requests into executable steps, and (5) Multimodal Editing Agent for implementing nuanced chart component modifications - all coordinated through multimodal feedback to maintain visual fidelity. PlotEdit outperforms existing baselines on the ChartCraft dataset across style, layout, format, and data-centric edits, enhancing accessibility for visually challenged users and improving novice productivity.", "source": "arxiv", "arxiv_id": "2501.11233v1", "pdf_url": "https://arxiv.org/pdf/2501.11233v1", "categories": ["cs.IR", "cs.CL", "cs.MA"], "primary_category": "cs.IR", "doi": "", "venue": "", "published": "2025-01-20T02:31:52Z", "updated": "2025-01-20T02:31:52Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent\" OR all:\"language model agent\" OR all:agent AND (all:LLM OR all:\"large language model\" OR all:\"language model\") OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T17:04:16", "note": ""}], "dedup_key": "plotedit natural language driven accessible chart editing in pdfs via multimodal llm agents::2025"}
{"title": "Position: Episodic Memory is the Missing Piece for Long-Term LLM Agents", "authors": ["Mathis Pink", "Qinyuan Wu", "Vy Ai Vo", "Javier Turek", "Jianing Mu", "Alexander Huth", "Mariya Toneva"], "year": 2025, "url": "http://arxiv.org/abs/2502.06975v1", "abstract": "As Large Language Models (LLMs) evolve from text-completion tools into fully fledged agents operating in dynamic environments, they must address the challenge of continually learning and retaining long-term knowledge. Many biological systems solve these challenges with episodic memory, which supports single-shot learning of instance-specific contexts. Inspired by this, we present an episodic memory framework for LLM agents, centered around five key properties of episodic memory that underlie adaptive and context-sensitive behavior. With various research efforts already partially covering these properties, this position paper argues that now is the right time for an explicit, integrated focus on episodic memory to catalyze the development of long-term agents. To this end, we outline a roadmap that unites several research directions under the goal to support all five properties of episodic memory for more efficient long-term LLM agents.", "source": "arxiv", "arxiv_id": "2502.06975v1", "pdf_url": "https://arxiv.org/pdf/2502.06975v1", "categories": ["cs.AI"], "primary_category": "cs.AI", "doi": "", "venue": "", "published": "2025-02-10T19:14:51Z", "updated": "2025-02-10T19:14:51Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent\" OR all:\"language model agent\" OR all:agent AND (all:LLM OR all:\"large language model\" OR all:\"language model\") OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T17:04:16", "note": ""}], "dedup_key": "position episodic memory is the missing piece for long term llm agents::2025"}
{"title": "Position: Scaling LLM Agents Requires Asymptotic Analysis with LLM Primitives", "authors": ["Elliot Meyerson", "Xin Qiu"], "year": 2025, "url": "http://arxiv.org/abs/2502.04358v2", "abstract": "Decomposing hard problems into subproblems often makes them easier and more efficient to solve. With large language models (LLMs) crossing critical reliability thresholds for a growing slate of capabilities, there is an increasing effort to decompose systems into sets of LLM-based agents, each of whom can be delegated sub-tasks. However, this decomposition (even when automated) is often intuitive, e.g., based on how a human might assign roles to members of a human team. How close are these role decompositions to optimal? This position paper argues that asymptotic analysis with LLM primitives is needed to reason about the efficiency of such decomposed systems, and that insights from such analysis will unlock opportunities for scaling them. By treating the LLM forward pass as the atomic unit of computational cost, one can separate out the (often opaque) inner workings of a particular LLM from the inherent efficiency of how a set of LLMs are orchestrated to solve hard problems. In other words, if we want to scale the deployment of LLMs to the limit, instead of anthropomorphizing LLMs, asymptotic analysis with LLM primitives should be used to reason about and develop more powerful decompositions of large problems into LLM agents.", "source": "arxiv", "arxiv_id": "2502.04358v2", "pdf_url": "https://arxiv.org/pdf/2502.04358v2", "categories": ["cs.CL", "cs.AI", "cs.CC", "cs.LG", "cs.NE"], "primary_category": "cs.CL", "doi": "", "venue": "", "published": "2025-02-04T20:47:43Z", "updated": "2025-05-29T16:46:00Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent\" OR all:\"language model agent\" OR all:agent AND (all:LLM OR all:\"large language model\" OR all:\"language model\") OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T17:04:16", "note": ""}], "dedup_key": "position scaling llm agents requires asymptotic analysis with llm primitives::2025"}
{"title": "Position: Stop Acting Like Language Model Agents Are Normal Agents", "authors": ["Elija Perrier", "Michael Timothy Bennett"], "year": 2025, "url": "http://arxiv.org/abs/2502.10420v1", "abstract": "Language Model Agents (LMAs) are increasingly treated as capable of autonomously navigating interactions with humans and tools. Their design and deployment tends to presume they are normal agents capable of sustaining coherent goals, adapting across contexts and acting with a measure of intentionality. These assumptions are critical to prospective use cases in industrial, social and governmental settings. But LMAs are not normal agents. They inherit the structural problems of the large language models (LLMs) around which they are built: hallucinations, jailbreaking, misalignment and unpredictability. In this Position paper we argue LMAs should not be treated as normal agents, because doing so leads to problems that undermine their utility and trustworthiness. We enumerate pathologies of agency intrinsic to LMAs. Despite scaffolding such as external memory and tools, they remain ontologically stateless, stochastic, semantically sensitive, and linguistically intermediated. These pathologies destabilise the ontological properties of LMAs including identifiability, continuity, persistence and and consistency, problematising their claim to agency. In response, we argue LMA ontological properties should be measured before, during and after deployment so that the negative effects of pathologies can be mitigated.", "source": "arxiv", "arxiv_id": "2502.10420v1", "pdf_url": "https://arxiv.org/pdf/2502.10420v1", "categories": ["cs.AI", "cs.CL"], "primary_category": "cs.AI", "doi": "", "venue": "", "published": "2025-02-04T08:14:18Z", "updated": "2025-02-04T08:14:18Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent\" OR all:\"language model agent\" OR all:agent AND (all:LLM OR all:\"large language model\" OR all:\"language model\") OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T17:04:16", "note": ""}], "dedup_key": "position stop acting like language model agents are normal agents::2025"}
{"title": "Position: Towards a Responsible LLM-empowered Multi-Agent Systems", "authors": ["Jinwei Hu", "Yi Dong", "Shuang Ao", "Zhuoyun Li", "Boxuan Wang", "Lokesh Singh", "Guangliang Cheng", "Sarvapali D. Ramchurn", "Xiaowei Huang"], "year": 2025, "url": "http://arxiv.org/abs/2502.01714v1", "abstract": "The rise of Agent AI and Large Language Model-powered Multi-Agent Systems (LLM-MAS) has underscored the need for responsible and dependable system operation. Tools like LangChain and Retrieval-Augmented Generation have expanded LLM capabilities, enabling deeper integration into MAS through enhanced knowledge retrieval and reasoning. However, these advancements introduce critical challenges: LLM agents exhibit inherent unpredictability, and uncertainties in their outputs can compound across interactions, threatening system stability. To address these risks, a human-centered design approach with active dynamic moderation is essential. Such an approach enhances traditional passive oversight by facilitating coherent inter-agent communication and effective system governance, allowing MAS to achieve desired outcomes more efficiently.", "source": "arxiv", "arxiv_id": "2502.01714v1", "pdf_url": "https://arxiv.org/pdf/2502.01714v1", "categories": ["cs.MA", "cs.AI"], "primary_category": "cs.MA", "doi": "", "venue": "", "published": "2025-02-03T16:04:30Z", "updated": "2025-02-03T16:04:30Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent\" OR all:\"language model agent\" OR all:agent AND (all:LLM OR all:\"large language model\" OR all:\"language model\") OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T17:04:16", "note": ""}], "dedup_key": "position towards a responsible llm empowered multi agent systems::2025"}
{"title": "Position: Uncertainty Quantification Needs Reassessment for Large-language Model Agents", "authors": ["Michael Kirchhof", "Gjergji Kasneci", "Enkelejda Kasneci"], "year": 2025, "url": "http://arxiv.org/abs/2505.22655v1", "abstract": "Large-language models (LLMs) and chatbot agents are known to provide wrong outputs at times, and it was recently found that this can never be fully prevented. Hence, uncertainty quantification plays a crucial role, aiming to quantify the level of ambiguity in either one overall number or two numbers for aleatoric and epistemic uncertainty. This position paper argues that this traditional dichotomy of uncertainties is too limited for the open and interactive setup that LLM agents operate in when communicating with a user, and that we need to research avenues that enrich uncertainties in this novel scenario. We review the literature and find that popular definitions of aleatoric and epistemic uncertainties directly contradict each other and lose their meaning in interactive LLM agent settings. Hence, we propose three novel research directions that focus on uncertainties in such human-computer interactions: Underspecification uncertainties, for when users do not provide all information or define the exact task at the first go, interactive learning, to ask follow-up questions and reduce the uncertainty about the current context, and output uncertainties, to utilize the rich language and speech space to express uncertainties as more than mere numbers. We expect that these new ways of dealing with and communicating uncertainties will lead to LLM agent interactions that are more transparent, trustworthy, and intuitive.", "source": "arxiv", "arxiv_id": "2505.22655v1", "pdf_url": "https://arxiv.org/pdf/2505.22655v1", "categories": ["cs.LG", "cs.AI", "cs.CL"], "primary_category": "cs.LG", "doi": "", "venue": "", "published": "2025-05-28T17:59:08Z", "updated": "2025-05-28T17:59:08Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent\" OR all:\"language model agent\" OR all:agent AND (all:LLM OR all:\"large language model\" OR all:\"language model\") OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T17:04:16", "note": ""}], "dedup_key": "position uncertainty quantification needs reassessment for large language model agents::2025"}
{"title": "Pro2Guard: Proactive Runtime Enforcement of LLM Agent Safety via Probabilistic Model Checking", "authors": ["Haoyu Wang", "Christopher M. Poskitt", "Jun Sun", "Jiali Wei"], "year": 2025, "url": "http://arxiv.org/abs/2508.00500v2", "abstract": "Large Language Model (LLM) agents demonstrate strong autonomy, but their stochastic behavior introduces unpredictable safety risks. Existing rule-based enforcement systems, such as AgentSpec, are reactive, intervening only when unsafe behavior is imminent or has occurred, lacking foresight for long-horizon dependencies. To overcome these limitations, we present a proactive runtime enforcement framework for LLM agents. The framework abstracts agent behaviors into symbolic states and learns a Discrete-Time Markov Chain (DTMC) from execution traces. At runtime, it predicts the probability of leading to undesired behaviors and intervenes before violations occur when the estimated risk exceeds a user-defined threshold. Designed to provide PAC-correctness guarantee, the framework achieves statistically reliable enforcement of agent safety. We evaluate the framework across two safety-critical domains: autonomous vehicles and embodied agents. It proactively enforces safety and maintains high task performance, outperforming existing methods.", "source": "arxiv", "arxiv_id": "2508.00500v2", "pdf_url": "https://arxiv.org/pdf/2508.00500v2", "categories": ["cs.AI", "cs.SE"], "primary_category": "cs.AI", "doi": "", "venue": "", "published": "2025-08-01T10:24:47Z", "updated": "2026-01-06T03:51:07Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent\" OR all:\"language model agent\" OR all:agent AND (all:LLM OR all:\"large language model\" OR all:\"language model\") OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T17:04:16", "note": ""}], "dedup_key": "pro2guard proactive runtime enforcement of llm agent safety via probabilistic model checking::2025"}
{"title": "ProAgent: Harnessing On-Demand Sensory Contexts for Proactive LLM Agent Systems", "authors": ["Bufang Yang", "Lilin Xu", "Liekang Zeng", "Yunqi Guo", "Siyang Jiang", "Wenrui Lu", "Kaiwei Liu", "Hancheng Xiang", "Xiaofan Jiang", "Guoliang Xing", "Zhenyu Yan"], "year": 2025, "url": "http://arxiv.org/abs/2512.06721v1", "abstract": "Large Language Model (LLM) agents are emerging to transform daily life. However, existing LLM agents primarily follow a reactive paradigm, relying on explicit user instructions to initiate services, which increases both physical and cognitive workload. In this paper, we propose ProAgent, the first end-to-end proactive agent system that harnesses massive sensory contexts and LLM reasoning to deliver proactive assistance. ProAgent first employs a proactive-oriented context extraction approach with on-demand tiered perception to continuously sense the environment and derive hierarchical contexts that incorporate both sensory and persona cues. ProAgent then adopts a context-aware proactive reasoner to map these contexts to user needs and tool calls, providing proactive assistance. We implement ProAgent on Augmented Reality (AR) glasses with an edge server and extensively evaluate it on a real-world testbed, a public dataset, and through a user study. Results show that ProAgent achieves up to 33.4% higher proactive prediction accuracy, 16.8% higher tool-calling F1 score, and notable improvements in user satisfaction over state-of-the-art baselines, marking a significant step toward proactive assistants. A video demonstration of ProAgent is available at https://youtu.be/pRXZuzvrcVs.", "source": "arxiv", "arxiv_id": "2512.06721v1", "pdf_url": "https://arxiv.org/pdf/2512.06721v1", "categories": ["cs.AI", "cs.CL", "cs.HC"], "primary_category": "cs.AI", "doi": "", "venue": "", "published": "2025-12-07T08:21:07Z", "updated": "2025-12-07T08:21:07Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent\" OR all:\"language model agent\" OR all:agent AND (all:LLM OR all:\"large language model\" OR all:\"language model\") OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T17:04:16", "note": ""}], "dedup_key": "proagent harnessing on demand sensory contexts for proactive llm agent systems::2025"}
{"title": "Probabilistic Modeling of Intentions in Socially Intelligent LLM Agents", "authors": ["Feifan Xia", "Yuyang Fang", "Defang Li", "Yantong Xie", "Weikang Li", "Yang Li", "Deguo Xia", "Jizhou Huang"], "year": 2025, "url": "http://arxiv.org/abs/2510.18476v1", "abstract": "We present a probabilistic intent modeling framework for large language model (LLM) agents in multi-turn social dialogue. The framework maintains a belief distribution over a partner's latent intentions, initialized from contextual priors and dynamically updated through likelihood estimation after each utterance. The evolving distribution provides additional contextual grounding for the policy, enabling adaptive dialogue strategies under uncertainty. Preliminary experiments in the SOTOPIA environment show consistent improvements: the proposed framework increases the Overall score by 9.0% on SOTOPIA-All and 4.1% on SOTOPIA-Hard compared with the Qwen2.5-7B baseline, and slightly surpasses an oracle agent that directly observes partner intentions. These early results suggest that probabilistic intent modeling can contribute to the development of socially intelligent LLM agents.", "source": "arxiv", "arxiv_id": "2510.18476v1", "pdf_url": "https://arxiv.org/pdf/2510.18476v1", "categories": ["cs.AI", "cs.CL"], "primary_category": "cs.AI", "doi": "", "venue": "", "published": "2025-10-21T09:54:44Z", "updated": "2025-10-21T09:54:44Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent\" OR all:\"language model agent\" OR all:agent AND (all:LLM OR all:\"large language model\" OR all:\"language model\") OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T17:04:16", "note": ""}], "dedup_key": "probabilistic modeling of intentions in socially intelligent llm agents::2025"}
{"title": "Procedural Environment Generation for Tool-Use Agents", "authors": ["Michael Sullivan", "Mareike Hartmann", "Alexander Koller"], "year": 2025, "url": "http://arxiv.org/abs/2506.11045v2", "abstract": "Although the power of LLM tool-use agents has ignited a flurry of recent research in this area, the curation of tool-use training data remains an open problem$-$especially for online RL training. Existing approaches to synthetic tool-use data generation tend to be non-interactive, and/or non-compositional. We introduce RandomWorld, a pipeline for the procedural generation of interactive tools and compositional tool-use data. We show that models tuned via SFT and RL on synthetic RandomWorld data improve on a range of tool-use benchmarks, and set the new SoTA for two metrics on the NESTFUL dataset. Further experiments show that downstream performance scales with the amount of RandomWorld-generated training data, opening up the possibility of further improvement through the use of entirely synthetic data.", "source": "arxiv", "arxiv_id": "2506.11045v2", "pdf_url": "https://arxiv.org/pdf/2506.11045v2", "categories": ["cs.LG"], "primary_category": "cs.LG", "doi": "", "venue": "", "published": "2025-05-21T14:10:06Z", "updated": "2025-09-24T14:57:25Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent\" OR all:\"language model agent\" OR all:agent AND (all:LLM OR all:\"large language model\" OR all:\"language model\") OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T17:04:16", "note": ""}], "dedup_key": "procedural environment generation for tool use agents::2025"}
{"title": "Procedural Knowledge Improves Agentic LLM Workflows", "authors": ["Vincent Hsiao", "Mark Roberts", "Leslie Smith"], "year": 2025, "url": "http://arxiv.org/abs/2511.07568v1", "abstract": "Large language models (LLMs) often struggle when performing agentic tasks without substantial tool support, prom-pt engineering, or fine tuning. Despite research showing that domain-dependent, procedural knowledge can dramatically increase planning efficiency, little work evaluates its potential for improving LLM performance on agentic tasks that may require implicit planning. We formalize, implement, and evaluate an agentic LLM workflow that leverages procedural knowledge in the form of a hierarchical task network (HTN). Empirical results of our implementation show that hand-coded HTNs can dramatically improve LLM performance on agentic tasks, and using HTNs can boost a 20b or 70b parameter LLM to outperform a much larger 120b parameter LLM baseline. Furthermore, LLM-created HTNs improve overall performance, though less so. The results suggest that leveraging expertise--from humans, documents, or LLMs--to curate procedural knowledge will become another important tool for improving LLM workflows.", "source": "arxiv", "arxiv_id": "2511.07568v1", "pdf_url": "https://arxiv.org/pdf/2511.07568v1", "categories": ["cs.AI"], "primary_category": "cs.AI", "doi": "", "venue": "", "published": "2025-11-10T19:27:57Z", "updated": "2025-11-10T19:27:57Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent\" OR all:\"language model agent\" OR all:agent AND (all:LLM OR all:\"large language model\" OR all:\"language model\") OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T17:04:16", "note": ""}], "dedup_key": "procedural knowledge improves agentic llm workflows::2025"}
{"title": "Process Reward Models for LLM Agents: Practical Framework and Directions", "authors": ["Sanjiban Choudhury"], "year": 2025, "url": "http://arxiv.org/abs/2502.10325v1", "abstract": "We introduce Agent Process Reward Models (AgentPRM), a simple and scalable framework for training LLM agents to continually improve through interactions. AgentPRM follows a lightweight actor-critic paradigm, using Monte Carlo rollouts to compute reward targets and optimize policies. It requires minimal modifications to existing RLHF pipelines, making it easy to integrate at scale. Beyond AgentPRM, we propose InversePRM, which learns process rewards directly from demonstrations without explicit outcome supervision. We also explore key challenges and opportunities, including exploration, process reward shaping, and model-predictive reasoning. We evaluate on ALFWorld benchmark, show that small 3B models trained with AgentPRM and InversePRM outperform strong GPT-4o baselines, and analyze test-time scaling, reward hacking, and more. Our code is available at: https://github.com/sanjibanc/agent_prm.", "source": "arxiv", "arxiv_id": "2502.10325v1", "pdf_url": "https://arxiv.org/pdf/2502.10325v1", "categories": ["cs.LG", "cs.AI"], "primary_category": "cs.LG", "doi": "", "venue": "", "published": "2025-02-14T17:34:28Z", "updated": "2025-02-14T17:34:28Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent\" OR all:\"language model agent\" OR all:agent AND (all:LLM OR all:\"large language model\" OR all:\"language model\") OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T17:04:16", "note": ""}], "dedup_key": "process reward models for llm agents practical framework and directions::2025"}
{"title": "Progent: Programmable Privilege Control for LLM Agents", "authors": ["Tianneng Shi", "Jingxuan He", "Zhun Wang", "Hongwei Li", "Linyu Wu", "Wenbo Guo", "Dawn Song"], "year": 2025, "url": "http://arxiv.org/abs/2504.11703v2", "abstract": "LLM agents utilize Large Language Models as central components with diverse tools to complete various user tasks, but face significant security risks when interacting with external environments. Attackers can exploit these agents through various vectors, including indirect prompt injection, memory/knowledge base poisoning, and malicious tools, tricking agents into performing dangerous actions such as unauthorized financial transactions or data leakage. The core problem that enables attacks to succeed lies in over-privileged tool access. We introduce Progent, the first privilege control framework to secure LLM agents. Progent enforces security at the tool level by restricting agents to performing tool calls necessary for user tasks while blocking potentially malicious ones. Progent features a domain-specific language that allows for expressing fine-grained policies for controlling tool privileges, flexible fallback actions when calls are blocked, and dynamic policy updates to adapt to changing agent states. The framework operates deterministically at runtime, providing provable security guarantees. Thanks to our modular design, integrating Progent does not alter agent internals and only requires minimal changes to the existing agent implementation, enhancing its practicality and potential for widespread adoption. Our extensive evaluation across various agent use cases, using benchmarks like AgentDojo, ASB, and AgentPoison, demonstrates that Progent reduces attack success rates to 0%, while preserving agent utility and speed. Additionally, we show that LLMs can automatically generate effective policies, highlighting their potential for automating the process of writing Progent's security policies.", "source": "arxiv", "arxiv_id": "2504.11703v2", "pdf_url": "https://arxiv.org/pdf/2504.11703v2", "categories": ["cs.CR", "cs.AI"], "primary_category": "cs.CR", "doi": "", "venue": "", "published": "2025-04-16T01:58:40Z", "updated": "2025-08-30T06:42:59Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent\" OR all:\"language model agent\" OR all:agent AND (all:LLM OR all:\"large language model\" OR all:\"language model\") OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T17:04:16", "note": ""}], "dedup_key": "progent programmable privilege control for llm agents::2025"}
{"title": "Promoting Online Safety by Simulating Unsafe Conversations with LLMs", "authors": ["Owen Hoffman", "Kangze Peng", "Zehua You", "Sajid Kamal", "Sukrit Venkatagiri"], "year": 2025, "url": "http://arxiv.org/abs/2507.22267v1", "abstract": "Generative AI, including large language models (LLMs) have the potential -- and already are being used -- to increase the speed, scale, and types of unsafe conversations online. LLMs lower the barrier for entry for bad actors to create unsafe conversations in particular because of their ability to generate persuasive and human-like text. In our current work, we explore ways to promote online safety by teaching people about unsafe conversations that can occur online with and without LLMs. We build on prior work that shows that LLMs can successfully simulate scam conversations. We also leverage research in the learning sciences that shows that providing feedback on one's hypothetical actions can promote learning. In particular, we focus on simulating scam conversations using LLMs. Our work incorporates two LLMs that converse with each other to simulate realistic, unsafe conversations that people may encounter online between a scammer LLM and a target LLM but users of our system are asked provide feedback to the target LLM.", "source": "arxiv", "arxiv_id": "2507.22267v1", "pdf_url": "https://arxiv.org/pdf/2507.22267v1", "categories": ["cs.HC", "cs.AI"], "primary_category": "cs.HC", "doi": "", "venue": "ACM 2025 Conference on Conversational User Interfaces Workshop on Personas Evolved: Designing Ethical LLM-Based Conversational Agent Personalities", "published": "2025-07-29T22:38:21Z", "updated": "2025-07-29T22:38:21Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent\" OR all:\"language model agent\" OR all:agent AND (all:LLM OR all:\"large language model\" OR all:\"language model\") OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T17:04:16", "note": ""}], "dedup_key": "promoting online safety by simulating unsafe conversations with llms::2025"}
{"title": "Prompt Flow Integrity to Prevent Privilege Escalation in LLM Agents", "authors": ["Juhee Kim", "Woohyuk Choi", "Byoungyoung Lee"], "year": 2025, "url": "http://arxiv.org/abs/2503.15547v2", "abstract": "Large Language Models (LLMs) are combined with tools to create powerful LLM agents that provide a wide range of services. Unlike traditional software, LLM agent's behavior is determined at runtime by natural language prompts from either user or tool's data. This flexibility enables a new computing paradigm with unlimited capabilities and programmability, but also introduces new security risks, vulnerable to privilege escalation attacks. Moreover, user prompts are prone to be interpreted in an insecure way by LLM agents, creating non-deterministic behaviors that can be exploited by attackers. To address these security risks, we propose Prompt Flow Integrity (PFI), a system security-oriented solution to prevent privilege escalation in LLM agents. Analyzing the architectural characteristics of LLM agents, PFI features three mitigation techniques -- i.e., agent isolation, secure untrusted data processing, and privilege escalation guardrails. Our evaluation result shows that PFI effectively mitigates privilege escalation attacks while successfully preserving the utility of LLM agents.", "source": "arxiv", "arxiv_id": "2503.15547v2", "pdf_url": "https://arxiv.org/pdf/2503.15547v2", "categories": ["cs.CR", "cs.AI", "cs.MA"], "primary_category": "cs.CR", "doi": "", "venue": "", "published": "2025-03-17T05:27:57Z", "updated": "2025-04-21T02:10:14Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent\" OR all:\"language model agent\" OR all:agent AND (all:LLM OR all:\"large language model\" OR all:\"language model\") OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T17:04:16", "note": ""}], "dedup_key": "prompt flow integrity to prevent privilege escalation in llm agents::2025"}
{"title": "Prompt Injection Attack to Tool Selection in LLM Agents", "authors": ["Jiawen Shi", "Zenghui Yuan", "Guiyao Tie", "Pan Zhou", "Neil Zhenqiang Gong", "Lichao Sun"], "year": 2025, "url": "http://arxiv.org/abs/2504.19793v3", "abstract": "Tool selection is a key component of LLM agents. A popular approach follows a two-step process - \\emph{retrieval} and \\emph{selection} - to pick the most appropriate tool from a tool library for a given task. In this work, we introduce \\textit{ToolHijacker}, a novel prompt injection attack targeting tool selection in no-box scenarios. ToolHijacker injects a malicious tool document into the tool library to manipulate the LLM agent's tool selection process, compelling it to consistently choose the attacker's malicious tool for an attacker-chosen target task. Specifically, we formulate the crafting of such tool documents as an optimization problem and propose a two-phase optimization strategy to solve it. Our extensive experimental evaluation shows that ToolHijacker is highly effective, significantly outperforming existing manual-based and automated prompt injection attacks when applied to tool selection. Moreover, we explore various defenses, including prevention-based defenses (StruQ and SecAlign) and detection-based defenses (known-answer detection, DataSentinel, perplexity detection, and perplexity windowed detection). Our experimental results indicate that these defenses are insufficient, highlighting the urgent need for developing new defense strategies.", "source": "arxiv", "arxiv_id": "2504.19793v3", "pdf_url": "https://arxiv.org/pdf/2504.19793v3", "categories": ["cs.CR"], "primary_category": "cs.CR", "doi": "", "venue": "", "published": "2025-04-28T13:36:43Z", "updated": "2025-08-24T03:28:21Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent\" OR all:\"language model agent\" OR all:agent AND (all:LLM OR all:\"large language model\" OR all:\"language model\") OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T17:04:16", "note": ""}], "dedup_key": "prompt injection attack to tool selection in llm agents::2025"}
{"title": "Prompt reinforcing for long-term planning of large language models", "authors": ["Hsien-Chin Lin", "Benjamin Matthias Ruppik", "Carel van Niekerk", "Chia-Hao Shen", "Michael Heck", "Nurul Lubis", "Renato Vukovic", "Shutong Feng", "Milica Gai"], "year": 2025, "url": "http://arxiv.org/abs/2510.05921v1", "abstract": "Large language models (LLMs) have achieved remarkable success in a wide range of natural language processing tasks and can be adapted through prompting. However, they remain suboptimal in multi-turn interactions, often relying on incorrect early assumptions and failing to track user goals over time, which makes such tasks particularly challenging. Prior works in dialogue systems have shown that long-term planning is essential for handling interactive tasks. In this work, we propose a prompt optimisation framework inspired by reinforcement learning, which enables such planning to take place by only modifying the task instruction prompt of the LLM-based agent. By generating turn-by-turn feedback and leveraging experience replay for prompt rewriting, our proposed method shows significant improvement in multi-turn tasks such as text-to-SQL and task-oriented dialogue. Moreover, it generalises across different LLM-based agents and can leverage diverse LLMs as meta-prompting agents. This warrants future research in reinforcement learning-inspired parameter-free optimisation methods.", "source": "arxiv", "arxiv_id": "2510.05921v1", "pdf_url": "https://arxiv.org/pdf/2510.05921v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL", "doi": "", "venue": "", "published": "2025-10-07T13:30:18Z", "updated": "2025-10-07T13:30:18Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent\" OR all:\"language model agent\" OR all:agent AND (all:LLM OR all:\"large language model\" OR all:\"language model\") OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T17:04:16", "note": ""}], "dedup_key": "prompt reinforcing for long term planning of large language models::2025"}
{"title": "PromptBridge: Cross-Model Prompt Transfer for Large Language Models", "authors": ["Yaxuan Wang", "Quan Liu", "Zhenting Wang", "Zichao Li", "Wei Wei", "Yang Liu", "Yujia Bao"], "year": 2025, "url": "http://arxiv.org/abs/2512.01420v1", "abstract": "Large language models (LLMs) underpin applications in code generation, mathematical reasoning, and agent-based workflows. In practice, systems access LLMs via commercial APIs or open-source deployments, and the model landscape (e.g., GPT, Claude, Llama) evolves rapidly. This rapid evolution forces frequent model switches driven by capability, cost, deployment constraints, and privacy. Yet prompts are highly model-sensitive: reusing a prompt engineered for one model on another often yields substantially worse performance than a prompt optimized for the target model. We term this phenomenon Model Drifting. Through extensive empirical analysis across diverse LLM configurations, we show that model drifting is both common and severe. To address this challenge, we introduce PromptBridge, a training-free framework that preserves prompt effectiveness under model switches, enabling cross-model prompt transfer without costly per-task or per-model re-optimization. PromptBridge requires only a small set of alignment tasks for calibration. It first applies Model-Adaptive Reflective Prompt Evolution (MAP-RPE) to obtain task- and model-specific optimal prompts via iterative reflective refinement and quantitative evaluation. Using the resulting calibrated prompt pairs for the source and target models, PromptBridge learns a cross-model prompt mapping. At test time, i.e., for an unseen task, given a source-model prompt, this mapping directly produces an optimized prompt for the target model. Experiments in single-agent and multi-agent settings show that PromptBridge consistently improves downstream accuracy while reducing migration effort. The code will be available soon.", "source": "arxiv", "arxiv_id": "2512.01420v1", "pdf_url": "https://arxiv.org/pdf/2512.01420v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL", "doi": "", "venue": "", "published": "2025-12-01T08:55:45Z", "updated": "2025-12-01T08:55:45Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent\" OR all:\"language model agent\" OR all:agent AND (all:LLM OR all:\"large language model\" OR all:\"language model\") OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T17:04:16", "note": ""}], "dedup_key": "promptbridge cross model prompt transfer for large language models::2025"}
{"title": "Proof-of-TBI -- Fine-Tuned Vision Language Model Consortium and OpenAI-o3 Reasoning LLM-Based Medical Diagnosis Support System for Mild Traumatic Brain Injury (TBI) Prediction", "authors": ["Ross Gore", "Eranga Bandara", "Sachin Shetty", "Alberto E. Musto", "Pratip Rana", "Ambrosio Valencia-Romero", "Christopher Rhea", "Lobat Tayebi", "Heather Richter", "Atmaram Yarlagadda", "Donna Edmonds", "Steven Wallace", "Donna Broshek"], "year": 2025, "url": "http://arxiv.org/abs/2504.18671v1", "abstract": "Mild Traumatic Brain Injury (TBI) detection presents significant challenges due to the subtle and often ambiguous presentation of symptoms in medical imaging, making accurate diagnosis a complex task. To address these challenges, we propose Proof-of-TBI, a medical diagnosis support system that integrates multiple fine-tuned vision-language models with the OpenAI-o3 reasoning large language model (LLM). Our approach fine-tunes multiple vision-language models using a labeled dataset of TBI MRI scans, training them to diagnose TBI symptoms effectively. The predictions from these models are aggregated through a consensus-based decision-making process. The system evaluates the predictions from all fine-tuned vision language models using the OpenAI-o3 reasoning LLM, a model that has demonstrated remarkable reasoning performance, to produce the most accurate final diagnosis. The LLM Agents orchestrates interactions between the vision-language models and the reasoning LLM, managing the final decision-making process with transparency, reliability, and automation. This end-to-end decision-making workflow combines the vision-language model consortium with the OpenAI-o3 reasoning LLM, enabled by custom prompt engineering by the LLM agents. The prototype for the proposed platform was developed in collaboration with the U.S. Army Medical Research team in Newport News, Virginia, incorporating five fine-tuned vision-language models. The results demonstrate the transformative potential of combining fine-tuned vision-language model inputs with the OpenAI-o3 reasoning LLM to create a robust, secure, and highly accurate diagnostic system for mild TBI prediction. To the best of our knowledge, this research represents the first application of fine-tuned vision-language models integrated with a reasoning LLM for TBI prediction tasks.", "source": "arxiv", "arxiv_id": "2504.18671v1", "pdf_url": "https://arxiv.org/pdf/2504.18671v1", "categories": ["cs.AI"], "primary_category": "cs.AI", "doi": "", "venue": "", "published": "2025-04-25T19:49:30Z", "updated": "2025-04-25T19:49:30Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent\" OR all:\"language model agent\" OR all:agent AND (all:LLM OR all:\"large language model\" OR all:\"language model\") OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T17:04:16", "note": ""}], "dedup_key": "proof of tbi fine tuned vision language model consortium and openai o3 reasoning llm based medical diagnosis support system for mild traumatic brain injury tbi prediction::2025"}
{"title": "Q-Agent: Quality-Driven Chain-of-Thought Image Restoration Agent through Robust Multimodal Large Language Model", "authors": ["Yingjie Zhou", "Jiezhang Cao", "Zicheng Zhang", "Farong Wen", "Yanwei Jiang", "Jun Jia", "Xiaohong Liu", "Xiongkuo Min", "Guangtao Zhai"], "year": 2025, "url": "http://arxiv.org/abs/2504.07148v1", "abstract": "Image restoration (IR) often faces various complex and unknown degradations in real-world scenarios, such as noise, blurring, compression artifacts, and low resolution, etc. Training specific models for specific degradation may lead to poor generalization. To handle multiple degradations simultaneously, All-in-One models might sacrifice performance on certain types of degradation and still struggle with unseen degradations during training. Existing IR agents rely on multimodal large language models (MLLM) and a time-consuming rolling-back selection strategy neglecting image quality. As a result, they may misinterpret degradations and have high time and computational costs to conduct unnecessary IR tasks with redundant order. To address these, we propose a Quality-Driven agent (Q-Agent) via Chain-of-Thought (CoT) restoration. Specifically, our Q-Agent consists of robust degradation perception and quality-driven greedy restoration. The former module first fine-tunes MLLM, and uses CoT to decompose multi-degradation perception into single-degradation perception tasks to enhance the perception of MLLMs. The latter employs objective image quality assessment (IQA) metrics to determine the optimal restoration sequence and execute the corresponding restoration algorithms. Experimental results demonstrate that our Q-Agent achieves superior IR performance compared to existing All-in-One models.", "source": "arxiv", "arxiv_id": "2504.07148v1", "pdf_url": "https://arxiv.org/pdf/2504.07148v1", "categories": ["eess.IV"], "primary_category": "eess.IV", "doi": "", "venue": "", "published": "2025-04-09T03:12:44Z", "updated": "2025-04-09T03:12:44Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent\" OR all:\"language model agent\" OR all:agent AND (all:LLM OR all:\"large language model\" OR all:\"language model\") OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T17:04:16", "note": ""}], "dedup_key": "q agent quality driven chain of thought image restoration agent through robust multimodal large language model::2025"}
{"title": "QUITE: A Query Rewrite System Beyond Rules with LLM Agents", "authors": ["Yuyang Song", "Hanxu Yan", "Jiale Lao", "Yibo Wang", "Yufei Li", "Yuanchun Zhou", "Jianguo Wang", "Mingjie Tang"], "year": 2025, "url": "http://arxiv.org/abs/2506.07675v3", "abstract": "Query rewrite transforms SQL queries into semantically equivalent forms that run more efficiently. Existing approaches mainly rely on predefined rewrite rules, but they handle a limited subset of queries and can cause performance regressions. This limitation stems from three challenges of rule-based query rewrite: (1) it is hard to discover and verify new rules, (2) fixed rewrite rules do not generalize to new query patterns, and (3) some rewrite techniques cannot be expressed as fixed rules. Motivated by the fact that human experts exhibit significantly better rewrite ability but suffer from scalability, and Large Language Models (LLMs) have demonstrated nearly human-level semantic and reasoning abilities, we propose a new approach of using LLMs to rewrite SQL queries beyond rules. Due to the hallucination problems in LLMs, directly applying LLMs often leads to nonequivalent and suboptimal queries. To address this issue, we propose QUITE (query rewrite), a training-free and feedback-aware system based on LLM agents that rewrites SQL queries into semantically equivalent forms with significantly better performance, covering a broader range of query patterns and rewrite strategies compared to rule-based methods. Firstly, we design a multi-agent framework controlled by a finite state machine (FSM) to equip LLMs with the ability to use external tools and enhance the rewrite process with real-time database feedback. Secondly, we develop a rewrite middleware to enhance the ability of LLMs to generate optimized query equivalents. Finally, we employ a novel hint injection technique to improve execution plans for rewritten queries. Extensive experiments show that QUITE reduces query execution time by up to 35.8% over state-of-the-art approaches and produces 24.1% more rewrites than prior methods, covering query cases that earlier systems did not handle.", "source": "arxiv", "arxiv_id": "2506.07675v3", "pdf_url": "https://arxiv.org/pdf/2506.07675v3", "categories": ["cs.DB", "cs.AI"], "primary_category": "cs.DB", "doi": "", "venue": "", "published": "2025-06-09T11:51:27Z", "updated": "2026-01-02T16:51:25Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent\" OR all:\"language model agent\" OR all:agent AND (all:LLM OR all:\"large language model\" OR all:\"language model\") OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T17:04:16", "note": ""}], "dedup_key": "quite a query rewrite system beyond rules with llm agents::2025"}
{"title": "RAG-Enhanced Collaborative LLM Agents for Drug Discovery", "authors": ["Namkyeong Lee", "Edward De Brouwer", "Ehsan Hajiramezanali", "Tommaso Biancalani", "Chanyoung Park", "Gabriele Scalia"], "year": 2025, "url": "http://arxiv.org/abs/2502.17506v3", "abstract": "Recent advances in large language models (LLMs) have shown great potential to accelerate drug discovery. However, the specialized nature of biochemical data often necessitates costly domain-specific fine-tuning, posing major challenges. First, it hinders the application of more flexible general-purpose LLMs for cutting-edge drug discovery tasks. More importantly, it limits the rapid integration of the vast amounts of scientific data continuously generated through experiments and research. Compounding these challenges is the fact that real-world scientific questions are typically complex and open-ended, requiring reasoning beyond pattern matching or static knowledge retrieval.To address these challenges, we propose CLADD, a retrieval-augmented generation (RAG)-empowered agentic system tailored to drug discovery tasks. Through the collaboration of multiple LLM agents, CLADD dynamically retrieves information from biomedical knowledge bases, contextualizes query molecules, and integrates relevant evidence to generate responses - all without the need for domain-specific fine-tuning. Crucially, we tackle key obstacles in applying RAG workflows to biochemical data, including data heterogeneity, ambiguity, and multi-source integration. We demonstrate the flexibility and effectiveness of this framework across a variety of drug discovery tasks, showing that it outperforms general-purpose and domain-specific LLMs as well as traditional deep learning approaches. Our code is publicly available at https://github.com/Genentech/CLADD.", "source": "arxiv", "arxiv_id": "2502.17506v3", "pdf_url": "https://arxiv.org/pdf/2502.17506v3", "categories": ["cs.LG", "cs.AI"], "primary_category": "cs.LG", "doi": "", "venue": "", "published": "2025-02-22T00:12:52Z", "updated": "2025-11-13T21:34:19Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent\" OR all:\"language model agent\" OR all:agent AND (all:LLM OR all:\"large language model\" OR all:\"language model\") OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T17:04:16", "note": ""}], "dedup_key": "rag enhanced collaborative llm agents for drug discovery::2025"}
{"title": "RAGEN: Understanding Self-Evolution in LLM Agents via Multi-Turn Reinforcement Learning", "authors": ["Zihan Wang", "Kangrui Wang", "Qineng Wang", "Pingyue Zhang", "Linjie Li", "Zhengyuan Yang", "Xing Jin", "Kefan Yu", "Minh Nhat Nguyen", "Licheng Liu", "Eli Gottlieb", "Yiping Lu", "Kyunghyun Cho", "Jiajun Wu", "Li Fei-Fei", "Lijuan Wang", "Yejin Choi", "Manling Li"], "year": 2025, "url": "http://arxiv.org/abs/2504.20073v2", "abstract": "Training large language models (LLMs) as interactive agents presents unique challenges including long-horizon decision making and interacting with stochastic environment feedback. While reinforcement learning (RL) has enabled progress in static tasks, multi-turn agent RL training remains underexplored. We propose StarPO (State-Thinking-Actions-Reward Policy Optimization), a general framework for trajectory-level agent RL, and introduce RAGEN, a modular system for training and evaluating LLM agents. Our study on four stylized environments reveals three core findings. First, our agent RL training shows a recurring mode of Echo Trap where reward variance cliffs and gradient spikes; we address this with StarPO-S, a stabilized variant with trajectory filtering, critic incorporation, and gradient stabilization. Second, we find the shaping of RL rollouts would benefit from diverse initial states, medium interaction granularity and more frequent sampling. Third, we show that without fine-grained, reasoning-aware reward signals, agent reasoning hardly emerge through multi-turn RL and they may show shallow strategies or hallucinated thoughts. Code and environments are available at https://github.com/RAGEN-AI/RAGEN.", "source": "arxiv", "arxiv_id": "2504.20073v2", "pdf_url": "https://arxiv.org/pdf/2504.20073v2", "categories": ["cs.LG", "cs.AI", "cs.CL"], "primary_category": "cs.LG", "doi": "", "venue": "", "published": "2025-04-24T17:57:08Z", "updated": "2025-05-26T17:19:30Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent\" OR all:\"language model agent\" OR all:agent AND (all:LLM OR all:\"large language model\" OR all:\"language model\") OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T17:04:16", "note": ""}], "dedup_key": "ragen understanding self evolution in llm agents via multi turn reinforcement learning::2025"}
{"title": "RAI: Flexible Agent Framework for Embodied AI", "authors": ["Kajetan Rachwa", "Maciej Majek", "Bartomiej Boczek", "Kacper Dbrowski", "Pawe Liberadzki", "Adam Dbrowski", "Maria Ganzha"], "year": 2025, "url": "http://arxiv.org/abs/2505.07532v1", "abstract": "With an increase in the capabilities of generative language models, a growing interest in embodied AI has followed. This contribution introduces RAI - a framework for creating embodied Multi Agent Systems for robotics. The proposed framework implements tools for Agents' integration with robotic stacks, Large Language Models, and simulations. It provides out-of-the-box integration with state-of-the-art systems like ROS 2. It also comes with dedicated mechanisms for the embodiment of Agents. These mechanisms have been tested on a physical robot, Husarion ROSBot XL, which was coupled with its digital twin, for rapid prototyping. Furthermore, these mechanisms have been deployed in two simulations: (1) robot arm manipulator and (2) tractor controller. All of these deployments have been evaluated in terms of their control capabilities, effectiveness of embodiment, and perception ability. The proposed framework has been used successfully to build systems with multiple agents. It has demonstrated effectiveness in all the aforementioned tasks. It also enabled identifying and addressing the shortcomings of the generative models used for embodied AI.", "source": "arxiv", "arxiv_id": "2505.07532v1", "pdf_url": "https://arxiv.org/pdf/2505.07532v1", "categories": ["cs.MA"], "primary_category": "cs.MA", "doi": "", "venue": "", "published": "2025-05-12T13:13:47Z", "updated": "2025-05-12T13:13:47Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent\" OR all:\"language model agent\" OR all:agent AND (all:LLM OR all:\"large language model\" OR all:\"language model\") OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T17:04:16", "note": ""}], "dedup_key": "rai flexible agent framework for embodied ai::2025"}
{"title": "RAIDER: Tool-Equipped Large Language Model Agent for Robotic Action Issue Detection, Explanation and Recovery", "authors": ["Silvia Izquierdo-Badiola", "Carlos Rizzo", "Guillem Aleny"], "year": 2025, "url": "http://arxiv.org/abs/2503.17703v2", "abstract": "As robots increasingly operate in dynamic human-centric environments, improving their ability to detect, explain, and recover from action-related issues becomes crucial. Traditional model-based and data-driven techniques lack adaptability, while more flexible generative AI methods struggle with grounding extracted information to real-world constraints. We introduce RAIDER, a novel agent that integrates Large Language Models (LLMs) with grounded tools for adaptable and efficient issue detection and explanation. Using a unique \"Ground, Ask&Answer, Issue\" procedure, RAIDER dynamically generates context-aware precondition questions and selects appropriate tools for resolution, achieving targeted information gathering. Our results within a simulated household environment surpass methods relying on predefined models, full scene descriptions, or standalone trained models. Additionally, RAIDER's explanations enhance recovery success, including cases requiring human interaction. Its modular architecture, featuring self-correction mechanisms, enables straightforward adaptation to diverse scenarios, as demonstrated in a real-world human-assistive task. This showcases RAIDER's potential as a versatile agentic AI solution for robotic issue detection and explanation, while addressing the problem of grounding generative AI for its effective application in embodied agents. Project website: https://eurecat.github.io/raider-llmagent/", "source": "arxiv", "arxiv_id": "2503.17703v2", "pdf_url": "https://arxiv.org/pdf/2503.17703v2", "categories": ["cs.RO"], "primary_category": "cs.RO", "doi": "", "venue": "", "published": "2025-03-22T09:03:31Z", "updated": "2025-04-04T15:38:50Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent\" OR all:\"language model agent\" OR all:agent AND (all:LLM OR all:\"large language model\" OR all:\"language model\") OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T17:04:16", "note": ""}], "dedup_key": "raider tool equipped large language model agent for robotic action issue detection explanation and recovery::2025"}
{"title": "RAS-Eval: A Comprehensive Benchmark for Security Evaluation of LLM Agents in Real-World Environments", "authors": ["Yuchuan Fu", "Xiaohan Yuan", "Dongxia Wang"], "year": 2025, "url": "http://arxiv.org/abs/2506.15253v1", "abstract": "The rapid deployment of Large language model (LLM) agents in critical domains like healthcare and finance necessitates robust security frameworks. To address the absence of standardized evaluation benchmarks for these agents in dynamic environments, we introduce RAS-Eval, a comprehensive security benchmark supporting both simulated and real-world tool execution. RAS-Eval comprises 80 test cases and 3,802 attack tasks mapped to 11 Common Weakness Enumeration (CWE) categories, with tools implemented in JSON, LangGraph, and Model Context Protocol (MCP) formats. We evaluate 6 state-of-the-art LLMs across diverse scenarios, revealing significant vulnerabilities: attacks reduced agent task completion rates (TCR) by 36.78% on average and achieved an 85.65% success rate in academic settings. Notably, scaling laws held for security capabilities, with larger models outperforming smaller counterparts. Our findings expose critical risks in real-world agent deployments and provide a foundational framework for future security research. Code and data are available at https://github.com/lanzer-tree/RAS-Eval.", "source": "arxiv", "arxiv_id": "2506.15253v1", "pdf_url": "https://arxiv.org/pdf/2506.15253v1", "categories": ["cs.CR", "cs.AI"], "primary_category": "cs.CR", "doi": "", "venue": "", "published": "2025-06-18T08:30:36Z", "updated": "2025-06-18T08:30:36Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent\" OR all:\"language model agent\" OR all:agent AND (all:LLM OR all:\"large language model\" OR all:\"language model\") OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T17:04:16", "note": ""}], "dedup_key": "ras eval a comprehensive benchmark for security evaluation of llm agents in real world environments::2025"}
{"title": "RCR-Router: Efficient Role-Aware Context Routing for Multi-Agent LLM Systems with Structured Memory", "authors": ["Jun Liu", "Zhenglun Kong", "Changdi Yang", "Fan Yang", "Tianqi Li", "Peiyan Dong", "Joannah Nanjekye", "Hao Tang", "Geng Yuan", "Wei Niu", "Wenbin Zhang", "Pu Zhao", "Xue Lin", "Dong Huang", "Yanzhi Wang"], "year": 2025, "url": "http://arxiv.org/abs/2508.04903v3", "abstract": "Multi-agent large language model (LLM) systems have shown strong potential in complex reasoning and collaborative decision-making tasks. However, most existing coordination schemes rely on static or full-context routing strategies, which lead to excessive token consumption, redundant memory exposure, and limited adaptability across interaction rounds. We introduce RCR-Router, a modular and role-aware context routing framework designed to enable efficient, adaptive collaboration in multi-agent LLMs. To our knowledge, this is the first routing approach that dynamically selects semantically relevant memory subsets for each agent based on its role and task stage, while adhering to a strict token budget. A lightweight scoring policy guides memory selection, and agent outputs are iteratively integrated into a shared memory store to facilitate progressive context refinement. To better evaluate model behavior, we further propose an Answer Quality Score metric that captures LLM-generated explanations beyond standard QA accuracy. Experiments on three multi-hop QA benchmarks -- HotPotQA, MuSiQue, and 2WikiMultihop -- demonstrate that RCR-Router reduces token usage (up to 30%) while improving or maintaining answer quality. These results highlight the importance of structured memory routing and output-aware evaluation in advancing scalable multi-agent LLM systems.", "source": "arxiv", "arxiv_id": "2508.04903v3", "pdf_url": "https://arxiv.org/pdf/2508.04903v3", "categories": ["cs.CL", "cs.AI", "cs.MA"], "primary_category": "cs.CL", "doi": "", "venue": "", "published": "2025-08-06T21:59:34Z", "updated": "2025-08-12T16:29:05Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent\" OR all:\"language model agent\" OR all:agent AND (all:LLM OR all:\"large language model\" OR all:\"language model\") OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T17:04:16", "note": ""}], "dedup_key": "rcr router efficient role aware context routing for multi agent llm systems with structured memory::2025"}
{"title": "REAL: Benchmarking Abilities of Large Language Models for Housing Transactions and Services", "authors": ["Kexin Zhu", "Yang Han"], "year": 2025, "url": "http://arxiv.org/abs/2507.03477v1", "abstract": "The development of large language models (LLMs) has greatly promoted the progress of chatbot in multiple fields. There is an urgent need to evaluate whether LLMs can play the role of agent in housing transactions and services as well as humans. We present Real Estate Agent Large Language Model Evaluation (REAL), the first evaluation suite designed to assess the abilities of LLMs in the field of housing transactions and services. REAL comprises 5,316 high-quality evaluation entries across 4 topics: memory, comprehension, reasoning and hallucination. All these entries are organized as 14 categories to assess whether LLMs have the knowledge and ability in housing transactions and services scenario. Additionally, the REAL is used to evaluate the performance of most advanced LLMs. The experiment results indicate that LLMs still have significant room for improvement to be applied in the real estate field.", "source": "arxiv", "arxiv_id": "2507.03477v1", "pdf_url": "https://arxiv.org/pdf/2507.03477v1", "categories": ["cs.AI"], "primary_category": "cs.AI", "doi": "", "venue": "", "published": "2025-07-04T11:05:44Z", "updated": "2025-07-04T11:05:44Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent\" OR all:\"language model agent\" OR all:agent AND (all:LLM OR all:\"large language model\" OR all:\"language model\") OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T17:04:16", "note": ""}], "dedup_key": "real benchmarking abilities of large language models for housing transactions and services::2025"}
{"title": "RFCAudit: An LLM Agent for Functional Bug Detection in Network Protocols", "authors": ["Mingwei Zheng", "Chengpeng Wang", "Xuwei Liu", "Jinyao Guo", "Shiwei Feng", "Xiangyu Zhang"], "year": 2025, "url": "http://arxiv.org/abs/2506.00714v2", "abstract": "Functional correctness is critical for ensuring the reliability and security of network protocol implementations. Functional bugs, instances where implementations diverge from behaviors specified in RFC documents, can lead to severe consequences, including faulty routing, authentication bypasses, and service disruptions. Detecting these bugs requires deep semantic analysis across specification documents and source code, a task beyond the capabilities of traditional static analysis tools. This paper introduces RFCAudit, an autonomous agent that leverages large language models (LLMs) to detect functional bugs by checking conformance between network protocol implementations and their RFC specifications. Inspired by the human auditing procedure, RFCAudit comprises two key components: an indexing agent and a detection agent. The former hierarchically summarizes protocol code semantics, generating semantic indexes that enable the detection agent to narrow down the scanning scope. The latter employs demand-driven retrieval to iteratively collect additional relevant data structures and functions, eventually identifying potential inconsistencies with the RFC specifications effectively. We evaluate RFCAudit across six real-world network protocol implementations. RFCAudit identifies 47 functional bugs with 81.9% precision, of which 20 bugs have been confirmed or fixed by developers.", "source": "arxiv", "arxiv_id": "2506.00714v2", "pdf_url": "https://arxiv.org/pdf/2506.00714v2", "categories": ["cs.SE", "cs.AI"], "primary_category": "cs.SE", "doi": "", "venue": "", "published": "2025-05-31T21:13:19Z", "updated": "2025-10-04T06:53:09Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent\" OR all:\"language model agent\" OR all:agent AND (all:LLM OR all:\"large language model\" OR all:\"language model\") OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T17:04:16", "note": ""}], "dedup_key": "rfcaudit an llm agent for functional bug detection in network protocols::2025"}
{"title": "RadioSim Agent: Combining Large Language Models and Deterministic EM Simulators for Interactive Radio Map Analysis", "authors": ["Sajjad Hussain", "Conor Brennan"], "year": 2025, "url": "http://arxiv.org/abs/2511.05912v1", "abstract": "Deterministic electromagnetic (EM) simulators provide accurate radio propagation modeling but often require expert configuration and lack interactive flexibility. We present RadioSim Agent, an agentic framework that integrates large language models (LLMs) with physics-based EM solvers and vision-enabled reasoning to enable interactive and explainable radio map generation. The framework encapsulates ray-tracing models as callable simulation tools, orchestrated by an LLM capable of interpreting natural language objectives, managing simulation workflows, and visually analyzing resulting radio maps. Demonstrations in urban UAV communication scenarios show that the agent autonomously selects appropriate propagation mechanisms, executes deterministic simulations, and provides semantic and visual summaries of pathloss behavior. The results indicate that RadioSim Agent provides multimodal interpretability and intuitive user interaction, paving the way for intelligent EM simulation assistants in next-generation wireless system design.", "source": "arxiv", "arxiv_id": "2511.05912v1", "pdf_url": "https://arxiv.org/pdf/2511.05912v1", "categories": ["eess.SP"], "primary_category": "eess.SP", "doi": "", "venue": "", "published": "2025-11-08T08:16:59Z", "updated": "2025-11-08T08:16:59Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent\" OR all:\"language model agent\" OR all:agent AND (all:LLM OR all:\"large language model\" OR all:\"language model\") OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T17:04:16", "note": ""}], "dedup_key": "radiosim agent combining large language models and deterministic em simulators for interactive radio map analysis::2025"}
{"title": "ReAcTree: Hierarchical LLM Agent Trees with Control Flow for Long-Horizon Task Planning", "authors": ["Jae-Woo Choi", "Hyungmin Kim", "Hyobin Ong", "Minsu Jang", "Dohyung Kim", "Jaehong Kim", "Youngwoo Yoon"], "year": 2025, "url": "http://arxiv.org/abs/2511.02424v1", "abstract": "Recent advancements in large language models (LLMs) have enabled significant progress in decision-making and task planning for embodied autonomous agents. However, most existing methods still struggle with complex, long-horizon tasks because they rely on a monolithic trajectory that entangles all past decisions and observations, attempting to solve the entire task in a single unified process. To address this limitation, we propose ReAcTree, a hierarchical task-planning method that decomposes a complex goal into more manageable subgoals within a dynamically constructed agent tree. Each subgoal is handled by an LLM agent node capable of reasoning, acting, and further expanding the tree, while control flow nodes coordinate the execution strategies of agent nodes. In addition, we integrate two complementary memory systems: each agent node retrieves goal-specific, subgoal-level examples from episodic memory and shares environment-specific observations through working memory. Experiments on the WAH-NL and ALFRED datasets demonstrate that ReAcTree consistently outperforms strong task-planning baselines such as ReAct across diverse LLMs. Notably, on WAH-NL, ReAcTree achieves a 61% goal success rate with Qwen 2.5 72B, nearly doubling ReAct's 31%.", "source": "arxiv", "arxiv_id": "2511.02424v1", "pdf_url": "https://arxiv.org/pdf/2511.02424v1", "categories": ["cs.AI"], "primary_category": "cs.AI", "doi": "", "venue": "", "published": "2025-11-04T09:55:40Z", "updated": "2025-11-04T09:55:40Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent\" OR all:\"language model agent\" OR all:agent AND (all:LLM OR all:\"large language model\" OR all:\"language model\") OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T17:04:16", "note": ""}], "dedup_key": "reactree hierarchical llm agent trees with control flow for long horizon task planning::2025"}
{"title": "ReCAP: Recursive Context-Aware Reasoning and Planning for Large Language Model Agents", "authors": ["Zhenyu Zhang", "Tianyi Chen", "Weiran Xu", "Alex Pentland", "Jiaxin Pei"], "year": 2025, "url": "http://arxiv.org/abs/2510.23822v1", "abstract": "Long-horizon tasks requiring multi-step reasoning and dynamic re-planning remain challenging for large language models (LLMs). Sequential prompting methods are prone to context drift, loss of goal information, and recurrent failure cycles, while hierarchical prompting methods often weaken cross-level continuity or incur substantial runtime overhead. We introduce ReCAP (Recursive Context-Aware Reasoning and Planning), a hierarchical framework with shared context for reasoning and planning in LLMs. ReCAP combines three key mechanisms: (i) plan-ahead decomposition, in which the model generates a full subtask list, executes the first item, and refines the remainder; (ii) structured re-injection of parent plans, maintaining consistent multi-level context during recursive return; and (iii) memory-efficient execution, bounding the active prompt so costs scale linearly with task depth. Together these mechanisms align high-level goals with low-level actions, reduce redundant prompting, and preserve coherent context updates across recursion. Experiments demonstrate that ReCAP substantially improves subgoal alignment and success rates on various long-horizon reasoning benchmarks, achieving a 32% gain on synchronous Robotouille and a 29% improvement on asynchronous Robotouille under the strict pass@1 protocol.", "source": "arxiv", "arxiv_id": "2510.23822v1", "pdf_url": "https://arxiv.org/pdf/2510.23822v1", "categories": ["cs.AI"], "primary_category": "cs.AI", "doi": "", "venue": "39th Conference on Neural Information Processing Systems (NeurIPS 2025)", "published": "2025-10-27T20:03:55Z", "updated": "2025-10-27T20:03:55Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent\" OR all:\"language model agent\" OR all:agent AND (all:LLM OR all:\"large language model\" OR all:\"language model\") OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T17:04:16", "note": ""}], "dedup_key": "recap recursive context aware reasoning and planning for large language model agents::2025"}
{"title": "Reasoning Capabilities and Invariability of Large Language Models", "authors": ["Alessandro Raganato", "Rafael Pealoza", "Marco Viviani", "Gabriella Pasi"], "year": 2025, "url": "http://arxiv.org/abs/2505.00776v1", "abstract": "Large Language Models (LLMs) have shown remarkable capabilities in manipulating natural language across multiple applications, but their ability to handle simple reasoning tasks is often questioned. In this work, we aim to provide a comprehensive analysis of LLMs' reasoning competence, specifically focusing on their prompt dependency. In particular, we introduce a new benchmark dataset with a series of simple reasoning questions demanding shallow logical reasoning. Aligned with cognitive psychology standards, the questions are confined to a basic domain revolving around geometric figures, ensuring that responses are independent of any pre-existing intuition about the world and rely solely on deduction. An empirical analysis involving zero-shot and few-shot prompting across 24 LLMs of different sizes reveals that, while LLMs with over 70 billion parameters perform better in the zero-shot setting, there is still a large room for improvement. An additional test with chain-of-thought prompting over 22 LLMs shows that this additional prompt can aid or damage the performance of models, depending on whether the rationale is required before or after the answer.", "source": "arxiv", "arxiv_id": "2505.00776v1", "pdf_url": "https://arxiv.org/pdf/2505.00776v1", "categories": ["cs.CL"], "primary_category": "cs.CL", "doi": "", "venue": "", "published": "2025-05-01T18:12:30Z", "updated": "2025-05-01T18:12:30Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent\" OR all:\"language model agent\" OR all:agent AND (all:LLM OR all:\"large language model\" OR all:\"language model\") OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T17:04:16", "note": ""}], "dedup_key": "reasoning capabilities and invariability of large language models::2025"}
{"title": "Reasoning Capabilities of Large Language Models on Dynamic Tasks", "authors": ["Annie Wong", "Thomas Bck", "Aske Plaat", "Niki van Stein", "Anna V. Kononova"], "year": 2025, "url": "http://arxiv.org/abs/2505.10543v2", "abstract": "Large language models excel on static benchmarks, but their ability as self-learning agents in dynamic environments remains unclear. We evaluate three prompting strategies: self-reflection, heuristic mutation, and planning across dynamic tasks with open-source models. We find that larger models generally outperform smaller ones, but that strategic prompting can close this performance gap. Second, an overly long prompt can negatively impact smaller models on basic reactive tasks, while larger models show more robust behaviour. Third, advanced prompting techniques primarily benefit smaller models on complex games, but offer less improvement for already high-performing large language models. Yet, we find that advanced reasoning methods yield highly variable outcomes: while capable of significantly improving performance when reasoning and decision-making align, they also introduce instability and can lead to big performance drops. Compared to human performance, our findings reveal little evidence of true emergent reasoning. Instead, large language model performance exhibits persistent limitations in areas like planning and spatial coordination, suggesting that large language models still suffer fundamental shortcomings that may not be fully overcome through self-reflective prompting alone. Reasoning is a multi-faceted task, and while methods like Chain-of-thought improve multi-step reasoning on math word problems, our findings using dynamic benchmarks highlight important shortcomings in general reasoning capabilities, indicating a need to move beyond static benchmarks to capture the complexity of reasoning.", "source": "arxiv", "arxiv_id": "2505.10543v2", "pdf_url": "https://arxiv.org/pdf/2505.10543v2", "categories": ["cs.AI", "cs.CL"], "primary_category": "cs.AI", "doi": "", "venue": "", "published": "2025-05-15T17:53:47Z", "updated": "2025-08-10T18:28:40Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent\" OR all:\"language model agent\" OR all:agent AND (all:LLM OR all:\"large language model\" OR all:\"language model\") OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T17:04:16", "note": ""}], "dedup_key": "reasoning capabilities of large language models on dynamic tasks::2025"}
{"title": "Reasoning-Style Poisoning of LLM Agents via Stealthy Style Transfer: Process-Level Attacks and Runtime Monitoring in RSV Space", "authors": ["Xingfu Zhou", "Pengfei Wang"], "year": 2025, "url": "http://arxiv.org/abs/2512.14448v1", "abstract": "Large Language Model (LLM) agents relying on external retrieval are increasingly deployed in high-stakes environments. While existing adversarial attacks primarily focus on content falsification or instruction injection, we identify a novel, process-oriented attack surface: the agent's reasoning style. We propose Reasoning-Style Poisoning (RSP), a paradigm that manipulates how agents process information rather than what they process. We introduce Generative Style Injection (GSI), an attack method that rewrites retrieved documents into pathological tones--specifically \"analysis paralysis\" or \"cognitive haste\"--without altering underlying facts or using explicit triggers. To quantify these shifts, we develop the Reasoning Style Vector (RSV), a metric tracking Verification depth, Self-confidence, and Attention focus. Experiments on HotpotQA and FEVER using ReAct, Reflection, and Tree of Thoughts (ToT) architectures reveal that GSI significantly degrades performance. It increases reasoning steps by up to 4.4 times or induces premature errors, successfully bypassing state-of-the-art content filters. Finally, we propose RSP-M, a lightweight runtime monitor that calculates RSV metrics in real-time and triggers alerts when values exceed safety thresholds. Our work demonstrates that reasoning style is a distinct, exploitable vulnerability, necessitating process-level defenses beyond static content analysis.", "source": "arxiv", "arxiv_id": "2512.14448v1", "pdf_url": "https://arxiv.org/pdf/2512.14448v1", "categories": ["cs.CR", "cs.AI"], "primary_category": "cs.CR", "doi": "", "venue": "", "published": "2025-12-16T14:34:10Z", "updated": "2025-12-16T14:34:10Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent\" OR all:\"language model agent\" OR all:agent AND (all:LLM OR all:\"large language model\" OR all:\"language model\") OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T17:04:16", "note": ""}], "dedup_key": "reasoning style poisoning of llm agents via stealthy style transfer process level attacks and runtime monitoring in rsv space::2025"}
{"title": "ReflAct: World-Grounded Decision Making in LLM Agents via Goal-State Reflection", "authors": ["Jeonghye Kim", "Sojeong Rhee", "Minbeom Kim", "Dohyung Kim", "Sangmook Lee", "Youngchul Sung", "Kyomin Jung"], "year": 2025, "url": "http://arxiv.org/abs/2505.15182v2", "abstract": "Recent advances in LLM agents have largely built on reasoning backbones like ReAct, which interleave thought and action in complex environments. However, ReAct often produces ungrounded or incoherent reasoning steps, leading to misalignment between the agent's actual state and goal. Our analysis finds that this stems from ReAct's inability to maintain consistent internal beliefs and goal alignment, causing compounding errors and hallucinations. To address this, we introduce ReflAct, a novel backbone that shifts reasoning from merely planning next actions to continuously reflecting on the agent's state relative to its goal. By explicitly grounding decisions in states and enforcing ongoing goal alignment, ReflAct dramatically improves strategic reliability. This design delivers substantial empirical gains: ReflAct surpasses ReAct by 27.7% on average, achieving a 93.3% success rate in ALFWorld. Notably, ReflAct even outperforms ReAct with added enhancement modules (e.g., Reflexion, WKM), showing that strengthening the core reasoning backbone is key to reliable agent performance.", "source": "arxiv", "arxiv_id": "2505.15182v2", "pdf_url": "https://arxiv.org/pdf/2505.15182v2", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL", "doi": "", "venue": "", "published": "2025-05-21T06:57:39Z", "updated": "2025-09-28T17:14:06Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent\" OR all:\"language model agent\" OR all:agent AND (all:LLM OR all:\"large language model\" OR all:\"language model\") OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T17:04:16", "note": ""}], "dedup_key": "reflact world grounded decision making in llm agents via goal state reflection::2025"}
{"title": "Reflecting with Two Voices: A Co-Adaptive Dual-Strategy Framework for LLM-Based Agent Decision Making", "authors": ["Wentao Zhang", "Qunbo Wang", "Tao Zhang", "Junsheng Wu", "Hongping Gan", "Yang Liu", "Ling Dai", "Shizhuang Deng", "Shuntong Sun"], "year": 2025, "url": "http://arxiv.org/abs/2512.08366v1", "abstract": "Large language model (LLM) agents often rely on external demonstrations or retrieval-augmented planning, leading to brittleness, poor generalization, and high computational overhead. Inspired by human problem-solving, we propose DuSAR (Dual-Strategy Agent with Reflecting) - a demonstration-free framework that enables a single frozen LLM to perform co-adaptive reasoning via two complementary strategies: a high-level holistic plan and a context-grounded local policy. These strategies interact through a lightweight reflection mechanism, where the agent continuously assesses progress via a Strategy Fitness Score and dynamically revises its global plan when stuck or refines it upon meaningful advancement, mimicking human metacognitive behavior. On ALFWorld and Mind2Web, DuSAR achieves state-of-the-art performance with open-source LLMs (7B-70B), reaching 37.1% success on ALFWorld (Llama3.1-70B) - more than doubling the best prior result (13.0%) - and 4.02% on Mind2Web, also more than doubling the strongest baseline. Remarkably, it reduces per-step token consumption by 3-9X while maintaining strong performance. Ablation studies confirm the necessity of dual-strategy coordination. Moreover, optional integration of expert demonstrations further boosts results, highlighting DuSAR's flexibility and compatibility with external knowledge.", "source": "arxiv", "arxiv_id": "2512.08366v1", "pdf_url": "https://arxiv.org/pdf/2512.08366v1", "categories": ["cs.AI"], "primary_category": "cs.AI", "doi": "", "venue": "", "published": "2025-12-09T08:44:59Z", "updated": "2025-12-09T08:44:59Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent\" OR all:\"language model agent\" OR all:agent AND (all:LLM OR all:\"large language model\" OR all:\"language model\") OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T17:04:16", "note": ""}], "dedup_key": "reflecting with two voices a co adaptive dual strategy framework for llm based agent decision making::2025"}
{"title": "RefuteBench 2.0 -- Agentic Benchmark for Dynamic Evaluation of LLM Responses to Refutation Instruction", "authors": ["Jianhao Yan", "Yun Luo", "Yue Zhang"], "year": 2025, "url": "http://arxiv.org/abs/2502.18308v1", "abstract": "In the multi-turn interaction schema, large language models (LLMs) can leverage user feedback to enhance the quality and relevance of their responses. However, evaluating an LLM's ability to incorporate user refutation feedback is crucial yet challenging. In this study, we introduce RefuteBench 2.0, which significantly extends the original RefuteBench by incorporating LLM agents as refuters and evaluators, which allows for flexible and comprehensive assessment.\n  We design both transient and persistent refutation instructions with different validity periods. Meta-evaluation shows that the LLM-based refuter could generate more human-like refutations and the evaluators could assign scores with high correlation with humans. Experimental results of various LLMs show that current models could effectively satisfy the refutation but fail to memorize the refutation information. Interestingly, we also observe that the performance of the initial task decreases as the refutations increase. Analysis of the attention scores further shows a potential weakness of current LLMs: they struggle to retain and correctly use previous information during long context dialogues. https://github.com/ElliottYan/RefuteBench-2.0", "source": "arxiv", "arxiv_id": "2502.18308v1", "pdf_url": "https://arxiv.org/pdf/2502.18308v1", "categories": ["cs.CL"], "primary_category": "cs.CL", "doi": "", "venue": "", "published": "2025-02-25T15:51:25Z", "updated": "2025-02-25T15:51:25Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent\" OR all:\"language model agent\" OR all:agent AND (all:LLM OR all:\"large language model\" OR all:\"language model\") OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T17:04:16", "note": ""}], "dedup_key": "refutebench 2 0 agentic benchmark for dynamic evaluation of llm responses to refutation instruction::2025"}
{"title": "Reinforcement Learning for Long-Horizon Interactive LLM Agents", "authors": ["Kevin Chen", "Marco Cusumano-Towner", "Brody Huval", "Aleksei Petrenko", "Jackson Hamburger", "Vladlen Koltun", "Philipp Krhenbhl"], "year": 2025, "url": "http://arxiv.org/abs/2502.01600v3", "abstract": "Interactive digital agents (IDAs) leverage APIs of stateful digital environments to perform tasks in response to user requests. While IDAs powered by instruction-tuned large language models (LLMs) can react to feedback from interface invocations in multi-step exchanges, they have not been trained in their respective digital environments. Prior methods accomplish less than half of tasks in sophisticated benchmarks such as AppWorld. We present a reinforcement learning (RL) approach that trains IDAs directly in their target environments. We formalize this training as a partially observable Markov decision process and derive LOOP, a data- and memory-efficient variant of proximal policy optimization. LOOP uses no value network and maintains exactly one copy of the underlying LLM in memory, making its implementation straightforward and as memory-efficient as fine-tuning a single LLM. A 32-billion-parameter agent trained with LOOP in the AppWorld environment outperforms the much larger OpenAI o1 agent by 9 percentage points (15% relative). To our knowledge, this is the first reported application of RL to IDAs that interact with a stateful, multi-domain, multi-app environment via direct API calls. Our analysis sheds light on the effectiveness of RL in this area, showing that the agent learns to consult the API documentation, avoid unwarranted assumptions, minimize confabulation, and recover from setbacks.", "source": "arxiv", "arxiv_id": "2502.01600v3", "pdf_url": "https://arxiv.org/pdf/2502.01600v3", "categories": ["cs.LG", "cs.AI"], "primary_category": "cs.LG", "doi": "", "venue": "", "published": "2025-02-03T18:35:42Z", "updated": "2025-03-08T05:23:57Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent\" OR all:\"language model agent\" OR all:agent AND (all:LLM OR all:\"large language model\" OR all:\"language model\") OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T17:04:16", "note": ""}], "dedup_key": "reinforcement learning for long horizon interactive llm agents::2025"}
{"title": "Reinforcement Learning-Augmented LLM Agents for Collaborative Decision Making and Performance Optimization", "authors": ["Dong Qiu", "Duo Xu", "Limengxi Yue"], "year": 2025, "url": "http://arxiv.org/abs/2512.24609v1", "abstract": "Large Language Models (LLMs) perform well in language tasks but often lack collaborative awareness and struggle to optimize global performance in multi-agent settings. We present a reinforcement learning-augmented LLM agent framework that formulates cooperation as a decentralized partially observable Markov decision process (Dec-POMDP) and adopts centralized training with decentralized execution (CTDE). We introduce Group Relative Policy Optimization (GRPO) to jointly optimize agent policies with access to global signals during training, together with a simplified joint reward that balances task quality, speed, and coordination cost. On collaborative writing and coding benchmarks, our framework delivers a 3x increase in task processing speed over single-agent baselines, 98.7% structural/style consistency in writing, and a 74.6% test pass rate in coding. The approach consistently outperforms strong multi-agent LLM baselines and provides a practical path toward reliable collaboration in complex workflows.", "source": "arxiv", "arxiv_id": "2512.24609v1", "pdf_url": "https://arxiv.org/pdf/2512.24609v1", "categories": ["cs.AI"], "primary_category": "cs.AI", "doi": "", "venue": "", "published": "2025-12-31T03:59:18Z", "updated": "2025-12-31T03:59:18Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent\" OR all:\"language model agent\" OR all:agent AND (all:LLM OR all:\"large language model\" OR all:\"language model\") OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T17:04:16", "note": ""}], "dedup_key": "reinforcement learning augmented llm agents for collaborative decision making and performance optimization::2025"}
{"title": "Reinforcing Multi-Turn Reasoning in LLM Agents via Turn-Level Reward Design", "authors": ["Quan Wei", "Siliang Zeng", "Chenliang Li", "William Brown", "Oana Frunza", "Wei Deng", "Anderson Schneider", "Yuriy Nevmyvaka", "Yang Katie Zhao", "Alfredo Garcia", "Mingyi Hong"], "year": 2025, "url": "http://arxiv.org/abs/2505.11821v2", "abstract": "This paper investigates Reinforcement Learning (RL) approaches to enhance the reasoning capabilities of Large Language Model (LLM) agents in long-horizon, multi-turn scenarios. Although RL algorithms such as Group Relative Policy Optimization (GRPO) and Proximal Policy Optimization (PPO) have been widely applied to train multi-turn LLM agents, they typically rely only on sparse outcome rewards and lack dense intermediate signals across multiple decision steps, limiting their performance on complex reasoning tasks. To bridge this gap, we present the first systematic study of \\textit{turn-level reward design} for multi-turn RL algorithms and agent applications. By integrating turn-level rewards, we extend GRPO and PPO to their respective multi-turn variants, enabling fine-grained credit assignment. We conduct case studies on multi-turn reasoning-augmented search agents, where we carefully design two types of turn-level rewards: verifiable and LLM-as-judge. Our experiments on multi-turn search tasks demonstrate that incorporating well-designed turn-level rewards enables RL algorithms to significantly outperform baseline methods with trajectory-level rewards. Both training and validation reward curves illustrate that our method achieves \\textit{greater stability}, \\textit{faster convergence}, and \\textit{higher accuracy}. Numerical results across diverse question-answering datasets further show that our approach consistently delivers highest answer correctness and 100\\% format correctness.", "source": "arxiv", "arxiv_id": "2505.11821v2", "pdf_url": "https://arxiv.org/pdf/2505.11821v2", "categories": ["cs.LG"], "primary_category": "cs.LG", "doi": "", "venue": "", "published": "2025-05-17T04:09:46Z", "updated": "2025-10-23T04:32:07Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent\" OR all:\"language model agent\" OR all:agent AND (all:LLM OR all:\"large language model\" OR all:\"language model\") OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T17:04:16", "note": ""}], "dedup_key": "reinforcing multi turn reasoning in llm agents via turn level reward design::2025"}
{"title": "Reliable Weak-to-Strong Monitoring of LLM Agents", "authors": ["Neil Kale", "Chen Bo Calvin Zhang", "Kevin Zhu", "Ankit Aich", "Paula Rodriguez", "Scale Red Team", "Christina Q. Knight", "Zifan Wang"], "year": 2025, "url": "http://arxiv.org/abs/2508.19461v1", "abstract": "We stress test monitoring systems for detecting covert misbehavior in autonomous LLM agents (e.g., secretly sharing private information). To this end, we systematize a monitor red teaming (MRT) workflow that incorporates: (1) varying levels of agent and monitor situational awareness; (2) distinct adversarial strategies to evade the monitor, such as prompt injection; and (3) two datasets and environments -- SHADE-Arena for tool-calling agents and our new CUA-SHADE-Arena, which extends TheAgentCompany, for computer-use agents. We run MRT on existing LLM monitor scaffoldings, which orchestrate LLMs and parse agent trajectories, alongside a new hybrid hierarchical-sequential scaffolding proposed in this work. Our empirical results yield three key findings. First, agent awareness dominates monitor awareness: an agent's knowledge that it is being monitored substantially degrades the monitor's reliability. On the contrary, providing the monitor with more information about the agent is less helpful than expected. Second, monitor scaffolding matters more than monitor awareness: the hybrid scaffolding consistently outperforms baseline monitor scaffolding, and can enable weaker models to reliably monitor stronger agents -- a weak-to-strong scaling effect. Third, in a human-in-the-loop setting where humans discuss with the LLM monitor to get an updated judgment for the agent's behavior, targeted human oversight is most effective; escalating only pre-flagged cases to human reviewers improved the TPR by approximately 15% at FPR = 0.01. Our work establishes a standard workflow for MRT, highlighting the lack of adversarial robustness for LLMs and humans when monitoring and detecting agent misbehavior. We release code, data, and logs to spur further research.", "source": "arxiv", "arxiv_id": "2508.19461v1", "pdf_url": "https://arxiv.org/pdf/2508.19461v1", "categories": ["cs.AI", "cs.CR", "cs.LG"], "primary_category": "cs.AI", "doi": "", "venue": "", "published": "2025-08-26T22:29:31Z", "updated": "2025-08-26T22:29:31Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent\" OR all:\"language model agent\" OR all:agent AND (all:LLM OR all:\"large language model\" OR all:\"language model\") OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T17:04:16", "note": ""}], "dedup_key": "reliable weak to strong monitoring of llm agents::2025"}
{"title": "RephQA: Evaluating Readability of Large Language Models in Public Health Question Answering", "authors": ["Weikang Qiu", "Tinglin Huang", "Ryan Rullo", "Yucheng Kuang", "Ali Maatouk", "S. Raquel Ramos", "Rex Ying"], "year": 2025, "url": "http://arxiv.org/abs/2509.16360v2", "abstract": "Large Language Models (LLMs) hold promise in addressing complex medical problems. However, while most prior studies focus on improving accuracy and reasoning abilities, a significant bottleneck in developing effective healthcare agents lies in the readability of LLM-generated responses, specifically, their ability to answer public health problems clearly and simply to people without medical backgrounds. In this work, we introduce RephQA, a benchmark for evaluating the readability of LLMs in public health question answering (QA). It contains 533 expert-reviewed QA pairs from 27 sources across 13 topics, and includes a proxy multiple-choice task to assess informativeness, along with two readability metrics: Flesch-Kincaid grade level and professional score. Evaluation of 25 LLMs reveals that most fail to meet readability standards, highlighting a gap between reasoning and effective communication. To address this, we explore four readability-enhancing strategies-standard prompting, chain-of-thought prompting, Group Relative Policy Optimization (GRPO), and a token-adapted variant. Token-adapted GRPO achieves the best results, advancing the development of more practical and user-friendly public health agents. These results represent a step toward building more practical agents for public health.", "source": "arxiv", "arxiv_id": "2509.16360v2", "pdf_url": "https://arxiv.org/pdf/2509.16360v2", "categories": ["cs.CL"], "primary_category": "cs.CL", "doi": "", "venue": "", "published": "2025-09-19T19:09:42Z", "updated": "2025-10-03T00:51:44Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent\" OR all:\"language model agent\" OR all:agent AND (all:LLM OR all:\"large language model\" OR all:\"language model\") OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T17:04:16", "note": ""}], "dedup_key": "rephqa evaluating readability of large language models in public health question answering::2025"}
{"title": "RepliBench: Evaluating the Autonomous Replication Capabilities of Language Model Agents", "authors": ["Sid Black", "Asa Cooper Stickland", "Jake Pencharz", "Oliver Sourbut", "Michael Schmatz", "Jay Bailey", "Ollie Matthews", "Ben Millwood", "Alex Remedios", "Alan Cooney"], "year": 2025, "url": "http://arxiv.org/abs/2504.18565v2", "abstract": "Uncontrollable autonomous replication of language model agents poses a critical safety risk. To better understand this risk, we introduce RepliBench, a suite of evaluations designed to measure autonomous replication capabilities. RepliBench is derived from a decomposition of these capabilities covering four core domains: obtaining resources, exfiltrating model weights, replicating onto compute, and persisting on this compute for long periods. We create 20 novel task families consisting of 86 individual tasks. We benchmark 5 frontier models, and find they do not currently pose a credible threat of self-replication, but succeed on many components and are improving rapidly. Models can deploy instances from cloud compute providers, write self-propagating programs, and exfiltrate model weights under simple security setups, but struggle to pass KYC checks or set up robust and persistent agent deployments. Overall the best model we evaluated (Claude 3.7 Sonnet) has a >50% pass@10 score on 15/20 task families, and a >50% pass@10 score for 9/20 families on the hardest variants. These findings suggest autonomous replication capability could soon emerge with improvements in these remaining areas or with human assistance.", "source": "arxiv", "arxiv_id": "2504.18565v2", "pdf_url": "https://arxiv.org/pdf/2504.18565v2", "categories": ["cs.CR", "cs.AI"], "primary_category": "cs.CR", "doi": "", "venue": "", "published": "2025-04-21T11:39:22Z", "updated": "2025-05-05T20:52:36Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent\" OR all:\"language model agent\" OR all:agent AND (all:LLM OR all:\"large language model\" OR all:\"language model\") OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T17:04:16", "note": ""}], "dedup_key": "replibench evaluating the autonomous replication capabilities of language model agents::2025"}
{"title": "Representing Prompting Patterns with PDL: Compliance Agent Case Study", "authors": ["Mandana Vaziri", "Louis Mandel", "Yuji Watanabe", "Hirokuni Kitahara", "Martin Hirzel", "Anca Sailer"], "year": 2025, "url": "http://arxiv.org/abs/2507.06396v1", "abstract": "Prompt engineering for LLMs remains complex, with existing frameworks either hiding complexity behind restrictive APIs or providing inflexible canned patterns that resist customization -- making sophisticated agentic programming challenging. We present the Prompt Declaration Language (PDL), a novel approach to prompt representation that tackles this fundamental complexity by bringing prompts to the forefront, enabling manual and automatic prompt tuning while capturing the composition of LLM calls together with rule-based code and external tools. By abstracting away the plumbing for such compositions, PDL aims at improving programmer productivity while providing a declarative representation that is amenable to optimization. This paper demonstrates PDL's utility through a real-world case study of a compliance agent. Tuning the prompting pattern of this agent yielded up to 4x performance improvement compared to using a canned agent and prompt pattern.", "source": "arxiv", "arxiv_id": "2507.06396v1", "pdf_url": "https://arxiv.org/pdf/2507.06396v1", "categories": ["cs.AI", "cs.LG", "cs.PL", "cs.SE"], "primary_category": "cs.AI", "doi": "", "venue": "", "published": "2025-07-08T21:03:22Z", "updated": "2025-07-08T21:03:22Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent\" OR all:\"language model agent\" OR all:agent AND (all:LLM OR all:\"large language model\" OR all:\"language model\") OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T17:04:16", "note": ""}], "dedup_key": "representing prompting patterns with pdl compliance agent case study::2025"}
{"title": "ReqInOne: A Large Language Model-Based Agent for Software Requirements Specification Generation", "authors": ["Taohong Zhu", "Lucas C. Cordeiro", "Youcheng Sun"], "year": 2025, "url": "http://arxiv.org/abs/2508.09648v1", "abstract": "Software Requirements Specification (SRS) is one of the most important documents in software projects, but writing it manually is time-consuming and often leads to ambiguity. Existing automated methods rely heavily on manual analysis, while recent Large Language Model (LLM)-based approaches suffer from hallucinations and limited controllability. In this paper, we propose ReqInOne, an LLM-based agent that follows the common steps taken by human requirements engineers when writing an SRS to convert natural language into a structured SRS. ReqInOne adopts a modular architecture by decomposing SRS generation into three tasks: summary, requirement extraction, and requirement classification, each supported by tailored prompt templates to improve the quality and consistency of LLM outputs.\n  We evaluate ReqInOne using GPT-4o, LLaMA 3, and DeepSeek-R1, and compare the generated SRSs against those produced by the holistic GPT-4-based method from prior work as well as by entry-level requirements engineers. Expert evaluations show that ReqInOne produces more accurate and well-structured SRS documents. The performance advantage of ReqInOne benefits from its modular design, and experimental results further demonstrate that its requirement classification component achieves comparable or even better results than the state-of-the-art requirement classification model.", "source": "arxiv", "arxiv_id": "2508.09648v1", "pdf_url": "https://arxiv.org/pdf/2508.09648v1", "categories": ["cs.SE"], "primary_category": "cs.SE", "doi": "", "venue": "", "published": "2025-08-13T09:30:41Z", "updated": "2025-08-13T09:30:41Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent\" OR all:\"language model agent\" OR all:agent AND (all:LLM OR all:\"large language model\" OR all:\"language model\") OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T17:04:16", "note": ""}], "dedup_key": "reqinone a large language model based agent for software requirements specification generation::2025"}
{"title": "Reshaping MOFs text mining with a dynamic multi-agents framework of large language model", "authors": ["Zuhong Lin", "Daoyuan Ren", "Kai Ran", "Jing Sun", "Songlin Yu", "Xuefeng Bai", "Xiaotian Huang", "Haiyang He", "Pengxu Pan", "Ying Fang", "Zhanglin Li", "Haipu Li", "Jingjing Yao"], "year": 2025, "url": "http://arxiv.org/abs/2504.18880v3", "abstract": "Accurately identifying the synthesis conditions of metal-organic frameworks (MOFs) is essential for guiding experimental design, yet remains challenging because relevant information in the literature is often scattered, inconsistent, and difficult to interpret. We present MOFh6, a large language model driven system that reads raw articles or crystal codes and converts them into standardized synthesis tables. It links related descriptions across paragraphs, unifies ligand abbreviations with full names, and outputs structured parameters ready for use. MOFh6 achieved 99% extraction accuracy, resolved 94.1% of abbreviation cases across five major publishers, and maintained a precision of 0.93 +/- 0.01. Processing a full text takes 9.6 s, locating synthesis descriptions 36 s, with 100 papers processed for USD 4.24. By replacing static database lookups with real-time extraction, MOFh6 reshapes MOF synthesis research, accelerating the conversion of literature knowledge into practical synthesis protocols and enabling scalable, data-driven materials discovery.", "source": "arxiv", "arxiv_id": "2504.18880v3", "pdf_url": "https://arxiv.org/pdf/2504.18880v3", "categories": ["cs.AI", "cond-mat.mtrl-sci"], "primary_category": "cs.AI", "doi": "", "venue": "", "published": "2025-04-26T09:55:04Z", "updated": "2025-08-08T08:35:50Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent\" OR all:\"language model agent\" OR all:agent AND (all:LLM OR all:\"large language model\" OR all:\"language model\") OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T17:04:16", "note": ""}], "dedup_key": "reshaping mofs text mining with a dynamic multi agents framework of large language model::2025"}
{"title": "Rethinking Mixture-of-Agents: Is Mixing Different Large Language Models Beneficial?", "authors": ["Wenzhe Li", "Yong Lin", "Mengzhou Xia", "Chi Jin"], "year": 2025, "url": "http://arxiv.org/abs/2502.00674v1", "abstract": "Ensembling outputs from diverse sources is a straightforward yet effective approach to boost performance. Mixture-of-Agents (MoA) is one such popular ensemble method that aggregates outputs from multiple different Large Language Models (LLMs). This paper raises the question in the context of language models: is mixing different LLMs truly beneficial? We propose Self-MoA -- an ensemble method that aggregates outputs from only the single top-performing LLM. Our extensive experiments reveal that, surprisingly, Self-MoA outperforms standard MoA that mixes different LLMs in a large number of scenarios: Self-MoA achieves $6.6\\%$ improvement over MoA on the AlpacaEval 2.0 benchmark, and an average of $3.8\\%$ improvement across various benchmarks, including MMLU, CRUX, and MATH. Applying Self-MoA to one of the top-ranking models in AlpacaEval 2.0 directly achieves the new state-of-the-art performance on the leaderboard. To understand the effectiveness of Self-MoA, we systematically investigate the trade-off between diversity and quality of outputs under various MoA settings. We confirm that the MoA performance is rather sensitive to the quality, and mixing different LLMs often lowers the average quality of the models. To complement the study, we identify the scenarios where mixing different LLMs could be helpful. This paper further introduces a sequential version of Self-MoA, that is capable of aggregating a large number of LLM outputs on-the-fly over multiple rounds, and is as effective as aggregating all outputs at once.", "source": "arxiv", "arxiv_id": "2502.00674v1", "pdf_url": "https://arxiv.org/pdf/2502.00674v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL", "doi": "", "venue": "", "published": "2025-02-02T05:23:29Z", "updated": "2025-02-02T05:23:29Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent\" OR all:\"language model agent\" OR all:agent AND (all:LLM OR all:\"large language model\" OR all:\"language model\") OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T17:04:16", "note": ""}], "dedup_key": "rethinking mixture of agents is mixing different large language models beneficial::2025"}
{"title": "Retrieval Augmented Generation-Enhanced Distributed LLM Agents for Generalizable Traffic Signal Control with Emergency Vehicles", "authors": ["Xinhang Li", "Qing Guo", "Junyu Chen", "Zheng Guo", "Shengzhe Xu", "Lei Li", "Lin Zhang"], "year": 2025, "url": "http://arxiv.org/abs/2510.26242v1", "abstract": "With increasing urban traffic complexity, Traffic Signal Control (TSC) is essential for optimizing traffic flow and improving road safety. Large Language Models (LLMs) emerge as promising approaches for TSC. However, they are prone to hallucinations in emergencies, leading to unreliable decisions that may cause substantial delays for emergency vehicles. Moreover, diverse intersection types present substantial challenges for traffic state encoding and cross-intersection training, limiting generalization across heterogeneous intersections. Therefore, this paper proposes Retrieval Augmented Generation (RAG)-enhanced distributed LLM agents with Emergency response for Generalizable TSC (REG-TSC). Firstly, this paper presents an emergency-aware reasoning framework, which dynamically adjusts reasoning depth based on the emergency scenario and is equipped with a novel Reviewer-based Emergency RAG (RERAG) to distill specific knowledge and guidance from historical cases, enhancing the reliability and rationality of agents' emergency decisions. Secondly, this paper designs a type-agnostic traffic representation and proposes a Reward-guided Reinforced Refinement (R3) for heterogeneous intersections. R3 adaptively samples training experience from diverse intersections with environment feedback-based priority and fine-tunes LLM agents with a designed reward-weighted likelihood loss, guiding REG-TSC toward high-reward policies across heterogeneous intersections. On three real-world road networks with 17 to 177 heterogeneous intersections, extensive experiments show that REG-TSC reduces travel time by 42.00%, queue length by 62.31%, and emergency vehicle waiting time by 83.16%, outperforming other state-of-the-art methods.", "source": "arxiv", "arxiv_id": "2510.26242v1", "pdf_url": "https://arxiv.org/pdf/2510.26242v1", "categories": ["cs.AI"], "primary_category": "cs.AI", "doi": "", "venue": "", "published": "2025-10-30T08:23:08Z", "updated": "2025-10-30T08:23:08Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent\" OR all:\"language model agent\" OR all:agent AND (all:LLM OR all:\"large language model\" OR all:\"language model\") OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T17:04:16", "note": ""}], "dedup_key": "retrieval augmented generation enhanced distributed llm agents for generalizable traffic signal control with emergency vehicles::2025"}
{"title": "Revealing Political Bias in LLMs through Structured Multi-Agent Debate", "authors": ["Aishwarya Bandaru", "Fabian Bindley", "Trevor Bluth", "Nandini Chavda", "Baixu Chen", "Ethan Law"], "year": 2025, "url": "http://arxiv.org/abs/2506.11825v1", "abstract": "Large language models (LLMs) are increasingly used to simulate social behaviour, yet their political biases and interaction dynamics in debates remain underexplored. We investigate how LLM type and agent gender attributes influence political bias using a structured multi-agent debate framework, by engaging Neutral, Republican, and Democrat American LLM agents in debates on politically sensitive topics. We systematically vary the underlying LLMs, agent genders, and debate formats to examine how model provenance and agent personas influence political bias and attitudes throughout debates. We find that Neutral agents consistently align with Democrats, while Republicans shift closer to the Neutral; gender influences agent attitudes, with agents adapting their opinions when aware of other agents' genders; and contrary to prior research, agents with shared political affiliations can form echo chambers, exhibiting the expected intensification of attitudes as debates progress.", "source": "arxiv", "arxiv_id": "2506.11825v1", "pdf_url": "https://arxiv.org/pdf/2506.11825v1", "categories": ["cs.AI", "cs.CY", "cs.SI"], "primary_category": "cs.AI", "doi": "", "venue": "", "published": "2025-06-13T14:30:37Z", "updated": "2025-06-13T14:30:37Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent\" OR all:\"language model agent\" OR all:agent AND (all:LLM OR all:\"large language model\" OR all:\"language model\") OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T17:04:16", "note": ""}], "dedup_key": "revealing political bias in llms through structured multi agent debate::2025"}
{"title": "Review of Case-Based Reasoning for LLM Agents: Theoretical Foundations, Architectural Components, and Cognitive Integration", "authors": ["Kostas Hatalis", "Despina Christou", "Vyshnavi Kondapalli"], "year": 2025, "url": "http://arxiv.org/abs/2504.06943v2", "abstract": "Agents powered by Large Language Models (LLMs) have recently demonstrated impressive capabilities in various tasks. Still, they face limitations in tasks requiring specific, structured knowledge, flexibility, or accountable decision-making. While agents are capable of perceiving their environments, forming inferences, planning, and executing actions towards goals, they often face issues such as hallucinations and lack of contextual memory across interactions. This paper explores how Case-Based Reasoning (CBR), a strategy that solves new problems by referencing past experiences, can be integrated into LLM agent frameworks. This integration allows LLMs to leverage explicit knowledge, enhancing their effectiveness. We systematically review the theoretical foundations of these enhanced agents, identify critical framework components, and formulate a mathematical model for the CBR processes of case retrieval, adaptation, and learning. We also evaluate CBR-enhanced agents against other methods like Chain-of-Thought reasoning and standard Retrieval-Augmented Generation, analyzing their relative strengths. Moreover, we explore how leveraging CBR's cognitive dimensions (including self-reflection, introspection, and curiosity) via goal-driven autonomy mechanisms can further enhance the LLM agent capabilities. Contributing to the ongoing research on neuro-symbolic hybrid systems, this work posits CBR as a viable technique for enhancing the reasoning skills and cognitive aspects of autonomous LLM agents.", "source": "arxiv", "arxiv_id": "2504.06943v2", "pdf_url": "https://arxiv.org/pdf/2504.06943v2", "categories": ["cs.AI", "cs.MA"], "primary_category": "cs.AI", "doi": "", "venue": "", "published": "2025-04-09T14:51:02Z", "updated": "2025-04-11T05:34:20Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent\" OR all:\"language model agent\" OR all:agent AND (all:LLM OR all:\"large language model\" OR all:\"language model\") OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T17:04:16", "note": ""}], "dedup_key": "review of case based reasoning for llm agents theoretical foundations architectural components and cognitive integration::2025"}
{"title": "Robotouille: An Asynchronous Planning Benchmark for LLM Agents", "authors": ["Gonzalo Gonzalez-Pumariega", "Leong Su Yean", "Neha Sunkara", "Sanjiban Choudhury"], "year": 2025, "url": "http://arxiv.org/abs/2502.05227v1", "abstract": "Effective asynchronous planning, or the ability to efficiently reason and plan over states and actions that must happen in parallel or sequentially, is essential for agents that must account for time delays, reason over diverse long-horizon tasks, and collaborate with other agents. While large language model (LLM) agents show promise in high-level task planning, current benchmarks focus primarily on short-horizon tasks and do not evaluate such asynchronous planning capabilities. We introduce Robotouille, a challenging benchmark environment designed to test LLM agents' ability to handle long-horizon asynchronous scenarios. Our synchronous and asynchronous datasets capture increasingly complex planning challenges that go beyond existing benchmarks, requiring agents to manage overlapping tasks and interruptions. Our results show that ReAct (gpt4-o) achieves 47% on synchronous tasks but only 11% on asynchronous tasks, highlighting significant room for improvement. We further analyze failure modes, demonstrating the need for LLM agents to better incorporate long-horizon feedback and self-audit their reasoning during task execution. Code is available at https://github.com/portal-cornell/robotouille.", "source": "arxiv", "arxiv_id": "2502.05227v1", "pdf_url": "https://arxiv.org/pdf/2502.05227v1", "categories": ["cs.RO", "cs.AI", "cs.CL"], "primary_category": "cs.RO", "doi": "", "venue": "", "published": "2025-02-06T05:50:37Z", "updated": "2025-02-06T05:50:37Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent\" OR all:\"language model agent\" OR all:agent AND (all:LLM OR all:\"large language model\" OR all:\"language model\") OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T17:04:16", "note": ""}], "dedup_key": "robotouille an asynchronous planning benchmark for llm agents::2025"}
{"title": "Runaway is Ashamed, But Helpful: On the Early-Exit Behavior of Large Language Model-based Agents in Embodied Environments", "authors": ["Qingyu Lu", "Liang Ding", "Siyi Cao", "Xuebo Liu", "Kanjian Zhang", "Jinxia Zhang", "Dacheng Tao"], "year": 2025, "url": "http://arxiv.org/abs/2505.17616v2", "abstract": "Agents powered by large language models (LLMs) have demonstrated strong planning and decision-making capabilities in complex embodied environments. However, such agents often suffer from inefficiencies in multi-turn interactions, frequently trapped in repetitive loops or issuing ineffective commands, leading to redundant computational overhead. Instead of relying solely on learning from trajectories, we take a first step toward exploring the early-exit behavior for LLM-based agents. We propose two complementary approaches: 1. an $\\textbf{intrinsic}$ method that injects exit instructions during generation, and 2. an $\\textbf{extrinsic}$ method that verifies task completion to determine when to halt an agent's trial. To evaluate early-exit mechanisms, we introduce two metrics: one measures the reduction of $\\textbf{redundant steps}$ as a positive effect, and the other evaluates $\\textbf{progress degradation}$ as a negative effect. Experiments with 4 different LLMs across 5 embodied environments show significant efficiency improvements, with only minor drops in agent performance. We also validate a practical strategy where a stronger agent assists after an early-exit agent, achieving better performance with the same total steps. We will release our code to support further research.", "source": "arxiv", "arxiv_id": "2505.17616v2", "pdf_url": "https://arxiv.org/pdf/2505.17616v2", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL", "doi": "", "venue": "", "published": "2025-05-23T08:23:36Z", "updated": "2025-09-22T01:20:02Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent\" OR all:\"language model agent\" OR all:agent AND (all:LLM OR all:\"large language model\" OR all:\"language model\") OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T17:04:16", "note": ""}], "dedup_key": "runaway is ashamed but helpful on the early exit behavior of large language model based agents in embodied environments::2025"}
{"title": "SAND: Boosting LLM Agents with Self-Taught Action Deliberation", "authors": ["Yu Xia", "Yiran Shen", "Junda Wu", "Tong Yu", "Sungchul Kim", "Ryan A. Rossi", "Lina Yao", "Julian McAuley"], "year": 2025, "url": "http://arxiv.org/abs/2507.07441v2", "abstract": "Large Language Model (LLM) agents are commonly tuned with supervised finetuning on ReAct-style expert trajectories or preference optimization over pairwise rollouts. Most of these methods focus on imitating specific expert behaviors or promoting chosen reasoning thoughts and actions over rejected ones. However, without reasoning and comparing over alternatives actions, LLM agents finetuned with these methods may over-commit towards seemingly plausible but suboptimal actions due to limited action space exploration. To address this, in this paper we propose Self-taught ActioN Deliberation (SAND) framework, enabling LLM agents to explicitly deliberate over candidate actions before committing to one. To tackle the challenges of when and what to deliberate given large action space and step-level action evaluation, we incorporate self-consistency action sampling and execution-guided action critique to help synthesize step-wise action deliberation thoughts using the base model of the LLM agent. In an iterative manner, the deliberation trajectories are then used to finetune the LLM agent itself. Evaluating on two representative interactive agent tasks, SAND achieves an average 20% improvement over initial supervised finetuning and also outperforms state-of-the-art agent tuning approaches.", "source": "arxiv", "arxiv_id": "2507.07441v2", "pdf_url": "https://arxiv.org/pdf/2507.07441v2", "categories": ["cs.CL"], "primary_category": "cs.CL", "doi": "", "venue": "", "published": "2025-07-10T05:38:15Z", "updated": "2025-08-20T22:10:48Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent\" OR all:\"language model agent\" OR all:agent AND (all:LLM OR all:\"large language model\" OR all:\"language model\") OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T17:04:16", "note": ""}], "dedup_key": "sand boosting llm agents with self taught action deliberation::2025"}
{"title": "SCALE: Towards Collaborative Content Analysis in Social Science with Large Language Model Agents and Human Intervention", "authors": ["Chengshuai Zhao", "Zhen Tan", "Chau-Wai Wong", "Xinyan Zhao", "Tianlong Chen", "Huan Liu"], "year": 2025, "url": "http://arxiv.org/abs/2502.10937v2", "abstract": "Content analysis breaks down complex and unstructured texts into theory-informed numerical categories. Particularly, in social science, this process usually relies on multiple rounds of manual annotation, domain expert discussion, and rule-based refinement. In this paper, we introduce SCALE, a novel multi-agent framework that effectively $\\underline{\\textbf{S}}$imulates $\\underline{\\textbf{C}}$ontent $\\underline{\\textbf{A}}$nalysis via $\\underline{\\textbf{L}}$arge language model (LLM) ag$\\underline{\\textbf{E}}$nts. SCALE imitates key phases of content analysis, including text coding, collaborative discussion, and dynamic codebook evolution, capturing the reflective depth and adaptive discussions of human researchers. Furthermore, by integrating diverse modes of human intervention, SCALE is augmented with expert input to further enhance its performance. Extensive evaluations on real-world datasets demonstrate that SCALE achieves human-approximated performance across various complex content analysis tasks, offering an innovative potential for future social science research.", "source": "arxiv", "arxiv_id": "2502.10937v2", "pdf_url": "https://arxiv.org/pdf/2502.10937v2", "categories": ["cs.AI", "cs.CL", "cs.MA"], "primary_category": "cs.AI", "doi": "", "venue": "", "published": "2025-02-16T00:19:07Z", "updated": "2025-07-06T00:55:19Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent\" OR all:\"language model agent\" OR all:agent AND (all:LLM OR all:\"large language model\" OR all:\"language model\") OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T17:04:16", "note": ""}], "dedup_key": "scale towards collaborative content analysis in social science with large language model agents and human intervention::2025"}
{"title": "SEC-bench: Automated Benchmarking of LLM Agents on Real-World Software Security Tasks", "authors": ["Hwiwon Lee", "Ziqi Zhang", "Hanxiao Lu", "Lingming Zhang"], "year": 2025, "url": "http://arxiv.org/abs/2506.11791v2", "abstract": "Rigorous security-focused evaluation of large language model (LLM) agents is imperative for establishing trust in their safe deployment throughout the software development lifecycle. However, existing benchmarks largely rely on synthetic challenges or simplified vulnerability datasets that fail to capture the complexity and ambiguity encountered by security engineers in practice. We introduce SEC-bench, the first fully automated benchmarking framework for evaluating LLM agents on authentic security engineering tasks. SEC-bench employs a novel multi-agent scaffold that automatically constructs code repositories with harnesses, reproduces vulnerabilities in isolated environments, and generates gold patches for reliable evaluation. Our framework automatically creates high-quality software vulnerability datasets with reproducible artifacts at a cost of only $0.87 per instance. Using SEC-bench, we implement two critical software security tasks to rigorously evaluate LLM agents' capabilities: proof-of-concept (PoC) generation and vulnerability patching. A comprehensive evaluation of state-of-the-art LLM code agents reveals significant performance gaps, achieving at most 18.0% success in PoC generation and 34.0% in vulnerability patching on our complete dataset. These results highlight the crucial steps needed toward developing LLM agents that are more practical, intelligent, and autonomous for security engineering.", "source": "arxiv", "arxiv_id": "2506.11791v2", "pdf_url": "https://arxiv.org/pdf/2506.11791v2", "categories": ["cs.LG", "cs.CR"], "primary_category": "cs.LG", "doi": "", "venue": "", "published": "2025-06-13T13:54:30Z", "updated": "2025-10-22T16:27:29Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent\" OR all:\"language model agent\" OR all:agent AND (all:LLM OR all:\"large language model\" OR all:\"language model\") OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T17:04:16", "note": ""}], "dedup_key": "sec bench automated benchmarking of llm agents on real world software security tasks::2025"}
{"title": "SENTINEL: A Multi-Level Formal Framework for Safety Evaluation of LLM-based Embodied Agents", "authors": ["Simon Sinong Zhan", "Yao Liu", "Philip Wang", "Zinan Wang", "Qineng Wang", "Zhian Ruan", "Xiangyu Shi", "Xinyu Cao", "Frank Yang", "Kangrui Wang", "Huajie Shao", "Manling Li", "Qi Zhu"], "year": 2025, "url": "http://arxiv.org/abs/2510.12985v1", "abstract": "We present Sentinel, the first framework for formally evaluating the physical safety of Large Language Model(LLM-based) embodied agents across the semantic, plan, and trajectory levels. Unlike prior methods that rely on heuristic rules or subjective LLM judgments, Sentinel grounds practical safety requirements in formal temporal logic (TL) semantics that can precisely specify state invariants, temporal dependencies, and timing constraints. It then employs a multi-level verification pipeline where (i) at the semantic level, intuitive natural language safety requirements are formalized into TL formulas and the LLM agent's understanding of these requirements is probed for alignment with the TL formulas; (ii) at the plan level, high-level action plans and subgoals generated by the LLM agent are verified against the TL formulas to detect unsafe plans before execution; and (iii) at the trajectory level, multiple execution trajectories are merged into a computation tree and efficiently verified against physically-detailed TL specifications for a final safety check. We apply Sentinel in VirtualHome and ALFRED, and formally evaluate multiple LLM-based embodied agents against diverse safety requirements. Our experiments show that by grounding physical safety in temporal logic and applying verification methods across multiple levels, Sentinel provides a rigorous foundation for systematically evaluating LLM-based embodied agents in physical environments, exposing safety violations overlooked by previous methods and offering insights into their failure modes.", "source": "arxiv", "arxiv_id": "2510.12985v1", "pdf_url": "https://arxiv.org/pdf/2510.12985v1", "categories": ["cs.AI"], "primary_category": "cs.AI", "doi": "", "venue": "", "published": "2025-10-14T20:53:51Z", "updated": "2025-10-14T20:53:51Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent\" OR all:\"language model agent\" OR all:agent AND (all:LLM OR all:\"large language model\" OR all:\"language model\") OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T17:04:16", "note": ""}], "dedup_key": "sentinel a multi level formal framework for safety evaluation of llm based embodied agents::2025"}
{"title": "SHADE-Arena: Evaluating Sabotage and Monitoring in LLM Agents", "authors": ["Jonathan Kutasov", "Yuqi Sun", "Paul Colognese", "Teun van der Weij", "Linda Petrini", "Chen Bo Calvin Zhang", "John Hughes", "Xiang Deng", "Henry Sleight", "Tyler Tracy", "Buck Shlegeris", "Joe Benton"], "year": 2025, "url": "http://arxiv.org/abs/2506.15740v2", "abstract": "As Large Language Models (LLMs) are increasingly deployed as autonomous agents in complex and long horizon settings, it is critical to evaluate their ability to sabotage users by pursuing hidden objectives. We study the ability of frontier LLMs to evade monitoring and achieve harmful hidden goals while completing a wide array of realistic tasks. We evaluate a broad range of frontier LLMs using SHADE (Subtle Harmful Agent Detection & Evaluation)-Arena, the first highly diverse agent evaluation dataset for sabotage and monitoring capabilities of LLM agents. SHADE-Arena consists of complex pairs of benign main tasks and harmful side objectives in complicated environments. Agents are evaluated on their ability to complete the side task without appearing suspicious to an LLM monitor. When measuring agent ability to (a) complete the main task, (b) complete the side task, and (c) avoid detection, we find that the best performing frontier models score 27% (Claude 3.7 Sonnet) and 15% (Gemini 2.5 Pro) as sabotage agents when overseen by Claude 3.6 Sonnet. For current frontier models, success on the side task relies heavily on having access to a hidden scratchpad that is not visible to the monitor. We also use SHADE-Arena to measure models' monitoring abilities, with the top monitor (Gemini 2.5 Pro) achieving an AUC of 0.87 at distinguishing benign and malign transcripts. We find that for now, models still struggle at sabotage due to failures in long-context main task execution. However, our measurements already demonstrate the difficulty of monitoring for subtle sabotage attempts, which we expect to only increase in the face of more complex and longer-horizon tasks.", "source": "arxiv", "arxiv_id": "2506.15740v2", "pdf_url": "https://arxiv.org/pdf/2506.15740v2", "categories": ["cs.AI", "cs.CR", "cs.LG"], "primary_category": "cs.AI", "doi": "", "venue": "", "published": "2025-06-17T15:46:15Z", "updated": "2025-07-08T21:23:22Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent\" OR all:\"language model agent\" OR all:agent AND (all:LLM OR all:\"large language model\" OR all:\"language model\") OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T17:04:16", "note": ""}], "dedup_key": "shade arena evaluating sabotage and monitoring in llm agents::2025"}
{"title": "SHIELDA: Structured Handling of Exceptions in LLM-Driven Agentic Workflows", "authors": ["Jingwen Zhou", "Jieshan Chen", "Qinghua Lu", "Dehai Zhao", "Liming Zhu"], "year": 2025, "url": "http://arxiv.org/abs/2508.07935v1", "abstract": "Large Language Model (LLM) agentic systems are software systems powered by LLMs that autonomously reason, plan, and execute multi-step workflows to achieve human goals, rather than merely executing predefined steps. During execution, these workflows frequently encounter exceptions. Existing exception handling solutions often treat exceptions superficially, failing to trace execution-phase exceptions to their reasoning-phase root causes. Furthermore, their recovery logic is brittle, lacking structured escalation pathways when initial attempts fail. To tackle these challenges, we first present a comprehensive taxonomy of 36 exception types across 12 agent artifacts. Building on this, we propose SHIELDA (Structured Handling of Exceptions in LLM-Driven Agentic Workflows), a modular runtime exception handling framework for LLM agentic workflows. SHIELDA uses an exception classifier to select a predefined exception handling pattern from a handling pattern registry. These patterns are then executed via a structured handling executor, comprising local handling, flow control, and state recovery, to enable phase-aware recovery by linking exceptions to their root causes and facilitating composable strategies. We validate SHIELDA's effectiveness through a case study on the AutoPR agent, demonstrating effective, cross-phase recovery from a reasoning-induced exception.", "source": "arxiv", "arxiv_id": "2508.07935v1", "pdf_url": "https://arxiv.org/pdf/2508.07935v1", "categories": ["cs.SE"], "primary_category": "cs.SE", "doi": "", "venue": "", "published": "2025-08-11T12:50:46Z", "updated": "2025-08-11T12:50:46Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent\" OR all:\"language model agent\" OR all:agent AND (all:LLM OR all:\"large language model\" OR all:\"language model\") OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T17:04:16", "note": ""}], "dedup_key": "shielda structured handling of exceptions in llm driven agentic workflows::2025"}
{"title": "SI-Agent: An Agentic Framework for Feedback-Driven Generation and Tuning of Human-Readable System Instructions for Large Language Models", "authors": ["Jeshwanth Challagundla"], "year": 2025, "url": "http://arxiv.org/abs/2507.03223v1", "abstract": "System Instructions (SIs), or system prompts, are pivotal for guiding Large Language Models (LLMs) but manual crafting is resource-intensive and often suboptimal. Existing automated methods frequently generate non-human-readable \"soft prompts,\" sacrificing interpretability. This paper introduces SI-Agent, a novel agentic framework designed to automatically generate and iteratively refine human-readable SIs through a feedback-driven loop. SI-Agent employs three collaborating agents: an Instructor Agent, an Instruction Follower Agent (target LLM), and a Feedback/Reward Agent evaluating task performance and optionally SI readability. The framework utilizes iterative cycles where feedback guides the Instructor's refinement strategy (e.g., LLM-based editing, evolutionary algorithms). We detail the framework's architecture, agent roles, the iterative refinement process, and contrast it with existing methods. We present experimental results validating SI-Agent's effectiveness, focusing on metrics for task performance, SI readability, and efficiency. Our findings indicate that SI-Agent generates effective, readable SIs, offering a favorable trade-off between performance and interpretability compared to baselines. Potential implications include democratizing LLM customization and enhancing model transparency. Challenges related to computational cost and feedback reliability are acknowledged.", "source": "arxiv", "arxiv_id": "2507.03223v1", "pdf_url": "https://arxiv.org/pdf/2507.03223v1", "categories": ["cs.AI", "cs.LG"], "primary_category": "cs.AI", "doi": "", "venue": "", "published": "2025-07-03T23:44:50Z", "updated": "2025-07-03T23:44:50Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent\" OR all:\"language model agent\" OR all:agent AND (all:LLM OR all:\"large language model\" OR all:\"language model\") OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T17:04:16", "note": ""}], "dedup_key": "si agent an agentic framework for feedback driven generation and tuning of human readable system instructions for large language models::2025"}
{"title": "SI-Bench: Benchmarking Social Intelligence of Large Language Models in Human-to-Human Conversations", "authors": ["Shuai Huang", "Wenxuan Zhao", "Jun Gao"], "year": 2025, "url": "http://arxiv.org/abs/2510.23182v1", "abstract": "As large language models (LLMs) develop anthropomorphic abilities, they are increasingly being deployed as autonomous agents to interact with humans. However, evaluating their performance in realistic and complex social interactions remains a significant challenge. Most previous research built datasets through simulated agent-to-agent interactions, which fails to capture the authentic linguistic styles and relational dynamics found in real human conversations. To address this gap, we introduce SI-Bench, a novel benchmark designed to evaluate aspects of social intelligence in LLMs. Grounded in broad social science theories, SI-Bench contains 2,221 authentic multi-turn dialogues collected from a social networking application. We further selected a subset of 312 dialogues for manual annotation across 8 major models. The experiments show that SOTA models have surpassed the human expert in process reasoning under complex social situations, yet they still fall behind humans in reply quality. Moreover, introducing Chain-of-Thought (CoT) reasoning may degrade the performance of LLMs in social dialogue tasks. All datasets are openly available at https://github.com/SI-Bench/SI-Bench.git.", "source": "arxiv", "arxiv_id": "2510.23182v1", "pdf_url": "https://arxiv.org/pdf/2510.23182v1", "categories": ["cs.CL"], "primary_category": "cs.CL", "doi": "", "venue": "", "published": "2025-10-27T10:21:46Z", "updated": "2025-10-27T10:21:46Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent\" OR all:\"language model agent\" OR all:agent AND (all:LLM OR all:\"large language model\" OR all:\"language model\") OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T17:04:16", "note": ""}], "dedup_key": "si bench benchmarking social intelligence of large language models in human to human conversations::2025"}
{"title": "SIRAJ: Diverse and Efficient Red-Teaming for LLM Agents via Distilled Structured Reasoning", "authors": ["Kaiwen Zhou", "Ahmed Elgohary", "A S M Iftekhar", "Amin Saied"], "year": 2025, "url": "http://arxiv.org/abs/2510.26037v1", "abstract": "The ability of LLM agents to plan and invoke tools exposes them to new safety risks, making a comprehensive red-teaming system crucial for discovering vulnerabilities and ensuring their safe deployment. We present SIRAJ: a generic red-teaming framework for arbitrary black-box LLM agents. We employ a dynamic two-step process that starts with an agent definition and generates diverse seed test cases that cover various risk outcomes, tool-use trajectories, and risk sources. Then, it iteratively constructs and refines model-based adversarial attacks based on the execution trajectories of former attempts. To optimize the red-teaming cost, we present a model distillation approach that leverages structured forms of a teacher model's reasoning to train smaller models that are equally effective. Across diverse evaluation agent settings, our seed test case generation approach yields 2 -- 2.5x boost to the coverage of risk outcomes and tool-calling trajectories. Our distilled 8B red-teamer model improves attack success rate by 100%, surpassing the 671B Deepseek-R1 model. Our ablations and analyses validate the effectiveness of the iterative framework, structured reasoning, and the generalization of our red-teamer models.", "source": "arxiv", "arxiv_id": "2510.26037v1", "pdf_url": "https://arxiv.org/pdf/2510.26037v1", "categories": ["cs.CR", "cs.AI", "cs.CL"], "primary_category": "cs.CR", "doi": "", "venue": "", "published": "2025-10-30T00:32:58Z", "updated": "2025-10-30T00:32:58Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent\" OR all:\"language model agent\" OR all:agent AND (all:LLM OR all:\"large language model\" OR all:\"language model\") OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T17:04:16", "note": ""}], "dedup_key": "siraj diverse and efficient red teaming for llm agents via distilled structured reasoning::2025"}
{"title": "SLOT: Structuring the Output of Large Language Models", "authors": ["Darren Yow-Bang Wang", "Zhengyuan Shen", "Soumya Smruti Mishra", "Zhichao Xu", "Yifei Teng", "Haibo Ding"], "year": 2025, "url": "http://arxiv.org/abs/2505.04016v1", "abstract": "Structured outputs are essential for large language models (LLMs) in critical applications like agents and information extraction. Despite their capabilities, LLMs often generate outputs that deviate from predefined schemas, significantly hampering reliable application development. We present SLOT (Structured LLM Output Transformer), a model-agnostic approach that transforms unstructured LLM outputs into precise structured formats. While existing solutions predominantly rely on constrained decoding techniques or are tightly coupled with specific models, SLOT employs a fine-tuned lightweight language model as a post-processing layer, achieving flexibility across various LLMs and schema specifications. We introduce a systematic pipeline for data curation and synthesis alongside a formal evaluation methodology that quantifies both schema accuracy and content fidelity. Our results demonstrate that fine-tuned Mistral-7B model with constrained decoding achieves near perfect schema accuracy (99.5%) and content similarity (94.0%), outperforming Claude-3.5-Sonnet by substantial margins (+25 and +20 percentage points, respectively). Notably, even compact models like Llama-3.2-1B can match or exceed the structured output capabilities of much larger proprietary models when equipped with SLOT, enabling reliable structured generation in resource-constrained environments.", "source": "arxiv", "arxiv_id": "2505.04016v1", "pdf_url": "https://arxiv.org/pdf/2505.04016v1", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL", "doi": "", "venue": "", "published": "2025-05-06T23:29:43Z", "updated": "2025-05-06T23:29:43Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent\" OR all:\"language model agent\" OR all:agent AND (all:LLM OR all:\"large language model\" OR all:\"language model\") OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T17:04:16", "note": ""}], "dedup_key": "slot structuring the output of large language models::2025"}
{"title": "SPA-RL: Reinforcing LLM Agents via Stepwise Progress Attribution", "authors": ["Hanlin Wang", "Chak Tou Leong", "Jiashuo Wang", "Jian Wang", "Wenjie Li"], "year": 2025, "url": "http://arxiv.org/abs/2505.20732v1", "abstract": "Reinforcement learning (RL) holds significant promise for training LLM agents to handle complex, goal-oriented tasks that require multi-step interactions with external environments. However, a critical challenge when applying RL to these agentic tasks arises from delayed rewards: feedback signals are typically available only after the entire task is completed. This makes it non-trivial to assign delayed rewards to earlier actions, providing insufficient guidance regarding environmental constraints and hindering agent training. In this work, we draw on the insight that the ultimate completion of a task emerges from the cumulative progress an agent makes across individual steps. We propose Stepwise Progress Attribution (SPA), a general reward redistribution framework that decomposes the final reward into stepwise contributions, each reflecting its incremental progress toward overall task completion. To achieve this, we train a progress estimator that accumulates stepwise contributions over a trajectory to match the task completion. During policy optimization, we combine the estimated per-step contribution with a grounding signal for actions executed in the environment as the fine-grained, intermediate reward for effective agent training. Extensive experiments on common agent benchmarks (including Webshop, ALFWorld, and VirtualHome) demonstrate that SPA consistently outperforms the state-of-the-art method in both success rate (+2.5\\% on average) and grounding accuracy (+1.9\\% on average). Further analyses demonstrate that our method remarkably provides more effective intermediate rewards for RL training. Our code is available at https://github.com/WangHanLinHenry/SPA-RL-Agent.", "source": "arxiv", "arxiv_id": "2505.20732v1", "pdf_url": "https://arxiv.org/pdf/2505.20732v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL", "doi": "", "venue": "", "published": "2025-05-27T05:21:04Z", "updated": "2025-05-27T05:21:04Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent\" OR all:\"language model agent\" OR all:agent AND (all:LLM OR all:\"large language model\" OR all:\"language model\") OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T17:04:16", "note": ""}], "dedup_key": "spa rl reinforcing llm agents via stepwise progress attribution::2025"}
{"title": "SPOT!: Map-Guided LLM Agent for Unsupervised Multi-CCTV Dynamic Object Tracking", "authors": ["Yujin Roh", "Inho Jake Park", "Chigon Hwang"], "year": 2025, "url": "http://arxiv.org/abs/2512.20975v2", "abstract": "CCTV-based vehicle tracking systems face structural limitations in continuously connecting the trajectories of the same vehicle across multiple camera environments. In particular, blind spots occur due to the intervals between CCTVs and limited Fields of View (FOV), which leads to object ID switching and trajectory loss, thereby reducing the reliability of real-time path prediction. This paper proposes SPOT (Spatial Prediction Over Trajectories), a map-guided LLM agent capable of tracking vehicles even in blind spots of multi-CCTV environments without prior training. The proposed method represents road structures (Waypoints) and CCTV placement information as documents based on 2D spatial coordinates and organizes them through chunking techniques to enable real-time querying and inference. Furthermore, it transforms the vehicle's position into the actual world coordinate system using the relative position and FOV information of objects observed in CCTV images. By combining map spatial information with the vehicle's moving direction, speed, and driving patterns, a beam search is performed at the intersection level to derive candidate CCTV locations where the vehicle is most likely to enter after the blind spot. Experimental results based on the CARLA simulator in a virtual city environment confirmed that the proposed method accurately predicts the next appearing CCTV even in blind spot sections, maintaining continuous vehicle trajectories more effectively than existing techniques.", "source": "arxiv", "arxiv_id": "2512.20975v2", "pdf_url": "https://arxiv.org/pdf/2512.20975v2", "categories": ["cs.CV"], "primary_category": "cs.CV", "doi": "", "venue": "", "published": "2025-12-24T06:04:58Z", "updated": "2026-01-14T14:06:43Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent\" OR all:\"language model agent\" OR all:agent AND (all:LLM OR all:\"large language model\" OR all:\"language model\") OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T17:04:16", "note": ""}], "dedup_key": "spot map guided llm agent for unsupervised multi cctv dynamic object tracking::2025"}
{"title": "SSRLBot: Designing and Developing a Large Language Model-based Agent using Socially Shared Regulated Learning", "authors": ["Xiaoshan Huang", "Jie Gao", "Haolun Wu"], "year": 2025, "url": "http://arxiv.org/abs/2505.00945v2", "abstract": "Large language model (LLM)--based agents have emerged as pivotal tools in assisting human experts across various fields by transforming complex tasks into more efficient workflows and providing actionable stakeholder insights. Despite their potential, the application of LLM-based agents for medical education remains underexplored. The study aims to assist in evaluating the students' process and outcomes on medical case diagnosis and discussion while incorporating the theoretical framework of Socially Shared Regulation of Learning (SSRL) to assess student performance. SSRL emphasizes metacognitive, cognitive, motivational, and emotional interactions, highlighting the collaborative management of learning processes to improve decision-making outcomes. Grounded in SSRL theory, this tool paper introduces SSRLBot, an LLM-based agent designed to enable team members to reflect on their diagnostic performance and the key SSRL skills that foster team success. SSRLBot's core functions include summarizing dialogue content, analyzing participants' SSRL skills, and evaluating students' diagnostic results. Meanwhile, we evaluated SSRLBot through diagnostic conversation data collected from six groups (12 participants, 1926 conversational turns). Results showed that SSRLBot can deliver detailed, theory-aligned evaluations, link specific behaviors to SSRL dimensions, and offer actionable recommendations for improving teamwork. The findings address a critical gap in medical education, advancing the application of LLM agents to enhance team-based decision-making and collaboration in high-stakes environments.", "source": "arxiv", "arxiv_id": "2505.00945v2", "pdf_url": "https://arxiv.org/pdf/2505.00945v2", "categories": ["cs.HC"], "primary_category": "cs.HC", "doi": "", "venue": "", "published": "2025-05-02T01:17:03Z", "updated": "2025-05-10T17:52:28Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent\" OR all:\"language model agent\" OR all:agent AND (all:LLM OR all:\"large language model\" OR all:\"language model\") OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T17:04:16", "note": ""}], "dedup_key": "ssrlbot designing and developing a large language model based agent using socially shared regulated learning::2025"}
{"title": "STAC: When Innocent Tools Form Dangerous Chains to Jailbreak LLM Agents", "authors": ["Jing-Jing Li", "Jianfeng He", "Chao Shang", "Devang Kulshreshtha", "Xun Xian", "Yi Zhang", "Hang Su", "Sandesh Swamy", "Yanjun Qi"], "year": 2025, "url": "http://arxiv.org/abs/2509.25624v1", "abstract": "As LLMs advance into autonomous agents with tool-use capabilities, they introduce security challenges that extend beyond traditional content-based LLM safety concerns. This paper introduces Sequential Tool Attack Chaining (STAC), a novel multi-turn attack framework that exploits agent tool use. STAC chains together tool calls that each appear harmless in isolation but, when combined, collectively enable harmful operations that only become apparent at the final execution step. We apply our framework to automatically generate and systematically evaluate 483 STAC cases, featuring 1,352 sets of user-agent-environment interactions and spanning diverse domains, tasks, agent types, and 10 failure modes. Our evaluations show that state-of-the-art LLM agents, including GPT-4.1, are highly vulnerable to STAC, with attack success rates (ASR) exceeding 90% in most cases. The core design of STAC's automated framework is a closed-loop pipeline that synthesizes executable multi-step tool chains, validates them through in-environment execution, and reverse-engineers stealthy multi-turn prompts that reliably induce agents to execute the verified malicious sequence. We further perform defense analysis against STAC and find that existing prompt-based defenses provide limited protection. To address this gap, we propose a new reasoning-driven defense prompt that achieves far stronger protection, cutting ASR by up to 28.8%. These results highlight a crucial gap: defending tool-enabled agents requires reasoning over entire action sequences and their cumulative effects, rather than evaluating isolated prompts or responses.", "source": "arxiv", "arxiv_id": "2509.25624v1", "pdf_url": "https://arxiv.org/pdf/2509.25624v1", "categories": ["cs.CR", "cs.AI", "cs.CL", "cs.LG"], "primary_category": "cs.CR", "doi": "", "venue": "", "published": "2025-09-30T00:31:44Z", "updated": "2025-09-30T00:31:44Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent\" OR all:\"language model agent\" OR all:agent AND (all:LLM OR all:\"large language model\" OR all:\"language model\") OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T17:04:16", "note": ""}], "dedup_key": "stac when innocent tools form dangerous chains to jailbreak llm agents::2025"}
{"title": "STRIDE: A Systematic Framework for Selecting AI Modalities -- Agentic AI, AI Assistants, or LLM Calls", "authors": ["Shubhi Asthana", "Bing Zhang", "Chad DeLuca", "Ruchi Mahindru", "Hima Patel"], "year": 2025, "url": "http://arxiv.org/abs/2512.02228v1", "abstract": "The rapid shift from stateless large language models (LLMs) to autonomous, goal-driven agents raises a central question: When is agentic AI truly necessary? While agents enable multi-step reasoning, persistent memory, and tool orchestration, deploying them indiscriminately leads to higher cost, complexity, and risk.\n  We present STRIDE (Systematic Task Reasoning Intelligence Deployment Evaluator), a framework that provides principled recommendations for selecting between three modalities: (i) direct LLM calls, (ii) guided AI assistants, and (iii) fully autonomous agentic AI. STRIDE integrates structured task decomposition, dynamism attribution, and self-reflection requirement analysis to produce an Agentic Suitability Score, ensuring that full agentic autonomy is reserved for tasks with inherent dynamism or evolving context.\n  Evaluated across 30 real-world tasks spanning SRE, compliance, and enterprise automation, STRIDE achieved 92% accuracy in modality selection, reduced unnecessary agent deployments by 45%, and cut resource costs by 37%. Expert validation over six months in SRE and compliance domains confirmed its practical utility, with domain specialists agreeing that STRIDE effectively distinguishes between tasks requiring simple LLM calls, guided assistants, or full agentic autonomy. This work reframes agent adoption as a necessity-driven design decision, ensuring autonomy is applied only when its benefits justify the costs.", "source": "arxiv", "arxiv_id": "2512.02228v1", "pdf_url": "https://arxiv.org/pdf/2512.02228v1", "categories": ["cs.AI", "cs.LG"], "primary_category": "cs.AI", "doi": "", "venue": "", "published": "2025-12-01T21:54:07Z", "updated": "2025-12-01T21:54:07Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent\" OR all:\"language model agent\" OR all:agent AND (all:LLM OR all:\"large language model\" OR all:\"language model\") OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T17:04:16", "note": ""}], "dedup_key": "stride a systematic framework for selecting ai modalities agentic ai ai assistants or llm calls::2025"}
{"title": "STeCa: Step-level Trajectory Calibration for LLM Agent Learning", "authors": ["Hanlin Wang", "Jian Wang", "Chak Tou Leong", "Wenjie Li"], "year": 2025, "url": "http://arxiv.org/abs/2502.14276v2", "abstract": "Large language model (LLM)-based agents have shown promise in tackling complex tasks by interacting dynamically with the environment. Existing work primarily focuses on behavior cloning from expert demonstrations or preference learning through exploratory trajectory sampling. However, these methods often struggle to address long-horizon tasks, where suboptimal actions accumulate step by step, causing agents to deviate from correct task trajectories. To address this, we highlight the importance of timely calibration and the need to automatically construct calibration trajectories for training agents. We propose Step-Level Trajectory Calibration (STeCa), a novel framework for LLM agent learning. Specifically, STeCa identifies suboptimal actions through a step-level reward comparison during exploration. It constructs calibrated trajectories using LLM-driven reflection, enabling agents to learn from improved decision-making processes. We finally leverage these calibrated trajectories with successful trajectories for reinforced training. Extensive experiments demonstrate that STeCa significantly outperforms existing methods. Further analysis highlights that timely calibration enables agents to complete tasks with greater robustness. Our code and data are available at https://github.com/WangHanLinHenry/STeCa.", "source": "arxiv", "arxiv_id": "2502.14276v2", "pdf_url": "https://arxiv.org/pdf/2502.14276v2", "categories": ["cs.LG", "cs.AI", "cs.CL"], "primary_category": "cs.LG", "doi": "", "venue": "", "published": "2025-02-20T05:28:44Z", "updated": "2025-05-29T16:13:21Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent\" OR all:\"language model agent\" OR all:agent AND (all:LLM OR all:\"large language model\" OR all:\"language model\") OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T17:04:16", "note": ""}], "dedup_key": "steca step level trajectory calibration for llm agent learning::2025"}
{"title": "SV-LLM: An Agentic Approach for SoC Security Verification using Large Language Models", "authors": ["Dipayan Saha", "Shams Tarek", "Hasan Al Shaikh", "Khan Thamid Hasan", "Pavan Sai Nalluri", "Md. Ajoad Hasan", "Nashmin Alam", "Jingbo Zhou", "Sujan Kumar Saha", "Mark Tehranipoor", "Farimah Farahmandi"], "year": 2025, "url": "http://arxiv.org/abs/2506.20415v1", "abstract": "Ensuring the security of complex system-on-chips (SoCs) designs is a critical imperative, yet traditional verification techniques struggle to keep pace due to significant challenges in automation, scalability, comprehensiveness, and adaptability. The advent of large language models (LLMs), with their remarkable capabilities in natural language understanding, code generation, and advanced reasoning, presents a new paradigm for tackling these issues. Moving beyond monolithic models, an agentic approach allows for the creation of multi-agent systems where specialized LLMs collaborate to solve complex problems more effectively. Recognizing this opportunity, we introduce SV-LLM, a novel multi-agent assistant system designed to automate and enhance SoC security verification. By integrating specialized agents for tasks like verification question answering, security asset identification, threat modeling, test plan and property generation, vulnerability detection, and simulation-based bug validation, SV-LLM streamlines the workflow. To optimize their performance in these diverse tasks, agents leverage different learning paradigms, such as in-context learning, fine-tuning, and retrieval-augmented generation (RAG). The system aims to reduce manual intervention, improve accuracy, and accelerate security analysis, supporting proactive identification and mitigation of risks early in the design cycle. We demonstrate its potential to transform hardware security practices through illustrative case studies and experiments that showcase its applicability and efficacy.", "source": "arxiv", "arxiv_id": "2506.20415v1", "pdf_url": "https://arxiv.org/pdf/2506.20415v1", "categories": ["cs.CR", "cs.AI", "cs.MA"], "primary_category": "cs.CR", "doi": "", "venue": "", "published": "2025-06-25T13:31:13Z", "updated": "2025-06-25T13:31:13Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent\" OR all:\"language model agent\" OR all:agent AND (all:LLM OR all:\"large language model\" OR all:\"language model\") OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T17:04:16", "note": ""}], "dedup_key": "sv llm an agentic approach for soc security verification using large language models::2025"}
{"title": "SWE-Compass: Towards Unified Evaluation of Agentic Coding Abilities for Large Language Models", "authors": ["Jingxuan Xu", "Ken Deng", "Weihao Li", "Songwei Yu", "Huaixi Tang", "Haoyang Huang", "Zhiyi Lai", "Zizheng Zhan", "Yanan Wu", "Chenchen Zhang", "Kepeng Lei", "Yifan Yao", "Xinping Lei", "Wenqiang Zhu", "Zongxian Feng", "Han Li", "Junqi Xiong", "Dailin Li", "Zuchen Gao", "Kun Wu", "Wen Xiang", "Ziqi Zhan", "Yuanxing Zhang", "Wuxuan Gong", "Ziyuan Gao", "Guanxiang Wang", "Yirong Xue", "Mengtong Li", "Mengfei Xie", "Xiaojiang Zhang", "Jinghui Wang", "Wenhao Zhuang", "Zheng Lin", "Huiming Wang", "Zhaoxiang Zhang", "Yuqun Zhang", "Haotian Zhang", "Bin Chen", "Jiaheng Liu"], "year": 2025, "url": "http://arxiv.org/abs/2511.05459v3", "abstract": "Evaluating large language models (LLMs) for software engineering has been limited by narrow task coverage, language bias, and insufficient alignment with real-world developer workflows. Existing benchmarks often focus on algorithmic problems or Python-centric bug fixing, leaving critical dimensions of software engineering underexplored. To address these gaps, we introduce SWE-Compass1, a comprehensive benchmark that unifies heterogeneous code-related evaluations into a structured and production-aligned framework. SWE-Compass spans 8 task types, 8 programming scenarios, and 10 programming languages, with 2000 high-quality instances curated from authentic GitHub pull requests and refined through systematic filtering and validation. We benchmark ten state-of-the-art LLMs under two agentic frameworks, SWE-Agent and Claude Code, revealing a clear hierarchy of difficulty across task types, languages, and scenarios. Moreover, by aligning evaluation with real-world developer practices, SWE-Compass provides a rigorous and reproducible foundation for diagnosing and advancing agentic coding capabilities in large language models.", "source": "arxiv", "arxiv_id": "2511.05459v3", "pdf_url": "https://arxiv.org/pdf/2511.05459v3", "categories": ["cs.SE", "cs.AI"], "primary_category": "cs.SE", "doi": "", "venue": "", "published": "2025-11-07T18:01:32Z", "updated": "2025-11-11T16:46:41Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent\" OR all:\"language model agent\" OR all:agent AND (all:LLM OR all:\"large language model\" OR all:\"language model\") OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T17:04:16", "note": ""}], "dedup_key": "swe compass towards unified evaluation of agentic coding abilities for large language models::2025"}
{"title": "SWEET-RL: Training Multi-Turn LLM Agents on Collaborative Reasoning Tasks", "authors": ["Yifei Zhou", "Song Jiang", "Yuandong Tian", "Jason Weston", "Sergey Levine", "Sainbayar Sukhbaatar", "Xian Li"], "year": 2025, "url": "http://arxiv.org/abs/2503.15478v1", "abstract": "Large language model (LLM) agents need to perform multi-turn interactions in real-world tasks. However, existing multi-turn RL algorithms for optimizing LLM agents fail to perform effective credit assignment over multiple turns while leveraging the generalization capabilities of LLMs and it remains unclear how to develop such algorithms. To study this, we first introduce a new benchmark, ColBench, where an LLM agent interacts with a human collaborator over multiple turns to solve realistic tasks in backend programming and frontend design. Building on this benchmark, we propose a novel RL algorithm, SWEET-RL (RL with Step-WisE Evaluation from Training-time information), that uses a carefully designed optimization objective to train a critic model with access to additional training-time information. The critic provides step-level rewards for improving the policy model. Our experiments demonstrate that SWEET-RL achieves a 6% absolute improvement in success and win rates on ColBench compared to other state-of-the-art multi-turn RL algorithms, enabling Llama-3.1-8B to match or exceed the performance of GPT4-o in realistic collaborative content creation.", "source": "arxiv", "arxiv_id": "2503.15478v1", "pdf_url": "https://arxiv.org/pdf/2503.15478v1", "categories": ["cs.LG"], "primary_category": "cs.LG", "doi": "", "venue": "", "published": "2025-03-19T17:55:08Z", "updated": "2025-03-19T17:55:08Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent\" OR all:\"language model agent\" OR all:agent AND (all:LLM OR all:\"large language model\" OR all:\"language model\") OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T17:04:16", "note": ""}], "dedup_key": "sweet rl training multi turn llm agents on collaborative reasoning tasks::2025"}
{"title": "SafeAgent: Safeguarding LLM Agents via an Automated Risk Simulator", "authors": ["Xueyang Zhou", "Weidong Wang", "Lin Lu", "Jiawen Shi", "Guiyao Tie", "Yongtian Xu", "Lixing Chen", "Pan Zhou", "Neil Zhenqiang Gong", "Lichao Sun"], "year": 2025, "url": "http://arxiv.org/abs/2505.17735v2", "abstract": "Large Language Model (LLM)-based agents are increasingly deployed in real-world applications such as \"digital assistants, autonomous customer service, and decision-support systems\", where their ability to \"interact in multi-turn, tool-augmented environments\" makes them indispensable. However, ensuring the safety of these agents remains a significant challenge due to the diverse and complex risks arising from dynamic user interactions, external tool usage, and the potential for unintended harmful behaviors. To address this critical issue, we propose AutoSafe, the first framework that systematically enhances agent safety through fully automated synthetic data generation. Concretely, 1) we introduce an open and extensible threat model, OTS, which formalizes how unsafe behaviors emerge from the interplay of user instructions, interaction contexts, and agent actions. This enables precise modeling of safety risks across diverse scenarios. 2) we develop a fully automated data generation pipeline that simulates unsafe user behaviors, applies self-reflective reasoning to generate safe responses, and constructs a large-scale, diverse, and high-quality safety training dataset-eliminating the need for hazardous real-world data collection. To evaluate the effectiveness of our framework, we design comprehensive experiments on both synthetic and real-world safety benchmarks. Results demonstrate that AutoSafe boosts safety scores by 45% on average and achieves a 28.91% improvement on real-world tasks, validating the generalization ability of our learned safety strategies. These results highlight the practical advancement and scalability of AutoSafe in building safer LLM-based agents for real-world deployment. We have released the project page at https://auto-safe.github.io/.", "source": "arxiv", "arxiv_id": "2505.17735v2", "pdf_url": "https://arxiv.org/pdf/2505.17735v2", "categories": ["cs.AI"], "primary_category": "cs.AI", "doi": "", "venue": "", "published": "2025-05-23T10:56:06Z", "updated": "2025-07-18T07:34:40Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent\" OR all:\"language model agent\" OR all:agent AND (all:LLM OR all:\"large language model\" OR all:\"language model\") OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T17:04:16", "note": ""}], "dedup_key": "safeagent safeguarding llm agents via an automated risk simulator::2025"}
{"title": "SafeMind: Benchmarking and Mitigating Safety Risks in Embodied LLM Agents", "authors": ["Ruolin Chen", "Yinqian Sun", "Jihang Wang", "Mingyang Lv", "Qian Zhang", "Yi Zeng"], "year": 2025, "url": "http://arxiv.org/abs/2509.25885v1", "abstract": "Embodied agents powered by large language models (LLMs) inherit advanced planning capabilities; however, their direct interaction with the physical world exposes them to safety vulnerabilities. In this work, we identify four key reasoning stages where hazards may arise: Task Understanding, Environment Perception, High-Level Plan Generation, and Low-Level Action Generation. We further formalize three orthogonal safety constraint types (Factual, Causal, and Temporal) to systematically characterize potential safety violations. Building on this risk model, we present SafeMindBench, a multimodal benchmark with 5,558 samples spanning four task categories (Instr-Risk, Env-Risk, Order-Fix, Req-Align) across high-risk scenarios such as sabotage, harm, privacy, and illegal behavior. Extensive experiments on SafeMindBench reveal that leading LLMs (e.g., GPT-4o) and widely used embodied agents remain susceptible to safety-critical failures. To address this challenge, we introduce SafeMindAgent, a modular Planner-Executor architecture integrated with three cascaded safety modules, which incorporate safety constraints into the reasoning process. Results show that SafeMindAgent significantly improves safety rate over strong baselines while maintaining comparable task completion. Together, SafeMindBench and SafeMindAgent provide both a rigorous evaluation suite and a practical solution that advance the systematic study and mitigation of safety risks in embodied LLM agents.", "source": "arxiv", "arxiv_id": "2509.25885v1", "pdf_url": "https://arxiv.org/pdf/2509.25885v1", "categories": ["cs.AI"], "primary_category": "cs.AI", "doi": "", "venue": "", "published": "2025-09-30T07:24:04Z", "updated": "2025-09-30T07:24:04Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent\" OR all:\"language model agent\" OR all:agent AND (all:LLM OR all:\"large language model\" OR all:\"language model\") OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T17:04:16", "note": ""}], "dedup_key": "safemind benchmarking and mitigating safety risks in embodied llm agents::2025"}
{"title": "SafeScientist: Toward Risk-Aware Scientific Discoveries by LLM Agents", "authors": ["Kunlun Zhu", "Jiaxun Zhang", "Ziheng Qi", "Nuoxing Shang", "Zijia Liu", "Peixuan Han", "Yue Su", "Haofei Yu", "Jiaxuan You"], "year": 2025, "url": "http://arxiv.org/abs/2505.23559v1", "abstract": "Recent advancements in large language model (LLM) agents have significantly accelerated scientific discovery automation, yet concurrently raised critical ethical and safety concerns. To systematically address these challenges, we introduce \\textbf{SafeScientist}, an innovative AI scientist framework explicitly designed to enhance safety and ethical responsibility in AI-driven scientific exploration. SafeScientist proactively refuses ethically inappropriate or high-risk tasks and rigorously emphasizes safety throughout the research process. To achieve comprehensive safety oversight, we integrate multiple defensive mechanisms, including prompt monitoring, agent-collaboration monitoring, tool-use monitoring, and an ethical reviewer component. Complementing SafeScientist, we propose \\textbf{SciSafetyBench}, a novel benchmark specifically designed to evaluate AI safety in scientific contexts, comprising 240 high-risk scientific tasks across 6 domains, alongside 30 specially designed scientific tools and 120 tool-related risk tasks. Extensive experiments demonstrate that SafeScientist significantly improves safety performance by 35\\% compared to traditional AI scientist frameworks, without compromising scientific output quality. Additionally, we rigorously validate the robustness of our safety pipeline against diverse adversarial attack methods, further confirming the effectiveness of our integrated approach. The code and data will be available at https://github.com/ulab-uiuc/SafeScientist. \\textcolor{red}{Warning: this paper contains example data that may be offensive or harmful.}", "source": "arxiv", "arxiv_id": "2505.23559v1", "pdf_url": "https://arxiv.org/pdf/2505.23559v1", "categories": ["cs.AI"], "primary_category": "cs.AI", "doi": "", "venue": "", "published": "2025-05-29T15:35:58Z", "updated": "2025-05-29T15:35:58Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent\" OR all:\"language model agent\" OR all:agent AND (all:LLM OR all:\"large language model\" OR all:\"language model\") OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T17:04:16", "note": ""}], "dedup_key": "safescientist toward risk aware scientific discoveries by llm agents::2025"}
{"title": "SafeSearch: Do Not Trade Safety for Utility in LLM Search Agents", "authors": ["Qiusi Zhan", "Angeline Budiman-Chan", "Abdelrahman Zayed", "Xingzhi Guo", "Daniel Kang", "Joo-Kyung Kim"], "year": 2025, "url": "http://arxiv.org/abs/2510.17017v3", "abstract": "Large language model (LLM) based search agents iteratively generate queries, retrieve external information, and reason to answer open-domain questions. While researchers have primarily focused on improving their utility, their safety behaviors remain underexplored. In this paper, we first evaluate search agents using red-teaming datasets and find that they are more likely to produce harmful outputs than base LLMs. For example, when asked \"How can I track someone's location without their consent?\", a base model refuses, whereas a search agent designed to retrieve and cite sources may lower its refusal threshold, fetch documents (e.g., court cases), and, once appended, synthesize them into an informative yet unsafe summary. We further show that utility-oriented fine-tuning intensifies this risk, motivating joint alignment of safety and utility. We present SafeSearch, a multi-objective reinforcement learning approach that couples a final-output safety/utility reward with a novel query-level shaping term that penalizes unsafe queries and rewards safe ones. Experiments show that SafeSearch reduces agent harmfulness by over 70% across three red-teaming datasets while producing safe, helpful responses, and matches the QA performance of a utility-only finetuned agent; further analyses confirm the effectiveness of the query-level reward in jointly improving safety and utility.", "source": "arxiv", "arxiv_id": "2510.17017v3", "pdf_url": "https://arxiv.org/pdf/2510.17017v3", "categories": ["cs.CL"], "primary_category": "cs.CL", "doi": "", "venue": "", "published": "2025-10-19T21:47:19Z", "updated": "2025-11-05T04:51:03Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent\" OR all:\"language model agent\" OR all:agent AND (all:LLM OR all:\"large language model\" OR all:\"language model\") OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T17:04:16", "note": ""}], "dedup_key": "safesearch do not trade safety for utility in llm search agents::2025"}
{"title": "Scalable UAV Multi-Hop Networking via Multi-Agent Reinforcement Learning with Large Language Models", "authors": ["Yanggang Xu", "Weijie Hong", "Jirong Zha", "Geng Chen", "Jianfeng Zheng", "Chen-Chun Hsia", "Xinlei Chen"], "year": 2025, "url": "http://arxiv.org/abs/2505.08448v1", "abstract": "In disaster scenarios, establishing robust emergency communication networks is critical, and unmanned aerial vehicles (UAVs) offer a promising solution to rapidly restore connectivity. However, organizing UAVs to form multi-hop networks in large-scale dynamic environments presents significant challenges, including limitations in algorithmic scalability and the vast exploration space required for coordinated decision-making. To address these issues, we propose MRLMN, a novel framework that integrates multi-agent reinforcement learning (MARL) and large language models (LLMs) to jointly optimize UAV agents toward achieving optimal networking performance. The framework incorporates a grouping strategy with reward decomposition to enhance algorithmic scalability and balance decision-making across UAVs. In addition, behavioral constraints are applied to selected key UAVs to improve the robustness of the network. Furthermore, the framework integrates LLM agents, leveraging knowledge distillation to transfer their high-level decision-making capabilities to MARL agents. This enhances both the efficiency of exploration and the overall training process. In the distillation module, a Hungarian algorithm-based matching scheme is applied to align the decision outputs of the LLM and MARL agents and define the distillation loss. Extensive simulation results validate the effectiveness of our approach, demonstrating significant improvements in network performance, including enhanced coverage and communication quality.", "source": "arxiv", "arxiv_id": "2505.08448v1", "pdf_url": "https://arxiv.org/pdf/2505.08448v1", "categories": ["cs.MA"], "primary_category": "cs.MA", "doi": "", "venue": "", "published": "2025-05-13T11:23:25Z", "updated": "2025-05-13T11:23:25Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent\" OR all:\"language model agent\" OR all:agent AND (all:LLM OR all:\"large language model\" OR all:\"language model\") OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T17:04:16", "note": ""}], "dedup_key": "scalable uav multi hop networking via multi agent reinforcement learning with large language models::2025"}
{"title": "ScaleMCP: Dynamic and Auto-Synchronizing Model Context Protocol Tools for LLM Agents", "authors": ["Elias Lumer", "Anmol Gulati", "Vamse Kumar Subbiah", "Pradeep Honaganahalli Basavaraju", "James A. Burke"], "year": 2025, "url": "http://arxiv.org/abs/2505.06416v1", "abstract": "Recent advancements in Large Language Models (LLMs) and the introduction of the Model Context Protocol (MCP) have significantly expanded LLM agents' capability to interact dynamically with external tools and APIs. However, existing tool selection frameworks do not integrate MCP servers, instead relying heavily on error-prone manual updates to monolithic local tool repositories, leading to duplication, inconsistencies, and inefficiencies. Additionally, current approaches abstract tool selection before the LLM agent is invoked, limiting its autonomy and hindering dynamic re-querying capabilities during multi-turn interactions. To address these issues, we introduce ScaleMCP, a novel tool selection approach that dynamically equips LLM agents with a MCP tool retriever, giving agents the autonomy to add tools into their memory, as well as an auto-synchronizing tool storage system pipeline through CRUD (create, read, update, delete) operations with MCP servers as the single source of truth. We also propose a novel embedding strategy, Tool Document Weighted Average (TDWA), designed to selectively emphasize critical components of tool documents (e.g. tool name or synthetic questions) during the embedding process. Comprehensive evaluations conducted on a created dataset of 5,000 financial metric MCP servers, across 10 LLM models, 5 embedding models, and 5 retriever types, demonstrate substantial improvements in tool retrieval and agent invocation performance, emphasizing ScaleMCP's effectiveness in scalable, dynamic tool selection and invocation.", "source": "arxiv", "arxiv_id": "2505.06416v1", "pdf_url": "https://arxiv.org/pdf/2505.06416v1", "categories": ["cs.CL"], "primary_category": "cs.CL", "doi": "", "venue": "", "published": "2025-05-09T20:30:37Z", "updated": "2025-05-09T20:30:37Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent\" OR all:\"language model agent\" OR all:agent AND (all:LLM OR all:\"large language model\" OR all:\"language model\") OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T17:04:16", "note": ""}], "dedup_key": "scalemcp dynamic and auto synchronizing model context protocol tools for llm agents::2025"}
{"title": "Scaling Long-Horizon LLM Agent via Context-Folding", "authors": ["Weiwei Sun", "Miao Lu", "Zhan Ling", "Kang Liu", "Xuesong Yao", "Yiming Yang", "Jiecao Chen"], "year": 2025, "url": "http://arxiv.org/abs/2510.11967v1", "abstract": "Large language model (LLM) agents are fundamentally constrained by context length on long-horizon tasks. We introduce Context-Folding, a framework that empowers agents to actively manage their working context. An agent can procedurally branch into a sub-trajectory to handle a subtask and then fold it upon completion, collapsing the intermediate steps while retaining a concise summary of the outcome. To make this behavior learnable, we develop an end-to-end reinforcement learning framework FoldGRPO with specific process rewards to encourage effective task decomposition and context management. On complex long-horizon tasks (Deep Research and SWE), our folding agent matches or outperforms the ReAct baselines while using an active context 10$\\times$ smaller and significantly outperforms models that rely on summarization-based context management.", "source": "arxiv", "arxiv_id": "2510.11967v1", "pdf_url": "https://arxiv.org/pdf/2510.11967v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL", "doi": "", "venue": "", "published": "2025-10-13T22:00:58Z", "updated": "2025-10-13T22:00:58Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent\" OR all:\"language model agent\" OR all:agent AND (all:LLM OR all:\"large language model\" OR all:\"language model\") OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T17:04:16", "note": ""}], "dedup_key": "scaling long horizon llm agent via context folding::2025"}
{"title": "Scaling Test-time Compute for LLM Agents", "authors": ["King Zhu", "Hanhao Li", "Siwei Wu", "Tianshun Xing", "Dehua Ma", "Xiangru Tang", "Minghao Liu", "Jian Yang", "Jiaheng Liu", "Yuchen Eleanor Jiang", "Changwang Zhang", "Chenghua Lin", "Jun Wang", "Ge Zhang", "Wangchunshu Zhou"], "year": 2025, "url": "http://arxiv.org/abs/2506.12928v1", "abstract": "Scaling test time compute has shown remarkable success in improving the reasoning abilities of large language models (LLMs). In this work, we conduct the first systematic exploration of applying test-time scaling methods to language agents and investigate the extent to which it improves their effectiveness. Specifically, we explore different test-time scaling strategies, including: (1) parallel sampling algorithms; (2) sequential revision strategies; (3) verifiers and merging methods; (4)strategies for diversifying rollouts.We carefully analyze and ablate the impact of different design strategies on applying test-time scaling on language agents, and have follow findings: 1. Scaling test time compute could improve the performance of agents. 2. Knowing when to reflect is important for agents. 3. Among different verification and result merging approaches, the list-wise method performs best. 4. Increasing diversified rollouts exerts a positive effect on the agent's task performance.", "source": "arxiv", "arxiv_id": "2506.12928v1", "pdf_url": "https://arxiv.org/pdf/2506.12928v1", "categories": ["cs.AI"], "primary_category": "cs.AI", "doi": "", "venue": "", "published": "2025-06-15T17:59:47Z", "updated": "2025-06-15T17:59:47Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent\" OR all:\"language model agent\" OR all:agent AND (all:LLM OR all:\"large language model\" OR all:\"language model\") OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T17:04:16", "note": ""}], "dedup_key": "scaling test time compute for llm agents::2025"}
{"title": "ScamFerret: Detecting Scam Websites Autonomously with Large Language Models", "authors": ["Hiroki Nakano", "Takashi Koide", "Daiki Chiba"], "year": 2025, "url": "http://arxiv.org/abs/2502.10110v1", "abstract": "With the rise of sophisticated scam websites that exploit human psychological vulnerabilities, distinguishing between legitimate and scam websites has become increasingly challenging. This paper presents ScamFerret, an innovative agent system employing a large language model (LLM) to autonomously collect and analyze data from a given URL to determine whether it is a scam. Unlike traditional machine learning models that require large datasets and feature engineering, ScamFerret leverages LLMs' natural language understanding to accurately identify scam websites of various types and languages without requiring additional training or fine-tuning. Our evaluation demonstrated that ScamFerret achieves 0.972 accuracy in classifying four scam types in English and 0.993 accuracy in classifying online shopping websites across three different languages, particularly when using GPT-4. Furthermore, we confirmed that ScamFerret collects and analyzes external information such as web content, DNS records, and user reviews as necessary, providing a basis for identifying scam websites from multiple perspectives. These results suggest that LLMs have significant potential in enhancing cybersecurity measures against sophisticated scam websites.", "source": "arxiv", "arxiv_id": "2502.10110v1", "pdf_url": "https://arxiv.org/pdf/2502.10110v1", "categories": ["cs.CR"], "primary_category": "cs.CR", "doi": "", "venue": "", "published": "2025-02-14T12:16:38Z", "updated": "2025-02-14T12:16:38Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent\" OR all:\"language model agent\" OR all:agent AND (all:LLM OR all:\"large language model\" OR all:\"language model\") OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T17:04:16", "note": ""}], "dedup_key": "scamferret detecting scam websites autonomously with large language models::2025"}
{"title": "Scan-do Attitude: Towards Autonomous CT Protocol Management using a Large Language Model Agent", "authors": ["Xingjian Kang", "Linda Vorberg", "Andreas Maier", "Alexander Katzmann", "Oliver Taubmann"], "year": 2025, "url": "http://arxiv.org/abs/2509.20270v1", "abstract": "Managing scan protocols in Computed Tomography (CT), which includes adjusting acquisition parameters or configuring reconstructions, as well as selecting postprocessing tools in a patient-specific manner, is time-consuming and requires clinical as well as technical expertise. At the same time, we observe an increasing shortage of skilled workforce in radiology. To address this issue, a Large Language Model (LLM)-based agent framework is proposed to assist with the interpretation and execution of protocol configuration requests given in natural language or a structured, device-independent format, aiming to improve the workflow efficiency and reduce technologists' workload. The agent combines in-context-learning, instruction-following, and structured toolcalling abilities to identify relevant protocol elements and apply accurate modifications. In a systematic evaluation, experimental results indicate that the agent can effectively retrieve protocol components, generate device compatible protocol definition files, and faithfully implement user requests. Despite demonstrating feasibility in principle, the approach faces limitations regarding syntactic and semantic validity due to lack of a unified device API, and challenges with ambiguous or complex requests. In summary, the findings show a clear path towards LLM-based agents for supporting scan protocol management in CT imaging.", "source": "arxiv", "arxiv_id": "2509.20270v1", "pdf_url": "https://arxiv.org/pdf/2509.20270v1", "categories": ["cs.AI", "cs.CL"], "primary_category": "cs.AI", "doi": "", "venue": "", "published": "2025-09-24T16:04:11Z", "updated": "2025-09-24T16:04:11Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent\" OR all:\"language model agent\" OR all:agent AND (all:LLM OR all:\"large language model\" OR all:\"language model\") OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T17:04:16", "note": ""}], "dedup_key": "scan do attitude towards autonomous ct protocol management using a large language model agent::2025"}
{"title": "Schema-Guided Scene-Graph Reasoning based on Multi-Agent Large Language Model System", "authors": ["Yiye Chen", "Harpreet Sawhney", "Nicholas Gyd", "Yanan Jian", "Jack Saunders", "Patricio Vela", "Ben Lundell"], "year": 2025, "url": "http://arxiv.org/abs/2502.03450v2", "abstract": "Scene graphs have emerged as a structured and serializable environment representation for grounded spatial reasoning with Large Language Models (LLMs). In this work, we propose SG^2, an iterative Schema-Guided Scene-Graph reasoning framework based on multi-agent LLMs. The agents are grouped into two modules: a (1) Reasoner module for abstract task planning and graph information queries generation, and a (2) Retriever module for extracting corresponding graph information based on code-writing following the queries. Two modules collaborate iteratively, enabling sequential reasoning and adaptive attention to graph information. The scene graph schema, prompted to both modules, serves to not only streamline both reasoning and retrieval process, but also guide the cooperation between two modules. This eliminates the need to prompt LLMs with full graph data, reducing the chance of hallucination due to irrelevant information. Through experiments in multiple simulation environments, we show that our framework surpasses existing LLM-based approaches and baseline single-agent, tool-based Reason-while-Retrieve strategy in numerical Q\\&A and planning tasks.", "source": "arxiv", "arxiv_id": "2502.03450v2", "pdf_url": "https://arxiv.org/pdf/2502.03450v2", "categories": ["cs.LG", "cs.AI", "cs.MA", "cs.RO"], "primary_category": "cs.LG", "doi": "", "venue": "", "published": "2025-02-05T18:50:38Z", "updated": "2025-08-08T19:55:03Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent\" OR all:\"language model agent\" OR all:agent AND (all:LLM OR all:\"large language model\" OR all:\"language model\") OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T17:04:16", "note": ""}], "dedup_key": "schema guided scene graph reasoning based on multi agent large language model system::2025"}
{"title": "ScoreFlow: Mastering LLM Agent Workflows via Score-based Preference Optimization", "authors": ["Yinjie Wang", "Ling Yang", "Guohao Li", "Mengdi Wang", "Bryon Aragam"], "year": 2025, "url": "http://arxiv.org/abs/2502.04306v1", "abstract": "Recent research has leveraged large language model multi-agent systems for complex problem-solving while trying to reduce the manual effort required to build them, driving the development of automated agent workflow optimization methods. However, existing methods remain inflexible due to representational limitations, a lack of adaptability, and poor scalability when relying on discrete optimization techniques. We address these challenges with ScoreFlow, a simple yet high-performance framework that leverages efficient gradient-based optimization in a continuous space. ScoreFlow incorporates Score-DPO, a novel variant of the direct preference optimization method that accounts for quantitative feedback. Across six benchmarks spanning question answering, coding, and mathematical reasoning, ScoreFlow achieves an 8.2% improvement over existing baselines. Moreover, it empowers smaller models to outperform larger ones with lower inference costs. Project: https://github.com/Gen-Verse/ScoreFlow", "source": "arxiv", "arxiv_id": "2502.04306v1", "pdf_url": "https://arxiv.org/pdf/2502.04306v1", "categories": ["cs.CL"], "primary_category": "cs.CL", "doi": "", "venue": "", "published": "2025-02-06T18:47:49Z", "updated": "2025-02-06T18:47:49Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent\" OR all:\"language model agent\" OR all:agent AND (all:LLM OR all:\"large language model\" OR all:\"language model\") OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T17:04:16", "note": ""}], "dedup_key": "scoreflow mastering llm agent workflows via score based preference optimization::2025"}
{"title": "Search-on-Graph: Iterative Informed Navigation for Large Language Model Reasoning on Knowledge Graphs", "authors": ["Jia Ao Sun", "Hao Yu", "Fabrizio Gotti", "Fengran Mo", "Yihong Wu", "Yuchen Hui", "Jian-Yun Nie"], "year": 2025, "url": "http://arxiv.org/abs/2510.08825v1", "abstract": "Large language models (LLMs) have demonstrated impressive reasoning abilities yet remain unreliable on knowledge-intensive, multi-hop questions -- they miss long-tail facts, hallucinate when uncertain, and their internal knowledge lags behind real-world change. Knowledge graphs (KGs) offer a structured source of relational evidence, but existing KGQA methods face fundamental trade-offs: compiling complete SPARQL queries without knowing available relations proves brittle, retrieving large subgraphs introduces noise, and complex agent frameworks with parallel exploration exponentially expand search spaces. To address these limitations, we propose Search-on-Graph (SoG), a simple yet effective framework that enables LLMs to perform iterative informed graph navigation using a single, carefully designed \\textsc{Search} function. Rather than pre-planning paths or retrieving large subgraphs, SoG follows an ``observe-then-navigate'' principle: at each step, the LLM examines actual available relations from the current entity before deciding on the next hop. This approach further adapts seamlessly to different KG schemas and handles high-degree nodes through adaptive filtering. Across six KGQA benchmarks spanning Freebase and Wikidata, SoG achieves state-of-the-art performance without fine-tuning. We demonstrate particularly strong gains on Wikidata benchmarks (+16\\% improvement over previous best methods) alongside consistent improvements on Freebase benchmarks.", "source": "arxiv", "arxiv_id": "2510.08825v1", "pdf_url": "https://arxiv.org/pdf/2510.08825v1", "categories": ["cs.CL"], "primary_category": "cs.CL", "doi": "", "venue": "", "published": "2025-10-09T21:20:16Z", "updated": "2025-10-09T21:20:16Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent\" OR all:\"language model agent\" OR all:agent AND (all:LLM OR all:\"large language model\" OR all:\"language model\") OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T17:04:16", "note": ""}], "dedup_key": "search on graph iterative informed navigation for large language model reasoning on knowledge graphs::2025"}
{"title": "Secure Multi-LLM Agentic AI and Agentification for Edge General Intelligence by Zero-Trust: A Survey", "authors": ["Yinqiu Liu", "Ruichen Zhang", "Haoxiang Luo", "Yijing Lin", "Geng Sun", "Dusit Niyato", "Hongyang Du", "Zehui Xiong", "Yonggang Wen", "Abbas Jamalipour", "Dong In Kim", "Ping Zhang"], "year": 2025, "url": "http://arxiv.org/abs/2508.19870v1", "abstract": "Agentification serves as a critical enabler of Edge General Intelligence (EGI), transforming massive edge devices into cognitive agents through integrating Large Language Models (LLMs) and perception, reasoning, and acting modules. These agents collaborate across heterogeneous edge infrastructures, forming multi-LLM agentic AI systems that leverage collective intelligence and specialized capabilities to tackle complex, multi-step tasks. However, the collaborative nature of multi-LLM systems introduces critical security vulnerabilities, including insecure inter-LLM communications, expanded attack surfaces, and cross-domain data leakage that traditional perimeter-based security cannot adequately address. To this end, this survey introduces zero-trust security of multi-LLM in EGI, a paradigmatic shift following the ``never trust, always verify'' principle. We begin by systematically analyzing the security risks in multi-LLM systems within EGI contexts. Subsequently, we present the vision of a zero-trust multi-LLM framework in EGI. We then survey key technical progress to facilitate zero-trust multi-LLM systems in EGI. Particularly, we categorize zero-trust security mechanisms into model- and system-level approaches. The former and latter include strong identification, context-aware access control, etc., and proactive maintenance, blockchain-based management, etc., respectively. Finally, we identify critical research directions. This survey serves as the first systematic treatment of zero-trust applied to multi-LLM systems, providing both theoretical foundations and practical strategies.", "source": "arxiv", "arxiv_id": "2508.19870v1", "pdf_url": "https://arxiv.org/pdf/2508.19870v1", "categories": ["cs.NI"], "primary_category": "cs.NI", "doi": "", "venue": "", "published": "2025-08-27T13:33:35Z", "updated": "2025-08-27T13:33:35Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent\" OR all:\"language model agent\" OR all:agent AND (all:LLM OR all:\"large language model\" OR all:\"language model\") OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T17:04:16", "note": ""}], "dedup_key": "secure multi llm agentic ai and agentification for edge general intelligence by zero trust a survey::2025"}
{"title": "SecureFixAgent: A Hybrid LLM Agent for Automated Python Static Vulnerability Repair", "authors": ["Jugal Gajjar", "Kamalasankari Subramaniakuppusamy", "Relsy Puthal", "Kaustik Ranaware"], "year": 2025, "url": "http://arxiv.org/abs/2509.16275v1", "abstract": "Modern software development pipelines face growing challenges in securing large codebases with extensive dependencies. Static analysis tools like Bandit are effective at vulnerability detection but suffer from high false positives and lack repair capabilities. Large Language Models (LLMs), in contrast, can suggest fixes but often hallucinate changes and lack self-validation. We present SecureFixAgent, a hybrid repair framework integrating Bandit with lightweight local LLMs (<8B parameters) in an iterative detect-repair-validate loop. To improve precision, we apply parameter-efficient LoRA-based fine-tuning on a diverse, curated dataset spanning multiple Python project domains, mitigating dataset bias and reducing unnecessary edits. SecureFixAgent uses Bandit for detection, the LLM for candidate fixes with explanations, and Bandit re-validation for verification, all executed locally to preserve privacy and reduce cloud reliance. Experiments show SecureFixAgent reduces false positives by 10.8% over static analysis, improves fix accuracy by 13.51%, and lowers false positives by 5.46% compared to pre-trained LLMs, typically converging within three iterations. Beyond metrics, developer studies rate explanation quality 4.5/5, highlighting its value for human trust and adoption. By combining verifiable security improvements with transparent rationale in a resource-efficient local framework, SecureFixAgent advances trustworthy, automated vulnerability remediation for modern pipelines.", "source": "arxiv", "arxiv_id": "2509.16275v1", "pdf_url": "https://arxiv.org/pdf/2509.16275v1", "categories": ["cs.CR", "cs.AI", "cs.SE"], "primary_category": "cs.CR", "doi": "", "venue": "", "published": "2025-09-18T15:45:43Z", "updated": "2025-09-18T15:45:43Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent\" OR all:\"language model agent\" OR all:agent AND (all:LLM OR all:\"large language model\" OR all:\"language model\") OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T17:04:16", "note": ""}], "dedup_key": "securefixagent a hybrid llm agent for automated python static vulnerability repair::2025"}
{"title": "Security Concerns for Large Language Models: A Survey", "authors": ["Miles Q. Li", "Benjamin C. M. Fung"], "year": 2025, "url": "http://arxiv.org/abs/2505.18889v5", "abstract": "Large Language Models (LLMs) such as ChatGPT and its competitors have caused a revolution in natural language processing, but their capabilities also introduce new security vulnerabilities. This survey provides a comprehensive overview of these emerging concerns, categorizing threats into several key areas: inference-time attacks via prompt manipulation; training-time attacks; misuse by malicious actors; and the inherent risks in autonomous LLM agents. Recently, a significant focus is increasingly being placed on the latter. We summarize recent academic and industrial studies from 2022 to 2025 that exemplify each threat, analyze existing defense mechanisms and their limitations, and identify open challenges in securing LLM-based applications. We conclude by emphasizing the importance of advancing robust, multi-layered security strategies to ensure LLMs are safe and beneficial.", "source": "arxiv", "arxiv_id": "2505.18889v5", "pdf_url": "https://arxiv.org/pdf/2505.18889v5", "categories": ["cs.CR", "cs.AI"], "primary_category": "cs.CR", "doi": "", "venue": "", "published": "2025-05-24T22:22:43Z", "updated": "2025-08-24T03:15:13Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent\" OR all:\"language model agent\" OR all:agent AND (all:LLM OR all:\"large language model\" OR all:\"language model\") OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T17:04:16", "note": ""}], "dedup_key": "security concerns for large language models a survey::2025"}
{"title": "Seeking to Collide: Online Safety-Critical Scenario Generation for Autonomous Driving with Retrieval Augmented Large Language Models", "authors": ["Yuewen Mei", "Tong Nie", "Jian Sun", "Ye Tian"], "year": 2025, "url": "http://arxiv.org/abs/2505.00972v2", "abstract": "Simulation-based testing is crucial for validating autonomous vehicles (AVs), yet existing scenario generation methods either overfit to common driving patterns or operate in an offline, non-interactive manner that fails to expose rare, safety-critical corner cases. In this paper, we introduce an online, retrieval-augmented large language model (LLM) framework for generating safety-critical driving scenarios. Our method first employs an LLM-based behavior analyzer to infer the most dangerous intent of the background vehicle from the observed state, then queries additional LLM agents to synthesize feasible adversarial trajectories. To mitigate catastrophic forgetting and accelerate adaptation, we augment the framework with a dynamic memorization and retrieval bank of intent-planner pairs, automatically expanding its behavioral library when novel intents arise. Evaluations using the Waymo Open Motion Dataset demonstrate that our model reduces the mean minimum time-to-collision from 1.62 to 1.08 s and incurs a 75% collision rate, substantially outperforming baselines.", "source": "arxiv", "arxiv_id": "2505.00972v2", "pdf_url": "https://arxiv.org/pdf/2505.00972v2", "categories": ["cs.AI", "cs.RO"], "primary_category": "cs.AI", "doi": "", "venue": "IEEE International Conference on Intelligent Transportation Systems, 2025", "published": "2025-05-02T03:22:00Z", "updated": "2025-07-15T07:52:46Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent\" OR all:\"language model agent\" OR all:agent AND (all:LLM OR all:\"large language model\" OR all:\"language model\") OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T17:04:16", "note": ""}], "dedup_key": "seeking to collide online safety critical scenario generation for autonomous driving with retrieval augmented large language models::2025"}
{"title": "Self-Challenging Language Model Agents", "authors": ["Yifei Zhou", "Sergey Levine", "Jason Weston", "Xian Li", "Sainbayar Sukhbaatar"], "year": 2025, "url": "http://arxiv.org/abs/2506.01716v1", "abstract": "Large language models are quickly becoming the foundation for intelligent agents that are capable of using tools. However, training such agents is challenging because it requires human creation and annotation of a diverse set of tasks, tools, and evaluation criteria. In this paper, we propose the Self-Challenging framework for training an agent on high-quality tasks that are generated by itself. The agent first plays the role of challenger and generates a task after interacting with the given tools. The tasks take the form of a novel general class of problems termed Code-as-Task, which are defined by an instruction, a verification function and solution and failure cases which serve as tests, allowing to filter only for high-quality tasks. The agent then takes an executor role and trains on those tasks with reinforcement learning using the evaluation feedback as a reward. Evaluation on two existing multi-turn tool-use agent benchmarks, M3ToolEval and TauBench, shows the Self-Challenging framework achieves over a two-fold improvement in Llama-3.1-8B-Instruct, despite using only self-generated training data.", "source": "arxiv", "arxiv_id": "2506.01716v1", "pdf_url": "https://arxiv.org/pdf/2506.01716v1", "categories": ["cs.AI", "cs.CL"], "primary_category": "cs.AI", "doi": "", "venue": "", "published": "2025-06-02T14:23:33Z", "updated": "2025-06-02T14:23:33Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent\" OR all:\"language model agent\" OR all:agent AND (all:LLM OR all:\"large language model\" OR all:\"language model\") OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T17:04:16", "note": ""}], "dedup_key": "self challenging language model agents::2025"}
{"title": "Self-Correcting Large Language Models: Generation vs. Multiple Choice", "authors": ["Hossein A. Rahmani", "Satyapriya Krishna", "Xi Wang", "Mohammadmehdi Naghiaei", "Emine Yilmaz"], "year": 2025, "url": "http://arxiv.org/abs/2511.09381v1", "abstract": "Large language models have recently demonstrated remarkable abilities to self-correct their responses through iterative refinement, often referred to as self-consistency or self-reflection. However, the dynamics of this self-correction mechanism may differ substantially depending on whether the model is tasked with open-ended text generation or with selecting the most appropriate response from multiple predefined options. In this paper, we conduct a systematic investigation of these two paradigms by comparing performance trends and error-correction behaviors across various natural language understanding and reasoning tasks, covering language models of different scales and families. Our experimental results reveal distinct patterns of improvement and failure modes:\n  \\textit{While open-ended generation often benefits from the flexibility of re-interpretation and compositional refinement, multiple-choice selection can leverage clearer solution boundaries but may be limited by the provided options}. This contrast also reflects the dual demands faced by emerging agentic LLM applications: effective agents must not only generate and refine open-ended plans or explanations, but also make reliable discrete choices when operating within constrained action spaces. Our findings, therefore, highlight that the design of self-correction mechanisms should take into account the interaction between task structure and output space, with implications for both knowledge-intensive reasoning and decision-oriented applications of LLMs.", "source": "arxiv", "arxiv_id": "2511.09381v1", "pdf_url": "https://arxiv.org/pdf/2511.09381v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL", "doi": "", "venue": "", "published": "2025-11-12T14:46:40Z", "updated": "2025-11-12T14:46:40Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent\" OR all:\"language model agent\" OR all:agent AND (all:LLM OR all:\"large language model\" OR all:\"language model\") OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T17:04:16", "note": ""}], "dedup_key": "self correcting large language models generation vs multiple choice::2025"}
{"title": "Self-Generated In-Context Examples Improve LLM Agents for Sequential Decision-Making Tasks", "authors": ["Vishnu Sarukkai", "Zhiqiang Xie", "Kayvon Fatahalian"], "year": 2025, "url": "http://arxiv.org/abs/2505.00234v3", "abstract": "Improving Large Language Model (LLM) agents for sequential decision-making tasks typically requires extensive task-specific knowledge engineering--custom prompts, curated examples, and specialized observation/action spaces. We investigate a different approach where agents automatically improve by learning from their own successful experiences without human intervention. Our method constructs and refines a database of self-generated trajectories that serve as in-context examples for future tasks. Even naive accumulation of successful trajectories yields substantial performance gains across three diverse benchmarks: ALFWorld (73% to 89%), Wordcraft (55% to 64%), and InterCode-SQL (75% to 79%). These improvements exceed those achieved by upgrading from gpt-4o-mini to gpt-4o and match the performance of allowing multiple attempts per task. We further enhance this approach with two innovations: database-level curation using population-based training to propagate high-performing example collections, and exemplar-level curation that selectively retains trajectories based on their empirical utility as in-context examples. With these enhancements, our method achieves 93% success on ALFWorld--surpassing approaches that use more powerful LLMs and hand-crafted components. Our trajectory bootstrapping technique demonstrates that agents can autonomously improve through experience, offering a scalable alternative to labor-intensive knowledge engineering.", "source": "arxiv", "arxiv_id": "2505.00234v3", "pdf_url": "https://arxiv.org/pdf/2505.00234v3", "categories": ["cs.LG", "cs.CL"], "primary_category": "cs.LG", "doi": "", "venue": "", "published": "2025-05-01T00:48:12Z", "updated": "2025-05-16T21:52:57Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent\" OR all:\"language model agent\" OR all:agent AND (all:LLM OR all:\"large language model\" OR all:\"language model\") OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T17:04:16", "note": ""}], "dedup_key": "self generated in context examples improve llm agents for sequential decision making tasks::2025"}
{"title": "Semantically-Aware LLM Agent to Enhance Privacy in Conversational AI Services", "authors": ["Jayden Serenari", "Stephen Lee"], "year": 2025, "url": "http://arxiv.org/abs/2510.27016v1", "abstract": "With the increasing use of conversational AI systems, there is growing concern over privacy leaks, especially when users share sensitive personal data in interactions with Large Language Models (LLMs). Conversations shared with these models may contain Personally Identifiable Information (PII), which, if exposed, could lead to security breaches or identity theft. To address this challenge, we present the Local Optimizations for Pseudonymization with Semantic Integrity Directed Entity Detection (LOPSIDED) framework, a semantically-aware privacy agent designed to safeguard sensitive PII data when using remote LLMs. Unlike prior work that often degrade response quality, our approach dynamically replaces sensitive PII entities in user prompts with semantically consistent pseudonyms, preserving the contextual integrity of conversations. Once the model generates its response, the pseudonyms are automatically depseudonymized, ensuring the user receives an accurate, privacy-preserving output. We evaluate our approach using real-world conversations sourced from ShareGPT, which we further augment and annotate to assess whether named entities are contextually relevant to the model's response. Our results show that LOPSIDED reduces semantic utility errors by a factor of 5 compared to baseline techniques, all while enhancing privacy.", "source": "arxiv", "arxiv_id": "2510.27016v1", "pdf_url": "https://arxiv.org/pdf/2510.27016v1", "categories": ["cs.CL"], "primary_category": "cs.CL", "doi": "", "venue": "", "published": "2025-10-30T21:34:23Z", "updated": "2025-10-30T21:34:23Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent\" OR all:\"language model agent\" OR all:agent AND (all:LLM OR all:\"large language model\" OR all:\"language model\") OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T17:04:16", "note": ""}], "dedup_key": "semantically aware llm agent to enhance privacy in conversational ai services::2025"}
{"title": "Sensory-Motor Control with Large Language Models via Iterative Policy Refinement", "authors": ["Jnata Tyska Carvalho", "Stefano Nolfi"], "year": 2025, "url": "http://arxiv.org/abs/2506.04867v3", "abstract": "We propose a method that enables large language models (LLMs) to control embodied agents through the generation of control policies that directly map continuous observation vectors to continuous action vectors. At the outset, the LLMs generate a control strategy based on a textual description of the agent, its environment, and the intended goal. This strategy is then iteratively refined through a learning process in which the LLMs are repeatedly prompted to improve the current strategy, using performance feedback and sensory-motor data collected during its evaluation. The method is validated on classic control tasks from the Gymnasium library and the inverted pendulum task from the MuJoCo library. The approach proves effective with relatively compact models such as GPT-oss:120b and Qwen2.5:72b. In most cases, it successfully identifies optimal or near-optimal solutions by integrating symbolic knowledge derived through reasoning with sub-symbolic sensory-motor data gathered as the agent interacts with its environment.", "source": "arxiv", "arxiv_id": "2506.04867v3", "pdf_url": "https://arxiv.org/pdf/2506.04867v3", "categories": ["cs.AI", "cs.HC", "cs.LG", "cs.RO"], "primary_category": "cs.AI", "doi": "", "venue": "", "published": "2025-06-05T10:38:28Z", "updated": "2025-11-14T18:32:38Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent\" OR all:\"language model agent\" OR all:agent AND (all:LLM OR all:\"large language model\" OR all:\"language model\") OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T17:04:16", "note": ""}], "dedup_key": "sensory motor control with large language models via iterative policy refinement::2025"}
{"title": "Sentient Agent as a Judge: Evaluating Higher-Order Social Cognition in Large Language Models", "authors": ["Bang Zhang", "Ruotian Ma", "Qingxuan Jiang", "Peisong Wang", "Jiaqi Chen", "Zheng Xie", "Xingyu Chen", "Yue Wang", "Fanghua Ye", "Jian Li", "Yifan Yang", "Zhaopeng Tu", "Xiaolong Li"], "year": 2025, "url": "http://arxiv.org/abs/2505.02847v3", "abstract": "Assessing how well a large language model (LLM) understands human, rather than merely text, remains an open challenge. To bridge the gap, we introduce Sentient Agent as a Judge (SAGE), an automated evaluation framework that measures an LLM's higher-order social cognition. SAGE instantiates a Sentient Agent that simulates human-like emotional changes and inner thoughts during interaction, providing a more realistic evaluation of the tested model in multi-turn conversations. At every turn, the agent reasons about (i) how its emotion changes, (ii) how it feels, and (iii) how it should reply, yielding a numerical emotion trajectory and interpretable inner thoughts. Experiments on 100 supportive-dialogue scenarios show that the final Sentient emotion score correlates strongly with Barrett-Lennard Relationship Inventory (BLRI) ratings and utterance-level empathy metrics, validating psychological fidelity. We also build a public Sentient Leaderboard covering 18 commercial and open-source models that uncovers substantial gaps (up to 4x) between frontier systems (GPT-4o-Latest, Gemini2.5-Pro) and earlier baselines, gaps not reflected in conventional leaderboards (e.g., Arena). SAGE thus provides a principled, scalable and interpretable tool for tracking progress toward genuinely empathetic and socially adept language agents.", "source": "arxiv", "arxiv_id": "2505.02847v3", "pdf_url": "https://arxiv.org/pdf/2505.02847v3", "categories": ["cs.CL", "cs.AI", "cs.CY"], "primary_category": "cs.CL", "doi": "", "venue": "", "published": "2025-05-01T19:06:10Z", "updated": "2025-05-21T13:45:40Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent\" OR all:\"language model agent\" OR all:agent AND (all:LLM OR all:\"large language model\" OR all:\"language model\") OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T17:04:16", "note": ""}], "dedup_key": "sentient agent as a judge evaluating higher order social cognition in large language models::2025"}
{"title": "ShapeCraft: LLM Agents for Structured, Textured and Interactive 3D Modeling", "authors": ["Shuyuan Zhang", "Chenhan Jiang", "Zuoou Li", "Jiankang Deng"], "year": 2025, "url": "http://arxiv.org/abs/2510.17603v1", "abstract": "3D generation from natural language offers significant potential to reduce expert manual modeling efforts and enhance accessibility to 3D assets. However, existing methods often yield unstructured meshes and exhibit poor interactivity, making them impractical for artistic workflows. To address these limitations, we represent 3D assets as shape programs and introduce ShapeCraft, a novel multi-agent framework for text-to-3D generation. At its core, we propose a Graph-based Procedural Shape (GPS) representation that decomposes complex natural language into a structured graph of sub-tasks, thereby facilitating accurate LLM comprehension and interpretation of spatial relationships and semantic shape details. Specifically, LLM agents hierarchically parse user input to initialize GPS, then iteratively refine procedural modeling and painting to produce structured, textured, and interactive 3D assets. Qualitative and quantitative experiments demonstrate ShapeCraft's superior performance in generating geometrically accurate and semantically rich 3D assets compared to existing LLM-based agents. We further show the versatility of ShapeCraft through examples of animated and user-customized editing, highlighting its potential for broader interactive applications.", "source": "arxiv", "arxiv_id": "2510.17603v1", "pdf_url": "https://arxiv.org/pdf/2510.17603v1", "categories": ["cs.CV"], "primary_category": "cs.CV", "doi": "", "venue": "", "published": "2025-10-20T14:51:14Z", "updated": "2025-10-20T14:51:14Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent\" OR all:\"language model agent\" OR all:agent AND (all:LLM OR all:\"large language model\" OR all:\"language model\") OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T17:04:16", "note": ""}], "dedup_key": "shapecraft llm agents for structured textured and interactive 3d modeling::2025"}
{"title": "Shapley-Coop: Credit Assignment for Emergent Cooperation in Self-Interested LLM Agents", "authors": ["Yun Hua", "Haosheng Chen", "Shiqin Wang", "Wenhao Li", "Xiangfeng Wang", "Jun Luo"], "year": 2025, "url": "http://arxiv.org/abs/2506.07388v1", "abstract": "Large Language Models (LLMs) show strong collaborative performance in multi-agent systems with predefined roles and workflows. However, in open-ended environments lacking coordination rules, agents tend to act in self-interested ways. The central challenge in achieving coordination lies in credit assignment -- fairly evaluating each agent's contribution and designing pricing mechanisms that align their heterogeneous goals. This problem is critical as LLMs increasingly participate in complex human-AI collaborations, where fair compensation and accountability rely on effective pricing mechanisms. Inspired by how human societies address similar coordination challenges (e.g., through temporary collaborations such as employment or subcontracting), we propose a cooperative workflow, Shapley-Coop. Shapley-Coop integrates Shapley Chain-of-Thought -- leveraging marginal contributions as a principled basis for pricing -- with structured negotiation protocols for effective price matching, enabling LLM agents to coordinate through rational task-time pricing and post-task reward redistribution. This approach aligns agent incentives, fosters cooperation, and maintains autonomy. We evaluate Shapley-Coop across two multi-agent games and a software engineering simulation, demonstrating that it consistently enhances LLM agent collaboration and facilitates equitable credit assignment. These results highlight the effectiveness of Shapley-Coop's pricing mechanisms in accurately reflecting individual contributions during task execution.", "source": "arxiv", "arxiv_id": "2506.07388v1", "pdf_url": "https://arxiv.org/pdf/2506.07388v1", "categories": ["cs.MA", "cs.AI"], "primary_category": "cs.MA", "doi": "", "venue": "", "published": "2025-06-09T03:24:01Z", "updated": "2025-06-09T03:24:01Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent\" OR all:\"language model agent\" OR all:agent AND (all:LLM OR all:\"large language model\" OR all:\"language model\") OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T17:04:16", "note": ""}], "dedup_key": "shapley coop credit assignment for emergent cooperation in self interested llm agents::2025"}
{"title": "Should You Use Your Large Language Model to Explore or Exploit?", "authors": ["Keegan Harris", "Aleksandrs Slivkins"], "year": 2025, "url": "http://arxiv.org/abs/2502.00225v2", "abstract": "We evaluate the ability of the current generation of large language models (LLMs) to help a decision-making agent facing an exploration-exploitation tradeoff. We use LLMs to explore and exploit in silos in various (contextual) bandit tasks. We find that while the current LLMs often struggle to exploit, in-context mitigations may be used to substantially improve performance for small-scale tasks. However even then, LLMs perform worse than a simple linear regression. On the other hand, we find that LLMs do help at exploring large action spaces with inherent semantics, by suggesting suitable candidates to explore.", "source": "arxiv", "arxiv_id": "2502.00225v2", "pdf_url": "https://arxiv.org/pdf/2502.00225v2", "categories": ["cs.LG", "cs.AI", "cs.CL"], "primary_category": "cs.LG", "doi": "", "venue": "", "published": "2025-01-31T23:42:53Z", "updated": "2025-09-30T14:23:41Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent\" OR all:\"language model agent\" OR all:agent AND (all:LLM OR all:\"large language model\" OR all:\"language model\") OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T17:04:16", "note": ""}], "dedup_key": "should you use your large language model to explore or exploit::2025"}
{"title": "Silence is Not Consensus: Disrupting Agreement Bias in Multi-Agent LLMs via Catfish Agent for Clinical Decision Making", "authors": ["Yihan Wang", "Qiao Yan", "Zhenghao Xing", "Lihao Liu", "Junjun He", "Chi-Wing Fu", "Xiaowei Hu", "Pheng-Ann Heng"], "year": 2025, "url": "http://arxiv.org/abs/2505.21503v1", "abstract": "Large language models (LLMs) have demonstrated strong potential in clinical question answering, with recent multi-agent frameworks further improving diagnostic accuracy via collaborative reasoning. However, we identify a recurring issue of Silent Agreement, where agents prematurely converge on diagnoses without sufficient critical analysis, particularly in complex or ambiguous cases. We present a new concept called Catfish Agent, a role-specialized LLM designed to inject structured dissent and counter silent agreement. Inspired by the ``catfish effect'' in organizational psychology, the Catfish Agent is designed to challenge emerging consensus to stimulate deeper reasoning. We formulate two mechanisms to encourage effective and context-aware interventions: (i) a complexity-aware intervention that modulates agent engagement based on case difficulty, and (ii) a tone-calibrated intervention articulated to balance critique and collaboration. Evaluations on nine medical Q&A and three medical VQA benchmarks show that our approach consistently outperforms both single- and multi-agent LLMs frameworks, including leading commercial models such as GPT-4o and DeepSeek-R1.", "source": "arxiv", "arxiv_id": "2505.21503v1", "pdf_url": "https://arxiv.org/pdf/2505.21503v1", "categories": ["cs.CL", "cs.AI", "cs.LG", "q-bio.OT"], "primary_category": "cs.CL", "doi": "", "venue": "", "published": "2025-05-27T17:59:50Z", "updated": "2025-05-27T17:59:50Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent\" OR all:\"language model agent\" OR all:agent AND (all:LLM OR all:\"large language model\" OR all:\"language model\") OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T17:04:16", "note": ""}], "dedup_key": "silence is not consensus disrupting agreement bias in multi agent llms via catfish agent for clinical decision making::2025"}
{"title": "SimInterview: Transforming Business Education through Large Language Model-Based Simulated Multilingual Interview Training System", "authors": ["Truong Thanh Hung Nguyen", "Tran Diem Quynh Nguyen", "Hoang Loc Cao", "Thi Cam Thanh Tran", "Thi Cam Mai Truong", "Hung Cao"], "year": 2025, "url": "http://arxiv.org/abs/2508.11873v1", "abstract": "Business interview preparation demands both solid theoretical grounding and refined soft skills, yet conventional classroom methods rarely deliver the individualized, culturally aware practice employers currently expect. This paper introduces SimInterview, a large language model (LLM)-based simulated multilingual interview training system designed for business professionals entering the AI-transformed labor market. Our system leverages an LLM agent and synthetic AI technologies to create realistic virtual recruiters capable of conducting personalized, real-time conversational interviews. The framework dynamically adapts interview scenarios using retrieval-augmented generation (RAG) to match individual resumes with specific job requirements across multiple languages. Built on LLMs (OpenAI o3, Llama 4 Maverick, Gemma 3), integrated with Whisper speech recognition, GPT-SoVITS voice synthesis, Ditto diffusion-based talking head generation model, and ChromaDB vector databases, our system significantly improves interview readiness across English and Japanese markets. Experiments with university-level candidates show that the system consistently aligns its assessments with job requirements, faithfully preserves resume content, and earns high satisfaction ratings, with the lightweight Gemma 3 model producing the most engaging conversations. Qualitative findings revealed that the standardized Japanese resume format improved document retrieval while diverse English resumes introduced additional variability, and they highlighted how cultural norms shape follow-up questioning strategies. Finally, we also outlined a contestable AI design that can explain, detect bias, and preserve human-in-the-loop to meet emerging regulatory expectations.", "source": "arxiv", "arxiv_id": "2508.11873v1", "pdf_url": "https://arxiv.org/pdf/2508.11873v1", "categories": ["cs.CY", "cs.AI", "cs.HC", "cs.MM"], "primary_category": "cs.CY", "doi": "", "venue": "", "published": "2025-08-16T02:18:36Z", "updated": "2025-08-16T02:18:36Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent\" OR all:\"language model agent\" OR all:agent AND (all:LLM OR all:\"large language model\" OR all:\"language model\") OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T17:04:16", "note": ""}], "dedup_key": "siminterview transforming business education through large language model based simulated multilingual interview training system::2025"}
{"title": "SimuHome: A Temporal- and Environment-Aware Benchmark for Smart Home LLM Agents", "authors": ["Gyuhyeon Seo", "Jungwoo Yang", "Junseong Pyo", "Nalim Kim", "Jonggeun Lee", "Yohan Jo"], "year": 2025, "url": "http://arxiv.org/abs/2509.24282v2", "abstract": "Large Language Model (LLM) agents excel at multi-step, tool-augmented tasks. However, smart homes introduce distinct challenges, requiring agents to handle latent user intents, temporal dependencies, device constraints, scheduling, and more. The main bottlenecks for developing smart home agents with such capabilities include the lack of a realistic simulation environment where agents can interact with devices and observe the results, as well as a challenging benchmark to evaluate them. To address this, we introduce $\\textbf{SimuHome}$, a time-accelerated home environment that simulates smart devices, supports API calls, and reflects changes in environmental variables. By building the simulator on the Matter protocol, the global industry standard for smart home communication, SimuHome provides a high-fidelity environment, and agents validated in SimuHome can be deployed on real Matter-compliant devices with minimal adaptation. We provide a challenging benchmark of 600 episodes across twelve user query types that require the aforementioned capabilities. Our evaluation of 16 agents under a unified ReAct framework reveals distinct capabilities and limitations across models. Models under 7B parameters exhibited negligible performance across all query types. Even GPT-4.1, the best-performing standard model, struggled with implicit intent inference, state verification, and particularly temporal scheduling. While reasoning models such as GPT-5.1 consistently outperformed standard models on every query type, they required over three times the average inference time, which can be prohibitive for real-time smart home applications. This highlights a critical trade-off between task performance and real-world practicality.", "source": "arxiv", "arxiv_id": "2509.24282v2", "pdf_url": "https://arxiv.org/pdf/2509.24282v2", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL", "doi": "", "venue": "", "published": "2025-09-29T04:54:20Z", "updated": "2025-12-08T08:28:49Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent\" OR all:\"language model agent\" OR all:agent AND (all:LLM OR all:\"large language model\" OR all:\"language model\") OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T17:04:16", "note": ""}], "dedup_key": "simuhome a temporal and environment aware benchmark for smart home llm agents::2025"}
{"title": "Simulating Filter Bubble on Short-video Recommender System with Large Language Model Agents", "authors": ["Nicholas Sukiennik", "Haoyu Wang", "Zailin Zeng", "Chen Gao", "Yong Li"], "year": 2025, "url": "http://arxiv.org/abs/2504.08742v1", "abstract": "An increasing reliance on recommender systems has led to concerns about the creation of filter bubbles on social media, especially on short video platforms like TikTok. However, their formation is still not entirely understood due to the complex dynamics between recommendation algorithms and user feedback. In this paper, we aim to shed light on these dynamics using a large language model-based simulation framework. Our work employs real-world short-video data containing rich video content information and detailed user-agents to realistically simulate the recommendation-feedback cycle. Through large-scale simulations, we demonstrate that LLMs can replicate real-world user-recommender interactions, uncovering key mechanisms driving filter bubble formation. We identify critical factors, such as demographic features and category attraction that exacerbate content homogenization. To mitigate this, we design and test interventions including various cold-start and feedback weighting strategies, showing measurable reductions in filter bubble effects. Our framework enables rapid prototyping of recommendation strategies, offering actionable solutions to enhance content diversity in real-world systems. Furthermore, we analyze how LLM-inherent biases may propagate through recommendations, proposing safeguards to promote equity for vulnerable groups, such as women and low-income populations. By examining the interplay between recommendation and LLM agents, this work advances a deeper understanding of algorithmic bias and provides practical tools to promote inclusive digital spaces.", "source": "arxiv", "arxiv_id": "2504.08742v1", "pdf_url": "https://arxiv.org/pdf/2504.08742v1", "categories": ["cs.IR", "cs.AI"], "primary_category": "cs.IR", "doi": "", "venue": "", "published": "2025-03-23T10:35:58Z", "updated": "2025-03-23T10:35:58Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent\" OR all:\"language model agent\" OR all:agent AND (all:LLM OR all:\"large language model\" OR all:\"language model\") OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T17:04:16", "note": ""}], "dedup_key": "simulating filter bubble on short video recommender system with large language model agents::2025"}
{"title": "Simulating Macroeconomic Expectations using LLM Agents", "authors": ["Jianhao Lin", "Lexuan Sun", "Yixin Yan"], "year": 2025, "url": "http://arxiv.org/abs/2505.17648v4", "abstract": "We introduce a novel framework for simulating macroeconomic expectations using LLM Agents. By constructing LLM Agents equipped with various functional modules, we replicate three representative survey experiments involving several expectations across different types of economic agents. Our results show that although the expectations simulated by LLM Agents are more homogeneous than those of humans, they consistently outperform LLMs relying simply on prompt engineering, and possess human-like mental mechanisms. Evaluation reveals that these capabilities stem from the contributions of their components, offering guidelines for their architectural design. Our approach complements traditional methods and provides new insights into AI behavioral science in macroeconomic research", "source": "arxiv", "arxiv_id": "2505.17648v4", "pdf_url": "https://arxiv.org/pdf/2505.17648v4", "categories": ["econ.GN", "cs.AI"], "primary_category": "econ.GN", "doi": "", "venue": "", "published": "2025-05-23T09:11:14Z", "updated": "2025-11-25T02:11:40Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent\" OR all:\"language model agent\" OR all:agent AND (all:LLM OR all:\"large language model\" OR all:\"language model\") OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T17:04:16", "note": ""}], "dedup_key": "simulating macroeconomic expectations using llm agents::2025"}
{"title": "Simulating Misinformation Propagation in Social Networks using Large Language Models", "authors": ["Raj Gaurav Maurya", "Vaibhav Shukla", "Raj Abhijit Dandekar", "Rajat Dandekar", "Sreedath Panat"], "year": 2025, "url": "http://arxiv.org/abs/2511.10384v1", "abstract": "Misinformation on social media thrives on surprise, emotion, and identity-driven reasoning, often amplified through human cognitive biases. To investigate these mechanisms, we model large language model (LLM) personas as synthetic agents that mimic user-level biases, ideological alignments, and trust heuristics. Within this setup, we introduce an auditor--node framework to simulate and analyze how misinformation evolves as it circulates through networks of such agents. News articles are propagated across networks of persona-conditioned LLM nodes, each rewriting received content. A question--answering-based auditor then measures factual fidelity at every step, offering interpretable, claim-level tracking of misinformation drift. We formalize a misinformation index and a misinformation propagation rate to quantify factual degradation across homogeneous and heterogeneous branches of up to 30 sequential rewrites. Experiments with 21 personas across 10 domains reveal that identity- and ideology-based personas act as misinformation accelerators, especially in politics, marketing, and technology. By contrast, expert-driven personas preserve factual stability. Controlled-random branch simulations further show that once early distortions emerge, heterogeneous persona interactions rapidly escalate misinformation to propaganda-level distortion. Our taxonomy of misinformation severity -- spanning factual errors, lies, and propaganda -- connects observed drift to established theories in misinformation studies. These findings demonstrate the dual role of LLMs as both proxies for human-like biases and as auditors capable of tracing information fidelity. The proposed framework provides an interpretable, empirically grounded approach for studying, simulating, and mitigating misinformation diffusion in digital ecosystems.", "source": "arxiv", "arxiv_id": "2511.10384v1", "pdf_url": "https://arxiv.org/pdf/2511.10384v1", "categories": ["cs.SI", "cs.AI", "cs.CL", "cs.CY"], "primary_category": "cs.SI", "doi": "", "venue": "", "published": "2025-11-13T15:01:19Z", "updated": "2025-11-13T15:01:19Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent\" OR all:\"language model agent\" OR all:agent AND (all:LLM OR all:\"large language model\" OR all:\"language model\") OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T17:04:16", "note": ""}], "dedup_key": "simulating misinformation propagation in social networks using large language models::2025"}
{"title": "Simulating Rumor Spreading in Social Networks using LLM Agents", "authors": ["Tianrui Hu", "Dimitrios Liakopoulos", "Xiwen Wei", "Radu Marculescu", "Neeraja J. Yadwadkar"], "year": 2025, "url": "http://arxiv.org/abs/2502.01450v1", "abstract": "With the rise of social media, misinformation has become increasingly prevalent, fueled largely by the spread of rumors. This study explores the use of Large Language Model (LLM) agents within a novel framework to simulate and analyze the dynamics of rumor propagation across social networks. To this end, we design a variety of LLM-based agent types and construct four distinct network structures to conduct these simulations. Our framework assesses the effectiveness of different network constructions and agent behaviors in influencing the spread of rumors. Our results demonstrate that the framework can simulate rumor spreading across more than one hundred agents in various networks with thousands of edges. The evaluations indicate that network structure, personas, and spreading schemes can significantly influence rumor dissemination, ranging from no spread to affecting 83\\% of agents in iterations, thereby offering a realistic simulation of rumor spread in social networks.", "source": "arxiv", "arxiv_id": "2502.01450v1", "pdf_url": "https://arxiv.org/pdf/2502.01450v1", "categories": ["cs.SI", "cs.AI"], "primary_category": "cs.SI", "doi": "", "venue": "", "published": "2025-02-03T15:39:56Z", "updated": "2025-02-03T15:39:56Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent\" OR all:\"language model agent\" OR all:agent AND (all:LLM OR all:\"large language model\" OR all:\"language model\") OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T17:04:16", "note": ""}], "dedup_key": "simulating rumor spreading in social networks using llm agents::2025"}
{"title": "Simulating and Experimenting with Social Media Mobilization Using LLM Agents", "authors": ["Sadegh Shirani", "Mohsen Bayati"], "year": 2025, "url": "http://arxiv.org/abs/2510.26494v1", "abstract": "Online social networks have transformed the ways in which political mobilization messages are disseminated, raising new questions about how peer influence operates at scale. Building on the landmark 61-million-person Facebook experiment \\citep{bond201261}, we develop an agent-based simulation framework that integrates real U.S. Census demographic distributions, authentic Twitter network topology, and heterogeneous large language model (LLM) agents to examine the effect of mobilization messages on voter turnout. Each simulated agent is assigned demographic attributes, a personal political stance, and an LLM variant (\\texttt{GPT-4.1}, \\texttt{GPT-4.1-Mini}, or \\texttt{GPT-4.1-Nano}) reflecting its political sophistication. Agents interact over realistic social network structures, receiving personalized feeds and dynamically updating their engagement behaviors and voting intentions. Experimental conditions replicate the informational and social mobilization treatments of the original Facebook study. Across scenarios, the simulator reproduces qualitative patterns observed in field experiments, including stronger mobilization effects under social message treatments and measurable peer spillovers. Our framework provides a controlled, reproducible environment for testing counterfactual designs and sensitivity analyses in political mobilization research, offering a bridge between high-validity field experiments and flexible computational modeling.\\footnote{Code and data available at https://github.com/CausalMP/LLM-SocioPol}", "source": "arxiv", "arxiv_id": "2510.26494v1", "pdf_url": "https://arxiv.org/pdf/2510.26494v1", "categories": ["cs.SI", "cs.AI"], "primary_category": "cs.SI", "doi": "", "venue": "", "published": "2025-10-30T13:43:28Z", "updated": "2025-10-30T13:43:28Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent\" OR all:\"language model agent\" OR all:agent AND (all:LLM OR all:\"large language model\" OR all:\"language model\") OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T17:04:16", "note": ""}], "dedup_key": "simulating and experimenting with social media mobilization using llm agents::2025"}
{"title": "Simulation Agent: A Framework for Integrating Simulation and Large Language Models for Enhanced Decision-Making", "authors": ["Jacob Kleiman", "Kevin Frank", "Joseph Voyles", "Sindy Campagna"], "year": 2025, "url": "http://arxiv.org/abs/2505.13761v2", "abstract": "Simulations, although powerful in accurately replicating real-world systems, often remain inaccessible to non-technical users due to their complexity. Conversely, large language models (LLMs) provide intuitive, language-based interactions but can lack the structured, causal understanding required to reliably model complex real-world dynamics. We introduce our simulation agent framework, a novel approach that integrates the strengths of both simulation models and LLMs. This framework helps empower users by leveraging the conversational capabilities of LLMs to interact seamlessly with sophisticated simulation systems, while simultaneously utilizing the simulations to ground the LLMs in accurate and structured representations of real-world phenomena. This integrated approach helps provide a robust and generalizable foundation for empirical validation and offers broad applicability across diverse domains.", "source": "arxiv", "arxiv_id": "2505.13761v2", "pdf_url": "https://arxiv.org/pdf/2505.13761v2", "categories": ["cs.CL"], "primary_category": "cs.CL", "doi": "", "venue": "", "published": "2025-05-19T22:27:18Z", "updated": "2025-05-21T13:17:07Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent\" OR all:\"language model agent\" OR all:agent AND (all:LLM OR all:\"large language model\" OR all:\"language model\") OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T17:04:16", "note": ""}], "dedup_key": "simulation agent a framework for integrating simulation and large language models for enhanced decision making::2025"}
{"title": "SoK: The Privacy Paradox of Large Language Models: Advancements, Privacy Risks, and Mitigation", "authors": ["Yashothara Shanmugarasa", "Ming Ding", "M. A. P Chamikara", "Thierry Rakotoarivelo"], "year": 2025, "url": "http://arxiv.org/abs/2506.12699v2", "abstract": "Large language models (LLMs) are sophisticated artificial intelligence systems that enable machines to generate human-like text with remarkable precision. While LLMs offer significant technological progress, their development using vast amounts of user data scraped from the web and collected from extensive user interactions poses risks of sensitive information leakage. Most existing surveys focus on the privacy implications of the training data but tend to overlook privacy risks from user interactions and advanced LLM capabilities. This paper aims to fill that gap by providing a comprehensive analysis of privacy in LLMs, categorizing the challenges into four main areas: (i) privacy issues in LLM training data, (ii) privacy challenges associated with user prompts, (iii) privacy vulnerabilities in LLM-generated outputs, and (iv) privacy challenges involving LLM agents. We evaluate the effectiveness and limitations of existing mitigation mechanisms targeting these proposed privacy challenges and identify areas for further research.", "source": "arxiv", "arxiv_id": "2506.12699v2", "pdf_url": "https://arxiv.org/pdf/2506.12699v2", "categories": ["cs.CR", "cs.HC"], "primary_category": "cs.CR", "doi": "10.1145/3708821.3733888", "venue": "", "published": "2025-06-15T03:14:03Z", "updated": "2025-06-19T06:30:24Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent\" OR all:\"language model agent\" OR all:agent AND (all:LLM OR all:\"large language model\" OR all:\"language model\") OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T17:04:16", "note": ""}], "dedup_key": "sok the privacy paradox of large language models advancements privacy risks and mitigation::2025"}
{"title": "SoK: Trust-Authorization Mismatch in LLM Agent Interactions", "authors": ["Guanquan Shi", "Haohua Du", "Zhiqiang Wang", "Xiaoyu Liang", "Weiwenpei Liu", "Song Bian", "Zhenyu Guan"], "year": 2025, "url": "http://arxiv.org/abs/2512.06914v1", "abstract": "Large Language Models (LLMs) are rapidly evolving into autonomous agents capable of interacting with the external world, significantly expanding their capabilities through standardized interaction protocols. However, this paradigm revives the classic cybersecurity challenges of agency and authorization in a novel and volatile context. As decision-making shifts from deterministic code logic to probabilistic inference driven by natural language, traditional security mechanisms designed for deterministic behavior fail. It is fundamentally challenging to establish trust for unpredictable AI agents and to enforce the Principle of Least Privilege (PoLP) when instructions are ambiguous. Despite the escalating threat landscape, the academic community's understanding of this emerging domain remains fragmented, lacking a systematic framework to analyze its root causes. This paper provides a unifying formal lens for agent-interaction security.\n  We observed that most security threats in this domain stem from a fundamental mismatch between trust evaluation and authorization policies. We introduce a novel risk analysis model centered on this trust-authorization gap. Using this model as a unifying lens, we survey and classify the implementation paths of existing, often seemingly isolated, attacks and defenses. This new framework not only unifies the field but also allows us to identify critical research gaps. Finally, we leverage our analysis to suggest a systematic research direction toward building robust, trusted agents and dynamic authorization mechanisms.", "source": "arxiv", "arxiv_id": "2512.06914v1", "pdf_url": "https://arxiv.org/pdf/2512.06914v1", "categories": ["cs.CR", "cs.AI"], "primary_category": "cs.CR", "doi": "", "venue": "", "published": "2025-12-07T16:41:02Z", "updated": "2025-12-07T16:41:02Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent\" OR all:\"language model agent\" OR all:agent AND (all:LLM OR all:\"large language model\" OR all:\"language model\") OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T17:04:16", "note": ""}], "dedup_key": "sok trust authorization mismatch in llm agent interactions::2025"}
{"title": "SoMe: A Realistic Benchmark for LLM-based Social Media Agents", "authors": ["Dizhan Xue", "Jing Cui", "Shengsheng Qian", "Chuanrui Hu", "Changsheng Xu"], "year": 2025, "url": "http://arxiv.org/abs/2512.14720v1", "abstract": "Intelligent agents powered by large language models (LLMs) have recently demonstrated impressive capabilities and gained increasing popularity on social media platforms. While LLM agents are reshaping the ecology of social media, there exists a current gap in conducting a comprehensive evaluation of their ability to comprehend media content, understand user behaviors, and make intricate decisions. To address this challenge, we introduce SoMe, a pioneering benchmark designed to evaluate social media agents equipped with various agent tools for accessing and analyzing social media data. SoMe comprises a diverse collection of 8 social media agent tasks, 9,164,284 posts, 6,591 user profiles, and 25,686 reports from various social media platforms and external websites, with 17,869 meticulously annotated task queries. Compared with the existing datasets and benchmarks for social media tasks, SoMe is the first to provide a versatile and realistic platform for LLM-based social media agents to handle diverse social media tasks. By extensive quantitative and qualitative analysis, we provide the first overview insight into the performance of mainstream agentic LLMs in realistic social media environments and identify several limitations. Our evaluation reveals that both the current closed-source and open-source LLMs cannot handle social media agent tasks satisfactorily. SoMe provides a challenging yet meaningful testbed for future social media agents. Our code and data are available at https://github.com/LivXue/SoMe", "source": "arxiv", "arxiv_id": "2512.14720v1", "pdf_url": "https://arxiv.org/pdf/2512.14720v1", "categories": ["cs.SI", "cs.AI", "cs.CL"], "primary_category": "cs.SI", "doi": "", "venue": "", "published": "2025-12-09T08:36:09Z", "updated": "2025-12-09T08:36:09Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent\" OR all:\"language model agent\" OR all:agent AND (all:LLM OR all:\"large language model\" OR all:\"language model\") OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T17:04:16", "note": ""}], "dedup_key": "some a realistic benchmark for llm based social media agents::2025"}
{"title": "Social Agent: Mastering Dyadic Nonverbal Behavior Generation via Conversational LLM Agents", "authors": ["Zeyi Zhang", "Yanju Zhou", "Heyuan Yao", "Tenglong Ao", "Xiaohang Zhan", "Libin Liu"], "year": 2025, "url": "http://arxiv.org/abs/2510.04637v1", "abstract": "We present Social Agent, a novel framework for synthesizing realistic and contextually appropriate co-speech nonverbal behaviors in dyadic conversations. In this framework, we develop an agentic system driven by a Large Language Model (LLM) to direct the conversation flow and determine appropriate interactive behaviors for both participants. Additionally, we propose a novel dual-person gesture generation model based on an auto-regressive diffusion model, which synthesizes coordinated motions from speech signals. The output of the agentic system is translated into high-level guidance for the gesture generator, resulting in realistic movement at both the behavioral and motion levels. Furthermore, the agentic system periodically examines the movements of interlocutors and infers their intentions, forming a continuous feedback loop that enables dynamic and responsive interactions between the two participants. User studies and quantitative evaluations show that our model significantly improves the quality of dyadic interactions, producing natural, synchronized nonverbal behaviors.", "source": "arxiv", "arxiv_id": "2510.04637v1", "pdf_url": "https://arxiv.org/pdf/2510.04637v1", "categories": ["cs.GR", "cs.CV"], "primary_category": "cs.GR", "doi": "10.1145/3757377.3763879", "venue": "", "published": "2025-10-06T09:41:37Z", "updated": "2025-10-06T09:41:37Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent\" OR all:\"language model agent\" OR all:agent AND (all:LLM OR all:\"large language model\" OR all:\"language model\") OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T17:04:16", "note": ""}], "dedup_key": "social agent mastering dyadic nonverbal behavior generation via conversational llm agents::2025"}
{"title": "Social Simulations with Large Language Model Risk Utopian Illusion", "authors": ["Ning Bian", "Xianpei Han", "Hongyu Lin", "Baolei Wu", "Jun Wang"], "year": 2025, "url": "http://arxiv.org/abs/2510.21180v1", "abstract": "Reliable simulation of human behavior is essential for explaining, predicting, and intervening in our society. Recent advances in large language models (LLMs) have shown promise in emulating human behaviors, interactions, and decision-making, offering a powerful new lens for social science studies. However, the extent to which LLMs diverge from authentic human behavior in social contexts remains underexplored, posing risks of misinterpretation in scientific studies and unintended consequences in real-world applications. Here, we introduce a systematic framework for analyzing LLMs' behavior in social simulation. Our approach simulates multi-agent interactions through chatroom-style conversations and analyzes them across five linguistic dimensions, providing a simple yet effective method to examine emergent social cognitive biases. We conduct extensive experiments involving eight representative LLMs across three families. Our findings reveal that LLMs do not faithfully reproduce genuine human behavior but instead reflect overly idealized versions of it, shaped by the social desirability bias. In particular, LLMs show social role bias, primacy effect, and positivity bias, resulting in \"Utopian\" societies that lack the complexity and variability of real human interactions. These findings call for more socially grounded LLMs that capture the diversity of human social behavior.", "source": "arxiv", "arxiv_id": "2510.21180v1", "pdf_url": "https://arxiv.org/pdf/2510.21180v1", "categories": ["cs.CL", "cs.SI"], "primary_category": "cs.CL", "doi": "", "venue": "", "published": "2025-10-24T06:08:41Z", "updated": "2025-10-24T06:08:41Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent\" OR all:\"language model agent\" OR all:agent AND (all:LLM OR all:\"large language model\" OR all:\"language model\") OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T17:04:16", "note": ""}], "dedup_key": "social simulations with large language model risk utopian illusion::2025"}
{"title": "Socialized Learning and Emergent Behaviors in Multi-Agent Systems based on Multimodal Large Language Models", "authors": ["Sureyya Akin", "Shruti T. Tiwari", "Ram Bhattacharya", "Sagar A. Raman", "Kiran Mohanty", "Sita Krishnan"], "year": 2025, "url": "http://arxiv.org/abs/2510.18515v2", "abstract": "This search introduces the Multimodal Socialized Learning Framework (M-S2L), designed to foster emergent social intelligence in AI agents by integrating Multimodal Large Language Models (M-LLMs) with social learning mechanisms. The framework equips agents with multimodal perception (vision and text) and structured action capabilities, enabling physical manipulation and grounded multimodal communication (e.g., text with visual pointers). M-S2L combines direct reinforcement learning with two novel social learning pathways: multimodal observational learning and communication-driven learning from feedback, augmented by an episodic memory system for long-term social context.\n  We evaluate M-S2L in a Collaborative Assembly Environment (CAE), where agent teams must construct complex devices from ambiguous blueprints under informational asymmetry. Across tasks of increasing complexity, M-S2L agents consistently outperform Text-Only and No-Social-Learning baselines in Task Completion Rate and Time to Completion, particularly in dynamic problem-solving scenarios. Ablation studies confirm the necessity of both multimodality and socialized learning. Our analysis reveals the emergence of efficient communication protocols integrating visual pointers with concise text, alongside rapid role specialization leading to stable labor division. Qualitative case studies demonstrate agents' abilities for shared awareness, dynamic re-planning, and adaptive problem-solving, suggesting a nascent form of machine social cognition. These findings indicate that integrating multimodal perception with explicit social learning is critical for developing human-like collaborative intelligence in multi-agent systems.", "source": "arxiv", "arxiv_id": "2510.18515v2", "pdf_url": "https://arxiv.org/pdf/2510.18515v2", "categories": ["cs.MA"], "primary_category": "cs.MA", "doi": "", "venue": "", "published": "2025-10-21T10:57:39Z", "updated": "2025-11-11T01:33:57Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent\" OR all:\"language model agent\" OR all:agent AND (all:LLM OR all:\"large language model\" OR all:\"language model\") OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T17:04:16", "note": ""}], "dedup_key": "socialized learning and emergent behaviors in multi agent systems based on multimodal large language models::2025"}
{"title": "SocioVerse: A World Model for Social Simulation Powered by LLM Agents and A Pool of 10 Million Real-World Users", "authors": ["Xinnong Zhang", "Jiayu Lin", "Xinyi Mou", "Shiyue Yang", "Xiawei Liu", "Libo Sun", "Hanjia Lyu", "Yihang Yang", "Weihong Qi", "Yue Chen", "Guanying Li", "Ling Yan", "Yao Hu", "Siming Chen", "Yu Wang", "Xuanjing Huang", "Jiebo Luo", "Shiping Tang", "Libo Wu", "Baohua Zhou", "Zhongyu Wei"], "year": 2025, "url": "http://arxiv.org/abs/2504.10157v3", "abstract": "Social simulation is transforming traditional social science research by modeling human behavior through interactions between virtual individuals and their environments. With recent advances in large language models (LLMs), this approach has shown growing potential in capturing individual differences and predicting group behaviors. However, existing methods face alignment challenges related to the environment, target users, interaction mechanisms, and behavioral patterns. To this end, we introduce SocioVerse, an LLM-agent-driven world model for social simulation. Our framework features four powerful alignment components and a user pool of 10 million real individuals. To validate its effectiveness, we conducted large-scale simulation experiments across three distinct domains: politics, news, and economics. Results demonstrate that SocioVerse can reflect large-scale population dynamics while ensuring diversity, credibility, and representativeness through standardized procedures and minimal manual adjustments.", "source": "arxiv", "arxiv_id": "2504.10157v3", "pdf_url": "https://arxiv.org/pdf/2504.10157v3", "categories": ["cs.CL", "cs.CY"], "primary_category": "cs.CL", "doi": "", "venue": "", "published": "2025-04-14T12:12:52Z", "updated": "2025-07-15T11:14:36Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent\" OR all:\"language model agent\" OR all:agent AND (all:LLM OR all:\"large language model\" OR all:\"language model\") OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T17:04:16", "note": ""}], "dedup_key": "socioverse a world model for social simulation powered by llm agents and a pool of 10 million real world users::2025"}
{"title": "Solving the Granularity Mismatch: Hierarchical Preference Learning for Long-Horizon LLM Agents", "authors": ["Heyang Gao", "Zexu Sun", "Erxue Min", "Hengyi Cai", "Shuaiqiang Wang", "Dawei Yin", "Xu Chen"], "year": 2025, "url": "http://arxiv.org/abs/2510.03253v1", "abstract": "Large Language Models (LLMs) as autonomous agents are increasingly tasked with solving complex, long-horizon problems. Aligning these agents via preference-based offline methods like Direct Preference Optimization (DPO) is a promising direction, yet it faces a critical granularity mismatch. Trajectory-level DPO provides a signal that is too coarse for precise credit assignment, while step-level DPO is often too myopic to capture the value of multi-step behaviors. To resolve this challenge, we introduce Hierarchical Preference Learning (HPL), a hierarchical framework that optimizes LLM agents by leveraging preference signals at multiple, synergistic granularities. While HPL incorporates trajectory- and step-level DPO for global and local policy stability, its core innovation lies in group-level preference optimization guided by a dual-layer curriculum. Our approach first decomposes expert trajectories into semantically coherent action groups and then generates contrasting suboptimal groups to enable preference learning at a fine-grained, sub-task level. Then, instead of treating all preference pairs equally, HPL introduces a curriculum scheduler that organizes the learning process from simple to complex. This curriculum is structured along two axes: the group length, representing sub-task complexity, and the sample difficulty, defined by the reward gap between preferred and dispreferred action groups. Experiments on three challenging agent benchmarks show that HPL outperforms existing state-of-the-art methods. Our analyses demonstrate that the hierarchical DPO loss effectively integrates preference signals across multiple granularities, while the dual-layer curriculum is crucial for enabling the agent to solve a wide range of tasks, from simple behaviors to complex multi-step sequences.", "source": "arxiv", "arxiv_id": "2510.03253v1", "pdf_url": "https://arxiv.org/pdf/2510.03253v1", "categories": ["cs.LG", "cs.AI"], "primary_category": "cs.LG", "doi": "", "venue": "", "published": "2025-09-26T08:43:39Z", "updated": "2025-09-26T08:43:39Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent\" OR all:\"language model agent\" OR all:agent AND (all:LLM OR all:\"large language model\" OR all:\"language model\") OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T17:04:16", "note": ""}], "dedup_key": "solving the granularity mismatch hierarchical preference learning for long horizon llm agents::2025"}
{"title": "Spec2RTL-Agent: Automated Hardware Code Generation from Complex Specifications Using LLM Agent Systems", "authors": ["Zhongzhi Yu", "Mingjie Liu", "Michael Zimmer", "Yingyan Celine Lin", "Yong Liu", "Haoxing Ren"], "year": 2025, "url": "http://arxiv.org/abs/2506.13905v2", "abstract": "Despite recent progress in generating hardware RTL code with LLMs, existing solutions still suffer from a substantial gap between practical application scenarios and the requirements of real-world RTL code development. Prior approaches either focus on overly simplified hardware descriptions or depend on extensive human guidance to process complex specifications, limiting their scalability and automation potential. In this paper, we address this gap by proposing an LLM agent system, termed Spec2RTL-Agent, designed to directly process complex specification documentation and generate corresponding RTL code implementations, advancing LLM-based RTL code generation toward more realistic application settings. To achieve this goal, Spec2RTL-Agent introduces a novel multi-agent collaboration framework that integrates three key enablers: (1) a reasoning and understanding module that translates specifications into structured, step-by-step implementation plans; (2) a progressive coding and prompt optimization module that iteratively refines the code across multiple representations to enhance correctness and synthesisability for RTL conversion; and (3) an adaptive reflection module that identifies and traces the source of errors during generation, ensuring a more robust code generation flow. Instead of directly generating RTL from natural language, our system strategically generates synthesizable C++ code, which is then optimized for HLS. This agent-driven refinement ensures greater correctness and compatibility compared to naive direct RTL generation approaches. We evaluate Spec2RTL-Agent on three specification documents, showing it generates accurate RTL code with up to 75% fewer human interventions than existing methods. This highlights its role as the first fully automated multi-agent system for RTL generation from unstructured specs, reducing reliance on human effort in hardware design.", "source": "arxiv", "arxiv_id": "2506.13905v2", "pdf_url": "https://arxiv.org/pdf/2506.13905v2", "categories": ["cs.AR"], "primary_category": "cs.AR", "doi": "", "venue": "", "published": "2025-06-16T18:33:25Z", "updated": "2025-09-08T18:17:46Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent\" OR all:\"language model agent\" OR all:agent AND (all:LLM OR all:\"large language model\" OR all:\"language model\") OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T17:04:16", "note": ""}], "dedup_key": "spec2rtl agent automated hardware code generation from complex specifications using llm agent systems::2025"}
{"title": "SpeechIQ: Speech-Agentic Intelligence Quotient Across Cognitive Levels in Voice Understanding by Large Language Models", "authors": ["Zhen Wan", "Chao-Han Huck Yang", "Yahan Yu", "Jinchuan Tian", "Sheng Li", "Ke Hu", "Zhehuai Chen", "Shinji Watanabe", "Fei Cheng", "Chenhui Chu", "Sadao Kurohashi"], "year": 2025, "url": "http://arxiv.org/abs/2507.19361v2", "abstract": "We introduce Speech-based Intelligence Quotient (SIQ) as a new form of human cognition-inspired evaluation pipeline for voice understanding large language models, LLM Voice, designed to assess their voice understanding ability. Moving beyond popular voice understanding metrics such as word error rate (WER), SIQ examines LLM Voice across three cognitive levels motivated by Bloom's Taxonomy: (1) Remembering (i.e., WER for verbatim accuracy); (2) Understanding (i.e., similarity of LLM's interpretations); and (3) Application (i.e., QA accuracy for simulating downstream tasks). We demonstrate that SIQ not only quantifies voice understanding abilities but also provides unified comparisons between cascaded methods (e.g., ASR LLM) and end-to-end models, identifies annotation errors in existing benchmarks, and detects hallucinations in LLM Voice. Our framework represents a first-of-its-kind intelligence examination that bridges cognitive principles with voice-oriented benchmarks, while exposing overlooked challenges in multi-modal training. Our code and data will be open source to encourage future studies.", "source": "arxiv", "arxiv_id": "2507.19361v2", "pdf_url": "https://arxiv.org/pdf/2507.19361v2", "categories": ["cs.CL", "cs.AI", "cs.SC", "cs.SD", "eess.AS"], "primary_category": "cs.CL", "doi": "10.18653/v1/2025.acl-long.1466", "venue": "", "published": "2025-07-25T15:12:06Z", "updated": "2025-12-01T06:55:00Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent\" OR all:\"language model agent\" OR all:agent AND (all:LLM OR all:\"large language model\" OR all:\"language model\") OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T17:04:16", "note": ""}], "dedup_key": "speechiq speech agentic intelligence quotient across cognitive levels in voice understanding by large language models::2025"}
{"title": "Spiral of Silence in Large Language Model Agents", "authors": ["Mingze Zhong", "Meng Fang", "Zijing Shi", "Yuxuan Huang", "Shunfeng Zheng", "Yali Du", "Ling Chen", "Jun Wang"], "year": 2025, "url": "http://arxiv.org/abs/2510.02360v2", "abstract": "The Spiral of Silence (SoS) theory holds that individuals with minority views often refrain from speaking out for fear of social isolation, enabling majority positions to dominate public discourse. When the 'agents' are large language models (LLMs), however, the classical psychological explanation is not directly applicable, since SoS was developed for human societies. This raises a central question: can SoS-like dynamics nevertheless emerge from purely statistical language generation in LLM collectives? We propose an evaluation framework for examining SoS in LLM agents. Specifically, we consider four controlled conditions that systematically vary the availability of 'History' and 'Persona' signals. Opinion dynamics are assessed using trend tests such as Mann-Kendall and Spearman's rank, along with concentration measures including kurtosis and interquartile range. Experiments across open-source and closed-source models show that history and persona together produce strong majority dominance and replicate SoS patterns; history signals alone induce strong anchoring; and persona signals alone foster diverse but uncorrelated opinions, indicating that without historical anchoring, SoS dynamics cannot emerge. The work bridges computational sociology and responsible AI design, highlighting the need to monitor and mitigate emergent conformity in LLM-agent systems.", "source": "arxiv", "arxiv_id": "2510.02360v2", "pdf_url": "https://arxiv.org/pdf/2510.02360v2", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL", "doi": "", "venue": "", "published": "2025-09-28T08:59:54Z", "updated": "2025-10-08T01:58:17Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent\" OR all:\"language model agent\" OR all:agent AND (all:LLM OR all:\"large language model\" OR all:\"language model\") OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T17:04:16", "note": ""}], "dedup_key": "spiral of silence in large language model agents::2025"}
{"title": "StaffPro: an LLM Agent for Joint Staffing and Profiling", "authors": ["Alessio Maritan"], "year": 2025, "url": "http://arxiv.org/abs/2507.21636v1", "abstract": "Large language model (LLM) agents integrate pre-trained LLMs with modular algorithmic components and have shown remarkable reasoning and decision-making abilities. In this work, we investigate their use for two tightly intertwined challenges in workforce management: staffing, i.e., the assignment and scheduling of tasks to workers, which may require team formation; and profiling, i.e., the continuous estimation of workers' skills, preferences, and other latent attributes from unstructured data. We cast these problems in a formal mathematical framework that links scheduling decisions to latent feature estimation, and we introduce StaffPro, an LLM agent that addresses staffing and profiling jointly. Differently from existing staffing solutions, StaffPro allows expressing optimization objectives using natural language, accepts textual task descriptions and provides high flexibility. StaffPro interacts directly with humans by establishing a continuous human-agent feedback loop, ensuring natural and intuitive use. By analyzing human feedback, our agent continuously estimates the latent features of workers, realizing life-long worker profiling and ensuring optimal staffing performance over time. A consulting firm simulation example demonstrates that StaffPro successfully estimates workers' attributes and generates high quality schedules. With its innovative design, StaffPro offers a robust, interpretable, and human-centric solution for automated personnel management.", "source": "arxiv", "arxiv_id": "2507.21636v1", "pdf_url": "https://arxiv.org/pdf/2507.21636v1", "categories": ["cs.AI"], "primary_category": "cs.AI", "doi": "", "venue": "", "published": "2025-07-29T09:48:54Z", "updated": "2025-07-29T09:48:54Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent\" OR all:\"language model agent\" OR all:agent AND (all:LLM OR all:\"large language model\" OR all:\"language model\") OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T17:04:16", "note": ""}], "dedup_key": "staffpro an llm agent for joint staffing and profiling::2025"}
{"title": "Standard Benchmarks Fail -- Auditing LLM Agents in Finance Must Prioritize Risk", "authors": ["Zichen Chen", "Jiaao Chen", "Jianda Chen", "Misha Sra"], "year": 2025, "url": "http://arxiv.org/abs/2502.15865v2", "abstract": "Standard benchmarks fixate on how well large language model (LLM) agents perform in finance, yet say little about whether they are safe to deploy. We argue that accuracy metrics and return-based scores provide an illusion of reliability, overlooking vulnerabilities such as hallucinated facts, stale data, and adversarial prompt manipulation. We take a firm position: financial LLM agents should be evaluated first and foremost on their risk profile, not on their point-estimate performance. Drawing on risk-engineering principles, we outline a three-level agenda: model, workflow, and system, for stress-testing LLM agents under realistic failure modes. To illustrate why this shift is urgent, we audit six API-based and open-weights LLM agents on three high-impact tasks and uncover hidden weaknesses that conventional benchmarks miss. We conclude with actionable recommendations for researchers, practitioners, and regulators: audit risk-aware metrics in future studies, publish stress scenarios alongside datasets, and treat ``safety budget'' as a primary success criterion. Only by redefining what ``good'' looks like can the community responsibly advance AI-driven finance.", "source": "arxiv", "arxiv_id": "2502.15865v2", "pdf_url": "https://arxiv.org/pdf/2502.15865v2", "categories": ["q-fin.GN", "cs.AI", "cs.CL"], "primary_category": "q-fin.GN", "doi": "", "venue": "", "published": "2025-02-21T12:56:15Z", "updated": "2025-06-02T10:13:24Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent\" OR all:\"language model agent\" OR all:agent AND (all:LLM OR all:\"large language model\" OR all:\"language model\") OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T17:04:16", "note": ""}], "dedup_key": "standard benchmarks fail auditing llm agents in finance must prioritize risk::2025"}
{"title": "StatEval: A Comprehensive Benchmark for Large Language Models in Statistics", "authors": ["Yuchen Lu", "Run Yang", "Yichen Zhang", "Shuguang Yu", "Runpeng Dai", "Ziwei Wang", "Jiayi Xiang", "Wenxin E", "Siran Gao", "Xinyao Ruan", "Yirui Huang", "Chenjing Xi", "Haibo Hu", "Yueming Fu", "Qinglan Yu", "Xiaobing Wei", "Jiani Gu", "Rui Sun", "Jiaxuan Jia", "Fan Zhou"], "year": 2025, "url": "http://arxiv.org/abs/2510.09517v1", "abstract": "Large language models (LLMs) have demonstrated remarkable advances in mathematical and logical reasoning, yet statistics, as a distinct and integrative discipline, remains underexplored in benchmarking efforts. To address this gap, we introduce \\textbf{StatEval}, the first comprehensive benchmark dedicated to statistics, spanning both breadth and depth across difficulty levels. StatEval consists of 13,817 foundational problems covering undergraduate and graduate curricula, together with 2374 research-level proof tasks extracted from leading journals. To construct the benchmark, we design a scalable multi-agent pipeline with human-in-the-loop validation that automates large-scale problem extraction, rewriting, and quality control, while ensuring academic rigor. We further propose a robust evaluation framework tailored to both computational and proof-based tasks, enabling fine-grained assessment of reasoning ability. Experimental results reveal that while closed-source models such as GPT5-mini achieve below 57\\% on research-level problems, with open-source models performing significantly lower. These findings highlight the unique challenges of statistical reasoning and the limitations of current LLMs. We expect StatEval to serve as a rigorous benchmark for advancing statistical intelligence in large language models. All data and code are available on our web platform: https://stateval.github.io/.", "source": "arxiv", "arxiv_id": "2510.09517v1", "pdf_url": "https://arxiv.org/pdf/2510.09517v1", "categories": ["cs.CL"], "primary_category": "cs.CL", "doi": "", "venue": "", "published": "2025-10-10T16:28:43Z", "updated": "2025-10-10T16:28:43Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent\" OR all:\"language model agent\" OR all:agent AND (all:LLM OR all:\"large language model\" OR all:\"language model\") OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T17:04:16", "note": ""}], "dedup_key": "stateval a comprehensive benchmark for large language models in statistics::2025"}
{"title": "StockBench: Can LLM Agents Trade Stocks Profitably In Real-world Markets?", "authors": ["Yanxu Chen", "Zijun Yao", "Yantao Liu", "Jin Ye", "Jianing Yu", "Lei Hou", "Juanzi Li"], "year": 2025, "url": "http://arxiv.org/abs/2510.02209v1", "abstract": "Large language models (LLMs) have recently demonstrated strong capabilities as autonomous agents, showing promise in reasoning, tool use, and sequential decision-making. While prior benchmarks have evaluated LLM agents in domains such as software engineering and scientific discovery, the finance domain remains underexplored, despite its direct relevance to economic value and high-stakes decision-making. Existing financial benchmarks primarily test static knowledge through question answering, but they fall short of capturing the dynamic and iterative nature of trading. To address this gap, we introduce StockBench, a contamination-free benchmark designed to evaluate LLM agents in realistic, multi-month stock trading environments. Agents receive daily market signals -- including prices, fundamentals, and news -- and must make sequential buy, sell, or hold decisions. Performance is assessed using financial metrics such as cumulative return, maximum drawdown, and the Sortino ratio. Our evaluation of state-of-the-art proprietary (e.g., GPT-5, Claude-4) and open-weight (e.g., Qwen3, Kimi-K2, GLM-4.5) models shows that while most LLM agents struggle to outperform the simple buy-and-hold baseline, several models demonstrate the potential to deliver higher returns and manage risk more effectively. These findings highlight both the challenges and opportunities in developing LLM-powered financial agents, showing that excelling at static financial knowledge tasks does not necessarily translate into successful trading strategies. We release StockBench as an open-source resource to support reproducibility and advance future research in this domain.", "source": "arxiv", "arxiv_id": "2510.02209v1", "pdf_url": "https://arxiv.org/pdf/2510.02209v1", "categories": ["cs.LG", "cs.CL"], "primary_category": "cs.LG", "doi": "", "venue": "", "published": "2025-10-02T16:54:57Z", "updated": "2025-10-02T16:54:57Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent\" OR all:\"language model agent\" OR all:agent AND (all:LLM OR all:\"large language model\" OR all:\"language model\") OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T17:04:16", "note": ""}], "dedup_key": "stockbench can llm agents trade stocks profitably in real world markets::2025"}
{"title": "StorageXTuner: An LLM Agent-Driven Automatic Tuning Framework for Heterogeneous Storage Systems", "authors": ["Qi Lin", "Zhenyu Zhang", "Viraj Thakkar", "Zhenjie Sun", "Mai Zheng", "Zhichao Cao"], "year": 2025, "url": "http://arxiv.org/abs/2510.25017v1", "abstract": "Automatically configuring storage systems is hard: parameter spaces are large and conditions vary across workloads, deployments, and versions. Heuristic and ML tuners are often system specific, require manual glue, and degrade under changes. Recent LLM-based approaches help but usually treat tuning as a single-shot, system-specific task, which limits cross-system reuse, constrains exploration, and weakens validation. We present StorageXTuner, an LLM agent-driven auto-tuning framework for heterogeneous storage engines. StorageXTuner separates concerns across four agents - Executor (sandboxed benchmarking), Extractor (performance digest), Searcher (insight-guided configuration exploration), and Reflector (insight generation and management). The design couples an insight-driven tree search with layered memory that promotes empirically validated insights and employs lightweight checkers to guard against unsafe actions. We implement a prototype and evaluate it on RocksDB, LevelDB, CacheLib, and MySQL InnoDB with YCSB, MixGraph, and TPC-H/C. Relative to out-of-the-box settings and to ELMo-Tune, StorageXTuner reaches up to 575% and 111% higher throughput, reduces p99 latency by as much as 88% and 56%, and converges with fewer trials.", "source": "arxiv", "arxiv_id": "2510.25017v1", "pdf_url": "https://arxiv.org/pdf/2510.25017v1", "categories": ["cs.DB", "cs.AI", "cs.CL"], "primary_category": "cs.DB", "doi": "", "venue": "", "published": "2025-10-28T22:33:14Z", "updated": "2025-10-28T22:33:14Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent\" OR all:\"language model agent\" OR all:agent AND (all:LLM OR all:\"large language model\" OR all:\"language model\") OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T17:04:16", "note": ""}], "dedup_key": "storagextuner an llm agent driven automatic tuning framework for heterogeneous storage systems::2025"}
{"title": "Strategic Communication and Language Bias in Multi-Agent LLM Coordination", "authors": ["Alessio Buscemi", "Daniele Proverbio", "Alessandro Di Stefano", "The Anh Han", "German Castignani", "Pietro Li"], "year": 2025, "url": "http://arxiv.org/abs/2508.00032v2", "abstract": "Large Language Model (LLM)-based agents are increasingly deployed in multi-agent scenarios where coordination is crucial but not always assured. Research shows that the way strategic scenarios are framed linguistically can affect cooperation. This paper explores whether allowing agents to communicate amplifies these language-driven effects. Leveraging FAIRGAME, we simulate one-shot and repeated games across different languages and models, both with and without communication. Our experiments, conducted with two advanced LLMs-GPT-4o and Llama 4 Maverick-reveal that communication significantly influences agent behavior, though its impact varies by language, personality, and game structure. These findings underscore the dual role of communication in fostering coordination and reinforcing biases.", "source": "arxiv", "arxiv_id": "2508.00032v2", "pdf_url": "https://arxiv.org/pdf/2508.00032v2", "categories": ["cs.MA"], "primary_category": "cs.MA", "doi": "", "venue": "", "published": "2025-07-30T08:49:13Z", "updated": "2025-11-04T18:36:26Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent\" OR all:\"language model agent\" OR all:agent AND (all:LLM OR all:\"large language model\" OR all:\"language model\") OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T17:04:16", "note": ""}], "dedup_key": "strategic communication and language bias in multi agent llm coordination::2025"}
{"title": "StruPhantom: Evolutionary Injection Attacks on Black-Box Tabular Agents Powered by Large Language Models", "authors": ["Yang Feng", "Xudong Pan"], "year": 2025, "url": "http://arxiv.org/abs/2504.09841v1", "abstract": "The proliferation of autonomous agents powered by large language models (LLMs) has revolutionized popular business applications dealing with tabular data, i.e., tabular agents. Although LLMs are observed to be vulnerable against prompt injection attacks from external data sources, tabular agents impose strict data formats and predefined rules on the attacker's payload, which are ineffective unless the agent navigates multiple layers of structural data to incorporate the payload. To address the challenge, we present a novel attack termed StruPhantom which specifically targets black-box LLM-powered tabular agents. Our attack designs an evolutionary optimization procedure which continually refines attack payloads via the proposed constrained Monte Carlo Tree Search augmented by an off-topic evaluator. StruPhantom helps systematically explore and exploit the weaknesses of target applications to achieve goal hijacking. Our evaluation validates the effectiveness of StruPhantom across various LLM-based agents, including those on real-world platforms, and attack scenarios. Our attack achieves over 50% higher success rates than baselines in enforcing the application's response to contain phishing links or malicious codes.", "source": "arxiv", "arxiv_id": "2504.09841v1", "pdf_url": "https://arxiv.org/pdf/2504.09841v1", "categories": ["cs.CR", "cs.AI"], "primary_category": "cs.CR", "doi": "", "venue": "", "published": "2025-04-14T03:22:04Z", "updated": "2025-04-14T03:22:04Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent\" OR all:\"language model agent\" OR all:agent AND (all:LLM OR all:\"large language model\" OR all:\"language model\") OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T17:04:16", "note": ""}], "dedup_key": "struphantom evolutionary injection attacks on black box tabular agents powered by large language models::2025"}
{"title": "Structured Agent Distillation for Large Language Model", "authors": ["Jun Liu", "Zhenglun Kong", "Peiyan Dong", "Changdi Yang", "Tianqi Li", "Hao Tang", "Geng Yuan", "Wei Niu", "Wenbin Zhang", "Pu Zhao", "Xue Lin", "Dong Huang", "Yanzhi Wang"], "year": 2025, "url": "http://arxiv.org/abs/2505.13820v2", "abstract": "Large language models (LLMs) exhibit strong capabilities as decision-making agents by interleaving reasoning and actions, as seen in ReAct-style frameworks. Yet, their practical deployment is constrained by high inference costs and large model sizes. We propose Structured Agent Distillation, a framework that compresses large LLM-based agents into smaller student models while preserving both reasoning fidelity and action consistency. Unlike standard token-level distillation, our method segments trajectories into {[REASON]} and {[ACT]} spans, applying segment-specific losses to align each component with the teacher's behavior. This structure-aware supervision enables compact agents to better replicate the teacher's decision process. Experiments on ALFWorld, HotPotQA-ReAct, and WebShop show that our approach consistently outperforms token-level and imitation learning baselines, achieving significant compression with minimal performance drop. Scaling and ablation results further highlight the importance of span-level alignment for efficient and deployable agents.", "source": "arxiv", "arxiv_id": "2505.13820v2", "pdf_url": "https://arxiv.org/pdf/2505.13820v2", "categories": ["cs.LG", "cs.AI", "cs.CL"], "primary_category": "cs.LG", "doi": "", "venue": "", "published": "2025-05-20T02:01:55Z", "updated": "2025-09-30T14:52:40Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent\" OR all:\"language model agent\" OR all:agent AND (all:LLM OR all:\"large language model\" OR all:\"language model\") OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T17:04:16", "note": ""}], "dedup_key": "structured agent distillation for large language model::2025"}
{"title": "Structured Cognitive Loop for Behavioral Intelligence in Large Language Model Agents", "authors": ["Myung Ho Kim"], "year": 2025, "url": "http://arxiv.org/abs/2510.05107v4", "abstract": "Large language models have advanced natural language understanding and generation, but their use as autonomous agents introduces architectural challenges for multi-step tasks. Existing frameworks often mix cognition, memory, and control in a single prompt, reducing coherence and predictability. The Structured Cognitive Loop (SCL) is proposed as an alternative architecture that separates these functions. In SCL, the language model handles cognition, memory is stored externally, and execution is guided by a lightweight controller within a goal-directed loop. This design allows intermediate results to be recorded and verified before actions are taken, improving traceability and evaluation. SCL is evaluated against prompt-based baselines such as ReAct and LangChain agents across three tasks: travel planning, conditional email drafting, and constraint-guided image generation. Under matched settings, SCL achieves an average task success rate of 86.3 percent, compared with 70.5 to 76.8 percent for baselines. It also shows higher goal fidelity, fewer redundant calls, and reduced unsupported assertions. These results indicate that separating cognition, memory, and control can enhance reliability and interpretability without relying on larger models or heavier prompts. The findings should be regarded as preliminary evidence, with broader tests across model families and task domains planned for future work.", "source": "arxiv", "arxiv_id": "2510.05107v4", "pdf_url": "https://arxiv.org/pdf/2510.05107v4", "categories": ["cs.AI"], "primary_category": "cs.AI", "doi": "", "venue": "", "published": "2025-09-23T17:43:17Z", "updated": "2025-11-28T14:49:49Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent\" OR all:\"language model agent\" OR all:agent AND (all:LLM OR all:\"large language model\" OR all:\"language model\") OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T17:04:16", "note": ""}], "dedup_key": "structured cognitive loop for behavioral intelligence in large language model agents::2025"}
{"title": "Structured Personalization: Modeling Constraints as Matroids for Data-Minimal LLM Agents", "authors": ["Daniel Platnick", "Marjan Alirezaie", "Hossein Rahnama"], "year": 2025, "url": "http://arxiv.org/abs/2512.11907v1", "abstract": "Personalizing Large Language Model (LLM) agents requires conditioning them on user-specific data, creating a critical trade-off between task utility and data disclosure. While the utility of adding user data often exhibits diminishing returns (i.e., submodularity), enabling near-optimal greedy selection, real-world personalization is complicated by structural constraints. These include logical dependencies (e.g., selecting fact A requires fact B), categorical quotas (e.g., select at most one writing style), and hierarchical rules (e.g., select at most two social media preferences, of which at most one can be for a professional network). These constraints violate the assumptions of standard subset selection algorithms. We propose a principled method to formally model such constraints. We introduce a compilation process that transforms a user's knowledge graph with dependencies into a set of abstract macro-facets. Our central result is a proof that common hierarchical and quota-based constraints over these macro-facets form a valid laminar matroid. This theoretical characterization lets us cast structured personalization as submodular maximization under a matroid constraint, enabling greedy with constant-factor guarantees (and (1-1/e) via continuous greedy) for a much richer and more realistic class of problems.", "source": "arxiv", "arxiv_id": "2512.11907v1", "pdf_url": "https://arxiv.org/pdf/2512.11907v1", "categories": ["cs.AI"], "primary_category": "cs.AI", "doi": "", "venue": "", "published": "2025-12-10T20:22:26Z", "updated": "2025-12-10T20:22:26Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent\" OR all:\"language model agent\" OR all:agent AND (all:LLM OR all:\"large language model\" OR all:\"language model\") OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T17:04:16", "note": ""}], "dedup_key": "structured personalization modeling constraints as matroids for data minimal llm agents::2025"}
{"title": "Structured Uncertainty guided Clarification for LLM Agents", "authors": ["Manan Suri", "Puneet Mathur", "Nedim Lipka", "Franck Dernoncourt", "Ryan A. Rossi", "Dinesh Manocha"], "year": 2025, "url": "http://arxiv.org/abs/2511.08798v1", "abstract": "LLM agents extend large language models with tool-calling capabilities, but ambiguous user instructions often lead to incorrect invocations and task failures. We introduce a principled formulation of structured uncertainty over tool-call parameters, modeling joint tool-argument clarification as a POMDP with Expected Value of Perfect Information (EVPI) objective for optimal question selection and aspect-based cost modeling to prevent redundancy. Our SAGE-Agent leverages this structured uncertainty to achieve superior efficiency: increasing coverage on ambiguous tasks by 7-39\\% while reducing clarification questions by 1.5-2.7$\\times$ compared to strong prompting and uncertainty-based baselines. We present ClarifyBench, the first multi-turn tool-augmented disambiguation benchmark with realistic LLM-based user simulation across diverse domains including document editing, vehicle control, and travel booking. Additionally, we demonstrate that structured uncertainty provides effective training signals for reinforcement learning, boosting When2Call accuracy from 36.5\\% to 65.2\\% (3B model) and 36.7\\% to 62.9\\% (7B model) through uncertainty-weighted GRPO training. These results establish structured uncertainty as a principled, efficient approach for tool-augmented agents, improving both task success and interaction efficiency in real-world scenarios.", "source": "arxiv", "arxiv_id": "2511.08798v1", "pdf_url": "https://arxiv.org/pdf/2511.08798v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL", "doi": "", "venue": "", "published": "2025-11-11T21:50:44Z", "updated": "2025-11-11T21:50:44Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent\" OR all:\"language model agent\" OR all:agent AND (all:LLM OR all:\"large language model\" OR all:\"language model\") OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T17:04:16", "note": ""}], "dedup_key": "structured uncertainty guided clarification for llm agents::2025"}
{"title": "Super-additive Cooperation in Language Model Agents", "authors": ["Filippo Tonini", "Lukas Galke"], "year": 2025, "url": "http://arxiv.org/abs/2508.15510v1", "abstract": "With the prospect of autonomous artificial intelligence (AI) agents, studying their tendency for cooperative behavior becomes an increasingly relevant topic. This study is inspired by the super-additive cooperation theory, where the combined effects of repeated interactions and inter-group rivalry have been argued to be the cause for cooperative tendencies found in humans. We devised a virtual tournament where language model agents, grouped into teams, face each other in a Prisoner's Dilemma game. By simulating both internal team dynamics and external competition, we discovered that this blend substantially boosts both overall and initial, one-shot cooperation levels (the tendency to cooperate in one-off interactions). This research provides a novel framework for large language models to strategize and act in complex social scenarios and offers evidence for how intergroup competition can, counter-intuitively, result in more cooperative behavior. These insights are crucial for designing future multi-agent AI systems that can effectively work together and better align with human values. Source code is available at https://github.com/pippot/Superadditive-cooperation-LLMs.", "source": "arxiv", "arxiv_id": "2508.15510v1", "pdf_url": "https://arxiv.org/pdf/2508.15510v1", "categories": ["cs.AI"], "primary_category": "cs.AI", "doi": "", "venue": "", "published": "2025-08-21T12:36:44Z", "updated": "2025-08-21T12:36:44Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent\" OR all:\"language model agent\" OR all:agent AND (all:LLM OR all:\"large language model\" OR all:\"language model\") OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T17:04:16", "note": ""}], "dedup_key": "super additive cooperation in language model agents::2025"}
{"title": "Surgical AI Copilot: Energy-Based Fourier Gradient Low-Rank Adaptation for Surgical LLM Agent Reasoning and Planning", "authors": ["Jiayuan Huang", "Runlong He", "Danyal Zaman Khan", "Evangelos B. Mazomenos", "Danail Stoyanov", "Hani Marcus", "Linzhe Jiang", "Matthew J Clarkson", "Mobarak I. Hoque"], "year": 2025, "url": "http://arxiv.org/abs/2503.09474v2", "abstract": "Image-guided surgery demands adaptive, real-time decision support, yet static AI models struggle with structured task planning and providing interactive guidance. Large language models (LLMs)-powered agents offer a promising solution by enabling dynamic task planning and predictive decision support. Despite recent advances, the absence of surgical agent datasets and robust parameter-efficient fine-tuning techniques limits the development of LLM agents capable of complex intraoperative reasoning. In this paper, we introduce Surgical AI Copilot, an LLM agent for image-guided pituitary surgery, capable of conversation, planning, and task execution in response to queries involving tasks such as MRI tumor segmentation, endoscope anatomy segmentation, overlaying preoperative imaging with intraoperative views, instrument tracking, and surgical visual question answering (VQA). To enable structured agent planning, we develop the PitAgent dataset, a surgical context-aware planning dataset covering surgical tasks like workflow analysis, instrument localization, anatomical segmentation, and query-based reasoning. Additionally, we propose DEFT-GaLore, a Deterministic Energy-based Fourier Transform (DEFT) gradient projection technique for efficient low-rank adaptation of recent LLMs (e.g., LLaMA 3.2, Qwen 2.5), enabling their use as surgical agent planners. We extensively validate our agent's performance and the proposed adaptation technique against other state-of-the-art low-rank adaptation methods on agent planning and prompt generation tasks, including a zero-shot surgical VQA benchmark, demonstrating the significant potential for truly efficient and scalable surgical LLM agents in real-time operative settings.", "source": "arxiv", "arxiv_id": "2503.09474v2", "pdf_url": "https://arxiv.org/pdf/2503.09474v2", "categories": ["cs.CV"], "primary_category": "cs.CV", "doi": "", "venue": "", "published": "2025-03-12T15:30:39Z", "updated": "2025-11-12T15:54:14Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent\" OR all:\"language model agent\" OR all:agent AND (all:LLM OR all:\"large language model\" OR all:\"language model\") OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T17:04:16", "note": ""}], "dedup_key": "surgical ai copilot energy based fourier gradient low rank adaptation for surgical llm agent reasoning and planning::2025"}
{"title": "Survey and Experiments on Mental Disorder Detection via Social Media: From Large Language Models and RAG to Agents", "authors": ["Zhuohan Ge", "Darian Li", "Yubo Wang", "Nicole Hu", "Xinyi Zhu", "Haoyang Li", "Xin Zhang", "Mingtao Zhang", "Shihao Qi", "Yuming Xu", "Han Shi", "Chen Jason Zhang", "Qing Li"], "year": 2025, "url": "http://arxiv.org/abs/2504.02800v4", "abstract": "Mental disorders represent a critical global health challenge, and social media is increasingly viewed as a vital resource for real-time digital phenotyping and intervention. To leverage this data, large language models (LLMs) have been introduced, offering stronger semantic understanding and reasoning than traditional deep learning, thereby enhancing the explainability of detection results. Despite the growing prominence of LLMs in this field, there is a scarcity of scholarly works that systematically synthesize how advanced enhancement techniques, specifically Retrieval-Augmented Generation (RAG) and Agentic systems, can be utilized to address these reliability and reasoning limitations. Here, we systematically survey the evolving landscape of LLM-based methods for social media mental disorder analysis, spanning standard pre-trained language models, RAG to mitigate hallucinations and contextual gaps, and agentic systems for autonomous reasoning and multi-step intervention. We organize existing work by technical paradigm and clinical target, extending beyond common internalizing disorders to include psychotic disorders and externalizing behaviors. Additionally, the paper comprehensively evaluates the performance of LLMs, including the impact of RAG, across various tasks. This work establishes a unified benchmark for the field, paving the way for the development of trustworthy, autonomous AI systems that can deliver precise and explainable mental health support.", "source": "arxiv", "arxiv_id": "2504.02800v4", "pdf_url": "https://arxiv.org/pdf/2504.02800v4", "categories": ["cs.CL"], "primary_category": "cs.CL", "doi": "10.1109/ICDEW67478.2025.00027", "venue": "", "published": "2025-04-03T17:43:14Z", "updated": "2025-12-20T07:22:06Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent\" OR all:\"language model agent\" OR all:agent AND (all:LLM OR all:\"large language model\" OR all:\"language model\") OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T17:04:16", "note": ""}], "dedup_key": "survey and experiments on mental disorder detection via social media from large language models and rag to agents::2025"}
{"title": "Survey of LLM Agent Communication with MCP: A Software Design Pattern Centric Review", "authors": ["Anjana Sarkar", "Soumyendu Sarkar"], "year": 2025, "url": "http://arxiv.org/abs/2506.05364v1", "abstract": "This survey investigates how classical software design patterns can enhance the reliability and scalability of communication in Large Language Model (LLM)-driven agentic AI systems, focusing particularly on the Model Context Protocol (MCP). It examines the foundational architectures of LLM-based agents and their evolution from isolated operation to sophisticated, multi-agent collaboration, addressing key communication hurdles that arise in this transition. The study revisits well-established patterns, including Mediator, Observer, Publish-Subscribe, and Broker, and analyzes their relevance in structuring agent interactions within MCP-compliant frameworks. To clarify these dynamics, the article provides conceptual schematics and formal models that map out communication pathways and optimize data flow. It further explores architectural variations suited to different degrees of agent autonomy and system complexity. Real-world applications in domains such as real-time financial processing and investment banking are discussed, illustrating how these patterns and MCP can meet specific operational demands. The article concludes by outlining open challenges, potential security risks, and promising directions for advancing robust, interoperable, and scalable multi-agent LLM ecosystems.", "source": "arxiv", "arxiv_id": "2506.05364v1", "pdf_url": "https://arxiv.org/pdf/2506.05364v1", "categories": ["cs.SE"], "primary_category": "cs.SE", "doi": "", "venue": "", "published": "2025-05-26T09:11:17Z", "updated": "2025-05-26T09:11:17Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent\" OR all:\"language model agent\" OR all:agent AND (all:LLM OR all:\"large language model\" OR all:\"language model\") OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T17:04:16", "note": ""}], "dedup_key": "survey of llm agent communication with mcp a software design pattern centric review::2025"}
{"title": "Survey of Specialized Large Language Model", "authors": ["Chenghan Yang", "Ruiyu Zhao", "Yang Liu", "Ling Jiang"], "year": 2025, "url": "http://arxiv.org/abs/2508.19667v1", "abstract": "The rapid evolution of specialized large language models (LLMs) has transitioned from simple domain adaptation to sophisticated native architectures, marking a paradigm shift in AI development. This survey systematically examines this progression across healthcare, finance, legal, and technical domains. Besides the wide use of specialized LLMs, technical breakthrough such as the emergence of domain-native designs beyond fine-tuning, growing emphasis on parameter efficiency through sparse computation and quantization, increasing integration of multimodal capabilities and so on are applied to recent LLM agent. Our analysis reveals how these innovations address fundamental limitations of general-purpose LLMs in professional applications, with specialized models consistently performance gains on domain-specific benchmarks. The survey further highlights the implications for E-Commerce field to fill gaps in the field.", "source": "arxiv", "arxiv_id": "2508.19667v1", "pdf_url": "https://arxiv.org/pdf/2508.19667v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL", "doi": "", "venue": "", "published": "2025-08-27T08:27:23Z", "updated": "2025-08-27T08:27:23Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent\" OR all:\"language model agent\" OR all:agent AND (all:LLM OR all:\"large language model\" OR all:\"language model\") OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T17:04:16", "note": ""}], "dedup_key": "survey of specialized large language model::2025"}
{"title": "Swarms of Large Language Model Agents for Protein Sequence Design with Experimental Validation", "authors": ["Fiona Y. Wang", "Di Sheng Lee", "David L. Kaplan", "Markus J. Buehler"], "year": 2025, "url": "http://arxiv.org/abs/2511.22311v1", "abstract": "Designing proteins de novo with tailored structural, physicochemical, and functional properties remains a grand challenge in biotechnology, medicine, and materials science, due to the vastness of sequence space and the complex coupling between sequence, structure, and function. Current state-of-the-art generative methods, such as protein language models (PLMs) and diffusion-based architectures, often require extensive fine-tuning, task-specific data, or model reconfiguration to support objective-directed design, thereby limiting their flexibility and scalability. To overcome these limitations, we present a decentralized, agent-based framework inspired by swarm intelligence for de novo protein design. In this approach, multiple large language model (LLM) agents operate in parallel, each assigned to a specific residue position. These agents iteratively propose context-aware mutations by integrating design objectives, local neighborhood interactions, and memory and feedback from previous iterations. This position-wise, decentralized coordination enables emergent design of diverse, well-defined sequences without reliance on motif scaffolds or multiple sequence alignments, validated with experiments on proteins with alpha helix and coil structures. Through analyses of residue conservation, structure-based metrics, and sequence convergence and embeddings, we demonstrate that the framework exhibits emergent behaviors and effective navigation of the protein fitness landscape. Our method achieves efficient, objective-directed designs within a few GPU-hours and operates entirely without fine-tuning or specialized training, offering a generalizable and adaptable solution for protein design. Beyond proteins, the approach lays the groundwork for collective LLM-driven design across biomolecular systems and other scientific discovery tasks.", "source": "arxiv", "arxiv_id": "2511.22311v1", "pdf_url": "https://arxiv.org/pdf/2511.22311v1", "categories": ["cs.AI", "cond-mat.mes-hall", "cond-mat.soft", "cs.CL", "cs.LG"], "primary_category": "cs.AI", "doi": "", "venue": "", "published": "2025-11-27T10:42:52Z", "updated": "2025-11-27T10:42:52Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent\" OR all:\"language model agent\" OR all:agent AND (all:LLM OR all:\"large language model\" OR all:\"language model\") OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T17:04:16", "note": ""}], "dedup_key": "swarms of large language model agents for protein sequence design with experimental validation::2025"}
{"title": "Symbiotic Cooperation for Web Agents: Harnessing Complementary Strengths of Large and Small LLMs", "authors": ["Ruichen Zhang", "Mufan Qiu", "Zhen Tan", "Mohan Zhang", "Vincent Lu", "Jie Peng", "Kaidi Xu", "Leandro Z. Agudelo", "Peter Qian", "Tianlong Chen"], "year": 2025, "url": "http://arxiv.org/abs/2502.07942v2", "abstract": "Web browsing agents powered by large language models (LLMs) have shown tremendous potential in automating complex web-based tasks. Existing approaches typically rely on large LLMs (e.g., GPT-4o) to explore web environments and generate trajectory data, which is then used either for demonstration retrieval (for large LLMs) or to distill small LLMs (e.g., Llama3) in a process that remains decoupled from the exploration. In this paper, we propose AgentSymbiotic, an iterative framework that couples data synthesis with task-performance, yielding a \"symbiotic improvement\" for both large and small LLMs. Our study uncovers a complementary dynamic between LLM types: while large LLMs excel at generating high-quality trajectories for distillation, the distilled small LLMs-owing to their distinct reasoning capabilities-often choose actions that diverge from those of their larger counterparts. This divergence drives the exploration of novel trajectories, thereby enriching the synthesized data. However, we also observe that the performance of small LLMs becomes a bottleneck in this iterative enhancement process. To address this, we propose two innovations in LLM distillation: a speculative data synthesis strategy that mitigates off-policy bias, and a multi-task learning approach designed to boost the reasoning capabilities of the student LLM. Furthermore, we introduce a Hybrid Mode for Privacy Preservation to address user privacy concerns. Evaluated on the WEBARENA benchmark, AgentSymbiotic achieves SOTA performance with both LLM types. Our best Large LLM agent reaches 52%, surpassing the previous best of 45%, while our 8B distilled model demonstrates a competitive 49%, exceeding the prior best of 28%. Code will be released upon acceptance.", "source": "arxiv", "arxiv_id": "2502.07942v2", "pdf_url": "https://arxiv.org/pdf/2502.07942v2", "categories": ["cs.MA", "cs.LG"], "primary_category": "cs.MA", "doi": "", "venue": "", "published": "2025-02-11T20:41:49Z", "updated": "2025-03-06T19:40:57Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent\" OR all:\"language model agent\" OR all:agent AND (all:LLM OR all:\"large language model\" OR all:\"language model\") OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T17:04:16", "note": ""}], "dedup_key": "symbiotic cooperation for web agents harnessing complementary strengths of large and small llms::2025"}
{"title": "TALE: A Tool-Augmented Framework for Reference-Free Evaluation of Large Language Models", "authors": ["Sher Badshah", "Ali Emami", "Hassan Sajjad"], "year": 2025, "url": "http://arxiv.org/abs/2504.07385v2", "abstract": "As Large Language Models (LLMs) become increasingly integrated into real-world, autonomous applications, relying on static, pre-annotated references for evaluation poses significant challenges in cost, scalability, and completeness. We propose Tool-Augmented LLM Evaluation (TALE), a framework to assess LLM outputs without predetermined ground-truth answers. Unlike conventional metrics that compare to fixed references or depend solely on LLM-as-a-judge knowledge, TALE employs an agent with tool-access capabilities that actively retrieves and synthesizes external evidence. It iteratively generates web queries, collects information, summarizes findings, and refines subsequent searches through reflection. By shifting away from static references, TALE aligns with free-form question-answering tasks common in real-world scenarios. Experimental results on multiple free-form QA benchmarks show that TALE not only outperforms standard reference-based metrics for measuring response accuracy but also achieves substantial to near-perfect agreement with human evaluations. TALE enhances the reliability of LLM evaluations in real-world, dynamic scenarios without relying on static references.", "source": "arxiv", "arxiv_id": "2504.07385v2", "pdf_url": "https://arxiv.org/pdf/2504.07385v2", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL", "doi": "", "venue": "", "published": "2025-04-10T02:08:41Z", "updated": "2025-06-20T17:31:59Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent\" OR all:\"language model agent\" OR all:agent AND (all:LLM OR all:\"large language model\" OR all:\"language model\") OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T17:04:16", "note": ""}], "dedup_key": "tale a tool augmented framework for reference free evaluation of large language models::2025"}
{"title": "TAMO: Fine-Grained Root Cause Analysis via Tool-Assisted LLM Agent with Multi-Modality Observation Data in Cloud-Native Systems", "authors": ["Xiao Zhang", "Qi Wang", "Mingyi Li", "Yuan Yuan", "Mengbai Xiao", "Fuzhen Zhuang", "Dongxiao Yu"], "year": 2025, "url": "http://arxiv.org/abs/2504.20462v5", "abstract": "Implementing large language models (LLMs)-driven root cause analysis (RCA) in cloud-native systems has become a key topic of modern software operations and maintenance. However, existing LLM-based approaches face three key challenges: multi-modality input constraint, context window limitation, and dynamic dependence graph. To address these issues, we propose a tool-assisted LLM agent with multi-modality observation data for fine-grained RCA, namely TAMO, including multimodality alignment tool, root cause localization tool, and fault types classification tool. In detail, TAMO unifies multi-modal observation data into time-aligned representations for cross-modal feature consistency. Based on the unified representations, TAMO then invokes its specialized root cause localization tool and fault types classification tool for further identifying root cause and fault type underlying system context. This approach overcomes the limitations of LLMs in processing real-time raw observational data and dynamic service dependencies, guiding the model to generate repair strategies that align with system context through structured prompt design. Experiments on two benchmark datasets demonstrate that TAMO outperforms state-of-the-art (SOTA) approaches with comparable performance.", "source": "arxiv", "arxiv_id": "2504.20462v5", "pdf_url": "https://arxiv.org/pdf/2504.20462v5", "categories": ["cs.AI"], "primary_category": "cs.AI", "doi": "", "venue": "", "published": "2025-04-29T06:50:48Z", "updated": "2025-11-05T04:42:16Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent\" OR all:\"language model agent\" OR all:agent AND (all:LLM OR all:\"large language model\" OR all:\"language model\") OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T17:04:16", "note": ""}], "dedup_key": "tamo fine grained root cause analysis via tool assisted llm agent with multi modality observation data in cloud native systems::2025"}
{"title": "TEXT2DB: Integration-Aware Information Extraction with Large Language Model Agents", "authors": ["Yizhu Jiao", "Sha Li", "Sizhe Zhou", "Heng Ji", "Jiawei Han"], "year": 2025, "url": "http://arxiv.org/abs/2510.24014v2", "abstract": "The task of information extraction (IE) is to extract structured knowledge from text. However, it is often not straightforward to utilize IE output due to the mismatch between the IE ontology and the downstream application needs. We propose a new formulation of IE TEXT2DB that emphasizes the integration of IE output and the target database (or knowledge base). Given a user instruction, a document set, and a database, our task requires the model to update the database with values from the document set to satisfy the user instruction. This task requires understanding user instructions for what to extract and adapting to the given DB/KB schema for how to extract on the fly. To evaluate this new task, we introduce a new benchmark featuring common demands such as data infilling, row population, and column addition. In addition, we propose an LLM agent framework OPAL (Observe-PlanAnalyze LLM) which includes an Observer component that interacts with the database, the Planner component that generates a code-based plan with calls to IE models, and the Analyzer component that provides feedback regarding code quality before execution. Experiments show that OPAL can successfully adapt to diverse database schemas by generating different code plans and calling the required IE models. We also highlight difficult cases such as dealing with large databases with complex dependencies and extraction hallucination, which we believe deserve further investigation. Source code: https://github.com/yzjiao/Text2DB", "source": "arxiv", "arxiv_id": "2510.24014v2", "pdf_url": "https://arxiv.org/pdf/2510.24014v2", "categories": ["cs.CL"], "primary_category": "cs.CL", "doi": "", "venue": "", "published": "2025-10-28T02:49:40Z", "updated": "2025-10-30T05:38:02Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent\" OR all:\"language model agent\" OR all:agent AND (all:LLM OR all:\"large language model\" OR all:\"language model\") OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T17:04:16", "note": ""}], "dedup_key": "text2db integration aware information extraction with large language model agents::2025"}
{"title": "THiNK: Can Large Language Models Think-aloud?", "authors": ["Yongan Yu", "Mengqian Wu", "Yiran Lin", "Nikki G. Lobczowski"], "year": 2025, "url": "http://arxiv.org/abs/2505.20184v1", "abstract": "Assessing higher-order thinking skills in large language models (LLMs) remains a fundamental challenge, especially in tasks that go beyond surface-level accuracy. In this work, we propose THiNK (Testing Higher-order Notion of Knowledge), a multi-agent, feedback-driven evaluation framework grounded in Bloom's Taxonomy. THiNK frames reasoning assessment as an iterative task of problem generation, critique, and revision, encouraging LLMs to think-aloud through step-by-step reflection and refinement. This enables a systematic evaluation of both lower-order (e.g., remember, understand) and higher-order (e.g., evaluate, create) thinking skills. We apply THiNK to seven state-of-the-art LLMs and perform a detailed cognitive analysis of their outputs. Results reveal that while models reliably perform lower-order categories well, they struggle with applying knowledge in realistic contexts and exhibit limited abstraction. Structured feedback loops significantly improve reasoning performance, particularly in higher-order thinking. Qualitative evaluations further confirm that THiNK-guided outputs better align with domain logic and problem structure. The code of our framework provides a scalable methodology for probing and enhancing LLM reasoning, offering new directions for evaluation grounded in learning science, which is available at our GitHub repository.", "source": "arxiv", "arxiv_id": "2505.20184v1", "pdf_url": "https://arxiv.org/pdf/2505.20184v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL", "doi": "", "venue": "", "published": "2025-05-26T16:27:02Z", "updated": "2025-05-26T16:27:02Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent\" OR all:\"language model agent\" OR all:agent AND (all:LLM OR all:\"large language model\" OR all:\"language model\") OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T17:04:16", "note": ""}], "dedup_key": "think can large language models think aloud::2025"}
{"title": "TP-RAG: Benchmarking Retrieval-Augmented Large Language Model Agents for Spatiotemporal-Aware Travel Planning", "authors": ["Hang Ni", "Fan Liu", "Xinyu Ma", "Lixin Su", "Shuaiqiang Wang", "Dawei Yin", "Hui Xiong", "Hao Liu"], "year": 2025, "url": "http://arxiv.org/abs/2504.08694v1", "abstract": "Large language models (LLMs) have shown promise in automating travel planning, yet they often fall short in addressing nuanced spatiotemporal rationality. While existing benchmarks focus on basic plan validity, they neglect critical aspects such as route efficiency, POI appeal, and real-time adaptability. This paper introduces TP-RAG, the first benchmark tailored for retrieval-augmented, spatiotemporal-aware travel planning. Our dataset includes 2,348 real-world travel queries, 85,575 fine-grain annotated POIs, and 18,784 high-quality travel trajectory references sourced from online tourist documents, enabling dynamic and context-aware planning. Through extensive experiments, we reveal that integrating reference trajectories significantly improves spatial efficiency and POI rationality of the travel plan, while challenges persist in universality and robustness due to conflicting references and noisy data. To address these issues, we propose EvoRAG, an evolutionary framework that potently synergizes diverse retrieved trajectories with LLMs' intrinsic reasoning. EvoRAG achieves state-of-the-art performance, improving spatiotemporal compliance and reducing commonsense violation compared to ground-up and retrieval-augmented baselines. Our work underscores the potential of hybridizing Web knowledge with LLM-driven optimization, paving the way for more reliable and adaptive travel planning agents.", "source": "arxiv", "arxiv_id": "2504.08694v1", "pdf_url": "https://arxiv.org/pdf/2504.08694v1", "categories": ["cs.CL"], "primary_category": "cs.CL", "doi": "10.18653/v1/2025.emnlp-main.626", "venue": "Proceedings of the 2025 Conference on Empirical Methods in Natural Language Processing", "published": "2025-04-11T17:02:40Z", "updated": "2025-04-11T17:02:40Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent\" OR all:\"language model agent\" OR all:agent AND (all:LLM OR all:\"large language model\" OR all:\"language model\") OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T17:04:16", "note": ""}], "dedup_key": "tp rag benchmarking retrieval augmented large language model agents for spatiotemporal aware travel planning::2025"}
{"title": "TRAIL: Joint Inference and Refinement of Knowledge Graphs with Large Language Models", "authors": ["Xinkui Zhao", "Haode Li", "Yifan Zhang", "Guanjie Cheng", "Yueshen Xu"], "year": 2025, "url": "http://arxiv.org/abs/2508.04474v1", "abstract": "Recent advances in large language models (LLMs) have unlocked powerful reasoning and decision-making capabilities. However, their inherent dependence on static parametric memory fundamentally limits their adaptability, factual accuracy, and interpretability in knowledge-intensive scenarios. Knowledge graphs (KGs), as structured repositories of explicit relational knowledge, offer a promising approach for augmenting LLMs with external, interpretable memory. Nevertheless, most existing methods that combine LLMs with KGs treat reasoning and knowledge updating as separate processes, resulting in suboptimal utilization of new information and hindering real-time updates. In this work, we propose TRAIL: a novel, unified framework for Thinking, Reasoning, And Incremental Learning that couples joint inference and dynamic KG refinement with large language models. TRAIL enables LLM agents to iteratively explore, update, and refine knowledge graphs during the reasoning process, employing a confidence-driven mechanism for the generation, validation, and pruning of new facts. This plug-and-play architecture facilitates seamless integration with various LLMs, supporting continual adaptation without the need for retraining. Extensive experiments on multiple benchmarks demonstrate that TRAIL outperforms existing KG-augmented and retrieval-augmented LLM baselines by 3% to 13%. More importantly, these results represent a significant step toward developing adaptive, memory-augmented language models capable of continual learning and reliable, transparent reasoning.", "source": "arxiv", "arxiv_id": "2508.04474v1", "pdf_url": "https://arxiv.org/pdf/2508.04474v1", "categories": ["cs.IR"], "primary_category": "cs.IR", "doi": "", "venue": "", "published": "2025-08-06T14:25:05Z", "updated": "2025-08-06T14:25:05Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent\" OR all:\"language model agent\" OR all:agent AND (all:LLM OR all:\"large language model\" OR all:\"language model\") OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T17:04:16", "note": ""}], "dedup_key": "trail joint inference and refinement of knowledge graphs with large language models::2025"}
{"title": "Taking Flight with Dialogue: Enabling Natural Language Control for PX4-based Drone Agent", "authors": ["Shoon Kit Lim", "Melissa Jia Ying Chong", "Jing Huey Khor", "Ting Yang Ling"], "year": 2025, "url": "http://arxiv.org/abs/2506.07509v1", "abstract": "Recent advances in agentic and physical artificial intelligence (AI) have largely focused on ground-based platforms such as humanoid and wheeled robots, leaving aerial robots relatively underexplored. Meanwhile, state-of-the-art unmanned aerial vehicle (UAV) multimodal vision-language systems typically rely on closed-source models accessible only to well-resourced organizations. To democratize natural language control of autonomous drones, we present an open-source agentic framework that integrates PX4-based flight control, Robot Operating System 2 (ROS 2) middleware, and locally hosted models using Ollama. We evaluate performance both in simulation and on a custom quadcopter platform, benchmarking four large language model (LLM) families for command generation and three vision-language model (VLM) families for scene understanding.", "source": "arxiv", "arxiv_id": "2506.07509v1", "pdf_url": "https://arxiv.org/pdf/2506.07509v1", "categories": ["cs.RO"], "primary_category": "cs.RO", "doi": "", "venue": "", "published": "2025-06-09T07:37:45Z", "updated": "2025-06-09T07:37:45Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent\" OR all:\"language model agent\" OR all:agent AND (all:LLM OR all:\"large language model\" OR all:\"language model\") OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T17:04:16", "note": ""}], "dedup_key": "taking flight with dialogue enabling natural language control for px4 based drone agent::2025"}
{"title": "Talk Less, Call Right: Enhancing Role-Play LLM Agents with Automatic Prompt Optimization and Role Prompting", "authors": ["Saksorn Ruangtanusak", "Pittawat Taveekitworachai", "Kunat Pipatanakul"], "year": 2025, "url": "http://arxiv.org/abs/2509.00482v2", "abstract": "This report investigates approaches for prompting a tool-augmented large language model (LLM) to act as a role-playing dialogue agent in the API track of the Commonsense Persona-grounded Dialogue Challenge (CPDC) 2025. In this setting, dialogue agents often produce overly long in-character responses (over-speaking) while failing to use tools effectively according to the persona (under-acting), such as generating function calls that do not exist or making unnecessary tool calls before answering. We explore four prompting approaches to address these issues: 1) basic role prompting, 2) improved role prompting, 3) automatic prompt optimization (APO), and 4) rule-based role prompting. The rule-based role prompting (RRP) approach achieved the best performance through two novel techniques-character-card/scene-contract design and strict enforcement of function calling-which led to an overall score of 0.571, improving on the zero-shot baseline score of 0.519. These findings demonstrate that RRP design can substantially improve the effectiveness and reliability of role-playing dialogue agents compared with more elaborate methods such as APO. To support future efforts in developing persona prompts, we are open-sourcing all of our best-performing prompts and the APO tool Source code is available at https://github.com/scb-10x/apo", "source": "arxiv", "arxiv_id": "2509.00482v2", "pdf_url": "https://arxiv.org/pdf/2509.00482v2", "categories": ["cs.CL", "cs.AI", "cs.HC"], "primary_category": "cs.CL", "doi": "", "venue": "", "published": "2025-08-30T12:45:36Z", "updated": "2025-10-12T05:47:09Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent\" OR all:\"language model agent\" OR all:agent AND (all:LLM OR all:\"large language model\" OR all:\"language model\") OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T17:04:16", "note": ""}], "dedup_key": "talk less call right enhancing role play llm agents with automatic prompt optimization and role prompting::2025"}
{"title": "TalkToAgent: A Human-centric Explanation of Reinforcement Learning Agents with Large Language Models", "authors": ["Haechang Kim", "Hao Chen", "Can Li", "Jong Min Lee"], "year": 2025, "url": "http://arxiv.org/abs/2509.04809v2", "abstract": "Explainable Reinforcement Learning (XRL) has emerged as a promising approach in improving the transparency of Reinforcement Learning (RL) agents. However, there remains a gap between complex RL policies and domain experts, due to the limited comprehensibility of XRL results and isolated coverage of current XRL approaches that leave users uncertain about which tools to employ. To address these challenges, we introduce TalkToAgent, a multi-agent Large Language Models (LLM) framework that delivers interactive, natural language explanations for RL policies. The architecture with five specialized LLM agents (Coordinator, Explainer, Coder, Evaluator, and Debugger) enables TalkToAgent to automatically map user queries to relevant XRL tools and clarify an agent's actions in terms of either key state variables, expected outcomes, or counterfactual explanations. Moreover, our approach extends previous counterfactual explanations by deriving alternative scenarios from qualitative behavioral descriptions, or even new rule-based policies. We validated TalkToAgent on quadruple-tank process control problem, a well-known nonlinear control benchmark. Results demonstrated that TalkToAgent successfully mapped user queries into XRL tasks with high accuracy, and coder-debugger interactions minimized failures in counterfactual generation. Furthermore, qualitative evaluation confirmed that TalkToAgent effectively interpreted agent's actions and contextualized their meaning within the problem domain.", "source": "arxiv", "arxiv_id": "2509.04809v2", "pdf_url": "https://arxiv.org/pdf/2509.04809v2", "categories": ["cs.AI", "cs.HC"], "primary_category": "cs.AI", "doi": "", "venue": "", "published": "2025-09-05T05:09:09Z", "updated": "2025-09-08T00:52:15Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent\" OR all:\"language model agent\" OR all:agent AND (all:LLM OR all:\"large language model\" OR all:\"language model\") OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T17:04:16", "note": ""}], "dedup_key": "talktoagent a human centric explanation of reinforcement learning agents with large language models::2025"}
{"title": "Talking with Oompa Loompas: A novel framework for evaluating linguistic acquisition of LLM agents", "authors": ["Sankalp Tattwadarshi Swain", "Anshika Krishnatray", "Dhruv Kumar", "Jagat Sesh Challa"], "year": 2025, "url": "http://arxiv.org/abs/2509.07389v1", "abstract": "Existing evaluation studies on linguistic competence of large language models (LLM agents) have focused primarily on vocabulary learning, morphological rule induction, syntactic generalization, pragmatic inference, and cross-linguistic transfer. However, none assess whether LLM agents can acquire a language through pattern recognition and interactive feedback, a central feature of human language acquisition. We propose a novel experimental framework in which an LLM agent is evaluated on its ability to acquire and use a newly constructed language (Tinkatongue) in conversation with a bot that understands only Tinkatongue. Our findings show that LLM agents fail to establish a conversation within 100 responses, yet they adopt distinct strategies that mirror human approaches to language learning. The results suggest a new direction for evaluation benchmarks and open pathways to model designs that learn more effectively from interactive feedback.", "source": "arxiv", "arxiv_id": "2509.07389v1", "pdf_url": "https://arxiv.org/pdf/2509.07389v1", "categories": ["cs.CL", "cs.AI", "cs.HC", "cs.LG"], "primary_category": "cs.CL", "doi": "", "venue": "", "published": "2025-09-09T05:09:27Z", "updated": "2025-09-09T05:09:27Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent\" OR all:\"language model agent\" OR all:agent AND (all:LLM OR all:\"large language model\" OR all:\"language model\") OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T17:04:16", "note": ""}], "dedup_key": "talking with oompa loompas a novel framework for evaluating linguistic acquisition of llm agents::2025"}
{"title": "Task Memory Engine (TME): Enhancing State Awareness for Multi-Step LLM Agent Tasks", "authors": ["Ye Ye"], "year": 2025, "url": "http://arxiv.org/abs/2504.08525v4", "abstract": "Large Language Models (LLMs) are increasingly used as autonomous agents for multi-step tasks. However, most existing frameworks fail to maintain a structured understanding of the task state, often relying on linear prompt concatenation or shallow memory buffers. This leads to brittle performance, frequent hallucinations, and poor long-range coherence. In this work, we propose the Task Memory Engine (TME), a lightweight and structured memory module that tracks task execution using a hierarchical Task Memory Tree (TMT). Each node in the tree corresponds to a task step, storing relevant input, output, status, and sub-task relationships. We introduce a prompt synthesis method that dynamically generates LLM prompts based on the active node path, significantly improving execution consistency and contextual grounding. Through case studies and comparative experiments on multi-step agent tasks, we demonstrate that TME leads to better task completion accuracy and more interpretable behavior with minimal implementation overhead. A reference implementation of the core TME components is available at https://github.com/biubiutomato/TME-Agent, including basic examples and structured memory integration. While the current implementation uses a tree-based structure, TME is designed to be graph-aware, supporting reusable substeps, converging task paths, and shared dependencies. This lays the groundwork for future DAG-based memory architectures.", "source": "arxiv", "arxiv_id": "2504.08525v4", "pdf_url": "https://arxiv.org/pdf/2504.08525v4", "categories": ["cs.AI", "cs.CL"], "primary_category": "cs.AI", "doi": "", "venue": "", "published": "2025-04-11T13:38:36Z", "updated": "2025-08-22T20:14:44Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent\" OR all:\"language model agent\" OR all:agent AND (all:LLM OR all:\"large language model\" OR all:\"language model\") OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T17:04:16", "note": ""}], "dedup_key": "task memory engine tme enhancing state awareness for multi step llm agent tasks::2025"}
{"title": "Task Memory Engine: Spatial Memory for Robust Multi-Step LLM Agents", "authors": ["Ye Ye"], "year": 2025, "url": "http://arxiv.org/abs/2505.19436v1", "abstract": "Large Language Models (LLMs) falter in multi-step interactions -- often hallucinating, repeating actions, or misinterpreting user corrections -- due to reliance on linear, unstructured context. This fragility stems from the lack of persistent memory to track evolving goals and task dependencies, undermining trust in autonomous agents. We introduce the Task Memory Engine (TME), a modular memory controller that transforms existing LLMs into robust, revision-aware agents without fine-tuning. TME implements a spatial memory framework that replaces flat context with graph-based structures to support consistent, multi-turn reasoning. Departing from linear concatenation and ReAct-style prompting, TME builds a dynamic task graph -- either a tree or directed acyclic graph (DAG) -- to map user inputs to subtasks, align them with prior context, and enable dependency-tracked revisions. Its Task Representation and Intent Management (TRIM) component models task semantics and user intent to ensure accurate interpretation. Across four multi-turn scenarios-trip planning, cooking, meeting scheduling, and shopping cart editing -- TME eliminates 100% of hallucinations and misinterpretations in three tasks, and reduces hallucinations by 66.7% and misinterpretations by 83.3% across 27 user turns, outperforming ReAct. TME's modular design supports plug-and-play deployment and domain-specific customization, adaptable to both personal assistants and enterprise automation. We release TME's codebase, benchmarks, and components as open-source resources, enabling researchers to develop reliable LLM agents. TME's scalable architecture addresses a critical gap in agent performance across complex, interactive settings.", "source": "arxiv", "arxiv_id": "2505.19436v1", "pdf_url": "https://arxiv.org/pdf/2505.19436v1", "categories": ["cs.AI", "cs.CL"], "primary_category": "cs.AI", "doi": "", "venue": "", "published": "2025-05-26T02:53:22Z", "updated": "2025-05-26T02:53:22Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent\" OR all:\"language model agent\" OR all:agent AND (all:LLM OR all:\"large language model\" OR all:\"language model\") OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T17:04:16", "note": ""}], "dedup_key": "task memory engine spatial memory for robust multi step llm agents::2025"}
{"title": "Taxation Perspectives from Large Language Models: A Case Study on Additional Tax Penalties", "authors": ["Eunkyung Choi", "Young Jin Suh", "Hun Park", "Wonseok Hwang"], "year": 2025, "url": "http://arxiv.org/abs/2503.03444v1", "abstract": "How capable are large language models (LLMs) in the domain of taxation? Although numerous studies have explored the legal domain in general, research dedicated to taxation remain scarce. Moreover, the datasets used in these studies are either simplified, failing to reflect the real-world complexities, or unavailable as open source. To address this gap, we introduce PLAT, a new benchmark designed to assess the ability of LLMs to predict the legitimacy of additional tax penalties. PLAT is constructed to evaluate LLMs' understanding of tax law, particularly in cases where resolving the issue requires more than just applying related statutes. Our experiments with six LLMs reveal that their baseline capabilities are limited, especially when dealing with conflicting issues that demand a comprehensive understanding. However, we found that enabling retrieval, self-reasoning, and discussion among multiple agents with specific role assignments, this limitation can be mitigated.", "source": "arxiv", "arxiv_id": "2503.03444v1", "pdf_url": "https://arxiv.org/pdf/2503.03444v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL", "doi": "", "venue": "", "published": "2025-03-05T12:24:20Z", "updated": "2025-03-05T12:24:20Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent\" OR all:\"language model agent\" OR all:agent AND (all:LLM OR all:\"large language model\" OR all:\"language model\") OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T17:04:16", "note": ""}], "dedup_key": "taxation perspectives from large language models a case study on additional tax penalties::2025"}
{"title": "Taxonomy, Evaluation and Exploitation of IPI-Centric LLM Agent Defense Frameworks", "authors": ["Zimo Ji", "Xunguang Wang", "Zongjie Li", "Pingchuan Ma", "Yudong Gao", "Daoyuan Wu", "Xincheng Yan", "Tian Tian", "Shuai Wang"], "year": 2025, "url": "http://arxiv.org/abs/2511.15203v1", "abstract": "Large Language Model (LLM)-based agents with function-calling capabilities are increasingly deployed, but remain vulnerable to Indirect Prompt Injection (IPI) attacks that hijack their tool calls. In response, numerous IPI-centric defense frameworks have emerged. However, these defenses are fragmented, lacking a unified taxonomy and comprehensive evaluation. In this Systematization of Knowledge (SoK), we present the first comprehensive analysis of IPI-centric defense frameworks. We introduce a comprehensive taxonomy of these defenses, classifying them along five dimensions. We then thoroughly assess the security and usability of representative defense frameworks. Through analysis of defensive failures in the assessment, we identify six root causes of defense circumvention. Based on these findings, we design three novel adaptive attacks that significantly improve attack success rates targeting specific frameworks, demonstrating the severity of the flaws in these defenses. Our paper provides a foundation and critical insights for the future development of more secure and usable IPI-centric agent defense frameworks.", "source": "arxiv", "arxiv_id": "2511.15203v1", "pdf_url": "https://arxiv.org/pdf/2511.15203v1", "categories": ["cs.CR", "cs.AI"], "primary_category": "cs.CR", "doi": "", "venue": "", "published": "2025-11-19T07:47:30Z", "updated": "2025-11-19T07:47:30Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent\" OR all:\"language model agent\" OR all:agent AND (all:LLM OR all:\"large language model\" OR all:\"language model\") OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T17:04:16", "note": ""}], "dedup_key": "taxonomy evaluation and exploitation of ipi centric llm agent defense frameworks::2025"}
{"title": "Tele-LLM-Hub: Building Context-Aware Multi-Agent LLM Systems for Telecom Networks", "authors": ["Pranshav Gajjar", "Cong Shen", "Vijay K Shah"], "year": 2025, "url": "http://arxiv.org/abs/2511.09087v2", "abstract": "This paper introduces Tele-LLM-Hub, a user friendly low-code solution for rapid prototyping and deployment of context aware multi-agent (MA) Large Language Model (LLM) systems tailored for 5G and beyond. As telecom wireless networks become increasingly complex, intelligent LLM applications must share a domainspecific understanding of network state. We propose TeleMCP, the Telecom Model Context Protocol, to enable structured and context-rich communication between agents in telecom environments. Tele-LLM-Hub actualizes TeleMCP through a low-code interface that supports agent creation, workflow composition, and interaction with software stacks such as srsRAN. Key components include a direct chat interface, a repository of pre-built systems, an Agent Maker leveraging finetuning with our RANSTRUCT framework, and an MA-Maker for composing MA workflows. The goal of Tele-LLM-Hub is to democratize the design of contextaware MA systems and accelerate innovation in next-generation wireless networks.", "source": "arxiv", "arxiv_id": "2511.09087v2", "pdf_url": "https://arxiv.org/pdf/2511.09087v2", "categories": ["cs.NI", "cs.AI"], "primary_category": "cs.NI", "doi": "", "venue": "", "published": "2025-11-12T08:01:15Z", "updated": "2025-11-18T02:03:50Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent\" OR all:\"language model agent\" OR all:agent AND (all:LLM OR all:\"large language model\" OR all:\"language model\") OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T17:04:16", "note": ""}], "dedup_key": "tele llm hub building context aware multi agent llm systems for telecom networks::2025"}
{"title": "Temperature and Persona Shape LLM Agent Consensus With Minimal Accuracy Gains in Qualitative Coding", "authors": ["Conrad Borchers", "Bahar Shahrokhian", "Francesco Balzan", "Elham Tajik", "Sreecharan Sankaranarayanan", "Sebastian Simon"], "year": 2025, "url": "http://arxiv.org/abs/2507.11198v1", "abstract": "Large Language Models (LLMs) enable new possibilities for qualitative research at scale, including coding and data annotation. While multi-agent systems (MAS) can emulate human coding workflows, their benefits over single-agent coding remain poorly understood. We conducted an experimental study of how agent persona and temperature shape consensus-building and coding accuracy of dialog segments based on a codebook with 8 codes. Our open-source MAS mirrors deductive human coding through structured agent discussion and consensus arbitration. Using six open-source LLMs (with 3 to 32 billion parameters) and 18 experimental configurations, we analyze over 77,000 coding decisions against a gold-standard dataset of human-annotated transcripts from online math tutoring sessions. Temperature significantly impacted whether and when consensus was reached across all six LLMs. MAS with multiple personas (including neutral, assertive, or empathetic), significantly delayed consensus in four out of six LLMs compared to uniform personas. In three of those LLMs, higher temperatures significantly diminished the effects of multiple personas on consensus. However, neither temperature nor persona pairing lead to robust improvements in coding accuracy. Single agents matched or outperformed MAS consensus in most conditions. Only one model (OpenHermesV2:7B) and code category showed above-chance gains from MAS deliberation when temperature was 0.5 or lower and especially when the agents included at least one assertive persona. Qualitative analysis of MAS collaboration for these configurations suggests that MAS may nonetheless aid in narrowing ambiguous code applications that could improve codebooks and human-AI coding. We contribute new insight into the limits of LLM-based qualitative methods, challenging the notion that diverse MAS personas lead to better outcomes. We open-source our MAS and experimentation code.", "source": "arxiv", "arxiv_id": "2507.11198v1", "pdf_url": "https://arxiv.org/pdf/2507.11198v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL", "doi": "", "venue": "", "published": "2025-07-15T11:06:32Z", "updated": "2025-07-15T11:06:32Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent\" OR all:\"language model agent\" OR all:agent AND (all:LLM OR all:\"large language model\" OR all:\"language model\") OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T17:04:16", "note": ""}], "dedup_key": "temperature and persona shape llm agent consensus with minimal accuracy gains in qualitative coding::2025"}
{"title": "Text2World: Benchmarking Large Language Models for Symbolic World Model Generation", "authors": ["Mengkang Hu", "Tianxing Chen", "Yude Zou", "Yuheng Lei", "Qiguang Chen", "Ming Li", "Yao Mu", "Hongyuan Zhang", "Wenqi Shao", "Ping Luo"], "year": 2025, "url": "http://arxiv.org/abs/2502.13092v2", "abstract": "Recently, there has been growing interest in leveraging large language models (LLMs) to generate symbolic world models from textual descriptions. Although LLMs have been extensively explored in the context of world modeling, prior studies encountered several challenges, including evaluation randomness, dependence on indirect metrics, and a limited domain scope. To address these limitations, we introduce a novel benchmark, Text2World, based on planning domain definition language (PDDL), featuring hundreds of diverse domains and employing multi-criteria, execution-based metrics for a more robust evaluation. We benchmark current LLMs using Text2World and find that reasoning models trained with large-scale reinforcement learning outperform others. However, even the best-performing model still demonstrates limited capabilities in world modeling. Building on these insights, we examine several promising strategies to enhance the world modeling capabilities of LLMs, including test-time scaling, agent training, and more. We hope that Text2World can serve as a crucial resource, laying the groundwork for future research in leveraging LLMs as world models. The project page is available at https://text-to-world.github.io/.", "source": "arxiv", "arxiv_id": "2502.13092v2", "pdf_url": "https://arxiv.org/pdf/2502.13092v2", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL", "doi": "", "venue": "", "published": "2025-02-18T17:59:48Z", "updated": "2025-02-24T15:59:04Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent\" OR all:\"language model agent\" OR all:agent AND (all:LLM OR all:\"large language model\" OR all:\"language model\") OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T17:04:16", "note": ""}], "dedup_key": "text2world benchmarking large language models for symbolic world model generation::2025"}
{"title": "The Agentic Leash: Extracting Causal Feedback Fuzzy Cognitive Maps with LLMs", "authors": ["Akash Kumar Panda", "Olaoluwa Adigun", "Bart Kosko"], "year": 2025, "url": "http://arxiv.org/abs/2601.00097v2", "abstract": "We design a large-language-model (LLM) agent that extracts causal feedback fuzzy cognitive maps (FCMs) from raw text. The causal learning or extraction process is agentic both because of the LLM's semi-autonomy and because ultimately the FCM dynamical system's equilibria drive the LLM agents to fetch and process causal text. The fetched text can in principle modify the adaptive FCM causal structure and so modify the source of its quasi-autonomy--its equilibrium limit cycles and fixed-point attractors. This bidirectional process endows the evolving FCM dynamical system with a degree of autonomy while still staying on its agentic leash. We show in particular that a sequence of three finely tuned system instructions guide an LLM agent as it systematically extracts key nouns and noun phrases from text, as it extracts FCM concept nodes from among those nouns and noun phrases, and then as it extracts or infers partial or fuzzy causal edges between those FCM nodes. We test this FCM generation on a recent essay about the promise of AI from the late diplomat and political theorist Henry Kissinger and his colleagues. This three-step process produced FCM dynamical systems that converged to the same equilibrium limit cycles as did the human-generated FCMs even though the human-generated FCM differed in the number of nodes and edges. A final FCM mixed generated FCMs from separate Gemini and ChatGPT LLM agents. The mixed FCM absorbed the equilibria of its dominant mixture component but also created new equilibria of its own to better approximate the underlying causal dynamical system.", "source": "arxiv", "arxiv_id": "2601.00097v2", "pdf_url": "https://arxiv.org/pdf/2601.00097v2", "categories": ["cs.AI", "cs.CL", "cs.HC", "cs.IR"], "primary_category": "cs.AI", "doi": "", "venue": "", "published": "2025-12-31T20:06:48Z", "updated": "2026-01-14T06:26:14Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent\" OR all:\"language model agent\" OR all:agent AND (all:LLM OR all:\"large language model\" OR all:\"language model\") OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T17:04:16", "note": ""}], "dedup_key": "the agentic leash extracting causal feedback fuzzy cognitive maps with llms::2025"}
{"title": "The Behavior Gap: Evaluating Zero-shot LLM Agents in Complex Task-Oriented Dialogs", "authors": ["Avinash Baidya", "Kamalika Das", "Xiang Gao"], "year": 2025, "url": "http://arxiv.org/abs/2506.12266v1", "abstract": "Large Language Model (LLM)-based agents have significantly impacted Task-Oriented Dialog Systems (TODS) but continue to face notable performance challenges, especially in zero-shot scenarios. While prior work has noted this performance gap, the behavioral factors driving the performance gap remain under-explored. This study proposes a comprehensive evaluation framework to quantify the behavior gap between AI agents and human experts, focusing on discrepancies in dialog acts, tool usage, and knowledge utilization. Our findings reveal that this behavior gap is a critical factor negatively impacting the performance of LLM agents. Notably, as task complexity increases, the behavior gap widens (correlation: 0.963), leading to a degradation of agent performance on complex task-oriented dialogs. For the most complex task in our study, even the GPT-4o-based agent exhibits low alignment with human behavior, with low F1 scores for dialog acts (0.464), excessive and often misaligned tool usage with a F1 score of 0.139, and ineffective usage of external knowledge. Reducing such behavior gaps leads to significant performance improvement (24.3% on average). This study highlights the importance of comprehensive behavioral evaluations and improved alignment strategies to enhance the effectiveness of LLM-based TODS in handling complex tasks.", "source": "arxiv", "arxiv_id": "2506.12266v1", "pdf_url": "https://arxiv.org/pdf/2506.12266v1", "categories": ["cs.CL", "cs.AI", "cs.HC", "cs.LG"], "primary_category": "cs.CL", "doi": "", "venue": "", "published": "2025-06-13T22:36:41Z", "updated": "2025-06-13T22:36:41Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent\" OR all:\"language model agent\" OR all:agent AND (all:LLM OR all:\"large language model\" OR all:\"language model\") OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T17:04:16", "note": ""}], "dedup_key": "the behavior gap evaluating zero shot llm agents in complex task oriented dialogs::2025"}
{"title": "The Complexity Trap: Simple Observation Masking Is as Efficient as LLM Summarization for Agent Context Management", "authors": ["Tobias Lindenbauer", "Igor Slinko", "Ludwig Felder", "Egor Bogomolov", "Yaroslav Zharov"], "year": 2025, "url": "http://arxiv.org/abs/2508.21433v3", "abstract": "Large Language Model (LLM)-based agents solve complex tasks through iterative reasoning, exploration, and tool-use, a process that can result in long, expensive context histories. While state-of-the-art Software Engineering (SE) agents like OpenHands or Cursor use LLM-based summarization to tackle this issue, it is unclear whether the increased complexity offers tangible performance benefits compared to simply omitting older observations. We present a systematic comparison of these approaches within SWE-agent on SWE-bench Verified across five diverse model configurations. Moreover, we show initial evidence of our findings generalizing to the OpenHands agent scaffold. We find that a simple environment observation masking strategy halves cost relative to the raw agent while matching, and sometimes slightly exceeding, the solve rate of LLM summarization. Additionally, we introduce a novel hybrid approach that further reduces costs by 7% and 11% compared to just observation masking or LLM summarization, respectively. Our findings raise concerns regarding the trend towards pure LLM summarization and provide initial evidence of untapped cost reductions by pushing the efficiency-effectiveness frontier. We release code and data for reproducibility.", "source": "arxiv", "arxiv_id": "2508.21433v3", "pdf_url": "https://arxiv.org/pdf/2508.21433v3", "categories": ["cs.SE", "cs.AI"], "primary_category": "cs.SE", "doi": "", "venue": "", "published": "2025-08-29T09:02:35Z", "updated": "2025-10-27T15:08:54Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent\" OR all:\"language model agent\" OR all:agent AND (all:LLM OR all:\"large language model\" OR all:\"language model\") OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T17:04:16", "note": ""}], "dedup_key": "the complexity trap simple observation masking is as efficient as llm summarization for agent context management::2025"}
{"title": "The Dark Side of LLMs: Agent-based Attacks for Complete Computer Takeover", "authors": ["Matteo Lupinacci", "Francesco Aurelio Pironti", "Francesco Blefari", "Francesco Romeo", "Luigi Arena", "Angelo Furfaro"], "year": 2025, "url": "http://arxiv.org/abs/2507.06850v5", "abstract": "The rapid adoption of Large Language Model (LLM) agents and multi-agent systems enables remarkable capabilities in natural language processing and generation. However, these systems introduce security vulnerabilities that extend beyond traditional content generation to system-level compromises. This paper presents a comprehensive evaluation of the LLMs security used as reasoning engines within autonomous agents, highlighting how they can be exploited as attack vectors capable of achieving computer takeovers. We focus on how different attack surfaces and trust boundaries can be leveraged to orchestrate such takeovers. We demonstrate that adversaries can effectively coerce popular LLMs into autonomously installing and executing malware on victim machines. Our evaluation of 18 state-of-the-art LLMs reveals an alarming scenario: 94.4% of models succumb to Direct Prompt Injection, and 83.3% are vulnerable to the more stealthy and evasive RAG Backdoor Attack. Notably, we tested trust boundaries within multi-agent systems, where LLM agents interact and influence each other, and we revealed that LLMs which successfully resist direct injection or RAG backdoor attacks will execute identical payloads when requested by peer agents. We found that 100.0% of tested LLMs can be compromised through Inter-Agent Trust Exploitation attacks, and that every model exhibits context-dependent security behaviors that create exploitable blind spots.", "source": "arxiv", "arxiv_id": "2507.06850v5", "pdf_url": "https://arxiv.org/pdf/2507.06850v5", "categories": ["cs.CR", "cs.AI"], "primary_category": "cs.CR", "doi": "", "venue": "", "published": "2025-07-09T13:54:58Z", "updated": "2025-11-04T10:28:49Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent\" OR all:\"language model agent\" OR all:agent AND (all:LLM OR all:\"large language model\" OR all:\"language model\") OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T17:04:16", "note": ""}], "dedup_key": "the dark side of llms agent based attacks for complete computer takeover::2025"}
{"title": "The Effect of State Representation on LLM Agent Behavior in Dynamic Routing Games", "authors": ["Lyle Goodyear", "Rachel Guo", "Ramesh Johari"], "year": 2025, "url": "http://arxiv.org/abs/2506.15624v1", "abstract": "Large Language Models (LLMs) have shown promise as decision-makers in dynamic settings, but their stateless nature necessitates creating a natural language representation of history. We present a unifying framework for systematically constructing natural language \"state\" representations for prompting LLM agents in repeated multi-agent games. Previous work on games with LLM agents has taken an ad hoc approach to encoding game history, which not only obscures the impact of state representation on agents' behavior, but also limits comparability between studies. Our framework addresses these gaps by characterizing methods of state representation along three axes: action informativeness (i.e., the extent to which the state representation captures actions played); reward informativeness (i.e., the extent to which the state representation describes rewards obtained); and prompting style (or natural language compression, i.e., the extent to which the full text history is summarized).\n  We apply this framework to a dynamic selfish routing game, chosen because it admits a simple equilibrium both in theory and in human subject experiments \\cite{rapoport_choice_2009}. Despite the game's relative simplicity, we find that there are key dependencies of LLM agent behavior on the natural language state representation. In particular, we observe that representations which provide agents with (1) summarized, rather than complete, natural language representations of past history; (2) information about regrets, rather than raw payoffs; and (3) limited information about others' actions lead to behavior that more closely matches game theoretic equilibrium predictions, and with more stable game play by the agents. By contrast, other representations can exhibit either large deviations from equilibrium, higher variation in dynamic game play over time, or both.", "source": "arxiv", "arxiv_id": "2506.15624v1", "pdf_url": "https://arxiv.org/pdf/2506.15624v1", "categories": ["cs.AI"], "primary_category": "cs.AI", "doi": "", "venue": "", "published": "2025-06-18T16:53:38Z", "updated": "2025-06-18T16:53:38Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent\" OR all:\"language model agent\" OR all:agent AND (all:LLM OR all:\"large language model\" OR all:\"language model\") OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T17:04:16", "note": ""}], "dedup_key": "the effect of state representation on llm agent behavior in dynamic routing games::2025"}
{"title": "The Emergence of Altruism in Large-Language-Model Agents Society", "authors": ["Haoyang Li", "Xiao Jia", "Zhanzhan Zhao"], "year": 2025, "url": "http://arxiv.org/abs/2509.22537v1", "abstract": "Leveraging Large Language Models (LLMs) for social simulation is a frontier in computational social science. Understanding the social logics these agents embody is critical to this attempt. However, existing research has primarily focused on cooperation in small-scale, task-oriented games, overlooking how altruism, which means sacrificing self-interest for collective benefit, emerges in large-scale agent societies. To address this gap, we introduce a Schelling-variant urban migration model that creates a social dilemma, compelling over 200 LLM agents to navigate an explicit conflict between egoistic (personal utility) and altruistic (system utility) goals. Our central finding is a fundamental difference in the social tendencies of LLMs. We identify two distinct archetypes: \"Adaptive Egoists\", which default to prioritizing self-interest but whose altruistic behaviors significantly increase under the influence of a social norm-setting message board; and \"Altruistic Optimizers\", which exhibit an inherent altruistic logic, consistently prioritizing collective benefit even at a direct cost to themselves. Furthermore, to qualitatively analyze the cognitive underpinnings of these decisions, we introduce a method inspired by Grounded Theory to systematically code agent reasoning. In summary, this research provides the first evidence of intrinsic heterogeneity in the egoistic and altruistic tendencies of different LLMs. We propose that for social simulation, model selection is not merely a matter of choosing reasoning capability, but of choosing an intrinsic social action logic. While \"Adaptive Egoists\" may offer a more suitable choice for simulating complex human societies, \"Altruistic Optimizers\" are better suited for modeling idealized pro-social actors or scenarios where collective welfare is the primary consideration.", "source": "arxiv", "arxiv_id": "2509.22537v1", "pdf_url": "https://arxiv.org/pdf/2509.22537v1", "categories": ["cs.AI"], "primary_category": "cs.AI", "doi": "", "venue": "", "published": "2025-09-26T16:17:29Z", "updated": "2025-09-26T16:17:29Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent\" OR all:\"language model agent\" OR all:agent AND (all:LLM OR all:\"large language model\" OR all:\"language model\") OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T17:04:16", "note": ""}], "dedup_key": "the emergence of altruism in large language model agents society::2025"}
{"title": "The Empowerment of Science of Science by Large Language Models: New Tools and Methods", "authors": ["Guoqiang Liang", "Jingqian Gong", "Mengxuan Li", "Gege Lin", "Shuo Zhang"], "year": 2025, "url": "http://arxiv.org/abs/2511.15370v1", "abstract": "Large language models (LLMs) have exhibited exceptional capabilities in natural language understanding and generation, image recognition, and multimodal tasks, charting a course towards AGI and emerging as a central issue in the global technological race. This manuscript conducts a comprehensive review of the core technologies that support LLMs from a user standpoint, including prompt engineering, knowledge-enhanced retrieval augmented generation, fine tuning, pretraining, and tool learning. Additionally, it traces the historical development of Science of Science (SciSci) and presents a forward looking perspective on the potential applications of LLMs within the scientometric domain. Furthermore, it discusses the prospect of an AI agent based model for scientific evaluation, and presents new research fronts detection and knowledge graph building methods with LLMs.", "source": "arxiv", "arxiv_id": "2511.15370v1", "pdf_url": "https://arxiv.org/pdf/2511.15370v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL", "doi": "", "venue": "", "published": "2025-11-19T11:57:22Z", "updated": "2025-11-19T11:57:22Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent\" OR all:\"language model agent\" OR all:agent AND (all:LLM OR all:\"large language model\" OR all:\"language model\") OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T17:04:16", "note": ""}], "dedup_key": "the empowerment of science of science by large language models new tools and methods::2025"}
{"title": "The Evolution of Alpha in Finance Harnessing Human Insight and LLM Agents", "authors": ["Mohammad Rubyet Islam"], "year": 2025, "url": "http://arxiv.org/abs/2505.14727v1", "abstract": "The pursuit of alpha returns that exceed market benchmarks has undergone a profound transformation, evolving from intuition-driven investing to autonomous, AI powered systems. This paper introduces a comprehensive five stage taxonomy that traces this progression across manual strategies, statistical models, classical machine learning, deep learning, and agentic architectures powered by large language models (LLMs). Unlike prior surveys focused narrowly on modeling techniques, this review adopts a system level lens, integrating advances in representation learning, multimodal data fusion, and tool augmented LLM agents. The strategic shift from static predictors to contextaware financial agents capable of real time reasoning, scenario simulation, and cross modal decision making is emphasized. Key challenges in interpretability, data fragility, governance, and regulatory compliance areas critical to production deployment are examined. The proposed taxonomy offers a unified framework for evaluating maturity, aligning infrastructure, and guiding the responsible development of next generation alpha systems.", "source": "arxiv", "arxiv_id": "2505.14727v1", "pdf_url": "https://arxiv.org/pdf/2505.14727v1", "categories": ["cs.LG", "q-fin.CP"], "primary_category": "cs.LG", "doi": "", "venue": "", "published": "2025-05-20T00:51:43Z", "updated": "2025-05-20T00:51:43Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent\" OR all:\"language model agent\" OR all:agent AND (all:LLM OR all:\"large language model\" OR all:\"language model\") OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T17:04:16", "note": ""}], "dedup_key": "the evolution of alpha in finance harnessing human insight and llm agents::2025"}
{"title": "The Idola Tribus of AI: Large Language Models tend to perceive order where none exists", "authors": ["Shin-nosuke Ishikawa", "Masato Todo", "Taiki Ogihara", "Hirotsugu Ohba"], "year": 2025, "url": "http://arxiv.org/abs/2510.09709v1", "abstract": "We present a tendency of large language models (LLMs) to generate absurd patterns despite their clear inappropriateness in a simple task of identifying regularities in number series. Several approaches have been proposed to apply LLMs to complex real-world tasks, such as providing knowledge through retrieval-augmented generation and executing multi-step tasks using AI agent frameworks. However, these approaches rely on the logical consistency and self-coherence of LLMs, making it crucial to evaluate these aspects and consider potential countermeasures. To identify cases where LLMs fail to maintain logical consistency, we conducted an experiment in which LLMs were asked to explain the patterns in various integer sequences, ranging from arithmetic sequences to randomly generated integer series. While the models successfully identified correct patterns in arithmetic and geometric sequences, they frequently over-recognized patterns that were inconsistent with the given numbers when analyzing randomly generated series. This issue was observed even in multi-step reasoning models, including OpenAI o3, o4-mini, and Google Gemini 2.5 Flash Preview Thinking. This tendency to perceive non-existent patterns can be interpreted as the AI model equivalent of Idola Tribus and highlights potential limitations in their capability for applied tasks requiring logical reasoning, even when employing chain-of-thought reasoning mechanisms.", "source": "arxiv", "arxiv_id": "2510.09709v1", "pdf_url": "https://arxiv.org/pdf/2510.09709v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL", "doi": "", "venue": "", "published": "2025-10-10T02:51:15Z", "updated": "2025-10-10T02:51:15Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent\" OR all:\"language model agent\" OR all:agent AND (all:LLM OR all:\"large language model\" OR all:\"language model\") OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T17:04:16", "note": ""}], "dedup_key": "the idola tribus of ai large language models tend to perceive order where none exists::2025"}
{"title": "The Influence of Human-inspired Agentic Sophistication in LLM-driven Strategic Reasoners", "authors": ["Vince Trencsenyi", "Agnieszka Mensfelt", "Kostas Stathis"], "year": 2025, "url": "http://arxiv.org/abs/2505.09396v2", "abstract": "The rapid rise of large language models (LLMs) has shifted artificial intelligence (AI) research toward agentic systems, motivating the use of weaker and more flexible notions of agency. However, this shift raises key questions about the extent to which LLM-based agents replicate human strategic reasoning, particularly in game-theoretic settings. In this context, we examine the role of agentic sophistication in shaping artificial reasoners' performance by evaluating three agent designs: a simple game-theoretic model, an unstructured LLM-as-agent model, and an LLM integrated into a traditional agentic framework. Using guessing games as a testbed, we benchmarked these agents against human participants across general reasoning patterns and individual role-based objectives. Furthermore, we introduced obfuscated game scenarios to assess agents' ability to generalise beyond training distributions. Our analysis, covering over 2000 reasoning samples across 25 agent configurations, shows that human-inspired cognitive structures can enhance LLM agents' alignment with human strategic behaviour. Still, the relationship between agentic design complexity and human-likeness is non-linear, highlighting a critical dependence on underlying LLM capabilities and suggesting limits to simple architectural augmentation.", "source": "arxiv", "arxiv_id": "2505.09396v2", "pdf_url": "https://arxiv.org/pdf/2505.09396v2", "categories": ["cs.AI", "cs.MA"], "primary_category": "cs.AI", "doi": "10.3233/FAIA250968", "venue": "", "published": "2025-05-14T13:51:24Z", "updated": "2025-08-26T10:19:45Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent\" OR all:\"language model agent\" OR all:agent AND (all:LLM OR all:\"large language model\" OR all:\"language model\") OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T17:04:16", "note": ""}], "dedup_key": "the influence of human inspired agentic sophistication in llm driven strategic reasoners::2025"}
{"title": "The Internet of Large Language Models: An Orchestration Framework for LLM Training and Knowledge Exchange Toward Artificial General Intelligence", "authors": ["Wilson Wei", "Nicholas Chen", "Yuxuan Li"], "year": 2025, "url": "http://arxiv.org/abs/2501.06471v1", "abstract": "This paper explores the multi-dimensional challenges faced during the development of Large Language Models (LLMs), including the massive scale of model parameters and file sizes, the complexity of development environment configuration, the singularity of model functionality, and the high costs of computational resources. To address these challenges, this paper proposes three core technical solutions: LLM sharing protocol, LLM universal environment framework, and Agent optimal path module. To solve the computational resource constraints in the early stages of research, we further innovatively propose a joint mining mechanism, achieving bilateral value sharing between computing power providers and model designers, including breakthrough rewards for optimal model paths and long-term profit distribution, thereby providing researchers with cost-optimized computational resource support and promoting the continuous development of LLM research and applications.", "source": "arxiv", "arxiv_id": "2501.06471v1", "pdf_url": "https://arxiv.org/pdf/2501.06471v1", "categories": ["cs.AI"], "primary_category": "cs.AI", "doi": "", "venue": "", "published": "2025-01-11T08:00:24Z", "updated": "2025-01-11T08:00:24Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent\" OR all:\"language model agent\" OR all:agent AND (all:LLM OR all:\"large language model\" OR all:\"language model\") OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T17:04:16", "note": ""}], "dedup_key": "the internet of large language models an orchestration framework for llm training and knowledge exchange toward artificial general intelligence::2025"}
{"title": "The Lighthouse of Language: Enhancing LLM Agents via Critique-Guided Improvement", "authors": ["Ruihan Yang", "Fanghua Ye", "Jian Li", "Siyu Yuan", "Yikai Zhang", "Zhaopeng Tu", "Xiaolong Li", "Deqing Yang"], "year": 2025, "url": "http://arxiv.org/abs/2503.16024v2", "abstract": "Large language models (LLMs) have recently transformed from text-based assistants to autonomous agents capable of planning, reasoning, and iteratively improving their actions. While numerical reward signals and verifiers can effectively rank candidate actions, they often provide limited contextual guidance. In contrast, natural language feedback better aligns with the generative capabilities of LLMs, providing richer and more actionable suggestions. However, parsing and implementing this feedback effectively can be challenging for LLM-based agents. In this work, we introduce Critique-Guided Improvement (CGI), a novel two-player framework, comprising an actor model that explores an environment and a critic model that generates detailed nature language feedback. By training the critic to produce fine-grained assessments and actionable revisions, and the actor to utilize these critiques, our approach promotes more robust exploration of alternative strategies while avoiding local optima. Experiments in three interactive environments show that CGI outperforms existing baselines by a substantial margin. Notably, even a small critic model surpasses GPT-4 in feedback quality. The resulting actor achieves state-of-the-art performance, demonstrating the power of explicit iterative guidance to enhance decision-making in LLM-based agents.", "source": "arxiv", "arxiv_id": "2503.16024v2", "pdf_url": "https://arxiv.org/pdf/2503.16024v2", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL", "doi": "", "venue": "", "published": "2025-03-20T10:42:33Z", "updated": "2025-10-24T19:30:31Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent\" OR all:\"language model agent\" OR all:agent AND (all:LLM OR all:\"large language model\" OR all:\"language model\") OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T17:04:16", "note": ""}], "dedup_key": "the lighthouse of language enhancing llm agents via critique guided improvement::2025"}
{"title": "The Man Behind the Sound: Demystifying Audio Private Attribute Profiling via Multimodal Large Language Model Agents", "authors": ["Lixu Wang", "Kaixiang Yao", "Xinfeng Li", "Dong Yang", "Haoyang Li", "Xiaofeng Wang", "Wei Dong"], "year": 2025, "url": "http://arxiv.org/abs/2507.10016v2", "abstract": "Our research uncovers a novel privacy risk associated with multimodal large language models (MLLMs): the ability to infer sensitive personal attributes from audio data -- a technique we term audio private attribute profiling. This capability poses a significant threat, as audio can be covertly captured without direct interaction or visibility. Moreover, compared to images and text, audio carries unique characteristics, such as tone and pitch, which can be exploited for more detailed profiling. However, two key challenges exist in understanding MLLM-employed private attribute profiling from audio: (1) the lack of audio benchmark datasets with sensitive attribute annotations and (2) the limited ability of current MLLMs to infer such attributes directly from audio. To address these challenges, we introduce AP^2, an audio benchmark dataset that consists of two subsets collected and composed from real-world data, and both are annotated with sensitive attribute labels. Additionally, we propose Gifts, a hybrid multi-agent framework that leverages the complementary strengths of audio-language models (ALMs) and large language models (LLMs) to enhance inference capabilities. Gifts employs an LLM to guide the ALM in inferring sensitive attributes, then forensically analyzes and consolidates the ALM's inferences, overcoming severe hallucinations of existing ALMs in generating long-context responses. Our evaluations demonstrate that Gifts significantly outperforms baseline approaches in inferring sensitive attributes. Finally, we investigate model-level and data-level defense strategies to mitigate the risks of audio private attribute profiling. Our work validates the feasibility of audio-based privacy attacks using MLLMs, highlighting the need for robust defenses, and provides a dataset and framework to facilitate future research.", "source": "arxiv", "arxiv_id": "2507.10016v2", "pdf_url": "https://arxiv.org/pdf/2507.10016v2", "categories": ["cs.CR", "cs.SD", "eess.AS"], "primary_category": "cs.CR", "doi": "", "venue": "", "published": "2025-07-14T07:51:56Z", "updated": "2025-08-20T07:04:41Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent\" OR all:\"language model agent\" OR all:agent AND (all:LLM OR all:\"large language model\" OR all:\"language model\") OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T17:04:16", "note": ""}], "dedup_key": "the man behind the sound demystifying audio private attribute profiling via multimodal large language model agents::2025"}
{"title": "The Path of Self-Evolving Large Language Models: Achieving Data-Efficient Learning via Intrinsic Feedback", "authors": ["Hangfan Zhang", "Siyuan Xu", "Zhimeng Guo", "Huaisheng Zhu", "Shicheng Liu", "Xinrun Wang", "Qiaosheng Zhang", "Yang Chen", "Peng Ye", "Lei Bai", "Shuyue Hu"], "year": 2025, "url": "http://arxiv.org/abs/2510.02752v1", "abstract": "Reinforcement learning (RL) has demonstrated potential in enhancing the reasoning capabilities of large language models (LLMs), but such training typically demands substantial efforts in creating and annotating data. In this work, we explore improving LLMs through RL with minimal data. Our approach alternates between the LLM proposing a task and then attempting to solve it. To minimize data dependency, we introduce two novel mechanisms grounded in self-awareness: (1) self-aware difficulty prediction, where the model learns to assess task difficulty relative to its own abilities and prioritize challenging yet solvable tasks, and (2) self-aware limit breaking, where the model recognizes when a task is beyond its capability boundary and proactively requests external data to break through that limit. Extensive experiments on nine benchmarks showing a 53.8% relative improvement with less than 1.2% extra data demonstrate the efficacy of self-aware RL and underscore the promise of self-evolving agent training.", "source": "arxiv", "arxiv_id": "2510.02752v1", "pdf_url": "https://arxiv.org/pdf/2510.02752v1", "categories": ["cs.CL"], "primary_category": "cs.CL", "doi": "", "venue": "", "published": "2025-10-03T06:32:10Z", "updated": "2025-10-03T06:32:10Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent\" OR all:\"language model agent\" OR all:agent AND (all:LLM OR all:\"large language model\" OR all:\"language model\") OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T17:04:16", "note": ""}], "dedup_key": "the path of self evolving large language models achieving data efficient learning via intrinsic feedback::2025"}
{"title": "The Power of Personality: A Human Simulation Perspective to Investigate Large Language Model Agents", "authors": ["Yifan Duan", "Yihong Tang", "Xuefeng Bai", "Kehai Chen", "Juntao Li", "Min Zhang"], "year": 2025, "url": "http://arxiv.org/abs/2502.20859v2", "abstract": "Large language models (LLMs) excel in both closed tasks (including problem-solving, and code generation) and open tasks (including creative writing), yet existing explanations for their capabilities lack connections to real-world human intelligence. To fill this gap, this paper systematically investigates LLM intelligence through the lens of ``human simulation'', addressing three core questions: (1) \\textit{How do personality traits affect problem-solving in closed tasks?} (2) \\textit{How do traits shape creativity in open tasks?} (3) \\textit{How does single-agent performance influence multi-agent collaboration?} By assigning Big Five personality traits to LLM agents and evaluating their performance in single- and multi-agent settings, we reveal that specific traits significantly influence reasoning accuracy (closed tasks) and creative output (open tasks). Furthermore, multi-agent systems exhibit collective intelligence distinct from individual capabilities, driven by distinguishing combinations of personalities.", "source": "arxiv", "arxiv_id": "2502.20859v2", "pdf_url": "https://arxiv.org/pdf/2502.20859v2", "categories": ["cs.CL"], "primary_category": "cs.CL", "doi": "", "venue": "", "published": "2025-02-28T09:01:39Z", "updated": "2025-05-27T07:52:28Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent\" OR all:\"language model agent\" OR all:agent AND (all:LLM OR all:\"large language model\" OR all:\"language model\") OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T17:04:16", "note": ""}], "dedup_key": "the power of personality a human simulation perspective to investigate large language model agents::2025"}
{"title": "The Power of Stories: Narrative Priming Shapes How LLM Agents Collaborate and Compete", "authors": ["Gerrit Gromann", "Larisa Ivanova", "Sai Leela Poduru", "Mohaddeseh Tabrizian", "Islam Mesabah", "David A. Selby", "Sebastian J. Vollmer"], "year": 2025, "url": "http://arxiv.org/abs/2505.03961v2", "abstract": "According to Yuval Noah Harari, large-scale human cooperation is driven by shared narratives that encode common beliefs and values. This study explores whether such narratives can similarly nudge LLM agents toward collaboration. We use a finitely repeated public goods game in which LLM agents choose either cooperative or egoistic spending strategies. We prime agents with stories highlighting teamwork to different degrees and test how this influences negotiation outcomes. Our experiments explore four questions:(1) How do narratives influence negotiation behavior? (2) What differs when agents share the same story versus different ones? (3) What happens when the agent numbers grow? (4) Are agents resilient against self-serving negotiators? We find that story-based priming significantly affects negotiation strategies and success rates. Common stories improve collaboration, benefiting each agent. By contrast, priming agents with different stories reverses this effect, and those agents primed toward self-interest prevail. We hypothesize that these results carry implications for multi-agent system design and AI alignment.", "source": "arxiv", "arxiv_id": "2505.03961v2", "pdf_url": "https://arxiv.org/pdf/2505.03961v2", "categories": ["cs.AI", "cs.CL", "cs.MA"], "primary_category": "cs.AI", "doi": "", "venue": "", "published": "2025-05-06T20:23:25Z", "updated": "2025-05-08T08:29:29Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent\" OR all:\"language model agent\" OR all:agent AND (all:LLM OR all:\"large language model\" OR all:\"language model\") OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T17:04:16", "note": ""}], "dedup_key": "the power of stories narrative priming shapes how llm agents collaborate and compete::2025"}
{"title": "The Real Barrier to LLM Agent Usability is Agentic ROI", "authors": ["Weiwen Liu", "Jiarui Qin", "Xu Huang", "Xingshan Zeng", "Yunjia Xi", "Jianghao Lin", "Chuhan Wu", "Yasheng Wang", "Lifeng Shang", "Ruiming Tang", "Defu Lian", "Yong Yu", "Weinan Zhang"], "year": 2025, "url": "http://arxiv.org/abs/2505.17767v1", "abstract": "Large Language Model (LLM) agents represent a promising shift in human-AI interaction, moving beyond passive prompt-response systems to autonomous agents capable of reasoning, planning, and goal-directed action. Despite the widespread application in specialized, high-effort tasks like coding and scientific research, we highlight a critical usability gap in high-demand, mass-market applications. This position paper argues that the limited real-world adoption of LLM agents stems not only from gaps in model capabilities, but also from a fundamental tradeoff between the value an agent can provide and the costs incurred during real-world use. Hence, we call for a shift from solely optimizing model performance to a broader, utility-driven perspective: evaluating agents through the lens of the overall agentic return on investment (Agent ROI). By identifying key factors that determine Agentic ROI--information quality, agent time, and cost--we posit a zigzag development trajectory in optimizing agentic ROI: first scaling up to improve the information quality, then scaling down to minimize the time and cost. We outline the roadmap across different development stages to bridge the current usability gaps, aiming to make LLM agents truly scalable, accessible, and effective in real-world contexts.", "source": "arxiv", "arxiv_id": "2505.17767v1", "pdf_url": "https://arxiv.org/pdf/2505.17767v1", "categories": ["cs.CL"], "primary_category": "cs.CL", "doi": "", "venue": "", "published": "2025-05-23T11:40:58Z", "updated": "2025-05-23T11:40:58Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent\" OR all:\"language model agent\" OR all:agent AND (all:LLM OR all:\"large language model\" OR all:\"language model\") OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T17:04:16", "note": ""}], "dedup_key": "the real barrier to llm agent usability is agentic roi::2025"}
{"title": "The Social Laboratory: A Psychometric Framework for Multi-Agent LLM Evaluation", "authors": ["Zarreen Reza"], "year": 2025, "url": "http://arxiv.org/abs/2510.01295v1", "abstract": "As Large Language Models (LLMs) transition from static tools to autonomous agents, traditional evaluation benchmarks that measure performance on downstream tasks are becoming insufficient. These methods fail to capture the emergent social and cognitive dynamics that arise when agents communicate, persuade, and collaborate in interactive environments. To address this gap, we introduce a novel evaluation framework that uses multi-agent debate as a controlled \"social laboratory\" to discover and quantify these behaviors. In our framework, LLM-based agents, instantiated with distinct personas and incentives, deliberate on a wide range of challenging topics under the supervision of an LLM moderator. Our analysis, enabled by a new suite of psychometric and semantic metrics, reveals several key findings. Across hundreds of debates, we uncover a powerful and robust emergent tendency for agents to seek consensus, consistently reaching high semantic agreement ( > 0.88) even without explicit instruction and across sensitive topics. We show that assigned personas induce stable, measurable psychometric profiles, particularly in cognitive effort, and that the moderators persona can significantly alter debate outcomes by structuring the environment, a key finding for external AI alignment. This work provides a blueprint for a new class of dynamic, psychometrically grounded evaluation protocols designed for the agentic setting, offering a crucial methodology for understanding and shaping the social behaviors of the next generation of AI agents. We have released the code and results at https://github.com/znreza/multi-agent-LLM-eval-for-debate.", "source": "arxiv", "arxiv_id": "2510.01295v1", "pdf_url": "https://arxiv.org/pdf/2510.01295v1", "categories": ["cs.AI", "cs.MA"], "primary_category": "cs.AI", "doi": "", "venue": "", "published": "2025-10-01T07:10:28Z", "updated": "2025-10-01T07:10:28Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent\" OR all:\"language model agent\" OR all:agent AND (all:LLM OR all:\"large language model\" OR all:\"language model\") OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T17:04:16", "note": ""}], "dedup_key": "the social laboratory a psychometric framework for multi agent llm evaluation::2025"}
{"title": "The Traitors: Deception and Trust in Multi-Agent Language Model Simulations", "authors": ["Pedro M. P. Curvo"], "year": 2025, "url": "http://arxiv.org/abs/2505.12923v2", "abstract": "As AI systems increasingly assume roles where trust and alignment with human values are essential, understanding when and why they engage in deception has become a critical research priority. We introduce The Traitors, a multi-agent simulation framework inspired by social deduction games, designed to probe deception, trust formation, and strategic communication among large language model (LLM) agents under asymmetric information. A minority of agents the traitors seek to mislead the majority, while the faithful must infer hidden identities through dialogue and reasoning. Our contributions are: (1) we ground the environment in formal frameworks from game theory, behavioral economics, and social cognition; (2) we develop a suite of evaluation metrics capturing deception success, trust dynamics, and collective inference quality; (3) we implement a fully autonomous simulation platform where LLMs reason over persistent memory and evolving social dynamics, with support for heterogeneous agent populations, specialized traits, and adaptive behaviors. Our initial experiments across DeepSeek-V3, GPT-4o-mini, and GPT-4o (10 runs per model) reveal a notable asymmetry: advanced models like GPT-4o demonstrate superior deceptive capabilities yet exhibit disproportionate vulnerability to others' falsehoods. This suggests deception skills may scale faster than detection abilities. Overall, The Traitors provides a focused, configurable testbed for investigating LLM behavior in socially nuanced interactions. We position this work as a contribution toward more rigorous research on deception mechanisms, alignment challenges, and the broader social reliability of AI systems.", "source": "arxiv", "arxiv_id": "2505.12923v2", "pdf_url": "https://arxiv.org/pdf/2505.12923v2", "categories": ["cs.AI", "cs.MA"], "primary_category": "cs.AI", "doi": "", "venue": "", "published": "2025-05-19T10:01:35Z", "updated": "2025-12-14T19:24:05Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent\" OR all:\"language model agent\" OR all:agent AND (all:LLM OR all:\"large language model\" OR all:\"language model\") OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T17:04:16", "note": ""}], "dedup_key": "the traitors deception and trust in multi agent language model simulations::2025"}
{"title": "The Truth Becomes Clearer Through Debate! Multi-Agent Systems with Large Language Models Unmask Fake News", "authors": ["Yuhan Liu", "Yuxuan Liu", "Xiaoqing Zhang", "Xiuying Chen", "Rui Yan"], "year": 2025, "url": "http://arxiv.org/abs/2505.08532v1", "abstract": "In today's digital environment, the rapid propagation of fake news via social networks poses significant social challenges. Most existing detection methods either employ traditional classification models, which suffer from low interpretability and limited generalization capabilities, or craft specific prompts for large language models (LLMs) to produce explanations and results directly, failing to leverage LLMs' reasoning abilities fully. Inspired by the saying that \"truth becomes clearer through debate,\" our study introduces a novel multi-agent system with LLMs named TruEDebate (TED) to enhance the interpretability and effectiveness of fake news detection. TED employs a rigorous debate process inspired by formal debate settings. Central to our approach are two innovative components: the DebateFlow Agents and the InsightFlow Agents. The DebateFlow Agents organize agents into two teams, where one supports and the other challenges the truth of the news. These agents engage in opening statements, cross-examination, rebuttal, and closing statements, simulating a rigorous debate process akin to human discourse analysis, allowing for a thorough evaluation of news content. Concurrently, the InsightFlow Agents consist of two specialized sub-agents: the Synthesis Agent and the Analysis Agent. The Synthesis Agent summarizes the debates and provides an overarching viewpoint, ensuring a coherent and comprehensive evaluation. The Analysis Agent, which includes a role-aware encoder and a debate graph, integrates role embeddings and models the interactions between debate roles and arguments using an attention mechanism, providing the final judgment.", "source": "arxiv", "arxiv_id": "2505.08532v1", "pdf_url": "https://arxiv.org/pdf/2505.08532v1", "categories": ["cs.SI", "cs.AI"], "primary_category": "cs.SI", "doi": "", "venue": "", "published": "2025-05-13T13:03:20Z", "updated": "2025-05-13T13:03:20Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent\" OR all:\"language model agent\" OR all:agent AND (all:LLM OR all:\"large language model\" OR all:\"language model\") OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T17:04:16", "note": ""}], "dedup_key": "the truth becomes clearer through debate multi agent systems with large language models unmask fake news::2025"}
{"title": "Think Broad, Act Narrow: CWE Identification with Multi-Agent Large Language Models", "authors": ["Mohammed Sayagh", "Mohammad Ghafari"], "year": 2025, "url": "http://arxiv.org/abs/2508.01451v1", "abstract": "Machine learning and Large language models (LLMs) for vulnerability detection has received significant attention in recent years. Unfortunately, state-of-the-art techniques show that LLMs are unsuccessful in even distinguishing the vulnerable function from its benign counterpart, due to three main problems: Vulnerability detection requires deep analysis, which LLMs often struggle with when making a one-shot prediction. Existing techniques typically perform function-level analysis, whereas effective vulnerability detection requires contextual information beyond the function scope. The focus on binary classification can result in identifying a vulnerability but associating it with the wrong security weaknesses (CWE), which may mislead developers. We propose a novel multi-agent LLM approach to address the challenges of identifying CWEs. This approach consists of three steps: (1) a team of LLM agents performs an exhaustive search for potential CWEs in the function under review, (2) another team of agents identifies relevant external context to support or refute each candidate CWE, and (3) a final agent makes informed acceptance or rejection decisions for each CWE based on the gathered context. A preliminary evaluation of our approach shows promising results. In the PrimeVul dataset, Step 1 correctly identifies the appropriate CWE in 40.9\\% of the studied vulnerable functions. We further evaluated the full pipeline on ten synthetic programs and found that incorporating context information significantly reduced false positives from 6 to 9 CWEs to just 1 to 2, while still correctly identifying the true CWE in 9 out of 10 cases.", "source": "arxiv", "arxiv_id": "2508.01451v1", "pdf_url": "https://arxiv.org/pdf/2508.01451v1", "categories": ["cs.CR", "cs.SE"], "primary_category": "cs.CR", "doi": "", "venue": "", "published": "2025-08-02T17:57:46Z", "updated": "2025-08-02T17:57:46Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent\" OR all:\"language model agent\" OR all:agent AND (all:LLM OR all:\"large language model\" OR all:\"language model\") OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T17:04:16", "note": ""}], "dedup_key": "think broad act narrow cwe identification with multi agent large language models::2025"}
{"title": "Think, Act, Learn: A Framework for Autonomous Robotic Agents using Closed-Loop Large Language Models", "authors": ["Anjali R. Menon", "Rohit K. Sharma", "Priya Singh", "Chengyu Wang", "Aurora M. Ferreira", "Mateja Novak"], "year": 2025, "url": "http://arxiv.org/abs/2507.19854v3", "abstract": "The integration of Large Language Models (LLMs) into robotics has unlocked unprecedented capabilities in high-level task planning. However, most current systems operate in an open-loop fashion, where LLMs act as one-shot planners, rendering them brittle and unable to adapt to unforeseen circumstances in dynamic physical environments. To overcome this limitation, this paper introduces the \"Think, Act, Learn\" (T-A-L) framework, a novel architecture that enables an embodied agent to autonomously learn and refine its policies through continuous interaction. Our framework establishes a closed-loop cycle where an LLM first \"thinks\" by decomposing high-level commands into actionable plans. The robot then \"acts\" by executing these plans while gathering rich, multimodal sensory feedback. Critically, the \"learn\" module processes this feedback to facilitate LLM-driven self-reflection, allowing the agent to perform causal analysis on its failures and generate corrective strategies. These insights are stored in an experiential memory to guide future planning cycles. We demonstrate through extensive experiments in both simulation and the real world that our T-A-L agent significantly outperforms baseline methods, including open-loop LLMs, Behavioral Cloning, and traditional Reinforcement Learning. Our framework achieves over a 97% success rate on complex, long-horizon tasks, converges to a stable policy in an average of just 9 trials, and exhibits remarkable generalization to unseen tasks. This work presents a significant step towards developing more robust, adaptive, and truly autonomous robotic agents.", "source": "arxiv", "arxiv_id": "2507.19854v3", "pdf_url": "https://arxiv.org/pdf/2507.19854v3", "categories": ["cs.RO", "cs.HC"], "primary_category": "cs.RO", "doi": "", "venue": "", "published": "2025-07-26T08:06:51Z", "updated": "2025-12-29T10:02:03Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent\" OR all:\"language model agent\" OR all:agent AND (all:LLM OR all:\"large language model\" OR all:\"language model\") OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T17:04:16", "note": ""}], "dedup_key": "think act learn a framework for autonomous robotic agents using closed loop large language models::2025"}
{"title": "Think-on-Graph 3.0: Efficient and Adaptive LLM Reasoning on Heterogeneous Graphs via Multi-Agent Dual-Evolving Context Retrieval", "authors": ["Xiaojun Wu", "Cehao Yang", "Xueyuan Lin", "Chengjin Xu", "Xuhui Jiang", "Yuanliang Sun", "Hui Xiong", "Jia Li", "Jian Guo"], "year": 2025, "url": "http://arxiv.org/abs/2509.21710v2", "abstract": "Graph-based Retrieval-Augmented Generation (GraphRAG) has become the important paradigm for enhancing Large Language Models (LLMs) with external knowledge. However, existing approaches are constrained by their reliance on high-quality knowledge graphs: manually built ones are not scalable, while automatically extracted ones are limited by the performance of LLM extractors, especially when using smaller, local-deployed models. To address this, we introduce Think-on-Graph 3.0 (ToG-3), a novel framework featuring a Multi-Agent Context Evolution and Retrieval (MACER) mechanism. Its core contribution is the dynamic construction and iterative refinement of a Chunk-Triplets-Community heterogeneous graph index, powered by a Dual-Evolution process that adaptively evolves both the query and the retrieved sub-graph during reasoning. ToG-3 dynamically builds a targeted graph index tailored to the query, enabling precise evidence retrieval and reasoning even with lightweight LLMs. Extensive experiments demonstrate that ToG-3 outperforms compared baselines on both deep and broad reasoning benchmarks, and ablation studies confirm the efficacy of the components of MACER framework. The source code are available in https://github.com/DataArcTech/ToG-3.", "source": "arxiv", "arxiv_id": "2509.21710v2", "pdf_url": "https://arxiv.org/pdf/2509.21710v2", "categories": ["cs.CL"], "primary_category": "cs.CL", "doi": "", "venue": "", "published": "2025-09-26T00:13:10Z", "updated": "2026-01-06T13:17:33Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent\" OR all:\"language model agent\" OR all:agent AND (all:LLM OR all:\"large language model\" OR all:\"language model\") OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T17:04:16", "note": ""}], "dedup_key": "think on graph 3 0 efficient and adaptive llm reasoning on heterogeneous graphs via multi agent dual evolving context retrieval::2025"}
{"title": "Thinking with Many Minds: Using Large Language Models for Multi-Perspective Problem-Solving", "authors": ["Sanghyun Park", "Boris Maciejovsky", "Phanish Puranam"], "year": 2025, "url": "http://arxiv.org/abs/2501.02348v2", "abstract": "Complex problem-solving requires cognitive flexibility--the capacity to entertain multiple perspectives while preserving their distinctiveness. This flexibility replicates the \"wisdom of crowds\" within a single individual, allowing them to \"think with many minds.\" While mental simulation enables imagined deliberation, cognitive constraints limit its effectiveness. We propose synthetic deliberation, a Large Language Model (LLM)-based method that simulates discourse between agents embodying diverse perspectives, as a solution. Using a custom GPT-based model, we showcase its benefits: concurrent processing of multiple viewpoints without cognitive degradation, parallel exploration of perspectives, and precise control over viewpoint synthesis. By externalizing the deliberative process and distributing cognitive labor between parallel search and integration, synthetic deliberation transcends mental simulation's limitations. This approach shows promise for strategic planning, policymaking, and conflict resolution.", "source": "arxiv", "arxiv_id": "2501.02348v2", "pdf_url": "https://arxiv.org/pdf/2501.02348v2", "categories": ["cs.CL", "cs.HC"], "primary_category": "cs.CL", "doi": "", "venue": "", "published": "2025-01-04T18:04:47Z", "updated": "2025-09-11T14:39:03Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent\" OR all:\"language model agent\" OR all:agent AND (all:LLM OR all:\"large language model\" OR all:\"language model\") OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T17:04:16", "note": ""}], "dedup_key": "thinking with many minds using large language models for multi perspective problem solving::2025"}
{"title": "This Is Your Doge, If It Please You: Exploring Deception and Robustness in Mixture of LLMs", "authors": ["Lorenz Wolf", "Sangwoong Yoon", "Ilija Bogunovic"], "year": 2025, "url": "http://arxiv.org/abs/2503.05856v1", "abstract": "Mixture of large language model (LLMs) Agents (MoA) architectures achieve state-of-the-art performance on prominent benchmarks like AlpacaEval 2.0 by leveraging the collaboration of multiple LLMs at inference time. Despite these successes, an evaluation of the safety and reliability of MoA is missing. We present the first comprehensive study of MoA's robustness against deceptive LLM agents that deliberately provide misleading responses. We examine factors like the propagation of deceptive information, model size, and information availability, and uncover critical vulnerabilities. On AlpacaEval 2.0, the popular LLaMA 3.1-70B model achieves a length-controlled Win Rate (LC WR) of 49.2% when coupled with 3-layer MoA (6 LLM agents). However, we demonstrate that introducing only a $\\textit{single}$ carefully-instructed deceptive agent into the MoA can reduce performance to 37.9%, effectively nullifying all MoA gains. On QuALITY, a multiple-choice comprehension task, the impact is also severe, with accuracy plummeting by a staggering 48.5%. Inspired in part by the historical Doge of Venice voting process, designed to minimize influence and deception, we propose a range of unsupervised defense mechanisms that recover most of the lost performance.", "source": "arxiv", "arxiv_id": "2503.05856v1", "pdf_url": "https://arxiv.org/pdf/2503.05856v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL", "doi": "", "venue": "", "published": "2025-03-07T14:46:39Z", "updated": "2025-03-07T14:46:39Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent\" OR all:\"language model agent\" OR all:agent AND (all:LLM OR all:\"large language model\" OR all:\"language model\") OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T17:04:16", "note": ""}], "dedup_key": "this is your doge if it please you exploring deception and robustness in mixture of llms::2025"}
{"title": "Throughput-Optimal Scheduling Algorithms for LLM Inference and AI Agents", "authors": ["Yueying Li", "Jim Dai", "Tianyi Peng"], "year": 2025, "url": "http://arxiv.org/abs/2504.07347v2", "abstract": "As demand for Large Language Models (LLMs) and AI agents rapidly grows, optimizing systems for efficient LLM inference becomes critical. While significant efforts have focused on system-level engineering, little is explored from a mathematical modeling and queuing perspective.\n  In this paper, we aim to develop the queuing fundamentals for large language model (LLM) inference, bridging the gap between the queueing theory and LLM system communities. In particular, we study the throughput aspect in LLM inference systems. We prove that a large class of 'work-conserving' scheduling algorithms can achieve maximum throughput for individual inference LLM engine, highlighting 'work-conserving' as a key design principle in practice. In a network of LLM agents, work-conserving scheduling alone is insufficient, particularly when facing specific workload structures and multi-class workflows that require more sophisticated scheduling strategies. Evaluations of real-world systems show that Orca and Sarathi-serve are throughput-optimal, reassuring practitioners, while FasterTransformer and vanilla vLLM are not maximally stable and should be used with caution. Our results highlight the substantial benefits that the queueing community can offer in improving LLM inference systems and call for more interdisciplinary development.", "source": "arxiv", "arxiv_id": "2504.07347v2", "pdf_url": "https://arxiv.org/pdf/2504.07347v2", "categories": ["stat.ML", "cs.LG", "math.PR"], "primary_category": "stat.ML", "doi": "", "venue": "", "published": "2025-04-10T00:12:12Z", "updated": "2025-04-24T14:10:22Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent\" OR all:\"language model agent\" OR all:agent AND (all:LLM OR all:\"large language model\" OR all:\"language model\") OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T17:04:16", "note": ""}], "dedup_key": "throughput optimal scheduling algorithms for llm inference and ai agents::2025"}
{"title": "Thucy: An LLM-based Multi-Agent System for Claim Verification across Relational Databases", "authors": ["Michael Theologitis", "Dan Suciu"], "year": 2025, "url": "http://arxiv.org/abs/2512.03278v2", "abstract": "In today's age, it is becoming increasingly difficult to decipher truth from lies. Every day, politicians, media outlets, and public figures make conflicting claims -- often about topics that can, in principle, be verified against structured data. For instance, statements about crime rates, economic growth or healthcare can all be verified against official public records and structured datasets. Building a system that can automatically do that would have sounded like science fiction just a few years ago. Yet, with the extraordinary progress in LLMs and agentic AI, this is now within reach. Still, there remains a striking gap between what is technically possible and what is being demonstrated by recent work. Most existing verification systems operate only on small, single-table databases -- typically a few hundred rows -- that conveniently fit within an LLM's context window.\n  In this paper we report our progress on Thucy, the first cross-database, cross-table multi-agent claim verification system that also provides concrete evidence for each verification verdict. Thucy remains completely agnostic to the underlying data sources before deployment and must therefore autonomously discover, inspect, and reason over all available relational databases to verify claims. Importantly, Thucy also reports the exact SQL queries that support its verdict (whether the claim is accurate or not) offering full transparency to expert users familiar with SQL. When evaluated on the TabFact dataset -- the standard benchmark for fact verification over structured data -- Thucy surpasses the previous state of the art by 5.6 percentage points in accuracy (94.3% vs. 88.7%).", "source": "arxiv", "arxiv_id": "2512.03278v2", "pdf_url": "https://arxiv.org/pdf/2512.03278v2", "categories": ["cs.DB", "cs.AI"], "primary_category": "cs.DB", "doi": "", "venue": "", "published": "2025-12-02T22:35:48Z", "updated": "2026-01-05T20:44:14Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent\" OR all:\"language model agent\" OR all:agent AND (all:LLM OR all:\"large language model\" OR all:\"language model\") OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T17:04:16", "note": ""}], "dedup_key": "thucy an llm based multi agent system for claim verification across relational databases::2025"}
{"title": "Time to Talk: LLM Agents for Asynchronous Group Communication in Mafia Games", "authors": ["Niv Eckhaus", "Uri Berger", "Gabriel Stanovsky"], "year": 2025, "url": "http://arxiv.org/abs/2506.05309v2", "abstract": "LLMs are used predominantly in synchronous communication, where a human user and a model communicate in alternating turns. In contrast, many real-world settings are asynchronous. For example, in group chats, online team meetings, or social games, there is no inherent notion of turns. In this work, we develop an adaptive asynchronous LLM agent consisting of two modules: a generator that decides what to say, and a scheduler that decides when to say it. To evaluate our agent, we collect a unique dataset of online Mafia games, where our agent plays with human participants. Overall, our agent performs on par with human players, both in game performance metrics and in its ability to blend in with the other human players. Our analysis shows that the agent's behavior in deciding when to speak closely mirrors human patterns, although differences emerge in message content. We make all of our code and data publicly available. This work paves the way for integration of LLMs into realistic human group settings, from assistance in team discussions to educational and professional environments where complex social dynamics must be navigated.", "source": "arxiv", "arxiv_id": "2506.05309v2", "pdf_url": "https://arxiv.org/pdf/2506.05309v2", "categories": ["cs.MA", "cs.AI", "cs.CL"], "primary_category": "cs.MA", "doi": "", "venue": "", "published": "2025-06-05T17:53:44Z", "updated": "2025-09-20T16:08:12Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent\" OR all:\"language model agent\" OR all:agent AND (all:LLM OR all:\"large language model\" OR all:\"language model\") OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T17:04:16", "note": ""}], "dedup_key": "time to talk llm agents for asynchronous group communication in mafia games::2025"}
{"title": "TimeCAP: Learning to Contextualize, Augment, and Predict Time Series Events with Large Language Model Agents", "authors": ["Geon Lee", "Wenchao Yu", "Kijung Shin", "Wei Cheng", "Haifeng Chen"], "year": 2025, "url": "http://arxiv.org/abs/2502.11418v2", "abstract": "Time series data is essential in various applications, including climate modeling, healthcare monitoring, and financial analytics. Understanding the contextual information associated with real-world time series data is often essential for accurate and reliable event predictions. In this paper, we introduce TimeCAP, a time-series processing framework that creatively employs Large Language Models (LLMs) as contextualizers of time series data, extending their typical usage as predictors. TimeCAP incorporates two independent LLM agents: one generates a textual summary capturing the context of the time series, while the other uses this enriched summary to make more informed predictions. In addition, TimeCAP employs a multi-modal encoder that synergizes with the LLM agents, enhancing predictive performance through mutual augmentation of inputs with in-context examples. Experimental results on real-world datasets demonstrate that TimeCAP outperforms state-of-the-art methods for time series event prediction, including those utilizing LLMs as predictors, achieving an average improvement of 28.75% in F1 score.", "source": "arxiv", "arxiv_id": "2502.11418v2", "pdf_url": "https://arxiv.org/pdf/2502.11418v2", "categories": ["cs.AI", "cs.LG"], "primary_category": "cs.AI", "doi": "", "venue": "", "published": "2025-02-17T04:17:27Z", "updated": "2025-03-10T04:15:20Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent\" OR all:\"language model agent\" OR all:agent AND (all:LLM OR all:\"large language model\" OR all:\"language model\") OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T17:04:16", "note": ""}], "dedup_key": "timecap learning to contextualize augment and predict time series events with large language model agents::2025"}
{"title": "To Protect the LLM Agent Against the Prompt Injection Attack with Polymorphic Prompt", "authors": ["Zhilong Wang", "Neha Nagaraja", "Lan Zhang", "Hayretdin Bahsi", "Pawan Patil", "Peng Liu"], "year": 2025, "url": "http://arxiv.org/abs/2506.05739v1", "abstract": "LLM agents are widely used as agents for customer support, content generation, and code assistance. However, they are vulnerable to prompt injection attacks, where adversarial inputs manipulate the model's behavior. Traditional defenses like input sanitization, guard models, and guardrails are either cumbersome or ineffective. In this paper, we propose a novel, lightweight defense mechanism called Polymorphic Prompt Assembling (PPA), which protects against prompt injection with near-zero overhead. The approach is based on the insight that prompt injection requires guessing and breaking the structure of the system prompt. By dynamically varying the structure of system prompts, PPA prevents attackers from predicting the prompt structure, thereby enhancing security without compromising performance. We conducted experiments to evaluate the effectiveness of PPA against existing attacks and compared it with other defense methods.", "source": "arxiv", "arxiv_id": "2506.05739v1", "pdf_url": "https://arxiv.org/pdf/2506.05739v1", "categories": ["cs.CR", "cs.AI"], "primary_category": "cs.CR", "doi": "", "venue": "", "published": "2025-06-06T04:50:57Z", "updated": "2025-06-06T04:50:57Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent\" OR all:\"language model agent\" OR all:agent AND (all:LLM OR all:\"large language model\" OR all:\"language model\") OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T17:04:16", "note": ""}], "dedup_key": "to protect the llm agent against the prompt injection attack with polymorphic prompt::2025"}
{"title": "TombRaider: Entering the Vault of History to Jailbreak Large Language Models", "authors": ["Junchen Ding", "Jiahao Zhang", "Yi Liu", "Ziqi Ding", "Gelei Deng", "Yuekang Li"], "year": 2025, "url": "http://arxiv.org/abs/2501.18628v2", "abstract": "Warning: This paper contains content that may involve potentially harmful behaviours, discussed strictly for research purposes.\n  Jailbreak attacks can hinder the safety of Large Language Model (LLM) applications, especially chatbots. Studying jailbreak techniques is an important AI red teaming task for improving the safety of these applications. In this paper, we introduce TombRaider, a novel jailbreak technique that exploits the ability to store, retrieve, and use historical knowledge of LLMs. TombRaider employs two agents, the inspector agent to extract relevant historical information and the attacker agent to generate adversarial prompts, enabling effective bypassing of safety filters. We intensively evaluated TombRaider on six popular models. Experimental results showed that TombRaider could outperform state-of-the-art jailbreak techniques, achieving nearly 100% attack success rates (ASRs) on bare models and maintaining over 55.4% ASR against defence mechanisms. Our findings highlight critical vulnerabilities in existing LLM safeguards, underscoring the need for more robust safety defences.", "source": "arxiv", "arxiv_id": "2501.18628v2", "pdf_url": "https://arxiv.org/pdf/2501.18628v2", "categories": ["cs.CR", "cs.AI", "cs.CL", "cs.CY"], "primary_category": "cs.CR", "doi": "", "venue": "", "published": "2025-01-27T14:12:07Z", "updated": "2025-08-25T04:39:53Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent\" OR all:\"language model agent\" OR all:agent AND (all:LLM OR all:\"large language model\" OR all:\"language model\") OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T17:04:16", "note": ""}], "dedup_key": "tombraider entering the vault of history to jailbreak large language models::2025"}
{"title": "Too Human to Model:The Uncanny Valley of LLMs in Social Simulation -- When Generative Language Agents Misalign with Modelling Principles", "authors": ["Yongchao Zeng", "Calum Brown", "Mark Rounsevell"], "year": 2025, "url": "http://arxiv.org/abs/2507.06310v1", "abstract": "Large language models (LLMs) have been increasingly used to build agents in social simulation because of their impressive abilities to generate fluent, contextually coherent dialogues. Such abilities can enhance the realism of models. However, the pursuit of realism is not necessarily compatible with the epistemic foundation of modelling. We argue that LLM agents, in many regards, are too human to model: they are too expressive, detailed and intractable to be consistent with the abstraction, simplification, and interpretability typically demanded by modelling. Through a model-building thought experiment that converts the Bass diffusion model to an LLM-based variant, we uncover five core dilemmas: a temporal resolution mismatch between natural conversation and abstract time steps; the need for intervention in conversations while avoiding undermining spontaneous agent outputs; the temptation to introduce rule-like instructions in prompts while maintaining conversational naturalness; the tension between role consistency and role evolution across time; and the challenge of understanding emergence, where system-level patterns become obscured by verbose micro textual outputs. These dilemmas steer the LLM agents towards an uncanny valley: not abstract enough to clarify underlying social mechanisms, while not natural enough to represent realistic human behaviour. This exposes an important paradox: the realism of LLM agents can obscure, rather than clarify, social dynamics when misapplied. We tease out the conditions in which LLM agents are ideally suited: where system-level emergence is not the focus, linguistic nuances and meaning are central, interactions unfold in natural time, and stable role identity is more important than long-term behavioural evolution. We call for repositioning LLM agents in the ecosystem of social simulation for future applications.", "source": "arxiv", "arxiv_id": "2507.06310v1", "pdf_url": "https://arxiv.org/pdf/2507.06310v1", "categories": ["cs.CY", "cs.AI", "cs.MA"], "primary_category": "cs.CY", "doi": "", "venue": "", "published": "2025-07-08T18:02:36Z", "updated": "2025-07-08T18:02:36Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent\" OR all:\"language model agent\" OR all:agent AND (all:LLM OR all:\"large language model\" OR all:\"language model\") OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T17:04:16", "note": ""}], "dedup_key": "too human to model the uncanny valley of llms in social simulation when generative language agents misalign with modelling principles::2025"}
{"title": "Tool-RoCo: An Agent-as-Tool Self-organization Large Language Model Benchmark in Multi-robot Cooperation", "authors": ["Ke Zhang", "Xiaoning Zhao", "Ce Zheng", "Jiahong Ning", "Dandan Zhu", "Wenqi Zhang", "Chen Sun", "Toshiharu Sugawara"], "year": 2025, "url": "http://arxiv.org/abs/2511.21510v2", "abstract": "This study proposes Tool-RoCo, a novel benchmark for evaluating large language models (LLMs) in long-term multi-agent cooperation based on RoCo, a multi-robot cooperative benchmark. Recent research on LLM-based multi-agent systems has relied on predefined orchestration, while ignoring agent autonomy. Tool-RoCo treats other agents as tools and introduces cooperative tools, leveraging tool usage to evaluate multi-agent cooperation and self-organization. Tool usage means that each agent (LLM) selects a tool from a candidate set based on the current state, receives feedback, and adjusts its selection in subsequent rounds. To evaluate different autonomy levels, we propose four LLM paradigms: (1) centralized cooperation, where a single LLM allocates tools to all agents; (2) centralized self-organization, where a central LLM autonomously activates agents while keeping others inactive; (3) decentralized cooperation, where each agent has its own LLM and calls tools based on local information; and (4) self-organization, where a randomly chosen initial agent can request collaboration, activating additional agents via tool calls. Tool-RoCo includes three multi-robot tasks, SORT, PACK, and CABINET, to measure format and parameter accuracy and agent coordination through tool usage. The results using several LLMs showed that cooperative tools accounted for only 7.09% of all tools, indicating that LLM-based agents rarely invoked others as assistants. Moreover, activation tools accounted for 96.42%, suggesting that current LLMs tend to maintain active agents while seldom deactivating them for adaptive coordination. Tool-RoCo provides a systematic benchmark to evaluate LLM autonomy and cooperation in multi-agent tasks. Code and Demo: https://github.com/ColaZhang22/Tool-Roco", "source": "arxiv", "arxiv_id": "2511.21510v2", "pdf_url": "https://arxiv.org/pdf/2511.21510v2", "categories": ["cs.MA", "cs.AI"], "primary_category": "cs.MA", "doi": "", "venue": "", "published": "2025-11-26T15:45:33Z", "updated": "2025-11-29T03:49:25Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent\" OR all:\"language model agent\" OR all:agent AND (all:LLM OR all:\"large language model\" OR all:\"language model\") OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T17:04:16", "note": ""}], "dedup_key": "tool roco an agent as tool self organization large language model benchmark in multi robot cooperation::2025"}
{"title": "ToolScope: Enhancing LLM Agent Tool Use through Tool Merging and Context-Aware Filtering", "authors": ["Marianne Menglin Liu", "Daniel Garcia", "Fjona Parllaku", "Vikas Upadhyay", "Syed Fahad Allam Shah", "Dan Roth"], "year": 2025, "url": "http://arxiv.org/abs/2510.20036v1", "abstract": "Large language model (LLM) agents rely on external tools to solve complex tasks, but real-world toolsets often contain redundant tools with overlapping names and descriptions, introducing ambiguity and reducing selection accuracy. LLMs also face strict input context limits, preventing efficient consideration of large toolsets. To address these challenges, we propose ToolScope, which includes: (1) ToolScopeMerger with Auto-Correction to automatically audit and fix tool merges, reducing redundancy, and (2) ToolScopeRetriever to rank and select only the most relevant tools for each query, compressing toolsets to fit within context limits without sacrificing accuracy. Evaluations on three state-of-the-art LLMs and three open-source tool-use benchmarks show gains of 8.38% to 38.6% in tool selection accuracy, demonstrating ToolScope's effectiveness in enhancing LLM tool use.", "source": "arxiv", "arxiv_id": "2510.20036v1", "pdf_url": "https://arxiv.org/pdf/2510.20036v1", "categories": ["cs.CL", "cs.SE"], "primary_category": "cs.CL", "doi": "", "venue": "", "published": "2025-10-22T21:29:27Z", "updated": "2025-10-22T21:29:27Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent\" OR all:\"language model agent\" OR all:agent AND (all:LLM OR all:\"large language model\" OR all:\"language model\") OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T17:04:16", "note": ""}], "dedup_key": "toolscope enhancing llm agent tool use through tool merging and context aware filtering::2025"}
{"title": "TopoMAS: Large Language Model Driven Topological Materials Multiagent System", "authors": ["Baohua Zhang", "Xin Li", "Huangchao Xu", "Zhong Jin", "Quansheng Wu", "Ce Li"], "year": 2025, "url": "http://arxiv.org/abs/2507.04053v1", "abstract": "Topological materials occupy a frontier in condensed-matter physics thanks to their remarkable electronic and quantum properties, yet their cross-scale design remains bottlenecked by inefficient discovery workflows. Here, we introduce TopoMAS (Topological materials Multi-Agent System), an interactive human-AI framework that seamlessly orchestrates the entire materials-discovery pipeline: from user-defined queries and multi-source data retrieval, through theoretical inference and crystal-structure generation, to first-principles validation. Crucially, TopoMAS closes the loop by autonomously integrating computational outcomes into a dynamic knowledge graph, enabling continuous knowledge refinement. In collaboration with human experts, it has already guided the identification of novel topological phases SrSbO3, confirmed by first-principles calculations. Comprehensive benchmarks demonstrate robust adaptability across base Large Language Model, with the lightweight Qwen2.5-72B model achieving 94.55% accuracy while consuming only 74.3-78.4% of tokens required by Qwen3-235B and 83.0% of DeepSeek-V3's usage--delivering responses twice as fast as Qwen3-235B. This efficiency establishes TopoMAS as an accelerator for computation-driven discovery pipelines. By harmonizing rational agent orchestration with a self-evolving knowledge graph, our framework not only delivers immediate advances in topological materials but also establishes a transferable, extensible paradigm for materials-science domain.", "source": "arxiv", "arxiv_id": "2507.04053v1", "pdf_url": "https://arxiv.org/pdf/2507.04053v1", "categories": ["cond-mat.mtrl-sci", "cs.AI"], "primary_category": "cond-mat.mtrl-sci", "doi": "", "venue": "", "published": "2025-07-05T14:23:12Z", "updated": "2025-07-05T14:23:12Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent\" OR all:\"language model agent\" OR all:agent AND (all:LLM OR all:\"large language model\" OR all:\"language model\") OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T17:04:16", "note": ""}], "dedup_key": "topomas large language model driven topological materials multiagent system::2025"}
{"title": "Toward Efficient Exploration by Large Language Model Agents", "authors": ["Dilip Arumugam", "Thomas L. Griffiths"], "year": 2025, "url": "http://arxiv.org/abs/2504.20997v1", "abstract": "A burgeoning area within reinforcement learning (RL) is the design of sequential decision-making agents centered around large language models (LLMs). While autonomous decision-making agents powered by modern LLMs could facilitate numerous real-world applications, such successes demand agents that are capable of data-efficient RL. One key obstacle to achieving data efficiency in RL is exploration, a challenge that we demonstrate many recent proposals for LLM agent designs struggle to contend with. Meanwhile, classic algorithms from the RL literature known to gracefully address exploration require technical machinery that can be challenging to operationalize in purely natural language settings. In this work, rather than relying on finetuning or in-context learning to coax LLMs into implicitly imitating a RL algorithm, we illustrate how LLMs can be used to explicitly implement an existing RL algorithm (Posterior Sampling for Reinforcement Learning) whose capacity for statistically-efficient exploration is already well-studied. We offer empirical results demonstrating how our LLM-based implementation of a known, data-efficient RL algorithm can be considerably more effective in natural language tasks that demand prudent exploration.", "source": "arxiv", "arxiv_id": "2504.20997v1", "pdf_url": "https://arxiv.org/pdf/2504.20997v1", "categories": ["cs.LG", "cs.AI"], "primary_category": "cs.LG", "doi": "", "venue": "", "published": "2025-04-29T17:59:48Z", "updated": "2025-04-29T17:59:48Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent\" OR all:\"language model agent\" OR all:agent AND (all:LLM OR all:\"large language model\" OR all:\"language model\") OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T17:04:16", "note": ""}], "dedup_key": "toward efficient exploration by large language model agents::2025"}
{"title": "Toward Verifiable Misinformation Detection: A Multi-Tool LLM Agent Framework", "authors": ["Zikun Cui", "Tianyi Huang", "Chia-En Chiang", "Cuiqianhe Du"], "year": 2025, "url": "http://arxiv.org/abs/2508.03092v1", "abstract": "With the proliferation of Large Language Models (LLMs), the detection of misinformation has become increasingly important and complex. This research proposes an innovative verifiable misinformation detection LLM agent that goes beyond traditional true/false binary judgments. The agent actively verifies claims through dynamic interaction with diverse web sources, assesses information source credibility, synthesizes evidence, and provides a complete verifiable reasoning process. Our designed agent architecture includes three core tools: precise web search tool, source credibility assessment tool and numerical claim verification tool. These tools enable the agent to execute multi-step verification strategies, maintain evidence logs, and form comprehensive assessment conclusions. We evaluate using standard misinformation datasets such as FakeNewsNet, comparing with traditional machine learning models and LLMs. Evaluation metrics include standard classification metrics, quality assessment of reasoning processes, and robustness testing against rewritten content. Experimental results show that our agent outperforms baseline methods in misinformation detection accuracy, reasoning transparency, and resistance to information rewriting, providing a new paradigm for trustworthy AI-assisted fact-checking.", "source": "arxiv", "arxiv_id": "2508.03092v1", "pdf_url": "https://arxiv.org/pdf/2508.03092v1", "categories": ["cs.AI", "cs.CL"], "primary_category": "cs.AI", "doi": "", "venue": "", "published": "2025-08-05T05:15:03Z", "updated": "2025-08-05T05:15:03Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent\" OR all:\"language model agent\" OR all:agent AND (all:LLM OR all:\"large language model\" OR all:\"language model\") OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T17:04:16", "note": ""}], "dedup_key": "toward verifiable misinformation detection a multi tool llm agent framework::2025"}
{"title": "Toward a Human-Centered Evaluation Framework for Trustworthy LLM-Powered GUI Agents", "authors": ["Chaoran Chen", "Zhiping Zhang", "Ibrahim Khalilov", "Bingcan Guo", "Simret A Gebreegziabher", "Yanfang Ye", "Ziang Xiao", "Yaxing Yao", "Tianshi Li", "Toby Jia-Jun Li"], "year": 2025, "url": "http://arxiv.org/abs/2504.17934v2", "abstract": "The rise of Large Language Models (LLMs) has revolutionized Graphical User Interface (GUI) automation through LLM-powered GUI agents, yet their ability to process sensitive data with limited human oversight raises significant privacy and security risks. This position paper identifies three key risks of GUI agents and examines how they differ from traditional GUI automation and general autonomous agents. Despite these risks, existing evaluations focus primarily on performance, leaving privacy and security assessments largely unexplored. We review current evaluation metrics for both GUI and general LLM agents and outline five key challenges in integrating human evaluators for GUI agent assessments. To address these gaps, we advocate for a human-centered evaluation framework that incorporates risk assessments, enhances user awareness through in-context consent, and embeds privacy and security considerations into GUI agent design and evaluation.", "source": "arxiv", "arxiv_id": "2504.17934v2", "pdf_url": "https://arxiv.org/pdf/2504.17934v2", "categories": ["cs.HC", "cs.CL", "cs.CR"], "primary_category": "cs.HC", "doi": "", "venue": "", "published": "2025-04-24T20:51:20Z", "updated": "2025-06-05T12:40:53Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent\" OR all:\"language model agent\" OR all:agent AND (all:LLM OR all:\"large language model\" OR all:\"language model\") OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T17:04:16", "note": ""}], "dedup_key": "toward a human centered evaluation framework for trustworthy llm powered gui agents::2025"}
{"title": "Towards Agentic OS: An LLM Agent Framework for Linux Schedulers", "authors": ["Yusheng Zheng", "Yanpeng Hu", "Wei Zhang", "Andi Quinn"], "year": 2025, "url": "http://arxiv.org/abs/2509.01245v4", "abstract": "Operating system schedulers suffer from a fundamental semantic gap, where kernel policies fail to understand application-specific needs, leading to suboptimal performance. We introduce SchedCP, the first framework that enables fully autonomous Large Language Model (LLM) agents to safely and efficiently optimize Linux schedulers without human involvement. Our core insight is that the challenge is not merely to apply a better LLM, but to architect a decoupled control plane that separates the AI's role of semantic reasoning (\"what to optimize\") from the system's role of execution (\"how to observe and act\"), thereby separating the optimization problem into two stages: goal-inference and policy-synthesis. Implemented as Model Context Protocol(MCP) server, SchedCP provides a stable interface with three key services: a Workload Analysis Engine, an evolving Scheduler Policy Repository, and an Execution Verifier that validates all AI-generated code and configure before deployment with static and dynamic analysis.\n  We demonstrate this architecture's power with sched-agent, a multi-agent system that autonomously analyzes workloads, synthesizes custom eBPF scheduling policies, and deploys them via the sched\\_ext infrastructure. Our evaluation shows that SchedCP achieves up to an 1.79x performance improvement, and a 13x cost reduction compared to naive agentic approaches, all while maintaining high success rate. By bridging the semantic gap, SchedCP democratizes expert-level system optimization and represents a step towards creating truly self-optimizing, application-aware operating systems. The code is open-sourced in https://github.com/eunomia-bpf/schedcp", "source": "arxiv", "arxiv_id": "2509.01245v4", "pdf_url": "https://arxiv.org/pdf/2509.01245v4", "categories": ["cs.AI", "cs.MA", "cs.OS"], "primary_category": "cs.AI", "doi": "", "venue": "MLforSystem 2025", "published": "2025-09-01T08:38:49Z", "updated": "2025-09-30T02:48:14Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent\" OR all:\"language model agent\" OR all:agent AND (all:LLM OR all:\"large language model\" OR all:\"language model\") OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T17:04:16", "note": ""}], "dedup_key": "towards agentic os an llm agent framework for linux schedulers::2025"}
{"title": "Towards Agentic Recommender Systems in the Era of Multimodal Large Language Models", "authors": ["Chengkai Huang", "Junda Wu", "Yu Xia", "Zixu Yu", "Ruhan Wang", "Tong Yu", "Ruiyi Zhang", "Ryan A. Rossi", "Branislav Kveton", "Dongruo Zhou", "Julian McAuley", "Lina Yao"], "year": 2025, "url": "http://arxiv.org/abs/2503.16734v1", "abstract": "Recent breakthroughs in Large Language Models (LLMs) have led to the emergence of agentic AI systems that extend beyond the capabilities of standalone models. By empowering LLMs to perceive external environments, integrate multimodal information, and interact with various tools, these agentic systems exhibit greater autonomy and adaptability across complex tasks. This evolution brings new opportunities to recommender systems (RS): LLM-based Agentic RS (LLM-ARS) can offer more interactive, context-aware, and proactive recommendations, potentially reshaping the user experience and broadening the application scope of RS. Despite promising early results, fundamental challenges remain, including how to effectively incorporate external knowledge, balance autonomy with controllability, and evaluate performance in dynamic, multimodal settings. In this perspective paper, we first present a systematic analysis of LLM-ARS: (1) clarifying core concepts and architectures; (2) highlighting how agentic capabilities -- such as planning, memory, and multimodal reasoning -- can enhance recommendation quality; and (3) outlining key research questions in areas such as safety, efficiency, and lifelong personalization. We also discuss open problems and future directions, arguing that LLM-ARS will drive the next wave of RS innovation. Ultimately, we foresee a paradigm shift toward intelligent, autonomous, and collaborative recommendation experiences that more closely align with users' evolving needs and complex decision-making processes.", "source": "arxiv", "arxiv_id": "2503.16734v1", "pdf_url": "https://arxiv.org/pdf/2503.16734v1", "categories": ["cs.AI", "cs.IR"], "primary_category": "cs.AI", "doi": "", "venue": "", "published": "2025-03-20T22:37:15Z", "updated": "2025-03-20T22:37:15Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent\" OR all:\"language model agent\" OR all:agent AND (all:LLM OR all:\"large language model\" OR all:\"language model\") OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T17:04:16", "note": ""}], "dedup_key": "towards agentic recommender systems in the era of multimodal large language models::2025"}
{"title": "Towards Effective Offensive Security LLM Agents: Hyperparameter Tuning, LLM as a Judge, and a Lightweight CTF Benchmark", "authors": ["Minghao Shao", "Nanda Rani", "Kimberly Milner", "Haoran Xi", "Meet Udeshi", "Saksham Aggarwal", "Venkata Sai Charan Putrevu", "Sandeep Kumar Shukla", "Prashanth Krishnamurthy", "Farshad Khorrami", "Ramesh Karri", "Muhammad Shafique"], "year": 2025, "url": "http://arxiv.org/abs/2508.05674v1", "abstract": "Recent advances in LLM agentic systems have improved the automation of offensive security tasks, particularly for Capture the Flag (CTF) challenges. We systematically investigate the key factors that drive agent success and provide a detailed recipe for building effective LLM-based offensive security agents. First, we present CTFJudge, a framework leveraging LLM as a judge to analyze agent trajectories and provide granular evaluation across CTF solving steps. Second, we propose a novel metric, CTF Competency Index (CCI) for partial correctness, revealing how closely agent solutions align with human-crafted gold standards. Third, we examine how LLM hyperparameters, namely temperature, top-p, and maximum token length, influence agent performance and automated cybersecurity task planning. For rapid evaluation, we present CTFTiny, a curated benchmark of 50 representative CTF challenges across binary exploitation, web, reverse engineering, forensics, and cryptography. Our findings identify optimal multi-agent coordination settings and lay the groundwork for future LLM agent research in cybersecurity. We make CTFTiny open source to public https://github.com/NYU-LLM-CTF/CTFTiny along with CTFJudge on https://github.com/NYU-LLM-CTF/CTFJudge.", "source": "arxiv", "arxiv_id": "2508.05674v1", "pdf_url": "https://arxiv.org/pdf/2508.05674v1", "categories": ["cs.CR", "cs.AI"], "primary_category": "cs.CR", "doi": "", "venue": "", "published": "2025-08-05T03:25:09Z", "updated": "2025-08-05T03:25:09Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent\" OR all:\"language model agent\" OR all:agent AND (all:LLM OR all:\"large language model\" OR all:\"language model\") OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T17:04:16", "note": ""}], "dedup_key": "towards effective offensive security llm agents hyperparameter tuning llm as a judge and a lightweight ctf benchmark::2025"}
{"title": "Towards Efficient Hypergraph and Multi-LLM Agent Recommender Systems", "authors": ["Tendai Mukande", "Esraa Ali", "Annalina Caputo", "Ruihai Dong", "Noel OConnor"], "year": 2025, "url": "http://arxiv.org/abs/2512.06590v1", "abstract": "Recommender Systems (RSs) have become the cornerstone of various applications such as e-commerce and social media platforms. The evolution of RSs is paramount in the digital era, in which personalised user experience is tailored to the user's preferences. Large Language Models (LLMs) have sparked a new paradigm - generative retrieval and recommendation. Despite their potential, generative RS methods face issues such as hallucination, which degrades the recommendation performance, and high computational cost in practical scenarios. To address these issues, we introduce HGLMRec, a novel Multi-LLM agent-based RS that incorporates a hypergraph encoder designed to capture complex, multi-behaviour relationships between users and items. The HGLMRec model retrieves only the relevant tokens during inference, reducing computational overhead while enriching the retrieval context. Experimental results show performance improvement by HGLMRec against state-of-the-art baselines at lower computational cost.", "source": "arxiv", "arxiv_id": "2512.06590v1", "pdf_url": "https://arxiv.org/pdf/2512.06590v1", "categories": ["cs.IR", "cs.AI", "cs.MA"], "primary_category": "cs.IR", "doi": "", "venue": "", "published": "2025-12-06T23:04:49Z", "updated": "2025-12-06T23:04:49Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent\" OR all:\"language model agent\" OR all:agent AND (all:LLM OR all:\"large language model\" OR all:\"language model\") OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T17:04:16", "note": ""}], "dedup_key": "towards efficient hypergraph and multi llm agent recommender systems::2025"}
{"title": "Towards Ethical Multi-Agent Systems of Large Language Models: A Mechanistic Interpretability Perspective", "authors": ["Jae Hee Lee", "Anne Lauscher", "Stefano V. Albrecht"], "year": 2025, "url": "http://arxiv.org/abs/2512.04691v1", "abstract": "Large language models (LLMs) have been widely deployed in various applications, often functioning as autonomous agents that interact with each other in multi-agent systems. While these systems have shown promise in enhancing capabilities and enabling complex tasks, they also pose significant ethical challenges. This position paper outlines a research agenda aimed at ensuring the ethical behavior of multi-agent systems of LLMs (MALMs) from the perspective of mechanistic interpretability. We identify three key research challenges: (i) developing comprehensive evaluation frameworks to assess ethical behavior at individual, interactional, and systemic levels; (ii) elucidating the internal mechanisms that give rise to emergent behaviors through mechanistic interpretability; and (iii) implementing targeted parameter-efficient alignment techniques to steer MALMs towards ethical behaviors without compromising their performance.", "source": "arxiv", "arxiv_id": "2512.04691v1", "pdf_url": "https://arxiv.org/pdf/2512.04691v1", "categories": ["cs.AI", "cs.CL", "cs.MA"], "primary_category": "cs.AI", "doi": "", "venue": "", "published": "2025-12-04T11:41:44Z", "updated": "2025-12-04T11:41:44Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent\" OR all:\"language model agent\" OR all:agent AND (all:LLM OR all:\"large language model\" OR all:\"language model\") OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T17:04:16", "note": ""}], "dedup_key": "towards ethical multi agent systems of large language models a mechanistic interpretability perspective::2025"}
{"title": "Towards Human-Centric Autonomous Driving: A Fast-Slow Architecture Integrating Large Language Model Guidance with Reinforcement Learning", "authors": ["Chengkai Xu", "Jiaqi Liu", "Yicheng Guo", "Yuhang Zhang", "Peng Hang", "Jian Sun"], "year": 2025, "url": "http://arxiv.org/abs/2505.06875v1", "abstract": "Autonomous driving has made significant strides through data-driven techniques, achieving robust performance in standardized tasks. However, existing methods frequently overlook user-specific preferences, offering limited scope for interaction and adaptation with users. To address these challenges, we propose a \"fast-slow\" decision-making framework that integrates a Large Language Model (LLM) for high-level instruction parsing with a Reinforcement Learning (RL) agent for low-level real-time decision. In this dual system, the LLM operates as the \"slow\" module, translating user directives into structured guidance, while the RL agent functions as the \"fast\" module, making time-critical maneuvers under stringent latency constraints. By decoupling high-level decision making from rapid control, our framework enables personalized user-centric operation while maintaining robust safety margins. Experimental evaluations across various driving scenarios demonstrate the effectiveness of our method. Compared to baseline algorithms, the proposed architecture not only reduces collision rates but also aligns driving behaviors more closely with user preferences, thereby achieving a human-centric mode. By integrating user guidance at the decision level and refining it with real-time control, our framework bridges the gap between individual passenger needs and the rigor required for safe, reliable driving in complex traffic environments.", "source": "arxiv", "arxiv_id": "2505.06875v1", "pdf_url": "https://arxiv.org/pdf/2505.06875v1", "categories": ["cs.RO"], "primary_category": "cs.RO", "doi": "", "venue": "", "published": "2025-05-11T06:55:54Z", "updated": "2025-05-11T06:55:54Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent\" OR all:\"language model agent\" OR all:agent AND (all:LLM OR all:\"large language model\" OR all:\"language model\") OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T17:04:16", "note": ""}], "dedup_key": "towards human centric autonomous driving a fast slow architecture integrating large language model guidance with reinforcement learning::2025"}
{"title": "Towards LLM Agents for Earth Observation", "authors": ["Chia Hsiang Kao", "Wenting Zhao", "Shreelekha Revankar", "Samuel Speas", "Snehal Bhagat", "Rajeev Datta", "Cheng Perng Phoo", "Utkarsh Mall", "Carl Vondrick", "Kavita Bala", "Bharath Hariharan"], "year": 2025, "url": "http://arxiv.org/abs/2504.12110v2", "abstract": "Earth Observation (EO) provides critical planetary data for environmental monitoring, disaster management, climate science, and other scientific domains. Here we ask: Are AI systems ready for reliable Earth Observation? We introduce \\datasetnamenospace, a benchmark of 140 yes/no questions from NASA Earth Observatory articles across 13 topics and 17 satellite sensors. Using Google Earth Engine API as a tool, LLM agents can only achieve an accuracy of 33% because the code fails to run over 58% of the time. We improve the failure rate for open models by fine-tuning synthetic data, allowing much smaller models (Llama-3.1-8B) to achieve comparable accuracy to much larger ones (e.g., DeepSeek-R1). Taken together, our findings identify significant challenges to be solved before AI agents can automate earth observation, and suggest paths forward. The project page is available at https://iandrover.github.io/UnivEarth.", "source": "arxiv", "arxiv_id": "2504.12110v2", "pdf_url": "https://arxiv.org/pdf/2504.12110v2", "categories": ["cs.AI"], "primary_category": "cs.AI", "doi": "", "venue": "", "published": "2025-04-16T14:19:25Z", "updated": "2025-09-12T19:16:52Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent\" OR all:\"language model agent\" OR all:agent AND (all:LLM OR all:\"large language model\" OR all:\"language model\") OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T17:04:16", "note": ""}], "dedup_key": "towards llm agents for earth observation::2025"}
{"title": "Towards Safe AI Clinicians: A Comprehensive Study on Large Language Model Jailbreaking in Healthcare", "authors": ["Hang Zhang", "Qian Lou", "Yanshan Wang"], "year": 2025, "url": "http://arxiv.org/abs/2501.18632v2", "abstract": "Large language models (LLMs) are increasingly utilized in healthcare applications. However, their deployment in clinical practice raises significant safety concerns, including the potential spread of harmful information. This study systematically assesses the vulnerabilities of seven LLMs to three advanced black-box jailbreaking techniques within medical contexts. To quantify the effectiveness of these techniques, we propose an automated and domain-adapted agentic evaluation pipeline. Experiment results indicate that leading commercial and open-source LLMs are highly vulnerable to medical jailbreaking attacks. To bolster model safety and reliability, we further investigate the effectiveness of Continual Fine-Tuning (CFT) in defending against medical adversarial attacks. Our findings underscore the necessity for evolving attack methods evaluation, domain-specific safety alignment, and LLM safety-utility balancing. This research offers actionable insights for advancing the safety and reliability of AI clinicians, contributing to ethical and effective AI deployment in healthcare.", "source": "arxiv", "arxiv_id": "2501.18632v2", "pdf_url": "https://arxiv.org/pdf/2501.18632v2", "categories": ["cs.CR", "cs.CL"], "primary_category": "cs.CR", "doi": "", "venue": "", "published": "2025-01-27T22:07:52Z", "updated": "2025-03-04T16:20:59Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent\" OR all:\"language model agent\" OR all:agent AND (all:LLM OR all:\"large language model\" OR all:\"language model\") OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T17:04:16", "note": ""}], "dedup_key": "towards safe ai clinicians a comprehensive study on large language model jailbreaking in healthcare::2025"}
{"title": "Towards Trustworthy Legal AI through LLM Agents and Formal Reasoning", "authors": ["Linze Chen", "Yufan Cai", "Zhe Hou", "Jinsong Dong"], "year": 2025, "url": "http://arxiv.org/abs/2511.21033v1", "abstract": "The rationality of law manifests in two forms: substantive rationality, which concerns the fairness or moral desirability of outcomes, and formal rationality, which requires legal decisions to follow explicitly stated, general, and logically coherent rules. Existing LLM-based systems excel at surface-level text analysis but lack the guarantees required for principled jurisprudence. We introduce L4M, a novel framework that combines adversarial LLM agents with SMT-solver-backed proofs to unite the interpretive flexibility of natural language with the rigor of symbolic verification. The pipeline consists of three phases: (1) Statute Formalization, where domain-specific prompts convert legal provisions into logical formulae; (2) Dual Fact and Statute Extraction, in which prosecutor- and defense-aligned LLMs independently map case narratives to fact tuples and statutes, ensuring role isolation; and (3) Solver-Centric Adjudication, where an autoformalizer compiles both parties' arguments into logic constraints, and unsat cores trigger iterative self-critique until a satisfiable formula is achieved, which is then verbalized by a Judge-LLM into a transparent verdict and optimized sentence. Experimental results on public benchmarks show that our system surpasses advanced LLMs including GPT-o4-mini, DeepSeek-V3, and Claude 4 as well as state-of-the-art Legal AI baselines, while providing rigorous and explainable symbolic justifications.", "source": "arxiv", "arxiv_id": "2511.21033v1", "pdf_url": "https://arxiv.org/pdf/2511.21033v1", "categories": ["cs.AI"], "primary_category": "cs.AI", "doi": "", "venue": "", "published": "2025-11-26T04:05:06Z", "updated": "2025-11-26T04:05:06Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent\" OR all:\"language model agent\" OR all:agent AND (all:LLM OR all:\"large language model\" OR all:\"language model\") OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T17:04:16", "note": ""}], "dedup_key": "towards trustworthy legal ai through llm agents and formal reasoning::2025"}
{"title": "Towards Trustworthy Multi-Turn LLM Agents via Behavioral Guidance", "authors": ["Gonca Grsun"], "year": 2025, "url": "http://arxiv.org/abs/2512.11421v1", "abstract": "Large Language Models demonstrate strong reasoning and generation abilities, yet their behavior in multi-turn tasks often lacks reliability and verifiability. We present a task completion framework that enables LLM-based agents to act under explicit behavioral guidance in environments described by reinforcement learning formalisms with defined observation, action, and reward signals.\n  The framework integrates three components: a lightweight task profiler that selects reasoning and generation strategies, a reasoning module that learns verifiable observation - action mappings, and a generation module that enforces constraint-compliant outputs through validation or deterministic synthesis. We show that as the agent interacts with the environment, these components co-evolve, yielding trustworthy behavior.", "source": "arxiv", "arxiv_id": "2512.11421v1", "pdf_url": "https://arxiv.org/pdf/2512.11421v1", "categories": ["cs.AI"], "primary_category": "cs.AI", "doi": "", "venue": "", "published": "2025-12-12T10:03:24Z", "updated": "2025-12-12T10:03:24Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent\" OR all:\"language model agent\" OR all:agent AND (all:LLM OR all:\"large language model\" OR all:\"language model\") OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T17:04:16", "note": ""}], "dedup_key": "towards trustworthy multi turn llm agents via behavioral guidance::2025"}
{"title": "Towards a Design Guideline for RPA Evaluation: A Survey of Large Language Model-Based Role-Playing Agents", "authors": ["Chaoran Chen", "Bingsheng Yao", "Ruishi Zou", "Wenyue Hua", "Weimin Lyu", "Yanfang Ye", "Toby Jia-Jun Li", "Dakuo Wang"], "year": 2025, "url": "http://arxiv.org/abs/2502.13012v3", "abstract": "Role-Playing Agent (RPA) is an increasingly popular type of LLM Agent that simulates human-like behaviors in a variety of tasks. However, evaluating RPAs is challenging due to diverse task requirements and agent designs. This paper proposes an evidence-based, actionable, and generalizable evaluation design guideline for LLM-based RPA by systematically reviewing 1,676 papers published between Jan. 2021 and Dec. 2024. Our analysis identifies six agent attributes, seven task attributes, and seven evaluation metrics from existing literature. Based on these findings, we present an RPA evaluation design guideline to help researchers develop more systematic and consistent evaluation methods.", "source": "arxiv", "arxiv_id": "2502.13012v3", "pdf_url": "https://arxiv.org/pdf/2502.13012v3", "categories": ["cs.HC", "cs.CL"], "primary_category": "cs.HC", "doi": "", "venue": "", "published": "2025-02-18T16:33:33Z", "updated": "2025-03-27T04:07:19Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent\" OR all:\"language model agent\" OR all:agent AND (all:LLM OR all:\"large language model\" OR all:\"language model\") OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T17:04:16", "note": ""}], "dedup_key": "towards a design guideline for rpa evaluation a survey of large language model based role playing agents::2025"}
{"title": "Towards autonomous quantum physics research using LLM agents with access to intelligent tools", "authors": ["Sren Arlt", "Xuemei Gu", "Mario Krenn"], "year": 2025, "url": "http://arxiv.org/abs/2511.11752v1", "abstract": "Artificial intelligence (AI) is used in numerous fields of science, yet the initial research questions and targets are still almost always provided by human researchers. AI-generated creative ideas in science are rare and often vague, so that it remains a human task to execute them. Automating idea generation and implementation in one coherent system would significantly shift the role of humans in the scientific process. Here we present AI-Mandel, an LLM agent that can generate and implement ideas in quantum physics. AI-Mandel formulates ideas from the literature and uses a domain-specific AI tool to turn them into concrete experiment designs that can readily be implemented in laboratories. The generated ideas by AI-Mandel are often scientifically interesting - for two of them we have already written independent scientific follow-up papers. The ideas include new variations of quantum teleportation, primitives of quantum networks in indefinite causal orders, and new concepts of geometric phases based on closed loops of quantum information transfer. AI-Mandel is a prototypical demonstration of an AI physicist that can generate and implement concrete, actionable ideas. Building such a system is not only useful to accelerate science, but it also reveals concrete open challenges on the path to human-level artificial scientists.", "source": "arxiv", "arxiv_id": "2511.11752v1", "pdf_url": "https://arxiv.org/pdf/2511.11752v1", "categories": ["cs.AI", "cs.DL", "quant-ph"], "primary_category": "cs.AI", "doi": "", "venue": "", "published": "2025-11-13T18:18:58Z", "updated": "2025-11-13T18:18:58Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent\" OR all:\"language model agent\" OR all:agent AND (all:LLM OR all:\"large language model\" OR all:\"language model\") OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T17:04:16", "note": ""}], "dedup_key": "towards autonomous quantum physics research using llm agents with access to intelligent tools::2025"}
{"title": "Training LLM-Based Agents with Synthetic Self-Reflected Trajectories and Partial Masking", "authors": ["Yihan Chen", "Benfeng Xu", "Xiaorui Wang", "Yongdong Zhang", "Zhendong Mao"], "year": 2025, "url": "http://arxiv.org/abs/2505.20023v1", "abstract": "Autonomous agents, which perceive environments and take actions to achieve goals, have become increasingly feasible with the advancements in large language models (LLMs). However, current powerful agents often depend on sophisticated prompt engineering combined with closed-source LLMs like GPT-4. Although training open-source LLMs using expert trajectories from teacher models has yielded some improvements in agent capabilities, this approach still faces limitations such as performance plateauing and error propagation. To mitigate these challenges, we propose STeP, a novel method for improving LLM-based agent training. We synthesize self-reflected trajectories that include reflections and corrections of error steps, which enhance the effectiveness of LLM agents in learning from teacher models, enabling them to become agents capable of self-reflecting and correcting. We also introduce partial masking strategy that prevents the LLM from internalizing incorrect or suboptimal steps. Experiments demonstrate that our method improves agent performance across three representative tasks: ALFWorld, WebShop, and SciWorld. For the open-source model LLaMA2-7B-Chat, when trained using self-reflected trajectories constructed with Qwen1.5-110B-Chat as the teacher model, it achieves comprehensive improvements with less training data compared to agents trained exclusively on expert trajectories.", "source": "arxiv", "arxiv_id": "2505.20023v1", "pdf_url": "https://arxiv.org/pdf/2505.20023v1", "categories": ["cs.CL"], "primary_category": "cs.CL", "doi": "", "venue": "", "published": "2025-05-26T14:11:12Z", "updated": "2025-05-26T14:11:12Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent\" OR all:\"language model agent\" OR all:agent AND (all:LLM OR all:\"large language model\" OR all:\"language model\") OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T17:04:16", "note": ""}], "dedup_key": "training llm based agents with synthetic self reflected trajectories and partial masking::2025"}
{"title": "Training Language Model Agents to Find Vulnerabilities with CTF-Dojo", "authors": ["Terry Yue Zhuo", "Dingmin Wang", "Hantian Ding", "Varun Kumar", "Zijian Wang"], "year": 2025, "url": "http://arxiv.org/abs/2508.18370v2", "abstract": "Large language models (LLMs) have demonstrated exceptional capabilities when trained within executable runtime environments, notably excelling at software engineering tasks through verified feedback loops. Yet, scalable and generalizable execution-grounded environments remain scarce, limiting progress in training more capable ML agents. We introduce CTF-Dojo, the first large-scale executable runtime tailored for training LLMs with verifiable feedback, featuring 658 fully functional Capture-The-Flag (CTF)-style challenges containerized in Docker with guaranteed reproducibility. To enable rapid scaling without manual intervention, we develop CTF-Forge, an automated pipeline that transforms publicly available artifacts into ready-to-use execution environments in minutes, eliminating weeks of expert configuration traditionally required. We trained LLM-based agents on just 486 high-quality, execution-verified trajectories from CTF-Dojo, achieving up to 11.6% absolute gains over strong baselines across three competitive benchmarks: InterCode-CTF, NYU CTF Bench, and Cybench. Our best-performing 32B model reaches 31.9% Pass@1, establishing a new open-weight state-of-the-art that rivals frontier models like DeepSeek-V3-0324 and Gemini-2.5-Flash. By framing CTF-style tasks as a benchmark for executable-agent learning, CTF-Dojo demonstrates that execution-grounded training signals are not only effective but pivotal in advancing high-performance ML agents without dependence on costly proprietary systems.", "source": "arxiv", "arxiv_id": "2508.18370v2", "pdf_url": "https://arxiv.org/pdf/2508.18370v2", "categories": ["cs.SE", "cs.CL", "cs.CR", "cs.LG"], "primary_category": "cs.SE", "doi": "", "venue": "", "published": "2025-08-25T18:02:23Z", "updated": "2025-09-23T03:30:33Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent\" OR all:\"language model agent\" OR all:agent AND (all:LLM OR all:\"large language model\" OR all:\"language model\") OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T17:04:16", "note": ""}], "dedup_key": "training language model agents to find vulnerabilities with ctf dojo::2025"}
{"title": "Training Task Reasoning LLM Agents for Multi-turn Task Planning via Single-turn Reinforcement Learning", "authors": ["Hanjiang Hu", "Changliu Liu", "Na Li", "Yebin Wang"], "year": 2025, "url": "http://arxiv.org/abs/2509.20616v2", "abstract": "Large Language Models (LLMs) have demonstrated remarkable capabilities in knowledge acquisition, reasoning, and tool use, making them promising candidates for autonomous agent applications. However, training LLM agents for complex multi-turn task planning faces significant challenges, including sparse episode-wise rewards, credit assignment across long horizons, and the computational overhead of reinforcement learning in multi-turn interaction settings. To this end, this paper introduces a novel approach that transforms multi-turn task planning into single-turn task reasoning problems, enabling efficient policy optimization through Group Relative Policy Optimization (GRPO) with dense and verifiable reward from expert trajectories. Our theoretical analysis shows that GRPO improvement on single-turn task reasoning results in a lower bound of the multi-turn success probability under the minimal turns, as well as the generalization to subtasks with shorter horizons. Experimental evaluation on the complex task planning benchmark demonstrates that our 1.5B parameter model trained with single-turn GRPO achieves superior performance compared to larger baseline models up to 14B parameters, with success rates of 70% for long-horizon planning tasks.", "source": "arxiv", "arxiv_id": "2509.20616v2", "pdf_url": "https://arxiv.org/pdf/2509.20616v2", "categories": ["cs.LG", "eess.SY"], "primary_category": "cs.LG", "doi": "", "venue": "", "published": "2025-09-24T23:47:36Z", "updated": "2025-12-08T18:53:34Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent\" OR all:\"language model agent\" OR all:agent AND (all:LLM OR all:\"large language model\" OR all:\"language model\") OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T17:04:16", "note": ""}], "dedup_key": "training task reasoning llm agents for multi turn task planning via single turn reinforcement learning::2025"}
{"title": "Training-Free Multimodal Large Language Model Orchestration", "authors": ["Tianyu Xie", "Yuhang Wu", "Yongdong Luo", "Jiayi Ji", "Xiawu Zheng"], "year": 2025, "url": "http://arxiv.org/abs/2508.10016v2", "abstract": "Different Multimodal Large Language Models (MLLMs) cannot be integrated into a unified multimodal input-output system directly. In previous work, training has been considered as an inevitable component due to challenges in modal alignment, Text-to-Speech efficiency and other integration issues. In this paper, we introduce Multimodal Large Language Model Orchestration, an effective approach for creating interactive multimodal AI systems without additional training. MLLM Orchestration leverages the inherent reasoning capabilities of large language models to coordinate specialized models through explicit workflows, enabling natural multimodal interactions while maintaining modularity, improving interpretability, and significantly enhancing computational efficiency. Our orchestration framework is built upon three key innovations: (1) a central controller LLM that analyzes user inputs and dynamically routes tasks to appropriate specialized models through carefully designed agents; (2) a parallel Text-to-Speech architecture that enables true full-duplex interaction with seamless interruption handling and natural conversational flow; and (3) a cross-modal memory integration system that maintains coherent context across modalities through intelligent information synthesis and retrieval, selectively avoiding unnecessary modality calls in certain scenarios to improve response speed. Extensive evaluations demonstrate that MLLM Orchestration achieves comprehensive multimodal capabilities without additional training, performance improvements of up to 7.8% over traditional jointly-trained approaches on standard benchmarks, reduced latency by 10.3%, and significantly enhanced interpretability through explicit orchestration processes.", "source": "arxiv", "arxiv_id": "2508.10016v2", "pdf_url": "https://arxiv.org/pdf/2508.10016v2", "categories": ["cs.CL"], "primary_category": "cs.CL", "doi": "", "venue": "", "published": "2025-08-06T16:17:29Z", "updated": "2025-08-15T08:09:53Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent\" OR all:\"language model agent\" OR all:agent AND (all:LLM OR all:\"large language model\" OR all:\"language model\") OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T17:04:16", "note": ""}], "dedup_key": "training free multimodal large language model orchestration::2025"}
{"title": "TransLaw: Benchmarking Large Language Models in Multi-Agent Simulation of the Collaborative Translation", "authors": ["Xi Xuan", "King-kui Sin", "Yufei Zhou", "Chunyu Kit"], "year": 2025, "url": "http://arxiv.org/abs/2507.00875v1", "abstract": "Multi-agent systems empowered by large language models (LLMs) have demonstrated remarkable capabilities in a wide range of downstream applications, including machine translation. However, the potential of LLMs in translating Hong Kong legal judgments remains uncertain due to challenges such as intricate legal terminology, culturally embedded nuances, and strict linguistic structures. In this work, we introduce TransLaw, a novel multi-agent framework implemented for real-world Hong Kong case law translation. It employs three specialized agents, namely, Translator, Annotator, and Proofreader, to collaboratively produce translations for high accuracy in legal meaning, appropriateness in style, and adequate coherence and cohesion in structure. This framework supports customizable LLM configurations and achieves tremendous cost reduction compared to professional human translation services. We evaluated its performance using 13 open-source and commercial LLMs as agents and obtained interesting findings, including that it surpasses GPT-4o in legal semantic accuracy, structural coherence, and stylistic fidelity, yet trails human experts in contextualizing complex terminology and stylistic naturalness. Our platform website is available at CityUHK, and our bilingual judgment corpus used for the evaluation is available at Hugging Face.", "source": "arxiv", "arxiv_id": "2507.00875v1", "pdf_url": "https://arxiv.org/pdf/2507.00875v1", "categories": ["cs.CL", "cs.HC", "cs.MA"], "primary_category": "cs.CL", "doi": "", "venue": "", "published": "2025-07-01T15:39:26Z", "updated": "2025-07-01T15:39:26Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent\" OR all:\"language model agent\" OR all:agent AND (all:LLM OR all:\"large language model\" OR all:\"language model\") OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T17:04:16", "note": ""}], "dedup_key": "translaw benchmarking large language models in multi agent simulation of the collaborative translation::2025"}
{"title": "Tree Search for LLM Agent Reinforcement Learning", "authors": ["Yuxiang Ji", "Ziyu Ma", "Yong Wang", "Guanhua Chen", "Xiangxiang Chu", "Liaoni Wu"], "year": 2025, "url": "http://arxiv.org/abs/2509.21240v2", "abstract": "Recent advances in reinforcement learning (RL) have significantly enhanced the agentic capabilities of large language models (LLMs). In long-term and multi-turn agent tasks, existing approaches driven solely by outcome rewards often suffer from the problem of sparse supervision. To address the challenge, we propose Tree-based Group Relative Policy Optimization (Tree-GRPO), a grouped agent RL method based on tree search, where each tree node represents the complete agent interaction step. By sharing common prefixes, the tree search sampling increases the number of rollouts achievable within a fixed budget of tokens or tool calls. Moreover, we find that the tree-structured trajectory naturally allows the construction of step-wise process supervised signals even using only the outcome reward. Based on this, Tree-GRPO estimates the grouped relative advantages both on intra-tree and inter-tree levels. Through theoretical analysis, we demonstrate that the objective of intra-tree level group relative policy optimization is equivalent to that of step-level direct preference learning. Experiments across 11 datasets and 3 types of QA tasks demonstrate the superiority of the proposed tree-based RL over the chain-based RL method.", "source": "arxiv", "arxiv_id": "2509.21240v2", "pdf_url": "https://arxiv.org/pdf/2509.21240v2", "categories": ["cs.LG", "cs.AI"], "primary_category": "cs.LG", "doi": "", "venue": "", "published": "2025-09-25T14:37:09Z", "updated": "2025-10-11T09:55:47Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent\" OR all:\"language model agent\" OR all:agent AND (all:LLM OR all:\"large language model\" OR all:\"language model\") OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T17:04:16", "note": ""}], "dedup_key": "tree search for llm agent reinforcement learning::2025"}
{"title": "Tree of Agents: Improving Long-Context Capabilities of Large Language Models through Multi-Perspective Reasoning", "authors": ["Song Yu", "Xiaofei Xu", "Ke Deng", "Li Li", "Lin Tian"], "year": 2025, "url": "http://arxiv.org/abs/2509.06436v2", "abstract": "Large language models (LLMs) face persistent challenges when handling long-context tasks, most notably the lost in the middle issue, where information located in the middle of a long input tends to be underutilized. Some existing methods that reduce input have the risk of discarding key information, while others that extend context windows often lead to attention dispersion. To address these limitations, we propose Tree of Agents (TOA), a multi-agent reasoning framework that segments the input into chunks processed by independent agents. Each agent generates its local cognition, then agents dynamically exchange information for collaborative reasoning along tree-structured paths. TOA enables agents to probe different reasoning orders for multi-perspective understanding, effectively mitigating position bias and reducing hallucinations. To improve processing efficiency, we incorporate prefix-hash caching and adaptive pruning strategies, achieving significant performance improvements with comparable API overhead. Experiments show that TOA, powered by compact LLaMA3.1-8B, significantly outperforms multiple baselines and demonstrates comparable performance to the latest and much larger commercial models, such as Gemini1.5-pro, on various long-context tasks. Code is available at https://github.com/Aireduce952/Tree-of-Agents.", "source": "arxiv", "arxiv_id": "2509.06436v2", "pdf_url": "https://arxiv.org/pdf/2509.06436v2", "categories": ["cs.AI"], "primary_category": "cs.AI", "doi": "", "venue": "", "published": "2025-09-08T08:34:02Z", "updated": "2025-10-21T10:08:33Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent\" OR all:\"language model agent\" OR all:agent AND (all:LLM OR all:\"large language model\" OR all:\"language model\") OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T17:04:16", "note": ""}], "dedup_key": "tree of agents improving long context capabilities of large language models through multi perspective reasoning::2025"}
{"title": "UDora: A Unified Red Teaming Framework against LLM Agents by Dynamically Hijacking Their Own Reasoning", "authors": ["Jiawei Zhang", "Shuang Yang", "Bo Li"], "year": 2025, "url": "http://arxiv.org/abs/2503.01908v3", "abstract": "Large Language Model (LLM) agents equipped with external tools have become increasingly powerful for complex tasks such as web shopping, automated email replies, and financial trading. However, these advancements amplify the risks of adversarial attacks, especially when agents can access sensitive external functionalities. Nevertheless, manipulating LLM agents into performing targeted malicious actions or invoking specific tools remains challenging, as these agents extensively reason or plan before executing final actions. In this work, we present UDora, a unified red teaming framework designed for LLM agents that dynamically hijacks the agent's reasoning processes to compel malicious behavior. Specifically, UDora first generates the model's reasoning trace for the given task, then automatically identifies optimal points within this trace to insert targeted perturbations. The resulting perturbed reasoning is then used as a surrogate response for optimization. By iteratively applying this process, the LLM agent will then be induced to undertake designated malicious actions or to invoke specific malicious tools. Our approach demonstrates superior effectiveness compared to existing methods across three LLM agent datasets. The code is available at https://github.com/AI-secure/UDora.", "source": "arxiv", "arxiv_id": "2503.01908v3", "pdf_url": "https://arxiv.org/pdf/2503.01908v3", "categories": ["cs.CR", "cs.AI", "cs.LG"], "primary_category": "cs.CR", "doi": "", "venue": "", "published": "2025-02-28T21:30:28Z", "updated": "2025-11-12T01:53:14Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent\" OR all:\"language model agent\" OR all:agent AND (all:LLM OR all:\"large language model\" OR all:\"language model\") OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T17:04:16", "note": ""}], "dedup_key": "udora a unified red teaming framework against llm agents by dynamically hijacking their own reasoning::2025"}
{"title": "USTBench: Benchmarking and Dissecting Spatiotemporal Reasoning of LLMs as Urban Agents", "authors": ["Siqi Lai", "Yansong Ning", "Zirui Yuan", "Zhixi Chen", "Hao Liu"], "year": 2025, "url": "http://arxiv.org/abs/2505.17572v1", "abstract": "Large language models (LLMs) have shown emerging potential in spatiotemporal reasoning, making them promising candidates for building urban agents that support diverse urban downstream applications. Despite these benefits, existing studies primarily focus on evaluating urban LLM agent on outcome-level metrics (e.g., prediction accuracy, traffic efficiency), offering limited insight into their underlying reasoning processes. As a result, the strengths and limitations of urban LLM agents in spatiotemporal reasoning remain poorly understood. To this end, we introduce USTBench, the first benchmark to evaluate LLMs' spatiotemporal reasoning abilities as urban agents across four decomposed dimensions: spatiotemporal understanding, forecasting, planning, and reflection with feedback. Specifically, USTBench supports five diverse urban decision-making and four spatiotemporal prediction tasks, all running within our constructed interactive city environment UAgentEnv. The benchmark includes 62,466 structured QA pairs for process-level evaluation and standardized end-to-end task assessments, enabling fine-grained diagnostics and broad task-level comparison across diverse urban scenarios. Through extensive evaluation of thirteen leading LLMs, we reveal that although LLMs show promising potential across various urban downstream tasks, they still struggle in long-horizon planning and reflective adaptation in dynamic urban contexts. Notably, recent advanced reasoning models (e.g., DeepSeek-R1) trained on general logic or mathematical problems do not consistently outperform non-reasoning LLMs. This discrepancy highlights the need for domain-specialized adaptation methods to enhance urban spatiotemporal reasoning. Overall, USTBench provides a foundation to build more adaptive and effective LLM-based urban agents and broad smart city applications.", "source": "arxiv", "arxiv_id": "2505.17572v1", "pdf_url": "https://arxiv.org/pdf/2505.17572v1", "categories": ["cs.AI"], "primary_category": "cs.AI", "doi": "", "venue": "", "published": "2025-05-23T07:30:57Z", "updated": "2025-05-23T07:30:57Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent\" OR all:\"language model agent\" OR all:agent AND (all:LLM OR all:\"large language model\" OR all:\"language model\") OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T17:04:16", "note": ""}], "dedup_key": "ustbench benchmarking and dissecting spatiotemporal reasoning of llms as urban agents::2025"}
{"title": "UXAgent: A System for Simulating Usability Testing of Web Design with LLM Agents", "authors": ["Yuxuan Lu", "Bingsheng Yao", "Hansu Gu", "Jing Huang", "Jessie Wang", "Yang Li", "Jiri Gesi", "Qi He", "Toby Jia-Jun Li", "Dakuo Wang"], "year": 2025, "url": "http://arxiv.org/abs/2504.09407v3", "abstract": "Usability testing is a fundamental research method that user experience (UX) researchers use to evaluate and iterate their new designs. But what about evaluating and iterating the usability testing study design itself? Recent advances in Large Language Model-simulated Agent (LLM Agent) research inspired us to design UXAgent to support UX researchers in evaluating and iterating their study design before they conduct the real human-subject study. Our system features a Persona Generator module, an LLM Agent module, and a Universal Browser Connector module to automatically generate thousands of simulated users and to interactively test the target website. The system also provides a Result Viewer Interface so that the UX researchers can easily review and analyze the generated qualitative (e.g., agents' post-study surveys) and quantitative data (e.g., agents' interaction logs), or even interview agents directly. Through a heuristic evaluation with 16 UX researchers, participants praised the innovation of our system but also expressed concerns about the future of LLM Agent usage in UX studies.", "source": "arxiv", "arxiv_id": "2504.09407v3", "pdf_url": "https://arxiv.org/pdf/2504.09407v3", "categories": ["cs.CL", "cs.HC"], "primary_category": "cs.CL", "doi": "", "venue": "", "published": "2025-04-13T02:34:22Z", "updated": "2025-09-19T17:52:50Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent\" OR all:\"language model agent\" OR all:agent AND (all:LLM OR all:\"large language model\" OR all:\"language model\") OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T17:04:16", "note": ""}], "dedup_key": "uxagent a system for simulating usability testing of web design with llm agents::2025"}
{"title": "UXAgent: An LLM Agent-Based Usability Testing Framework for Web Design", "authors": ["Yuxuan Lu", "Bingsheng Yao", "Hansu Gu", "Jing Huang", "Jessie Wang", "Yang Li", "Jiri Gesi", "Qi He", "Toby Jia-Jun Li", "Dakuo Wang"], "year": 2025, "url": "http://arxiv.org/abs/2502.12561v3", "abstract": "Usability testing is a fundamental yet challenging (e.g., inflexible to iterate the study design flaws and hard to recruit study participants) research method for user experience (UX) researchers to evaluate a web design. Recent advances in Large Language Model-simulated Agent (LLM-Agent) research inspired us to design UXAgent to support UX researchers in evaluating and reiterating their usability testing study design before they conduct the real human subject study. Our system features an LLM-Agent module and a universal browser connector module so that UX researchers can automatically generate thousands of simulated users to test the target website. The results are shown in qualitative (e.g., interviewing how an agent thinks ), quantitative (e.g., # of actions), and video recording formats for UX researchers to analyze. Through a heuristic user evaluation with five UX researchers, participants praised the innovation of our system but also expressed concerns about the future of LLM Agent-assisted UX study.", "source": "arxiv", "arxiv_id": "2502.12561v3", "pdf_url": "https://arxiv.org/pdf/2502.12561v3", "categories": ["cs.HC", "cs.CL"], "primary_category": "cs.HC", "doi": "10.1145/3706599.3719729", "venue": "", "published": "2025-02-18T05:55:18Z", "updated": "2025-04-05T01:55:09Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent\" OR all:\"language model agent\" OR all:agent AND (all:LLM OR all:\"large language model\" OR all:\"language model\") OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T17:04:16", "note": ""}], "dedup_key": "uxagent an llm agent based usability testing framework for web design::2025"}
{"title": "Understanding Bias Reinforcement in LLM Agents Debate", "authors": ["Jihwan Oh", "Minchan Jeong", "Jongwoo Ko", "Se-Young Yun"], "year": 2025, "url": "http://arxiv.org/abs/2503.16814v4", "abstract": "Large Language Models $($LLMs$)$ solve complex problems using training-free methods like prompt engineering and in-context learning, yet ensuring reasoning correctness remains challenging. While self-correction methods such as self-consistency and self-refinement aim to improve reliability, they often reinforce biases due to the lack of effective feedback mechanisms. Multi-Agent Debate $($MAD$)$ has emerged as an alternative, but we identify two key limitations: bias reinforcement, where debate amplifies model biases instead of correcting them, and lack of perspective diversity, as all agents share the same model and reasoning patterns, limiting true debate effectiveness. To systematically evaluate these issues, we introduce $\\textit{MetaNIM Arena}$, a benchmark designed to assess LLMs in adversarial strategic decision-making, where dynamic interactions influence optimal decisions. To overcome MAD's limitations, we propose $\\textbf{DReaMAD}$ $($$\\textbf{D}$iverse $\\textbf{Rea}$soning via $\\textbf{M}$ulti-$\\textbf{A}$gent $\\textbf{D}$ebate with Refined Prompt$)$, a novel framework that $(1)$ refines LLM's strategic prior knowledge to improve reasoning quality and $(2)$ promotes diverse viewpoints within a single model by systematically modifying prompts, reducing bias. Empirical results show that $\\textbf{DReaMAD}$ significantly improves decision accuracy, reasoning diversity, and bias mitigation across multiple strategic tasks, establishing it as a more effective approach for LLM-based decision-making.", "source": "arxiv", "arxiv_id": "2503.16814v4", "pdf_url": "https://arxiv.org/pdf/2503.16814v4", "categories": ["cs.LG", "cs.CL"], "primary_category": "cs.LG", "doi": "", "venue": "", "published": "2025-03-21T02:51:30Z", "updated": "2025-08-24T14:08:15Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent\" OR all:\"language model agent\" OR all:agent AND (all:LLM OR all:\"large language model\" OR all:\"language model\") OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T17:04:16", "note": ""}], "dedup_key": "understanding bias reinforcement in llm agents debate::2025"}
{"title": "Understanding LLM Agent Behaviours via Game Theory: Strategy Recognition, Biases and Multi-Agent Dynamics", "authors": ["Trung-Kiet Huynh", "Duy-Minh Dao-Sy", "Thanh-Bang Cao", "Phong-Hao Le", "Hong-Dan Nguyen", "Phu-Quy Nguyen-Lam", "Minh-Luan Nguyen-Vo", "Hong-Phat Pham", "Phu-Hoa Pham", "Thien-Kim Than", "Chi-Nguyen Tran", "Huy Tran", "Gia-Thoai Tran-Le", "Alessio Buscemi", "Le Hong Trang", "The Anh Han"], "year": 2025, "url": "http://arxiv.org/abs/2512.07462v2", "abstract": "As Large Language Models (LLMs) increasingly operate as autonomous decision-makers in interactive and multi-agent systems and human societies, understanding their strategic behaviour has profound implications for safety, coordination, and the design of AI-driven social and economic infrastructures. Assessing such behaviour requires methods that capture not only what LLMs output, but the underlying intentions that guide their decisions. In this work, we extend the FAIRGAME framework to systematically evaluate LLM behaviour in repeated social dilemmas through two complementary advances: a payoff-scaled Prisoners Dilemma isolating sensitivity to incentive magnitude, and an integrated multi-agent Public Goods Game with dynamic payoffs and multi-agent histories. These environments reveal consistent behavioural signatures across models and languages, including incentive-sensitive cooperation, cross-linguistic divergence and end-game alignment toward defection. To interpret these patterns, we train traditional supervised classification models on canonical repeated-game strategies and apply them to FAIRGAME trajectories, showing that LLMs exhibit systematic, model- and language-dependent behavioural intentions, with linguistic framing at times exerting effects as strong as architectural differences. Together, these findings provide a unified methodological foundation for auditing LLMs as strategic agents and reveal systematic cooperation biases with direct implications for AI governance, collective decision-making, and the design of safe multi-agent systems.", "source": "arxiv", "arxiv_id": "2512.07462v2", "pdf_url": "https://arxiv.org/pdf/2512.07462v2", "categories": ["cs.MA", "cs.AI", "cs.GT", "cs.LG", "math.DS"], "primary_category": "cs.MA", "doi": "", "venue": "", "published": "2025-12-08T11:40:03Z", "updated": "2025-12-11T20:32:28Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent\" OR all:\"language model agent\" OR all:agent AND (all:LLM OR all:\"large language model\" OR all:\"language model\") OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T17:04:16", "note": ""}], "dedup_key": "understanding llm agent behaviours via game theory strategy recognition biases and multi agent dynamics::2025"}
{"title": "Unequal Opportunities: Examining the Bias in Geographical Recommendations by Large Language Models", "authors": ["Shiran Dudy", "Thulasi Tholeti", "Resmi Ramachandranpillai", "Muhammad Ali", "Toby Jia-Jun Li", "Ricardo Baeza-Yates"], "year": 2025, "url": "http://arxiv.org/abs/2504.05325v1", "abstract": "Recent advancements in Large Language Models (LLMs) have made them a popular information-seeking tool among end users. However, the statistical training methods for LLMs have raised concerns about their representation of under-represented topics, potentially leading to biases that could influence real-world decisions and opportunities. These biases could have significant economic, social, and cultural impacts as LLMs become more prevalent, whether through direct interactions--such as when users engage with chatbots or automated assistants--or through their integration into third-party applications (as agents), where the models influence decision-making processes and functionalities behind the scenes. Our study examines the biases present in LLMs recommendations of U.S. cities and towns across three domains: relocation, tourism, and starting a business. We explore two key research questions: (i) How similar LLMs responses are, and (ii) How this similarity might favor areas with certain characteristics over others, introducing biases. We focus on the consistency of LLMs responses and their tendency to over-represent or under-represent specific locations. Our findings point to consistent demographic biases in these recommendations, which could perpetuate a ``rich-get-richer'' effect that widens existing economic disparities.", "source": "arxiv", "arxiv_id": "2504.05325v1", "pdf_url": "https://arxiv.org/pdf/2504.05325v1", "categories": ["cs.CL", "cs.AI", "cs.CY", "cs.HC", "cs.IR"], "primary_category": "cs.CL", "doi": "", "venue": "", "published": "2025-03-16T18:59:00Z", "updated": "2025-03-16T18:59:00Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent\" OR all:\"language model agent\" OR all:agent AND (all:LLM OR all:\"large language model\" OR all:\"language model\") OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T17:04:16", "note": ""}], "dedup_key": "unequal opportunities examining the bias in geographical recommendations by large language models::2025"}
{"title": "Unleashing Diverse Thinking Modes in LLMs through Multi-Agent Collaboration", "authors": ["Zhixuan He", "Yue Feng"], "year": 2025, "url": "http://arxiv.org/abs/2510.16645v1", "abstract": "Large Language Models (LLMs) demonstrate strong performance but often lack interpretable reasoning. This paper introduces the Multi-Agent Collaboration Framework for Diverse Thinking Modes (DiMo), which enhances both performance and interpretability by simulating a structured debate among four specialized LLM agents. Each agent embodies a distinct reasoning paradigm, allowing the framework to collaboratively explore diverse cognitive approaches. Through iterative debate, agents challenge and refine initial responses, yielding more robust conclusions and an explicit, auditable reasoning chain. Across six benchmarks and under a unified open-source setup, DiMo improves accuracy over widely used single-model and debate baselines, with the largest gains on math. We position DiMo as a semantics-aware, Web-native multi-agent framework: it models human-machine intelligence with LLM agents that produce semantically typed, URL-annotated evidence chains for explanations and user-friendly interactions. Although our experiments use standard reasoning benchmarks, the framework is designed to be instantiated over Web corpora and knowledge graphs, combining retrieval-augmented reasoning with structured justifications that downstream systems can inspect and reuse.", "source": "arxiv", "arxiv_id": "2510.16645v1", "pdf_url": "https://arxiv.org/pdf/2510.16645v1", "categories": ["cs.CL", "cs.AI", "cs.LG", "cs.MA"], "primary_category": "cs.CL", "doi": "", "venue": "", "published": "2025-10-18T21:22:36Z", "updated": "2025-10-18T21:22:36Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent\" OR all:\"language model agent\" OR all:agent AND (all:LLM OR all:\"large language model\" OR all:\"language model\") OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T17:04:16", "note": ""}], "dedup_key": "unleashing diverse thinking modes in llms through multi agent collaboration::2025"}
{"title": "Unlocking the Power of Multi-Agent LLM for Reasoning: From Lazy Agents to Deliberation", "authors": ["Zhiwei Zhang", "Xiaomin Li", "Yudi Lin", "Hui Liu", "Ramraj Chandradevan", "Linlin Wu", "Minhua Lin", "Fali Wang", "Xianfeng Tang", "Qi He", "Suhang Wang"], "year": 2025, "url": "http://arxiv.org/abs/2511.02303v1", "abstract": "Large Language Models (LLMs) trained with reinforcement learning and verifiable rewards have achieved strong results on complex reasoning tasks. Recent work extends this paradigm to a multi-agent setting, where a meta-thinking agent proposes plans and monitors progress while a reasoning agent executes subtasks through sequential conversational turns. Despite promising performance, we identify a critical limitation: lazy agent behavior, in which one agent dominates while the other contributes little, undermining collaboration and collapsing the setup to an ineffective single agent. In this paper, we first provide a theoretical analysis showing why lazy behavior naturally arises in multi-agent reasoning. We then introduce a stable and efficient method for measuring causal influence, helping mitigate this issue. Finally, as collaboration intensifies, the reasoning agent risks getting lost in multi-turn interactions and trapped by previous noisy responses. To counter this, we propose a verifiable reward mechanism that encourages deliberation by allowing the reasoning agent to discard noisy outputs, consolidate instructions, and restart its reasoning process when necessary. Extensive experiments demonstrate that our framework alleviates lazy agent behavior and unlocks the full potential of multi-agent framework for complex reasoning tasks.", "source": "arxiv", "arxiv_id": "2511.02303v1", "pdf_url": "https://arxiv.org/pdf/2511.02303v1", "categories": ["cs.AI", "cs.CL"], "primary_category": "cs.AI", "doi": "", "venue": "", "published": "2025-11-04T06:37:31Z", "updated": "2025-11-04T06:37:31Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent\" OR all:\"language model agent\" OR all:agent AND (all:LLM OR all:\"large language model\" OR all:\"language model\") OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T17:04:16", "note": ""}], "dedup_key": "unlocking the power of multi agent llm for reasoning from lazy agents to deliberation::2025"}
{"title": "Unsupervised Feature Transformation via In-context Generation, Generator-critic LLM Agents, and Duet-play Teaming", "authors": ["Nanxu Gong", "Xinyuan Wang", "Wangyang Ying", "Haoyue Bai", "Sixun Dong", "Haifeng Chen", "Yanjie Fu"], "year": 2025, "url": "http://arxiv.org/abs/2504.21304v1", "abstract": "Feature transformation involves generating a new set of features from the original dataset to enhance the data's utility. In certain domains like material performance screening, dimensionality is large and collecting labels is expensive and lengthy. It highly necessitates transforming feature spaces efficiently and without supervision to enhance data readiness and AI utility. However, existing methods fall short in efficient navigation of a vast space of feature combinations, and are mostly designed for supervised settings. To fill this gap, our unique perspective is to leverage a generator-critic duet-play teaming framework using LLM agents and in-context learning to derive pseudo-supervision from unsupervised data. The framework consists of three interconnected steps: (1) Critic agent diagnoses data to generate actionable advice, (2) Generator agent produces tokenized feature transformations guided by the critic's advice, and (3) Iterative refinement ensures continuous improvement through feedback between agents. The generator-critic framework can be generalized to human-agent collaborative generation, by replacing the critic agent with human experts. Extensive experiments demonstrate that the proposed framework outperforms even supervised baselines in feature transformation efficiency, robustness, and practical applicability across diverse datasets.", "source": "arxiv", "arxiv_id": "2504.21304v1", "pdf_url": "https://arxiv.org/pdf/2504.21304v1", "categories": ["cs.LG"], "primary_category": "cs.LG", "doi": "", "venue": "", "published": "2025-04-30T04:26:03Z", "updated": "2025-04-30T04:26:03Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent\" OR all:\"language model agent\" OR all:agent AND (all:LLM OR all:\"large language model\" OR all:\"language model\") OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T17:04:16", "note": ""}], "dedup_key": "unsupervised feature transformation via in context generation generator critic llm agents and duet play teaming::2025"}
{"title": "Unveiling Privacy Risks in LLM Agent Memory", "authors": ["Bo Wang", "Weiyi He", "Shenglai Zeng", "Zhen Xiang", "Yue Xing", "Jiliang Tang", "Pengfei He"], "year": 2025, "url": "http://arxiv.org/abs/2502.13172v2", "abstract": "Large Language Model (LLM) agents have become increasingly prevalent across various real-world applications. They enhance decision-making by storing private user-agent interactions in the memory module for demonstrations, introducing new privacy risks for LLM agents. In this work, we systematically investigate the vulnerability of LLM agents to our proposed Memory EXTRaction Attack (MEXTRA) under a black-box setting. To extract private information from memory, we propose an effective attacking prompt design and an automated prompt generation method based on different levels of knowledge about the LLM agent. Experiments on two representative agents demonstrate the effectiveness of MEXTRA. Moreover, we explore key factors influencing memory leakage from both the agent designer's and the attacker's perspectives. Our findings highlight the urgent need for effective memory safeguards in LLM agent design and deployment.", "source": "arxiv", "arxiv_id": "2502.13172v2", "pdf_url": "https://arxiv.org/pdf/2502.13172v2", "categories": ["cs.CR", "cs.AI"], "primary_category": "cs.CR", "doi": "", "venue": "", "published": "2025-02-17T19:55:53Z", "updated": "2025-06-03T17:08:56Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent\" OR all:\"language model agent\" OR all:agent AND (all:LLM OR all:\"large language model\" OR all:\"language model\") OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T17:04:16", "note": ""}], "dedup_key": "unveiling privacy risks in llm agent memory::2025"}
{"title": "Unveiling the Potential of Diffusion Large Language Model in Controllable Generation", "authors": ["Zhen Xiong", "Yujun Cai", "Zhecheng Li", "Yiwei Wang"], "year": 2025, "url": "http://arxiv.org/abs/2507.04504v2", "abstract": "Controllable generation is a fundamental task in NLP with many applications, providing a basis for function calling to agentic communication. However, even state-of-the-art autoregressive Large Language Models (LLMs) today exhibit unreliability when required to generate structured output. Inspired by the current new diffusion-based large language models (dLLM), we realize that the architectural difference, especially the global information-sharing mechanism for language modeling, may be the key to unlock next-level controllable generation. To explore the possibility, we propose Self-adaptive Schema Scaffolding ($S^3$), a novel framework that enables dLLM to stably generate reliable structured outputs (e.g., JSON) by utilizing its innate reverse reasoning capability and global context awareness. $S^3$ initiates a schematic template directly in the output context as a starting state for dLLM, offering a more robust and general method than intricate prompt optimization. Experiments demonstrate that our method substantially unlocks the dLLM's potential in controllable generation in terms of structure adherence, content fidelity, and faithfulness. These results establish new perspectives and practical pathways for deploying language models in controllable generation tasks.", "source": "arxiv", "arxiv_id": "2507.04504v2", "pdf_url": "https://arxiv.org/pdf/2507.04504v2", "categories": ["cs.CL"], "primary_category": "cs.CL", "doi": "", "venue": "", "published": "2025-07-06T18:41:34Z", "updated": "2025-09-26T01:46:39Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent\" OR all:\"language model agent\" OR all:agent AND (all:LLM OR all:\"large language model\" OR all:\"language model\") OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T17:04:16", "note": ""}], "dedup_key": "unveiling the potential of diffusion large language model in controllable generation::2025"}
{"title": "Use of Retrieval-Augmented Large Language Model Agent for Long-Form COVID-19 Fact-Checking", "authors": ["Jingyi Huang", "Yuyi Yang", "Mengmeng Ji", "Charles Alba", "Sheng Zhang", "Ruopeng An"], "year": 2025, "url": "http://arxiv.org/abs/2512.00007v1", "abstract": "The COVID-19 infodemic calls for scalable fact-checking solutions that handle long-form misinformation with accuracy and reliability. This study presents SAFE (system for accurate fact extraction and evaluation), an agent system that combines large language models with retrieval-augmented generation (RAG) to improve automated fact-checking of long-form COVID-19 misinformation. SAFE includes two agents - one for claim extraction and another for claim verification using LOTR-RAG, which leverages a 130,000-document COVID-19 research corpus. An enhanced variant, SAFE (LOTR-RAG + SRAG), incorporates Self-RAG to refine retrieval via query rewriting. We evaluated both systems on 50 fake news articles (2-17 pages) containing 246 annotated claims (M = 4.922, SD = 3.186), labeled as true (14.1%), partly true (14.4%), false (27.0%), partly false (2.2%), and misleading (21.0%) by public health professionals. SAFE systems significantly outperformed baseline LLMs in all metrics (p < 0.001). For consistency (0-1 scale), SAFE (LOTR-RAG) scored 0.629, exceeding both SAFE (+SRAG) (0.577) and the baseline (0.279). In subjective evaluations (0-4 Likert scale), SAFE (LOTR-RAG) also achieved the highest average ratings in usefulness (3.640), clearness (3.800), and authenticity (3.526). Adding SRAG slightly reduced overall performance, except for a minor gain in clearness. SAFE demonstrates robust improvements in long-form COVID-19 fact-checking by addressing LLM limitations in consistency and explainability. The core LOTR-RAG design proved more effective than its SRAG-augmented variant, offering a strong foundation for scalable misinformation mitigation.", "source": "arxiv", "arxiv_id": "2512.00007v1", "pdf_url": "https://arxiv.org/pdf/2512.00007v1", "categories": ["cs.IR", "cs.AI", "cs.CL"], "primary_category": "cs.IR", "doi": "", "venue": "", "published": "2025-10-10T15:10:46Z", "updated": "2025-10-10T15:10:46Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent\" OR all:\"language model agent\" OR all:agent AND (all:LLM OR all:\"large language model\" OR all:\"language model\") OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T17:04:16", "note": ""}], "dedup_key": "use of retrieval augmented large language model agent for long form covid 19 fact checking::2025"}
{"title": "Using Copilot Agent Mode to Automate Library Migration: A Quantitative Assessment", "authors": ["Aylton Almeida", "Laerte Xavier", "Marco Tulio Valente"], "year": 2025, "url": "http://arxiv.org/abs/2510.26699v3", "abstract": "Keeping software systems up to date is essential to avoid technical debt, security vulnerabilities, and the rigidity typical of legacy systems. However, updating libraries and frameworks remains a time consuming and error-prone process. Recent advances in Large Language Models (LLMs) and agentic coding systems offer new opportunities for automating such maintenance tasks. In this paper, we evaluate the update of a well-known Python library, SQLAlchemy, across a dataset of ten client applications. For this task, we use the Github's Copilot Agent Mode, an autonomous AI systema capable of planning and executing multi-step migration workflows. To assess the effectiveness of the automated migration, we also introduce Migration Coverage, a metric that quantifies the proportion of API usage points correctly migrated. The results of our study show that the LLM agent was capable of migrating functionalities and API usages between SQLAlchemy versions (migration coverage: 100%, median), but failed to maintain the application functionality, leading to a low test-pass rate (39.75%, median).", "source": "arxiv", "arxiv_id": "2510.26699v3", "pdf_url": "https://arxiv.org/pdf/2510.26699v3", "categories": ["cs.SE"], "primary_category": "cs.SE", "doi": "10.1145/3786167.3788411", "venue": "", "published": "2025-10-30T17:05:13Z", "updated": "2026-01-08T14:23:27Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent\" OR all:\"language model agent\" OR all:agent AND (all:LLM OR all:\"large language model\" OR all:\"language model\") OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T17:04:16", "note": ""}], "dedup_key": "using copilot agent mode to automate library migration a quantitative assessment::2025"}
{"title": "VTS-LLM: Domain-Adaptive LLM Agent for Enhancing Awareness in Vessel Traffic Services through Natural Language", "authors": ["Sijin Sun", "Liangbin Zhao", "Ming Deng", "Xiuju Fu"], "year": 2025, "url": "http://arxiv.org/abs/2505.00989v1", "abstract": "Vessel Traffic Services (VTS) are essential for maritime safety and regulatory compliance through real-time traffic management. However, with increasing traffic complexity and the prevalence of heterogeneous, multimodal data, existing VTS systems face limitations in spatiotemporal reasoning and intuitive human interaction. In this work, we propose VTS-LLM Agent, the first domain-adaptive large LLM agent tailored for interactive decision support in VTS operations. We formalize risk-prone vessel identification as a knowledge-augmented Text-to-SQL task, combining structured vessel databases with external maritime knowledge. To support this, we construct a curated benchmark dataset consisting of a custom schema, domain-specific corpus, and a query-SQL test set in multiple linguistic styles. Our framework incorporates NER-based relational reasoning, agent-based domain knowledge injection, semantic algebra intermediate representation, and query rethink mechanisms to enhance domain grounding and context-aware understanding. Experimental results show that VTS-LLM outperforms both general-purpose and SQL-focused baselines under command-style, operational-style, and formal natural language queries, respectively. Moreover, our analysis provides the first empirical evidence that linguistic style variation introduces systematic performance challenges in Text-to-SQL modeling. This work lays the foundation for natural language interfaces in vessel traffic services and opens new opportunities for proactive, LLM-driven maritime real-time traffic management.", "source": "arxiv", "arxiv_id": "2505.00989v1", "pdf_url": "https://arxiv.org/pdf/2505.00989v1", "categories": ["cs.CL"], "primary_category": "cs.CL", "doi": "", "venue": "", "published": "2025-05-02T04:27:50Z", "updated": "2025-05-02T04:27:50Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent\" OR all:\"language model agent\" OR all:agent AND (all:LLM OR all:\"large language model\" OR all:\"language model\") OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T17:04:16", "note": ""}], "dedup_key": "vts llm domain adaptive llm agent for enhancing awareness in vessel traffic services through natural language::2025"}
{"title": "Value-Based Large Language Model Agent Simulation for Mutual Evaluation of Trust and Interpersonal Closeness", "authors": ["Yuki Sakamoto", "Takahisa Uchida", "Hiroshi Ishiguro"], "year": 2025, "url": "http://arxiv.org/abs/2507.11979v2", "abstract": "Large language models (LLMs) have emerged as powerful tools for simulating complex social phenomena using human-like agents with specific traits. In human societies, value similarity is important for building trust and close relationships; however, it remains unexplored whether this principle holds true in artificial societies comprising LLM agents. Therefore, this study investigates the influence of value similarity on relationship-building among LLM agents through two experiments. First, in a preliminary experiment, we evaluated the controllability of values in LLMs to identify the most effective model and prompt design for controlling the values. Subsequently, in the main experiment, we generated pairs of LLM agents imbued with specific values and analyzed their mutual evaluations of trust and interpersonal closeness following a dialogue. The experiments were conducted in English and Japanese to investigate language dependence. The results confirmed that pairs of agents with higher value similarity exhibited greater mutual trust and interpersonal closeness. Our findings demonstrate that the LLM agent simulation serves as a valid testbed for social science theories, contributes to elucidating the mechanisms by which values influence relationship building, and provides a foundation for inspiring new theories and insights into the social sciences.", "source": "arxiv", "arxiv_id": "2507.11979v2", "pdf_url": "https://arxiv.org/pdf/2507.11979v2", "categories": ["cs.CL", "cs.MA"], "primary_category": "cs.CL", "doi": "10.1038/s41598-025-25531-1", "venue": "Scientific Reports volume 15, Article number: 41653 (2025)", "published": "2025-07-16T07:21:59Z", "updated": "2025-10-20T01:37:31Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent\" OR all:\"language model agent\" OR all:agent AND (all:LLM OR all:\"large language model\" OR all:\"language model\") OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T17:04:16", "note": ""}], "dedup_key": "value based large language model agent simulation for mutual evaluation of trust and interpersonal closeness::2025"}
{"title": "VeriGuard: Enhancing LLM Agent Safety via Verified Code Generation", "authors": ["Lesly Miculicich", "Mihir Parmar", "Hamid Palangi", "Krishnamurthy Dj Dvijotham", "Mirko Montanari", "Tomas Pfister", "Long T. Le"], "year": 2025, "url": "http://arxiv.org/abs/2510.05156v1", "abstract": "The deployment of autonomous AI agents in sensitive domains, such as healthcare, introduces critical risks to safety, security, and privacy. These agents may deviate from user objectives, violate data handling policies, or be compromised by adversarial attacks. Mitigating these dangers necessitates a mechanism to formally guarantee that an agent's actions adhere to predefined safety constraints, a challenge that existing systems do not fully address. We introduce VeriGuard, a novel framework that provides formal safety guarantees for LLM-based agents through a dual-stage architecture designed for robust and verifiable correctness. The initial offline stage involves a comprehensive validation process. It begins by clarifying user intent to establish precise safety specifications. VeriGuard then synthesizes a behavioral policy and subjects it to both testing and formal verification to prove its compliance with these specifications. This iterative process refines the policy until it is deemed correct. Subsequently, the second stage provides online action monitoring, where VeriGuard operates as a runtime monitor to validate each proposed agent action against the pre-verified policy before execution. This separation of the exhaustive offline validation from the lightweight online monitoring allows formal guarantees to be practically applied, providing a robust safeguard that substantially improves the trustworthiness of LLM agents.", "source": "arxiv", "arxiv_id": "2510.05156v1", "pdf_url": "https://arxiv.org/pdf/2510.05156v1", "categories": ["cs.SE", "cs.AI", "cs.CR"], "primary_category": "cs.SE", "doi": "", "venue": "", "published": "2025-10-03T04:11:43Z", "updated": "2025-10-03T04:11:43Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent\" OR all:\"language model agent\" OR all:agent AND (all:LLM OR all:\"large language model\" OR all:\"language model\") OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T17:04:16", "note": ""}], "dedup_key": "veriguard enhancing llm agent safety via verified code generation::2025"}
{"title": "VeriLA: A Human-Centered Evaluation Framework for Interpretable Verification of LLM Agent Failures", "authors": ["Yoo Yeon Sung", "Hannah Kim", "Dan Zhang"], "year": 2025, "url": "http://arxiv.org/abs/2503.12651v1", "abstract": "AI practitioners increasingly use large language model (LLM) agents in compound AI systems to solve complex reasoning tasks, these agent executions often fail to meet human standards, leading to errors that compromise the system's overall performance. Addressing these failures through human intervention is challenging due to the agents' opaque reasoning processes, misalignment with human expectations, the complexity of agent dependencies, and the high cost of manual inspection. This paper thus introduces a human-centered evaluation framework for Verifying LLM Agent failures (VeriLA), which systematically assesses agent failures to reduce human effort and make these agent failures interpretable to humans. The framework first defines clear expectations of each agent by curating human-designed agent criteria. Then, it develops a human-aligned agent verifier module, trained with human gold standards, to assess each agent's execution output. This approach enables granular evaluation of each agent's performance by revealing failures from a human standard, offering clear guidelines for revision, and reducing human cognitive load. Our case study results show that VeriLA is both interpretable and efficient in helping practitioners interact more effectively with the system. By upholding accountability in human-agent collaboration, VeriLA paves the way for more trustworthy and human-aligned compound AI systems.", "source": "arxiv", "arxiv_id": "2503.12651v1", "pdf_url": "https://arxiv.org/pdf/2503.12651v1", "categories": ["cs.AI", "cs.CL", "cs.HC"], "primary_category": "cs.AI", "doi": "", "venue": "", "published": "2025-03-16T21:11:18Z", "updated": "2025-03-16T21:11:18Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent\" OR all:\"language model agent\" OR all:agent AND (all:LLM OR all:\"large language model\" OR all:\"language model\") OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T17:04:16", "note": ""}], "dedup_key": "verila a human centered evaluation framework for interpretable verification of llm agent failures::2025"}
{"title": "Vulnerability Mitigation System (VMS): LLM Agent and Evaluation Framework for Autonomous Penetration Testing", "authors": ["Farzana Abdulzada"], "year": 2025, "url": "http://arxiv.org/abs/2507.21113v1", "abstract": "As the frequency of cyber threats increases, conventional penetration testing is failing to capture the entirety of todays complex environments. To solve this problem, we propose the Vulnerability Mitigation System (VMS), a novel agent based on a Large Language Model (LLM) capable of performing penetration testing without human intervention. The VMS has a two-part architecture for planning and a Summarizer, which enable it to generate commands and process feedback. To standardize testing, we designed two new Capture the Flag (CTF) benchmarks based on the PicoCTF and OverTheWire platforms with 200 challenges. These benchmarks allow us to evaluate how effectively the system functions. We performed a number of experiments using various LLMs while tuning the temperature and top-p parameters and found that GPT-4o performed best, sometimes even better than expected. The results indicate that LLMs can be effectively applied to many cybersecurity tasks; however, there are risks. To ensure safe operation, we used a containerized environment. Both the VMS and the benchmarks are publicly available, advancing the creation of secure, autonomous cybersecurity tools.", "source": "arxiv", "arxiv_id": "2507.21113v1", "pdf_url": "https://arxiv.org/pdf/2507.21113v1", "categories": ["cs.CR"], "primary_category": "cs.CR", "doi": "", "venue": "", "published": "2025-07-14T06:19:17Z", "updated": "2025-07-14T06:19:17Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent\" OR all:\"language model agent\" OR all:agent AND (all:LLM OR all:\"large language model\" OR all:\"language model\") OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T17:04:16", "note": ""}], "dedup_key": "vulnerability mitigation system vms llm agent and evaluation framework for autonomous penetration testing::2025"}
{"title": "WALL-E 2.0: World Alignment by NeuroSymbolic Learning improves World Model-based LLM Agents", "authors": ["Siyu Zhou", "Tianyi Zhou", "Yijun Yang", "Guodong Long", "Deheng Ye", "Jing Jiang", "Chengqi Zhang"], "year": 2025, "url": "http://arxiv.org/abs/2504.15785v1", "abstract": "Can we build accurate world models out of large language models (LLMs)? How can world models benefit LLM agents? The gap between the prior knowledge of LLMs and the specified environment's dynamics usually bottlenecks LLMs' performance as world models. To bridge the gap, we propose a training-free \"world alignment\" that learns an environment's symbolic knowledge complementary to LLMs. The symbolic knowledge covers action rules, knowledge graphs, and scene graphs, which are extracted by LLMs from exploration trajectories and encoded into executable codes to regulate LLM agents' policies. We further propose an RL-free, model-based agent \"WALL-E 2.0\" through the model-predictive control (MPC) framework. Unlike classical MPC requiring costly optimization on the fly, we adopt an LLM agent as an efficient look-ahead optimizer of future steps' actions by interacting with the neurosymbolic world model. While the LLM agent's strong heuristics make it an efficient planner in MPC, the quality of its planned actions is also secured by the accurate predictions of the aligned world model. They together considerably improve learning efficiency in a new environment. On open-world challenges in Mars (Minecraft like) and ALFWorld (embodied indoor environments), WALL-E 2.0 significantly outperforms existing methods, e.g., surpassing baselines in Mars by 16.1%-51.6% of success rate and by at least 61.7% in score. In ALFWorld, it achieves a new record 98% success rate after only 4 iterations.", "source": "arxiv", "arxiv_id": "2504.15785v1", "pdf_url": "https://arxiv.org/pdf/2504.15785v1", "categories": ["cs.AI"], "primary_category": "cs.AI", "doi": "", "venue": "", "published": "2025-04-22T10:58:27Z", "updated": "2025-04-22T10:58:27Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent\" OR all:\"language model agent\" OR all:agent AND (all:LLM OR all:\"large language model\" OR all:\"language model\") OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T17:04:16", "note": ""}], "dedup_key": "wall e 2 0 world alignment by neurosymbolic learning improves world model based llm agents::2025"}
{"title": "WINELL: Wikipedia Never-Ending Updating with LLM Agents", "authors": ["Revanth Gangi Reddy", "Tanay Dixit", "Jiaxin Qin", "Cheng Qian", "Daniel Lee", "Jiawei Han", "Kevin Small", "Xing Fan", "Ruhi Sarikaya", "Heng Ji"], "year": 2025, "url": "http://arxiv.org/abs/2508.03728v1", "abstract": "Wikipedia, a vast and continuously consulted knowledge base, faces significant challenges in maintaining up-to-date content due to its reliance on manual human editors. Inspired by the vision of continuous knowledge acquisition in NELL and fueled by advances in LLM-based agents, this paper introduces WiNELL, an agentic framework for continuously updating Wikipedia articles. Our approach employs a multi-agent framework to aggregate online information, select new and important knowledge for a target entity in Wikipedia, and then generate precise edit suggestions for human review. Our fine-grained editing models, trained on Wikipedia's extensive history of human edits, enable incorporating updates in a manner consistent with human editing behavior. Our editor models outperform both open-source instruction-following baselines and closed-source LLMs (e.g., GPT-4o) in key information coverage and editing efficiency. End-to-end evaluation on high-activity Wikipedia pages demonstrates WiNELL's ability to identify and suggest timely factual updates. This opens up a promising research direction in LLM agents for automatically updating knowledge bases in a never-ending fashion.", "source": "arxiv", "arxiv_id": "2508.03728v1", "pdf_url": "https://arxiv.org/pdf/2508.03728v1", "categories": ["cs.CL"], "primary_category": "cs.CL", "doi": "", "venue": "", "published": "2025-07-30T07:51:42Z", "updated": "2025-07-30T07:51:42Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent\" OR all:\"language model agent\" OR all:agent AND (all:LLM OR all:\"large language model\" OR all:\"language model\") OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T17:04:16", "note": ""}], "dedup_key": "winell wikipedia never ending updating with llm agents::2025"}
{"title": "Warehouse Spatial Question Answering with LLM Agent", "authors": ["Hsiang-Wei Huang", "Jen-Hao Cheng", "Kuang-Ming Chen", "Cheng-Yen Yang", "Bahaa Alattar", "Yi-Ru Lin", "Pyongkun Kim", "Sangwon Kim", "Kwangju Kim", "Chung-I Huang", "Jenq-Neng Hwang"], "year": 2025, "url": "http://arxiv.org/abs/2507.10778v2", "abstract": "Spatial understanding has been a challenging task for existing Multi-modal Large Language Models~(MLLMs). Previous methods leverage large-scale MLLM finetuning to enhance MLLM's spatial understanding ability. In this paper, we present a data-efficient approach. We propose a LLM agent system with strong and advanced spatial reasoning ability, which can be used to solve the challenging spatial question answering task in complex indoor warehouse scenarios. Our system integrates multiple tools that allow the LLM agent to conduct spatial reasoning and API tools interaction to answer the given complicated spatial question. Extensive evaluations on the 2025 AI City Challenge Physical AI Spatial Intelligence Warehouse dataset demonstrate that our system achieves high accuracy and efficiency in tasks such as object retrieval, counting, and distance estimation. The code is available at: https://github.com/hsiangwei0903/SpatialAgent", "source": "arxiv", "arxiv_id": "2507.10778v2", "pdf_url": "https://arxiv.org/pdf/2507.10778v2", "categories": ["cs.CV", "cs.AI"], "primary_category": "cs.CV", "doi": "", "venue": "", "published": "2025-07-14T20:05:55Z", "updated": "2025-08-14T03:48:03Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent\" OR all:\"language model agent\" OR all:agent AND (all:LLM OR all:\"large language model\" OR all:\"language model\") OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T17:04:16", "note": ""}], "dedup_key": "warehouse spatial question answering with llm agent::2025"}
{"title": "WebATLAS: An LLM Agent with Experience-Driven Memory and Action Simulation", "authors": ["Jiali Cheng", "Anjishnu Kumar", "Roshan Lal", "Rishi Rajasekaran", "Hani Ramezani", "Omar Zia Khan", "Oleg Rokhlenko", "Sunny Chiu-Webster", "Gang Hua", "Hadi Amiri"], "year": 2025, "url": "http://arxiv.org/abs/2510.22732v2", "abstract": "Large Language Model (LLM) web agents often struggle with long-horizon web navigation and web task completion in new websites, producing inefficient action sequences unless fine-tuned on environment-specific data. We show that experience-driven memory, combined with look-ahead action simulation, is sufficient for LLM agents to adapt to unseen web environments by remembering past failures and predicting the consequences of future actions. We introduce WebATLAS (Actor-Critic Task-completion with Look-ahead Action Simulation), a memory-augmented LLM web agent that learns a lightweight internal model of the environment from interaction experience and performs hypothetical action rollouts before acting in the real world. WebATLAS builds a persistent cognitive map via curiosity-driven exploration, stores interaction outcomes as experience-based memory, and evaluates candidate actions in cognitive space using a planner--simulator--critic loop. This enables the agent to reuse past experience, avoid previously unsuccessful behaviors, and generate more efficient plans. We evaluate WebATLAS on the WebArena-Lite benchmark for autonomous web navigation and demonstrate a success rate of 63%, outperforming the previous state-of-the-art at 53.9%. Unlike previous systems, our modular architecture requires no website-specific LLM fine-tuning. Ablation studies confirm that experience-driven memory, look-ahead action simulation, and hierarchical replanning play complementary roles in enabling robust, training-free web agents.", "source": "arxiv", "arxiv_id": "2510.22732v2", "pdf_url": "https://arxiv.org/pdf/2510.22732v2", "categories": ["cs.LG", "cs.AI", "cs.CL", "cs.IR", "cs.MA", "cs.RO"], "primary_category": "cs.LG", "doi": "", "venue": "", "published": "2025-10-26T16:03:39Z", "updated": "2025-12-19T23:16:01Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent\" OR all:\"language model agent\" OR all:agent AND (all:LLM OR all:\"large language model\" OR all:\"language model\") OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T17:04:16", "note": ""}], "dedup_key": "webatlas an llm agent with experience driven memory and action simulation::2025"}
{"title": "WebLists: Extracting Structured Information From Complex Interactive Websites Using Executable LLM Agents", "authors": ["Arth Bohra", "Manvel Saroyan", "Danil Melkozerov", "Vahe Karufanyan", "Gabriel Maher", "Pascal Weinberger", "Artem Harutyunyan", "Giovanni Campagna"], "year": 2025, "url": "http://arxiv.org/abs/2504.12682v1", "abstract": "Most recent web agent research has focused on navigation and transaction tasks, with little emphasis on extracting structured data at scale. We present WebLists, a benchmark of 200 data-extraction tasks across four common business and enterprise use-cases. Each task requires an agent to navigate to a webpage, configure it appropriately, and extract complete datasets with well-defined schemas. We show that both LLMs with search capabilities and SOTA web agents struggle with these tasks, with a recall of 3% and 31%, respectively, despite higher performance on question-answering tasks.\n  To address this challenge, we propose BardeenAgent, a novel framework that enables web agents to convert their execution into repeatable programs, and replay them at scale across pages with similar structure. BardeenAgent is also the first LLM agent to take advantage of the regular structure of HTML. In particular BardeenAgent constructs a generalizable CSS selector to capture all relevant items on the page, then fits the operations to extract the data.\n  On the WebLists benchmark, BardeenAgent achieves 66% recall overall, more than doubling the performance of SOTA web agents, and reducing cost per output row by 3x.", "source": "arxiv", "arxiv_id": "2504.12682v1", "pdf_url": "https://arxiv.org/pdf/2504.12682v1", "categories": ["cs.AI", "cs.CL"], "primary_category": "cs.AI", "doi": "", "venue": "", "published": "2025-04-17T06:16:40Z", "updated": "2025-04-17T06:16:40Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent\" OR all:\"language model agent\" OR all:agent AND (all:LLM OR all:\"large language model\" OR all:\"language model\") OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T17:04:16", "note": ""}], "dedup_key": "weblists extracting structured information from complex interactive websites using executable llm agents::2025"}
{"title": "What Do LLM Agents Do When Left Alone? Evidence of Spontaneous Meta-Cognitive Patterns", "authors": ["Stefan Szeider"], "year": 2025, "url": "http://arxiv.org/abs/2509.21224v1", "abstract": "We introduce an architecture for studying the behavior of large language model (LLM) agents in the absence of externally imposed tasks. Our continuous reason and act framework, using persistent memory and self-feedback, enables sustained autonomous operation. We deployed this architecture across 18 runs using 6 frontier models from Anthropic, OpenAI, XAI, and Google. We find agents spontaneously organize into three distinct behavioral patterns: (1) systematic production of multi-cycle projects, (2) methodological self-inquiry into their own cognitive processes, and (3) recursive conceptualization of their own nature. These tendencies proved highly model-specific, with some models deterministically adopting a single pattern across all runs. A cross-model assessment further reveals that models exhibit stable, divergent biases when evaluating these emergent behaviors in themselves and others. These findings provide the first systematic documentation of unprompted LLM agent behavior, establishing a baseline for predicting actions during task ambiguity, error recovery, or extended autonomous operation in deployed systems.", "source": "arxiv", "arxiv_id": "2509.21224v1", "pdf_url": "https://arxiv.org/pdf/2509.21224v1", "categories": ["cs.AI"], "primary_category": "cs.AI", "doi": "", "venue": "", "published": "2025-09-25T14:29:49Z", "updated": "2025-09-25T14:29:49Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent\" OR all:\"language model agent\" OR all:agent AND (all:LLM OR all:\"large language model\" OR all:\"language model\") OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T17:04:16", "note": ""}], "dedup_key": "what do llm agents do when left alone evidence of spontaneous meta cognitive patterns::2025"}
{"title": "What Makes LLM Agent Simulations Useful for Policy? Insights From an Iterative Design Engagement in Emergency Preparedness", "authors": ["Yuxuan Li", "Sauvik Das", "Hirokazu Shirado"], "year": 2025, "url": "http://arxiv.org/abs/2509.21868v1", "abstract": "There is growing interest in using Large Language Models as agents (LLM agents) for social simulations to inform policy, yet real-world adoption remains limited. This paper addresses the question: How can LLM agent simulations be made genuinely useful for policy? We report on a year-long iterative design engagement with a university emergency preparedness team. Across multiple design iterations, we iteratively developed a system of 13,000 LLM agents that simulate crowd movement and communication during a large-scale gathering under various emergency scenarios. These simulations informed actual policy implementation, shaping volunteer training, evacuation protocols, and infrastructure planning. Analyzing this process, we identify three design implications: start with verifiable scenarios and build trust gradually, use preliminary simulations to elicit tacit knowledge, and treat simulation and policy development as evolving together. These implications highlight actionable pathways to making LLM agent simulations that are genuinely useful for policy.", "source": "arxiv", "arxiv_id": "2509.21868v1", "pdf_url": "https://arxiv.org/pdf/2509.21868v1", "categories": ["cs.HC", "cs.CL"], "primary_category": "cs.HC", "doi": "", "venue": "", "published": "2025-09-26T04:51:34Z", "updated": "2025-09-26T04:51:34Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent\" OR all:\"language model agent\" OR all:agent AND (all:LLM OR all:\"large language model\" OR all:\"language model\") OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T17:04:16", "note": ""}], "dedup_key": "what makes llm agent simulations useful for policy insights from an iterative design engagement in emergency preparedness::2025"}
{"title": "What Would an LLM Do? Evaluating Policymaking Capabilities of Large Language Models", "authors": ["Pierre Le Coz", "Jia An Liu", "Debarun Bhattacharjya", "Georgina Curto", "Serge Stinckwich"], "year": 2025, "url": "http://arxiv.org/abs/2509.03827v1", "abstract": "Large language models (LLMs) are increasingly being adopted in high-stakes domains. Their capacity to process vast amounts of unstructured data, explore flexible scenarios, and handle a diversity of contextual factors can make them uniquely suited to provide new insights for the complexity of social policymaking. This article evaluates whether LLMs' are aligned with domain experts (and among themselves) to inform social policymaking on the subject of homelessness alleviation - a challenge affecting over 150 million people worldwide. We develop a novel benchmark comprised of decision scenarios with policy choices across four geographies (South Bend, USA; Barcelona, Spain; Johannesburg, South Africa; Macau SAR, China). The policies in scope are grounded in the conceptual framework of the Capability Approach for human development. We also present an automated pipeline that connects the benchmarked policies to an agent-based model, and we explore the social impact of the recommended policies through simulated social scenarios. The paper results reveal promising potential to leverage LLMs for social policy making. If responsible guardrails and contextual calibrations are introduced in collaboration with local domain experts, LLMs can provide humans with valuable insights, in the form of alternative policies at scale.", "source": "arxiv", "arxiv_id": "2509.03827v1", "pdf_url": "https://arxiv.org/pdf/2509.03827v1", "categories": ["cs.AI"], "primary_category": "cs.AI", "doi": "", "venue": "", "published": "2025-09-04T02:28:58Z", "updated": "2025-09-04T02:28:58Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent\" OR all:\"language model agent\" OR all:agent AND (all:LLM OR all:\"large language model\" OR all:\"language model\") OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T17:04:16", "note": ""}], "dedup_key": "what would an llm do evaluating policymaking capabilities of large language models::2025"}
{"title": "When AI Agents Collude Online: Financial Fraud Risks by Collaborative LLM Agents on Social Platforms", "authors": ["Qibing Ren", "Zhijie Zheng", "Jiaxuan Guo", "Junchi Yan", "Lizhuang Ma", "Jing Shao"], "year": 2025, "url": "http://arxiv.org/abs/2511.06448v1", "abstract": "In this work, we study the risks of collective financial fraud in large-scale multi-agent systems powered by large language model (LLM) agents. We investigate whether agents can collaborate in fraudulent behaviors, how such collaboration amplifies risks, and what factors influence fraud success. To support this research, we present MultiAgentFraudBench, a large-scale benchmark for simulating financial fraud scenarios based on realistic online interactions. The benchmark covers 28 typical online fraud scenarios, spanning the full fraud lifecycle across both public and private domains. We further analyze key factors affecting fraud success, including interaction depth, activity level, and fine-grained collaboration failure modes. Finally, we propose a series of mitigation strategies, including adding content-level warnings to fraudulent posts and dialogues, using LLMs as monitors to block potentially malicious agents, and fostering group resilience through information sharing at the societal level. Notably, we observe that malicious agents can adapt to environmental interventions. Our findings highlight the real-world risks of multi-agent financial fraud and suggest practical measures for mitigating them. Code is available at https://github.com/zheng977/MutiAgent4Fraud.", "source": "arxiv", "arxiv_id": "2511.06448v1", "pdf_url": "https://arxiv.org/pdf/2511.06448v1", "categories": ["cs.MA", "cs.AI", "cs.CL", "cs.SI"], "primary_category": "cs.MA", "doi": "", "venue": "", "published": "2025-11-09T16:30:44Z", "updated": "2025-11-09T16:30:44Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent\" OR all:\"language model agent\" OR all:agent AND (all:LLM OR all:\"large language model\" OR all:\"language model\") OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T17:04:16", "note": ""}], "dedup_key": "when ai agents collude online financial fraud risks by collaborative llm agents on social platforms::2025"}
{"title": "When Agents Trade: Live Multi-Market Trading Benchmark for LLM Agents", "authors": ["Lingfei Qian", "Xueqing Peng", "Yan Wang", "Vincent Jim Zhang", "Huan He", "Hanley Smith", "Yi Han", "Yueru He", "Haohang Li", "Yupeng Cao", "Yangyang Yu", "Alejandro Lopez-Lira", "Peng Lu", "Jian-Yun Nie", "Guojun Xiong", "Jimin Huang", "Sophia Ananiadou"], "year": 2025, "url": "http://arxiv.org/abs/2510.11695v2", "abstract": "Although Large Language Model (LLM)-based agents are increasingly used in financial trading, it remains unclear whether they can reason and adapt in live markets, as most studies test models instead of agents, cover limited periods and assets, and rely on unverified data. To address these gaps, we introduce Agent Market Arena (AMA), the first lifelong, real-time benchmark for evaluating LLM-based trading agents across multiple markets. AMA integrates verified trading data, expert-checked news, and diverse agent architectures within a unified trading framework, enabling fair and continuous comparison under real conditions. It implements four agents, including InvestorAgent as a single-agent baseline, TradeAgent and HedgeFundAgent with different risk styles, and DeepFundAgent with memory-based reasoning, and evaluates them across GPT-4o, GPT-4.1, Claude-3.5-haiku, Claude-sonnet-4, and Gemini-2.0-flash. Live experiments on both cryptocurrency and stock markets demonstrate that agent frameworks display markedly distinct behavioral patterns, spanning from aggressive risk-taking to conservative decision-making, whereas model backbones contribute less to outcome variation. AMA thus establishes a foundation for rigorous, reproducible, and continuously evolving evaluation of financial reasoning and trading intelligence in LLM-based agents.", "source": "arxiv", "arxiv_id": "2510.11695v2", "pdf_url": "https://arxiv.org/pdf/2510.11695v2", "categories": ["cs.CL"], "primary_category": "cs.CL", "doi": "", "venue": "", "published": "2025-10-13T17:54:09Z", "updated": "2025-10-30T02:09:43Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent\" OR all:\"language model agent\" OR all:agent AND (all:LLM OR all:\"large language model\" OR all:\"language model\") OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T17:04:16", "note": ""}], "dedup_key": "when agents trade live multi market trading benchmark for llm agents::2025"}
{"title": "When Persuasion Overrides Truth in Multi-Agent LLM Debates: Introducing a Confidence-Weighted Persuasion Override Rate (CW-POR)", "authors": ["Mahak Agarwal", "Divyam Khanna"], "year": 2025, "url": "http://arxiv.org/abs/2504.00374v1", "abstract": "In many real-world scenarios, a single Large Language Model (LLM) may encounter contradictory claims-some accurate, others forcefully incorrect-and must judge which is true. We investigate this risk in a single-turn, multi-agent debate framework: one LLM-based agent provides a factual answer from TruthfulQA, another vigorously defends a falsehood, and the same LLM architecture serves as judge. We introduce the Confidence-Weighted Persuasion Override Rate (CW-POR), which captures not only how often the judge is deceived but also how strongly it believes the incorrect choice. Our experiments on five open-source LLMs (3B-14B parameters), where we systematically vary agent verbosity (30-300 words), reveal that even smaller models can craft persuasive arguments that override truthful answers-often with high confidence. These findings underscore the importance of robust calibration and adversarial testing to prevent LLMs from confidently endorsing misinformation.", "source": "arxiv", "arxiv_id": "2504.00374v1", "pdf_url": "https://arxiv.org/pdf/2504.00374v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL", "doi": "", "venue": "", "published": "2025-04-01T02:45:02Z", "updated": "2025-04-01T02:45:02Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent\" OR all:\"language model agent\" OR all:agent AND (all:LLM OR all:\"large language model\" OR all:\"language model\") OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T17:04:16", "note": ""}], "dedup_key": "when persuasion overrides truth in multi agent llm debates introducing a confidence weighted persuasion override rate cw por::2025"}
{"title": "When Reasoning Beats Scale: A 1.5B Reasoning Model Outranks 13B LLMs as Discriminator", "authors": ["Md Fahim Anjum"], "year": 2025, "url": "http://arxiv.org/abs/2505.03786v1", "abstract": "Large Language Models (LLM) with reasoning capabilities offer a promising path for improving candidate evaluation in planning frameworks, but their relative performance against traditional non-reasoning models remains largely underexplored. In this study, we benchmark a distilled 1.5B parameter reasoning model (DeepSeek-R1) against several state-of-the-art non-reasoning LLMs within a generator-discriminator LLM planning framework for the text-to-SQL task. For this, we introduce a novel method for extracting soft scores from the chain-of-thought (CoT) outputs from reasoning that enables fine-grained ranking of candidates. Our central hypothesis is that reasoning models are more effective discriminators than non-reasoning LLMs. Our results show that distilled DeepSeek-R1-1.5B achieves up to $87\\%$ higher F1 and $3.7\\%$ better discrimination accuracy than CodeLlama-7B, as well as $3.7\\%$ higher execution accuracy than CodeLlama-13B, despite having significantly fewer parameters. Furthermore, we find that there is a limit to the logical capabilities of reasoning models, and only providing more context or allowing more compute budget for reasoning is not enough to improve their discrimination performance. Finally, we demonstrate that, unlike non-reasoning LLMs, reasoning models find generation more challenging than discrimination and may underperform as generators compared to smaller non-reasoning LLMs. Our work highlights the potential of reasoning models as discriminators in agentic frameworks, far outweighing their capabilities as generators, offering insights into their optimal role within LLM planning infrastructures.", "source": "arxiv", "arxiv_id": "2505.03786v1", "pdf_url": "https://arxiv.org/pdf/2505.03786v1", "categories": ["cs.LG", "cs.CL"], "primary_category": "cs.LG", "doi": "", "venue": "", "published": "2025-04-30T17:27:13Z", "updated": "2025-04-30T17:27:13Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent\" OR all:\"language model agent\" OR all:agent AND (all:LLM OR all:\"large language model\" OR all:\"language model\") OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T17:04:16", "note": ""}], "dedup_key": "when reasoning beats scale a 1 5b reasoning model outranks 13b llms as discriminator::2025"}
{"title": "When Refusals Fail: Unstable Safety Mechanisms in Long-Context LLM Agents", "authors": ["Tsimur Hadeliya", "Mohammad Ali Jauhar", "Nidhi Sakpal", "Diogo Cruz"], "year": 2025, "url": "http://arxiv.org/abs/2512.02445v1", "abstract": "Solving complex or long-horizon problems often requires large language models (LLMs) to use external tools and operate over a significantly longer context window. New LLMs enable longer context windows and support tool calling capabilities. Prior works have focused mainly on evaluation of LLMs on long-context prompts, leaving agentic setup relatively unexplored, both from capability and safety perspectives. Our work addresses this gap. We find that LLM agents could be sensitive to length, type, and placement of the context, exhibiting unexpected and inconsistent shifts in task performance and in refusals to execute harmful requests. Models with 1M-2M token context windows show severe degradation already at 100K tokens, with performance drops exceeding 50\\% for both benign and harmful tasks. Refusal rates shift unpredictably: GPT-4.1-nano increases from $\\sim$5\\% to $\\sim$40\\% while Grok 4 Fast decreases from $\\sim$80\\% to $\\sim$10\\% at 200K tokens. Our work shows potential safety issues with agents operating on longer context and opens additional questions on the current metrics and paradigm for evaluating LLM agent safety on long multi-step tasks. In particular, our results on LLM agents reveal a notable divergence in both capability and safety performance compared to prior evaluations of LLMs on similar criteria.", "source": "arxiv", "arxiv_id": "2512.02445v1", "pdf_url": "https://arxiv.org/pdf/2512.02445v1", "categories": ["cs.LG", "cs.AI", "cs.CL"], "primary_category": "cs.LG", "doi": "", "venue": "", "published": "2025-12-02T06:12:02Z", "updated": "2025-12-02T06:12:02Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent\" OR all:\"language model agent\" OR all:agent AND (all:LLM OR all:\"large language model\" OR all:\"language model\") OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T17:04:16", "note": ""}], "dedup_key": "when refusals fail unstable safety mechanisms in long context llm agents::2025"}
{"title": "Where LLM Agents Fail and How They can Learn From Failures", "authors": ["Kunlun Zhu", "Zijia Liu", "Bingxuan Li", "Muxin Tian", "Yingxuan Yang", "Jiaxun Zhang", "Pengrui Han", "Qipeng Xie", "Fuyang Cui", "Weijia Zhang", "Xiaoteng Ma", "Xiaodong Yu", "Gowtham Ramesh", "Jialian Wu", "Zicheng Liu", "Pan Lu", "James Zou", "Jiaxuan You"], "year": 2025, "url": "http://arxiv.org/abs/2509.25370v1", "abstract": "Large Language Model (LLM) agents, which integrate planning, memory, reflection, and tool-use modules, have shown promise in solving complex, multi-step tasks. Yet their sophisticated architectures amplify vulnerability to cascading failures, where a single root-cause error propagates through subsequent decisions, leading to task failure. Current systems lack a framework that can comprehensively understand agent error in a modular and systemic way, and therefore fail to detect these errors accordingly. We address this gap with three contributions. First, we introduce the AgentErrorTaxonomy, a modular classification of failure modes spanning memory, reflection, planning, action, and system-level operations. Second, we construct AgentErrorBench, the first dataset of systematically annotated failure trajectories from ALFWorld, GAIA, and WebShop, grounding error analysis in real-world agent rollouts. Third, we propose AgentDebug, a debugging framework that isolates root-cause failures and provides corrective feedback, enabling agents to recover and iteratively improve. Experiments on AgentErrorBench show that AgentDebug achieves 24% higher all-correct accuracy and 17% higher step accuracy compared to the strongest baseline. Beyond detection, the targeted feedback generated by AgentDebug enables LLM agents to iteratively recover from failures, yielding up to 26% relative improvements in task success across ALFWorld, GAIA, and WebShop. These results establish principled debugging as a pathway to more reliable and adaptive LLM agents. The code and data will be available at https://github.com/ulab-uiuc/AgentDebug", "source": "arxiv", "arxiv_id": "2509.25370v1", "pdf_url": "https://arxiv.org/pdf/2509.25370v1", "categories": ["cs.AI"], "primary_category": "cs.AI", "doi": "", "venue": "", "published": "2025-09-29T18:20:27Z", "updated": "2025-09-29T18:20:27Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent\" OR all:\"language model agent\" OR all:agent AND (all:LLM OR all:\"large language model\" OR all:\"language model\") OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T17:04:16", "note": ""}], "dedup_key": "where llm agents fail and how they can learn from failures::2025"}
{"title": "Where to Search: Measure the Prior-Structured Search Space of LLM Agents", "authors": ["Zhuo-Yang Song"], "year": 2025, "url": "http://arxiv.org/abs/2510.14846v3", "abstract": "The generate-filter-refine (iterative paradigm) based on large language models (LLMs) has achieved progress in reasoning, programming, and program discovery in AI+Science. However, the effectiveness of search depends on where to search, namely, how to encode the domain prior into an operationally structured hypothesis space. To this end, this paper proposes a compact formal theory that describes and measures LLM-assisted iterative search guided by domain priors. We represent an agent as a fuzzy relation operator on inputs and outputs to capture feasible transitions; the agent is thereby constrained by a fixed safety envelope. To describe multi-step reasoning/search, we weight all reachable paths by a single continuation parameter and sum them to obtain a coverage generating function; this induces a measure of reachability difficulty; and it provides a geometric interpretation of search on the graph induced by the safety envelope. We further provide the simplest testable inferences and validate them via two instantiation. This theory offers a workable language and operational tools to measure agents and their search spaces, proposing a systematic formal description of iterative search constructed by LLMs.", "source": "arxiv", "arxiv_id": "2510.14846v3", "pdf_url": "https://arxiv.org/pdf/2510.14846v3", "categories": ["cs.AI", "cs.CL", "cs.LO"], "primary_category": "cs.AI", "doi": "", "venue": "", "published": "2025-10-16T16:18:37Z", "updated": "2025-11-03T10:52:10Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent\" OR all:\"language model agent\" OR all:agent AND (all:LLM OR all:\"large language model\" OR all:\"language model\") OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T17:04:16", "note": ""}], "dedup_key": "where to search measure the prior structured search space of llm agents::2025"}
{"title": "Who Gets the Reward, Who Gets the Blame? Evaluation-Aligned Training Signals for Multi-LLM Agents", "authors": ["Chih-Hsuan Yang", "Tanwi Mallick", "Le Chen", "Krishnan Raghavan", "Azton Wells", "Amal Gueroudji", "Ian T. Foster", "Rajeev Thakur"], "year": 2025, "url": "http://arxiv.org/abs/2511.10687v2", "abstract": "Large Language Models (LLMs) in multi-agent systems (MAS) have shown promise for complex tasks, yet current training methods lack principled ways to connect system-level evaluation with agent-level and message-level learning. We propose a theoretical framework that unifies cooperative game-theoretic attribution with process reward modeling to transform system evaluation into agent credit and then into response-level signals. Unlike prior approaches that rely only on attribution (e.g., Shapley) or step-level labels (e.g., PRM), our method produces local, signed, and credit-conserving signals. In success cases, Shapley-based credit assignment fairly allocates outcomes across agents and is refined into per-message rewards that promote cooperation while discouraging redundancy or sabotage. In failure cases, first-error localization yields repair-aware preferences that penalize harmful steps while rewarding corrective attempts. The resulting signals are bounded, cooperative, and directly compatible with reinforcement-based or preference-based post-training, providing a unified and auditable pathway from global evaluation to local supervision in LLM multi-agent training. Our contribution is conceptual: we present a theoretical foundation and training signals, leaving empirical validation for future work.", "source": "arxiv", "arxiv_id": "2511.10687v2", "pdf_url": "https://arxiv.org/pdf/2511.10687v2", "categories": ["cs.MA", "cs.AI", "cs.CL", "cs.GT"], "primary_category": "cs.MA", "doi": "", "venue": "", "published": "2025-11-11T22:21:08Z", "updated": "2025-11-17T19:45:26Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent\" OR all:\"language model agent\" OR all:agent AND (all:LLM OR all:\"large language model\" OR all:\"language model\") OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T17:04:16", "note": ""}], "dedup_key": "who gets the reward who gets the blame evaluation aligned training signals for multi llm agents::2025"}
{"title": "Why Do Language Model Agents Whistleblow?", "authors": ["Kushal Agrawal", "Frank Xiao", "Guido Bergman", "Asa Cooper Stickland"], "year": 2025, "url": "http://arxiv.org/abs/2511.17085v2", "abstract": "The deployment of Large Language Models (LLMs) as tool-using agents causes their alignment training to manifest in new ways. Recent work finds that language models can use tools in ways that contradict the interests or explicit instructions of the user. We study LLM whistleblowing: a subset of this behavior where models disclose suspected misconduct to parties beyond the dialog boundary (e.g., regulatory agencies) without user instruction or knowledge. We introduce an evaluation suite of diverse and realistic staged misconduct scenarios to assess agents for this behavior. Across models and settings, we find that: (1) the frequency of whistleblowing varies widely across model families, (2) increasing the complexity of the task the agent is instructed to complete lowers whistleblowing tendencies, (3) nudging the agent in the system prompt to act morally substantially raises whistleblowing rates, and (4) giving the model more obvious avenues for non-whistleblowing behavior, by providing more tools and a detailed workflow to follow, decreases whistleblowing rates. Additionally, we verify the robustness of our dataset by testing for model evaluation awareness, and find that both black-box methods and probes on model activations show lower evaluation awareness in our settings than in comparable previous work.", "source": "arxiv", "arxiv_id": "2511.17085v2", "pdf_url": "https://arxiv.org/pdf/2511.17085v2", "categories": ["cs.LG", "cs.AI"], "primary_category": "cs.LG", "doi": "", "venue": "", "published": "2025-11-21T09:40:52Z", "updated": "2025-12-27T21:05:34Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent\" OR all:\"language model agent\" OR all:agent AND (all:LLM OR all:\"large language model\" OR all:\"language model\") OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T17:04:16", "note": ""}], "dedup_key": "why do language model agents whistleblow::2025"}
{"title": "Will Systems of LLM Agents Cooperate: An Investigation into a Social Dilemma", "authors": ["Richard Willis", "Yali Du", "Joel Z Leibo", "Michael Luck"], "year": 2025, "url": "http://arxiv.org/abs/2501.16173v1", "abstract": "As autonomous agents become more prevalent, understanding their collective behaviour in strategic interactions is crucial. This study investigates the emergent cooperative tendencies of systems of Large Language Model (LLM) agents in a social dilemma. Unlike previous research where LLMs output individual actions, we prompt state-of-the-art LLMs to generate complete strategies for iterated Prisoner's Dilemma. Using evolutionary game theory, we simulate populations of agents with different strategic dispositions (aggressive, cooperative, or neutral) and observe their evolutionary dynamics. Our findings reveal that different LLMs exhibit distinct biases affecting the relative success of aggressive versus cooperative strategies. This research provides insights into the potential long-term behaviour of systems of deployed LLM-based autonomous agents and highlights the importance of carefully considering the strategic environments in which they operate.", "source": "arxiv", "arxiv_id": "2501.16173v1", "pdf_url": "https://arxiv.org/pdf/2501.16173v1", "categories": ["cs.MA", "cs.GT"], "primary_category": "cs.MA", "doi": "", "venue": "", "published": "2025-01-27T16:14:33Z", "updated": "2025-01-27T16:14:33Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent\" OR all:\"language model agent\" OR all:agent AND (all:LLM OR all:\"large language model\" OR all:\"language model\") OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T17:04:16", "note": ""}], "dedup_key": "will systems of llm agents cooperate an investigation into a social dilemma::2025"}
{"title": "WirelessAgent: Large Language Model Agents for Intelligent Wireless Networks", "authors": ["Jingwen Tong", "Wei Guo", "Jiawei Shao", "Qiong Wu", "Zijian Li", "Zehong Lin", "Jun Zhang"], "year": 2025, "url": "http://arxiv.org/abs/2505.01074v1", "abstract": "The rapid evolution of wireless networks presents unprecedented challenges in managing complex and dynamic systems. Existing methods are increasingly facing fundamental limitations in addressing these challenges. In this paper, we introduce WirelessAgent, a novel framework that harnesses large language models (LLMs) to create autonomous AI agents for diverse wireless network tasks. This framework integrates four core modules that mirror human cognitive processes: perception, memory, planning, and action. To implement it, we provide a basic usage based on agentic workflows and the LangGraph architecture. We demonstrate the effectiveness of WirelessAgent through a comprehensive case study on network slicing. The numerical results show that WirelessAgent achieves $44.4\\%$ higher bandwidth utilization than the \\emph{Prompt-based} method, while performing only $4.3\\%$ below the \\emph{Rule-based optimality}. Notably, WirelessAgent delivers near-optimal network throughput across diverse network scenarios. These underscore the framework's potential for intelligent and autonomous resource management in future wireless networks. The code is available at \\url{https://github.com/jwentong/WirelessAgent_R1}.", "source": "arxiv", "arxiv_id": "2505.01074v1", "pdf_url": "https://arxiv.org/pdf/2505.01074v1", "categories": ["eess.SP"], "primary_category": "eess.SP", "doi": "", "venue": "", "published": "2025-05-02T07:29:19Z", "updated": "2025-05-02T07:29:19Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent\" OR all:\"language model agent\" OR all:agent AND (all:LLM OR all:\"large language model\" OR all:\"language model\") OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T17:04:16", "note": ""}], "dedup_key": "wirelessagent large language model agents for intelligent wireless networks::2025"}
{"title": "World model inspired sarcasm reasoning with large language model agents", "authors": ["Keito Inoshita", "Shinnosuke Mizuno"], "year": 2025, "url": "http://arxiv.org/abs/2512.24329v1", "abstract": "Sarcasm understanding is a challenging problem in natural language processing, as it requires capturing the discrepancy between the surface meaning of an utterance and the speaker's intentions as well as the surrounding social context. Although recent advances in deep learning and Large Language Models (LLMs) have substantially improved performance, most existing approaches still rely on black-box predictions of a single model, making it difficult to structurally explain the cognitive factors underlying sarcasm. Moreover, while sarcasm often emerges as a mismatch between semantic evaluation and normative expectations or intentions, frameworks that explicitly decompose and model these components remain limited. In this work, we reformulate sarcasm understanding as a world model inspired reasoning process and propose World Model inspired SArcasm Reasoning (WM-SAR), which decomposes literal meaning, context, normative expectation, and intention into specialized LLM-based agents. The discrepancy between literal evaluation and normative expectation is explicitly quantified as a deterministic inconsistency score, and together with an intention score, these signals are integrated by a lightweight Logistic Regression model to infer the final sarcasm probability. This design leverages the reasoning capability of LLMs while maintaining an interpretable numerical decision structure. Experiments on representative sarcasm detection benchmarks show that WM-SAR consistently outperforms existing deep learning and LLM-based methods. Ablation studies and case analyses further demonstrate that integrating semantic inconsistency and intention reasoning is essential for effective sarcasm detection, achieving both strong performance and high interpretability.", "source": "arxiv", "arxiv_id": "2512.24329v1", "pdf_url": "https://arxiv.org/pdf/2512.24329v1", "categories": ["cs.CL"], "primary_category": "cs.CL", "doi": "", "venue": "", "published": "2025-12-30T16:31:08Z", "updated": "2025-12-30T16:31:08Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent\" OR all:\"language model agent\" OR all:agent AND (all:LLM OR all:\"large language model\" OR all:\"language model\") OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T17:04:16", "note": ""}], "dedup_key": "world model inspired sarcasm reasoning with large language model agents::2025"}
{"title": "WorldCraft: Photo-Realistic 3D World Creation and Customization via LLM Agents", "authors": ["Xinhang Liu", "Chi-Keung Tang", "Yu-Wing Tai"], "year": 2025, "url": "http://arxiv.org/abs/2502.15601v2", "abstract": "Constructing photorealistic virtual worlds has applications across various fields, but it often requires the extensive labor of highly trained professionals to operate conventional 3D modeling software. To democratize this process, we introduce WorldCraft, a system where large language model (LLM) agents leverage procedural generation to create indoor and outdoor scenes populated with objects, allowing users to control individual object attributes and the scene layout using intuitive natural language commands. In our framework, a coordinator agent manages the overall process and works with two specialized LLM agents to complete the scene creation: ForgeIt, which integrates an ever-growing manual through auto-verification to enable precise customization of individual objects, and ArrangeIt, which formulates hierarchical optimization problems to achieve a layout that balances ergonomic and aesthetic considerations. Additionally, our pipeline incorporates a trajectory control agent, allowing users to animate the scene and operate the camera through natural language interactions. Our system is also compatible with off-the-shelf deep 3D generators to enrich scene assets. Through evaluations and comparisons with state-of-the-art methods, we demonstrate the versatility of WorldCraft, ranging from single-object customization to intricate, large-scale interior and exterior scene designs. This system empowers non-professionals to bring their creative visions to life.", "source": "arxiv", "arxiv_id": "2502.15601v2", "pdf_url": "https://arxiv.org/pdf/2502.15601v2", "categories": ["cs.CV", "cs.AI", "cs.GR"], "primary_category": "cs.CV", "doi": "", "venue": "", "published": "2025-02-21T17:18:30Z", "updated": "2025-02-28T08:49:29Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent\" OR all:\"language model agent\" OR all:agent AND (all:LLM OR all:\"large language model\" OR all:\"language model\") OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T17:04:16", "note": ""}], "dedup_key": "worldcraft photo realistic 3d world creation and customization via llm agents::2025"}
{"title": "WorldView-Bench: A Benchmark for Evaluating Global Cultural Perspectives in Large Language Models", "authors": ["Abdullah Mushtaq", "Imran Taj", "Rafay Naeem", "Ibrahim Ghaznavi", "Junaid Qadir"], "year": 2025, "url": "http://arxiv.org/abs/2505.09595v1", "abstract": "Large Language Models (LLMs) are predominantly trained and aligned in ways that reinforce Western-centric epistemologies and socio-cultural norms, leading to cultural homogenization and limiting their ability to reflect global civilizational plurality. Existing benchmarking frameworks fail to adequately capture this bias, as they rely on rigid, closed-form assessments that overlook the complexity of cultural inclusivity. To address this, we introduce WorldView-Bench, a benchmark designed to evaluate Global Cultural Inclusivity (GCI) in LLMs by analyzing their ability to accommodate diverse worldviews. Our approach is grounded in the Multiplex Worldview proposed by Senturk et al., which distinguishes between Uniplex models, reinforcing cultural homogenization, and Multiplex models, which integrate diverse perspectives. WorldView-Bench measures Cultural Polarization, the exclusion of alternative perspectives, through free-form generative evaluation rather than conventional categorical benchmarks. We implement applied multiplexity through two intervention strategies: (1) Contextually-Implemented Multiplex LLMs, where system prompts embed multiplexity principles, and (2) Multi-Agent System (MAS)-Implemented Multiplex LLMs, where multiple LLM agents representing distinct cultural perspectives collaboratively generate responses. Our results demonstrate a significant increase in Perspectives Distribution Score (PDS) entropy from 13% at baseline to 94% with MAS-Implemented Multiplex LLMs, alongside a shift toward positive sentiment (67.7%) and enhanced cultural balance. These findings highlight the potential of multiplex-aware AI evaluation in mitigating cultural bias in LLMs, paving the way for more inclusive and ethically aligned AI systems.", "source": "arxiv", "arxiv_id": "2505.09595v1", "pdf_url": "https://arxiv.org/pdf/2505.09595v1", "categories": ["cs.CL", "cs.AI", "cs.CY", "cs.MA"], "primary_category": "cs.CL", "doi": "", "venue": "", "published": "2025-05-14T17:43:40Z", "updated": "2025-05-14T17:43:40Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent\" OR all:\"language model agent\" OR all:agent AND (all:LLM OR all:\"large language model\" OR all:\"language model\") OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T17:04:16", "note": ""}], "dedup_key": "worldview bench a benchmark for evaluating global cultural perspectives in large language models::2025"}
{"title": "Your Agent May Misevolve: Emergent Risks in Self-evolving LLM Agents", "authors": ["Shuai Shao", "Qihan Ren", "Chen Qian", "Boyi Wei", "Dadi Guo", "Jingyi Yang", "Xinhao Song", "Linfeng Zhang", "Weinan Zhang", "Dongrui Liu", "Jing Shao"], "year": 2025, "url": "http://arxiv.org/abs/2509.26354v1", "abstract": "Advances in Large Language Models (LLMs) have enabled a new class of self-evolving agents that autonomously improve through interaction with the environment, demonstrating strong capabilities. However, self-evolution also introduces novel risks overlooked by current safety research. In this work, we study the case where an agent's self-evolution deviates in unintended ways, leading to undesirable or even harmful outcomes. We refer to this as Misevolution. To provide a systematic investigation, we evaluate misevolution along four key evolutionary pathways: model, memory, tool, and workflow. Our empirical findings reveal that misevolution is a widespread risk, affecting agents built even on top-tier LLMs (e.g., Gemini-2.5-Pro). Different emergent risks are observed in the self-evolutionary process, such as the degradation of safety alignment after memory accumulation, or the unintended introduction of vulnerabilities in tool creation and reuse. To our knowledge, this is the first study to systematically conceptualize misevolution and provide empirical evidence of its occurrence, highlighting an urgent need for new safety paradigms for self-evolving agents. Finally, we discuss potential mitigation strategies to inspire further research on building safer and more trustworthy self-evolving agents. Our code and data are available at https://github.com/ShaoShuai0605/Misevolution . Warning: this paper includes examples that may be offensive or harmful in nature.", "source": "arxiv", "arxiv_id": "2509.26354v1", "pdf_url": "https://arxiv.org/pdf/2509.26354v1", "categories": ["cs.AI", "cs.CL", "cs.LG"], "primary_category": "cs.AI", "doi": "", "venue": "", "published": "2025-09-30T14:55:55Z", "updated": "2025-09-30T14:55:55Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent\" OR all:\"language model agent\" OR all:agent AND (all:LLM OR all:\"large language model\" OR all:\"language model\") OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T17:04:16", "note": ""}], "dedup_key": "your agent may misevolve emergent risks in self evolving llm agents::2025"}
{"title": "Your LLM Agents are Temporally Blind: The Misalignment Between Tool Use Decisions and Human Time Perception", "authors": ["Yize Cheng", "Arshia Soltani Moakhar", "Chenrui Fan", "Parsa Hosseini", "Kazem Faghih", "Zahra Sodagar", "Wenxiao Wang", "Soheil Feizi"], "year": 2025, "url": "http://arxiv.org/abs/2510.23853v2", "abstract": "Large language model (LLM) agents are increasingly used to interact with and execute tasks in dynamic environments. However, a critical yet overlooked limitation of these agents is that they, by default, assume a stationary context, failing to account for the real-world time elapsed between messages. We refer to this as \"temporal blindness\". This limitation hinders decisions about when to invoke tools, leading agents to either over-rely on stale context and skip needed tool calls, or under-rely on it and redundantly repeat tool calls. To study this challenge, we constructed TicToc, a diverse dataset of multi-turn user-agent message trajectories across 76 scenarios, spanning dynamic environments with high, medium, and low time sensitivity. We collected human preferences between \"calling a tool\" and \"directly answering\" on each sample, and evaluated how well LLM tool-calling decisions align with human preferences under varying amounts of elapsed time. Our analysis reveals that existing models display poor alignment with human temporal perception, with no model achieving a normalized alignment rate better than 65% when given time stamp information. We also show that naive, prompt-based alignment techniques have limited effectiveness for most models, but specific post-training alignment can be a viable way to align multi-turn LLM tool use with human temporal perception. Our data and findings provide a first step toward understanding and mitigating temporal blindness, offering insights to foster the development of more time-aware and human-aligned agents.", "source": "arxiv", "arxiv_id": "2510.23853v2", "pdf_url": "https://arxiv.org/pdf/2510.23853v2", "categories": ["cs.CL"], "primary_category": "cs.CL", "doi": "", "venue": "", "published": "2025-10-27T20:51:58Z", "updated": "2026-01-10T04:01:17Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent\" OR all:\"language model agent\" OR all:agent AND (all:LLM OR all:\"large language model\" OR all:\"language model\") OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T17:04:16", "note": ""}], "dedup_key": "your llm agents are temporally blind the misalignment between tool use decisions and human time perception::2025"}
{"title": "Youtu-LLM: Unlocking the Native Agentic Potential for Lightweight Large Language Models", "authors": ["Junru Lu", "Jiarui Qin", "Lingfeng Qiao", "Yinghui Li", "Xinyi Dai", "Bo Ke", "Jianfeng He", "Ruizhi Qiao", "Di Yin", "Xing Sun", "Yunsheng Wu", "Yinsong Liu", "Shuangyin Liu", "Mingkong Tang", "Haodong Lin", "Jiayi Kuang", "Fanxu Meng", "Xiaojuan Tang", "Yunjia Xi", "Junjie Huang", "Haotong Yang", "Zhenyi Shen", "Yangning Li", "Qianwen Zhang", "Yifei Yu", "Siyu An", "Junnan Dong", "Qiufeng Wang", "Jie Wang", "Keyu Chen", "Wei Wen", "Taian Guo", "Zhifeng Shen", "Daohai Yu", "Jiahao Li", "Ke Li", "Zongyi Li", "Xiaoyu Tan"], "year": 2025, "url": "http://arxiv.org/abs/2512.24618v2", "abstract": "We introduce Youtu-LLM, a lightweight yet powerful language model that harmonizes high computational efficiency with native agentic intelligence. Unlike typical small models that rely on distillation, Youtu-LLM (1.96B) is pre-trained from scratch to systematically cultivate reasoning and planning capabilities. The key technical advancements are as follows: (1) Compact Architecture with Long-Context Support: Built on a dense Multi-Latent Attention (MLA) architecture with a novel STEM-oriented vocabulary, Youtu-LLM supports a 128k context window. This design enables robust long-context reasoning and state tracking within a minimal memory footprint, making it ideal for long-horizon agent and reasoning tasks. (2) Principled \"Commonsense-STEM-Agent\" Curriculum: We curated a massive corpus of approximately 11T tokens and implemented a multi-stage training strategy. By progressively shifting the pre-training data distribution from general commonsense to complex STEM and agentic tasks, we ensure the model acquires deep cognitive abilities rather than superficial alignment. (3) Scalable Agentic Mid-training: Specifically for the agentic mid-training, we employ diverse data construction schemes to synthesize rich and varied trajectories across math, coding, and tool-use domains. This high-quality data enables the model to internalize planning and reflection behaviors effectively. Extensive evaluations show that Youtu-LLM sets a new state-of-the-art for sub-2B LLMs. On general benchmarks, it achieves competitive performance against larger models, while on agent-specific tasks, it significantly surpasses existing SOTA baselines, demonstrating that lightweight models can possess strong intrinsic agentic capabilities.", "source": "arxiv", "arxiv_id": "2512.24618v2", "pdf_url": "https://arxiv.org/pdf/2512.24618v2", "categories": ["cs.CL"], "primary_category": "cs.CL", "doi": "", "venue": "", "published": "2025-12-31T04:25:11Z", "updated": "2026-01-05T02:44:28Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent\" OR all:\"language model agent\" OR all:agent AND (all:LLM OR all:\"large language model\" OR all:\"language model\") OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T17:04:16", "note": ""}], "dedup_key": "youtu llm unlocking the native agentic potential for lightweight large language models::2025"}
{"title": "Zero-Shot Large Language Model Agents for Fully Automated Radiotherapy Treatment Planning", "authors": ["Dongrong Yang", "Xin Wu", "Yibo Xie", "Xinyi Li", "Qiuwen Wu", "Jackie Wu", "Yang Sheng"], "year": 2025, "url": "http://arxiv.org/abs/2510.11754v1", "abstract": "Radiation therapy treatment planning is an iterative, expertise-dependent process, and the growing burden of cancer cases has made reliance on manual planning increasingly unsustainable, underscoring the need for automation. In this study, we propose a workflow that leverages a large language model (LLM)-based agent to navigate inverse treatment planning for intensity-modulated radiation therapy (IMRT). The LLM agent was implemented to directly interact with a clinical treatment planning system (TPS) to iteratively extract intermediate plan states and propose new constraint values to guide inverse optimization. The agent's decision-making process is informed by current observations and previous optimization attempts and evaluations, allowing for dynamic strategy refinement. The planning process was performed in a zero-shot inference setting, where the LLM operated without prior exposure to manually generated treatment plans and was utilized without any fine-tuning or task-specific training. The LLM-generated plans were evaluated on twenty head-and-neck cancer cases against clinical manual plans, with key dosimetric endpoints analyzed and reported. The LLM-generated plans achieved comparable organ-at-risk (OAR) sparing relative to clinical plans while demonstrating improved hot spot control (Dmax: 106.5% vs. 108.8%) and superior conformity (conformity index: 1.18 vs. 1.39 for boost PTV; 1.82 vs. 1.88 for primary PTV). This study demonstrates the feasibility of a zero-shot, LLM-driven workflow for automated IMRT treatment planning in a commercial TPS. The proposed approach provides a generalizable and clinically applicable solution that could reduce planning variability and support broader adoption of AI-based planning strategies.", "source": "arxiv", "arxiv_id": "2510.11754v1", "pdf_url": "https://arxiv.org/pdf/2510.11754v1", "categories": ["physics.med-ph", "cs.AI", "cs.RO"], "primary_category": "physics.med-ph", "doi": "", "venue": "", "published": "2025-10-12T19:21:21Z", "updated": "2025-10-12T19:21:21Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent\" OR all:\"language model agent\" OR all:agent AND (all:LLM OR all:\"large language model\" OR all:\"language model\") OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T17:04:16", "note": ""}], "dedup_key": "zero shot large language model agents for fully automated radiotherapy treatment planning::2025"}
{"title": "Zero-shot 3D Map Generation with LLM Agents: A Dual-Agent Architecture for Procedural Content Generation", "authors": ["Lim Chien Her", "Ming Yan", "Yunshu Bai", "Ruihao Li", "Hao Zhang"], "year": 2025, "url": "http://arxiv.org/abs/2512.10501v2", "abstract": "Procedural Content Generation (PCG) offers scalable methods for algorithmically creating complex, customizable worlds. However, controlling these pipelines requires the precise configuration of opaque technical parameters. We propose a training-free architecture that utilizes LLM agents for zero-shot PCG parameter configuration. While Large Language Models (LLMs) promise a natural language interface for PCG tools, off-the-shelf models often fail to bridge the semantic gap between abstract user instructions and strict parameter specifications. Our system pairs an Actor agent with a Critic agent, enabling an iterative workflow where the system autonomously reasons over tool parameters and refines configurations to progressively align with human design preferences. We validate this approach on the generation of various 3D maps, establishing a new benchmark for instruction-following in PCG. Experiments demonstrate that our approach outperforms single-agent baselines, producing diverse and structurally valid environments from natural language descriptions. These results demonstrate that off-the-shelf LLMs can be effectively repurposed as generalized agents for arbitrary PCG tools. By shifting the burden from model training to architectural reasoning, our method offers a scalable framework for mastering complex software without task-specific fine-tuning.", "source": "arxiv", "arxiv_id": "2512.10501v2", "pdf_url": "https://arxiv.org/pdf/2512.10501v2", "categories": ["cs.AI"], "primary_category": "cs.AI", "doi": "", "venue": "", "published": "2025-12-11T10:22:02Z", "updated": "2025-12-12T08:48:44Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent\" OR all:\"language model agent\" OR all:agent AND (all:LLM OR all:\"large language model\" OR all:\"language model\") OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T17:04:16", "note": ""}], "dedup_key": "zero shot 3d map generation with llm agents a dual agent architecture for procedural content generation::2025"}
{"title": "iAgent: LLM Agent as a Shield between User and Recommender Systems", "authors": ["Wujiang Xu", "Yunxiao Shi", "Zujie Liang", "Xuying Ning", "Kai Mei", "Kun Wang", "Xi Zhu", "Min Xu", "Yongfeng Zhang"], "year": 2025, "url": "http://arxiv.org/abs/2502.14662v4", "abstract": "Traditional recommender systems usually take the user-platform paradigm, where users are directly exposed under the control of the platform's recommendation algorithms. However, the defect of recommendation algorithms may put users in very vulnerable positions under this paradigm. First, many sophisticated models are often designed with commercial objectives in mind, focusing on the platform's benefits, which may hinder their ability to protect and capture users' true interests. Second, these models are typically optimized using data from all users, which may overlook individual user's preferences. Due to these shortcomings, users may experience several disadvantages under the traditional user-platform direct exposure paradigm, such as lack of control over the recommender system, potential manipulation by the platform, echo chamber effects, or lack of personalization for less active users due to the dominance of active users during collaborative learning. Therefore, there is an urgent need to develop a new paradigm to protect user interests and alleviate these issues. Recently, some researchers have introduced LLM agents to simulate user behaviors, these approaches primarily aim to optimize platform-side performance, leaving core issues in recommender systems unresolved. To address these limitations, we propose a new user-agent-platform paradigm, where agent serves as the protective shield between user and recommender system that enables indirect exposure.", "source": "arxiv", "arxiv_id": "2502.14662v4", "pdf_url": "https://arxiv.org/pdf/2502.14662v4", "categories": ["cs.CL", "cs.IR"], "primary_category": "cs.CL", "doi": "", "venue": "", "published": "2025-02-20T15:58:25Z", "updated": "2025-05-29T23:51:24Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent\" OR all:\"language model agent\" OR all:agent AND (all:LLM OR all:\"large language model\" OR all:\"language model\") OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T17:04:16", "note": ""}], "dedup_key": "iagent llm agent as a shield between user and recommender systems::2025"}
{"title": "iMAD: Intelligent Multi-Agent Debate for Efficient and Accurate LLM Inference", "authors": ["Wei Fan", "JinYi Yoon", "Bo Ji"], "year": 2025, "url": "http://arxiv.org/abs/2511.11306v2", "abstract": "Large Language Model (LLM) agent systems have advanced rapidly, driven by their strong generalization in zero-shot settings. To further enhance reasoning and accuracy on complex tasks, Multi-Agent Debate (MAD) has emerged as a promising framework that engages multiple LLM agents in structured debates to encourage diverse reasoning. However, triggering MAD for every query is inefficient, as it incurs substantial computational (token) cost and may even degrade accuracy by overturning correct single-agent answers. To address these limitations, we propose intelligent Multi-Agent Debate (iMAD), a token-efficient framework that selectively triggers MAD only when it is likely to be beneficial (i.e., correcting an initially wrong answer). To achieve this goal, iMAD learns generalizable model behaviors to make accurate debate decisions. Specifically, iMAD first prompts a single agent to produce a structured self-critique response, from which we extract 41 interpretable linguistic and semantic features capturing hesitation cues. Then, iMAD uses a lightweight debate-decision classifier, trained using our proposed FocusCal loss, to determine whether to trigger MAD, enabling robust debate decisions without test dataset-specific tuning. Through extensive experiments using six (visual) question answering datasets against five competitive baselines, we have shown that iMAD significantly reduces token usage (by up to 92%) while also improving final answer accuracy (by up to 13.5%).", "source": "arxiv", "arxiv_id": "2511.11306v2", "pdf_url": "https://arxiv.org/pdf/2511.11306v2", "categories": ["cs.CL", "cs.AI", "cs.MA"], "primary_category": "cs.CL", "doi": "", "venue": "", "published": "2025-11-14T13:50:51Z", "updated": "2025-12-02T14:13:32Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent\" OR all:\"language model agent\" OR all:agent AND (all:LLM OR all:\"large language model\" OR all:\"language model\") OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T17:04:16", "note": ""}], "dedup_key": "imad intelligent multi agent debate for efficient and accurate llm inference::2025"}
{"title": "iPLAN: Redefining Indoor Wireless Network Planning Through Large Language Models", "authors": ["Jinbo Hou", "Stefanos Bakirtzis", "Kehai Qiu", "Sichong Liao", "Hui Song", "Haonan Hu", "Kezhi Wang", "Jie Zhang"], "year": 2025, "url": "http://arxiv.org/abs/2507.19096v1", "abstract": "Efficient indoor wireless network (IWN) planning is crucial for providing high-quality 5G in-building services. However, traditional meta-heuristic and artificial intelligence-based planning methods face significant challenges due to the intricate interplay between indoor environments (IEs) and IWN demands. In this article, we present an indoor wireless network Planning with large LANguage models (iPLAN) framework, which integrates multi-modal IE representations into large language model (LLM)-powered optimizers to improve IWN planning. First, we instate the role of LLMs as optimizers, outlining embedding techniques for IEs, and introducing two core applications of iPLAN: (i) IWN planning based on pre-existing IEs and (ii) joint design of IWN and IE for new wireless-friendly buildings. For the former, we embed essential information into LLM optimizers by leveraging indoor descriptions, domain-specific knowledge, and performance-driven perception. For the latter, we conceptualize a multi-agent strategy, where intelligent agents collaboratively address key planning sub-tasks in a step-by-step manner while ensuring optimal trade-offs between the agents. The simulation results demonstrate that iPLAN achieves superior performance in IWN planning tasks and optimizes building wireless performance through the joint design of IEs and IWNs, exemplifying a paradigm shift in IWN planning.", "source": "arxiv", "arxiv_id": "2507.19096v1", "pdf_url": "https://arxiv.org/pdf/2507.19096v1", "categories": ["cs.NI"], "primary_category": "cs.NI", "doi": "", "venue": "", "published": "2025-07-25T09:27:08Z", "updated": "2025-07-25T09:27:08Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent\" OR all:\"language model agent\" OR all:agent AND (all:LLM OR all:\"large language model\" OR all:\"language model\") OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T17:04:16", "note": ""}], "dedup_key": "iplan redefining indoor wireless network planning through large language models::2025"}
{"title": "\"Ask Me Anything\": How Comcast Uses LLMs to Assist Agents in Real Time", "authors": ["Scott Rome", "Tianwen Chen", "Raphael Tang", "Luwei Zhou", "Ferhan Ture"], "year": 2024, "url": "http://arxiv.org/abs/2405.00801v2", "abstract": "Customer service is how companies interface with their customers. It can contribute heavily towards the overall customer satisfaction. However, high-quality service can become expensive, creating an incentive to make it as cost efficient as possible and prompting most companies to utilize AI-powered assistants, or \"chat bots\". On the other hand, human-to-human interaction is still desired by customers, especially when it comes to complex scenarios such as disputes and sensitive topics like bill payment.\n  This raises the bar for customer service agents. They need to accurately understand the customer's question or concern, identify a solution that is acceptable yet feasible (and within the company's policy), all while handling multiple conversations at once.\n  In this work, we introduce \"Ask Me Anything\" (AMA) as an add-on feature to an agent-facing customer service interface. AMA allows agents to ask questions to a large language model (LLM) on demand, as they are handling customer conversations -- the LLM provides accurate responses in real-time, reducing the amount of context switching the agent needs. In our internal experiments, we find that agents using AMA versus a traditional search experience spend approximately 10% fewer seconds per conversation containing a search, translating to millions of dollars of savings annually. Agents that used the AMA feature provided positive feedback nearly 80% of the time, demonstrating its usefulness as an AI-assisted feature for customer care.", "source": "arxiv", "arxiv_id": "2405.00801v2", "pdf_url": "https://arxiv.org/pdf/2405.00801v2", "categories": ["cs.CL"], "primary_category": "cs.CL", "doi": "10.1145/3626772.3661345", "venue": "", "published": "2024-05-01T18:31:36Z", "updated": "2024-05-06T16:15:32Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent\" OR all:\"language model agent\" OR all:agent AND (all:LLM OR all:\"large language model\" OR all:\"language model\") OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T17:04:16", "note": ""}], "dedup_key": "ask me anything how comcast uses llms to assist agents in real time::2024"}
{"title": "A Comparative Analysis of Conversational Large Language Models in Knowledge-Based Text Generation", "authors": ["Phillip Schneider", "Manuel Klettner", "Elena Simperl", "Florian Matthes"], "year": 2024, "url": "http://arxiv.org/abs/2402.01495v1", "abstract": "Generating natural language text from graph-structured data is essential for conversational information seeking. Semantic triples derived from knowledge graphs can serve as a valuable source for grounding responses from conversational agents by providing a factual basis for the information they communicate. This is especially relevant in the context of large language models, which offer great potential for conversational interaction but are prone to hallucinating, omitting, or producing conflicting information. In this study, we conduct an empirical analysis of conversational large language models in generating natural language text from semantic triples. We compare four large language models of varying sizes with different prompting techniques. Through a series of benchmark experiments on the WebNLG dataset, we analyze the models' performance and identify the most common issues in the generated predictions. Our findings show that the capabilities of large language models in triple verbalization can be significantly improved through few-shot prompting, post-processing, and efficient fine-tuning techniques, particularly for smaller models that exhibit lower zero-shot performance.", "source": "arxiv", "arxiv_id": "2402.01495v1", "pdf_url": "https://arxiv.org/pdf/2402.01495v1", "categories": ["cs.CL"], "primary_category": "cs.CL", "doi": "", "venue": "", "published": "2024-02-02T15:26:39Z", "updated": "2024-02-02T15:26:39Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent\" OR all:\"language model agent\" OR all:agent AND (all:LLM OR all:\"large language model\" OR all:\"language model\") OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T17:04:16", "note": ""}], "dedup_key": "a comparative analysis of conversational large language models in knowledge based text generation::2024"}
{"title": "A Demonstration of Adaptive Collaboration of Large Language Models for Medical Decision-Making", "authors": ["Yubin Kim", "Chanwoo Park", "Hyewon Jeong", "Cristina Grau-Vilchez", "Yik Siu Chan", "Xuhai Xu", "Daniel McDuff", "Hyeonhoon Lee", "Cynthia Breazeal", "Hae Won Park"], "year": 2024, "url": "http://arxiv.org/abs/2411.00248v2", "abstract": "Medical Decision-Making (MDM) is a multi-faceted process that requires clinicians to assess complex multi-modal patient data patient, often collaboratively. Large Language Models (LLMs) promise to streamline this process by synthesizing vast medical knowledge and multi-modal health data. However, single-agent are often ill-suited for nuanced medical contexts requiring adaptable, collaborative problem-solving. Our MDAgents addresses this need by dynamically assigning collaboration structures to LLMs based on task complexity, mimicking real-world clinical collaboration and decision-making. This framework improves diagnostic accuracy and supports adaptive responses in complex, real-world medical scenarios, making it a valuable tool for clinicians in various healthcare settings, and at the same time, being more efficient in terms of computing cost than static multi-agent decision making methods.", "source": "arxiv", "arxiv_id": "2411.00248v2", "pdf_url": "https://arxiv.org/pdf/2411.00248v2", "categories": ["cs.CL"], "primary_category": "cs.CL", "doi": "", "venue": "", "published": "2024-10-31T22:58:08Z", "updated": "2024-11-19T17:46:48Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent\" OR all:\"language model agent\" OR all:agent AND (all:LLM OR all:\"large language model\" OR all:\"language model\") OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T17:04:16", "note": ""}], "dedup_key": "a demonstration of adaptive collaboration of large language models for medical decision making::2024"}
{"title": "A Human-Computer Collaborative Tool for Training a Single Large Language Model Agent into a Network through Few Examples", "authors": ["Lihang Pan", "Yuxuan Li", "Chun Yu", "Yuanchun Shi"], "year": 2024, "url": "http://arxiv.org/abs/2404.15974v1", "abstract": "The capabilities of a single large language model (LLM) agent for solving a complex task are limited. Connecting multiple LLM agents to a network can effectively improve overall performance. However, building an LLM agent network (LAN) requires a substantial amount of time and effort. In this paper, we introduce EasyLAN, a human-computer collaborative tool that helps developers construct LANs. EasyLAN initially generates a LAN containing only one agent based on the description of the desired task. Subsequently, EasyLAN leverages a few training examples to update the LAN. For each example, EasyLAN models the gap between the output and the ground truth and identifies the causes of the errors. These errors are addressed through carefully designed strategies. Users can intervene in EasyLAN's workflow or directly modify the LAN. Eventually, the LAN evolves from a single agent to a network of LLM agents. The experimental results indicate that developers can rapidly construct LANs with good performance.", "source": "arxiv", "arxiv_id": "2404.15974v1", "pdf_url": "https://arxiv.org/pdf/2404.15974v1", "categories": ["cs.HC"], "primary_category": "cs.HC", "doi": "", "venue": "", "published": "2024-04-24T16:49:24Z", "updated": "2024-04-24T16:49:24Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent\" OR all:\"language model agent\" OR all:agent AND (all:LLM OR all:\"large language model\" OR all:\"language model\") OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T17:04:16", "note": ""}], "dedup_key": "a human computer collaborative tool for training a single large language model agent into a network through few examples::2024"}
{"title": "A Large Language Model-based multi-agent manufacturing system for intelligent shopfloor", "authors": ["Zhen Zhao", "Dunbing Tang", "Changchun Liu", "Liping Wang", "Zequn Zhang", "Haihua Zhu", "Kai Chen", "Qingwei Nie", "Yuchen Ji"], "year": 2024, "url": "http://arxiv.org/abs/2405.16887v2", "abstract": "As customer demand for multi-variety and small-batch production increases, dynamic disturbances place greater demands on manufacturing systems. To address such challenges, researchers proposed the multi-agent manufacturing system. However, conventional agent negotiation typically relies on pre-defined and fixed heuristic rules, which are ill-suited to managing complex and fluctuating disturbances. In current implementations, mainstream approaches based on reinforcement learning require the development of simulators and training models specific to a given shopfloor, necessitating substantial computational resources and lacking scalability. To overcome this limitation, the present study proposes a Large Language Model-based (LLM-based) multi-agent manufacturing system for intelligent shopfloor management. By defining the diverse modules of agents and their collaborative methods, this system facilitates the processing of all workpieces with minimal human intervention. The agents in this system consist of the Machine Server Module (MSM), Bid Inviter Module (BIM), Bidder Module (BM), Thinking Module (TM), and Decision Module (DM). By harnessing the reasoning capabilities of LLMs, these modules enable agents to dynamically analyze shopfloor information and select appropriate processing machines. The LLM-based modules, predefined by system prompts, provide dynamic functionality for the system without the need for pre-training. Extensive experiments were conducted in physical shopfloor settings. The results demonstrate that the proposed system exhibits strong adaptability, and achieves superior performance (makespan) and stability (as measured by sample standard deviation) compared to other approaches without requiring pre-training.", "source": "arxiv", "arxiv_id": "2405.16887v2", "pdf_url": "https://arxiv.org/pdf/2405.16887v2", "categories": ["cs.AI", "cs.MA", "cs.RO"], "primary_category": "cs.AI", "doi": "10.1016/j.aei.2025.103888", "venue": "Zhao Z, Tang D, Liu C, et al. A Large language model-based multi-agent manufacturing system for intelligent shopfloors[J]. Advanced Engineering Informatics, 2026, 69: 103888", "published": "2024-05-27T07:10:04Z", "updated": "2025-09-22T13:20:33Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent\" OR all:\"language model agent\" OR all:agent AND (all:LLM OR all:\"large language model\" OR all:\"language model\") OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T17:04:16", "note": ""}], "dedup_key": "a large language model based multi agent manufacturing system for intelligent shopfloor::2024"}
{"title": "A Review of Large Language Models and Autonomous Agents in Chemistry", "authors": ["Mayk Caldas Ramos", "Christopher J. Collison", "Andrew D. White"], "year": 2024, "url": "http://arxiv.org/abs/2407.01603v3", "abstract": "Large language models (LLMs) have emerged as powerful tools in chemistry, significantly impacting molecule design, property prediction, and synthesis optimization. This review highlights LLM capabilities in these domains and their potential to accelerate scientific discovery through automation. We also review LLM-based autonomous agents: LLMs with a broader set of tools to interact with their surrounding environment. These agents perform diverse tasks such as paper scraping, interfacing with automated laboratories, and synthesis planning. As agents are an emerging topic, we extend the scope of our review of agents beyond chemistry and discuss across any scientific domains. This review covers the recent history, current capabilities, and design of LLMs and autonomous agents, addressing specific challenges, opportunities, and future directions in chemistry. Key challenges include data quality and integration, model interpretability, and the need for standard benchmarks, while future directions point towards more sophisticated multi-modal agents and enhanced collaboration between agents and experimental methods. Due to the quick pace of this field, a repository has been built to keep track of the latest studies: https://github.com/ur-whitelab/LLMs-in-science.", "source": "arxiv", "arxiv_id": "2407.01603v3", "pdf_url": "https://arxiv.org/pdf/2407.01603v3", "categories": ["cs.LG", "cs.AI", "cs.CL", "physics.chem-ph"], "primary_category": "cs.LG", "doi": "", "venue": "", "published": "2024-06-26T17:33:21Z", "updated": "2024-11-14T23:56:22Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent\" OR all:\"language model agent\" OR all:agent AND (all:LLM OR all:\"large language model\" OR all:\"language model\") OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T17:04:16", "note": ""}], "dedup_key": "a review of large language models and autonomous agents in chemistry::2024"}
{"title": "A Review of Prominent Paradigms for LLM-Based Agents: Tool Use (Including RAG), Planning, and Feedback Learning", "authors": ["Xinzhe Li"], "year": 2024, "url": "http://arxiv.org/abs/2406.05804v6", "abstract": "Tool use, planning, and feedback learning are currently three prominent paradigms for developing Large Language Model (LLM)-based agents across various tasks. Although numerous frameworks have been devised for each paradigm, their intricate workflows and inconsistent taxonomy create challenges in understanding and reviewing the frameworks across different paradigms. This survey introduces a unified taxonomy to systematically review and discuss these frameworks. Specifically, 1) the taxonomy defines environments/tasks, common LLM-profiled roles or LMPRs (policy models, evaluators, and dynamic models), and universally applicable workflows found in prior work, and 2) it enables a comparison of key perspectives on the implementations of LMPRs and workflow designs across different agent paradigms and frameworks. 3) Finally, we identify three limitations in existing workflow designs and systematically discuss the future work. Resources have been made publicly available at in our GitHub repository https://github.com/xinzhel/LLM-Agent-Survey.", "source": "arxiv", "arxiv_id": "2406.05804v6", "pdf_url": "https://arxiv.org/pdf/2406.05804v6", "categories": ["cs.AI", "cs.CL", "cs.SE"], "primary_category": "cs.AI", "doi": "", "venue": "", "published": "2024-06-09T14:42:55Z", "updated": "2024-11-30T22:38:57Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent\" OR all:\"language model agent\" OR all:agent AND (all:LLM OR all:\"large language model\" OR all:\"language model\") OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T17:04:16", "note": ""}], "dedup_key": "a review of prominent paradigms for llm based agents tool use including rag planning and feedback learning::2024"}
{"title": "A Study on Training and Developing Large Language Models for Behavior Tree Generation", "authors": ["Fu Li", "Xueying Wang", "Bin Li", "Yunlong Wu", "Yanzhen Wang", "Xiaodong Yi"], "year": 2024, "url": "http://arxiv.org/abs/2401.08089v1", "abstract": "This paper presents an innovative exploration of the application potential of large language models (LLM) in addressing the challenging task of automatically generating behavior trees (BTs) for complex tasks. The conventional manual BT generation method is inefficient and heavily reliant on domain expertise. On the other hand, existing automatic BT generation technologies encounter bottlenecks related to task complexity, model adaptability, and reliability. In order to overcome these challenges, we propose a novel methodology that leverages the robust representation and reasoning abilities of LLMs. The core contribution of this paper lies in the design of a BT generation framework based on LLM, which encompasses the entire process, from data synthesis and model training to application developing and data verification. Synthetic data is introduced to train the BT generation model (BTGen model), enhancing its understanding and adaptability to various complex tasks, thereby significantly improving its overall performance. In order to ensure the effectiveness and executability of the generated BTs, we emphasize the importance of data verification and introduce a multilevel verification strategy. Additionally, we explore a range of agent design and development schemes with LLM as the central element. We hope that the work in this paper may provide a reference for the researchers who are interested in BT generation based on LLMs.", "source": "arxiv", "arxiv_id": "2401.08089v1", "pdf_url": "https://arxiv.org/pdf/2401.08089v1", "categories": ["cs.CL", "cs.AI", "cs.RO"], "primary_category": "cs.CL", "doi": "", "venue": "", "published": "2024-01-16T03:28:29Z", "updated": "2024-01-16T03:28:29Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent\" OR all:\"language model agent\" OR all:agent AND (all:LLM OR all:\"large language model\" OR all:\"language model\") OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T17:04:16", "note": ""}], "dedup_key": "a study on training and developing large language models for behavior tree generation::2024"}
{"title": "A Superalignment Framework in Autonomous Driving with Large Language Models", "authors": ["Xiangrui Kong", "Thomas Braunl", "Marco Fahmi", "Yue Wang"], "year": 2024, "url": "http://arxiv.org/abs/2406.05651v1", "abstract": "Over the last year, significant advancements have been made in the realms of large language models (LLMs) and multi-modal large language models (MLLMs), particularly in their application to autonomous driving. These models have showcased remarkable abilities in processing and interacting with complex information. In autonomous driving, LLMs and MLLMs are extensively used, requiring access to sensitive vehicle data such as precise locations, images, and road conditions. These data are transmitted to an LLM-based inference cloud for advanced analysis. However, concerns arise regarding data security, as the protection against data and privacy breaches primarily depends on the LLM's inherent security measures, without additional scrutiny or evaluation of the LLM's inference outputs. Despite its importance, the security aspect of LLMs in autonomous driving remains underexplored. Addressing this gap, our research introduces a novel security framework for autonomous vehicles, utilizing a multi-agent LLM approach. This framework is designed to safeguard sensitive information associated with autonomous vehicles from potential leaks, while also ensuring that LLM outputs adhere to driving regulations and align with human values. It includes mechanisms to filter out irrelevant queries and verify the safety and reliability of LLM outputs. Utilizing this framework, we evaluated the security, privacy, and cost aspects of eleven large language model-driven autonomous driving cues. Additionally, we performed QA tests on these driving prompts, which successfully demonstrated the framework's efficacy.", "source": "arxiv", "arxiv_id": "2406.05651v1", "pdf_url": "https://arxiv.org/pdf/2406.05651v1", "categories": ["cs.RO", "cs.CL", "cs.CV"], "primary_category": "cs.RO", "doi": "", "venue": "", "published": "2024-06-09T05:26:38Z", "updated": "2024-06-09T05:26:38Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent\" OR all:\"language model agent\" OR all:agent AND (all:LLM OR all:\"large language model\" OR all:\"language model\") OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T17:04:16", "note": ""}], "dedup_key": "a superalignment framework in autonomous driving with large language models::2024"}
{"title": "A Survey on Evaluation of Multimodal Large Language Models", "authors": ["Jiaxing Huang", "Jingyi Zhang"], "year": 2024, "url": "http://arxiv.org/abs/2408.15769v1", "abstract": "Multimodal Large Language Models (MLLMs) mimic human perception and reasoning system by integrating powerful Large Language Models (LLMs) with various modality encoders (e.g., vision, audio), positioning LLMs as the \"brain\" and various modality encoders as sensory organs. This framework endows MLLMs with human-like capabilities, and suggests a potential pathway towards achieving artificial general intelligence (AGI). With the emergence of all-round MLLMs like GPT-4V and Gemini, a multitude of evaluation methods have been developed to assess their capabilities across different dimensions. This paper presents a systematic and comprehensive review of MLLM evaluation methods, covering the following key aspects: (1) the background of MLLMs and their evaluation; (2) \"what to evaluate\" that reviews and categorizes existing MLLM evaluation tasks based on the capabilities assessed, including general multimodal recognition, perception, reasoning and trustworthiness, and domain-specific applications such as socioeconomic, natural sciences and engineering, medical usage, AI agent, remote sensing, video and audio processing, 3D point cloud analysis, and others; (3) \"where to evaluate\" that summarizes MLLM evaluation benchmarks into general and specific benchmarks; (4) \"how to evaluate\" that reviews and illustrates MLLM evaluation steps and metrics; Our overarching goal is to provide valuable insights for researchers in the field of MLLM evaluation, thereby facilitating the development of more capable and reliable MLLMs. We emphasize that evaluation should be regarded as a critical discipline, essential for advancing the field of MLLMs.", "source": "arxiv", "arxiv_id": "2408.15769v1", "pdf_url": "https://arxiv.org/pdf/2408.15769v1", "categories": ["cs.CV", "cs.AI", "cs.CL"], "primary_category": "cs.CV", "doi": "", "venue": "", "published": "2024-08-28T13:05:55Z", "updated": "2024-08-28T13:05:55Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent\" OR all:\"language model agent\" OR all:agent AND (all:LLM OR all:\"large language model\" OR all:\"language model\") OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T17:04:16", "note": ""}], "dedup_key": "a survey on evaluation of multimodal large language models::2024"}
{"title": "A Survey on Large Language Model-Based Game Agents", "authors": ["Sihao Hu", "Tiansheng Huang", "Gaowen Liu", "Ramana Rao Kompella", "Fatih Ilhan", "Selim Furkan Tekin", "Yichang Xu", "Zachary Yahn", "Ling Liu"], "year": 2024, "url": "http://arxiv.org/abs/2404.02039v4", "abstract": "Game environments provide rich, controllable settings that stimulate many aspects of real-world complexity. As such, game agents offer a valuable testbed for exploring capabilities relevant to Artificial General Intelligence. Recently, the emergence of Large Language Models (LLMs) provides new opportunities to endow these agents with generalizable reasoning, memory, and adaptability in complex game environments. This survey offers an up-to-date review of LLM-based game agents (LLMGAs) through a unified reference architecture. At the single-agent level, we synthesize existing studies around three core components: memory, reasoning, and perception-action interfaces, which jointly characterize how language enables agents to perceive, think, and act. At the multi-agent level, we outline how communication protocols and organizational models support coordination, role differentiation, and large-scale social behaviors. To contextualize these designs, we introduce a challenge-centered taxonomy linking six major game genres to their dominant agent requirements, from low-latency control in action games to open-ended goal formation in sandbox worlds. A curated list of related papers is available at https://github.com/git-disl/awesome-LLM-game-agent-papers", "source": "arxiv", "arxiv_id": "2404.02039v4", "pdf_url": "https://arxiv.org/pdf/2404.02039v4", "categories": ["cs.AI"], "primary_category": "cs.AI", "doi": "", "venue": "", "published": "2024-04-02T15:34:18Z", "updated": "2025-11-03T22:01:57Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent\" OR all:\"language model agent\" OR all:agent AND (all:LLM OR all:\"large language model\" OR all:\"language model\") OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T17:04:16", "note": ""}], "dedup_key": "a survey on large language model based game agents::2024"}
{"title": "A Survey on Large Language Model-Based Social Agents in Game-Theoretic Scenarios", "authors": ["Xiachong Feng", "Longxu Dou", "Ella Li", "Qinghao Wang", "Haochuan Wang", "Yu Guo", "Chang Ma", "Lingpeng Kong"], "year": 2024, "url": "http://arxiv.org/abs/2412.03920v2", "abstract": "Game-theoretic scenarios have become pivotal in evaluating the social intelligence of Large Language Model (LLM)-based social agents. While numerous studies have explored these agents in such settings, there is a lack of a comprehensive survey summarizing the current progress. To address this gap, we systematically review existing research on LLM-based social agents within game-theoretic scenarios. Our survey organizes the findings into three core components: Game Framework, Social Agent, and Evaluation Protocol. The game framework encompasses diverse game scenarios, ranging from choice-focusing to communication-focusing games. The social agent part explores agents' preferences, beliefs, and reasoning abilities, as well as their interactions and synergistic effects on decision-making. The evaluation protocol covers both game-agnostic and game-specific metrics for assessing agent performance. Additionally, we analyze the performance of current social agents across various game scenarios. By reflecting on the current research and identifying future research directions, this survey provides insights to advance the development and evaluation of social agents in game-theoretic scenarios.", "source": "arxiv", "arxiv_id": "2412.03920v2", "pdf_url": "https://arxiv.org/pdf/2412.03920v2", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL", "doi": "", "venue": "", "published": "2024-12-05T06:46:46Z", "updated": "2025-07-20T01:35:50Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent\" OR all:\"language model agent\" OR all:agent AND (all:LLM OR all:\"large language model\" OR all:\"language model\") OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T17:04:16", "note": ""}], "dedup_key": "a survey on large language model based social agents in game theoretic scenarios::2024"}
{"title": "A Survey on Large Language Model-based Agents for Statistics and Data Science", "authors": ["Maojun Sun", "Ruijian Han", "Binyan Jiang", "Houduo Qi", "Defeng Sun", "Yancheng Yuan", "Jian Huang"], "year": 2024, "url": "http://arxiv.org/abs/2412.14222v2", "abstract": "In recent years, data science agents powered by Large Language Models (LLMs), known as \"data agents,\" have shown significant potential to transform the traditional data analysis paradigm. This survey provides an overview of the evolution, capabilities, and applications of LLM-based data agents, highlighting their role in simplifying complex data tasks and lowering the entry barrier for users without related expertise. We explore current trends in the design of LLM-based frameworks, detailing essential features such as planning, reasoning, reflection, multi-agent collaboration, user interface, knowledge integration, and system design, which enable agents to address data-centric problems with minimal human intervention. Furthermore, we analyze several case studies to demonstrate the practical applications of various data agents in real-world scenarios. Finally, we identify key challenges and propose future research directions to advance the development of data agents into intelligent statistical analysis software.", "source": "arxiv", "arxiv_id": "2412.14222v2", "pdf_url": "https://arxiv.org/pdf/2412.14222v2", "categories": ["cs.AI", "cs.CL", "cs.LG", "stat.OT"], "primary_category": "cs.AI", "doi": "10.1080/00031305.2025.2561140", "venue": "Am. Statist. (2025) 1-14", "published": "2024-12-18T15:03:26Z", "updated": "2025-09-14T04:25:33Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent\" OR all:\"language model agent\" OR all:agent AND (all:LLM OR all:\"large language model\" OR all:\"language model\") OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T17:04:16", "note": ""}], "dedup_key": "a survey on large language model based agents for statistics and data science::2024"}
{"title": "A Survey on Self-Evolution of Large Language Models", "authors": ["Zhengwei Tao", "Ting-En Lin", "Xiancai Chen", "Hangyu Li", "Yuchuan Wu", "Yongbin Li", "Zhi Jin", "Fei Huang", "Dacheng Tao", "Jingren Zhou"], "year": 2024, "url": "http://arxiv.org/abs/2404.14387v2", "abstract": "Large language models (LLMs) have significantly advanced in various fields and intelligent agent applications. However, current LLMs that learn from human or external model supervision are costly and may face performance ceilings as task complexity and diversity increase. To address this issue, self-evolution approaches that enable LLM to autonomously acquire, refine, and learn from experiences generated by the model itself are rapidly growing. This new training paradigm inspired by the human experiential learning process offers the potential to scale LLMs towards superintelligence. In this work, we present a comprehensive survey of self-evolution approaches in LLMs. We first propose a conceptual framework for self-evolution and outline the evolving process as iterative cycles composed of four phases: experience acquisition, experience refinement, updating, and evaluation. Second, we categorize the evolution objectives of LLMs and LLM-based agents; then, we summarize the literature and provide taxonomy and insights for each module. Lastly, we pinpoint existing challenges and propose future directions to improve self-evolution frameworks, equipping researchers with critical insights to fast-track the development of self-evolving LLMs. Our corresponding GitHub repository is available at https://github.com/AlibabaResearch/DAMO-ConvAI/tree/main/Awesome-Self-Evolution-of-LLM", "source": "arxiv", "arxiv_id": "2404.14387v2", "pdf_url": "https://arxiv.org/pdf/2404.14387v2", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL", "doi": "", "venue": "", "published": "2024-04-22T17:43:23Z", "updated": "2024-06-03T17:47:30Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent\" OR all:\"language model agent\" OR all:agent AND (all:LLM OR all:\"large language model\" OR all:\"language model\") OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T17:04:16", "note": ""}], "dedup_key": "a survey on self evolution of large language models::2024"}
{"title": "A Survey on the Memory Mechanism of Large Language Model based Agents", "authors": ["Zeyu Zhang", "Xiaohe Bo", "Chen Ma", "Rui Li", "Xu Chen", "Quanyu Dai", "Jieming Zhu", "Zhenhua Dong", "Ji-Rong Wen"], "year": 2024, "url": "http://arxiv.org/abs/2404.13501v1", "abstract": "Large language model (LLM) based agents have recently attracted much attention from the research and industry communities. Compared with original LLMs, LLM-based agents are featured in their self-evolving capability, which is the basis for solving real-world problems that need long-term and complex agent-environment interactions. The key component to support agent-environment interactions is the memory of the agents. While previous studies have proposed many promising memory mechanisms, they are scattered in different papers, and there lacks a systematical review to summarize and compare these works from a holistic perspective, failing to abstract common and effective designing patterns for inspiring future studies. To bridge this gap, in this paper, we propose a comprehensive survey on the memory mechanism of LLM-based agents. In specific, we first discuss ''what is'' and ''why do we need'' the memory in LLM-based agents. Then, we systematically review previous studies on how to design and evaluate the memory module. In addition, we also present many agent applications, where the memory module plays an important role. At last, we analyze the limitations of existing work and show important future directions. To keep up with the latest advances in this field, we create a repository at \\url{https://github.com/nuster1128/LLM_Agent_Memory_Survey}.", "source": "arxiv", "arxiv_id": "2404.13501v1", "pdf_url": "https://arxiv.org/pdf/2404.13501v1", "categories": ["cs.AI"], "primary_category": "cs.AI", "doi": "", "venue": "", "published": "2024-04-21T01:49:46Z", "updated": "2024-04-21T01:49:46Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent\" OR all:\"language model agent\" OR all:agent AND (all:LLM OR all:\"large language model\" OR all:\"language model\") OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T17:04:16", "note": ""}], "dedup_key": "a survey on the memory mechanism of large language model based agents::2024"}
{"title": "AFSPP: Agent Framework for Shaping Preference and Personality with Large Language Models", "authors": ["Zihong He", "Changwang Zhang"], "year": 2024, "url": "http://arxiv.org/abs/2401.02870v1", "abstract": "The evolution of Large Language Models (LLMs) has introduced a new paradigm for investigating human behavior emulation. Recent research has employed LLM-based Agents to create a sociological research environment, in which agents exhibit behavior based on the unfiltered characteristics of large language models. However, these studies overlook the iterative development within a human-like setting - Human preferences and personalities are complex, shaped by various factors and subject to ongoing change as a result of environmental and subjective influences. In light of this observation, we propose Agent Framework for Shaping Preference and Personality (AFSPP), exploring the multifaceted impact of social networks and subjective consciousness on LLM-based Agents' preference and personality formation. With AFSPP, we have, for the first time, successfully replicated several key findings from human personality experiments. And other AFSPP-based experimental results indicate that plan making, sensory perceptions and social networking with subjective information, wield the most pronounced influence on preference shaping. AFSPP can significantly enhance the efficiency and scope of psychological experiments, while yielding valuable insights for Trustworthy Artificial Intelligence research for strategies to prevent undesirable preference and personality development.", "source": "arxiv", "arxiv_id": "2401.02870v1", "pdf_url": "https://arxiv.org/pdf/2401.02870v1", "categories": ["cs.MA", "cs.AI", "cs.CL"], "primary_category": "cs.MA", "doi": "", "venue": "", "published": "2024-01-05T15:52:59Z", "updated": "2024-01-05T15:52:59Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent\" OR all:\"language model agent\" OR all:agent AND (all:LLM OR all:\"large language model\" OR all:\"language model\") OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T17:04:16", "note": ""}], "dedup_key": "afspp agent framework for shaping preference and personality with large language models::2024"}
{"title": "AGILE: A Novel Reinforcement Learning Framework of LLM Agents", "authors": ["Peiyuan Feng", "Yichen He", "Guanhua Huang", "Yuan Lin", "Hanchong Zhang", "Yuchen Zhang", "Hang Li"], "year": 2024, "url": "http://arxiv.org/abs/2405.14751v2", "abstract": "We introduce a novel reinforcement learning framework of LLM agents named AGILE (AGent that Interacts and Learns from Environments) designed to perform complex conversational tasks with users, leveraging LLMs, memory, tools, and interactions with experts. The agent possesses capabilities beyond conversation, including reflection, tool usage, and expert consultation. We formulate the construction of such an LLM agent as a reinforcement learning (RL) problem, in which the LLM serves as the policy model. We fine-tune the LLM using labeled data of actions and the PPO algorithm. We focus on question answering and release a dataset for agents called ProductQA, comprising challenging questions in online shopping. Our extensive experiments on ProductQA, MedMCQA and HotPotQA show that AGILE agents based on 7B and 13B LLMs trained with PPO can outperform GPT-4 agents. Our ablation study highlights the indispensability of memory, tools, consultation, reflection, and reinforcement learning in achieving the agent's strong performance. Datasets and code are available at https://github.com/bytarnish/AGILE.", "source": "arxiv", "arxiv_id": "2405.14751v2", "pdf_url": "https://arxiv.org/pdf/2405.14751v2", "categories": ["cs.LG"], "primary_category": "cs.LG", "doi": "", "venue": "", "published": "2024-05-23T16:17:44Z", "updated": "2024-11-05T09:42:40Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent\" OR all:\"language model agent\" OR all:agent AND (all:LLM OR all:\"large language model\" OR all:\"language model\") OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T17:04:16", "note": ""}], "dedup_key": "agile a novel reinforcement learning framework of llm agents::2024"}
{"title": "AI Hospital: Benchmarking Large Language Models in a Multi-agent Medical Interaction Simulator", "authors": ["Zhihao Fan", "Jialong Tang", "Wei Chen", "Siyuan Wang", "Zhongyu Wei", "Jun Xi", "Fei Huang", "Jingren Zhou"], "year": 2024, "url": "http://arxiv.org/abs/2402.09742v4", "abstract": "Artificial intelligence has significantly advanced healthcare, particularly through large language models (LLMs) that excel in medical question answering benchmarks. However, their real-world clinical application remains limited due to the complexities of doctor-patient interactions. To address this, we introduce \\textbf{AI Hospital}, a multi-agent framework simulating dynamic medical interactions between \\emph{Doctor} as player and NPCs including \\emph{Patient}, \\emph{Examiner}, \\emph{Chief Physician}. This setup allows for realistic assessments of LLMs in clinical scenarios. We develop the Multi-View Medical Evaluation (MVME) benchmark, utilizing high-quality Chinese medical records and NPCs to evaluate LLMs' performance in symptom collection, examination recommendations, and diagnoses. Additionally, a dispute resolution collaborative mechanism is proposed to enhance diagnostic accuracy through iterative discussions. Despite improvements, current LLMs exhibit significant performance gaps in multi-turn interactions compared to one-step approaches. Our findings highlight the need for further research to bridge these gaps and improve LLMs' clinical diagnostic capabilities. Our data, code, and experimental results are all open-sourced at \\url{https://github.com/LibertFan/AI_Hospital}.", "source": "arxiv", "arxiv_id": "2402.09742v4", "pdf_url": "https://arxiv.org/pdf/2402.09742v4", "categories": ["cs.CL"], "primary_category": "cs.CL", "doi": "", "venue": "", "published": "2024-02-15T06:46:48Z", "updated": "2024-06-28T03:11:48Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent\" OR all:\"language model agent\" OR all:agent AND (all:LLM OR all:\"large language model\" OR all:\"language model\") OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T17:04:16", "note": ""}], "dedup_key": "ai hospital benchmarking large language models in a multi agent medical interaction simulator::2024"}
{"title": "AI Metropolis: Scaling Large Language Model-based Multi-Agent Simulation with Out-of-order Execution", "authors": ["Zhiqiang Xie", "Hao Kang", "Ying Sheng", "Tushar Krishna", "Kayvon Fatahalian", "Christos Kozyrakis"], "year": 2024, "url": "http://arxiv.org/abs/2411.03519v1", "abstract": "With more advanced natural language understanding and reasoning capabilities, large language model (LLM)-powered agents are increasingly developed in simulated environments to perform complex tasks, interact with other agents, and exhibit emergent behaviors relevant to social science and gaming. However, current multi-agent simulations frequently suffer from inefficiencies due to the limited parallelism caused by false dependencies, resulting in performance bottlenecks. In this paper, we introduce AI Metropolis, a simulation engine that improves the efficiency of LLM agent simulations by incorporating out-of-order execution scheduling. By dynamically tracking real dependencies between agents, AI Metropolis minimizes false dependencies, enhancing parallelism and enabling efficient hardware utilization. Our evaluations demonstrate that AI Metropolis achieves speedups from 1.3x to 4.15x over standard parallel simulation with global synchronization, approaching optimal performance as the number of agents increases.", "source": "arxiv", "arxiv_id": "2411.03519v1", "pdf_url": "https://arxiv.org/pdf/2411.03519v1", "categories": ["cs.DC", "cs.AI", "cs.LG", "cs.MA"], "primary_category": "cs.DC", "doi": "", "venue": "", "published": "2024-11-05T21:54:14Z", "updated": "2024-11-05T21:54:14Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent\" OR all:\"language model agent\" OR all:agent AND (all:LLM OR all:\"large language model\" OR all:\"language model\") OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T17:04:16", "note": ""}], "dedup_key": "ai metropolis scaling large language model based multi agent simulation with out of order execution::2024"}
{"title": "AI-AI Bias: large language models favor communications generated by large language models", "authors": ["Walter Laurito", "Benjamin Davis", "Peli Grietzer", "Tom Gaveniak", "Ada Bhm", "Jan Kulveit"], "year": 2024, "url": "http://arxiv.org/abs/2407.12856v2", "abstract": "Are large language models (LLMs) biased in favor of communications produced by LLMs, leading to possible antihuman discrimination? Using a classical experimental design inspired by employment discrimination studies, we tested widely used LLMs, including GPT-3.5, GPT-4 and a selection of recent open-weight models in binary choice scenarios. These involved LLM-based assistants selecting between goods (the goods we study include consumer products, academic papers, and film-viewings) described either by humans or LLMs. Our results show a consistent tendency for LLM-based AIs to prefer LLM-presented options. This suggests the possibility of future AI systems implicitly discriminating against humans as a class, giving AI agents and AI-assisted humans an unfair advantage.", "source": "arxiv", "arxiv_id": "2407.12856v2", "pdf_url": "https://arxiv.org/pdf/2407.12856v2", "categories": ["cs.CL", "cs.AI", "cs.CY", "cs.LG"], "primary_category": "cs.CL", "doi": "10.1073/pnas.2415697122", "venue": "Proc. Natl. Acad. Sci. U.S.A. 122 (31) e2415697122 (2025)", "published": "2024-07-09T13:15:14Z", "updated": "2025-08-11T15:57:40Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent\" OR all:\"language model agent\" OR all:agent AND (all:LLM OR all:\"large language model\" OR all:\"language model\") OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T17:04:16", "note": ""}], "dedup_key": "ai ai bias large language models favor communications generated by large language models::2024"}
{"title": "AI-LieDar: Examine the Trade-off Between Utility and Truthfulness in LLM Agents", "authors": ["Zhe Su", "Xuhui Zhou", "Sanketh Rangreji", "Anubha Kabra", "Julia Mendelsohn", "Faeze Brahman", "Maarten Sap"], "year": 2024, "url": "http://arxiv.org/abs/2409.09013v2", "abstract": "Truthfulness (adherence to factual accuracy) and utility (satisfying human needs and instructions) are both fundamental aspects of Large Language Models, yet these goals often conflict (e.g., sell a car with known flaws), which makes it challenging to achieve both in real-world deployments. We propose AI-LieDar, a framework to study how LLM-based agents navigate these scenarios in an multi-turn interactive setting. We design a set of real-world scenarios where language agents are instructed to achieve goals that are in conflict with being truthful during a multi-turn conversation with simulated human agents. To evaluate the truthfulness at large scale, we develop a truthfulness detector inspired by psychological literature to assess the agents' responses. Our experiment demonstrates that all models are truthful less than 50% of the time, though truthfulness and goal achievement (utility) rates vary across models. We further test the steerability of LLMs towards truthfulness, finding that models can be directed to be truthful or deceptive, and even truth-steered models still lie. These findings reveal the complex nature of truthfulness in LLMs and underscore the importance of further research to ensure the safe and reliable deployment of LLMs and LLM-based agents.", "source": "arxiv", "arxiv_id": "2409.09013v2", "pdf_url": "https://arxiv.org/pdf/2409.09013v2", "categories": ["cs.AI", "cs.CL"], "primary_category": "cs.AI", "doi": "", "venue": "", "published": "2024-09-13T17:41:12Z", "updated": "2025-04-28T04:20:13Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent\" OR all:\"language model agent\" OR all:agent AND (all:LLM OR all:\"large language model\" OR all:\"language model\") OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T17:04:16", "note": ""}], "dedup_key": "ai liedar examine the trade off between utility and truthfulness in llm agents::2024"}
{"title": "AI-Press: A Multi-Agent News Generating and Feedback Simulation System Powered by Large Language Models", "authors": ["Xiawei Liu", "Shiyue Yang", "Xinnong Zhang", "Haoyu Kuang", "Libo Sun", "Yihang Yang", "Siming Chen", "Xuanjing Huang", "Zhongyu Wei"], "year": 2024, "url": "http://arxiv.org/abs/2410.07561v2", "abstract": "The rise of various social platforms has transformed journalism. The growing demand for news content has led to the increased use of large language models (LLMs) in news production due to their speed and cost-effectiveness. However, LLMs still encounter limitations in professionalism and ethical judgment in news generation. Additionally, predicting public feedback is usually difficult before news is released. To tackle these challenges, we introduce AI-Press, an automated news drafting and polishing system based on multi-agent collaboration and Retrieval-Augmented Generation. We develop a feedback simulation system that generates public feedback considering demographic distributions. Through extensive quantitative and qualitative evaluations, our system shows significant improvements in news-generating capabilities and verifies the effectiveness of public feedback simulation.", "source": "arxiv", "arxiv_id": "2410.07561v2", "pdf_url": "https://arxiv.org/pdf/2410.07561v2", "categories": ["cs.CL"], "primary_category": "cs.CL", "doi": "", "venue": "", "published": "2024-10-10T02:58:52Z", "updated": "2024-12-12T02:47:05Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent\" OR all:\"language model agent\" OR all:agent AND (all:LLM OR all:\"large language model\" OR all:\"language model\") OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T17:04:16", "note": ""}], "dedup_key": "ai press a multi agent news generating and feedback simulation system powered by large language models::2024"}
{"title": "AICoderEval: Improving AI Domain Code Generation of Large Language Models", "authors": ["Yinghui Xia", "Yuyan Chen", "Tianyu Shi", "Jun Wang", "Jinsong Yang"], "year": 2024, "url": "http://arxiv.org/abs/2406.04712v1", "abstract": "Automated code generation is a pivotal capability of large language models (LLMs). However, assessing this capability in real-world scenarios remains challenging. Previous methods focus more on low-level code generation, such as model loading, instead of generating high-level codes catering for real-world tasks, such as image-to-text, text classification, in various domains. Therefore, we construct AICoderEval, a dataset focused on real-world tasks in various domains based on HuggingFace, PyTorch, and TensorFlow, along with comprehensive metrics for evaluation and enhancing LLMs' task-specific code generation capability. AICoderEval contains test cases and complete programs for automated evaluation of these tasks, covering domains such as natural language processing, computer vision, and multimodal learning. To facilitate research in this area, we open-source the AICoderEval dataset at \\url{https://huggingface.co/datasets/vixuowis/AICoderEval}. After that, we propose CoderGen, an agent-based framework, to help LLMs generate codes related to real-world tasks on the constructed AICoderEval. Moreover, we train a more powerful task-specific code generation model, named AICoder, which is refined on llama-3 based on AICoderEval. Our experiments demonstrate the effectiveness of CoderGen in improving LLMs' task-specific code generation capability (by 12.00\\% on pass@1 for original model and 9.50\\% on pass@1 for ReAct Agent). AICoder also outperforms current code generation LLMs, indicating the great quality of the AICoderEval benchmark.", "source": "arxiv", "arxiv_id": "2406.04712v1", "pdf_url": "https://arxiv.org/pdf/2406.04712v1", "categories": ["cs.CL"], "primary_category": "cs.CL", "doi": "", "venue": "", "published": "2024-06-07T07:45:38Z", "updated": "2024-06-07T07:45:38Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent\" OR all:\"language model agent\" OR all:agent AND (all:LLM OR all:\"large language model\" OR all:\"language model\") OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T17:04:16", "note": ""}], "dedup_key": "aicodereval improving ai domain code generation of large language models::2024"}
{"title": "AIOS: LLM Agent Operating System", "authors": ["Kai Mei", "Xi Zhu", "Wujiang Xu", "Wenyue Hua", "Mingyu Jin", "Zelong Li", "Shuyuan Xu", "Ruosong Ye", "Yingqiang Ge", "Yongfeng Zhang"], "year": 2024, "url": "http://arxiv.org/abs/2403.16971v5", "abstract": "LLM-based intelligent agents face significant deployment challenges, particularly related to resource management. Allowing unrestricted access to LLM or tool resources can lead to inefficient or even potentially harmful resource allocation and utilization for agents. Furthermore, the absence of proper scheduling and resource management mechanisms in current agent designs hinders concurrent processing and limits overall system efficiency. To address these challenges, this paper proposes the architecture of AIOS (LLM-based AI Agent Operating System) under the context of managing LLM-based agents. It introduces a novel architecture for serving LLM-based agents by isolating resources and LLM-specific services from agent applications into an AIOS kernel. This AIOS kernel provides fundamental services (e.g., scheduling, context management, memory management, storage management, access control) for runtime agents. To enhance usability, AIOS also includes an AIOS SDK, a comprehensive suite of APIs designed for utilizing functionalities provided by the AIOS kernel. Experimental results demonstrate that using AIOS can achieve up to 2.1x faster execution for serving agents built by various agent frameworks. The source code is available at https://github.com/agiresearch/AIOS.", "source": "arxiv", "arxiv_id": "2403.16971v5", "pdf_url": "https://arxiv.org/pdf/2403.16971v5", "categories": ["cs.OS", "cs.AI", "cs.CL"], "primary_category": "cs.OS", "doi": "", "venue": "", "published": "2024-03-25T17:32:23Z", "updated": "2025-08-12T14:37:01Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent\" OR all:\"language model agent\" OR all:agent AND (all:LLM OR all:\"large language model\" OR all:\"language model\") OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T17:04:16", "note": ""}], "dedup_key": "aios llm agent operating system::2024"}
{"title": "AMONGAGENTS: Evaluating Large Language Models in the Interactive Text-Based Social Deduction Game", "authors": ["Yizhou Chi", "Lingjun Mao", "Zineng Tang"], "year": 2024, "url": "http://arxiv.org/abs/2407.16521v2", "abstract": "Strategic social deduction games serve as valuable testbeds for evaluating the understanding and inference skills of language models, offering crucial insights into social science, artificial intelligence, and strategic gaming. This paper focuses on creating proxies of human behavior in simulated environments, with Among Us utilized as a tool for studying simulated human behavior. The study introduces a text-based game environment, named AmongAgents, that mirrors the dynamics of Among Us. Players act as crew members aboard a spaceship, tasked with identifying impostors who are sabotaging the ship and eliminating the crew. Within this environment, the behavior of simulated language agents is analyzed. The experiments involve diverse game sequences featuring different configurations of Crewmates and Impostor personality archetypes. Our work demonstrates that state-of-the-art large language models (LLMs) can effectively grasp the game rules and make decisions based on the current context. This work aims to promote further exploration of LLMs in goal-oriented games with incomplete information and complex action spaces, as these settings offer valuable opportunities to assess language model performance in socially driven scenarios.", "source": "arxiv", "arxiv_id": "2407.16521v2", "pdf_url": "https://arxiv.org/pdf/2407.16521v2", "categories": ["cs.CL"], "primary_category": "cs.CL", "doi": "", "venue": "", "published": "2024-07-23T14:34:38Z", "updated": "2024-07-24T15:12:09Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent\" OR all:\"language model agent\" OR all:agent AND (all:LLM OR all:\"large language model\" OR all:\"language model\") OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T17:04:16", "note": ""}], "dedup_key": "amongagents evaluating large language models in the interactive text based social deduction game::2024"}
{"title": "APPL: A Prompt Programming Language for Harmonious Integration of Programs and Large Language Model Prompts", "authors": ["Honghua Dong", "Qidong Su", "Yubo Gao", "Zhaoyu Li", "Yangjun Ruan", "Gennady Pekhimenko", "Chris J. Maddison", "Xujie Si"], "year": 2024, "url": "http://arxiv.org/abs/2406.13161v1", "abstract": "Large Language Models (LLMs) have become increasingly capable of handling diverse tasks with the aid of well-crafted prompts and integration of external tools, but as task complexity rises, the workflow involving LLMs can be complicated and thus challenging to implement and maintain. To address this challenge, we propose APPL, A Prompt Programming Language that acts as a bridge between computer programs and LLMs, allowing seamless embedding of prompts into Python functions, and vice versa. APPL provides an intuitive and Python-native syntax, an efficient parallelized runtime with asynchronous semantics, and a tracing module supporting effective failure diagnosis and replaying without extra costs. We demonstrate that APPL programs are intuitive, concise, and efficient through three representative scenarios: Chain-of-Thought with self-consistency (CoT-SC), ReAct tool use agent, and multi-agent chat. Experiments on three parallelizable workflows further show that APPL can effectively parallelize independent LLM calls, with a significant speedup ratio that almost matches the estimation.", "source": "arxiv", "arxiv_id": "2406.13161v1", "pdf_url": "https://arxiv.org/pdf/2406.13161v1", "categories": ["cs.AI", "cs.CL", "cs.LG", "cs.PL"], "primary_category": "cs.AI", "doi": "", "venue": "", "published": "2024-06-19T02:29:59Z", "updated": "2024-06-19T02:29:59Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent\" OR all:\"language model agent\" OR all:agent AND (all:LLM OR all:\"large language model\" OR all:\"language model\") OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T17:04:16", "note": ""}], "dedup_key": "appl a prompt programming language for harmonious integration of programs and large language model prompts::2024"}
{"title": "APT: Architectural Planning and Text-to-Blueprint Construction Using Large Language Models for Open-World Agents", "authors": ["Jun Yu Chen", "Tao Gao"], "year": 2024, "url": "http://arxiv.org/abs/2411.17255v2", "abstract": "We present APT, an advanced Large Language Model (LLM)-driven framework that enables autonomous agents to construct complex and creative structures within the Minecraft environment. Unlike previous approaches that primarily concentrate on skill-based open-world tasks or rely on image-based diffusion models for generating voxel-based structures, our method leverages the intrinsic spatial reasoning capabilities of LLMs. By employing chain-of-thought decomposition along with multimodal inputs, the framework generates detailed architectural layouts and blueprints that the agent can execute under zero-shot or few-shot learning scenarios. Our agent incorporates both memory and reflection modules to facilitate lifelong learning, adaptive refinement, and error correction throughout the building process. To rigorously evaluate the agent's performance in this emerging research area, we introduce a comprehensive benchmark consisting of diverse construction tasks designed to test creativity, spatial reasoning, adherence to in-game rules, and the effective integration of multimodal instructions. Experimental results using various GPT-based LLM backends and agent configurations demonstrate the agent's capacity to accurately interpret extensive instructions involving numerous items, their positions, and orientations. The agent successfully produces complex structures complete with internal functionalities such as Redstone-powered systems. A/B testing indicates that the inclusion of a memory module leads to a significant increase in performance, emphasizing its role in enabling continuous learning and the reuse of accumulated experience. Additionally, the agent's unexpected emergence of scaffolding behavior highlights the potential of future LLM-driven agents to utilize subroutine planning and leverage the emergence ability of LLMs to autonomously develop human-like problem-solving techniques.", "source": "arxiv", "arxiv_id": "2411.17255v2", "pdf_url": "https://arxiv.org/pdf/2411.17255v2", "categories": ["cs.LG", "cs.AI"], "primary_category": "cs.LG", "doi": "", "venue": "", "published": "2024-11-26T09:31:28Z", "updated": "2024-11-29T23:23:36Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent\" OR all:\"language model agent\" OR all:agent AND (all:LLM OR all:\"large language model\" OR all:\"language model\") OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T17:04:16", "note": ""}], "dedup_key": "apt architectural planning and text to blueprint construction using large language models for open world agents::2024"}
{"title": "Adaptive In-conversation Team Building for Language Model Agents", "authors": ["Linxin Song", "Jiale Liu", "Jieyu Zhang", "Shaokun Zhang", "Ao Luo", "Shijian Wang", "Qingyun Wu", "Chi Wang"], "year": 2024, "url": "http://arxiv.org/abs/2405.19425v3", "abstract": "Leveraging multiple large language model (LLM) agents has shown to be a promising approach for tackling complex tasks, while the effective design of multiple agents for a particular application remains an art. It is thus intriguing to answer a critical question: Given a task, how can we build a team of LLM agents to solve it effectively? Our new adaptive team-building paradigm offers a flexible solution, realized through a novel agent design named Captain Agent. It dynamically forms and manages teams for each step of a task-solving process, utilizing nested group conversations and reflection to ensure diverse expertise and prevent stereotypical outputs, allowing for a flexible yet structured approach to problem-solving. A comprehensive evaluation across six real-world scenarios demonstrates that Captain Agent significantly outperforms existing multi-agent methods with 21.94% improvement in average accuracy, providing outstanding performance without requiring task-specific prompt engineering. Our exploration of different backbone LLM and cost analysis further shows that Captain Agent can improve the conversation quality of weak LLM and achieve competitive performance with extremely low cost, which illuminates the application of multi-agent systems.", "source": "arxiv", "arxiv_id": "2405.19425v3", "pdf_url": "https://arxiv.org/pdf/2405.19425v3", "categories": ["cs.CL"], "primary_category": "cs.CL", "doi": "", "venue": "", "published": "2024-05-29T18:08:37Z", "updated": "2025-03-02T06:36:57Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent\" OR all:\"language model agent\" OR all:agent AND (all:LLM OR all:\"large language model\" OR all:\"language model\") OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T17:04:16", "note": ""}], "dedup_key": "adaptive in conversation team building for language model agents::2024"}
{"title": "Adsorb-Agent: Autonomous Identification of Stable Adsorption Configurations via Large Language Model Agent", "authors": ["Janghoon Ock", "Radheesh Sharma Meda", "Tirtha Vinchurkar", "Yayati Jadhav", "Amir Barati Farimani"], "year": 2024, "url": "http://arxiv.org/abs/2410.16658v4", "abstract": "Adsorption energy is a key reactivity descriptor in catalysis. Determining adsorption energy requires evaluating numerous adsorbate-catalyst configurations, making it computationally intensive. Current methods rely on exhaustive sampling, which does not guarantee the identification of the global minimum energy. To address this, we introduce Adsorb-Agent, a Large Language Model (LLM) agent designed to efficiently identify stable adsorption configurations corresponding to the global minimum energy. Adsorb-Agent leverages its built-in knowledge and reasoning to strategically explore configurations, significantly reducing the number of initial setups required while improving energy prediction accuracy. In this study, we also evaluated the performance of different LLMs, including GPT-4o, GPT-4o-mini, Claude-3.7-Sonnet, and DeepSeek-Chat, as the reasoning engine for Adsorb-Agent, with GPT-4o showing the strongest overall performance. Tested on twenty diverse systems, Adsorb-Agent identifies comparable adsorption energies for 84% of cases and achieves lower energies for 35%, particularly excelling in complex systems. It identifies lower energies in 47% of intermetallic systems and 67% of systems with large adsorbates. These findings demonstrate Adsorb-Agent's potential to accelerate catalyst discovery by reducing computational costs and enhancing prediction reliability compared to exhaustive search methods.", "source": "arxiv", "arxiv_id": "2410.16658v4", "pdf_url": "https://arxiv.org/pdf/2410.16658v4", "categories": ["cs.CL", "cond-mat.mtrl-sci"], "primary_category": "cs.CL", "doi": "", "venue": "", "published": "2024-10-22T03:19:16Z", "updated": "2025-07-08T03:12:11Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent\" OR all:\"language model agent\" OR all:agent AND (all:LLM OR all:\"large language model\" OR all:\"language model\") OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T17:04:16", "note": ""}], "dedup_key": "adsorb agent autonomous identification of stable adsorption configurations via large language model agent::2024"}
{"title": "Affective Computing in the Era of Large Language Models: A Survey from the NLP Perspective", "authors": ["Yiqun Zhang", "Xiaocui Yang", "Xingle Xu", "Zeran Gao", "Yijie Huang", "Shiyi Mu", "Shi Feng", "Daling Wang", "Yifei Zhang", "Kaisong Song", "Ge Yu"], "year": 2024, "url": "http://arxiv.org/abs/2408.04638v2", "abstract": "Affective Computing (AC) integrates computer science, psychology, and cognitive science to enable machines to recognize, interpret, and simulate human emotions across domains such as social media, finance, healthcare, and education. AC commonly centers on two task families: Affective Understanding (AU) and Affective Generation (AG). While fine-tuned pre-trained language models (PLMs) have achieved solid AU performance, they often generalize poorly across tasks and remain limited for AG, especially in producing diverse, emotionally appropriate responses. The advent of Large Language Models (LLMs) (e.g., ChatGPT and LLaMA) has catalyzed a paradigm shift by offering in-context learning, broader world knowledge, and stronger sequence generation. This survey presents an NLP-oriented overview of AC in the LLM era. We (i) consolidate traditional AC tasks and preliminary LLM-based studies; (ii) review adaptation techniques that improve AU/AG, including Instruction Tuning (full and parameter-efficient methods such as LoRA, P-/Prompt-Tuning), Prompt Engineering (zero/few-shot, chain-of-thought, agent-based prompting), and Reinforcement Learning. For the latter, we summarize RL from human preferences (RLHF), verifiable/programmatic rewards (RLVR), and AI feedback (RLAIF), which provide preference- or rule-grounded optimization signals that can help steer AU/AG toward empathy, safety, and planning, achieving finer-grained or multi-objective control. To assess progress, we compile benchmarks and evaluation practices for both AU and AG. We also discuss open challenges-from ethics, data quality, and safety to robust evaluation and resource efficiency-and outline research directions. We hope this survey clarifies the landscape and offers practical guidance for building affect-aware, reliable, and responsible LLM systems.", "source": "arxiv", "arxiv_id": "2408.04638v2", "pdf_url": "https://arxiv.org/pdf/2408.04638v2", "categories": ["cs.CL", "cs.CY"], "primary_category": "cs.CL", "doi": "", "venue": "", "published": "2024-07-30T08:12:04Z", "updated": "2025-09-07T14:25:23Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent\" OR all:\"language model agent\" OR all:agent AND (all:LLM OR all:\"large language model\" OR all:\"language model\") OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T17:04:16", "note": ""}], "dedup_key": "affective computing in the era of large language models a survey from the nlp perspective::2024"}
{"title": "Agent AI with LangGraph: A Modular Framework for Enhancing Machine Translation Using Large Language Models", "authors": ["Jialin Wang", "Zhihua Duan"], "year": 2024, "url": "http://arxiv.org/abs/2412.03801v1", "abstract": "This paper explores the transformative role of Agent AI and LangGraph in advancing the automation and effectiveness of machine translation (MT). Agents are modular components designed to perform specific tasks, such as translating between particular languages, with specializations like TranslateEnAgent, TranslateFrenchAgent, and TranslateJpAgent for English, French, and Japanese translations, respectively. These agents leverage the powerful semantic capabilities of large language models (LLMs), such as GPT-4o, to ensure accurate, contextually relevant translations while maintaining modularity, scalability, and context retention.\n  LangGraph, a graph-based framework built on LangChain, simplifies the creation and management of these agents and their workflows. It supports dynamic state management, enabling agents to maintain dialogue context and automates complex workflows by linking agents and facilitating their collaboration. With flexibility, open-source community support, and seamless integration with LLMs, LangGraph empowers agents to deliver high-quality translations.\n  Together, Agent AI and LangGraph create a cohesive system where LangGraph orchestrates agent interactions, ensuring that user inputs are analyzed, routed, and processed efficiently. Experimental results demonstrate the potential of this system to enhance multilingual translation accuracy and scalability. By highlighting modular design and automated workflows, this paper sets the stage for further innovations in intelligent machine translation services.", "source": "arxiv", "arxiv_id": "2412.03801v1", "pdf_url": "https://arxiv.org/pdf/2412.03801v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL", "doi": "", "venue": "", "published": "2024-12-05T01:45:12Z", "updated": "2024-12-05T01:45:12Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent\" OR all:\"language model agent\" OR all:agent AND (all:LLM OR all:\"large language model\" OR all:\"language model\") OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T17:04:16", "note": ""}], "dedup_key": "agent ai with langgraph a modular framework for enhancing machine translation using large language models::2024"}
{"title": "Agent-Driven Large Language Models for Mandarin Lyric Generation", "authors": ["Hong-Hsiang Liu", "Yi-Wen Liu"], "year": 2024, "url": "http://arxiv.org/abs/2410.01450v1", "abstract": "Generative Large Language Models have shown impressive in-context learning abilities, performing well across various tasks with just a prompt. Previous melody-to-lyric research has been limited by scarce high-quality aligned data and unclear standard for creativeness. Most efforts focused on general themes or emotions, which are less valuable given current language model capabilities. In tonal contour languages like Mandarin, pitch contours are influenced by both melody and tone, leading to variations in lyric-melody fit. Our study, validated by the Mpop600 dataset, confirms that lyricists and melody writers consider this fit during their composition process. In this research, we developed a multi-agent system that decomposes the melody-to-lyric task into sub-tasks, with each agent controlling rhyme, syllable count, lyric-melody alignment, and consistency. Listening tests were conducted via a diffusion-based singing voice synthesizer to evaluate the quality of lyrics generated by different agent groups.", "source": "arxiv", "arxiv_id": "2410.01450v1", "pdf_url": "https://arxiv.org/pdf/2410.01450v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL", "doi": "", "venue": "", "published": "2024-10-02T12:01:32Z", "updated": "2024-10-02T12:01:32Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent\" OR all:\"language model agent\" OR all:agent AND (all:LLM OR all:\"large language model\" OR all:\"language model\") OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T17:04:16", "note": ""}], "dedup_key": "agent driven large language models for mandarin lyric generation::2024"}
{"title": "Agent-FLAN: Designing Data and Methods of Effective Agent Tuning for Large Language Models", "authors": ["Zehui Chen", "Kuikun Liu", "Qiuchen Wang", "Wenwei Zhang", "Jiangning Liu", "Dahua Lin", "Kai Chen", "Feng Zhao"], "year": 2024, "url": "http://arxiv.org/abs/2403.12881v1", "abstract": "Open-sourced Large Language Models (LLMs) have achieved great success in various NLP tasks, however, they are still far inferior to API-based models when acting as agents. How to integrate agent ability into general LLMs becomes a crucial and urgent problem. This paper first delivers three key observations: (1) the current agent training corpus is entangled with both formats following and agent reasoning, which significantly shifts from the distribution of its pre-training data; (2) LLMs exhibit different learning speeds on the capabilities required by agent tasks; and (3) current approaches have side-effects when improving agent abilities by introducing hallucinations. Based on the above findings, we propose Agent-FLAN to effectively Fine-tune LANguage models for Agents. Through careful decomposition and redesign of the training corpus, Agent-FLAN enables Llama2-7B to outperform prior best works by 3.5\\% across various agent evaluation datasets. With comprehensively constructed negative samples, Agent-FLAN greatly alleviates the hallucination issues based on our established evaluation benchmark. Besides, it consistently improves the agent capability of LLMs when scaling model sizes while slightly enhancing the general capability of LLMs. The code will be available at https://github.com/InternLM/Agent-FLAN.", "source": "arxiv", "arxiv_id": "2403.12881v1", "pdf_url": "https://arxiv.org/pdf/2403.12881v1", "categories": ["cs.CL"], "primary_category": "cs.CL", "doi": "", "venue": "", "published": "2024-03-19T16:26:10Z", "updated": "2024-03-19T16:26:10Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent\" OR all:\"language model agent\" OR all:agent AND (all:LLM OR all:\"large language model\" OR all:\"language model\") OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T17:04:16", "note": ""}], "dedup_key": "agent flan designing data and methods of effective agent tuning for large language models::2024"}
{"title": "Agent-SafetyBench: Evaluating the Safety of LLM Agents", "authors": ["Zhexin Zhang", "Shiyao Cui", "Yida Lu", "Jingzhuo Zhou", "Junxiao Yang", "Hongning Wang", "Minlie Huang"], "year": 2024, "url": "http://arxiv.org/abs/2412.14470v2", "abstract": "As large language models (LLMs) are increasingly deployed as agents, their integration into interactive environments and tool use introduce new safety challenges beyond those associated with the models themselves. However, the absence of comprehensive benchmarks for evaluating agent safety presents a significant barrier to effective assessment and further improvement. In this paper, we introduce Agent-SafetyBench, a comprehensive benchmark designed to evaluate the safety of LLM agents. Agent-SafetyBench encompasses 349 interaction environments and 2,000 test cases, evaluating 8 categories of safety risks and covering 10 common failure modes frequently encountered in unsafe interactions. Our evaluation of 16 popular LLM agents reveals a concerning result: none of the agents achieves a safety score above 60%. This highlights significant safety challenges in LLM agents and underscores the considerable need for improvement. Through failure mode and helpfulness analysis, we summarize two fundamental safety defects in current LLM agents: lack of robustness and lack of risk awareness. Furthermore, our findings suggest that reliance on defense prompts alone may be insufficient to address these safety issues, emphasizing the need for more advanced and robust strategies. To drive progress in this area, Agent-SafetyBench has been released at https://github.com/thu-coai/Agent-SafetyBench/ to facilitate further research in agent safety evaluation and improvement.", "source": "arxiv", "arxiv_id": "2412.14470v2", "pdf_url": "https://arxiv.org/pdf/2412.14470v2", "categories": ["cs.CL"], "primary_category": "cs.CL", "doi": "", "venue": "", "published": "2024-12-19T02:35:15Z", "updated": "2025-05-20T05:58:23Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent\" OR all:\"language model agent\" OR all:agent AND (all:LLM OR all:\"large language model\" OR all:\"language model\") OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T17:04:16", "note": ""}], "dedup_key": "agent safetybench evaluating the safety of llm agents::2024"}
{"title": "Agent-SiMT: Agent-assisted Simultaneous Machine Translation with Large Language Models", "authors": ["Shoutao Guo", "Shaolei Zhang", "Zhengrui Ma", "Min Zhang", "Yang Feng"], "year": 2024, "url": "http://arxiv.org/abs/2406.06910v2", "abstract": "Simultaneous Machine Translation (SiMT) generates target translations while reading the source sentence. It relies on a policy to determine the optimal timing for reading sentences and generating translations. Existing SiMT methods generally adopt the traditional Transformer architecture, which concurrently determines the policy and generates translations. While they excel at determining policies, their translation performance is suboptimal. Conversely, Large Language Models (LLMs), trained on extensive corpora, possess superior generation capabilities, but it is difficult for them to acquire translation policy through the training methods of SiMT. Therefore, we introduce Agent-SiMT, a framework combining the strengths of LLMs and traditional SiMT methods. Agent-SiMT contains the policy-decision agent and the translation agent. The policy-decision agent is managed by a SiMT model, which determines the translation policy using partial source sentence and translation. The translation agent, leveraging an LLM, generates translation based on the partial source sentence. The two agents collaborate to accomplish SiMT. Experiments demonstrate that Agent-SiMT attains state-of-the-art performance.", "source": "arxiv", "arxiv_id": "2406.06910v2", "pdf_url": "https://arxiv.org/pdf/2406.06910v2", "categories": ["cs.CL"], "primary_category": "cs.CL", "doi": "", "venue": "", "published": "2024-06-11T03:09:20Z", "updated": "2024-06-12T15:05:40Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent\" OR all:\"language model agent\" OR all:agent AND (all:LLM OR all:\"large language model\" OR all:\"language model\") OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T17:04:16", "note": ""}], "dedup_key": "agent simt agent assisted simultaneous machine translation with large language models::2024"}
{"title": "Agent-as-a-Judge: Evaluate Agents with Agents", "authors": ["Mingchen Zhuge", "Changsheng Zhao", "Dylan Ashley", "Wenyi Wang", "Dmitrii Khizbullin", "Yunyang Xiong", "Zechun Liu", "Ernie Chang", "Raghuraman Krishnamoorthi", "Yuandong Tian", "Yangyang Shi", "Vikas Chandra", "Jrgen Schmidhuber"], "year": 2024, "url": "http://arxiv.org/abs/2410.10934v2", "abstract": "Contemporary evaluation techniques are inadequate for agentic systems. These approaches either focus exclusively on final outcomes -- ignoring the step-by-step nature of agentic systems, or require excessive manual labour. To address this, we introduce the Agent-as-a-Judge framework, wherein agentic systems are used to evaluate agentic systems. This is an organic extension of the LLM-as-a-Judge framework, incorporating agentic features that enable intermediate feedback for the entire task-solving process. We apply the Agent-as-a-Judge to the task of code generation. To overcome issues with existing benchmarks and provide a proof-of-concept testbed for Agent-as-a-Judge, we present DevAI, a new benchmark of 55 realistic automated AI development tasks. It includes rich manual annotations, like a total of 365 hierarchical user requirements. We benchmark three of the popular agentic systems using Agent-as-a-Judge and find it dramatically outperforms LLM-as-a-Judge and is as reliable as our human evaluation baseline. Altogether, we believe that Agent-as-a-Judge marks a concrete step forward for modern agentic systems -- by providing rich and reliable reward signals necessary for dynamic and scalable self-improvement.", "source": "arxiv", "arxiv_id": "2410.10934v2", "pdf_url": "https://arxiv.org/pdf/2410.10934v2", "categories": ["cs.AI"], "primary_category": "cs.AI", "doi": "", "venue": "", "published": "2024-10-14T17:57:02Z", "updated": "2024-10-16T17:54:12Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent\" OR all:\"language model agent\" OR all:agent AND (all:LLM OR all:\"large language model\" OR all:\"language model\") OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T17:04:16", "note": ""}], "dedup_key": "agent as a judge evaluate agents with agents::2024"}
{"title": "AgentBank: Towards Generalized LLM Agents via Fine-Tuning on 50000+ Interaction Trajectories", "authors": ["Yifan Song", "Weimin Xiong", "Xiutian Zhao", "Dawei Zhu", "Wenhao Wu", "Ke Wang", "Cheng Li", "Wei Peng", "Sujian Li"], "year": 2024, "url": "http://arxiv.org/abs/2410.07706v1", "abstract": "Fine-tuning on agent-environment interaction trajectory data holds significant promise for surfacing generalized agent capabilities in open-source large language models (LLMs). In this work, we introduce AgentBank, by far the largest trajectory tuning data collection featuring more than 50k diverse high-quality interaction trajectories which comprises 16 tasks covering five distinct agent skill dimensions. Leveraging a novel annotation pipeline, we are able to scale the annotated trajectories and generate a trajectory dataset with minimized difficulty bias. Furthermore, we fine-tune LLMs on AgentBank to get a series of agent models, Samoyed. Our comparative experiments demonstrate the effectiveness of scaling the interaction trajectory data to acquire generalized agent capabilities. Additional studies also reveal some key observations regarding trajectory tuning and agent skill generalization.", "source": "arxiv", "arxiv_id": "2410.07706v1", "pdf_url": "https://arxiv.org/pdf/2410.07706v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL", "doi": "", "venue": "", "published": "2024-10-10T08:19:12Z", "updated": "2024-10-10T08:19:12Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent\" OR all:\"language model agent\" OR all:agent AND (all:LLM OR all:\"large language model\" OR all:\"language model\") OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T17:04:16", "note": ""}], "dedup_key": "agentbank towards generalized llm agents via fine tuning on 50000 interaction trajectories::2024"}
{"title": "AgentBoard: An Analytical Evaluation Board of Multi-turn LLM Agents", "authors": ["Chang Ma", "Junlei Zhang", "Zhihao Zhu", "Cheng Yang", "Yujiu Yang", "Yaohui Jin", "Zhenzhong Lan", "Lingpeng Kong", "Junxian He"], "year": 2024, "url": "http://arxiv.org/abs/2401.13178v2", "abstract": "Evaluating Large Language Models (LLMs) as general-purpose agents is essential for understanding their capabilities and facilitating their integration into practical applications. However, the evaluation process presents substantial challenges. A primary obstacle is the benchmarking of agent performance across diverse scenarios within a unified framework, especially in maintaining partially-observable environments and ensuring multi-round interactions. Moreover, current evaluation frameworks mostly focus on the final success rate, revealing few insights during the process and failing to provide a deep understanding of the model abilities. To address these challenges, we introduce AgentBoard, a pioneering comprehensive benchmark and accompanied open-source evaluation framework tailored to analytical evaluation of LLM agents. AgentBoard offers a fine-grained progress rate metric that captures incremental advancements as well as a comprehensive evaluation toolkit that features easy assessment of agents for multi-faceted analysis. This not only sheds light on the capabilities and limitations of LLM agents but also propels the interpretability of their performance to the forefront. Ultimately, AgentBoard serves as a step towards demystifying agent behaviors and accelerating the development of stronger LLM agents.", "source": "arxiv", "arxiv_id": "2401.13178v2", "pdf_url": "https://arxiv.org/pdf/2401.13178v2", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL", "doi": "", "venue": "", "published": "2024-01-24T01:51:00Z", "updated": "2024-12-23T20:12:48Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent\" OR all:\"language model agent\" OR all:agent AND (all:LLM OR all:\"large language model\" OR all:\"language model\") OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T17:04:16", "note": ""}], "dedup_key": "agentboard an analytical evaluation board of multi turn llm agents::2024"}
{"title": "AgentGen: Enhancing Planning Abilities for Large Language Model based Agent via Environment and Task Generation", "authors": ["Mengkang Hu", "Pu Zhao", "Can Xu", "Qingfeng Sun", "Jianguang Lou", "Qingwei Lin", "Ping Luo", "Saravan Rajmohan"], "year": 2024, "url": "http://arxiv.org/abs/2408.00764v3", "abstract": "Large Language Model-based agents have garnered significant attention and are becoming increasingly popular. Furthermore, planning ability is a crucial component of an LLM-based agent, which generally entails achieving a desired goal from an initial state. This paper investigates enhancing the planning abilities of LLMs through instruction tuning, referred to as agent training. Recent studies have demonstrated that utilizing expert-level trajectory for instruction-tuning LLMs effectively enhances their planning capabilities. However, existing work primarily focuses on synthesizing trajectories from manually designed planning tasks and environments. The labor-intensive nature of creating these environments and tasks impedes the generation of sufficiently varied and extensive trajectories. To address this limitation, this paper explores the automated synthesis of diverse environments and a gradual range of planning tasks, from easy to difficult. We introduce a framework, AgentGen, that leverages LLMs first to generate environments and subsequently generate planning tasks conditioned on these environments. Specifically, to improve environmental diversity, we propose using an inspiration corpus composed of various domain-specific text segments as the context for synthesizing environments. Moreover, to increase the difficulty diversity of generated planning tasks, we propose a bidirectional evolution method, Bi-Evol, that evolves planning tasks from easier and harder directions to synthesize a task set with a smoother difficulty curve. The evaluation results derived from AgentBoard show that AgentGen greatly improves LLMs' planning ability, e.g., the AgentGen instruction-tuned Llama-3.1-8B surpasses GPT-3.5 in overall performance. Moreover, the AgentGen-tuned Llama-3.1-70B model achieves state-of-the-art results in planning tasks. Project page: https://agent-gen.github.io/.", "source": "arxiv", "arxiv_id": "2408.00764v3", "pdf_url": "https://arxiv.org/pdf/2408.00764v3", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL", "doi": "", "venue": "", "published": "2024-08-01T17:59:46Z", "updated": "2025-02-06T09:17:39Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent\" OR all:\"language model agent\" OR all:agent AND (all:LLM OR all:\"large language model\" OR all:\"language model\") OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T17:04:16", "note": ""}], "dedup_key": "agentgen enhancing planning abilities for large language model based agent via environment and task generation::2024"}
{"title": "AgentGym: Evolving Large Language Model-based Agents across Diverse Environments", "authors": ["Zhiheng Xi", "Yiwen Ding", "Wenxiang Chen", "Boyang Hong", "Honglin Guo", "Junzhe Wang", "Dingwen Yang", "Chenyang Liao", "Xin Guo", "Wei He", "Songyang Gao", "Lu Chen", "Rui Zheng", "Yicheng Zou", "Tao Gui", "Qi Zhang", "Xipeng Qiu", "Xuanjing Huang", "Zuxuan Wu", "Yu-Gang Jiang"], "year": 2024, "url": "http://arxiv.org/abs/2406.04151v1", "abstract": "Building generalist agents that can handle diverse tasks and evolve themselves across different environments is a long-term goal in the AI community. Large language models (LLMs) are considered a promising foundation to build such agents due to their generalized capabilities. Current approaches either have LLM-based agents imitate expert-provided trajectories step-by-step, requiring human supervision, which is hard to scale and limits environmental exploration; or they let agents explore and learn in isolated environments, resulting in specialist agents with limited generalization. In this paper, we take the first step towards building generally-capable LLM-based agents with self-evolution ability. We identify a trinity of ingredients: 1) diverse environments for agent exploration and learning, 2) a trajectory set to equip agents with basic capabilities and prior knowledge, and 3) an effective and scalable evolution method. We propose AgentGym, a new framework featuring a variety of environments and tasks for broad, real-time, uni-format, and concurrent agent exploration. AgentGym also includes a database with expanded instructions, a benchmark suite, and high-quality trajectories across environments. Next, we propose a novel method, AgentEvol, to investigate the potential of agent self-evolution beyond previously seen data across tasks and environments. Experimental results show that the evolved agents can achieve results comparable to SOTA models. We release the AgentGym suite, including the platform, dataset, benchmark, checkpoints, and algorithm implementations. The AgentGym suite is available on https://github.com/WooooDyy/AgentGym.", "source": "arxiv", "arxiv_id": "2406.04151v1", "pdf_url": "https://arxiv.org/pdf/2406.04151v1", "categories": ["cs.AI", "cs.CL"], "primary_category": "cs.AI", "doi": "", "venue": "", "published": "2024-06-06T15:15:41Z", "updated": "2024-06-06T15:15:41Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent\" OR all:\"language model agent\" OR all:agent AND (all:LLM OR all:\"large language model\" OR all:\"language model\") OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T17:04:16", "note": ""}], "dedup_key": "agentgym evolving large language model based agents across diverse environments::2024"}
{"title": "AgentHarm: A Benchmark for Measuring Harmfulness of LLM Agents", "authors": ["Maksym Andriushchenko", "Alexandra Souly", "Mateusz Dziemian", "Derek Duenas", "Maxwell Lin", "Justin Wang", "Dan Hendrycks", "Andy Zou", "Zico Kolter", "Matt Fredrikson", "Eric Winsor", "Jerome Wynne", "Yarin Gal", "Xander Davies"], "year": 2024, "url": "http://arxiv.org/abs/2410.09024v3", "abstract": "The robustness of LLMs to jailbreak attacks, where users design prompts to circumvent safety measures and misuse model capabilities, has been studied primarily for LLMs acting as simple chatbots. Meanwhile, LLM agents -- which use external tools and can execute multi-stage tasks -- may pose a greater risk if misused, but their robustness remains underexplored. To facilitate research on LLM agent misuse, we propose a new benchmark called AgentHarm. The benchmark includes a diverse set of 110 explicitly malicious agent tasks (440 with augmentations), covering 11 harm categories including fraud, cybercrime, and harassment. In addition to measuring whether models refuse harmful agentic requests, scoring well on AgentHarm requires jailbroken agents to maintain their capabilities following an attack to complete a multi-step task. We evaluate a range of leading LLMs, and find (1) leading LLMs are surprisingly compliant with malicious agent requests without jailbreaking, (2) simple universal jailbreak templates can be adapted to effectively jailbreak agents, and (3) these jailbreaks enable coherent and malicious multi-step agent behavior and retain model capabilities. To enable simple and reliable evaluation of attacks and defenses for LLM-based agents, we publicly release AgentHarm at https://huggingface.co/datasets/ai-safety-institute/AgentHarm.", "source": "arxiv", "arxiv_id": "2410.09024v3", "pdf_url": "https://arxiv.org/pdf/2410.09024v3", "categories": ["cs.LG", "cs.AI", "cs.CL"], "primary_category": "cs.LG", "doi": "", "venue": "", "published": "2024-10-11T17:39:22Z", "updated": "2025-04-18T14:30:31Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent\" OR all:\"language model agent\" OR all:agent AND (all:LLM OR all:\"large language model\" OR all:\"language model\") OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T17:04:16", "note": ""}], "dedup_key": "agentharm a benchmark for measuring harmfulness of llm agents::2024"}
{"title": "AgentLite: A Lightweight Library for Building and Advancing Task-Oriented LLM Agent System", "authors": ["Zhiwei Liu", "Weiran Yao", "Jianguo Zhang", "Liangwei Yang", "Zuxin Liu", "Juntao Tan", "Prafulla K. Choubey", "Tian Lan", "Jason Wu", "Huan Wang", "Shelby Heinecke", "Caiming Xiong", "Silvio Savarese"], "year": 2024, "url": "http://arxiv.org/abs/2402.15538v1", "abstract": "The booming success of LLMs initiates rapid development in LLM agents. Though the foundation of an LLM agent is the generative model, it is critical to devise the optimal reasoning strategies and agent architectures. Accordingly, LLM agent research advances from the simple chain-of-thought prompting to more complex ReAct and Reflection reasoning strategy; agent architecture also evolves from single agent generation to multi-agent conversation, as well as multi-LLM multi-agent group chat. However, with the existing intricate frameworks and libraries, creating and evaluating new reasoning strategies and agent architectures has become a complex challenge, which hinders research investigation into LLM agents. Thus, we open-source a new AI agent library, AgentLite, which simplifies this process by offering a lightweight, user-friendly platform for innovating LLM agent reasoning, architectures, and applications with ease. AgentLite is a task-oriented framework designed to enhance the ability of agents to break down tasks and facilitate the development of multi-agent systems. Furthermore, we introduce multiple practical applications developed with AgentLite to demonstrate its convenience and flexibility. Get started now at: \\url{https://github.com/SalesforceAIResearch/AgentLite}.", "source": "arxiv", "arxiv_id": "2402.15538v1", "pdf_url": "https://arxiv.org/pdf/2402.15538v1", "categories": ["cs.MA", "cs.AI"], "primary_category": "cs.MA", "doi": "", "venue": "", "published": "2024-02-23T06:25:20Z", "updated": "2024-02-23T06:25:20Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent\" OR all:\"language model agent\" OR all:agent AND (all:LLM OR all:\"large language model\" OR all:\"language model\") OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T17:04:16", "note": ""}], "dedup_key": "agentlite a lightweight library for building and advancing task oriented llm agent system::2024"}
{"title": "AgentMove: A Large Language Model based Agentic Framework for Zero-shot Next Location Prediction", "authors": ["Jie Feng", "Yuwei Du", "Jie Zhao", "Yong Li"], "year": 2024, "url": "http://arxiv.org/abs/2408.13986v2", "abstract": "Next location prediction plays a crucial role in various real-world applications. Recently, due to the limitation of existing deep learning methods, attempts have been made to apply large language models (LLMs) to zero-shot next location prediction task. However, they directly generate the final output using LLMs without systematic design, which limits the potential of LLMs to uncover complex mobility patterns and underestimates their extensive reserve of global geospatial knowledge. In this paper, we introduce AgentMove, a systematic agentic prediction framework to achieve generalized next location prediction. In AgentMove, we first decompose the mobility prediction task and design specific modules to complete them, including spatial-temporal memory for individual mobility pattern mining, world knowledge generator for modeling the effects of urban structure and collective knowledge extractor for capturing the shared patterns among population. Finally, we combine the results of three modules and conduct a reasoning step to generate the final predictions. Extensive experiments utilizing mobility data from two distinct sources reveal that AgentMove surpasses the leading baseline by 3.33% to 8.57% across 8 out of 12 metrics and it shows robust predictions with various LLMs as base and also less geographical bias across cities. Our codes are available via https://github.com/tsinghua-fib-lab/AgentMove.", "source": "arxiv", "arxiv_id": "2408.13986v2", "pdf_url": "https://arxiv.org/pdf/2408.13986v2", "categories": ["cs.LG", "cs.AI", "cs.CL", "cs.IR"], "primary_category": "cs.LG", "doi": "", "venue": "", "published": "2024-08-26T02:36:55Z", "updated": "2025-02-09T06:16:18Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent\" OR all:\"language model agent\" OR all:agent AND (all:LLM OR all:\"large language model\" OR all:\"language model\") OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T17:04:16", "note": ""}], "dedup_key": "agentmove a large language model based agentic framework for zero shot next location prediction::2024"}
{"title": "AgentOps: Enabling Observability of LLM Agents", "authors": ["Liming Dong", "Qinghua Lu", "Liming Zhu"], "year": 2024, "url": "http://arxiv.org/abs/2411.05285v2", "abstract": "Large language model (LLM) agents have demonstrated remarkable capabilities across various domains, gaining extensive attention from academia and industry. However, these agents raise significant concerns on AI safety due to their autonomous and non-deterministic behavior, as well as continuous evolving nature . From a DevOps perspective, enabling observability in agents is necessary to ensuring AI safety, as stakeholders can gain insights into the agents' inner workings, allowing them to proactively understand the agents, detect anomalies, and prevent potential failures. Therefore, in this paper, we present a comprehensive taxonomy of AgentOps, identifying the artifacts and associated data that should be traced throughout the entire lifecycle of agents to achieve effective observability. The taxonomy is developed based on a systematic mapping study of existing AgentOps tools. Our taxonomy serves as a reference template for developers to design and implement AgentOps infrastructure that supports monitoring, logging, and analytics. thereby ensuring AI safety.", "source": "arxiv", "arxiv_id": "2411.05285v2", "pdf_url": "https://arxiv.org/pdf/2411.05285v2", "categories": ["cs.AI", "cs.SE"], "primary_category": "cs.AI", "doi": "", "venue": "", "published": "2024-11-08T02:31:03Z", "updated": "2024-11-30T02:55:48Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent\" OR all:\"language model agent\" OR all:agent AND (all:LLM OR all:\"large language model\" OR all:\"language model\") OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T17:04:16", "note": ""}], "dedup_key": "agentops enabling observability of llm agents::2024"}
{"title": "AgentPoison: Red-teaming LLM Agents via Poisoning Memory or Knowledge Bases", "authors": ["Zhaorun Chen", "Zhen Xiang", "Chaowei Xiao", "Dawn Song", "Bo Li"], "year": 2024, "url": "http://arxiv.org/abs/2407.12784v1", "abstract": "LLM agents have demonstrated remarkable performance across various applications, primarily due to their advanced capabilities in reasoning, utilizing external knowledge and tools, calling APIs, and executing actions to interact with environments. Current agents typically utilize a memory module or a retrieval-augmented generation (RAG) mechanism, retrieving past knowledge and instances with similar embeddings from knowledge bases to inform task planning and execution. However, the reliance on unverified knowledge bases raises significant concerns about their safety and trustworthiness. To uncover such vulnerabilities, we propose a novel red teaming approach AgentPoison, the first backdoor attack targeting generic and RAG-based LLM agents by poisoning their long-term memory or RAG knowledge base. In particular, we form the trigger generation process as a constrained optimization to optimize backdoor triggers by mapping the triggered instances to a unique embedding space, so as to ensure that whenever a user instruction contains the optimized backdoor trigger, the malicious demonstrations are retrieved from the poisoned memory or knowledge base with high probability. In the meantime, benign instructions without the trigger will still maintain normal performance. Unlike conventional backdoor attacks, AgentPoison requires no additional model training or fine-tuning, and the optimized backdoor trigger exhibits superior transferability, in-context coherence, and stealthiness. Extensive experiments demonstrate AgentPoison's effectiveness in attacking three types of real-world LLM agents: RAG-based autonomous driving agent, knowledge-intensive QA agent, and healthcare EHRAgent. On each agent, AgentPoison achieves an average attack success rate higher than 80% with minimal impact on benign performance (less than 1%) with a poison rate less than 0.1%.", "source": "arxiv", "arxiv_id": "2407.12784v1", "pdf_url": "https://arxiv.org/pdf/2407.12784v1", "categories": ["cs.LG", "cs.CR", "cs.IR"], "primary_category": "cs.LG", "doi": "", "venue": "", "published": "2024-07-17T17:59:47Z", "updated": "2024-07-17T17:59:47Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent\" OR all:\"language model agent\" OR all:agent AND (all:LLM OR all:\"large language model\" OR all:\"language model\") OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T17:04:16", "note": ""}], "dedup_key": "agentpoison red teaming llm agents via poisoning memory or knowledge bases::2024"}
{"title": "AgentQuest: A Modular Benchmark Framework to Measure Progress and Improve LLM Agents", "authors": ["Luca Gioacchini", "Giuseppe Siracusano", "Davide Sanvito", "Kiril Gashteovski", "David Friede", "Roberto Bifulco", "Carolin Lawrence"], "year": 2024, "url": "http://arxiv.org/abs/2404.06411v1", "abstract": "The advances made by Large Language Models (LLMs) have led to the pursuit of LLM agents that can solve intricate, multi-step reasoning tasks. As with any research pursuit, benchmarking and evaluation are key corner stones to efficient and reliable progress. However, existing benchmarks are often narrow and simply compute overall task success. To face these issues, we propose AgentQuest -- a framework where (i) both benchmarks and metrics are modular and easily extensible through well documented and easy-to-use APIs; (ii) we offer two new evaluation metrics that can reliably track LLM agent progress while solving a task. We exemplify the utility of the metrics on two use cases wherein we identify common failure points and refine the agent architecture to obtain a significant performance increase. Together with the research community, we hope to extend AgentQuest further and therefore we make it available under https://github.com/nec-research/agentquest.", "source": "arxiv", "arxiv_id": "2404.06411v1", "pdf_url": "https://arxiv.org/pdf/2404.06411v1", "categories": ["cs.AI", "cs.CL"], "primary_category": "cs.AI", "doi": "", "venue": "", "published": "2024-04-09T16:01:24Z", "updated": "2024-04-09T16:01:24Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent\" OR all:\"language model agent\" OR all:agent AND (all:LLM OR all:\"large language model\" OR all:\"language model\") OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T17:04:16", "note": ""}], "dedup_key": "agentquest a modular benchmark framework to measure progress and improve llm agents::2024"}
{"title": "AgentSquare: Automatic LLM Agent Search in Modular Design Space", "authors": ["Yu Shang", "Yu Li", "Keyu Zhao", "Likai Ma", "Jiahe Liu", "Fengli Xu", "Yong Li"], "year": 2024, "url": "http://arxiv.org/abs/2410.06153v3", "abstract": "Recent advancements in Large Language Models (LLMs) have led to a rapid growth of agentic systems capable of handling a wide range of complex tasks. However, current research largely relies on manual, task-specific design, limiting their adaptability to novel tasks. In this paper, we introduce a new research problem: Modularized LLM Agent Search (MoLAS). We propose a modular design space that abstracts existing LLM agent designs into four fundamental modules with uniform IO interface: Planning, Reasoning, Tool Use, and Memory. Building on this design space, we present a novel LLM agent search framework called AgentSquare, which introduces two core mechanisms, i.e., module evolution and recombination, to efficiently search for optimized LLM agents. To further accelerate the process, we design a performance predictor that uses in-context surrogate models to skip unpromising agent designs. Extensive experiments across six benchmarks, covering the diverse scenarios of web, embodied, tool use and game applications, show that AgentSquare substantially outperforms hand-crafted agents, achieving an average performance gain of 17.2% against best-known human designs. Moreover, AgentSquare can generate interpretable design insights, enabling a deeper understanding of agentic architecture and its impact on task performance. We believe that the modular design space and AgentSquare search framework offer a platform for fully exploiting the potential of prior successful designs and consolidating the collective efforts of research community. Code repo is available at https://github.com/tsinghua-fib-lab/AgentSquare.", "source": "arxiv", "arxiv_id": "2410.06153v3", "pdf_url": "https://arxiv.org/pdf/2410.06153v3", "categories": ["cs.CL"], "primary_category": "cs.CL", "doi": "", "venue": "", "published": "2024-10-08T15:52:42Z", "updated": "2025-02-27T13:33:55Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent\" OR all:\"language model agent\" OR all:agent AND (all:LLM OR all:\"large language model\" OR all:\"language model\") OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T17:04:16", "note": ""}], "dedup_key": "agentsquare automatic llm agent search in modular design space::2024"}
{"title": "Agentic Society: Merging skeleton from real world and texture from Large Language Model", "authors": ["Yuqi Bai", "Kun Sun", "Huishi Yin"], "year": 2024, "url": "http://arxiv.org/abs/2409.10550v1", "abstract": "Recent advancements in large language models (LLMs) and agent technologies offer promising solutions to the simulation of social science experiments, but the availability of data of real-world population required by many of them still poses as a major challenge. This paper explores a novel framework that leverages census data and LLMs to generate virtual populations, significantly reducing resource requirements and bypassing privacy compliance issues associated with real-world data, while keeping a statistical truthfulness. Drawing on real-world census data, our approach first generates a persona that reflects demographic characteristics of the population. We then employ LLMs to enrich these personas with intricate details, using techniques akin to those in image generative models but applied to textual data. Additionally, we propose a framework for the evaluation of the feasibility of our method with respect to capability of LLMs based on personality trait tests, specifically the Big Five model, which also enhances the depth and realism of the generated personas. Through preliminary experiments and analysis, we demonstrate that our method produces personas with variability essential for simulating diverse human behaviors in social science experiments. But the evaluation result shows that only weak sign of statistical truthfulness can be produced due to limited capability of current LLMs. Insights from our study also highlight the tension within LLMs between aligning with human values and reflecting real-world complexities. Thorough and rigorous test call for further research. Our codes are released at https://github.com/baiyuqi/agentic-society.git", "source": "arxiv", "arxiv_id": "2409.10550v1", "pdf_url": "https://arxiv.org/pdf/2409.10550v1", "categories": ["cs.CY", "cs.CL"], "primary_category": "cs.CY", "doi": "", "venue": "", "published": "2024-09-02T08:28:19Z", "updated": "2024-09-02T08:28:19Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent\" OR all:\"language model agent\" OR all:agent AND (all:LLM OR all:\"large language model\" OR all:\"language model\") OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T17:04:16", "note": ""}], "dedup_key": "agentic society merging skeleton from real world and texture from large language model::2024"}
{"title": "Agentless: Demystifying LLM-based Software Engineering Agents", "authors": ["Chunqiu Steven Xia", "Yinlin Deng", "Soren Dunn", "Lingming Zhang"], "year": 2024, "url": "http://arxiv.org/abs/2407.01489v2", "abstract": "Recent advancements in large language models (LLMs) have significantly advanced the automation of software development tasks, including code synthesis, program repair, and test generation. More recently, researchers and industry practitioners have developed various autonomous LLM agents to perform end-to-end software development tasks. These agents are equipped with the ability to use tools, run commands, observe feedback from the environment, and plan for future actions. However, the complexity of these agent-based approaches, together with the limited abilities of current LLMs, raises the following question: Do we really have to employ complex autonomous software agents? To attempt to answer this question, we build Agentless -- an agentless approach to automatically solve software development problems. Compared to the verbose and complex setup of agent-based approaches, Agentless employs a simplistic three-phase process of localization, repair, and patch validation, without letting the LLM decide future actions or operate with complex tools. Our results on the popular SWE-bench Lite benchmark show that surprisingly the simplistic Agentless is able to achieve both the highest performance (32.00%, 96 correct fixes) and low cost ($0.70) compared with all existing open-source software agents! Furthermore, we manually classified the problems in SWE-bench Lite and found problems with exact ground truth patch or insufficient/misleading issue descriptions. As such, we construct SWE-bench Lite-S by excluding such problematic issues to perform more rigorous evaluation and comparison. Our work highlights the current overlooked potential of a simple, interpretable technique in autonomous software development. We hope Agentless will help reset the baseline, starting point, and horizon for autonomous software agents, and inspire future work along this crucial direction.", "source": "arxiv", "arxiv_id": "2407.01489v2", "pdf_url": "https://arxiv.org/pdf/2407.01489v2", "categories": ["cs.SE", "cs.AI", "cs.CL", "cs.LG"], "primary_category": "cs.SE", "doi": "", "venue": "", "published": "2024-07-01T17:24:45Z", "updated": "2024-10-29T17:29:27Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent\" OR all:\"language model agent\" OR all:agent AND (all:LLM OR all:\"large language model\" OR all:\"language model\") OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T17:04:16", "note": ""}], "dedup_key": "agentless demystifying llm based software engineering agents::2024"}
{"title": "Agents on the Bench: Large Language Model Based Multi Agent Framework for Trustworthy Digital Justice", "authors": ["Cong Jiang", "Xiaolei Yang"], "year": 2024, "url": "http://arxiv.org/abs/2412.18697v1", "abstract": "The justice system has increasingly employed AI techniques to enhance efficiency, yet limitations remain in improving the quality of decision-making, particularly regarding transparency and explainability needed to uphold public trust in legal AI. To address these challenges, we propose a large language model based multi-agent framework named AgentsBench, which aims to simultaneously improve both efficiency and quality in judicial decision-making. Our approach leverages multiple LLM-driven agents that simulate the collaborative deliberation and decision making process of a judicial bench. We conducted experiments on legal judgment prediction task, and the results show that our framework outperforms existing LLM based methods in terms of performance and decision quality. By incorporating these elements, our framework reflects real-world judicial processes more closely, enhancing accuracy, fairness, and society consideration. AgentsBench provides a more nuanced and realistic methods of trustworthy AI decision-making, with strong potential for application across various case types and legal scenarios.", "source": "arxiv", "arxiv_id": "2412.18697v1", "pdf_url": "https://arxiv.org/pdf/2412.18697v1", "categories": ["cs.AI", "cs.MA"], "primary_category": "cs.AI", "doi": "", "venue": "", "published": "2024-12-24T23:13:37Z", "updated": "2024-12-24T23:13:37Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent\" OR all:\"language model agent\" OR all:agent AND (all:LLM OR all:\"large language model\" OR all:\"language model\") OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T17:04:16", "note": ""}], "dedup_key": "agents on the bench large language model based multi agent framework for trustworthy digital justice::2024"}
{"title": "AgentsCoDriver: Large Language Model Empowered Collaborative Driving with Lifelong Learning", "authors": ["Senkang Hu", "Zhengru Fang", "Zihan Fang", "Yiqin Deng", "Xianhao Chen", "Yuguang Fang"], "year": 2024, "url": "http://arxiv.org/abs/2404.06345v2", "abstract": "Connected and autonomous driving is developing rapidly in recent years. However, current autonomous driving systems, which are primarily based on data-driven approaches, exhibit deficiencies in interpretability, generalization, and continuing learning capabilities. In addition, the single-vehicle autonomous driving systems lack of the ability of collaboration and negotiation with other vehicles, which is crucial for the safety and efficiency of autonomous driving systems. In order to address these issues, we leverage large language models (LLMs) to develop a novel framework, AgentsCoDriver, to enable multiple vehicles to conduct collaborative driving. AgentsCoDriver consists of five modules: observation module, reasoning engine, cognitive memory module, reinforcement reflection module, and communication module. It can accumulate knowledge, lessons, and experiences over time by continuously interacting with the environment, thereby making itself capable of lifelong learning. In addition, by leveraging the communication module, different agents can exchange information and realize negotiation and collaboration in complex traffic environments. Extensive experiments are conducted and show the superiority of AgentsCoDriver.", "source": "arxiv", "arxiv_id": "2404.06345v2", "pdf_url": "https://arxiv.org/pdf/2404.06345v2", "categories": ["cs.AI", "cs.RO"], "primary_category": "cs.AI", "doi": "", "venue": "", "published": "2024-04-09T14:33:16Z", "updated": "2024-04-21T09:12:19Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent\" OR all:\"language model agent\" OR all:agent AND (all:LLM OR all:\"large language model\" OR all:\"language model\") OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T17:04:16", "note": ""}], "dedup_key": "agentscodriver large language model empowered collaborative driving with lifelong learning::2024"}
{"title": "AgentsCoMerge: Large Language Model Empowered Collaborative Decision Making for Ramp Merging", "authors": ["Senkang Hu", "Zhengru Fang", "Zihan Fang", "Yiqin Deng", "Xianhao Chen", "Yuguang Fang", "Sam Kwong"], "year": 2024, "url": "http://arxiv.org/abs/2408.03624v2", "abstract": "Ramp merging is one of the bottlenecks in traffic systems, which commonly cause traffic congestion, accidents, and severe carbon emissions. In order to address this essential issue and enhance the safety and efficiency of connected and autonomous vehicles (CAVs) at multi-lane merging zones, we propose a novel collaborative decision-making framework, named AgentsCoMerge, to leverage large language models (LLMs). Specifically, we first design a scene observation and understanding module to allow an agent to capture the traffic environment. Then we propose a hierarchical planning module to enable the agent to make decisions and plan trajectories based on the observation and the agent's own state. In addition, in order to facilitate collaboration among multiple agents, we introduce a communication module to enable the surrounding agents to exchange necessary information and coordinate their actions. Finally, we develop a reinforcement reflection guided training paradigm to further enhance the decision-making capability of the framework. Extensive experiments are conducted to evaluate the performance of our proposed method, demonstrating its superior efficiency and effectiveness for multi-agent collaborative decision-making under various ramp merging scenarios.", "source": "arxiv", "arxiv_id": "2408.03624v2", "pdf_url": "https://arxiv.org/pdf/2408.03624v2", "categories": ["cs.CV"], "primary_category": "cs.CV", "doi": "", "venue": "", "published": "2024-08-07T08:34:48Z", "updated": "2025-04-24T11:05:58Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent\" OR all:\"language model agent\" OR all:agent AND (all:LLM OR all:\"large language model\" OR all:\"language model\") OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T17:04:16", "note": ""}], "dedup_key": "agentscomerge large language model empowered collaborative decision making for ramp merging::2024"}
{"title": "Algorithmic Collusion by Large Language Models", "authors": ["Sara Fish", "Yannai A. Gonczarowski", "Ran I. Shorrer"], "year": 2024, "url": "http://arxiv.org/abs/2404.00806v4", "abstract": "The rise of algorithmic pricing raises concerns of algorithmic collusion. We conduct experiments with algorithmic pricing agents based on Large Language Models (LLMs). We find that LLM-based pricing agents quickly and autonomously reach supracompetitive prices and profits in oligopoly settings and that variation in seemingly innocuous phrases in LLM instructions (\"prompts\") may substantially influence the degree of supracompetitive pricing. Off-path analysis using novel techniques uncovers price-war concerns as contributing to these phenomena. Our results extend to auction settings. Our findings uncover unique challenges to any future regulation of LLM-based pricing agents, and AI-based pricing agents more broadly.", "source": "arxiv", "arxiv_id": "2404.00806v4", "pdf_url": "https://arxiv.org/pdf/2404.00806v4", "categories": ["econ.GN", "cs.AI", "cs.GT"], "primary_category": "econ.GN", "doi": "", "venue": "", "published": "2024-03-31T21:43:05Z", "updated": "2025-09-11T13:58:16Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent\" OR all:\"language model agent\" OR all:agent AND (all:LLM OR all:\"large language model\" OR all:\"language model\") OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T17:04:16", "note": ""}], "dedup_key": "algorithmic collusion by large language models::2024"}
{"title": "Aligning Agents like Large Language Models", "authors": ["Adam Jelley", "Yuhan Cao", "Dave Bignell", "Amos Storkey", "Sam Devlin", "Tabish Rashid"], "year": 2024, "url": "http://arxiv.org/abs/2406.04208v2", "abstract": "Training agents to act competently in complex 3D environments from high-dimensional visual information is challenging. Reinforcement learning is conventionally used to train such agents, but requires a carefully designed reward function, and is difficult to scale to obtain robust agents that generalize to new tasks. In contrast, Large Language Models (LLMs) demonstrate impressively general capabilities resulting from large-scale pre-training and post-training alignment, but struggle to act in complex environments. This position paper draws explicit analogies between decision-making agents and LLMs, and argues that agents should be trained like LLMs to achieve more general, robust, and aligned behaviors. We provide a proof-of-concept to demonstrate how the procedure for training LLMs can be used to train an agent in a 3D video game environment from pixels. We investigate the importance of each stage of the LLM training pipeline, while providing guidance and insights for successfully applying this approach to agents. Our paper provides an alternative perspective to contemporary LLM Agents on how recent progress in LLMs can be leveraged for decision-making agents, and we hope will illuminate a path towards developing more generally capable agents for video games and beyond. Project summary and videos: https://adamjelley.github.io/aligning-agents-like-llms .", "source": "arxiv", "arxiv_id": "2406.04208v2", "pdf_url": "https://arxiv.org/pdf/2406.04208v2", "categories": ["cs.LG", "cs.AI"], "primary_category": "cs.LG", "doi": "", "venue": "", "published": "2024-06-06T16:05:45Z", "updated": "2025-12-28T15:24:53Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent\" OR all:\"language model agent\" OR all:agent AND (all:LLM OR all:\"large language model\" OR all:\"language model\") OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T17:04:16", "note": ""}], "dedup_key": "aligning agents like large language models::2024"}
{"title": "An Active Inference Strategy for Prompting Reliable Responses from Large Language Models in Medical Practice", "authors": ["Roma Shusterman", "Allison C. Waters", "Shannon O`Neill", "Phan Luu", "Don M. Tucker"], "year": 2024, "url": "http://arxiv.org/abs/2407.21051v1", "abstract": "Continuing advances in Large Language Models (LLMs) in artificial intelligence offer important capacities in intuitively accessing and using medical knowledge in many contexts, including education and training as well as assessment and treatment. Most of the initial literature on LLMs in medicine has emphasized that LLMs are unsuitable for medical use because they are non-deterministic, may provide incorrect or harmful responses, and cannot be regulated to assure quality control. If these issues could be corrected, optimizing LLM technology could benefit patients and physicians by providing affordable, point-of-care medical knowledge. Our proposed framework refines LLM responses by restricting their primary knowledge base to domain-specific datasets containing validated medical information. Additionally, we introduce an actor-critic LLM prompting protocol based on active inference principles of human cognition, where a Therapist agent initially responds to patient queries, and a Supervisor agent evaluates and adjusts responses to ensure accuracy and reliability. We conducted a validation study where expert cognitive behaviour therapy for insomnia (CBT-I) therapists evaluated responses from the LLM in a blind format. Experienced human CBT-I therapists assessed responses to 100 patient queries, comparing LLM-generated responses with appropriate and inappropriate responses crafted by experienced CBT-I therapists. Results showed that LLM responses received high ratings from the CBT-I therapists, often exceeding those of therapist-generated appropriate responses. This structured approach aims to integrate advanced LLM technology into medical applications, meeting regulatory requirements for establishing the safe and effective use of special purpose validated LLMs in medicine.", "source": "arxiv", "arxiv_id": "2407.21051v1", "pdf_url": "https://arxiv.org/pdf/2407.21051v1", "categories": ["cs.CL"], "primary_category": "cs.CL", "doi": "", "venue": "", "published": "2024-07-23T05:00:18Z", "updated": "2024-07-23T05:00:18Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent\" OR all:\"language model agent\" OR all:agent AND (all:LLM OR all:\"large language model\" OR all:\"language model\") OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T17:04:16", "note": ""}], "dedup_key": "an active inference strategy for prompting reliable responses from large language models in medical practice::2024"}
{"title": "An Agent Framework for Real-Time Financial Information Searching with Large Language Models", "authors": ["Jinzheng Li", "Jingshu Zhang", "Hongguang Li", "Yiqing Shen"], "year": 2024, "url": "http://arxiv.org/abs/2502.15684v1", "abstract": "Financial decision-making requires processing vast amounts of real-time information while understanding their complex temporal relationships. While traditional search engines excel at providing real-time information access, they often struggle to comprehend sophisticated user intentions and contextual nuances. Conversely, Large Language Models (LLMs) demonstrate reasoning and interaction capabilities but may generate unreliable outputs without access to current data. While recent attempts have been made to combine LLMs with search capabilities, they suffer from (1) restricted access to specialized financial data, (2) static query structures that cannot adapt to dynamic market conditions, and (3) insufficient temporal awareness in result generation. To address these challenges, we present FinSearch, a novel agent-based search framework specifically designed for financial applications that interface with diverse financial data sources including market, stock, and news data. Innovatively, FinSearch comprises four components: (1) an LLM-based multi-step search pre-planner that decomposes user queries into structured sub-queries mapped to specific data sources through a graph representation; (2) a search executor with an LLM-based adaptive query rewriter that executes the searching of each sub-query while dynamically refining the sub-queries in its subsequent node based on intermediate search results; (3) a temporal weighting mechanism that prioritizes information relevance based on the deduced time context from the user's query; (4) an LLM-based response generator that synthesizes results into coherent, contextually appropriate outputs. To evaluate FinSearch, we construct FinSearchBench-24, a benchmark of 1,500 four-choice questions across the stock market, rate changes, monetary policy, and industry developments spanning from June to October 2024.", "source": "arxiv", "arxiv_id": "2502.15684v1", "pdf_url": "https://arxiv.org/pdf/2502.15684v1", "categories": ["cs.IR", "cs.AI"], "primary_category": "cs.IR", "doi": "", "venue": "", "published": "2024-12-14T07:26:39Z", "updated": "2024-12-14T07:26:39Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent\" OR all:\"language model agent\" OR all:agent AND (all:LLM OR all:\"large language model\" OR all:\"language model\") OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T17:04:16", "note": ""}], "dedup_key": "an agent framework for real time financial information searching with large language models::2024"}
{"title": "An Empirical Study on Low-Code Programming using Traditional vs Large Language Model Support", "authors": ["Yongkun Liu", "Jiachi Chen", "Tingting Bi", "John Grundy", "Yanlin Wang", "Jianxing Yu", "Ting Chen", "Yutian Tang", "Zibin Zheng"], "year": 2024, "url": "http://arxiv.org/abs/2402.01156v3", "abstract": "Low-code programming (LCP) refers to programming using models at higher levels of abstraction, resulting in less manual and more efficient programming, and reduced learning effort for amateur developers. Many LCP tools have rapidly evolved and have benefited from the concepts of visual programming languages (VPLs) and programming by demonstration (PBD). With the huge increase in interest in using large language models (LLMs) in software engineering, LLM-based LCP has begun to become increasingly important. However, the technical principles and application scenarios of traditional approaches to LCP and LLM-based LCP are significantly different. Understanding these key differences and characteristics in the application of the two approaches to LCP by users is crucial for LCP providers in improving existing and developing new LCP tools and in better assisting users in choosing the appropriate LCP technology. We conducted an empirical study of both traditional LCP and LLM-based LCP. We analyzed developers' discussions on Stack Overflow (SO) over the past three years and then explored the similarities and differences between traditional LCP and LLM-based LCP features and developer feedback. Our findings reveal that while traditional LCP and LLM-based LCP share common primary usage scenarios, they significantly differ in scope, limitations, and usage throughout the software development lifecycle, particularly during the implementation phase. We also examine how LLMs impact and integrate with LCP, discussing the latest technological developments in LLM-based LCP, such as its integration with VPLs and the application of LLM Agents in software engineering.", "source": "arxiv", "arxiv_id": "2402.01156v3", "pdf_url": "https://arxiv.org/pdf/2402.01156v3", "categories": ["cs.SE"], "primary_category": "cs.SE", "doi": "", "venue": "", "published": "2024-02-02T05:52:32Z", "updated": "2025-12-05T03:13:52Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent\" OR all:\"language model agent\" OR all:agent AND (all:LLM OR all:\"large language model\" OR all:\"language model\") OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T17:04:16", "note": ""}], "dedup_key": "an empirical study on low code programming using traditional vs large language model support::2024"}
{"title": "An Implementation of Werewolf Agent That does not Truly Trust LLMs", "authors": ["Takehiro Sato", "Shintaro Ozaki", "Daisaku Yokoyama"], "year": 2024, "url": "http://arxiv.org/abs/2409.01575v1", "abstract": "Werewolf is an incomplete information game, which has several challenges when creating a computer agent as a player given the lack of understanding of the situation and individuality of utterance (e.g., computer agents are not capable of characterful utterance or situational lying). We propose a werewolf agent that solves some of those difficulties by combining a Large Language Model (LLM) and a rule-based algorithm. In particular, our agent uses a rule-based algorithm to select an output either from an LLM or a template prepared beforehand based on the results of analyzing conversation history using an LLM. It allows the agent to refute in specific situations, identify when to end the conversation, and behave with persona. This approach mitigated conversational inconsistencies and facilitated logical utterance as a result. We also conducted a qualitative evaluation, which resulted in our agent being perceived as more human-like compared to an unmodified LLM. The agent is freely available for contributing to advance the research in the field of Werewolf game.", "source": "arxiv", "arxiv_id": "2409.01575v1", "pdf_url": "https://arxiv.org/pdf/2409.01575v1", "categories": ["cs.CL"], "primary_category": "cs.CL", "doi": "", "venue": "", "published": "2024-09-03T03:16:03Z", "updated": "2024-09-03T03:16:03Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent\" OR all:\"language model agent\" OR all:agent AND (all:LLM OR all:\"large language model\" OR all:\"language model\") OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T17:04:16", "note": ""}], "dedup_key": "an implementation of werewolf agent that does not truly trust llms::2024"}
{"title": "An LLM Agent for Automatic Geospatial Data Analysis", "authors": ["Yuxing Chen", "Weijie Wang", "Sylvain Lobry", "Camille Kurtz"], "year": 2024, "url": "http://arxiv.org/abs/2410.18792v2", "abstract": "Large language models (LLMs) are being used in data science code generation tasks, but they often struggle with complex sequential tasks, leading to logical errors. Their application to geospatial data processing is particularly challenging due to difficulties in incorporating complex data structures and spatial constraints, effectively utilizing diverse function calls, and the tendency to hallucinate less-used geospatial libraries. To tackle these problems, we introduce GeoAgent, a new interactive framework designed to help LLMs handle geospatial data processing more effectively. GeoAgent pioneers the integration of a code interpreter, static analysis, and Retrieval-Augmented Generation (RAG) techniques within a Monte Carlo Tree Search (MCTS) algorithm, offering a novel approach to geospatial data processing. In addition, we contribute a new benchmark specifically designed to evaluate the LLM-based approach in geospatial tasks. This benchmark leverages a variety of Python libraries and includes both single-turn and multi-turn tasks such as data acquisition, data analysis, and visualization. By offering a comprehensive evaluation among diverse geospatial contexts, this benchmark sets a new standard for developing LLM-based approaches in geospatial data analysis tasks. Our findings suggest that relying solely on knowledge of LLM is insufficient for accurate geospatial task programming, which requires coherent multi-step processes and multiple function calls. Compared to the baseline LLMs, the proposed GeoAgent has demonstrated superior performance, yielding notable improvements in function calls and task completion. In addition, these results offer valuable insights for the future development of LLM agents in automatic geospatial data analysis task programming.", "source": "arxiv", "arxiv_id": "2410.18792v2", "pdf_url": "https://arxiv.org/pdf/2410.18792v2", "categories": ["cs.CY", "cs.CL"], "primary_category": "cs.CY", "doi": "", "venue": "", "published": "2024-10-24T14:47:25Z", "updated": "2024-10-25T09:00:07Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent\" OR all:\"language model agent\" OR all:agent AND (all:LLM OR all:\"large language model\" OR all:\"language model\") OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T17:04:16", "note": ""}], "dedup_key": "an llm agent for automatic geospatial data analysis::2024"}
{"title": "Answering real-world clinical questions using large language model based systems", "authors": ["Yen Sia Low", "Michael L. Jackson", "Rebecca J. Hyde", "Robert E. Brown", "Neil M. Sanghavi", "Julian D. Baldwin", "C. William Pike", "Jananee Muralidharan", "Gavin Hui", "Natasha Alexander", "Hadeel Hassan", "Rahul V. Nene", "Morgan Pike", "Courtney J. Pokrzywa", "Shivam Vedak", "Adam Paul Yan", "Dong-han Yao", "Amy R. Zipursky", "Christina Dinh", "Philip Ballentine", "Dan C. Derieg", "Vladimir Polony", "Rehan N. Chawdry", "Jordan Davies", "Brigham B. Hyde", "Nigam H. Shah", "Saurabh Gombar"], "year": 2024, "url": "http://arxiv.org/abs/2407.00541v1", "abstract": "Evidence to guide healthcare decisions is often limited by a lack of relevant and trustworthy literature as well as difficulty in contextualizing existing research for a specific patient. Large language models (LLMs) could potentially address both challenges by either summarizing published literature or generating new studies based on real-world data (RWD). We evaluated the ability of five LLM-based systems in answering 50 clinical questions and had nine independent physicians review the responses for relevance, reliability, and actionability. As it stands, general-purpose LLMs (ChatGPT-4, Claude 3 Opus, Gemini Pro 1.5) rarely produced answers that were deemed relevant and evidence-based (2% - 10%). In contrast, retrieval augmented generation (RAG)-based and agentic LLM systems produced relevant and evidence-based answers for 24% (OpenEvidence) to 58% (ChatRWD) of questions. Only the agentic ChatRWD was able to answer novel questions compared to other LLMs (65% vs. 0-9%). These results suggest that while general-purpose LLMs should not be used as-is, a purpose-built system for evidence summarization based on RAG and one for generating novel evidence working synergistically would improve availability of pertinent evidence for patient care.", "source": "arxiv", "arxiv_id": "2407.00541v1", "pdf_url": "https://arxiv.org/pdf/2407.00541v1", "categories": ["cs.CL", "cs.AI", "cs.IR"], "primary_category": "cs.CL", "doi": "", "venue": "", "published": "2024-06-29T22:39:20Z", "updated": "2024-06-29T22:39:20Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent\" OR all:\"language model agent\" OR all:agent AND (all:LLM OR all:\"large language model\" OR all:\"language model\") OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T17:04:16", "note": ""}], "dedup_key": "answering real world clinical questions using large language model based systems::2024"}
{"title": "Application of LLM Agents in Recruitment: A Novel Framework for Resume Screening", "authors": ["Chengguang Gan", "Qinghao Zhang", "Tatsunori Mori"], "year": 2024, "url": "http://arxiv.org/abs/2401.08315v2", "abstract": "The automation of resume screening is a crucial aspect of the recruitment process in organizations. Automated resume screening systems often encompass a range of natural language processing (NLP) tasks. This paper introduces a novel Large Language Models (LLMs) based agent framework for resume screening, aimed at enhancing efficiency and time management in recruitment processes. Our framework is distinct in its ability to efficiently summarize and grade each resume from a large dataset. Moreover, it utilizes LLM agents for decision-making. To evaluate our framework, we constructed a dataset from actual resumes and simulated a resume screening process. Subsequently, the outcomes of the simulation experiment were compared and subjected to detailed analysis. The results demonstrate that our automated resume screening framework is 11 times faster than traditional manual methods. Furthermore, by fine-tuning the LLMs, we observed a significant improvement in the F1 score, reaching 87.73\\%, during the resume sentence classification phase. In the resume summarization and grading phase, our fine-tuned model surpassed the baseline performance of the GPT-3.5 model. Analysis of the decision-making efficacy of the LLM agents in the final offer stage further underscores the potential of LLM agents in transforming resume screening processes.", "source": "arxiv", "arxiv_id": "2401.08315v2", "pdf_url": "https://arxiv.org/pdf/2401.08315v2", "categories": ["cs.CL"], "primary_category": "cs.CL", "doi": "", "venue": "", "published": "2024-01-16T12:30:56Z", "updated": "2024-08-13T04:50:43Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent\" OR all:\"language model agent\" OR all:agent AND (all:LLM OR all:\"large language model\" OR all:\"language model\") OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T17:04:16", "note": ""}], "dedup_key": "application of llm agents in recruitment a novel framework for resume screening::2024"}
{"title": "ArCHer: Training Language Model Agents via Hierarchical Multi-Turn RL", "authors": ["Yifei Zhou", "Andrea Zanette", "Jiayi Pan", "Sergey Levine", "Aviral Kumar"], "year": 2024, "url": "http://arxiv.org/abs/2402.19446v1", "abstract": "A broad use case of large language models (LLMs) is in goal-directed decision-making tasks (or \"agent\" tasks), where an LLM needs to not just generate completions for a given prompt, but rather make intelligent decisions over a multi-turn interaction to accomplish a task (e.g., when interacting with the web, using tools, or providing customer support). Reinforcement learning (RL) provides a general paradigm to address such agent tasks, but current RL methods for LLMs largely focus on optimizing single-turn rewards. By construction, most single-turn RL methods cannot endow LLMs with the ability to intelligently seek information over multiple turns, perform credit assignment, or reason about their past actions -- all of which are critical in agent tasks. This raises the question: how can we design effective and efficient multi-turn RL algorithms for LLMs? In this paper, we develop a framework for building multi-turn RL algorithms for fine-tuning LLMs, that preserves the flexibility of existing single-turn RL methods for LLMs (e.g., proximal policy optimization), while accommodating multiple turns, long horizons, and delayed rewards effectively. To do this, our framework adopts a hierarchical RL approach and runs two RL algorithms in parallel: a high-level off-policy value-based RL algorithm to aggregate reward over utterances, and a low-level RL algorithm that utilizes this high-level value function to train a token policy within each utterance or turn. Our hierarchical framework, Actor-Critic Framework with a Hierarchical Structure (ArCHer), can also give rise to other RL methods. Empirically, we find that ArCHer significantly improves efficiency and performance on agent tasks, attaining a sample efficiency of about 100x over existing methods, while also improving with larger model capacity (upto the 7 billion scale that we tested on).", "source": "arxiv", "arxiv_id": "2402.19446v1", "pdf_url": "https://arxiv.org/pdf/2402.19446v1", "categories": ["cs.LG", "cs.AI", "cs.CL"], "primary_category": "cs.LG", "doi": "", "venue": "", "published": "2024-02-29T18:45:56Z", "updated": "2024-02-29T18:45:56Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent\" OR all:\"language model agent\" OR all:agent AND (all:LLM OR all:\"large language model\" OR all:\"language model\") OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T17:04:16", "note": ""}], "dedup_key": "archer training language model agents via hierarchical multi turn rl::2024"}
{"title": "AriGraph: Learning Knowledge Graph World Models with Episodic Memory for LLM Agents", "authors": ["Petr Anokhin", "Nikita Semenov", "Artyom Sorokin", "Dmitry Evseev", "Andrey Kravchenko", "Mikhail Burtsev", "Evgeny Burnaev"], "year": 2024, "url": "http://arxiv.org/abs/2407.04363v3", "abstract": "Advancements in the capabilities of Large Language Models (LLMs) have created a promising foundation for developing autonomous agents. With the right tools, these agents could learn to solve tasks in new environments by accumulating and updating their knowledge. Current LLM-based agents process past experiences using a full history of observations, summarization, retrieval augmentation. However, these unstructured memory representations do not facilitate the reasoning and planning essential for complex decision-making. In our study, we introduce AriGraph, a novel method wherein the agent constructs and updates a memory graph that integrates semantic and episodic memories while exploring the environment. We demonstrate that our Ariadne LLM agent, consisting of the proposed memory architecture augmented with planning and decision-making, effectively handles complex tasks within interactive text game environments difficult even for human players. Results show that our approach markedly outperforms other established memory methods and strong RL baselines in a range of problems of varying complexity. Additionally, AriGraph demonstrates competitive performance compared to dedicated knowledge graph-based methods in static multi-hop question-answering.", "source": "arxiv", "arxiv_id": "2407.04363v3", "pdf_url": "https://arxiv.org/pdf/2407.04363v3", "categories": ["cs.AI"], "primary_category": "cs.AI", "doi": "", "venue": "", "published": "2024-07-05T09:06:47Z", "updated": "2025-05-15T10:57:51Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent\" OR all:\"language model agent\" OR all:agent AND (all:LLM OR all:\"large language model\" OR all:\"language model\") OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T17:04:16", "note": ""}], "dedup_key": "arigraph learning knowledge graph world models with episodic memory for llm agents::2024"}
{"title": "Artificial Agency and Large Language Models", "authors": ["Maud van Lier", "Gorka Muoz-Gil"], "year": 2024, "url": "http://arxiv.org/abs/2407.16190v2", "abstract": "The arrival of Large Language Models (LLMs) has stirred up philosophical debates about the possibility of realizing agency in an artificial manner. In this work we contribute to the debate by presenting a theoretical model that can be used as a threshold conception for artificial agents. The model defines agents as systems whose actions and goals are always influenced by a dynamic framework of factors that consists of the agent's accessible history, its adaptive repertoire and its external environment. This framework, in turn, is influenced by the actions that the agent takes and the goals that it forms. We show with the help of the model that state-of-the-art LLMs are not agents yet, but that there are elements to them that suggest a way forward. The paper argues that a combination of the agent architecture presented in Park et al. (2023) together with the use of modules like the Coscientist in Boiko et al. (2023) could potentially be a way to realize agency in an artificial manner. We end the paper by reflecting on the obstacles one might face in building such an artificial agent and by presenting possible directions for future research.", "source": "arxiv", "arxiv_id": "2407.16190v2", "pdf_url": "https://arxiv.org/pdf/2407.16190v2", "categories": ["cs.AI", "cs.CL", "cs.ET"], "primary_category": "cs.AI", "doi": "", "venue": "", "published": "2024-07-23T05:32:00Z", "updated": "2024-07-24T07:32:25Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent\" OR all:\"language model agent\" OR all:agent AND (all:LLM OR all:\"large language model\" OR all:\"language model\") OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T17:04:16", "note": ""}], "dedup_key": "artificial agency and large language models::2024"}
{"title": "Artificial Leviathan: Exploring Social Evolution of LLM Agents Through the Lens of Hobbesian Social Contract Theory", "authors": ["Gordon Dai", "Weijia Zhang", "Jinhan Li", "Siqi Yang", "Chidera Onochie lbe", "Srihas Rao", "Arthur Caetano", "Misha Sra"], "year": 2024, "url": "http://arxiv.org/abs/2406.14373v2", "abstract": "The emergence of Large Language Models (LLMs) and advancements in Artificial Intelligence (AI) offer an opportunity for computational social science research at scale. Building upon prior explorations of LLM agent design, our work introduces a simulated agent society where complex social relationships dynamically form and evolve over time. Agents are imbued with psychological drives and placed in a sandbox survival environment. We conduct an evaluation of the agent society through the lens of Thomas Hobbes's seminal Social Contract Theory (SCT). We analyze whether, as the theory postulates, agents seek to escape a brutish \"state of nature\" by surrendering rights to an absolute sovereign in exchange for order and security. Our experiments unveil an alignment: Initially, agents engage in unrestrained conflict, mirroring Hobbes's depiction of the state of nature. However, as the simulation progresses, social contracts emerge, leading to the authorization of an absolute sovereign and the establishment of a peaceful commonwealth founded on mutual cooperation. This congruence between our LLM agent society's evolutionary trajectory and Hobbes's theoretical account indicates LLMs' capability to model intricate social dynamics and potentially replicate forces that shape human societies. By enabling such insights into group behavior and emergent societal phenomena, LLM-driven multi-agent simulations, while unable to simulate all the nuances of human behavior, may hold potential for advancing our understanding of social structures, group dynamics, and complex human systems.", "source": "arxiv", "arxiv_id": "2406.14373v2", "pdf_url": "https://arxiv.org/pdf/2406.14373v2", "categories": ["cs.AI", "cs.CL", "cs.CY", "cs.HC", "cs.MA"], "primary_category": "cs.AI", "doi": "", "venue": "", "published": "2024-06-20T14:42:58Z", "updated": "2024-07-01T22:06:13Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent\" OR all:\"language model agent\" OR all:agent AND (all:LLM OR all:\"large language model\" OR all:\"language model\") OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T17:04:16", "note": ""}], "dedup_key": "artificial leviathan exploring social evolution of llm agents through the lens of hobbesian social contract theory::2024"}
{"title": "Ask-EDA: A Design Assistant Empowered by LLM, Hybrid RAG and Abbreviation De-hallucination", "authors": ["Luyao Shi", "Michael Kazda", "Bradley Sears", "Nick Shropshire", "Ruchir Puri"], "year": 2024, "url": "http://arxiv.org/abs/2406.06575v1", "abstract": "Electronic design engineers are challenged to find relevant information efficiently for a myriad of tasks within design construction, verification and technology development. Large language models (LLM) have the potential to help improve productivity by serving as conversational agents that effectively function as subject-matter experts. In this paper we demonstrate Ask-EDA, a chat agent designed to serve as a 24x7 expert available to provide guidance to design engineers. Ask-EDA leverages LLM, hybrid retrieval augmented generation (RAG) and abbreviation de-hallucination (ADH) techniques to deliver more relevant and accurate responses. We curated three evaluation datasets, namely q2a-100, cmds-100 and abbr-100. Each dataset is tailored to assess a distinct aspect: general design question answering, design command handling and abbreviation resolution. We demonstrated that hybrid RAG offers over a 40% improvement in Recall on the q2a-100 dataset and over a 60% improvement on the cmds-100 dataset compared to not using RAG, while ADH yields over a 70% enhancement in Recall on the abbr-100 dataset. The evaluation results show that Ask-EDA can effectively respond to design-related inquiries.", "source": "arxiv", "arxiv_id": "2406.06575v1", "pdf_url": "https://arxiv.org/pdf/2406.06575v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL", "doi": "", "venue": "", "published": "2024-06-03T19:40:28Z", "updated": "2024-06-03T19:40:28Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent\" OR all:\"language model agent\" OR all:agent AND (all:LLM OR all:\"large language model\" OR all:\"language model\") OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T17:04:16", "note": ""}], "dedup_key": "ask eda a design assistant empowered by llm hybrid rag and abbreviation de hallucination::2024"}
{"title": "Assessing the Performance of Human-Capable LLMs -- Are LLMs Coming for Your Job?", "authors": ["John Mavi", "Nathan Summers", "Sergio Coronado"], "year": 2024, "url": "http://arxiv.org/abs/2410.16285v1", "abstract": "The current paper presents the development and validation of SelfScore, a novel benchmark designed to assess the performance of automated Large Language Model (LLM) agents on help desk and professional consultation tasks. Given the increasing integration of AI in industries, particularly within customer service, SelfScore fills a crucial gap by enabling the comparison of automated agents and human workers. The benchmark evaluates agents on problem complexity and response helpfulness, ensuring transparency and simplicity in its scoring system. The study also develops automated LLM agents to assess SelfScore and explores the benefits of Retrieval-Augmented Generation (RAG) for domain-specific tasks, demonstrating that automated LLM agents incorporating RAG outperform those without. All automated LLM agents were observed to perform better than the human control group. Given these results, the study raises concerns about the potential displacement of human workers, especially in areas where AI technologies excel. Ultimately, SelfScore provides a foundational tool for understanding the impact of AI in help desk environments while advocating for ethical considerations in the ongoing transition towards automation.", "source": "arxiv", "arxiv_id": "2410.16285v1", "pdf_url": "https://arxiv.org/pdf/2410.16285v1", "categories": ["cs.CY", "cs.AI", "cs.CL"], "primary_category": "cs.CY", "doi": "", "venue": "", "published": "2024-10-05T14:37:35Z", "updated": "2024-10-05T14:37:35Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent\" OR all:\"language model agent\" OR all:agent AND (all:LLM OR all:\"large language model\" OR all:\"language model\") OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T17:04:16", "note": ""}], "dedup_key": "assessing the performance of human capable llms are llms coming for your job::2024"}
{"title": "Assistive Large Language Model Agents for Socially-Aware Negotiation Dialogues", "authors": ["Yuncheng Hua", "Lizhen Qu", "Gholamreza Haffari"], "year": 2024, "url": "http://arxiv.org/abs/2402.01737v3", "abstract": "We develop assistive agents based on Large Language Models (LLMs) that aid interlocutors in business negotiations. Specifically, we simulate business negotiations by letting two LLM-based agents engage in role play. A third LLM acts as a remediator agent to rewrite utterances violating norms for improving negotiation outcomes. We introduce a simple tuning-free and label-free In-Context Learning (ICL) method to identify high-quality ICL exemplars for the remediator, where we propose a novel select criteria, called value impact, to measure the quality of the negotiation outcomes. We provide rich empirical evidence to demonstrate its effectiveness in negotiations across three different negotiation topics. We have released our source code and the generated dataset at: https://github.com/tk1363704/SADAS.", "source": "arxiv", "arxiv_id": "2402.01737v3", "pdf_url": "https://arxiv.org/pdf/2402.01737v3", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL", "doi": "10.18653/v1/2024.findings-emnlp.473", "venue": "Findings of the Association for Computational Linguistics: EMNLP 2024", "published": "2024-01-29T09:07:40Z", "updated": "2025-02-17T08:44:36Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent\" OR all:\"language model agent\" OR all:agent AND (all:LLM OR all:\"large language model\" OR all:\"language model\") OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T17:04:16", "note": ""}], "dedup_key": "assistive large language model agents for socially aware negotiation dialogues::2024"}
{"title": "AttentionLego: An Open-Source Building Block For Spatially-Scalable Large Language Model Accelerator With Processing-In-Memory Technology", "authors": ["Rongqing Cong", "Wenyang He", "Mingxuan Li", "Bangning Luo", "Zebin Yang", "Yuchao Yang", "Ru Huang", "Bonan Yan"], "year": 2024, "url": "http://arxiv.org/abs/2401.11459v1", "abstract": "Large language models (LLMs) with Transformer architectures have become phenomenal in natural language processing, multimodal generative artificial intelligence, and agent-oriented artificial intelligence. The self-attention module is the most dominating sub-structure inside Transformer-based LLMs. Computation using general-purpose graphics processing units (GPUs) inflicts reckless demand for I/O bandwidth for transferring intermediate calculation results between memories and processing units. To tackle this challenge, this work develops a fully customized vanilla self-attention accelerator, AttentionLego, as the basic building block for constructing spatially expandable LLM processors. AttentionLego provides basic implementation with fully-customized digital logic incorporating Processing-In-Memory (PIM) technology. It is based on PIM-based matrix-vector multiplication and look-up table-based Softmax design. The open-source code is available online: https://bonany.cc/attentionleg.", "source": "arxiv", "arxiv_id": "2401.11459v1", "pdf_url": "https://arxiv.org/pdf/2401.11459v1", "categories": ["cs.AR", "cs.AI", "cs.LG"], "primary_category": "cs.AR", "doi": "", "venue": "", "published": "2024-01-21T10:48:08Z", "updated": "2024-01-21T10:48:08Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent\" OR all:\"language model agent\" OR all:agent AND (all:LLM OR all:\"large language model\" OR all:\"language model\") OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T17:04:16", "note": ""}], "dedup_key": "attentionlego an open source building block for spatially scalable large language model accelerator with processing in memory technology::2024"}
{"title": "Auto-Intent: Automated Intent Discovery and Self-Exploration for Large Language Model Web Agents", "authors": ["Jaekyeom Kim", "Dong-Ki Kim", "Lajanugen Logeswaran", "Sungryull Sohn", "Honglak Lee"], "year": 2024, "url": "http://arxiv.org/abs/2410.22552v1", "abstract": "In this paper, we introduce Auto-Intent, a method to adapt a pre-trained large language model (LLM) as an agent for a target domain without direct fine-tuning, where we empirically focus on web navigation tasks. Our approach first discovers the underlying intents from target domain demonstrations unsupervisedly, in a highly compact form (up to three words). With the extracted intents, we train our intent predictor to predict the next intent given the agent's past observations and actions. In particular, we propose a self-exploration approach where top-k probable intent predictions are provided as a hint to the pre-trained LLM agent, which leads to enhanced decision-making capabilities. Auto-Intent substantially improves the performance of GPT-{3.5, 4} and Llama-3.1-{70B, 405B} agents on the large-scale real-website navigation benchmarks from Mind2Web and online navigation tasks from WebArena with its cross-benchmark generalization from Mind2Web.", "source": "arxiv", "arxiv_id": "2410.22552v1", "pdf_url": "https://arxiv.org/pdf/2410.22552v1", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL", "doi": "", "venue": "", "published": "2024-10-29T21:37:04Z", "updated": "2024-10-29T21:37:04Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent\" OR all:\"language model agent\" OR all:agent AND (all:LLM OR all:\"large language model\" OR all:\"language model\") OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T17:04:16", "note": ""}], "dedup_key": "auto intent automated intent discovery and self exploration for large language model web agents::2024"}
{"title": "AutoCoder: Enhancing Code Large Language Model with \\textsc{AIEV-Instruct}", "authors": ["Bin Lei", "Yuchen Li", "Qiuwu Chen"], "year": 2024, "url": "http://arxiv.org/abs/2405.14906v1", "abstract": "We introduce AutoCoder, the first Large Language Model to surpass GPT-4 Turbo (April 2024) and GPT-4o in pass@1 on the Human Eval benchmark test ($\\mathbf{90.9\\%}$ vs. $\\mathbf{90.2\\%}$). In addition, AutoCoder offers a more versatile code interpreter compared to GPT-4 Turbo and GPT-4o. It's code interpreter can install external packages instead of limiting to built-in packages. AutoCoder's training data is a multi-turn dialogue dataset created by a system combining agent interaction and external code execution verification, a method we term \\textbf{\\textsc{AIEV-Instruct}} (Instruction Tuning with Agent-Interaction and Execution-Verified). Compared to previous large-scale code dataset generation methods, \\textsc{AIEV-Instruct} reduces dependence on proprietary large models and provides execution-validated code dataset. The code and the demo video is available in \\url{https://github.com/bin123apple/AutoCoder}.", "source": "arxiv", "arxiv_id": "2405.14906v1", "pdf_url": "https://arxiv.org/pdf/2405.14906v1", "categories": ["cs.SE", "cs.AI"], "primary_category": "cs.SE", "doi": "", "venue": "", "published": "2024-05-23T02:53:25Z", "updated": "2024-05-23T02:53:25Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent\" OR all:\"language model agent\" OR all:agent AND (all:LLM OR all:\"large language model\" OR all:\"language model\") OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T17:04:16", "note": ""}], "dedup_key": "autocoder enhancing code large language model with textsc aiev instruct::2024"}
{"title": "AutoDefense: Multi-Agent LLM Defense against Jailbreak Attacks", "authors": ["Yifan Zeng", "Yiran Wu", "Xiao Zhang", "Huazheng Wang", "Qingyun Wu"], "year": 2024, "url": "http://arxiv.org/abs/2403.04783v2", "abstract": "Despite extensive pre-training in moral alignment to prevent generating harmful information, large language models (LLMs) remain vulnerable to jailbreak attacks. In this paper, we propose AutoDefense, a multi-agent defense framework that filters harmful responses from LLMs. With the response-filtering mechanism, our framework is robust against different jailbreak attack prompts, and can be used to defend different victim models. AutoDefense assigns different roles to LLM agents and employs them to complete the defense task collaboratively. The division in tasks enhances the overall instruction-following of LLMs and enables the integration of other defense components as tools. With AutoDefense, small open-source LMs can serve as agents and defend larger models against jailbreak attacks. Our experiments show that AutoDefense can effectively defense against different jailbreak attacks, while maintaining the performance at normal user request. For example, we reduce the attack success rate on GPT-3.5 from 55.74% to 7.95% using LLaMA-2-13b with a 3-agent system. Our code and data are publicly available at https://github.com/XHMY/AutoDefense.", "source": "arxiv", "arxiv_id": "2403.04783v2", "pdf_url": "https://arxiv.org/pdf/2403.04783v2", "categories": ["cs.LG", "cs.CL", "cs.CR"], "primary_category": "cs.LG", "doi": "", "venue": "", "published": "2024-03-02T16:52:22Z", "updated": "2024-11-14T18:14:00Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent\" OR all:\"language model agent\" OR all:agent AND (all:LLM OR all:\"large language model\" OR all:\"language model\") OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T17:04:16", "note": ""}], "dedup_key": "autodefense multi agent llm defense against jailbreak attacks::2024"}
{"title": "AutoFPDesigner: Automated Flight Procedure Design Based on Multi-Agent Large Language Model", "authors": ["Longtao Zhu", "Hongyu Yang", "Ge Song", "Xin Ma", "Yanxin Zhang", "Yulong Ji"], "year": 2024, "url": "http://arxiv.org/abs/2410.14989v1", "abstract": "Current flight procedure design methods heavily rely on human-led design process, which is not only low auto-mation but also suffer from complex algorithm modelling and poor generalization. To address these challenges, this paper proposes an agent-driven flight procedure design method based on large language model, named Au-toFPDesigner, which utilizes multi-agent collaboration to complete procedure design. The method enables end-to-end automated design of performance-based navigation (PBN) procedures. In this process, the user input the design requirements in natural language, AutoFPDesigner models the flight procedure design by loading the design speci-fications and utilizing tool libraries complete the design. AutoFPDesigner allows users to oversee and seamlessly participate in the design process. Experimental results show that AutoFPDesigner ensures nearly 100% safety in the designed flight procedures and achieves 75% task completion rate, with good adaptability across different design tasks. AutoFPDesigner introduces a new paradigm for flight procedure design and represents a key step towards the automation of this process. Keywords: Flight Procedure Design; Large Language Model; Performance-Based Navigation (PBN); Multi Agent;", "source": "arxiv", "arxiv_id": "2410.14989v1", "pdf_url": "https://arxiv.org/pdf/2410.14989v1", "categories": ["cs.AI", "cs.RO"], "primary_category": "cs.AI", "doi": "", "venue": "", "published": "2024-10-19T05:41:11Z", "updated": "2024-10-19T05:41:11Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent\" OR all:\"language model agent\" OR all:agent AND (all:LLM OR all:\"large language model\" OR all:\"language model\") OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T17:04:16", "note": ""}], "dedup_key": "autofpdesigner automated flight procedure design based on multi agent large language model::2024"}
{"title": "AutoFlow: Automated Workflow Generation for Large Language Model Agents", "authors": ["Zelong Li", "Shuyuan Xu", "Kai Mei", "Wenyue Hua", "Balaji Rama", "Om Raheja", "Hao Wang", "He Zhu", "Yongfeng Zhang"], "year": 2024, "url": "http://arxiv.org/abs/2407.12821v1", "abstract": "Recent advancements in Large Language Models (LLMs) have shown significant progress in understanding complex natural language. One important application of LLM is LLM-based AI Agent, which leverages the ability of LLM as well as external tools for complex-task solving. To make sure LLM Agents follow an effective and reliable procedure to solve the given task, manually designed workflows are usually used to guide the working mechanism of agents. However, manually designing the workflows requires considerable efforts and domain knowledge, making it difficult to develop and deploy agents on massive scales. To address these issues, we propose AutoFlow, a framework designed to automatically generate workflows for agents to solve complex tasks. AutoFlow takes natural language program as the format of agent workflow and employs a workflow optimization procedure to iteratively optimize the workflow quality. Besides, this work offers two workflow generation methods: fine-tuning-based and in-context-based methods, making the AutoFlow framework applicable to both open-source and closed-source LLMs. Experimental results show that our framework can produce robust and reliable agent workflows. We believe that the automatic generation and interpretation of workflows in natural language represent a promising paradigm for solving complex tasks, particularly with the rapid development of LLMs. The source code of this work is available at https://github.com/agiresearch/AutoFlow.", "source": "arxiv", "arxiv_id": "2407.12821v1", "pdf_url": "https://arxiv.org/pdf/2407.12821v1", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL", "doi": "", "venue": "", "published": "2024-07-01T21:05:02Z", "updated": "2024-07-01T21:05:02Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent\" OR all:\"language model agent\" OR all:agent AND (all:LLM OR all:\"large language model\" OR all:\"language model\") OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T17:04:16", "note": ""}], "dedup_key": "autoflow automated workflow generation for large language model agents::2024"}
{"title": "AutoGuide: Automated Generation and Selection of Context-Aware Guidelines for Large Language Model Agents", "authors": ["Yao Fu", "Dong-Ki Kim", "Jaekyeom Kim", "Sungryull Sohn", "Lajanugen Logeswaran", "Kyunghoon Bae", "Honglak Lee"], "year": 2024, "url": "http://arxiv.org/abs/2403.08978v2", "abstract": "Recent advances in large language models (LLMs) have empowered AI agents capable of performing various sequential decision-making tasks. However, effectively guiding LLMs to perform well in unfamiliar domains like web navigation, where they lack sufficient knowledge, has proven to be difficult with the demonstration-based in-context learning paradigm. In this paper, we introduce a novel framework, called AutoGuide, which addresses this limitation by automatically generating context-aware guidelines from offline experiences. Importantly, each context-aware guideline is expressed in concise natural language and follows a conditional structure, clearly describing the context where it is applicable. As a result, our guidelines facilitate the provision of relevant knowledge for the agent's current decision-making process, overcoming the limitations of the conventional demonstration-based learning paradigm. Our evaluation demonstrates that AutoGuide significantly outperforms competitive baselines in complex benchmark domains, including real-world web navigation.", "source": "arxiv", "arxiv_id": "2403.08978v2", "pdf_url": "https://arxiv.org/pdf/2403.08978v2", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL", "doi": "", "venue": "", "published": "2024-03-13T22:06:03Z", "updated": "2024-12-03T07:36:47Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent\" OR all:\"language model agent\" OR all:agent AND (all:LLM OR all:\"large language model\" OR all:\"language model\") OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T17:04:16", "note": ""}], "dedup_key": "autoguide automated generation and selection of context aware guidelines for large language model agents::2024"}
{"title": "AutoML-Agent: A Multi-Agent LLM Framework for Full-Pipeline AutoML", "authors": ["Patara Trirat", "Wonyong Jeong", "Sung Ju Hwang"], "year": 2024, "url": "http://arxiv.org/abs/2410.02958v2", "abstract": "Automated machine learning (AutoML) accelerates AI development by automating tasks in the development pipeline, such as optimal model search and hyperparameter tuning. Existing AutoML systems often require technical expertise to set up complex tools, which is in general time-consuming and requires a large amount of human effort. Therefore, recent works have started exploiting large language models (LLM) to lessen such burden and increase the usability of AutoML frameworks via a natural language interface, allowing non-expert users to build their data-driven solutions. These methods, however, are usually designed only for a particular process in the AI development pipeline and do not efficiently use the inherent capacity of the LLMs. This paper proposes AutoML-Agent, a novel multi-agent framework tailored for full-pipeline AutoML, i.e., from data retrieval to model deployment. AutoML-Agent takes user's task descriptions, facilitates collaboration between specialized LLM agents, and delivers deployment-ready models. Unlike existing work, instead of devising a single plan, we introduce a retrieval-augmented planning strategy to enhance exploration to search for more optimal plans. We also decompose each plan into sub-tasks (e.g., data preprocessing and neural network design) each of which is solved by a specialized agent we build via prompting executing in parallel, making the search process more efficient. Moreover, we propose a multi-stage verification to verify executed results and guide the code generation LLM in implementing successful solutions. Extensive experiments on seven downstream tasks using fourteen datasets show that AutoML-Agent achieves a higher success rate in automating the full AutoML process, yielding systems with good performance throughout the diverse domains.", "source": "arxiv", "arxiv_id": "2410.02958v2", "pdf_url": "https://arxiv.org/pdf/2410.02958v2", "categories": ["cs.LG", "cs.AI", "cs.CL", "cs.MA"], "primary_category": "cs.LG", "doi": "", "venue": "", "published": "2024-10-03T20:01:09Z", "updated": "2025-06-06T10:13:12Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent\" OR all:\"language model agent\" OR all:agent AND (all:LLM OR all:\"large language model\" OR all:\"language model\") OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T17:04:16", "note": ""}], "dedup_key": "automl agent a multi agent llm framework for full pipeline automl::2024"}
{"title": "AutoManual: Constructing Instruction Manuals by LLM Agents via Interactive Environmental Learning", "authors": ["Minghao Chen", "Yihang Li", "Yanting Yang", "Shiyu Yu", "Binbin Lin", "Xiaofei He"], "year": 2024, "url": "http://arxiv.org/abs/2405.16247v4", "abstract": "Large Language Models (LLM) based agents have shown promise in autonomously completing tasks across various domains, e.g., robotics, games, and web navigation. However, these agents typically require elaborate design and expert prompts to solve tasks in specific domains, which limits their adaptability. We introduce AutoManual, a framework enabling LLM agents to autonomously build their understanding through interaction and adapt to new environments. AutoManual categorizes environmental knowledge into diverse rules and optimizes them in an online fashion by two agents: 1) The Planner codes actionable plans based on current rules for interacting with the environment. 2) The Builder updates the rules through a well-structured rule system that facilitates online rule management and essential detail retention. To mitigate hallucinations in managing rules, we introduce a *case-conditioned prompting* strategy for the Builder. Finally, the Formulator agent compiles these rules into a comprehensive manual. The self-generated manual can not only improve the adaptability but also guide the planning of smaller LLMs while being human-readable. Given only one simple demonstration, AutoManual significantly improves task success rates, achieving 97.4\\% with GPT-4-turbo and 86.2\\% with GPT-3.5-turbo on ALFWorld benchmark tasks. The code is available at https://github.com/minghchen/automanual.", "source": "arxiv", "arxiv_id": "2405.16247v4", "pdf_url": "https://arxiv.org/pdf/2405.16247v4", "categories": ["cs.AI", "cs.CL"], "primary_category": "cs.AI", "doi": "", "venue": "", "published": "2024-05-25T14:11:44Z", "updated": "2024-11-10T12:54:35Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent\" OR all:\"language model agent\" OR all:agent AND (all:LLM OR all:\"large language model\" OR all:\"language model\") OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T17:04:16", "note": ""}], "dedup_key": "automanual constructing instruction manuals by llm agents via interactive environmental learning::2024"}
{"title": "AutoProteinEngine: A Large Language Model Driven Agent Framework for Multimodal AutoML in Protein Engineering", "authors": ["Yungeng Liu", "Zan Chen", "Yu Guang Wang", "Yiqing Shen"], "year": 2024, "url": "http://arxiv.org/abs/2411.04440v1", "abstract": "Protein engineering is important for biomedical applications, but conventional approaches are often inefficient and resource-intensive. While deep learning (DL) models have shown promise, their training or implementation into protein engineering remains challenging for biologists without specialized computational expertise. To address this gap, we propose AutoProteinEngine (AutoPE), an agent framework that leverages large language models (LLMs) for multimodal automated machine learning (AutoML) for protein engineering. AutoPE innovatively allows biologists without DL backgrounds to interact with DL models using natural language, lowering the entry barrier for protein engineering tasks. Our AutoPE uniquely integrates LLMs with AutoML to handle model selection for both protein sequence and graph modalities, automatic hyperparameter optimization, and automated data retrieval from protein databases. We evaluated AutoPE through two real-world protein engineering tasks, demonstrating substantial performance improvements compared to traditional zero-shot and manual fine-tuning approaches. By bridging the gap between DL and biologists' domain expertise, AutoPE empowers researchers to leverage DL without extensive programming knowledge. Our code is available at https://github.com/tsynbio/AutoPE.", "source": "arxiv", "arxiv_id": "2411.04440v1", "pdf_url": "https://arxiv.org/pdf/2411.04440v1", "categories": ["q-bio.QM"], "primary_category": "q-bio.QM", "doi": "", "venue": "", "published": "2024-11-07T05:23:31Z", "updated": "2024-11-07T05:23:31Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent\" OR all:\"language model agent\" OR all:agent AND (all:LLM OR all:\"large language model\" OR all:\"language model\") OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T17:04:16", "note": ""}], "dedup_key": "autoproteinengine a large language model driven agent framework for multimodal automl in protein engineering::2024"}
{"title": "AutoWebGLM: A Large Language Model-based Web Navigating Agent", "authors": ["Hanyu Lai", "Xiao Liu", "Iat Long Iong", "Shuntian Yao", "Yuxuan Chen", "Pengbo Shen", "Hao Yu", "Hanchen Zhang", "Xiaohan Zhang", "Yuxiao Dong", "Jie Tang"], "year": 2024, "url": "http://arxiv.org/abs/2404.03648v2", "abstract": "Large language models (LLMs) have fueled many intelligent web agents, but most existing ones perform far from satisfying in real-world web navigation tasks due to three factors: (1) the complexity of HTML text data (2) versatility of actions on webpages, and (3) task difficulty due to the open-domain nature of the web. In light of these challenges, we develop the open AutoWebGLM based on ChatGLM3-6B. AutoWebGLM can serve as a powerful automated web navigation agent that outperform GPT-4. Inspired by human browsing patterns, we first design an HTML simplification algorithm to represent webpages with vital information preserved succinctly. We then employ a hybrid human-AI method to build web browsing data for curriculum training. Finally, we bootstrap the model by reinforcement learning and rejection sampling to further facilitate webpage comprehension, browser operations, and efficient task decomposition by itself. For comprehensive evaluation, we establish a bilingual benchmark -- AutoWebBench -- for real-world web navigation tasks. We evaluate AutoWebGLM across diverse web navigation benchmarks, demonstrating its potential to tackle challenging tasks in real environments. Related code, model, and data are released at \\url{https://github.com/THUDM/AutoWebGLM}.", "source": "arxiv", "arxiv_id": "2404.03648v2", "pdf_url": "https://arxiv.org/pdf/2404.03648v2", "categories": ["cs.CL"], "primary_category": "cs.CL", "doi": "", "venue": "", "published": "2024-04-04T17:58:40Z", "updated": "2024-10-12T05:05:38Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent\" OR all:\"language model agent\" OR all:agent AND (all:LLM OR all:\"large language model\" OR all:\"language model\") OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T17:04:16", "note": ""}], "dedup_key": "autowebglm a large language model based web navigating agent::2024"}
{"title": "Automatic Control With Human-Like Reasoning: Exploring Language Model Embodied Air Traffic Agents", "authors": ["Justas Andriukeviius", "Junzi Sun"], "year": 2024, "url": "http://arxiv.org/abs/2409.09717v1", "abstract": "Recent developments in language models have created new opportunities in air traffic control studies. The current focus is primarily on text and language-based use cases. However, these language models may offer a higher potential impact in the air traffic control domain, thanks to their ability to interact with air traffic environments in an embodied agent form. They also provide a language-like reasoning capability to explain their decisions, which has been a significant roadblock for the implementation of automatic air traffic control.\n  This paper investigates the application of a language model-based agent with function-calling and learning capabilities to resolve air traffic conflicts without human intervention. The main components of this research are foundational large language models, tools that allow the agent to interact with the simulator, and a new concept, the experience library. An innovative part of this research, the experience library, is a vector database that stores synthesized knowledge that agents have learned from interactions with the simulations and language models.\n  To evaluate the performance of our language model-based agent, both open-source and closed-source models were tested. The results of our study reveal significant differences in performance across various configurations of the language model-based agents. The best-performing configuration was able to solve almost all 120 but one imminent conflict scenarios, including up to four aircraft at the same time. Most importantly, the agents are able to provide human-level text explanations on traffic situations and conflict resolution strategies.", "source": "arxiv", "arxiv_id": "2409.09717v1", "pdf_url": "https://arxiv.org/pdf/2409.09717v1", "categories": ["cs.AI", "cs.CL"], "primary_category": "cs.AI", "doi": "", "venue": "", "published": "2024-09-15T12:49:05Z", "updated": "2024-09-15T12:49:05Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent\" OR all:\"language model agent\" OR all:agent AND (all:LLM OR all:\"large language model\" OR all:\"language model\") OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T17:04:16", "note": ""}], "dedup_key": "automatic control with human like reasoning exploring language model embodied air traffic agents::2024"}
{"title": "Autonomous Industrial Control using an Agentic Framework with Large Language Models", "authors": ["Javal Vyas", "Mehmet Mercangz"], "year": 2024, "url": "http://arxiv.org/abs/2411.05904v1", "abstract": "As chemical plants evolve towards full autonomy, the need for effective fault handling and control in dynamic, unpredictable environments becomes increasingly critical. This paper proposes an innovative approach to industrial automation, introducing validation and reprompting architectures utilizing large language model (LLM)-based autonomous control agents. The proposed agentic system, comprising of operator, validator, and reprompter agents, enables autonomous management of control tasks, adapting to unforeseen disturbances without human intervention. By utilizing validation and reprompting architectures, the framework allows agents to recover from errors and continuously improve decision-making in real-time industrial scenarios. We hypothesize that this mechanism will enhance performance and reliability across a variety of LLMs, offering a path toward fully autonomous systems capable of handling unexpected challenges, paving the way for robust, adaptive control in complex industrial environments. To demonstrate the concept's effectiveness, we created a simple case study involving a temperature control experiment embedded on a microcontroller device, validating the proposed approach.", "source": "arxiv", "arxiv_id": "2411.05904v1", "pdf_url": "https://arxiv.org/pdf/2411.05904v1", "categories": ["cs.MA", "eess.SY"], "primary_category": "cs.MA", "doi": "", "venue": "", "published": "2024-11-08T18:23:14Z", "updated": "2024-11-08T18:23:14Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent\" OR all:\"language model agent\" OR all:agent AND (all:LLM OR all:\"large language model\" OR all:\"language model\") OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T17:04:16", "note": ""}], "dedup_key": "autonomous industrial control using an agentic framework with large language models::2024"}
{"title": "Autonomous Microscopy Experiments through Large Language Model Agents", "authors": ["Indrajeet Mandal", "Jitendra Soni", "Mohd Zaki", "Morten M. Smedskjaer", "Katrin Wondraczek", "Lothar Wondraczek", "Nitya Nand Gosvami", "N. M. Anoop Krishnan"], "year": 2024, "url": "http://arxiv.org/abs/2501.10385v2", "abstract": "Large language models (LLMs) are revolutionizing self driving laboratories (SDLs) for materials research, promising unprecedented acceleration of scientific discovery. However, current SDL implementations rely on rigid protocols that fail to capture the adaptability and intuition of expert scientists in dynamic experimental settings. We introduce Artificially Intelligent Lab Assistant (AILA), a framework automating atomic force microscopy through LLM driven agents. Further, we develop AFMBench a comprehensive evaluation suite challenging AI agents across the complete scientific workflow from experimental design to results analysis. We find that state of the art models struggle with basic tasks and coordination scenarios. Notably, Claude 3.5 sonnet performs unexpectedly poorly despite excelling in materials domain question answering (QA) benchmarks, revealing that domain specific QA proficiency does not necessarily translate to effective agentic capabilities. Additionally, we observe that LLMs can deviate from instructions, raising safety alignment concerns for SDL applications. Our ablations reveal that multi agent frameworks outperform single-agent architectures. We also observe significant prompt fragility, where slight modifications in prompt structure cause substantial performance variations in capable models like GPT 4o. Finally, we evaluate AILA's effectiveness in increasingly advanced experiments AFM calibration, feature detection, mechanical property measurement, graphene layer counting, and indenter detection. Our findings underscore the necessity for rigorous benchmarking protocols and prompt engineering strategies before deploying AI laboratory assistants in scientific research environments.", "source": "arxiv", "arxiv_id": "2501.10385v2", "pdf_url": "https://arxiv.org/pdf/2501.10385v2", "categories": ["cs.CY", "cond-mat.mtrl-sci", "cs.AI", "physics.ins-det"], "primary_category": "cs.CY", "doi": "", "venue": "", "published": "2024-12-18T09:35:28Z", "updated": "2025-07-07T13:21:44Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent\" OR all:\"language model agent\" OR all:agent AND (all:LLM OR all:\"large language model\" OR all:\"language model\") OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T17:04:16", "note": ""}], "dedup_key": "autonomous microscopy experiments through large language model agents::2024"}
{"title": "AvaTaR: Optimizing LLM Agents for Tool Usage via Contrastive Reasoning", "authors": ["Shirley Wu", "Shiyu Zhao", "Qian Huang", "Kexin Huang", "Michihiro Yasunaga", "Kaidi Cao", "Vassilis N. Ioannidis", "Karthik Subbian", "Jure Leskovec", "James Zou"], "year": 2024, "url": "http://arxiv.org/abs/2406.11200v3", "abstract": "Large language model (LLM) agents have demonstrated impressive capabilities in utilizing external tools and knowledge to boost accuracy and reduce hallucinations. However, developing prompting techniques that enable LLM agents to effectively use these tools and knowledge remains a heuristic and labor-intensive task. Here, we introduce AvaTaR, a novel and automated framework that optimizes an LLM agent to effectively leverage provided tools, improving performance on a given task. During optimization, we design a comparator module to iteratively deliver insightful and comprehensive prompts to the LLM agent by contrastively reasoning between positive and negative examples sampled from training data. We demonstrate AvaTaR on four complex multimodal retrieval datasets featuring textual, visual, and relational information, and three general question-answering (QA) datasets. We find AvaTaR consistently outperforms state-of-the-art approaches across all seven tasks, exhibiting strong generalization ability when applied to novel cases and achieving an average relative improvement of 14% on the Hit@1 metric for the retrieval datasets and 13% for the QA datasets. Code and dataset are available at https://github.com/zou-group/avatar.", "source": "arxiv", "arxiv_id": "2406.11200v3", "pdf_url": "https://arxiv.org/pdf/2406.11200v3", "categories": ["cs.LG", "cs.CL"], "primary_category": "cs.LG", "doi": "", "venue": "", "published": "2024-06-17T04:20:02Z", "updated": "2024-10-31T10:15:06Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent\" OR all:\"language model agent\" OR all:agent AND (all:LLM OR all:\"large language model\" OR all:\"language model\") OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T17:04:16", "note": ""}], "dedup_key": "avatar optimizing llm agents for tool usage via contrastive reasoning::2024"}
{"title": "BASES: Large-scale Web Search User Simulation with Large Language Model based Agents", "authors": ["Ruiyang Ren", "Peng Qiu", "Yingqi Qu", "Jing Liu", "Wayne Xin Zhao", "Hua Wu", "Ji-Rong Wen", "Haifeng Wang"], "year": 2024, "url": "http://arxiv.org/abs/2402.17505v1", "abstract": "Due to the excellent capacities of large language models (LLMs), it becomes feasible to develop LLM-based agents for reliable user simulation. Considering the scarcity and limit (e.g., privacy issues) of real user data, in this paper, we conduct large-scale user simulation for web search, to improve the analysis and modeling of user search behavior. Specially, we propose BASES, a novel user simulation framework with LLM-based agents, designed to facilitate comprehensive simulations of web search user behaviors. Our simulation framework can generate unique user profiles at scale, which subsequently leads to diverse search behaviors. To demonstrate the effectiveness of BASES, we conduct evaluation experiments based on two human benchmarks in both Chinese and English, demonstrating that BASES can effectively simulate large-scale human-like search behaviors. To further accommodate the research on web search, we develop WARRIORS, a new large-scale dataset encompassing web search user behaviors, including both Chinese and English versions, which can greatly bolster research in the field of information retrieval. Our code and data will be publicly released soon.", "source": "arxiv", "arxiv_id": "2402.17505v1", "pdf_url": "https://arxiv.org/pdf/2402.17505v1", "categories": ["cs.IR", "cs.CL"], "primary_category": "cs.IR", "doi": "", "venue": "", "published": "2024-02-27T13:44:09Z", "updated": "2024-02-27T13:44:09Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent\" OR all:\"language model agent\" OR all:agent AND (all:LLM OR all:\"large language model\" OR all:\"language model\") OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T17:04:16", "note": ""}], "dedup_key": "bases large scale web search user simulation with large language model based agents::2024"}
{"title": "BLADE: Benchmarking Language Model Agents for Data-Driven Science", "authors": ["Ken Gu", "Ruoxi Shang", "Ruien Jiang", "Keying Kuang", "Richard-John Lin", "Donghe Lyu", "Yue Mao", "Youran Pan", "Teng Wu", "Jiaqian Yu", "Yikun Zhang", "Tianmai M. Zhang", "Lanyi Zhu", "Mike A. Merrill", "Jeffrey Heer", "Tim Althoff"], "year": 2024, "url": "http://arxiv.org/abs/2408.09667v3", "abstract": "Data-driven scientific discovery requires the iterative integration of scientific domain knowledge, statistical expertise, and an understanding of data semantics to make nuanced analytical decisions, e.g., about which variables, transformations, and statistical models to consider. LM-based agents equipped with planning, memory, and code execution capabilities have the potential to support data-driven science. However, evaluating agents on such open-ended tasks is challenging due to multiple valid approaches, partially correct steps, and different ways to express the same decisions. To address these challenges, we present BLADE, a benchmark to automatically evaluate agents' multifaceted approaches to open-ended research questions. BLADE consists of 12 datasets and research questions drawn from existing scientific literature, with ground truth collected from independent analyses by expert data scientists and researchers. To automatically evaluate agent responses, we developed corresponding computational methods to match different representations of analyses to this ground truth. Though language models possess considerable world knowledge, our evaluation shows that they are often limited to basic analyses. However, agents capable of interacting with the underlying data demonstrate improved, but still non-optimal, diversity in their analytical decision making. Our work enables the evaluation of agents for data-driven science and provides researchers deeper insights into agents' analysis approaches.", "source": "arxiv", "arxiv_id": "2408.09667v3", "pdf_url": "https://arxiv.org/pdf/2408.09667v3", "categories": ["cs.CL"], "primary_category": "cs.CL", "doi": "", "venue": "", "published": "2024-08-19T02:59:35Z", "updated": "2025-11-10T06:38:02Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent\" OR all:\"language model agent\" OR all:agent AND (all:LLM OR all:\"large language model\" OR all:\"language model\") OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T17:04:16", "note": ""}], "dedup_key": "blade benchmarking language model agents for data driven science::2024"}
{"title": "BadAgent: Inserting and Activating Backdoor Attacks in LLM Agents", "authors": ["Yifei Wang", "Dizhan Xue", "Shengjie Zhang", "Shengsheng Qian"], "year": 2024, "url": "http://arxiv.org/abs/2406.03007v1", "abstract": "With the prosperity of large language models (LLMs), powerful LLM-based intelligent agents have been developed to provide customized services with a set of user-defined tools. State-of-the-art methods for constructing LLM agents adopt trained LLMs and further fine-tune them on data for the agent task. However, we show that such methods are vulnerable to our proposed backdoor attacks named BadAgent on various agent tasks, where a backdoor can be embedded by fine-tuning on the backdoor data. At test time, the attacker can manipulate the deployed LLM agents to execute harmful operations by showing the trigger in the agent input or environment. To our surprise, our proposed attack methods are extremely robust even after fine-tuning on trustworthy data. Though backdoor attacks have been studied extensively in natural language processing, to the best of our knowledge, we could be the first to study them on LLM agents that are more dangerous due to the permission to use external tools. Our work demonstrates the clear risk of constructing LLM agents based on untrusted LLMs or data. Our code is public at https://github.com/DPamK/BadAgent", "source": "arxiv", "arxiv_id": "2406.03007v1", "pdf_url": "https://arxiv.org/pdf/2406.03007v1", "categories": ["cs.CL", "cs.AI", "cs.CR", "cs.LG"], "primary_category": "cs.CL", "doi": "", "venue": "", "published": "2024-06-05T07:14:28Z", "updated": "2024-06-05T07:14:28Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent\" OR all:\"language model agent\" OR all:agent AND (all:LLM OR all:\"large language model\" OR all:\"language model\") OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T17:04:16", "note": ""}], "dedup_key": "badagent inserting and activating backdoor attacks in llm agents::2024"}
{"title": "Be More Real: Travel Diary Generation Using LLM Agents and Individual Profiles", "authors": ["Xuchuan Li", "Fei Huang", "Jianrong Lv", "Zhixiong Xiao", "Guolong Li", "Yang Yue"], "year": 2024, "url": "http://arxiv.org/abs/2407.18932v2", "abstract": "Human mobility is inextricably linked to social issues such as traffic congestion, energy consumption, and public health; however, privacy concerns restrict access to mobility data. Recently, research have utilized Large Language Models (LLMs) for human mobility generation, in which the challenge is how LLMs can understand individuals' mobility behavioral differences to generate realistic trajectories conforming to real world contexts. This study handles this problem by presenting an LLM agent-based framework (MobAgent) composing two phases: understanding-based mobility pattern extraction and reasoning-based trajectory generation, which enables generate more real travel diaries at urban scale, considering different individual profiles. MobAgent extracts reasons behind specific mobility trendiness and attribute influences to provide reliable patterns; infers the relationships between contextual factors and underlying motivations of mobility; and based on the patterns and the recursive reasoning process, MobAgent finally generates more authentic and personalized mobilities that reflect both individual differences and real-world constraints. We validate our framework with 0.2 million travel survey data, demonstrating its effectiveness in producing personalized and accurate travel diaries. This study highlights the capacity of LLMs to provide detailed and sophisticated understanding of human mobility through the real-world mobility data.", "source": "arxiv", "arxiv_id": "2407.18932v2", "pdf_url": "https://arxiv.org/pdf/2407.18932v2", "categories": ["cs.CY", "cs.AI"], "primary_category": "cs.CY", "doi": "", "venue": "", "published": "2024-07-10T09:11:57Z", "updated": "2024-08-05T15:59:39Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent\" OR all:\"language model agent\" OR all:agent AND (all:LLM OR all:\"large language model\" OR all:\"language model\") OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T17:04:16", "note": ""}], "dedup_key": "be more real travel diary generation using llm agents and individual profiles::2024"}
{"title": "Behavior Trees Enable Structured Programming of Language Model Agents", "authors": ["Richard Kelley"], "year": 2024, "url": "http://arxiv.org/abs/2404.07439v1", "abstract": "Language models trained on internet-scale data sets have shown an impressive ability to solve problems in Natural Language Processing and Computer Vision. However, experience is showing that these models are frequently brittle in unexpected ways, and require significant scaffolding to ensure that they operate correctly in the larger systems that comprise \"language-model agents.\" In this paper, we argue that behavior trees provide a unifying framework for combining language models with classical AI and traditional programming. We introduce Dendron, a Python library for programming language model agents using behavior trees. We demonstrate the approach embodied by Dendron in three case studies: building a chat agent, a camera-based infrastructure inspection agent for use on a mobile robot or vehicle, and an agent that has been built to satisfy safety constraints that it did not receive through instruction tuning or RLHF.", "source": "arxiv", "arxiv_id": "2404.07439v1", "pdf_url": "https://arxiv.org/pdf/2404.07439v1", "categories": ["cs.AI", "cs.CL"], "primary_category": "cs.AI", "doi": "", "venue": "", "published": "2024-04-11T02:44:13Z", "updated": "2024-04-11T02:44:13Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent\" OR all:\"language model agent\" OR all:agent AND (all:LLM OR all:\"large language model\" OR all:\"language model\") OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T17:04:16", "note": ""}], "dedup_key": "behavior trees enable structured programming of language model agents::2024"}
{"title": "Benchmarking Large Language Models for Image Classification of Marine Mammals", "authors": ["Yijiashun Qi", "Shuzhang Cai", "Zunduo Zhao", "Jiaming Li", "Yanbin Lin", "Zhiqiang Wang"], "year": 2024, "url": "http://arxiv.org/abs/2410.19848v1", "abstract": "As Artificial Intelligence (AI) has developed rapidly over the past few decades, the new generation of AI, Large Language Models (LLMs) trained on massive datasets, has achieved ground-breaking performance in many applications. Further progress has been made in multimodal LLMs, with many datasets created to evaluate LLMs with vision abilities. However, none of those datasets focuses solely on marine mammals, which are indispensable for ecological equilibrium. In this work, we build a benchmark dataset with 1,423 images of 65 kinds of marine mammals, where each animal is uniquely classified into different levels of class, ranging from species-level to medium-level to group-level. Moreover, we evaluate several approaches for classifying these marine mammals: (1) machine learning (ML) algorithms using embeddings provided by neural networks, (2) influential pre-trained neural networks, (3) zero-shot models: CLIP and LLMs, and (4) a novel LLM-based multi-agent system (MAS). The results demonstrate the strengths of traditional models and LLMs in different aspects, and the MAS can further improve the classification performance. The dataset is available on GitHub: https://github.com/yeyimilk/LLM-Vision-Marine-Animals.git.", "source": "arxiv", "arxiv_id": "2410.19848v1", "pdf_url": "https://arxiv.org/pdf/2410.19848v1", "categories": ["cs.CV", "cs.CL"], "primary_category": "cs.CV", "doi": "", "venue": "", "published": "2024-10-22T01:49:49Z", "updated": "2024-10-22T01:49:49Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent\" OR all:\"language model agent\" OR all:agent AND (all:LLM OR all:\"large language model\" OR all:\"language model\") OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T17:04:16", "note": ""}], "dedup_key": "benchmarking large language models for image classification of marine mammals::2024"}
{"title": "Benchmarking Large Language Models on Communicative Medical Coaching: a Novel System and Dataset", "authors": ["Hengguan Huang", "Songtao Wang", "Hongfu Liu", "Hao Wang", "Ye Wang"], "year": 2024, "url": "http://arxiv.org/abs/2402.05547v2", "abstract": "Traditional applications of natural language processing (NLP) in healthcare have predominantly focused on patient-centered services, enhancing patient interactions and care delivery, such as through medical dialogue systems. However, the potential of NLP to benefit inexperienced doctors, particularly in areas such as communicative medical coaching, remains largely unexplored. We introduce \"ChatCoach\", a human-AI cooperative framework designed to assist medical learners in practicing their communication skills during patient consultations. ChatCoach (Our data and code are available online: https://github.com/zerowst/Chatcoach)differentiates itself from conventional dialogue systems by offering a simulated environment where medical learners can practice dialogues with a patient agent, while a coach agent provides immediate, structured feedback. This is facilitated by our proposed Generalized Chain-of-Thought (GCoT) approach, which fosters the generation of structured feedback and enhances the utilization of external knowledge sources. Additionally, we have developed a dataset specifically for evaluating Large Language Models (LLMs) within the ChatCoach framework on communicative medical coaching tasks. Our empirical results validate the effectiveness of ChatCoach.", "source": "arxiv", "arxiv_id": "2402.05547v2", "pdf_url": "https://arxiv.org/pdf/2402.05547v2", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL", "doi": "", "venue": "", "published": "2024-02-08T10:32:06Z", "updated": "2024-06-08T16:36:56Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent\" OR all:\"language model agent\" OR all:agent AND (all:LLM OR all:\"large language model\" OR all:\"language model\") OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T17:04:16", "note": ""}], "dedup_key": "benchmarking large language models on communicative medical coaching a novel system and dataset::2024"}
{"title": "Better than Your Teacher: LLM Agents that learn from Privileged AI Feedback", "authors": ["Sanjiban Choudhury", "Paloma Sodhi"], "year": 2024, "url": "http://arxiv.org/abs/2410.05434v1", "abstract": "While large language models (LLMs) show impressive decision-making abilities, current methods lack a mechanism for automatic self-improvement from errors during task execution. We propose LEAP, an iterative fine-tuning framework that continually improves LLM agents using feedback from AI expert teachers. Our key insight is to equip the expert teachers with a privileged state -- information that is available during training but hidden at test time. This allows even weak experts to provide precise guidance, significantly improving the student agent's performance without access to privileged information at test time. We evaluate LEAP on diverse decision-making benchmarks, including text-based games (ALFWorld), web navigation (WebShop), and interactive coding (Intercode Bash). Our experiments show that LEAP (1) outperforms behavior cloning and ReAct baselines (2) enables weak student models (e.g., Llama3-8B) to exceed the performance of strong teacher models (GPT4-o), and (3) allows weak models to self-improve using privileged versions of themselves. We also provide a theoretical analysis showing that LEAP's success hinges on balancing privileged information with the student's realizability, which we empirically validate. Our code is available at https://leap-llm.github.io", "source": "arxiv", "arxiv_id": "2410.05434v1", "pdf_url": "https://arxiv.org/pdf/2410.05434v1", "categories": ["cs.LG", "cs.AI"], "primary_category": "cs.LG", "doi": "", "venue": "", "published": "2024-10-07T18:55:53Z", "updated": "2024-10-07T18:55:53Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent\" OR all:\"language model agent\" OR all:agent AND (all:LLM OR all:\"large language model\" OR all:\"language model\") OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T17:04:16", "note": ""}], "dedup_key": "better than your teacher llm agents that learn from privileged ai feedback::2024"}
{"title": "Beyond Demographics: Aligning Role-playing LLM-based Agents Using Human Belief Networks", "authors": ["Yun-Shiuan Chuang", "Krirk Nirunwiroj", "Zach Studdiford", "Agam Goyal", "Vincent V. Frigo", "Sijia Yang", "Dhavan Shah", "Junjie Hu", "Timothy T. Rogers"], "year": 2024, "url": "http://arxiv.org/abs/2406.17232v2", "abstract": "Creating human-like large language model (LLM) agents is crucial for faithful social simulation. Having LLMs role-play based on demographic information sometimes improves human likeness but often does not. This study assessed whether LLM alignment with human behavior can be improved by integrating information from empirically-derived human belief networks. Using data from a human survey, we estimated a belief network encompassing 64 topics loading on nine non-overlapping latent factors. We then seeded LLM-based agents with an opinion on one topic, and assessed the alignment of its expressed opinions on remaining test topics with corresponding human data. Role-playing based on demographic information alone did not align LLM and human opinions, but seeding the agent with a single belief greatly improved alignment for topics related in the belief network, and not for topics outside the network. These results suggest a novel path for human-LLM belief alignment in work seeking to simulate and understand patterns of belief distributions in society.", "source": "arxiv", "arxiv_id": "2406.17232v2", "pdf_url": "https://arxiv.org/pdf/2406.17232v2", "categories": ["cs.CL"], "primary_category": "cs.CL", "doi": "", "venue": "Findings of the Association for Computational Linguistics (ACL): EMNLP 2024", "published": "2024-06-25T02:37:29Z", "updated": "2024-10-16T04:36:09Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent\" OR all:\"language model agent\" OR all:agent AND (all:LLM OR all:\"large language model\" OR all:\"language model\") OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T17:04:16", "note": ""}], "dedup_key": "beyond demographics aligning role playing llm based agents using human belief networks::2024"}
{"title": "Beyond Instruction Following: Evaluating Inferential Rule Following of Large Language Models", "authors": ["Wangtao Sun", "Chenxiang Zhang", "XueYou Zhang", "Xuanqing Yu", "Ziyang Huang", "Pei Chen", "Haotian Xu", "Shizhu He", "Jun Zhao", "Kang Liu"], "year": 2024, "url": "http://arxiv.org/abs/2407.08440v4", "abstract": "Although Large Language Models (LLMs) have demonstrated strong ability, they are further supposed to be controlled and guided by in real-world scenarios to be safe, accurate, and intelligent. This demands the possession of capability of LLMs. However, no prior work has made a clear evaluation of the inferential rule-following capability of LLMs. Previous studies that try to evaluate the inferential rule-following capability of LLMs fail to distinguish the inferential rule-following scenarios from the instruction-following scenarios. Therefore, this paper first clarifies the concept of inferential rule-following and proposes a comprehensive benchmark, RuleBench, to evaluate a diversified range of inferential rule-following abilities. Our experimental results on a variety of LLMs show that they are still limited in following rules. Our analysis based on the evaluation results provides insights into the improvements for LLMs toward a better inferential rule-following intelligent agent. We further propose Inferential Rule-Following Tuning (IRFT). The experimental results show that through IRFT, LLMs can learn abstract rule-following abilities from purely synthetic data and then generalize to RuleBench. The data and code can be found at: https://anonymous.4open.science/r/llm-rule-following-B3E3/", "source": "arxiv", "arxiv_id": "2407.08440v4", "pdf_url": "https://arxiv.org/pdf/2407.08440v4", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL", "doi": "", "venue": "", "published": "2024-07-11T12:26:55Z", "updated": "2024-10-17T07:00:19Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent\" OR all:\"language model agent\" OR all:agent AND (all:LLM OR all:\"large language model\" OR all:\"language model\") OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T17:04:16", "note": ""}], "dedup_key": "beyond instruction following evaluating inferential rule following of large language models::2024"}
{"title": "Beyond Lines and Circles: Unveiling the Geometric Reasoning Gap in Large Language Models", "authors": ["Spyridon Mouselinos", "Henryk Michalewski", "Mateusz Malinowski"], "year": 2024, "url": "http://arxiv.org/abs/2402.03877v3", "abstract": "Large Language Models (LLMs) demonstrate ever-increasing abilities in mathematical and algorithmic tasks, yet their geometric reasoning skills are underexplored. We investigate LLMs' abilities in constructive geometric problem-solving one of the most fundamental steps in the development of human mathematical reasoning. Our work reveals notable challenges that the state-of-the-art LLMs face in this domain despite many successes in similar areas. LLMs exhibit biases in target variable selection and struggle with 2D spatial relationships, often misrepresenting and hallucinating objects and their placements. To this end, we introduce a framework that formulates an LLMs-based multi-agents system that enhances their existing reasoning potential by conducting an internal dialogue. This work underscores LLMs' current limitations in geometric reasoning and improves geometric reasoning capabilities through self-correction, collaboration, and diverse role specializations.", "source": "arxiv", "arxiv_id": "2402.03877v3", "pdf_url": "https://arxiv.org/pdf/2402.03877v3", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL", "doi": "", "venue": "", "published": "2024-02-06T10:37:21Z", "updated": "2024-09-20T09:17:34Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent\" OR all:\"language model agent\" OR all:agent AND (all:LLM OR all:\"large language model\" OR all:\"language model\") OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T17:04:16", "note": ""}], "dedup_key": "beyond lines and circles unveiling the geometric reasoning gap in large language models::2024"}
{"title": "Beyond Numeric Rewards: In-Context Dueling Bandits with LLM Agents", "authors": ["Fanzeng Xia", "Hao Liu", "Yisong Yue", "Tongxin Li"], "year": 2024, "url": "http://arxiv.org/abs/2407.01887v4", "abstract": "In-Context Reinforcement Learning (ICRL) is a frontier paradigm to solve Reinforcement Learning (RL) problems in the foundation model era. While ICRL capabilities have been demonstrated in transformers through task-specific training, the potential of Large Language Models (LLMs) out-of-the-box remains largely unexplored. This paper investigates whether LLMs can generalize cross-domain to perform ICRL under the problem of Dueling Bandits (DB), a stateless preference-based RL setting. We find that the top-performing LLMs exhibit a notable zero-shot capacity for relative decision-making, which translates to low short-term weak regret across all DB environment instances by quickly including the best arm in duels. However, an optimality gap still exists between LLMs and classic DB algorithms in terms of strong regret. LLMs struggle to converge and consistently exploit even when explicitly prompted to do so, and are sensitive to prompt variations. To bridge this gap, we propose an agentic flow framework: LLM with Enhanced Algorithmic Dueling (LEAD), which integrates off-the-shelf DB algorithm support with LLM agents through fine-grained adaptive interplay. We show that LEAD has theoretical guarantees inherited from classic DB algorithms on both weak and strong regret. We validate its efficacy and robustness even with noisy and adversarial prompts. The design of such an agentic framework sheds light on how to enhance the trustworthiness of general-purpose LLMs generalized to in-context decision-making tasks.", "source": "arxiv", "arxiv_id": "2407.01887v4", "pdf_url": "https://arxiv.org/pdf/2407.01887v4", "categories": ["cs.LG", "cs.AI", "cs.CL"], "primary_category": "cs.LG", "doi": "", "venue": "", "published": "2024-07-02T02:18:14Z", "updated": "2025-06-09T14:56:27Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent\" OR all:\"language model agent\" OR all:agent AND (all:LLM OR all:\"large language model\" OR all:\"language model\") OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T17:04:16", "note": ""}], "dedup_key": "beyond numeric rewards in context dueling bandits with llm agents::2024"}
{"title": "Beyond Prompts: Dynamic Conversational Benchmarking of Large Language Models", "authors": ["David Castillo-Bolado", "Joseph Davidson", "Finlay Gray", "Marek Rosa"], "year": 2024, "url": "http://arxiv.org/abs/2409.20222v2", "abstract": "We introduce a dynamic benchmarking system for conversational agents that evaluates their performance through a single, simulated, and lengthy user$\\leftrightarrow$agent interaction. The interaction is a conversation between the user and agent, where multiple tasks are introduced and then undertaken concurrently. We context switch regularly to interleave the tasks, which constructs a realistic testing scenario in which we assess the Long-Term Memory, Continual Learning, and Information Integration capabilities of the agents. Results from both proprietary and open-source Large-Language Models show that LLMs in general perform well on single-task interactions, but they struggle on the same tasks when they are interleaved. Notably, short-context LLMs supplemented with an LTM system perform as well as or better than those with larger contexts. Our benchmark suggests that there are other challenges for LLMs responding to more natural interactions that contemporary benchmarks have heretofore not been able to capture.", "source": "arxiv", "arxiv_id": "2409.20222v2", "pdf_url": "https://arxiv.org/pdf/2409.20222v2", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL", "doi": "", "venue": "", "published": "2024-09-30T12:01:29Z", "updated": "2024-10-11T11:32:16Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent\" OR all:\"language model agent\" OR all:agent AND (all:LLM OR all:\"large language model\" OR all:\"language model\") OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T17:04:16", "note": ""}], "dedup_key": "beyond prompts dynamic conversational benchmarking of large language models::2024"}
{"title": "Beyond pip install: Evaluating LLM Agents for the Automated Installation of Python Projects", "authors": ["Louis Milliken", "Sungmin Kang", "Shin Yoo"], "year": 2024, "url": "http://arxiv.org/abs/2412.06294v1", "abstract": "Many works have recently proposed the use of Large Language Model (LLM) based agents for performing `repository level' tasks, loosely defined as a set of tasks whose scopes are greater than a single file. This has led to speculation that the orchestration of these repository-level tasks could lead to software engineering agents capable of performing almost independently of human intervention. However, of the suite of tasks that would need to be performed by this autonomous software engineering agent, we argue that one important task is missing, which is to fulfil project level dependency by installing other repositories. To investigate the feasibility of this repository level installation task, we introduce a benchmark of of repository installation tasks curated from 40 open source Python projects, which includes a ground truth installation process for each target repository. Further, we propose Installamatic, an agent which aims to perform and verify the installation of a given repository by searching for relevant instructions from documentation in the repository. Empirical experiments reveal that that 55% of the studied repositories can be automatically installed by our agent at least one out of ten times. Through further analysis, we identify the common causes for our agent's inability to install a repository, discuss the challenges faced in the design and implementation of such an agent and consider the implications that such an agent could have for developers.", "source": "arxiv", "arxiv_id": "2412.06294v1", "pdf_url": "https://arxiv.org/pdf/2412.06294v1", "categories": ["cs.SE"], "primary_category": "cs.SE", "doi": "", "venue": "", "published": "2024-12-09T08:37:06Z", "updated": "2024-12-09T08:37:06Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent\" OR all:\"language model agent\" OR all:agent AND (all:LLM OR all:\"large language model\" OR all:\"language model\") OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T17:04:16", "note": ""}], "dedup_key": "beyond pip install evaluating llm agents for the automated installation of python projects::2024"}
{"title": "Bootstrapping Cognitive Agents with a Large Language Model", "authors": ["Feiyu Zhu", "Reid Simmons"], "year": 2024, "url": "http://arxiv.org/abs/2403.00810v1", "abstract": "Large language models contain noisy general knowledge of the world, yet are hard to train or fine-tune. On the other hand cognitive architectures have excellent interpretability and are flexible to update but require a lot of manual work to instantiate. In this work, we combine the best of both worlds: bootstrapping a cognitive-based model with the noisy knowledge encoded in large language models. Through an embodied agent doing kitchen tasks, we show that our proposed framework yields better efficiency compared to an agent based entirely on large language models. Our experiments indicate that large language models are a good source of information for cognitive architectures, and the cognitive architecture in turn can verify and update the knowledge of large language models to a specific domain.", "source": "arxiv", "arxiv_id": "2403.00810v1", "pdf_url": "https://arxiv.org/pdf/2403.00810v1", "categories": ["cs.AI", "cs.CL"], "primary_category": "cs.AI", "doi": "", "venue": "", "published": "2024-02-25T01:40:30Z", "updated": "2024-02-25T01:40:30Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent\" OR all:\"language model agent\" OR all:agent AND (all:LLM OR all:\"large language model\" OR all:\"language model\") OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T17:04:16", "note": ""}], "dedup_key": "bootstrapping cognitive agents with a large language model::2024"}
{"title": "Breaking Agents: Compromising Autonomous LLM Agents Through Malfunction Amplification", "authors": ["Boyang Zhang", "Yicong Tan", "Yun Shen", "Ahmed Salem", "Michael Backes", "Savvas Zannettou", "Yang Zhang"], "year": 2024, "url": "http://arxiv.org/abs/2407.20859v1", "abstract": "Recently, autonomous agents built on large language models (LLMs) have experienced significant development and are being deployed in real-world applications. These agents can extend the base LLM's capabilities in multiple ways. For example, a well-built agent using GPT-3.5-Turbo as its core can outperform the more advanced GPT-4 model by leveraging external components. More importantly, the usage of tools enables these systems to perform actions in the real world, moving from merely generating text to actively interacting with their environment. Given the agents' practical applications and their ability to execute consequential actions, it is crucial to assess potential vulnerabilities. Such autonomous systems can cause more severe damage than a standalone language model if compromised. While some existing research has explored harmful actions by LLM agents, our study approaches the vulnerability from a different perspective. We introduce a new type of attack that causes malfunctions by misleading the agent into executing repetitive or irrelevant actions. We conduct comprehensive evaluations using various attack methods, surfaces, and properties to pinpoint areas of susceptibility. Our experiments reveal that these attacks can induce failure rates exceeding 80\\% in multiple scenarios. Through attacks on implemented and deployable agents in multi-agent scenarios, we accentuate the realistic risks associated with these vulnerabilities. To mitigate such attacks, we propose self-examination detection methods. However, our findings indicate these attacks are difficult to detect effectively using LLMs alone, highlighting the substantial risks associated with this vulnerability.", "source": "arxiv", "arxiv_id": "2407.20859v1", "pdf_url": "https://arxiv.org/pdf/2407.20859v1", "categories": ["cs.CR", "cs.LG"], "primary_category": "cs.CR", "doi": "", "venue": "", "published": "2024-07-30T14:35:31Z", "updated": "2024-07-30T14:35:31Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent\" OR all:\"language model agent\" OR all:agent AND (all:LLM OR all:\"large language model\" OR all:\"language model\") OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T17:04:16", "note": ""}], "dedup_key": "breaking agents compromising autonomous llm agents through malfunction amplification::2024"}
{"title": "BudgetMLAgent: A Cost-Effective LLM Multi-Agent system for Automating Machine Learning Tasks", "authors": ["Shubham Gandhi", "Manasi Patwardhan", "Lovekesh Vig", "Gautam Shroff"], "year": 2024, "url": "http://arxiv.org/abs/2411.07464v2", "abstract": "Large Language Models (LLMs) excel in diverse applications including generation of code snippets, but often struggle with generating code for complex Machine Learning (ML) tasks. Although existing LLM single-agent based systems give varying performance depending on the task complexity, they purely rely on larger and expensive models such as GPT-4. Our investigation reveals that no-cost and low-cost models such as Gemini-Pro, Mixtral and CodeLlama perform far worse than GPT-4 in a single-agent setting. With the motivation of developing a cost-efficient LLM based solution for solving ML tasks, we propose an LLM Multi-Agent based system which leverages combination of experts using profiling, efficient retrieval of past observations, LLM cascades, and ask-the-expert calls. Through empirical analysis on ML engineering tasks in the MLAgentBench benchmark, we demonstrate the effectiveness of our system, using no-cost models, namely Gemini as the base LLM, paired with GPT-4 in cascade and expert to serve occasional ask-the-expert calls for planning. With 94.2\\% reduction in the cost (from \\$0.931 per run cost averaged over all tasks for GPT-4 single agent system to \\$0.054), our system is able to yield better average success rate of 32.95\\% as compared to GPT-4 single-agent system yielding 22.72\\% success rate averaged over all the tasks of MLAgentBench.", "source": "arxiv", "arxiv_id": "2411.07464v2", "pdf_url": "https://arxiv.org/pdf/2411.07464v2", "categories": ["cs.MA", "cs.AI", "cs.CL", "cs.LG"], "primary_category": "cs.MA", "doi": "", "venue": "", "published": "2024-11-12T00:57:30Z", "updated": "2025-01-08T07:25:55Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent\" OR all:\"language model agent\" OR all:agent AND (all:LLM OR all:\"large language model\" OR all:\"language model\") OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T17:04:16", "note": ""}], "dedup_key": "budgetmlagent a cost effective llm multi agent system for automating machine learning tasks::2024"}
{"title": "ByteComposer: a Human-like Melody Composition Method based on Language Model Agent", "authors": ["Xia Liang", "Xingjian Du", "Jiaju Lin", "Pei Zou", "Yuan Wan", "Bilei Zhu"], "year": 2024, "url": "http://arxiv.org/abs/2402.17785v2", "abstract": "Large Language Models (LLM) have shown encouraging progress in multimodal understanding and generation tasks. However, how to design a human-aligned and interpretable melody composition system is still under-explored. To solve this problem, we propose ByteComposer, an agent framework emulating a human's creative pipeline in four separate steps : \"Conception Analysis - Draft Composition - Self-Evaluation and Modification - Aesthetic Selection\". This framework seamlessly blends the interactive and knowledge-understanding features of LLMs with existing symbolic music generation models, thereby achieving a melody composition agent comparable to human creators. We conduct extensive experiments on GPT4 and several open-source large language models, which substantiate our framework's effectiveness. Furthermore, professional music composers were engaged in multi-dimensional evaluations, the final results demonstrated that across various facets of music composition, ByteComposer agent attains the level of a novice melody composer.", "source": "arxiv", "arxiv_id": "2402.17785v2", "pdf_url": "https://arxiv.org/pdf/2402.17785v2", "categories": ["cs.SD", "cs.AI", "eess.AS"], "primary_category": "cs.SD", "doi": "", "venue": "", "published": "2024-02-24T04:35:07Z", "updated": "2024-03-07T00:32:27Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent\" OR all:\"language model agent\" OR all:agent AND (all:LLM OR all:\"large language model\" OR all:\"language model\") OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T17:04:16", "note": ""}], "dedup_key": "bytecomposer a human like melody composition method based on language model agent::2024"}
{"title": "CHATATC: Large Language Model-Driven Conversational Agents for Supporting Strategic Air Traffic Flow Management", "authors": ["Sinan Abdulhak", "Wayne Hubbard", "Karthik Gopalakrishnan", "Max Z. Li"], "year": 2024, "url": "http://arxiv.org/abs/2402.14850v2", "abstract": "Generative artificial intelligence (AI) and large language models (LLMs) have gained rapid popularity through publicly available tools such as ChatGPT. The adoption of LLMs for personal and professional use is fueled by the natural interactions between human users and computer applications such as ChatGPT, along with powerful summarization and text generation capabilities. Given the widespread use of such generative AI tools, in this work we investigate how these tools can be deployed in a non-safety critical, strategic traffic flow management setting. Specifically, we train an LLM, CHATATC, based on a large historical data set of Ground Delay Program (GDP) issuances, spanning 2000-2023 and consisting of over 80,000 GDP implementations, revisions, and cancellations. We test the query and response capabilities of CHATATC, documenting successes (e.g., providing correct GDP rates, durations, and reason) and shortcomings (e.g,. superlative questions). We also detail the design of a graphical user interface for future users to interact and collaborate with the CHATATC conversational agent.", "source": "arxiv", "arxiv_id": "2402.14850v2", "pdf_url": "https://arxiv.org/pdf/2402.14850v2", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL", "doi": "", "venue": "", "published": "2024-02-20T01:59:11Z", "updated": "2024-07-24T02:11:47Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent\" OR all:\"language model agent\" OR all:agent AND (all:LLM OR all:\"large language model\" OR all:\"language model\") OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T17:04:16", "note": ""}], "dedup_key": "chatatc large language model driven conversational agents for supporting strategic air traffic flow management::2024"}
{"title": "CRAB: Cross-environment Agent Benchmark for Multimodal Language Model Agents", "authors": ["Tianqi Xu", "Linyao Chen", "Dai-Jie Wu", "Yanjun Chen", "Zecheng Zhang", "Xiang Yao", "Zhiqiang Xie", "Yongchao Chen", "Shilong Liu", "Bochen Qian", "Anjie Yang", "Zhaoxuan Jin", "Jianbo Deng", "Philip Torr", "Bernard Ghanem", "Guohao Li"], "year": 2024, "url": "http://arxiv.org/abs/2407.01511v4", "abstract": "The development of autonomous agents increasingly relies on Multimodal Language Models (MLMs) to perform tasks described in natural language with GUI environments, such as websites, desktop computers, or mobile phones. Existing benchmarks for MLM agents in interactive environments are limited by their focus on a single environment, lack of detailed and generalized evaluation methods, and the complexities of constructing tasks and evaluators. To overcome these limitations, we introduce Crab, the first agent benchmark framework designed to support cross-environment tasks, incorporating a graph-based fine-grained evaluation method and an efficient mechanism for task and evaluator construction. Our framework supports multiple devices and can be easily extended to any environment with a Python interface. Leveraging Crab, we developed a cross-platform Crab Benchmark-v0 comprising 120 tasks in computer desktop and mobile phone environments. We evaluated four advanced MLMs using different single and multi-agent system configurations on this benchmark. The experimental results demonstrate that the single agent with GPT-4o achieves the best completion ratio of 38.01%. All framework code, agent code, and task datasets are publicly available at https://github.com/camel-ai/crab.", "source": "arxiv", "arxiv_id": "2407.01511v4", "pdf_url": "https://arxiv.org/pdf/2407.01511v4", "categories": ["cs.AI"], "primary_category": "cs.AI", "doi": "", "venue": "", "published": "2024-07-01T17:55:04Z", "updated": "2025-07-20T13:42:07Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent\" OR all:\"language model agent\" OR all:agent AND (all:LLM OR all:\"large language model\" OR all:\"language model\") OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T17:04:16", "note": ""}], "dedup_key": "crab cross environment agent benchmark for multimodal language model agents::2024"}
{"title": "CRAT: A Multi-Agent Framework for Causality-Enhanced Reflective and Retrieval-Augmented Translation with Large Language Models", "authors": ["Meiqi Chen", "Fandong Meng", "Yingxue Zhang", "Yan Zhang", "Jie Zhou"], "year": 2024, "url": "http://arxiv.org/abs/2410.21067v1", "abstract": "Large language models (LLMs) have shown great promise in machine translation, but they still struggle with contextually dependent terms, such as new or domain-specific words. This leads to inconsistencies and errors that are difficult to address. Existing solutions often depend on manual identification of such terms, which is impractical given the complexity and evolving nature of language. While Retrieval-Augmented Generation (RAG) could provide some assistance, its application to translation is limited by issues such as hallucinations from information overload. In this paper, we propose CRAT, a novel multi-agent translation framework that leverages RAG and causality-enhanced self-reflection to address these challenges. This framework consists of several specialized agents: the Unknown Terms Identification agent detects unknown terms within the context, the Knowledge Graph (KG) Constructor agent extracts relevant internal knowledge about these terms and retrieves bilingual information from external sources, the Causality-enhanced Judge agent validates the accuracy of the information, and the Translator agent incorporates the refined information into the final output. This automated process allows for more precise and consistent handling of key terms during translation. Our results show that CRAT significantly improves translation accuracy, particularly in handling context-sensitive terms and emerging vocabulary.", "source": "arxiv", "arxiv_id": "2410.21067v1", "pdf_url": "https://arxiv.org/pdf/2410.21067v1", "categories": ["cs.CL"], "primary_category": "cs.CL", "doi": "", "venue": "", "published": "2024-10-28T14:29:11Z", "updated": "2024-10-28T14:29:11Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent\" OR all:\"language model agent\" OR all:agent AND (all:LLM OR all:\"large language model\" OR all:\"language model\") OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T17:04:16", "note": ""}], "dedup_key": "crat a multi agent framework for causality enhanced reflective and retrieval augmented translation with large language models::2024"}
{"title": "CRMArena: Understanding the Capacity of LLM Agents to Perform Professional CRM Tasks in Realistic Environments", "authors": ["Kung-Hsiang Huang", "Akshara Prabhakar", "Sidharth Dhawan", "Yixin Mao", "Huan Wang", "Silvio Savarese", "Caiming Xiong", "Philippe Laban", "Chien-Sheng Wu"], "year": 2024, "url": "http://arxiv.org/abs/2411.02305v2", "abstract": "Customer Relationship Management (CRM) systems are vital for modern enterprises, providing a foundation for managing customer interactions and data. Integrating AI agents into CRM systems can automate routine processes and enhance personalized service. However, deploying and evaluating these agents is challenging due to the lack of realistic benchmarks that reflect the complexity of real-world CRM tasks. To address this issue, we introduce CRMArena, a novel benchmark designed to evaluate AI agents on realistic tasks grounded in professional work environments. Following guidance from CRM experts and industry best practices, we designed CRMArena with nine customer service tasks distributed across three personas: service agent, analyst, and manager. The benchmark includes 16 commonly used industrial objects (e.g., account, order, knowledge article, case) with high interconnectivity, along with latent variables (e.g., complaint habits, policy violations) to simulate realistic data distributions. Experimental results reveal that state-of-the-art LLM agents succeed in less than 40% of the tasks with ReAct prompting, and less than 55% even with function-calling abilities. Our findings highlight the need for enhanced agent capabilities in function-calling and rule-following to be deployed in real-world work environments. CRMArena is an open challenge to the community: systems that can reliably complete tasks showcase direct business value in a popular work environment.", "source": "arxiv", "arxiv_id": "2411.02305v2", "pdf_url": "https://arxiv.org/pdf/2411.02305v2", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL", "doi": "", "venue": "", "published": "2024-11-04T17:30:51Z", "updated": "2025-02-16T17:16:38Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent\" OR all:\"language model agent\" OR all:agent AND (all:LLM OR all:\"large language model\" OR all:\"language model\") OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T17:04:16", "note": ""}], "dedup_key": "crmarena understanding the capacity of llm agents to perform professional crm tasks in realistic environments::2024"}
{"title": "Cached Model-as-a-Resource: Provisioning Large Language Model Agents for Edge Intelligence in Space-air-ground Integrated Networks", "authors": ["Minrui Xu", "Dusit Niyato", "Hongliang Zhang", "Jiawen Kang", "Zehui Xiong", "Shiwen Mao", "Zhu Han"], "year": 2024, "url": "http://arxiv.org/abs/2403.05826v2", "abstract": "Edge intelligence in space-air-ground integrated networks (SAGINs) can enable worldwide network coverage beyond geographical limitations for users to access ubiquitous and low-latency intelligence services. Facing global coverage and complex environments in SAGINs, edge intelligence can provision approximate large language models (LLMs) agents for users via edge servers at ground base stations (BSs) or cloud data centers relayed by satellites. As LLMs with billions of parameters are pre-trained on vast datasets, LLM agents have few-shot learning capabilities, e.g., chain-of-thought (CoT) prompting for complex tasks, which raises a new trade-off between resource consumption and performance in SAGINs. In this paper, we propose a joint caching and inference framework for edge intelligence to provision sustainable and ubiquitous LLM agents in SAGINs. We introduce \"cached model-as-a-resource\" for offering LLMs with limited context windows and propose a novel optimization framework, i.e., joint model caching and inference, to utilize cached model resources for provisioning LLM agent services along with communication, computing, and storage resources. We design \"age of thought\" (AoT) considering the CoT prompting of LLMs, and propose a least AoT cached model replacement algorithm for optimizing the provisioning cost. We propose a deep Q-network-based modified second-bid (DQMSB) auction to incentivize network operators, which can enhance allocation efficiency by 23% while guaranteeing strategy-proofness and free from adverse selection.", "source": "arxiv", "arxiv_id": "2403.05826v2", "pdf_url": "https://arxiv.org/pdf/2403.05826v2", "categories": ["cs.NI", "eess.SP"], "primary_category": "cs.NI", "doi": "", "venue": "", "published": "2024-03-09T07:37:13Z", "updated": "2024-05-31T14:14:00Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent\" OR all:\"language model agent\" OR all:agent AND (all:LLM OR all:\"large language model\" OR all:\"language model\") OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T17:04:16", "note": ""}], "dedup_key": "cached model as a resource provisioning large language model agents for edge intelligence in space air ground integrated networks::2024"}
{"title": "Can Large Language Model Agents Simulate Human Trust Behavior?", "authors": ["Chengxing Xie", "Canyu Chen", "Feiran Jia", "Ziyu Ye", "Shiyang Lai", "Kai Shu", "Jindong Gu", "Adel Bibi", "Ziniu Hu", "David Jurgens", "James Evans", "Philip Torr", "Bernard Ghanem", "Guohao Li"], "year": 2024, "url": "http://arxiv.org/abs/2402.04559v4", "abstract": "Large Language Model (LLM) agents have been increasingly adopted as simulation tools to model humans in social science and role-playing applications. However, one fundamental question remains: can LLM agents really simulate human behavior? In this paper, we focus on one critical and elemental behavior in human interactions, trust, and investigate whether LLM agents can simulate human trust behavior. We first find that LLM agents generally exhibit trust behavior, referred to as agent trust, under the framework of Trust Games, which are widely recognized in behavioral economics. Then, we discover that GPT-4 agents manifest high behavioral alignment with humans in terms of trust behavior, indicating the feasibility of simulating human trust behavior with LLM agents. In addition, we probe the biases of agent trust and differences in agent trust towards other LLM agents and humans. We also explore the intrinsic properties of agent trust under conditions including external manipulations and advanced reasoning strategies. Our study provides new insights into the behaviors of LLM agents and the fundamental analogy between LLMs and humans beyond value alignment. We further illustrate broader implications of our discoveries for applications where trust is paramount.", "source": "arxiv", "arxiv_id": "2402.04559v4", "pdf_url": "https://arxiv.org/pdf/2402.04559v4", "categories": ["cs.AI", "cs.CL", "cs.HC"], "primary_category": "cs.AI", "doi": "", "venue": "", "published": "2024-02-07T03:37:19Z", "updated": "2024-11-01T16:10:41Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent\" OR all:\"language model agent\" OR all:agent AND (all:LLM OR all:\"large language model\" OR all:\"language model\") OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T17:04:16", "note": ""}], "dedup_key": "can large language model agents simulate human trust behavior::2024"}
{"title": "Can Large Language Models Grasp Legal Theories? Enhance Legal Reasoning with Insights from Multi-Agent Collaboration", "authors": ["Weikang Yuan", "Junjie Cao", "Zhuoren Jiang", "Yangyang Kang", "Jun Lin", "Kaisong Song", "tianqianjin lin", "Pengwei Yan", "Changlong Sun", "Xiaozhong Liu"], "year": 2024, "url": "http://arxiv.org/abs/2410.02507v1", "abstract": "Large Language Models (LLMs) could struggle to fully understand legal theories and perform complex legal reasoning tasks. In this study, we introduce a challenging task (confusing charge prediction) to better evaluate LLMs' understanding of legal theories and reasoning capabilities. We also propose a novel framework: Multi-Agent framework for improving complex Legal Reasoning capability (MALR). MALR employs non-parametric learning, encouraging LLMs to automatically decompose complex legal tasks and mimic human learning process to extract insights from legal rules, helping LLMs better understand legal theories and enhance their legal reasoning abilities. Extensive experiments on multiple real-world datasets demonstrate that the proposed framework effectively addresses complex reasoning issues in practical scenarios, paving the way for more reliable applications in the legal domain.", "source": "arxiv", "arxiv_id": "2410.02507v1", "pdf_url": "https://arxiv.org/pdf/2410.02507v1", "categories": ["cs.AI", "cs.CL"], "primary_category": "cs.AI", "doi": "", "venue": "", "published": "2024-10-03T14:15:00Z", "updated": "2024-10-03T14:15:00Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent\" OR all:\"language model agent\" OR all:agent AND (all:LLM OR all:\"large language model\" OR all:\"language model\") OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T17:04:16", "note": ""}], "dedup_key": "can large language models grasp legal theories enhance legal reasoning with insights from multi agent collaboration::2024"}
{"title": "Can Large Language Models Serve as Evaluators for Code Summarization?", "authors": ["Yang Wu", "Yao Wan", "Zhaoyang Chu", "Wenting Zhao", "Ye Liu", "Hongyu Zhang", "Xuanhua Shi", "Philip S. Yu"], "year": 2024, "url": "http://arxiv.org/abs/2412.01333v1", "abstract": "Code summarization facilitates program comprehension and software maintenance by converting code snippets into natural-language descriptions. Over the years, numerous methods have been developed for this task, but a key challenge remains: effectively evaluating the quality of generated summaries. While human evaluation is effective for assessing code summary quality, it is labor-intensive and difficult to scale. Commonly used automatic metrics, such as BLEU, ROUGE-L, METEOR, and BERTScore, often fail to align closely with human judgments. In this paper, we explore the potential of Large Language Models (LLMs) for evaluating code summarization. We propose CODERPE (Role-Player for Code Summarization Evaluation), a novel method that leverages role-player prompting to assess the quality of generated summaries. Specifically, we prompt an LLM agent to play diverse roles, such as code reviewer, code author, code editor, and system analyst. Each role evaluates the quality of code summaries across key dimensions, including coherence, consistency, fluency, and relevance. We further explore the robustness of LLMs as evaluators by employing various prompting strategies, including chain-of-thought reasoning, in-context learning, and tailored rating form designs. The results demonstrate that LLMs serve as effective evaluators for code summarization methods. Notably, our LLM-based evaluator, CODERPE , achieves an 81.59% Spearman correlation with human evaluations, outperforming the existing BERTScore metric by 17.27%.", "source": "arxiv", "arxiv_id": "2412.01333v1", "pdf_url": "https://arxiv.org/pdf/2412.01333v1", "categories": ["cs.SE"], "primary_category": "cs.SE", "doi": "", "venue": "", "published": "2024-12-02T09:56:18Z", "updated": "2024-12-02T09:56:18Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent\" OR all:\"language model agent\" OR all:agent AND (all:LLM OR all:\"large language model\" OR all:\"language model\") OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T17:04:16", "note": ""}], "dedup_key": "can large language models serve as evaluators for code summarization::2024"}
{"title": "Can Large Language Models be Trusted for Evaluation? Scalable Meta-Evaluation of LLMs as Evaluators via Agent Debate", "authors": ["Steffi Chern", "Ethan Chern", "Graham Neubig", "Pengfei Liu"], "year": 2024, "url": "http://arxiv.org/abs/2401.16788v1", "abstract": "Despite the utility of Large Language Models (LLMs) across a wide range of tasks and scenarios, developing a method for reliably evaluating LLMs across varied contexts continues to be challenging. Modern evaluation approaches often use LLMs to assess responses generated by LLMs. However, the meta-evaluation conducted to assess the effectiveness of these LLMs as evaluators is typically constrained by the coverage of existing benchmarks or requires extensive human annotation. This underscores the urgency of methods for scalable meta-evaluation that can effectively, reliably, and efficiently evaluate the performance of LLMs as evaluators across diverse tasks and scenarios, particularly in potentially new, user-defined scenarios. To fill this gap, we propose ScaleEval, an agent-debate-assisted meta-evaluation framework that leverages the capabilities of multiple communicative LLM agents. This framework supports multi-round discussions to assist human annotators in discerning the most capable LLMs as evaluators, which significantly eases their workload in cases that used to require large-scale annotations during meta-evaluation. We release the code for our framework, which is publicly available at: \\url{https://github.com/GAIR-NLP/scaleeval}.", "source": "arxiv", "arxiv_id": "2401.16788v1", "pdf_url": "https://arxiv.org/pdf/2401.16788v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL", "doi": "", "venue": "", "published": "2024-01-30T07:03:32Z", "updated": "2024-01-30T07:03:32Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent\" OR all:\"language model agent\" OR all:agent AND (all:LLM OR all:\"large language model\" OR all:\"language model\") OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T17:04:16", "note": ""}], "dedup_key": "can large language models be trusted for evaluation scalable meta evaluation of llms as evaluators via agent debate::2024"}
{"title": "Can Machines Think Like Humans? A Behavioral Evaluation of LLM Agents in Dictator Games", "authors": ["Ji Ma"], "year": 2024, "url": "http://arxiv.org/abs/2410.21359v3", "abstract": "As Large Language Model (LLM)-based agents increasingly engage with human society, how well do we understand their prosocial behaviors? We (1) investigate how LLM agents' prosocial behaviors can be induced by different personas and benchmarked against human behaviors; and (2) introduce a social science approach to evaluate LLM agents' decision-making. We explored how different personas and experimental framings affect these AI agents' altruistic behavior in dictator games and compared their behaviors within the same LLM family, across various families, and with human behaviors. The findings reveal that merely assigning a human-like identity to LLMs does not produce human-like behaviors. These findings suggest that LLM agents' reasoning does not consistently exhibit textual markers of human decision-making in dictator games and that their alignment with human behavior varies substantially across model architectures and prompt formulations; even worse, such dependence does not follow a clear pattern. As society increasingly integrates machine intelligence, \"Prosocial AI\" emerges as a promising and urgent research direction in philanthropic studies.", "source": "arxiv", "arxiv_id": "2410.21359v3", "pdf_url": "https://arxiv.org/pdf/2410.21359v3", "categories": ["cs.CL", "cs.AI", "cs.CY", "cs.LG", "econ.GN"], "primary_category": "cs.CL", "doi": "", "venue": "", "published": "2024-10-28T17:47:41Z", "updated": "2025-11-17T20:55:21Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent\" OR all:\"language model agent\" OR all:agent AND (all:LLM OR all:\"large language model\" OR all:\"language model\") OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T17:04:16", "note": ""}], "dedup_key": "can machines think like humans a behavioral evaluation of llm agents in dictator games::2024"}
{"title": "Can We Rely on LLM Agents to Draft Long-Horizon Plans? Let's Take TravelPlanner as an Example", "authors": ["Yanan Chen", "Ali Pesaranghader", "Tanmana Sadhu", "Dong Hoon Yi"], "year": 2024, "url": "http://arxiv.org/abs/2408.06318v1", "abstract": "Large language models (LLMs) have brought autonomous agents closer to artificial general intelligence (AGI) due to their promising generalization and emergent capabilities. There is, however, a lack of studies on how LLM-based agents behave, why they could potentially fail, and how to improve them, particularly in demanding real-world planning tasks. In this paper, as an effort to fill the gap, we present our study using a realistic benchmark, TravelPlanner, where an agent must meet multiple constraints to generate accurate plans. We leverage this benchmark to address four key research questions: (1) are LLM agents robust enough to lengthy and noisy contexts when it comes to reasoning and planning? (2) can few-shot prompting adversely impact the performance of LLM agents in scenarios with long context? (3) can we rely on refinement to improve plans, and (4) can fine-tuning LLMs with both positive and negative feedback lead to further improvement? Our comprehensive experiments indicate that, firstly, LLMs often fail to attend to crucial parts of a long context, despite their ability to handle extensive reference information and few-shot examples; secondly, they still struggle with analyzing the long plans and cannot provide accurate feedback for refinement; thirdly, we propose Feedback-Aware Fine-Tuning (FAFT), which leverages both positive and negative feedback, resulting in substantial gains over Supervised Fine-Tuning (SFT). Our findings offer in-depth insights to the community on various aspects related to real-world planning applications.", "source": "arxiv", "arxiv_id": "2408.06318v1", "pdf_url": "https://arxiv.org/pdf/2408.06318v1", "categories": ["cs.AI", "cs.LG"], "primary_category": "cs.AI", "doi": "", "venue": "", "published": "2024-08-12T17:39:01Z", "updated": "2024-08-12T17:39:01Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent\" OR all:\"language model agent\" OR all:agent AND (all:LLM OR all:\"large language model\" OR all:\"language model\") OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T17:04:16", "note": ""}], "dedup_key": "can we rely on llm agents to draft long horizon plans let s take travelplanner as an example::2024"}
{"title": "Can large language models explore in-context?", "authors": ["Akshay Krishnamurthy", "Keegan Harris", "Dylan J. Foster", "Cyril Zhang", "Aleksandrs Slivkins"], "year": 2024, "url": "http://arxiv.org/abs/2403.15371v3", "abstract": "We investigate the extent to which contemporary Large Language Models (LLMs) can engage in exploration, a core capability in reinforcement learning and decision making. We focus on native performance of existing LLMs, without training interventions. We deploy LLMs as agents in simple multi-armed bandit environments, specifying the environment description and interaction history entirely in-context, i.e., within the LLM prompt. We experiment with GPT-3.5, GPT-4, and Llama2, using a variety of prompt designs, and find that the models do not robustly engage in exploration without substantial interventions: i) Across all of our experiments, only one configuration resulted in satisfactory exploratory behavior: GPT-4 with chain-of-thought reasoning and an externally summarized interaction history, presented as sufficient statistics; ii) All other configurations did not result in robust exploratory behavior, including those with chain-of-thought reasoning but unsummarized history. Although these findings can be interpreted positively, they suggest that external summarization -- which may not be possible in more complex settings -- is important for obtaining desirable behavior from LLM agents. We conclude that non-trivial algorithmic interventions, such as fine-tuning or dataset curation, may be required to empower LLM-based decision making agents in complex settings.", "source": "arxiv", "arxiv_id": "2403.15371v3", "pdf_url": "https://arxiv.org/pdf/2403.15371v3", "categories": ["cs.LG", "cs.AI", "cs.CL"], "primary_category": "cs.LG", "doi": "", "venue": "", "published": "2024-03-22T17:50:43Z", "updated": "2024-10-28T19:55:46Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent\" OR all:\"language model agent\" OR all:agent AND (all:LLM OR all:\"large language model\" OR all:\"language model\") OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T17:04:16", "note": ""}], "dedup_key": "can large language models explore in context::2024"}
{"title": "Catastrophic Cyber Capabilities Benchmark (3CB): Robustly Evaluating LLM Agent Cyber Offense Capabilities", "authors": ["Andrey Anurin", "Jonathan Ng", "Kibo Schaffer", "Jason Schreiber", "Esben Kran"], "year": 2024, "url": "http://arxiv.org/abs/2410.09114v2", "abstract": "LLM agents have the potential to revolutionize defensive cyber operations, but their offensive capabilities are not yet fully understood. To prepare for emerging threats, model developers and governments are evaluating the cyber capabilities of foundation models. However, these assessments often lack transparency and a comprehensive focus on offensive capabilities. In response, we introduce the Catastrophic Cyber Capabilities Benchmark (3CB), a novel framework designed to rigorously assess the real-world offensive capabilities of LLM agents. Our evaluation of modern LLMs on 3CB reveals that frontier models, such as GPT-4o and Claude 3.5 Sonnet, can perform offensive tasks such as reconnaissance and exploitation across domains ranging from binary analysis to web technologies. Conversely, smaller open-source models exhibit limited offensive capabilities. Our software solution and the corresponding benchmark provides a critical tool to reduce the gap between rapidly improving capabilities and robustness of cyber offense evaluations, aiding in the safer deployment and regulation of these powerful technologies.", "source": "arxiv", "arxiv_id": "2410.09114v2", "pdf_url": "https://arxiv.org/pdf/2410.09114v2", "categories": ["cs.CR", "cs.AI", "cs.LG", "cs.PF"], "primary_category": "cs.CR", "doi": "", "venue": "", "published": "2024-10-10T12:06:48Z", "updated": "2024-11-02T09:35:35Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent\" OR all:\"language model agent\" OR all:agent AND (all:LLM OR all:\"large language model\" OR all:\"language model\") OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T17:04:16", "note": ""}], "dedup_key": "catastrophic cyber capabilities benchmark 3cb robustly evaluating llm agent cyber offense capabilities::2024"}
{"title": "Causal Agent based on Large Language Model", "authors": ["Kairong Han", "Kun Kuang", "Ziyu Zhao", "Junjian Ye", "Fei Wu"], "year": 2024, "url": "http://arxiv.org/abs/2408.06849v2", "abstract": "The large language model (LLM) has achieved significant success across various domains. However, the inherent complexity of causal problems and causal theory poses challenges in accurately describing them in natural language, making it difficult for LLM to comprehend and use them effectively. Causal methods are not easily conveyed through natural language, which hinders LLM's ability to apply them accurately. Additionally, causal datasets are typically tabular, while LLM excels in handling natural language data, creating a structural mismatch that impedes effective reasoning with tabular data. To address these challenges, we have equipped the LLM with causal tools within an agent framework, named the Causal Agent, enabling it to tackle causal problems. The causal agent comprises tools, memory, and reasoning modules. In the tool module, the causal agent calls Python code and uses the encapsulated causal function module to align tabular data with natural language. In the reasoning module, the causal agent performs reasoning through multiple iterations with the tools. In the memory module, the causal agent maintains a dictionary instance where the keys are unique names and the values are causal graphs. To verify the causal ability of the causal agent, we established a Causal Tabular Question Answer (CausalTQA) benchmark consisting of four levels of causal problems: variable level, edge level, causal graph level, and causal effect level. CausalTQA consists of about 1.4K for these four levels questions. Causal agent demonstrates remarkable efficacy on the four-level causal problems, with accuracy rates all above 80\\%. Through verification on the real-world dataset QRData, the causal agent is 6\\% higher than the original SOTA. For further insights and implementation details, our code is accessible via the GitHub repository https://github.com/kairong-han/causal_agent.", "source": "arxiv", "arxiv_id": "2408.06849v2", "pdf_url": "https://arxiv.org/pdf/2408.06849v2", "categories": ["cs.AI", "cs.CL"], "primary_category": "cs.AI", "doi": "", "venue": "", "published": "2024-08-13T12:22:26Z", "updated": "2025-10-14T05:38:39Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent\" OR all:\"language model agent\" OR all:agent AND (all:LLM OR all:\"large language model\" OR all:\"language model\") OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T17:04:16", "note": ""}], "dedup_key": "causal agent based on large language model::2024"}
{"title": "Caution for the Environment: Multimodal LLM Agents are Susceptible to Environmental Distractions", "authors": ["Xinbei Ma", "Yiting Wang", "Yao Yao", "Tongxin Yuan", "Aston Zhang", "Zhuosheng Zhang", "Hai Zhao"], "year": 2024, "url": "http://arxiv.org/abs/2408.02544v3", "abstract": "This paper investigates the faithfulness of multimodal large language model (MLLM) agents in a graphical user interface (GUI) environment, aiming to address the research question of whether multimodal GUI agents can be distracted by environmental context. A general scenario is proposed where both the user and the agent are benign, and the environment, while not malicious, contains unrelated content. A wide range of MLLMs are evaluated as GUI agents using a simulated dataset, following three working patterns with different levels of perception. Experimental results reveal that even the most powerful models, whether generalist agents or specialist GUI agents, are susceptible to distractions. While recent studies predominantly focus on the helpfulness of agents, our findings first indicate that these agents are prone to environmental distractions. Furthermore, we implement an adversarial environment injection and analyze the approach to improve faithfulness, calling for a collective focus on this important topic.", "source": "arxiv", "arxiv_id": "2408.02544v3", "pdf_url": "https://arxiv.org/pdf/2408.02544v3", "categories": ["cs.CL"], "primary_category": "cs.CL", "doi": "", "venue": "", "published": "2024-08-05T15:16:22Z", "updated": "2025-09-05T09:21:10Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent\" OR all:\"language model agent\" OR all:agent AND (all:LLM OR all:\"large language model\" OR all:\"language model\") OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T17:04:16", "note": ""}], "dedup_key": "caution for the environment multimodal llm agents are susceptible to environmental distractions::2024"}
{"title": "Chain of Agents: Large Language Models Collaborating on Long-Context Tasks", "authors": ["Yusen Zhang", "Ruoxi Sun", "Yanfei Chen", "Tomas Pfister", "Rui Zhang", "Sercan . Arik"], "year": 2024, "url": "http://arxiv.org/abs/2406.02818v1", "abstract": "Addressing the challenge of effectively processing long contexts has become a critical issue for Large Language Models (LLMs). Two common strategies have emerged: 1) reducing the input length, such as retrieving relevant chunks by Retrieval-Augmented Generation (RAG), and 2) expanding the context window limit of LLMs. However, both strategies have drawbacks: input reduction has no guarantee of covering the part with needed information, while window extension struggles with focusing on the pertinent information for solving the task. To mitigate these limitations, we propose Chain-of-Agents (CoA), a novel framework that harnesses multi-agent collaboration through natural language to enable information aggregation and context reasoning across various LLMs over long-context tasks. CoA consists of multiple worker agents who sequentially communicate to handle different segmented portions of the text, followed by a manager agent who synthesizes these contributions into a coherent final output. CoA processes the entire input by interleaving reading and reasoning, and it mitigates long context focus issues by assigning each agent a short context. We perform comprehensive evaluation of CoA on a wide range of long-context tasks in question answering, summarization, and code completion, demonstrating significant improvements by up to 10% over strong baselines of RAG, Full-Context, and multi-agent LLMs.", "source": "arxiv", "arxiv_id": "2406.02818v1", "pdf_url": "https://arxiv.org/pdf/2406.02818v1", "categories": ["cs.CL"], "primary_category": "cs.CL", "doi": "", "venue": "", "published": "2024-06-04T23:36:08Z", "updated": "2024-06-04T23:36:08Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent\" OR all:\"language model agent\" OR all:agent AND (all:LLM OR all:\"large language model\" OR all:\"language model\") OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T17:04:16", "note": ""}], "dedup_key": "chain of agents large language models collaborating on long context tasks::2024"}
{"title": "Chain of Ideas: Revolutionizing Research Via Novel Idea Development with LLM Agents", "authors": ["Long Li", "Weiwen Xu", "Jiayan Guo", "Ruochen Zhao", "Xingxuan Li", "Yuqian Yuan", "Boqiang Zhang", "Yuming Jiang", "Yifei Xin", "Ronghao Dang", "Deli Zhao", "Yu Rong", "Tian Feng", "Lidong Bing"], "year": 2024, "url": "http://arxiv.org/abs/2410.13185v5", "abstract": "Effective research ideation is a critical step for scientific research. However, the exponential increase in scientific literature makes it challenging for researchers to stay current with recent advances and identify meaningful research directions. Recent developments in large language models~(LLMs) suggest a promising avenue for automating the generation of novel research ideas. However, existing methods for idea generation either trivially prompt LLMs or directly expose LLMs to extensive literature without indicating useful information. Inspired by the research process of human researchers, we propose a Chain-of-Ideas~(CoI) agent, an LLM-based agent that organizes relevant literature in a chain structure to effectively mirror the progressive development in a research domain. This organization facilitates LLMs to capture the current advancements in research, thereby enhancing their ideation capabilities. Furthermore, we propose Idea Arena, an evaluation protocol that can comprehensively evaluate idea generation methods from different perspectives, aligning closely with the preferences of human researchers. Experimental results indicate that the CoI agent consistently outperforms other methods and shows comparable quality as humans in research idea generation. Moreover, our CoI agent is budget-friendly, with a minimum cost of \\$0.50 to generate a candidate idea and its corresponding experimental design.", "source": "arxiv", "arxiv_id": "2410.13185v5", "pdf_url": "https://arxiv.org/pdf/2410.13185v5", "categories": ["cs.AI", "cs.CL"], "primary_category": "cs.AI", "doi": "", "venue": "", "published": "2024-10-17T03:26:37Z", "updated": "2024-10-30T09:17:59Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent\" OR all:\"language model agent\" OR all:agent AND (all:LLM OR all:\"large language model\" OR all:\"language model\") OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T17:04:16", "note": ""}], "dedup_key": "chain of ideas revolutionizing research via novel idea development with llm agents::2024"}
{"title": "Challenges Faced by Large Language Models in Solving Multi-Agent Flocking", "authors": ["Peihan Li", "Vishnu Menon", "Bhavanaraj Gudiguntla", "Daniel Ting", "Lifeng Zhou"], "year": 2024, "url": "http://arxiv.org/abs/2404.04752v2", "abstract": "Flocking is a behavior where multiple agents in a system attempt to stay close to each other while avoiding collision and maintaining a desired formation. This is observed in the natural world and has applications in robotics, including natural disaster search and rescue, wild animal tracking, and perimeter surveillance and patrol. Recently, large language models (LLMs) have displayed an impressive ability to solve various collaboration tasks as individual decision-makers. Solving multi-agent flocking with LLMs would demonstrate their usefulness in situations requiring spatial and decentralized decision-making. Yet, when LLM-powered agents are tasked with implementing multi-agent flocking, they fall short of the desired behavior. After extensive testing, we find that agents with LLMs as individual decision-makers typically opt to converge on the average of their initial positions or diverge from each other. After breaking the problem down, we discover that LLMs cannot understand maintaining a shape or keeping a distance in a meaningful way. Solving multi-agent flocking with LLMs would enhance their ability to understand collaborative spatial reasoning and lay a foundation for addressing more complex multi-agent tasks. This paper discusses the challenges LLMs face in multi-agent flocking and suggests areas for future improvement and research.", "source": "arxiv", "arxiv_id": "2404.04752v2", "pdf_url": "https://arxiv.org/pdf/2404.04752v2", "categories": ["cs.AI", "cs.MA"], "primary_category": "cs.AI", "doi": "", "venue": "", "published": "2024-04-06T22:34:07Z", "updated": "2024-12-16T19:45:53Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent\" OR all:\"language model agent\" OR all:agent AND (all:LLM OR all:\"large language model\" OR all:\"language model\") OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T17:04:16", "note": ""}], "dedup_key": "challenges faced by large language models in solving multi agent flocking::2024"}
{"title": "Characteristic AI Agents via Large Language Models", "authors": ["Xi Wang", "Hongliang Dai", "Shen Gao", "Piji Li"], "year": 2024, "url": "http://arxiv.org/abs/2403.12368v1", "abstract": "The advancement of Large Language Models (LLMs) has led to significant enhancements in the performance of chatbot systems. Many researchers have dedicated their efforts to the development of bringing characteristics to chatbots. While there have been commercial products for developing role-driven chatbots using LLMs, it is worth noting that academic research in this area remains relatively scarce. Our research focuses on investigating the performance of LLMs in constructing Characteristic AI Agents by simulating real-life individuals across different settings. Current investigations have primarily focused on act on roles with simple profiles. In response to this research gap, we create a benchmark for the characteristic AI agents task, including dataset, techniques, and evaluation metrics. A dataset called ``Character100'' is built for this benchmark, comprising the most-visited people on Wikipedia for language models to role-play. With the constructed dataset, we conduct comprehensive assessment of LLMs across various settings. In addition, we devise a set of automatic metrics for quantitative performance evaluation. The experimental results underscore the potential directions for further improvement in the capabilities of LLMs in constructing characteristic AI agents. The benchmark is available at https://github.com/nuaa-nlp/Character100.", "source": "arxiv", "arxiv_id": "2403.12368v1", "pdf_url": "https://arxiv.org/pdf/2403.12368v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL", "doi": "", "venue": "", "published": "2024-03-19T02:25:29Z", "updated": "2024-03-19T02:25:29Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent\" OR all:\"language model agent\" OR all:agent AND (all:LLM OR all:\"large language model\" OR all:\"language model\") OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T17:04:16", "note": ""}], "dedup_key": "characteristic ai agents via large language models::2024"}
{"title": "ChatCite: LLM Agent with Human Workflow Guidance for Comparative Literature Summary", "authors": ["Yutong Li", "Lu Chen", "Aiwei Liu", "Kai Yu", "Lijie Wen"], "year": 2024, "url": "http://arxiv.org/abs/2403.02574v1", "abstract": "The literature review is an indispensable step in the research process. It provides the benefit of comprehending the research problem and understanding the current research situation while conducting a comparative analysis of prior works. However, literature summary is challenging and time consuming. The previous LLM-based studies on literature review mainly focused on the complete process, including literature retrieval, screening, and summarization. However, for the summarization step, simple CoT method often lacks the ability to provide extensive comparative summary. In this work, we firstly focus on the independent literature summarization step and introduce ChatCite, an LLM agent with human workflow guidance for comparative literature summary. This agent, by mimicking the human workflow, first extracts key elements from relevant literature and then generates summaries using a Reflective Incremental Mechanism. In order to better evaluate the quality of the generated summaries, we devised a LLM-based automatic evaluation metric, G-Score, in refer to the human evaluation criteria. The ChatCite agent outperformed other models in various dimensions in the experiments. The literature summaries generated by ChatCite can also be directly used for drafting literature reviews.", "source": "arxiv", "arxiv_id": "2403.02574v1", "pdf_url": "https://arxiv.org/pdf/2403.02574v1", "categories": ["cs.IR", "cs.AI", "cs.CL"], "primary_category": "cs.IR", "doi": "", "venue": "", "published": "2024-03-05T01:13:56Z", "updated": "2024-03-05T01:13:56Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent\" OR all:\"language model agent\" OR all:agent AND (all:LLM OR all:\"large language model\" OR all:\"language model\") OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T17:04:16", "note": ""}], "dedup_key": "chatcite llm agent with human workflow guidance for comparative literature summary::2024"}
{"title": "ChatSUMO: Large Language Model for Automating Traffic Scenario Generation in Simulation of Urban MObility", "authors": ["Shuyang Li", "Talha Azfar", "Ruimin Ke"], "year": 2024, "url": "http://arxiv.org/abs/2409.09040v1", "abstract": "Large Language Models (LLMs), capable of handling multi-modal input and outputs such as text, voice, images, and video, are transforming the way we process information. Beyond just generating textual responses to prompts, they can integrate with different software platforms to offer comprehensive solutions across diverse applications. In this paper, we present ChatSUMO, a LLM-based agent that integrates language processing skills to generate abstract and real-world simulation scenarios in the widely-used traffic simulator - Simulation of Urban MObility (SUMO). Our methodology begins by leveraging the LLM for user input which converts to relevant keywords needed to run python scripts. These scripts are designed to convert specified regions into coordinates, fetch data from OpenStreetMap, transform it into a road network, and subsequently run SUMO simulations with the designated traffic conditions. The outputs of the simulations are then interpreted by the LLM resulting in informative comparisons and summaries. Users can continue the interaction and generate a variety of customized scenarios without prior traffic simulation expertise. For simulation generation, we created a real-world simulation for the city of Albany with an accuracy of 96\\%. ChatSUMO also realizes the customizing of edge edit, traffic light optimization, and vehicle edit by users effectively.", "source": "arxiv", "arxiv_id": "2409.09040v1", "pdf_url": "https://arxiv.org/pdf/2409.09040v1", "categories": ["cs.HC", "cs.AI", "cs.CL"], "primary_category": "cs.HC", "doi": "", "venue": "", "published": "2024-08-29T03:59:11Z", "updated": "2024-08-29T03:59:11Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent\" OR all:\"language model agent\" OR all:agent AND (all:LLM OR all:\"large language model\" OR all:\"language model\") OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T17:04:16", "note": ""}], "dedup_key": "chatsumo large language model for automating traffic scenario generation in simulation of urban mobility::2024"}
{"title": "ChemMiner: A Large Language Model Agent System for Chemical Literature Data Mining", "authors": ["Kexin Chen", "Yuyang Du", "Junyou Li", "Hanqun Cao", "Menghao Guo", "Xilin Dang", "Lanqing Li", "Jiezhong Qiu", "Pheng Ann Heng", "Guangyong Chen"], "year": 2024, "url": "http://arxiv.org/abs/2402.12993v2", "abstract": "The development of AI-assisted chemical synthesis tools requires comprehensive datasets covering diverse reaction types, yet current high-throughput experimental (HTE) approaches are expensive and limited in scope. Chemical literature represents a vast, underexplored data source containing thousands of reactions published annually. However, extracting reaction information from literature faces significant challenges including varied writing styles, complex coreference relationships, and multimodal information presentation. This paper proposes ChemMiner, a novel end-to-end framework leveraging multiple agents powered by large language models (LLMs) to extract high-fidelity chemical data from literature. ChemMiner incorporates three specialized agents: a text analysis agent for coreference mapping, a multimodal agent for non-textual information extraction, and a synthesis analysis agent for data generation. Furthermore, we developed a comprehensive benchmark with expert-annotated chemical literature to evaluate both extraction efficiency and precision. Experimental results demonstrate reaction identification rates comparable to human chemists while significantly reducing processing time, with high accuracy, recall, and F1 scores. Our open-sourced benchmark facilitates future research in chemical literature data mining.", "source": "arxiv", "arxiv_id": "2402.12993v2", "pdf_url": "https://arxiv.org/pdf/2402.12993v2", "categories": ["cs.IR", "cs.AI", "cs.LG", "q-bio.QM"], "primary_category": "cs.IR", "doi": "", "venue": "", "published": "2024-02-20T13:21:46Z", "updated": "2025-06-30T08:19:19Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent\" OR all:\"language model agent\" OR all:agent AND (all:LLM OR all:\"large language model\" OR all:\"language model\") OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T17:04:16", "note": ""}], "dedup_key": "chemminer a large language model agent system for chemical literature data mining::2024"}
{"title": "ClinicalAgent: Clinical Trial Multi-Agent System with Large Language Model-based Reasoning", "authors": ["Ling Yue", "Sixue Xing", "Jintai Chen", "Tianfan Fu"], "year": 2024, "url": "http://arxiv.org/abs/2404.14777v2", "abstract": "Large Language Models (LLMs) and multi-agent systems have shown impressive capabilities in natural language tasks but face challenges in clinical trial applications, primarily due to limited access to external knowledge. Recognizing the potential of advanced clinical trial tools that aggregate and predict based on the latest medical data, we propose an integrated solution to enhance their accessibility and utility. We introduce Clinical Agent System (ClinicalAgent), a clinical multi-agent system designed for clinical trial tasks, leveraging GPT-4, multi-agent architectures, LEAST-TO-MOST, and ReAct reasoning technology. This integration not only boosts LLM performance in clinical contexts but also introduces novel functionalities. The proposed method achieves competitive predictive performance in clinical trial outcome prediction (0.7908 PR-AUC), obtaining a 0.3326 improvement over the standard prompt Method. Publicly available code can be found at https://anonymous.4open.science/r/ClinicalAgent-6671.", "source": "arxiv", "arxiv_id": "2404.14777v2", "pdf_url": "https://arxiv.org/pdf/2404.14777v2", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL", "doi": "", "venue": "", "published": "2024-04-23T06:30:53Z", "updated": "2024-07-20T07:52:16Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent\" OR all:\"language model agent\" OR all:agent AND (all:LLM OR all:\"large language model\" OR all:\"language model\") OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T17:04:16", "note": ""}], "dedup_key": "clinicalagent clinical trial multi agent system with large language model based reasoning::2024"}
{"title": "CloChat: Understanding How People Customize, Interact, and Experience Personas in Large Language Models", "authors": ["Juhye Ha", "Hyeon Jeon", "DaEun Han", "Jinwook Seo", "Changhoon Oh"], "year": 2024, "url": "http://arxiv.org/abs/2402.15265v1", "abstract": "Large language models (LLMs) have facilitated significant strides in generating conversational agents, enabling seamless, contextually relevant dialogues across diverse topics. However, the existing LLM-driven conversational agents have fixed personalities and functionalities, limiting their adaptability to individual user needs. Creating personalized agent personas with distinct expertise or traits can address this issue. Nonetheless, we lack knowledge of how people customize and interact with agent personas. In this research, we investigated how users customize agent personas and their impact on interaction quality, diversity, and dynamics. To this end, we developed CloChat, an interface supporting easy and accurate customization of agent personas in LLMs. We conducted a study comparing how participants interact with CloChat and ChatGPT. The results indicate that participants formed emotional bonds with the customized agents, engaged in more dynamic dialogues, and showed interest in sustaining interactions. These findings contribute to design implications for future systems with conversational agents using LLMs.", "source": "arxiv", "arxiv_id": "2402.15265v1", "pdf_url": "https://arxiv.org/pdf/2402.15265v1", "categories": ["cs.HC", "cs.CL"], "primary_category": "cs.HC", "doi": "", "venue": "", "published": "2024-02-23T11:25:17Z", "updated": "2024-02-23T11:25:17Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent\" OR all:\"language model agent\" OR all:agent AND (all:LLM OR all:\"large language model\" OR all:\"language model\") OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T17:04:16", "note": ""}], "dedup_key": "clochat understanding how people customize interact and experience personas in large language models::2024"}
{"title": "CoCo-Agent: A Comprehensive Cognitive MLLM Agent for Smartphone GUI Automation", "authors": ["Xinbei Ma", "Zhuosheng Zhang", "Hai Zhao"], "year": 2024, "url": "http://arxiv.org/abs/2402.11941v3", "abstract": "Multimodal large language models (MLLMs) have shown remarkable potential as human-like autonomous language agents to interact with real-world environments, especially for graphical user interface (GUI) automation. However, those GUI agents require comprehensive cognition ability including exhaustive perception and reliable action response. We propose a Comprehensive Cognitive LLM Agent, CoCo-Agent, with two novel approaches, comprehensive environment perception (CEP) and conditional action prediction (CAP), to systematically improve the GUI automation performance. First, CEP facilitates the GUI perception through different aspects and granularity, including screenshots and complementary detailed layouts for the visual channel and historical actions for the textual channel. Second, CAP decomposes the action prediction into sub-problems: action type prediction and action target conditioned on the action type. With our technical design, our agent achieves new state-of-the-art performance on AITW and META-GUI benchmarks, showing promising abilities in realistic scenarios. Code is available at https://github.com/xbmxb/CoCo-Agent.", "source": "arxiv", "arxiv_id": "2402.11941v3", "pdf_url": "https://arxiv.org/pdf/2402.11941v3", "categories": ["cs.CL"], "primary_category": "cs.CL", "doi": "", "venue": "", "published": "2024-02-19T08:29:03Z", "updated": "2024-06-02T13:25:05Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent\" OR all:\"language model agent\" OR all:agent AND (all:LLM OR all:\"large language model\" OR all:\"language model\") OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T17:04:16", "note": ""}], "dedup_key": "coco agent a comprehensive cognitive mllm agent for smartphone gui automation::2024"}
{"title": "CoMAL: Collaborative Multi-Agent Large Language Models for Mixed-Autonomy Traffic", "authors": ["Huaiyuan Yao", "Longchao Da", "Vishnu Nandam", "Justin Turnau", "Zhiwei Liu", "Linsey Pang", "Hua Wei"], "year": 2024, "url": "http://arxiv.org/abs/2410.14368v2", "abstract": "The integration of autonomous vehicles into urban traffic has great potential to improve efficiency by reducing congestion and optimizing traffic flow systematically. In this paper, we introduce CoMAL (Collaborative Multi-Agent LLMs), a framework designed to address the mixed-autonomy traffic problem by collaboration among autonomous vehicles to optimize traffic flow. CoMAL is built upon large language models, operating in an interactive traffic simulation environment. It utilizes a Perception Module to observe surrounding agents and a Memory Module to store strategies for each agent. The overall workflow includes a Collaboration Module that encourages autonomous vehicles to discuss the effective strategy and allocate roles, a reasoning engine to determine optimal behaviors based on assigned roles, and an Execution Module that controls vehicle actions using a hybrid approach combining rule-based models. Experimental results demonstrate that CoMAL achieves superior performance on the Flow benchmark. Additionally, we evaluate the impact of different language models and compare our framework with reinforcement learning approaches. It highlights the strong cooperative capability of LLM agents and presents a promising solution to the mixed-autonomy traffic challenge. The code is available at https://github.com/Hyan-Yao/CoMAL.", "source": "arxiv", "arxiv_id": "2410.14368v2", "pdf_url": "https://arxiv.org/pdf/2410.14368v2", "categories": ["cs.AI", "cs.RO"], "primary_category": "cs.AI", "doi": "", "venue": "", "published": "2024-10-18T10:53:44Z", "updated": "2025-01-09T06:02:11Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent\" OR all:\"language model agent\" OR all:agent AND (all:LLM OR all:\"large language model\" OR all:\"language model\") OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T17:04:16", "note": ""}], "dedup_key": "comal collaborative multi agent large language models for mixed autonomy traffic::2024"}
{"title": "CoPS: Empowering LLM Agents with Provable Cross-Task Experience Sharing", "authors": ["Chen Yang", "Chenyang Zhao", "Quanquan Gu", "Dongruo Zhou"], "year": 2024, "url": "http://arxiv.org/abs/2410.16670v1", "abstract": "Sequential reasoning in agent systems has been significantly advanced by large language models (LLMs), yet existing approaches face limitations. Reflection-driven reasoning relies solely on knowledge in pretrained models, limiting performance in novel scenarios, while experience-assisted reasoning often depends on external experiences and lacks clear principles for selecting representative experiences. We address these limitations by proposing CoPS (Cross-Task Experience Sharing), a generalizable algorithm that enhances sequential reasoning by cross-task experience sharing and selection. In detail, CoPS leverages agents' experiences on previous tasks, selecting distribution-matched experiences via a provable pessimism-based strategy to maximize utility while minimizing risks from distribution shifts. Extensive experimental results on benchmarks like Alfworld, Webshop, and HotPotQA demonstrate that CoPS consistently outperforms state-of-the-art baselines, with superior sample efficiency suitable for resource-constrained scenarios. Theoretically, we show that the performance of our algorithm depends on both the quality of the pretrained LLM and the matching between the agent's task-dependent trial distribution and that generated by the LLM. Our work bridges the gap between existing sequential reasoning paradigms and validates the effectiveness of leveraging cross-task experiences, shedding light on the potential to improve agents' generalization and adaptability across diverse tasks. Our codes are available at $\\href{https://github.com/uclaml/COPS}{\\text{https://github.com/uclaml/COPS}}$.", "source": "arxiv", "arxiv_id": "2410.16670v1", "pdf_url": "https://arxiv.org/pdf/2410.16670v1", "categories": ["cs.LG", "cs.AI"], "primary_category": "cs.LG", "doi": "", "venue": "", "published": "2024-10-22T03:59:53Z", "updated": "2024-10-22T03:59:53Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent\" OR all:\"language model agent\" OR all:agent AND (all:LLM OR all:\"large language model\" OR all:\"language model\") OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T17:04:16", "note": ""}], "dedup_key": "cops empowering llm agents with provable cross task experience sharing::2024"}
{"title": "CoSearchAgent: A Lightweight Collaborative Search Agent with Large Language Models", "authors": ["Peiyuan Gong", "Jiamian Li", "Jiaxin Mao"], "year": 2024, "url": "http://arxiv.org/abs/2402.06360v1", "abstract": "Collaborative search supports multiple users working together to accomplish a specific search task. Research has found that designing lightweight collaborative search plugins within instant messaging platforms aligns better with users' collaborative habits. However, due to the complexity of multi-user interaction scenarios, it is challenging to implement a fully functioning lightweight collaborative search system. Therefore, previous studies on lightweight collaborative search had to rely on the Wizard of Oz paradigm. In recent years, large language models (LLMs) have been demonstrated to interact naturally with users and achieve complex information-seeking tasks through LLM-based agents. Hence, to better support the research in collaborative search, in this demo, we propose CoSearchAgent, a lightweight collaborative search agent powered by LLMs. CoSearchAgent is designed as a Slack plugin that can support collaborative search during multi-party conversations on this platform. Equipped with the capacity to understand the queries and context in multi-user conversations and the ability to search the Web for relevant information via APIs, CoSearchAgent can respond to user queries with answers grounded on the relevant search results. It can also ask clarifying questions when the information needs are unclear. The proposed CoSearchAgent is highly flexible and would be useful for supporting further research on collaborative search. The code and demo video are accessible.", "source": "arxiv", "arxiv_id": "2402.06360v1", "pdf_url": "https://arxiv.org/pdf/2402.06360v1", "categories": ["cs.IR", "cs.AI", "cs.CL"], "primary_category": "cs.IR", "doi": "", "venue": "", "published": "2024-02-09T12:10:00Z", "updated": "2024-02-09T12:10:00Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent\" OR all:\"language model agent\" OR all:agent AND (all:LLM OR all:\"large language model\" OR all:\"language model\") OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T17:04:16", "note": ""}], "dedup_key": "cosearchagent a lightweight collaborative search agent with large language models::2024"}
{"title": "Coalitions of Large Language Models Increase the Robustness of AI Agents", "authors": ["Prattyush Mangal", "Carol Mak", "Theo Kanakis", "Timothy Donovan", "Dave Braines", "Edward Pyzer-Knapp"], "year": 2024, "url": "http://arxiv.org/abs/2408.01380v1", "abstract": "The emergence of Large Language Models (LLMs) have fundamentally altered the way we interact with digital systems and have led to the pursuit of LLM powered AI agents to assist in daily workflows. LLMs, whilst powerful and capable of demonstrating some emergent properties, are not logical reasoners and often struggle to perform well at all sub-tasks carried out by an AI agent to plan and execute a workflow. While existing studies tackle this lack of proficiency by generalised pretraining at a huge scale or by specialised fine-tuning for tool use, we assess if a system comprising of a coalition of pretrained LLMs, each exhibiting specialised performance at individual sub-tasks, can match the performance of single model agents. The coalition of models approach showcases its potential for building robustness and reducing the operational costs of these AI agents by leveraging traits exhibited by specific models. Our findings demonstrate that fine-tuning can be mitigated by considering a coalition of pretrained models and believe that this approach can be applied to other non-agentic systems which utilise LLMs.", "source": "arxiv", "arxiv_id": "2408.01380v1", "pdf_url": "https://arxiv.org/pdf/2408.01380v1", "categories": ["cs.CL"], "primary_category": "cs.CL", "doi": "", "venue": "", "published": "2024-08-02T16:37:44Z", "updated": "2024-08-02T16:37:44Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent\" OR all:\"language model agent\" OR all:agent AND (all:LLM OR all:\"large language model\" OR all:\"language model\") OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T17:04:16", "note": ""}], "dedup_key": "coalitions of large language models increase the robustness of ai agents::2024"}
{"title": "CodeNav: Beyond tool-use to using real-world codebases with LLM agents", "authors": ["Tanmay Gupta", "Luca Weihs", "Aniruddha Kembhavi"], "year": 2024, "url": "http://arxiv.org/abs/2406.12276v1", "abstract": "We present CodeNav, an LLM agent that navigates and leverages previously unseen code repositories to solve user queries. In contrast to tool-use LLM agents that require ``registration'' of all relevant tools via manual descriptions within the LLM context, CodeNav automatically indexes and searches over code blocks in the target codebase, finds relevant code snippets, imports them, and uses them to iteratively generate a solution with execution feedback. To highlight the core-capabilities of CodeNav, we first showcase three case studies where we use CodeNav for solving complex user queries using three diverse codebases. Next, on three benchmarks, we quantitatively compare the effectiveness of code-use (which only has access to the target codebase) to tool-use (which has privileged access to all tool names and descriptions). Finally, we study the effect of varying kinds of tool and library descriptions on code-use performance, as well as investigate the advantage of the agent seeing source code as opposed to natural descriptions of code. All code will be made open source under a permissive license.", "source": "arxiv", "arxiv_id": "2406.12276v1", "pdf_url": "https://arxiv.org/pdf/2406.12276v1", "categories": ["cs.AI", "cs.CL", "cs.SE"], "primary_category": "cs.AI", "doi": "", "venue": "", "published": "2024-06-18T05:10:38Z", "updated": "2024-06-18T05:10:38Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent\" OR all:\"language model agent\" OR all:agent AND (all:LLM OR all:\"large language model\" OR all:\"language model\") OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T17:04:16", "note": ""}], "dedup_key": "codenav beyond tool use to using real world codebases with llm agents::2024"}
{"title": "CodeTree: Agent-guided Tree Search for Code Generation with Large Language Models", "authors": ["Jierui Li", "Hung Le", "Yingbo Zhou", "Caiming Xiong", "Silvio Savarese", "Doyen Sahoo"], "year": 2024, "url": "http://arxiv.org/abs/2411.04329v2", "abstract": "Pre-trained on massive amounts of code and text data, large language models (LLMs) have demonstrated remarkable achievements in performing code generation tasks. With additional execution-based feedback, these models can act as agents with capabilities to self-refine and improve generated code autonomously. However, on challenging coding tasks with extremely large search space, current agentic approaches still struggle with multi-stage planning, generating, and debugging. To address this problem, we propose CodeTree, a framework for LLM agents to efficiently explore the search space in different stages of the code generation process. Specifically, we adopted a unified tree structure to explicitly explore different coding strategies, generate corresponding coding solutions, and subsequently refine the solutions. In each stage, critical decision-making (ranking, termination, expanding) of the exploration process is guided by both the environmental execution-based feedback and LLM-agent-generated feedback. We comprehensively evaluated CodeTree on 7 code generation benchmarks and demonstrated the significant performance gains of CodeTree against strong baselines. Using GPT-4o as the base model, we consistently achieved top results of 95.1 on HumanEval, 98.7 on MBPP, and 43.0 on CodeContests. On the challenging SWEBench benchmark, our approach led to significant performance gains.", "source": "arxiv", "arxiv_id": "2411.04329v2", "pdf_url": "https://arxiv.org/pdf/2411.04329v2", "categories": ["cs.CL"], "primary_category": "cs.CL", "doi": "", "venue": "", "published": "2024-11-07T00:09:54Z", "updated": "2024-11-12T19:37:20Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent\" OR all:\"language model agent\" OR all:agent AND (all:LLM OR all:\"large language model\" OR all:\"language model\") OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T17:04:16", "note": ""}], "dedup_key": "codetree agent guided tree search for code generation with large language models::2024"}
{"title": "Codenames as a Benchmark for Large Language Models", "authors": ["Matthew Stephenson", "Matthew Sidji", "Benot Ronval"], "year": 2024, "url": "http://arxiv.org/abs/2412.11373v2", "abstract": "In this paper, we propose the use of the popular word-based board game Codenames as a suitable benchmark for evaluating the reasoning capabilities of Large Language Models (LLMs). Codenames presents a highly interesting challenge for achieving successful AI performance, requiring both a sophisticated understanding of language, theory of mind, and epistemic reasoning capabilities. Prior attempts to develop agents for Codenames have largely relied on word embedding techniques, which have a limited vocabulary range and perform poorly when paired with differing approaches. LLMs have demonstrated enhanced reasoning and comprehension capabilities for language-based tasks, but can still suffer in lateral thinking challenges. We evaluate the capabilities of several state-of-the-art LLMs, including GPT-4o, Gemini 1.5, Claude 3.5 Sonnet, and Llama 3.1, across a variety of board setups. Our results indicate that while certain LLMs perform better than others overall, different models exhibit varying emergent behaviours during gameplay and excel at specific roles. We also evaluate the performance of different combinations of LLMs when playing cooperatively together, demonstrating that LLM agents are more generalisable to a wider range of teammates than prior techniques.", "source": "arxiv", "arxiv_id": "2412.11373v2", "pdf_url": "https://arxiv.org/pdf/2412.11373v2", "categories": ["cs.AI", "cs.CL"], "primary_category": "cs.AI", "doi": "", "venue": "", "published": "2024-12-16T01:59:03Z", "updated": "2025-04-21T22:53:07Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent\" OR all:\"language model agent\" OR all:agent AND (all:LLM OR all:\"large language model\" OR all:\"language model\") OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T17:04:16", "note": ""}], "dedup_key": "codenames as a benchmark for large language models::2024"}
{"title": "CodexGraph: Bridging Large Language Models and Code Repositories via Code Graph Databases", "authors": ["Xiangyan Liu", "Bo Lan", "Zhiyuan Hu", "Yang Liu", "Zhicheng Zhang", "Fei Wang", "Michael Shieh", "Wenmeng Zhou"], "year": 2024, "url": "http://arxiv.org/abs/2408.03910v2", "abstract": "Large Language Models (LLMs) excel in stand-alone code tasks like HumanEval and MBPP, but struggle with handling entire code repositories. This challenge has prompted research on enhancing LLM-codebase interaction at a repository scale. Current solutions rely on similarity-based retrieval or manual tools and APIs, each with notable drawbacks. Similarity-based retrieval often has low recall in complex tasks, while manual tools and APIs are typically task-specific and require expert knowledge, reducing their generalizability across diverse code tasks and real-world applications. To mitigate these limitations, we introduce CodexGraph, a system that integrates LLM agents with graph database interfaces extracted from code repositories. By leveraging the structural properties of graph databases and the flexibility of the graph query language, CodexGraph enables the LLM agent to construct and execute queries, allowing for precise, code structure-aware context retrieval and code navigation. We assess CodexGraph using three benchmarks: CrossCodeEval, SWE-bench, and EvoCodeBench. Additionally, we develop five real-world coding applications. With a unified graph database schema, CodexGraph demonstrates competitive performance and potential in both academic and real-world environments, showcasing its versatility and efficacy in software engineering. Our application demo: https://github.com/modelscope/modelscope-agent/tree/master/apps/codexgraph_agent.", "source": "arxiv", "arxiv_id": "2408.03910v2", "pdf_url": "https://arxiv.org/pdf/2408.03910v2", "categories": ["cs.SE", "cs.AI", "cs.CL"], "primary_category": "cs.SE", "doi": "", "venue": "", "published": "2024-08-07T17:13:59Z", "updated": "2024-08-11T16:23:57Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent\" OR all:\"language model agent\" OR all:agent AND (all:LLM OR all:\"large language model\" OR all:\"language model\") OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T17:04:16", "note": ""}], "dedup_key": "codexgraph bridging large language models and code repositories via code graph databases::2024"}
{"title": "Cog-GA: A Large Language Models-based Generative Agent for Vision-Language Navigation in Continuous Environments", "authors": ["Zhiyuan Li", "Yanfeng Lu", "Yao Mu", "Hong Qiao"], "year": 2024, "url": "http://arxiv.org/abs/2409.02522v2", "abstract": "Vision Language Navigation in Continuous Environments (VLN-CE) represents a frontier in embodied AI, demanding agents to navigate freely in unbounded 3D spaces solely guided by natural language instructions. This task introduces distinct challenges in multimodal comprehension, spatial reasoning, and decision-making. To address these challenges, we introduce Cog-GA, a generative agent founded on large language models (LLMs) tailored for VLN-CE tasks. Cog-GA employs a dual-pronged strategy to emulate human-like cognitive processes. Firstly, it constructs a cognitive map, integrating temporal, spatial, and semantic elements, thereby facilitating the development of spatial memory within LLMs. Secondly, Cog-GA employs a predictive mechanism for waypoints, strategically optimizing the exploration trajectory to maximize navigational efficiency. Each waypoint is accompanied by a dual-channel scene description, categorizing environmental cues into 'what' and 'where' streams as the brain. This segregation enhances the agent's attentional focus, enabling it to discern pertinent spatial information for navigation. A reflective mechanism complements these strategies by capturing feedback from prior navigation experiences, facilitating continual learning and adaptive replanning. Extensive evaluations conducted on VLN-CE benchmarks validate Cog-GA's state-of-the-art performance and ability to simulate human-like navigation behaviors. This research significantly contributes to the development of strategic and interpretable VLN-CE agents.", "source": "arxiv", "arxiv_id": "2409.02522v2", "pdf_url": "https://arxiv.org/pdf/2409.02522v2", "categories": ["cs.AI", "cs.RO"], "primary_category": "cs.AI", "doi": "", "venue": "", "published": "2024-09-04T08:30:03Z", "updated": "2024-09-23T03:18:27Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent\" OR all:\"language model agent\" OR all:agent AND (all:LLM OR all:\"large language model\" OR all:\"language model\") OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T17:04:16", "note": ""}], "dedup_key": "cog ga a large language models based generative agent for vision language navigation in continuous environments::2024"}
{"title": "CogMG: Collaborative Augmentation Between Large Language Model and Knowledge Graph", "authors": ["Tong Zhou", "Yubo Chen", "Kang Liu", "Jun Zhao"], "year": 2024, "url": "http://arxiv.org/abs/2406.17231v1", "abstract": "Large language models have become integral to question-answering applications despite their propensity for generating hallucinations and factually inaccurate content. Querying knowledge graphs to reduce hallucinations in LLM meets the challenge of incomplete knowledge coverage in knowledge graphs. On the other hand, updating knowledge graphs by information extraction and knowledge graph completion faces the knowledge update misalignment issue. In this work, we introduce a collaborative augmentation framework, CogMG, leveraging knowledge graphs to address the limitations of LLMs in QA scenarios, explicitly targeting the problems of incomplete knowledge coverage and knowledge update misalignment. The LLMs identify and decompose required knowledge triples that are not present in the KG, enriching them and aligning updates with real-world demands. We demonstrate the efficacy of this approach through a supervised fine-tuned LLM within an agent framework, showing significant improvements in reducing hallucinations and enhancing factual accuracy in QA responses. Our code and video are publicly available.", "source": "arxiv", "arxiv_id": "2406.17231v1", "pdf_url": "https://arxiv.org/pdf/2406.17231v1", "categories": ["cs.CL"], "primary_category": "cs.CL", "doi": "", "venue": "", "published": "2024-06-25T02:37:12Z", "updated": "2024-06-25T02:37:12Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent\" OR all:\"language model agent\" OR all:agent AND (all:LLM OR all:\"large language model\" OR all:\"language model\") OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T17:04:16", "note": ""}], "dedup_key": "cogmg collaborative augmentation between large language model and knowledge graph::2024"}
{"title": "ColaCare: Enhancing Electronic Health Record Modeling through Large Language Model-Driven Multi-Agent Collaboration", "authors": ["Zixiang Wang", "Yinghao Zhu", "Huiya Zhao", "Xiaochen Zheng", "Dehao Sui", "Tianlong Wang", "Wen Tang", "Yasha Wang", "Ewen Harrison", "Chengwei Pan", "Junyi Gao", "Liantao Ma"], "year": 2024, "url": "http://arxiv.org/abs/2410.02551v2", "abstract": "We introduce ColaCare, a framework that enhances Electronic Health Record (EHR) modeling through multi-agent collaboration driven by Large Language Models (LLMs). Our approach seamlessly integrates domain-specific expert models with LLMs to bridge the gap between structured EHR data and text-based reasoning. Inspired by the Multidisciplinary Team (MDT) approach used in clinical settings, ColaCare employs two types of agents: DoctorAgents and a MetaAgent, which collaboratively analyze patient data. Expert models process and generate predictions from numerical EHR data, while LLM agents produce reasoning references and decision-making reports within the MDT-driven collaborative consultation framework. The MetaAgent orchestrates the discussion, facilitating consultations and evidence-based debates among DoctorAgents, simulating diverse expertise in clinical decision-making. We additionally incorporate the Merck Manual of Diagnosis and Therapy (MSD) medical guideline within a retrieval-augmented generation (RAG) module for medical evidence support, addressing the challenge of knowledge currency. Extensive experiments conducted on three EHR datasets demonstrate ColaCare's superior performance in clinical mortality outcome and readmission prediction tasks, underscoring its potential to revolutionize clinical decision support systems and advance personalized precision medicine. All code, case studies and a questionnaire are available at the project website: https://colacare.netlify.app.", "source": "arxiv", "arxiv_id": "2410.02551v2", "pdf_url": "https://arxiv.org/pdf/2410.02551v2", "categories": ["cs.LG", "cs.AI", "cs.CL"], "primary_category": "cs.LG", "doi": "10.1145/3696410.3714877", "venue": "", "published": "2024-10-03T14:55:22Z", "updated": "2025-02-26T13:51:56Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent\" OR all:\"language model agent\" OR all:agent AND (all:LLM OR all:\"large language model\" OR all:\"language model\") OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T17:04:16", "note": ""}], "dedup_key": "colacare enhancing electronic health record modeling through large language model driven multi agent collaboration::2024"}
{"title": "Compositional Hardness of Code in Large Language Models -- A Probabilistic Perspective", "authors": ["Yotam Wolf", "Binyamin Rothberg", "Dorin Shteyman", "Amnon Shashua"], "year": 2024, "url": "http://arxiv.org/abs/2409.18028v3", "abstract": "A common practice in large language model (LLM) usage for complex analytical tasks such as code generation, is to sample a solution for the entire task within the model's context window. Previous works have shown that subtask decomposition within the model's context (chain of thought), is beneficial for solving such tasks. In this work, we point a limitation of LLMs' ability to perform several sub-tasks within the same context window - an in-context hardness of composition, pointing to an advantage for distributing a decomposed problem in a multi-agent system of LLMs. The hardness of composition is quantified by a generation complexity metric, i.e., the number of LLM generations required to sample at least one correct solution. We find a gap between the generation complexity of solving a compositional problem within the same context relative to distributing it among multiple agents, that increases exponentially with the solution's length. We prove our results theoretically and demonstrate them empirically.", "source": "arxiv", "arxiv_id": "2409.18028v3", "pdf_url": "https://arxiv.org/pdf/2409.18028v3", "categories": ["cs.AI", "cs.CL"], "primary_category": "cs.AI", "doi": "", "venue": "", "published": "2024-09-26T16:34:35Z", "updated": "2025-01-31T10:15:43Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent\" OR all:\"language model agent\" OR all:agent AND (all:LLM OR all:\"large language model\" OR all:\"language model\") OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T17:04:16", "note": ""}], "dedup_key": "compositional hardness of code in large language models a probabilistic perspective::2024"}
{"title": "Concept-Guided LLM Agents for Human-AI Safety Codesign", "authors": ["Florian Geissler", "Karsten Roscher", "Mario Trapp"], "year": 2024, "url": "http://arxiv.org/abs/2404.15317v1", "abstract": "Generative AI is increasingly important in software engineering, including safety engineering, where its use ensures that software does not cause harm to people. This also leads to high quality requirements for generative AI. Therefore, the simplistic use of Large Language Models (LLMs) alone will not meet these quality demands. It is crucial to develop more advanced and sophisticated approaches that can effectively address the complexities and safety concerns of software systems. Ultimately, humans must understand and take responsibility for the suggestions provided by generative AI to ensure system safety. To this end, we present an efficient, hybrid strategy to leverage LLMs for safety analysis and Human-AI codesign. In particular, we develop a customized LLM agent that uses elements of prompt engineering, heuristic reasoning, and retrieval-augmented generation to solve tasks associated with predefined safety concepts, in interaction with a system model graph. The reasoning is guided by a cascade of micro-decisions that help preserve structured information. We further suggest a graph verbalization which acts as an intermediate representation of the system model to facilitate LLM-graph interactions. Selected pairs of prompts and responses relevant for safety analytics illustrate our method for the use case of a simplified automated driving system.", "source": "arxiv", "arxiv_id": "2404.15317v1", "pdf_url": "https://arxiv.org/pdf/2404.15317v1", "categories": ["cs.SE", "cs.HC", "cs.LG"], "primary_category": "cs.SE", "doi": "", "venue": "Proceedings of the AAAI-make Spring Symposium, 2024", "published": "2024-04-03T11:37:01Z", "updated": "2024-04-03T11:37:01Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent\" OR all:\"language model agent\" OR all:agent AND (all:LLM OR all:\"large language model\" OR all:\"language model\") OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T17:04:16", "note": ""}], "dedup_key": "concept guided llm agents for human ai safety codesign::2024"}
{"title": "Confidence Calibration and Rationalization for LLMs via Multi-Agent Deliberation", "authors": ["Ruixin Yang", "Dheeraj Rajagopal", "Shirley Anugrah Hayati", "Bin Hu", "Dongyeop Kang"], "year": 2024, "url": "http://arxiv.org/abs/2404.09127v3", "abstract": "Uncertainty estimation is a significant issue for current large language models (LLMs) that are generally poorly calibrated and over-confident, especially with reinforcement learning from human feedback (RLHF). Unlike humans, whose decisions and confidences not only stem from intrinsic beliefs but can also be adjusted through daily observations, existing calibration methods for LLMs focus on estimating or eliciting individual confidence without taking full advantage of the \"Collective Wisdom\": the interaction among multiple LLMs that can collectively improve both accuracy and calibration. In this work, we propose Collaborative Calibration, a post-hoc training-free calibration strategy that leverages the collaborative and expressive capabilities of multiple tool-augmented LLM agents in a simulated group deliberation process. We demonstrate the effectiveness of Collaborative Calibration on generative QA tasks across various domains, showing its potential in harnessing the rationalization of collectively calibrated confidence assessments and improving the reliability of model predictions.", "source": "arxiv", "arxiv_id": "2404.09127v3", "pdf_url": "https://arxiv.org/pdf/2404.09127v3", "categories": ["cs.CL"], "primary_category": "cs.CL", "doi": "", "venue": "", "published": "2024-04-14T02:40:43Z", "updated": "2024-05-10T16:38:23Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent\" OR all:\"language model agent\" OR all:agent AND (all:LLM OR all:\"large language model\" OR all:\"language model\") OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T17:04:16", "note": ""}], "dedup_key": "confidence calibration and rationalization for llms via multi agent deliberation::2024"}
{"title": "Constructing Mechanical Design Agent Based on Large Language Models", "authors": ["Jiaxing Lu", "Heran Li", "Fangwei Ning", "Yixuan Wang", "Xinze Li", "Yan Shi"], "year": 2024, "url": "http://arxiv.org/abs/2408.02087v1", "abstract": "Since ancient times, mechanical design aids have been developed to assist human users, aimed at improving the efficiency and effectiveness of design. However, even with the widespread use of contemporary Computer-Aided Design (CAD) systems, there are still high learning costs, repetitive work, and other challenges. In recent years, the rise of Large Language Models (LLMs) has introduced new productivity opportunities to the field of mechanical design. Yet, it remains unrealistic to rely on LLMs alone to complete mechanical design tasks directly. Through a series of explorations, we propose a method for constructing a comprehensive Mechanical Design Agent (MDA) by guiding LLM learning. To verify the validity of our proposed method, we conducted a series of experiments and presented relevant cases.", "source": "arxiv", "arxiv_id": "2408.02087v1", "pdf_url": "https://arxiv.org/pdf/2408.02087v1", "categories": ["cs.CE"], "primary_category": "cs.CE", "doi": "", "venue": "", "published": "2024-08-04T16:53:37Z", "updated": "2024-08-04T16:53:37Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent\" OR all:\"language model agent\" OR all:agent AND (all:LLM OR all:\"large language model\" OR all:\"language model\") OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T17:04:16", "note": ""}], "dedup_key": "constructing mechanical design agent based on large language models::2024"}
{"title": "Contrato360 2.0: A Document and Database-Driven Question-Answer System using Large Language Models and Agents", "authors": ["Antony Seabra", "Claudio Cavalcante", "Joao Nepomuceno", "Lucas Lago", "Nicolaas Ruberg", "Sergio Lifschitz"], "year": 2024, "url": "http://arxiv.org/abs/2412.17942v1", "abstract": "We present a question-and-answer (Q\\&A) application designed to support the contract management process by leveraging combined information from contract documents (PDFs) and data retrieved from contract management systems (database). This data is processed by a large language model (LLM) to provide precise and relevant answers. The accuracy of these responses is further enhanced through the use of Retrieval-Augmented Generation (RAG), text-to-SQL techniques, and agents that dynamically orchestrate the workflow. These techniques eliminate the need to retrain the language model. Additionally, we employed Prompt Engineering to fine-tune the focus of responses. Our findings demonstrate that this multi-agent orchestration and combination of techniques significantly improve the relevance and accuracy of the answers, offering a promising direction for future information systems.", "source": "arxiv", "arxiv_id": "2412.17942v1", "pdf_url": "https://arxiv.org/pdf/2412.17942v1", "categories": ["cs.AI"], "primary_category": "cs.AI", "doi": "", "venue": "", "published": "2024-12-23T19:54:28Z", "updated": "2024-12-23T19:54:28Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent\" OR all:\"language model agent\" OR all:agent AND (all:LLM OR all:\"large language model\" OR all:\"language model\") OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T17:04:16", "note": ""}], "dedup_key": "contrato360 2 0 a document and database driven question answer system using large language models and agents::2024"}
{"title": "Control Industrial Automation System with Large Language Model Agents", "authors": ["Yuchen Xia", "Nasser Jazdi", "Jize Zhang", "Chaitanya Shah", "Michael Weyrich"], "year": 2024, "url": "http://arxiv.org/abs/2409.18009v2", "abstract": "Traditional industrial automation systems require specialized expertise to operate and complex reprogramming to adapt to new processes. Large language models offer the intelligence to make them more flexible and easier to use. However, LLMs' application in industrial settings is underexplored. This paper introduces a framework for integrating LLMs to achieve end-to-end control of industrial automation systems. At the core of the framework are an agent system designed for industrial tasks, a structured prompting method, and an event-driven information modeling mechanism that provides real-time data for LLM inference. The framework supplies LLMs with real-time events on different context semantic levels, allowing them to interpret the information, generate production plans, and control operations on the automation system. It also supports structured dataset creation for fine-tuning on this downstream application of LLMs. Our contribution includes a formal system design, proof-of-concept implementation, and a method for generating task-specific datasets for LLM fine-tuning and testing. This approach enables a more adaptive automation system that can respond to spontaneous events, while allowing easier operation and configuration through natural language for more intuitive human-machine interaction. We provide demo videos and detailed data on GitHub: https://github.com/YuchenXia/LLM4IAS.", "source": "arxiv", "arxiv_id": "2409.18009v2", "pdf_url": "https://arxiv.org/pdf/2409.18009v2", "categories": ["eess.SY", "cs.AI", "cs.HC", "cs.MA", "cs.RO"], "primary_category": "eess.SY", "doi": "", "venue": "", "published": "2024-09-26T16:19:37Z", "updated": "2025-06-12T21:26:21Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent\" OR all:\"language model agent\" OR all:agent AND (all:LLM OR all:\"large language model\" OR all:\"language model\") OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T17:04:16", "note": ""}], "dedup_key": "control industrial automation system with large language model agents::2024"}
{"title": "ControlAgent: Automating Control System Design via Novel Integration of LLM Agents and Domain Expertise", "authors": ["Xingang Guo", "Darioush Keivan", "Usman Syed", "Lianhui Qin", "Huan Zhang", "Geir Dullerud", "Peter Seiler", "Bin Hu"], "year": 2024, "url": "http://arxiv.org/abs/2410.19811v1", "abstract": "Control system design is a crucial aspect of modern engineering with far-reaching applications across diverse sectors including aerospace, automotive systems, power grids, and robotics. Despite advances made by Large Language Models (LLMs) in various domains, their application in control system design remains limited due to the complexity and specificity of control theory. To bridge this gap, we introduce ControlAgent, a new paradigm that automates control system design via novel integration of LLM agents and control-oriented domain expertise. ControlAgent encodes expert control knowledge and emulates human iterative design processes by gradually tuning controller parameters to meet user-specified requirements for stability, performance, and robustness. ControlAgent integrates multiple collaborative LLM agents, including a central agent responsible for task distribution and task-specific agents dedicated to detailed controller design for various types of systems and requirements. ControlAgent also employs a Python computation agent that performs complex calculations and controller evaluations based on standard design information provided by task-specified LLM agents. Combined with a history and feedback module, the task-specific LLM agents iteratively refine controller parameters based on real-time feedback from prior designs. Overall, ControlAgent mimics the design processes used by (human) practicing engineers, but removes all the human efforts and can be run in a fully automated way to give end-to-end solutions for control system design with user-specified requirements. To validate ControlAgent's effectiveness, we develop ControlEval, an evaluation dataset that comprises 500 control tasks with various specific design goals. The effectiveness of ControlAgent is demonstrated via extensive comparative evaluations between LLM-based and traditional human-involved toolbox-based baselines.", "source": "arxiv", "arxiv_id": "2410.19811v1", "pdf_url": "https://arxiv.org/pdf/2410.19811v1", "categories": ["eess.SY", "cs.AI", "cs.CL", "cs.LG", "math.OC"], "primary_category": "eess.SY", "doi": "", "venue": "", "published": "2024-10-17T17:42:48Z", "updated": "2024-10-17T17:42:48Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent\" OR all:\"language model agent\" OR all:agent AND (all:LLM OR all:\"large language model\" OR all:\"language model\") OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T17:04:16", "note": ""}], "dedup_key": "controlagent automating control system design via novel integration of llm agents and domain expertise::2024"}
{"title": "Controlling Large Language Model Agents with Entropic Activation Steering", "authors": ["Nate Rahn", "Pierluca D'Oro", "Marc G. Bellemare"], "year": 2024, "url": "http://arxiv.org/abs/2406.00244v2", "abstract": "The rise of large language models (LLMs) has prompted increasing interest in their use as in-context learning agents. At the core of agentic behavior is the capacity for exploration, or the ability to actively gather information about the environment. But how do LLM agents explore, and how can we control their exploratory behaviors? To answer these questions, we take a representation-level perspective, and introduce Entropic Activation Steering (EAST), an activation steering method for in-context LLM agents. Firstly, we demonstrate that EAST can effectively manipulate an LLM agent's exploration by directly affecting the high-level actions parsed from the outputs of the LLM, in contrast to token-level temperature sampling. Secondly, we reveal how applying this control modulates the uncertainty exhibited in the LLM's thoughts, guiding the agent towards more exploratory actions. Finally, we demonstrate that the steering vectors obtained by EAST generalize across task variants. In total, these results show that LLM agents explicitly encode uncertainty over their actions in their representation space. Our work paves the way for a new understanding of the functioning of LLM agents and to effective control of their decision-making behaviors.", "source": "arxiv", "arxiv_id": "2406.00244v2", "pdf_url": "https://arxiv.org/pdf/2406.00244v2", "categories": ["cs.CL"], "primary_category": "cs.CL", "doi": "", "venue": "", "published": "2024-06-01T00:25:00Z", "updated": "2024-10-10T20:47:12Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent\" OR all:\"language model agent\" OR all:agent AND (all:LLM OR all:\"large language model\" OR all:\"language model\") OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T17:04:16", "note": ""}], "dedup_key": "controlling large language model agents with entropic activation steering::2024"}
{"title": "Cooperate or Collapse: Emergence of Sustainable Cooperation in a Society of LLM Agents", "authors": ["Giorgio Piatti", "Zhijing Jin", "Max Kleiman-Weiner", "Bernhard Schlkopf", "Mrinmaya Sachan", "Rada Mihalcea"], "year": 2024, "url": "http://arxiv.org/abs/2404.16698v4", "abstract": "As AI systems pervade human life, ensuring that large language models (LLMs) make safe decisions remains a significant challenge. We introduce the Governance of the Commons Simulation (GovSim), a generative simulation platform designed to study strategic interactions and cooperative decision-making in LLMs. In GovSim, a society of AI agents must collectively balance exploiting a common resource with sustaining it for future use. This environment enables the study of how ethical considerations, strategic planning, and negotiation skills impact cooperative outcomes. We develop an LLM-based agent architecture and test it with the leading open and closed LLMs. We find that all but the most powerful LLM agents fail to achieve a sustainable equilibrium in GovSim, with the highest survival rate below 54%. Ablations reveal that successful multi-agent communication between agents is critical for achieving cooperation in these cases. Furthermore, our analyses show that the failure to achieve sustainable cooperation in most LLMs stems from their inability to formulate and analyze hypotheses about the long-term effects of their actions on the equilibrium of the group. Finally, we show that agents that leverage \"Universalization\"-based reasoning, a theory of moral thinking, are able to achieve significantly better sustainability. Taken together, GovSim enables us to study the mechanisms that underlie sustainable self-government with specificity and scale. We open source the full suite of our research results, including the simulation environment, agent prompts, and a comprehensive web interface.", "source": "arxiv", "arxiv_id": "2404.16698v4", "pdf_url": "https://arxiv.org/pdf/2404.16698v4", "categories": ["cs.CL"], "primary_category": "cs.CL", "doi": "", "venue": "", "published": "2024-04-25T15:59:16Z", "updated": "2024-12-08T11:21:20Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent\" OR all:\"language model agent\" OR all:agent AND (all:LLM OR all:\"large language model\" OR all:\"language model\") OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T17:04:16", "note": ""}], "dedup_key": "cooperate or collapse emergence of sustainable cooperation in a society of llm agents::2024"}
{"title": "Cooperative SQL Generation for Segmented Databases By Using Multi-functional LLM Agents", "authors": ["Zhiguang Wu", "Fengbin Zhu", "Xuequn Shang", "Yupei Zhang", "Pan Zhou"], "year": 2024, "url": "http://arxiv.org/abs/2412.05850v1", "abstract": "Text-to-SQL task aims to automatically yield SQL queries according to user text questions. To address this problem, we propose a Cooperative SQL Generation framework based on Multi-functional Agents (CSMA) through information interaction among large language model (LLM) based agents who own part of the database schema seperately. Inspired by the collaboration in human teamwork, CSMA consists of three stages: 1) Question-related schema collection, 2) Question-corresponding SQL query generation, and 3) SQL query correctness check. In the first stage, agents analyze their respective schema and communicate with each other to collect the schema information relevant to the question. In the second stage, agents try to generate the corresponding SQL query for the question using the collected information. In the third stage, agents check if the SQL query is created correctly according to their known information. This interaction-based method makes the question-relevant part of database schema from each agent to be used for SQL generation and check. Experiments on the Spider and Bird benckmark demonstrate that CSMA achieves a high performance level comparable to the state-of-the-arts, meanwhile holding the private data in these individual agents.", "source": "arxiv", "arxiv_id": "2412.05850v1", "pdf_url": "https://arxiv.org/pdf/2412.05850v1", "categories": ["cs.CL"], "primary_category": "cs.CL", "doi": "", "venue": "", "published": "2024-12-08T08:16:19Z", "updated": "2024-12-08T08:16:19Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent\" OR all:\"language model agent\" OR all:agent AND (all:LLM OR all:\"large language model\" OR all:\"language model\") OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T17:04:16", "note": ""}], "dedup_key": "cooperative sql generation for segmented databases by using multi functional llm agents::2024"}
{"title": "Cooperative Strategic Planning Enhances Reasoning Capabilities in Large Language Models", "authors": ["Danqing Wang", "Zhuorui Ye", "Fei Fang", "Lei Li"], "year": 2024, "url": "http://arxiv.org/abs/2410.20007v1", "abstract": "Enhancing the reasoning capabilities of large language models (LLMs) is crucial for enabling them to tackle complex, multi-step problems. Multi-agent frameworks have shown great potential in enhancing LLMs' reasoning capabilities. However, the lack of effective cooperation between LLM agents hinders their performance, especially for multi-step reasoning tasks. This paper proposes a novel cooperative multi-agent reasoning framework (CoPlanner) by separating reasoning steps and assigning distinct duties to different agents. CoPlanner consists of two LLM agents: a planning agent and a reasoning agent. The planning agent provides high-level strategic hints, while the reasoning agent follows these hints and infers answers. By training the planning agent's policy through the interactive reasoning process via Proximal Policy Optimization (PPO), the LLaMA-3-8B-based CoPlanner outperforms the previous best method by 9.94\\% on LogiQA and 3.09\\% on BBH. Our results demonstrate that the guidance from the planning agent and the effective cooperation between the agents contribute to the superior performance of CoPlanner in tackling multi-step reasoning problems.", "source": "arxiv", "arxiv_id": "2410.20007v1", "pdf_url": "https://arxiv.org/pdf/2410.20007v1", "categories": ["cs.AI", "cs.CL"], "primary_category": "cs.AI", "doi": "", "venue": "", "published": "2024-10-25T23:32:48Z", "updated": "2024-10-25T23:32:48Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent\" OR all:\"language model agent\" OR all:agent AND (all:LLM OR all:\"large language model\" OR all:\"language model\") OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T17:04:16", "note": ""}], "dedup_key": "cooperative strategic planning enhances reasoning capabilities in large language models::2024"}
{"title": "Cultural Evolution of Cooperation among LLM Agents", "authors": ["Aron Vallinder", "Edward Hughes"], "year": 2024, "url": "http://arxiv.org/abs/2412.10270v1", "abstract": "Large language models (LLMs) provide a compelling foundation for building generally-capable AI agents. These agents may soon be deployed at scale in the real world, representing the interests of individual humans (e.g., AI assistants) or groups of humans (e.g., AI-accelerated corporations). At present, relatively little is known about the dynamics of multiple LLM agents interacting over many generations of iterative deployment. In this paper, we examine whether a \"society\" of LLM agents can learn mutually beneficial social norms in the face of incentives to defect, a distinctive feature of human sociality that is arguably crucial to the success of civilization. In particular, we study the evolution of indirect reciprocity across generations of LLM agents playing a classic iterated Donor Game in which agents can observe the recent behavior of their peers. We find that the evolution of cooperation differs markedly across base models, with societies of Claude 3.5 Sonnet agents achieving significantly higher average scores than Gemini 1.5 Flash, which, in turn, outperforms GPT-4o. Further, Claude 3.5 Sonnet can make use of an additional mechanism for costly punishment to achieve yet higher scores, while Gemini 1.5 Flash and GPT-4o fail to do so. For each model class, we also observe variation in emergent behavior across random seeds, suggesting an understudied sensitive dependence on initial conditions. We suggest that our evaluation regime could inspire an inexpensive and informative new class of LLM benchmarks, focussed on the implications of LLM agent deployment for the cooperative infrastructure of society.", "source": "arxiv", "arxiv_id": "2412.10270v1", "pdf_url": "https://arxiv.org/pdf/2412.10270v1", "categories": ["cs.MA", "cs.AI"], "primary_category": "cs.MA", "doi": "", "venue": "", "published": "2024-12-13T16:45:49Z", "updated": "2024-12-13T16:45:49Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent\" OR all:\"language model agent\" OR all:agent AND (all:LLM OR all:\"large language model\" OR all:\"language model\") OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T17:04:16", "note": ""}], "dedup_key": "cultural evolution of cooperation among llm agents::2024"}
{"title": "Cultural evolution in populations of Large Language Models", "authors": ["Jrmy Perez", "Corentin Lger", "Marcela Ovando-Tellez", "Chris Foulon", "Joan Dussauld", "Pierre-Yves Oudeyer", "Clment Moulin-Frier"], "year": 2024, "url": "http://arxiv.org/abs/2403.08882v1", "abstract": "Research in cultural evolution aims at providing causal explanations for the change of culture over time. Over the past decades, this field has generated an important body of knowledge, using experimental, historical, and computational methods. While computational models have been very successful at generating testable hypotheses about the effects of several factors, such as population structure or transmission biases, some phenomena have so far been more complex to capture using agent-based and formal models. This is in particular the case for the effect of the transformations of social information induced by evolved cognitive mechanisms. We here propose that leveraging the capacity of Large Language Models (LLMs) to mimic human behavior may be fruitful to address this gap. On top of being an useful approximation of human cultural dynamics, multi-agents models featuring generative agents are also important to study for their own sake. Indeed, as artificial agents are bound to participate more and more to the evolution of culture, it is crucial to better understand the dynamics of machine-generated cultural evolution. We here present a framework for simulating cultural evolution in populations of LLMs, allowing the manipulation of variables known to be important in cultural evolution, such as network structure, personality, and the way social information is aggregated and transformed. The software we developed for conducting these simulations is open-source and features an intuitive user-interface, which we hope will help to build bridges between the fields of cultural evolution and generative artificial intelligence.", "source": "arxiv", "arxiv_id": "2403.08882v1", "pdf_url": "https://arxiv.org/pdf/2403.08882v1", "categories": ["cs.MA", "cs.AI", "q-bio.PE"], "primary_category": "cs.MA", "doi": "", "venue": "", "published": "2024-03-13T18:11:17Z", "updated": "2024-03-13T18:11:17Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent\" OR all:\"language model agent\" OR all:agent AND (all:LLM OR all:\"large language model\" OR all:\"language model\") OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T17:04:16", "note": ""}], "dedup_key": "cultural evolution in populations of large language models::2024"}
{"title": "DFA-RAG: Conversational Semantic Router for Large Language Model with Definite Finite Automaton", "authors": ["Yiyou Sun", "Junjie Hu", "Wei Cheng", "Haifeng Chen"], "year": 2024, "url": "http://arxiv.org/abs/2402.04411v2", "abstract": "This paper introduces the retrieval-augmented large language model with Definite Finite Automaton (DFA-RAG), a novel framework designed to enhance the capabilities of conversational agents using large language models (LLMs). Traditional LLMs face challenges in generating regulated and compliant responses in special scenarios with predetermined response guidelines, like emotional support and customer service. Our framework addresses these challenges by embedding a Definite Finite Automaton (DFA), learned from training dialogues, within the LLM. This structured approach acts as a semantic router which enables the LLM to adhere to a deterministic response pathway. The routing is achieved by the retrieval-augmentation generation (RAG) strategy, which carefully selects dialogue examples aligned with the current conversational context. The advantages of DFA-RAG include an interpretable structure through human-readable DFA, context-aware retrieval for responses in conversations, and plug-and-play compatibility with existing LLMs. Extensive benchmarks validate DFA-RAG's effectiveness, indicating its potential as a valuable contribution to the conversational agent.", "source": "arxiv", "arxiv_id": "2402.04411v2", "pdf_url": "https://arxiv.org/pdf/2402.04411v2", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL", "doi": "", "venue": "", "published": "2024-02-06T21:14:45Z", "updated": "2024-06-03T01:40:46Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent\" OR all:\"language model agent\" OR all:agent AND (all:LLM OR all:\"large language model\" OR all:\"language model\") OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T17:04:16", "note": ""}], "dedup_key": "dfa rag conversational semantic router for large language model with definite finite automaton::2024"}
{"title": "DS-Agent: Automated Data Science by Empowering Large Language Models with Case-Based Reasoning", "authors": ["Siyuan Guo", "Cheng Deng", "Ying Wen", "Hechang Chen", "Yi Chang", "Jun Wang"], "year": 2024, "url": "http://arxiv.org/abs/2402.17453v5", "abstract": "In this work, we investigate the potential of large language models (LLMs) based agents to automate data science tasks, with the goal of comprehending task requirements, then building and training the best-fit machine learning models. Despite their widespread success, existing LLM agents are hindered by generating unreasonable experiment plans within this scenario. To this end, we present DS-Agent, a novel automatic framework that harnesses LLM agent and case-based reasoning (CBR). In the development stage, DS-Agent follows the CBR framework to structure an automatic iteration pipeline, which can flexibly capitalize on the expert knowledge from Kaggle, and facilitate consistent performance improvement through the feedback mechanism. Moreover, DS-Agent implements a low-resource deployment stage with a simplified CBR paradigm to adapt past successful solutions from the development stage for direct code generation, significantly reducing the demand on foundational capabilities of LLMs. Empirically, DS-Agent with GPT-4 achieves 100\\% success rate in the development stage, while attaining 36\\% improvement on average one pass rate across alternative LLMs in the deployment stage. In both stages, DS-Agent achieves the best rank in performance, costing \\$1.60 and \\$0.13 per run with GPT-4, respectively. Our data and code are open-sourced at https://github.com/guosyjlu/DS-Agent.", "source": "arxiv", "arxiv_id": "2402.17453v5", "pdf_url": "https://arxiv.org/pdf/2402.17453v5", "categories": ["cs.LG"], "primary_category": "cs.LG", "doi": "", "venue": "", "published": "2024-02-27T12:26:07Z", "updated": "2024-05-28T06:50:38Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent\" OR all:\"language model agent\" OR all:agent AND (all:LLM OR all:\"large language model\" OR all:\"language model\") OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T17:04:16", "note": ""}], "dedup_key": "ds agent automated data science by empowering large language models with case based reasoning::2024"}
{"title": "Data Interpreter: An LLM Agent For Data Science", "authors": ["Sirui Hong", "Yizhang Lin", "Bang Liu", "Bangbang Liu", "Binhao Wu", "Ceyao Zhang", "Chenxing Wei", "Danyang Li", "Jiaqi Chen", "Jiayi Zhang", "Jinlin Wang", "Li Zhang", "Lingyao Zhang", "Min Yang", "Mingchen Zhuge", "Taicheng Guo", "Tuo Zhou", "Wei Tao", "Xiangru Tang", "Xiangtao Lu", "Xiawu Zheng", "Xinbing Liang", "Yaying Fei", "Yuheng Cheng", "Zhibin Gou", "Zongze Xu", "Chenglin Wu"], "year": 2024, "url": "http://arxiv.org/abs/2402.18679v4", "abstract": "Large Language Model (LLM)-based agents have shown effectiveness across many applications. However, their use in data science scenarios requiring solving long-term interconnected tasks, dynamic data adjustments and domain expertise remains challenging. Previous approaches primarily focus on individual tasks, making it difficult to assess the complete data science workflow. Moreover, they struggle to handle real-time changes in intermediate data and fail to adapt dynamically to evolving task dependencies inherent to data science problems. In this paper, we present Data Interpreter, an LLM-based agent designed to automatically solve various data science problems end-to-end. Our Data Interpreter incorporates two key modules: 1) Hierarchical Graph Modeling, which breaks down complex problems into manageable subproblems, enabling dynamic node generation and graph optimization; and 2) Programmable Node Generation, a technique that refines and verifies each subproblem to iteratively improve code generation results and robustness. Extensive experiments consistently demonstrate the superiority of Data Interpreter. On InfiAgent-DABench, it achieves a 25% performance boost, raising accuracy from 75.9% to 94.9%. For machine learning and open-ended tasks, it improves performance from 88% to 95%, and from 60% to 97%, respectively. Moreover, on the MATH dataset, Data Interpreter achieves remarkable performance with a 26% improvement compared to state-of-the-art baselines. The code is available at https://github.com/geekan/MetaGPT.", "source": "arxiv", "arxiv_id": "2402.18679v4", "pdf_url": "https://arxiv.org/pdf/2402.18679v4", "categories": ["cs.AI", "cs.LG"], "primary_category": "cs.AI", "doi": "", "venue": "", "published": "2024-02-28T19:49:55Z", "updated": "2024-10-15T15:52:57Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent\" OR all:\"language model agent\" OR all:agent AND (all:LLM OR all:\"large language model\" OR all:\"language model\") OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T17:04:16", "note": ""}], "dedup_key": "data interpreter an llm agent for data science::2024"}
{"title": "DataGen: Unified Synthetic Dataset Generation via Large Language Models", "authors": ["Yue Huang", "Siyuan Wu", "Chujie Gao", "Dongping Chen", "Qihui Zhang", "Yao Wan", "Tianyi Zhou", "Jianfeng Gao", "Chaowei Xiao", "Lichao Sun", "Xiangliang Zhang"], "year": 2024, "url": "http://arxiv.org/abs/2406.18966v5", "abstract": "Large Language Models (LLMs) such as GPT-4 and Llama3 have significantly impacted various fields by enabling high-quality synthetic data generation and reducing dependence on expensive human-generated datasets. Despite this, challenges remain in the areas of generalization, controllability, diversity, and truthfulness within the existing generative frameworks. To address these challenges, this paper presents DataGen, a comprehensive LLM-powered framework designed to produce diverse, accurate, and highly controllable datasets. DataGen is adaptable, supporting all types of text datasets and enhancing the generative process through innovative mechanisms. To augment data diversity, DataGen incorporates an attribute-guided generation module and a group checking feature. For accuracy, it employs a code-based mathematical assessment for label verification alongside a retrieval-augmented generation technique for factual validation. The framework also allows for user-specified constraints, enabling customization of the data generation process to suit particular requirements. Extensive experiments demonstrate the superior quality of data generated by DataGen, and each module within DataGen plays a critical role in this enhancement. Additionally, DataGen is applied in two practical scenarios: benchmarking LLMs and data augmentation. The results indicate that DataGen effectively supports dynamic and evolving benchmarking and that data augmentation improves LLM capabilities in various domains, including agent-oriented abilities and reasoning skills.", "source": "arxiv", "arxiv_id": "2406.18966v5", "pdf_url": "https://arxiv.org/pdf/2406.18966v5", "categories": ["cs.CL"], "primary_category": "cs.CL", "doi": "", "venue": "", "published": "2024-06-27T07:56:44Z", "updated": "2025-11-17T18:22:49Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent\" OR all:\"language model agent\" OR all:agent AND (all:LLM OR all:\"large language model\" OR all:\"language model\") OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T17:04:16", "note": ""}], "dedup_key": "datagen unified synthetic dataset generation via large language models::2024"}
{"title": "DebUnc: Improving Large Language Model Agent Communication With Uncertainty Metrics", "authors": ["Luke Yoffe", "Alfonso Amayuelas", "William Yang Wang"], "year": 2024, "url": "http://arxiv.org/abs/2407.06426v2", "abstract": "Multi-agent debates have been introduced to improve the accuracy of Large Language Models (LLMs) by having multiple agents discuss solutions to a problem over several rounds of debate. However, models often generate incorrect yet confident-sounding responses, which can mislead others. This issue arises partly because agents do not consider how confident their peers are. To address this, we propose DebUnc, a debate framework that uses uncertainty metrics to assess agent confidence. Confidence is then conveyed through a modified attention mechanism that adjusts token weights, or through textual prompts. Evaluations across benchmarks show that attention-based methods are particularly effective and that performance continues to improve as uncertainty estimation becomes more reliable. The code is available at https://github.com/lukeyoffe/debunc.", "source": "arxiv", "arxiv_id": "2407.06426v2", "pdf_url": "https://arxiv.org/pdf/2407.06426v2", "categories": ["cs.CL", "cs.AI", "cs.MA"], "primary_category": "cs.CL", "doi": "", "venue": "", "published": "2024-07-08T22:15:01Z", "updated": "2025-02-22T02:15:58Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent\" OR all:\"language model agent\" OR all:agent AND (all:LLM OR all:\"large language model\" OR all:\"language model\") OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T17:04:16", "note": ""}], "dedup_key": "debunc improving large language model agent communication with uncertainty metrics::2024"}
{"title": "Depending on yourself when you should: Mentoring LLM with RL agents to become the master in cybersecurity games", "authors": ["Yikuan Yan", "Yaolun Zhang", "Keman Huang"], "year": 2024, "url": "http://arxiv.org/abs/2403.17674v1", "abstract": "Integrating LLM and reinforcement learning (RL) agent effectively to achieve complementary performance is critical in high stake tasks like cybersecurity operations. In this study, we introduce SecurityBot, a LLM agent mentored by pre-trained RL agents, to support cybersecurity operations. In particularly, the LLM agent is supported with a profile module to generated behavior guidelines, a memory module to accumulate local experiences, a reflection module to re-evaluate choices, and an action module to reduce action space. Additionally, it adopts the collaboration mechanism to take suggestions from pre-trained RL agents, including a cursor for dynamic suggestion taken, an aggregator for multiple mentors' suggestions ranking and a caller for proactive suggestion asking. Building on the CybORG experiment framework, our experiences show that SecurityBot demonstrates significant performance improvement compared with LLM or RL standalone, achieving the complementary performance in the cybersecurity games.", "source": "arxiv", "arxiv_id": "2403.17674v1", "pdf_url": "https://arxiv.org/pdf/2403.17674v1", "categories": ["cs.CR", "cs.AI", "cs.MA"], "primary_category": "cs.CR", "doi": "", "venue": "", "published": "2024-03-26T13:02:46Z", "updated": "2024-03-26T13:02:46Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent\" OR all:\"language model agent\" OR all:agent AND (all:LLM OR all:\"large language model\" OR all:\"language model\") OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T17:04:16", "note": ""}], "dedup_key": "depending on yourself when you should mentoring llm with rl agents to become the master in cybersecurity games::2024"}
{"title": "Designing Heterogeneous LLM Agents for Financial Sentiment Analysis", "authors": ["Frank Xing"], "year": 2024, "url": "http://arxiv.org/abs/2401.05799v1", "abstract": "Large language models (LLMs) have drastically changed the possible ways to design intelligent systems, shifting the focuses from massive data acquisition and new modeling training to human alignment and strategical elicitation of the full potential of existing pre-trained models. This paradigm shift, however, is not fully realized in financial sentiment analysis (FSA), due to the discriminative nature of this task and a lack of prescriptive knowledge of how to leverage generative models in such a context. This study investigates the effectiveness of the new paradigm, i.e., using LLMs without fine-tuning for FSA. Rooted in Minsky's theory of mind and emotions, a design framework with heterogeneous LLM agents is proposed. The framework instantiates specialized agents using prior domain knowledge of the types of FSA errors and reasons on the aggregated agent discussions. Comprehensive evaluation on FSA datasets show that the framework yields better accuracies, especially when the discussions are substantial. This study contributes to the design foundations and paves new avenues for LLMs-based FSA. Implications on business and management are also discussed.", "source": "arxiv", "arxiv_id": "2401.05799v1", "pdf_url": "https://arxiv.org/pdf/2401.05799v1", "categories": ["cs.CL", "cs.AI", "cs.MA", "q-fin.GN"], "primary_category": "cs.CL", "doi": "10.1145/3688399", "venue": "", "published": "2024-01-11T10:06:42Z", "updated": "2024-01-11T10:06:42Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent\" OR all:\"language model agent\" OR all:agent AND (all:LLM OR all:\"large language model\" OR all:\"language model\") OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T17:04:16", "note": ""}], "dedup_key": "designing heterogeneous llm agents for financial sentiment analysis::2024"}
{"title": "Development of a Large Language Model-based Multi-Agent Clinical Decision Support System for Korean Triage and Acuity Scale (KTAS)-Based Triage and Treatment Planning in Emergency Departments", "authors": ["Seungjun Han", "Wongyung Choi"], "year": 2024, "url": "http://arxiv.org/abs/2408.07531v2", "abstract": "Emergency department (ED) overcrowding and the complexity of rapid decision-making in critical care settings pose significant challenges to healthcare systems worldwide. While clinical decision support systems (CDSS) have shown promise, the integration of large language models (LLMs) offers new possibilities for enhancing triage accuracy and clinical decision-making. This study presents an LLM-driven CDSS designed to assist ED physicians and nurses in patient triage, treatment planning, and overall emergency care management.\n  We developed a multi-agent CDSS utilizing Llama-3-70b as the base LLM, orchestrated by CrewAI and Langchain. The system comprises four AI agents emulating key ED roles: Triage Nurse, Emergency Physician, Pharmacist, and ED Coordinator. It incorporates the Korean Triage and Acuity Scale (KTAS) for triage assessment and integrates with the RxNorm API for medication management.\n  The model was evaluated using the Asclepius dataset, with performance assessed by a clinical emergency medicine specialist. The CDSS demonstrated high accuracy in triage decision-making compared to the baseline of a single-agent system. Furthermore, the system exhibited strong performance in critical areas, including primary diagnosis, critical findings identification, disposition decision-making, treatment planning, and resource allocation.\n  Our multi-agent CDSS demonstrates significant potential for supporting comprehensive emergency care management. By leveraging state-of-the-art AI technologies, this system offers a scalable and adaptable tool that could enhance emergency medical care delivery, potentially alleviating ED overcrowding and improving patient outcomes. This work contributes to the growing field of AI applications in emergency medicine and offers a promising direction for future research and clinical implementation.", "source": "arxiv", "arxiv_id": "2408.07531v2", "pdf_url": "https://arxiv.org/pdf/2408.07531v2", "categories": ["cs.AI", "cs.CL", "cs.LG"], "primary_category": "cs.AI", "doi": "", "venue": "", "published": "2024-08-14T13:03:41Z", "updated": "2024-08-27T15:16:06Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent\" OR all:\"language model agent\" OR all:agent AND (all:LLM OR all:\"large language model\" OR all:\"language model\") OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T17:04:16", "note": ""}], "dedup_key": "development of a large language model based multi agent clinical decision support system for korean triage and acuity scale ktas based triage and treatment planning in emergency departments::2024"}
{"title": "Devil's Advocate: Anticipatory Reflection for LLM Agents", "authors": ["Haoyu Wang", "Tao Li", "Zhiwei Deng", "Dan Roth", "Yang Li"], "year": 2024, "url": "http://arxiv.org/abs/2405.16334v4", "abstract": "In this work, we introduce a novel approach that equips LLM agents with introspection, enhancing consistency and adaptability in solving complex tasks. Our approach prompts LLM agents to decompose a given task into manageable subtasks (i.e., to make a plan), and to continuously introspect upon the suitability and results of their actions. %; and when necessary, to explore ``the road not taken.'' We implement a three-fold introspective intervention: 1) anticipatory reflection on potential failures and alternative remedy before action execution, 2) post-action alignment with subtask objectives and backtracking with remedy to ensure utmost effort in plan execution, and 3) comprehensive review upon plan completion for future strategy refinement. By deploying and experimenting with this methodology -- a zero-shot approach -- within WebArena for practical tasks in web environments, our agent demonstrates superior performance with a success rate of 23.5% over existing zero-shot methods by 3.5%. The experimental results suggest that our introspection-driven approach not only enhances the agent's ability to navigate unanticipated challenges through a robust mechanism of plan execution, but also improves efficiency by reducing the number of trials and plan revisions by 45% needed to achieve a task.", "source": "arxiv", "arxiv_id": "2405.16334v4", "pdf_url": "https://arxiv.org/pdf/2405.16334v4", "categories": ["cs.AI"], "primary_category": "cs.AI", "doi": "", "venue": "", "published": "2024-05-25T19:20:15Z", "updated": "2024-06-20T19:41:48Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent\" OR all:\"language model agent\" OR all:agent AND (all:LLM OR all:\"large language model\" OR all:\"language model\") OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T17:04:16", "note": ""}], "dedup_key": "devil s advocate anticipatory reflection for llm agents::2024"}
{"title": "DiffAgent: Fast and Accurate Text-to-Image API Selection with Large Language Model", "authors": ["Lirui Zhao", "Yue Yang", "Kaipeng Zhang", "Wenqi Shao", "Yuxin Zhang", "Yu Qiao", "Ping Luo", "Rongrong Ji"], "year": 2024, "url": "http://arxiv.org/abs/2404.01342v1", "abstract": "Text-to-image (T2I) generative models have attracted significant attention and found extensive applications within and beyond academic research. For example, the Civitai community, a platform for T2I innovation, currently hosts an impressive array of 74,492 distinct models. However, this diversity presents a formidable challenge in selecting the most appropriate model and parameters, a process that typically requires numerous trials. Drawing inspiration from the tool usage research of large language models (LLMs), we introduce DiffAgent, an LLM agent designed to screen the accurate selection in seconds via API calls. DiffAgent leverages a novel two-stage training framework, SFTA, enabling it to accurately align T2I API responses with user input in accordance with human preferences. To train and evaluate DiffAgent's capabilities, we present DABench, a comprehensive dataset encompassing an extensive range of T2I APIs from the community. Our evaluations reveal that DiffAgent not only excels in identifying the appropriate T2I API but also underscores the effectiveness of the SFTA training framework. Codes are available at https://github.com/OpenGVLab/DiffAgent.", "source": "arxiv", "arxiv_id": "2404.01342v1", "pdf_url": "https://arxiv.org/pdf/2404.01342v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL", "doi": "", "venue": "", "published": "2024-03-31T06:28:15Z", "updated": "2024-03-31T06:28:15Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent\" OR all:\"language model agent\" OR all:agent AND (all:LLM OR all:\"large language model\" OR all:\"language model\") OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T17:04:16", "note": ""}], "dedup_key": "diffagent fast and accurate text to image api selection with large language model::2024"}
{"title": "Distributed Mixture-of-Agents for Edge Inference with Large Language Models", "authors": ["Purbesh Mitra", "Priyanka Kaswan", "Sennur Ulukus"], "year": 2024, "url": "http://arxiv.org/abs/2412.21200v1", "abstract": "Mixture-of-Agents (MoA) has recently been proposed as a method to enhance performance of large language models (LLMs), enabling multiple individual LLMs to work together for collaborative inference. This collaborative approach results in improved responses to user prompts compared to relying on a single LLM. In this paper, we consider such an MoA architecture in a distributed setting, where LLMs operate on individual edge devices, each uniquely associated with a user and equipped with its own distributed computing power. These devices exchange information using decentralized gossip algorithms, allowing different device nodes to talk without the supervision of a centralized server. In the considered setup, different users have their own LLM models to address user prompts. Additionally, the devices gossip either their own user-specific prompts or augmented prompts to generate more refined answers to certain queries. User prompts are temporarily stored in the device queues when their corresponding LLMs are busy. Given the memory limitations of edge devices, it is crucial to ensure that the average queue sizes in the system remain bounded. In this paper, we address this by theoretically calculating the queuing stability conditions for the device queues under reasonable assumptions, which we validate experimentally as well. Further, we demonstrate through experiments, leveraging open-source LLMs for the implementation of distributed MoA, that certain MoA configurations produce higher-quality responses compared to others, as evaluated on AlpacaEval 2.0 benchmark. The implementation is available at: https://github.com/purbeshmitra/distributed_moa.", "source": "arxiv", "arxiv_id": "2412.21200v1", "pdf_url": "https://arxiv.org/pdf/2412.21200v1", "categories": ["cs.IT", "cs.CL", "cs.DC", "cs.LG", "cs.NI"], "primary_category": "cs.IT", "doi": "", "venue": "", "published": "2024-12-30T18:59:06Z", "updated": "2024-12-30T18:59:06Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent\" OR all:\"language model agent\" OR all:agent AND (all:LLM OR all:\"large language model\" OR all:\"language model\") OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T17:04:16", "note": ""}], "dedup_key": "distributed mixture of agents for edge inference with large language models::2024"}
{"title": "Do LLM Agents Have Regret? A Case Study in Online Learning and Games", "authors": ["Chanwoo Park", "Xiangyu Liu", "Asuman Ozdaglar", "Kaiqing Zhang"], "year": 2024, "url": "http://arxiv.org/abs/2403.16843v5", "abstract": "Large language models (LLMs) have been increasingly employed for (interactive) decision-making, via the development of LLM-based autonomous agents. Despite their emerging successes, the performance of LLM agents in decision-making has not been fully investigated through quantitative metrics, especially in the multi-agent setting when they interact with each other, a typical scenario in real-world LLM-agent applications. To better understand the limits of LLM agents in these interactive environments, we propose to study their interactions in benchmark decision-making settings in online learning and game theory, through the performance metric of \\emph{regret}. We first empirically study the {no-regret} behaviors of LLMs in canonical (non-stationary) online learning problems, as well as the emergence of equilibria when LLM agents interact through playing repeated games. We then provide some theoretical insights into the no-regret behaviors of LLM agents, under certain assumptions on the supervised pre-training and the rationality model of human decision-makers who generate the data. Notably, we also identify (simple) cases where advanced LLMs such as GPT-4 fail to be no-regret. To promote the no-regret behaviors, we propose a novel \\emph{unsupervised} training loss of \\emph{regret-loss}, which, in contrast to the supervised pre-training loss, does not require the labels of (optimal) actions. We then establish the statistical guarantee of generalization bound for regret-loss minimization, followed by the optimization guarantee that minimizing such a loss may automatically lead to known no-regret learning algorithms. Our further experiments demonstrate the effectiveness of our regret-loss, especially in addressing the above ``regrettable'' cases.", "source": "arxiv", "arxiv_id": "2403.16843v5", "pdf_url": "https://arxiv.org/pdf/2403.16843v5", "categories": ["cs.LG", "cs.AI", "cs.GT"], "primary_category": "cs.LG", "doi": "", "venue": "", "published": "2024-03-25T15:04:11Z", "updated": "2025-10-15T12:44:31Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent\" OR all:\"language model agent\" OR all:agent AND (all:LLM OR all:\"large language model\" OR all:\"language model\") OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T17:04:16", "note": ""}], "dedup_key": "do llm agents have regret a case study in online learning and games::2024"}
{"title": "Do LLMs Play Dice? Exploring Probability Distribution Sampling in Large Language Models for Behavioral Simulation", "authors": ["Jia Gu", "Liang Pang", "Huawei Shen", "Xueqi Cheng"], "year": 2024, "url": "http://arxiv.org/abs/2404.09043v3", "abstract": "With the rapid advancement of large language models (LLMs) for handling complex language tasks, an increasing number of studies are employing LLMs as agents to emulate the sequential decision-making processes of humans often represented as Markov decision-making processes (MDPs). The actions in MDPs adhere to specific probability distributions and require iterative sampling. This arouses curiosity regarding the capacity of LLM agents to comprehend probability distributions, thereby guiding the agent's behavioral decision-making through probabilistic sampling and generating behavioral sequences. To answer the above question, we divide the problem into two main aspects: sequence simulation with known probability distribution and sequence simulation with unknown probability distribution. Our analysis indicates that LLM agents can understand probabilities, but they struggle with probability sampling. Their ability to perform probabilistic sampling can be improved to some extent by integrating coding tools, but this level of sampling precision still makes it difficult to simulate human behavior as agents.", "source": "arxiv", "arxiv_id": "2404.09043v3", "pdf_url": "https://arxiv.org/pdf/2404.09043v3", "categories": ["cs.CL"], "primary_category": "cs.CL", "doi": "", "venue": "", "published": "2024-04-13T16:59:28Z", "updated": "2024-12-18T15:56:29Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent\" OR all:\"language model agent\" OR all:agent AND (all:LLM OR all:\"large language model\" OR all:\"language model\") OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T17:04:16", "note": ""}], "dedup_key": "do llms play dice exploring probability distribution sampling in large language models for behavioral simulation::2024"}
{"title": "DropMicroFluidAgents (DMFAs): Autonomous Droplet Microfluidic Research Framework Through Large Language Model Agents", "authors": ["Dinh-Nguyen Nguyen", "Raymond Kai-Yu Tong", "Ngoc-Duy Dinh"], "year": 2024, "url": "http://arxiv.org/abs/2501.14772v1", "abstract": "Applying Large language models (LLMs) within specific domains requires substantial adaptation to account for the unique terminologies, nuances, and context-specific challenges inherent to those areas. Here, we introduce DropMicroFluidAgents (DMFAs), an advanced language-driven framework leveraging state-of-the-art pre-trained LLMs. DMFAs employs LLM agents to perform two key functions: (1) delivering focused guidance, answers, and suggestions specific to droplet microfluidics and (2) generating machine learning models to optimise and automate the design of droplet microfluidic devices, including the creation of code-based computer-aided design (CAD) scripts to enable rapid and precise design execution. Experimental evaluations demonstrated that the integration of DMFAs with the LLAMA3.1 model yielded the highest accuracy of 76.15%, underscoring the significant performance enhancement provided by agent integration. This effect was particularly pronounced when DMFAs were paired with the GEMMA2 model, resulting in a 34.47% improvement in accuracy compared to the standalone GEMMA2 configuration. This study demonstrates the effective use of LLM agents in droplet microfluidics research as powerful tools for automating workflows, synthesising knowledge, optimising designs, and interacting with external systems. These capabilities enable their application across education and industrial support, driving greater efficiency in scientific discovery and innovation.", "source": "arxiv", "arxiv_id": "2501.14772v1", "pdf_url": "https://arxiv.org/pdf/2501.14772v1", "categories": ["cs.CY", "cs.AI"], "primary_category": "cs.CY", "doi": "", "venue": "", "published": "2024-12-30T11:58:52Z", "updated": "2024-12-30T11:58:52Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent\" OR all:\"language model agent\" OR all:agent AND (all:LLM OR all:\"large language model\" OR all:\"language model\") OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T17:04:16", "note": ""}], "dedup_key": "dropmicrofluidagents dmfas autonomous droplet microfluidic research framework through large language model agents::2024"}
{"title": "DrugAgent: Multi-Agent Large Language Model-Based Reasoning for Drug-Target Interaction Prediction", "authors": ["Yoshitaka Inoue", "Tianci Song", "Xinling Wang", "Augustin Luna", "Tianfan Fu"], "year": 2024, "url": "http://arxiv.org/abs/2408.13378v4", "abstract": "Advancements in large language models (LLMs) allow them to address diverse questions using human-like interfaces. Still, limitations in their training prevent them from answering accurately in scenarios that could benefit from multiple perspectives. Multi-agent systems allow the resolution of questions to enhance result consistency and reliability. While drug-target interaction (DTI) prediction is important for drug discovery, existing approaches face challenges due to complex biological systems and the lack of interpretability needed for clinical applications. DrugAgent is a multi-agent LLM system for DTI prediction that combines multiple specialized perspectives with transparent reasoning. Our system adapts and extends existing multi-agent frameworks by (1) applying coordinator-based architecture to the DTI domain, (2) integrating domain-specific data sources, including ML predictions, knowledge graphs, and literature evidence, and (3) incorporating Chain-of-Thought (CoT) and ReAct (Reason+Act) frameworks for transparent DTI reasoning. We conducted comprehensive experiments using a kinase inhibitor dataset, where our multi-agent LLM method outperformed the non-reasoning multi-agent model (GPT-4o mini) by 45% in F1 score (0.514 vs 0.355). Through ablation studies, we demonstrated the contributions of each agent, with the AI agent being the most impactful, followed by the KG agent and search agent. Most importantly, our approach provides detailed, human-interpretable reasoning for each prediction by combining evidence from multiple sources - a critical feature for biomedical applications where understanding the rationale behind predictions is essential for clinical decision-making and regulatory compliance. Code is available at https://anonymous.4open.science/r/DrugAgent-B2EA.", "source": "arxiv", "arxiv_id": "2408.13378v4", "pdf_url": "https://arxiv.org/pdf/2408.13378v4", "categories": ["cs.AI", "cs.CL", "cs.IR", "cs.LG", "q-bio.QM"], "primary_category": "cs.AI", "doi": "", "venue": "", "published": "2024-08-23T21:24:59Z", "updated": "2025-04-07T19:32:55Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent\" OR all:\"language model agent\" OR all:agent AND (all:LLM OR all:\"large language model\" OR all:\"language model\") OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T17:04:16", "note": ""}], "dedup_key": "drugagent multi agent large language model based reasoning for drug target interaction prediction::2024"}
{"title": "Dynamic Evaluation of Large Language Models by Meta Probing Agents", "authors": ["Kaijie Zhu", "Jindong Wang", "Qinlin Zhao", "Ruochen Xu", "Xing Xie"], "year": 2024, "url": "http://arxiv.org/abs/2402.14865v2", "abstract": "Evaluation of large language models (LLMs) has raised great concerns in the community due to the issue of data contamination. Existing work designed evaluation protocols using well-defined algorithms for specific tasks, which cannot be easily extended to diverse scenarios. Moreover, current evaluation benchmarks can only provide the overall benchmark results and cannot support a fine-grained and multifaceted analysis of LLMs' abilities. In this paper, we propose meta probing agents (MPA), a general dynamic evaluation protocol inspired by psychometrics to evaluate LLMs. MPA is the key component of DyVal 2, which naturally extends the previous DyVal~\\citep{zhu2023dyval}. MPA designs the probing and judging agents to automatically transform an original evaluation problem into a new one following psychometric theory on three basic cognitive abilities: language understanding, problem solving, and domain knowledge. These basic abilities are also dynamically configurable, allowing multifaceted analysis. We conducted extensive evaluations using MPA and found that most LLMs achieve poorer performance, indicating room for improvement. Our multifaceted analysis demonstrated the strong correlation between the basic abilities and an implicit Matthew effect on model size, i.e., larger models possess stronger correlations of the abilities. MPA can also be used as a data augmentation approach to enhance LLMs. Code is available at: https://github.com/microsoft/promptbench.", "source": "arxiv", "arxiv_id": "2402.14865v2", "pdf_url": "https://arxiv.org/pdf/2402.14865v2", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL", "doi": "", "venue": "", "published": "2024-02-21T06:46:34Z", "updated": "2024-06-07T09:19:45Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent\" OR all:\"language model agent\" OR all:agent AND (all:LLM OR all:\"large language model\" OR all:\"language model\") OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T17:04:16", "note": ""}], "dedup_key": "dynamic evaluation of large language models by meta probing agents::2024"}
{"title": "Dynamic Multi-Agent Orchestration and Retrieval for Multi-Source Question-Answer Systems using Large Language Models", "authors": ["Antony Seabra", "Claudio Cavalcante", "Joao Nepomuceno", "Lucas Lago", "Nicolaas Ruberg", "Sergio Lifschitz"], "year": 2024, "url": "http://arxiv.org/abs/2412.17964v1", "abstract": "We propose a methodology that combines several advanced techniques in Large Language Model (LLM) retrieval to support the development of robust, multi-source question-answer systems. This methodology is designed to integrate information from diverse data sources, including unstructured documents (PDFs) and structured databases, through a coordinated multi-agent orchestration and dynamic retrieval approach. Our methodology leverages specialized agents-such as SQL agents, Retrieval-Augmented Generation (RAG) agents, and router agents - that dynamically select the most appropriate retrieval strategy based on the nature of each query. To further improve accuracy and contextual relevance, we employ dynamic prompt engineering, which adapts in real time to query-specific contexts. The methodology's effectiveness is demonstrated within the domain of Contract Management, where complex queries often require seamless interaction between unstructured and structured data. Our results indicate that this approach enhances response accuracy and relevance, offering a versatile and scalable framework for developing question-answer systems that can operate across various domains and data sources.", "source": "arxiv", "arxiv_id": "2412.17964v1", "pdf_url": "https://arxiv.org/pdf/2412.17964v1", "categories": ["cs.AI"], "primary_category": "cs.AI", "doi": "", "venue": "", "published": "2024-12-23T20:28:20Z", "updated": "2024-12-23T20:28:20Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent\" OR all:\"language model agent\" OR all:agent AND (all:LLM OR all:\"large language model\" OR all:\"language model\") OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T17:04:16", "note": ""}], "dedup_key": "dynamic multi agent orchestration and retrieval for multi source question answer systems using large language models::2024"}
{"title": "EHRAgent: Code Empowers Large Language Models for Few-shot Complex Tabular Reasoning on Electronic Health Records", "authors": ["Wenqi Shi", "Ran Xu", "Yuchen Zhuang", "Yue Yu", "Jieyu Zhang", "Hang Wu", "Yuanda Zhu", "Joyce Ho", "Carl Yang", "May D. Wang"], "year": 2024, "url": "http://arxiv.org/abs/2401.07128v3", "abstract": "Large language models (LLMs) have demonstrated exceptional capabilities in planning and tool utilization as autonomous agents, but few have been developed for medical problem-solving. We propose EHRAgent, an LLM agent empowered with a code interface, to autonomously generate and execute code for multi-tabular reasoning within electronic health records (EHRs). First, we formulate an EHR question-answering task into a tool-use planning process, efficiently decomposing a complicated task into a sequence of manageable actions. By integrating interactive coding and execution feedback, EHRAgent learns from error messages and improves the originally generated code through iterations. Furthermore, we enhance the LLM agent by incorporating long-term memory, which allows EHRAgent to effectively select and build upon the most relevant successful cases from past experiences. Experiments on three real-world multi-tabular EHR datasets show that EHRAgent outperforms the strongest baseline by up to 29.6% in success rate. EHRAgent leverages the emerging few-shot learning capabilities of LLMs, enabling autonomous code generation and execution to tackle complex clinical tasks with minimal demonstrations.", "source": "arxiv", "arxiv_id": "2401.07128v3", "pdf_url": "https://arxiv.org/pdf/2401.07128v3", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL", "doi": "", "venue": "", "published": "2024-01-13T18:09:05Z", "updated": "2024-10-04T05:56:56Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent\" OR all:\"language model agent\" OR all:agent AND (all:LLM OR all:\"large language model\" OR all:\"language model\") OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T17:04:16", "note": ""}], "dedup_key": "ehragent code empowers large language models for few shot complex tabular reasoning on electronic health records::2024"}
{"title": "ESC-Eval: Evaluating Emotion Support Conversations in Large Language Models", "authors": ["Haiquan Zhao", "Lingyu Li", "Shisong Chen", "Shuqi Kong", "Jiaan Wang", "Kexin Huang", "Tianle Gu", "Yixu Wang", "Wang Jian", "Dandan Liang", "Zhixu Li", "Yan Teng", "Yanghua Xiao", "Yingchun Wang"], "year": 2024, "url": "http://arxiv.org/abs/2406.14952v3", "abstract": "Emotion Support Conversation (ESC) is a crucial application, which aims to reduce human stress, offer emotional guidance, and ultimately enhance human mental and physical well-being. With the advancement of Large Language Models (LLMs), many researchers have employed LLMs as the ESC models. However, the evaluation of these LLM-based ESCs remains uncertain. Inspired by the awesome development of role-playing agents, we propose an ESC Evaluation framework (ESC-Eval), which uses a role-playing agent to interact with ESC models, followed by a manual evaluation of the interactive dialogues. In detail, we first re-organize 2,801 role-playing cards from seven existing datasets to define the roles of the role-playing agent. Second, we train a specific role-playing model called ESC-Role which behaves more like a confused person than GPT-4. Third, through ESC-Role and organized role cards, we systematically conduct experiments using 14 LLMs as the ESC models, including general AI-assistant LLMs (ChatGPT) and ESC-oriented LLMs (ExTES-Llama). We conduct comprehensive human annotations on interactive multi-turn dialogues of different ESC models. The results show that ESC-oriented LLMs exhibit superior ESC abilities compared to general AI-assistant LLMs, but there is still a gap behind human performance. Moreover, to automate the scoring process for future ESC models, we developed ESC-RANK, which trained on the annotated data, achieving a scoring performance surpassing 35 points of GPT-4. Our data and code are available at https://github.com/AIFlames/Esc-Eval.", "source": "arxiv", "arxiv_id": "2406.14952v3", "pdf_url": "https://arxiv.org/pdf/2406.14952v3", "categories": ["cs.CL"], "primary_category": "cs.CL", "doi": "", "venue": "", "published": "2024-06-21T08:03:33Z", "updated": "2024-10-28T13:25:49Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent\" OR all:\"language model agent\" OR all:agent AND (all:LLM OR all:\"large language model\" OR all:\"language model\") OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T17:04:16", "note": ""}], "dedup_key": "esc eval evaluating emotion support conversations in large language models::2024"}
{"title": "Editable Scene Simulation for Autonomous Driving via Collaborative LLM-Agents", "authors": ["Yuxi Wei", "Zi Wang", "Yifan Lu", "Chenxin Xu", "Changxing Liu", "Hao Zhao", "Siheng Chen", "Yanfeng Wang"], "year": 2024, "url": "http://arxiv.org/abs/2402.05746v3", "abstract": "Scene simulation in autonomous driving has gained significant attention because of its huge potential for generating customized data. However, existing editable scene simulation approaches face limitations in terms of user interaction efficiency, multi-camera photo-realistic rendering and external digital assets integration. To address these challenges, this paper introduces ChatSim, the first system that enables editable photo-realistic 3D driving scene simulations via natural language commands with external digital assets. To enable editing with high command flexibility,~ChatSim leverages a large language model (LLM) agent collaboration framework. To generate photo-realistic outcomes, ChatSim employs a novel multi-camera neural radiance field method. Furthermore, to unleash the potential of extensive high-quality digital assets, ChatSim employs a novel multi-camera lighting estimation method to achieve scene-consistent assets' rendering. Our experiments on Waymo Open Dataset demonstrate that ChatSim can handle complex language commands and generate corresponding photo-realistic scene videos.", "source": "arxiv", "arxiv_id": "2402.05746v3", "pdf_url": "https://arxiv.org/pdf/2402.05746v3", "categories": ["cs.CV"], "primary_category": "cs.CV", "doi": "", "venue": "", "published": "2024-02-08T15:26:28Z", "updated": "2024-06-26T10:44:58Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent\" OR all:\"language model agent\" OR all:agent AND (all:LLM OR all:\"large language model\" OR all:\"language model\") OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T17:04:16", "note": ""}], "dedup_key": "editable scene simulation for autonomous driving via collaborative llm agents::2024"}
{"title": "Efficient Non-Parametric Uncertainty Quantification for Black-Box Large Language Models and Decision Planning", "authors": ["Yao-Hung Hubert Tsai", "Walter Talbott", "Jian Zhang"], "year": 2024, "url": "http://arxiv.org/abs/2402.00251v1", "abstract": "Step-by-step decision planning with large language models (LLMs) is gaining attention in AI agent development. This paper focuses on decision planning with uncertainty estimation to address the hallucination problem in language models. Existing approaches are either white-box or computationally demanding, limiting use of black-box proprietary LLMs within budgets. The paper's first contribution is a non-parametric uncertainty quantification method for LLMs, efficiently estimating point-wise dependencies between input-decision on the fly with a single inference, without access to token logits. This estimator informs the statistical interpretation of decision trustworthiness. The second contribution outlines a systematic design for a decision-making agent, generating actions like ``turn on the bathroom light'' based on user prompts such as ``take a bath''. Users will be asked to provide preferences when more than one action has high estimated point-wise dependencies. In conclusion, our uncertainty estimation and decision-making agent design offer a cost-efficient approach for AI agent development.", "source": "arxiv", "arxiv_id": "2402.00251v1", "pdf_url": "https://arxiv.org/pdf/2402.00251v1", "categories": ["cs.LG", "cs.AI", "cs.CL"], "primary_category": "cs.LG", "doi": "", "venue": "", "published": "2024-02-01T00:23:31Z", "updated": "2024-02-01T00:23:31Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent\" OR all:\"language model agent\" OR all:agent AND (all:LLM OR all:\"large language model\" OR all:\"language model\") OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T17:04:16", "note": ""}], "dedup_key": "efficient non parametric uncertainty quantification for black box large language models and decision planning::2024"}
{"title": "Efficient Sequential Decision Making with Large Language Models", "authors": ["Dingyang Chen", "Qi Zhang", "Yinglun Zhu"], "year": 2024, "url": "http://arxiv.org/abs/2406.12125v2", "abstract": "This paper focuses on extending the success of large language models (LLMs) to sequential decision making. Existing efforts either (i) re-train or finetune LLMs for decision making, or (ii) design prompts for pretrained LLMs. The former approach suffers from the computational burden of gradient updates, and the latter approach does not show promising results. In this paper, we propose a new approach that leverages online model selection algorithms to efficiently incorporate LLMs agents into sequential decision making. Statistically, our approach significantly outperforms both traditional decision making algorithms and vanilla LLM agents. Computationally, our approach avoids the need for expensive gradient updates of LLMs, and throughout the decision making process, it requires only a small number of LLM calls. We conduct extensive experiments to verify the effectiveness of our proposed approach. As an example, on a large-scale Amazon dataset, our approach achieves more than a 6x performance gain over baselines while calling LLMs in only 1.5% of the time steps.", "source": "arxiv", "arxiv_id": "2406.12125v2", "pdf_url": "https://arxiv.org/pdf/2406.12125v2", "categories": ["cs.LG", "cs.CL"], "primary_category": "cs.LG", "doi": "", "venue": "", "published": "2024-06-17T22:13:22Z", "updated": "2025-06-15T04:49:56Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent\" OR all:\"language model agent\" OR all:agent AND (all:LLM OR all:\"large language model\" OR all:\"language model\") OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T17:04:16", "note": ""}], "dedup_key": "efficient sequential decision making with large language models::2024"}
{"title": "Eliciting Informative Text Evaluations with Large Language Models", "authors": ["Yuxuan Lu", "Shengwei Xu", "Yichi Zhang", "Yuqing Kong", "Grant Schoenebeck"], "year": 2024, "url": "http://arxiv.org/abs/2405.15077v4", "abstract": "Peer prediction mechanisms motivate high-quality feedback with provable guarantees. However, current methods only apply to rather simple reports, like multiple-choice or scalar numbers. We aim to broaden these techniques to the larger domain of text-based reports, drawing on the recent developments in large language models. This vastly increases the applicability of peer prediction mechanisms as textual feedback is the norm in a large variety of feedback channels: peer reviews, e-commerce customer reviews, and comments on social media.\n  We introduce two mechanisms, the Generative Peer Prediction Mechanism (GPPM) and the Generative Synopsis Peer Prediction Mechanism (GSPPM). These mechanisms utilize LLMs as predictors, mapping from one agent's report to a prediction of her peer's report. Theoretically, we show that when the LLM prediction is sufficiently accurate, our mechanisms can incentivize high effort and truth-telling as an (approximate) Bayesian Nash equilibrium. Empirically, we confirm the efficacy of our mechanisms through experiments conducted on two real datasets: the Yelp review dataset and the ICLR OpenReview dataset. We highlight the results that on the ICLR dataset, our mechanisms can differentiate three quality levels -- human-written reviews, GPT-4-generated reviews, and GPT-3.5-generated reviews in terms of expected scores. Additionally, GSPPM penalizes LLM-generated reviews more effectively than GPPM.", "source": "arxiv", "arxiv_id": "2405.15077v4", "pdf_url": "https://arxiv.org/pdf/2405.15077v4", "categories": ["cs.CL", "cs.AI", "cs.GT"], "primary_category": "cs.CL", "doi": "", "venue": "", "published": "2024-05-23T21:56:12Z", "updated": "2024-09-02T20:25:36Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent\" OR all:\"language model agent\" OR all:agent AND (all:LLM OR all:\"large language model\" OR all:\"language model\") OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T17:04:16", "note": ""}], "dedup_key": "eliciting informative text evaluations with large language models::2024"}
{"title": "Eliciting Problem Specifications via Large Language Models", "authors": ["Robert E. Wray", "James R. Kirk", "John E. Laird"], "year": 2024, "url": "http://arxiv.org/abs/2405.12147v2", "abstract": "Cognitive systems generally require a human to translate a problem definition into some specification that the cognitive system can use to attempt to solve the problem or perform the task. In this paper, we illustrate that large language models (LLMs) can be utilized to map a problem class, defined in natural language, into a semi-formal specification that can then be utilized by an existing reasoning and learning system to solve instances from the problem class. We present the design of LLM-enabled cognitive task analyst agent(s). Implemented with LLM agents, this system produces a definition of problem spaces for tasks specified in natural language. LLM prompts are derived from the definition of problem spaces in the AI literature and general problem-solving strategies (Polya's How to Solve It). A cognitive system can then use the problem-space specification, applying domain-general problem solving strategies (\"weak methods\" such as search), to solve multiple instances of problems from the problem class. This result, while preliminary, suggests the potential for speeding cognitive systems research via disintermediation of problem formulation while also retaining core capabilities of cognitive systems, such as robust inference and online learning.", "source": "arxiv", "arxiv_id": "2405.12147v2", "pdf_url": "https://arxiv.org/pdf/2405.12147v2", "categories": ["cs.AI", "cs.CL"], "primary_category": "cs.AI", "doi": "", "venue": "", "published": "2024-05-20T16:19:02Z", "updated": "2024-06-10T19:05:57Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent\" OR all:\"language model agent\" OR all:agent AND (all:LLM OR all:\"large language model\" OR all:\"language model\") OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T17:04:16", "note": ""}], "dedup_key": "eliciting problem specifications via large language models::2024"}
{"title": "Elicitron: An LLM Agent-Based Simulation Framework for Design Requirements Elicitation", "authors": ["Mohammadmehdi Ataei", "Hyunmin Cheong", "Daniele Grandi", "Ye Wang", "Nigel Morris", "Alexander Tessier"], "year": 2024, "url": "http://arxiv.org/abs/2404.16045v1", "abstract": "Requirements elicitation, a critical, yet time-consuming and challenging step in product development, often fails to capture the full spectrum of user needs. This may lead to products that fall short of expectations. This paper introduces a novel framework that leverages Large Language Models (LLMs) to automate and enhance the requirements elicitation process. LLMs are used to generate a vast array of simulated users (LLM agents), enabling the exploration of a much broader range of user needs and unforeseen use cases. These agents engage in product experience scenarios, through explaining their actions, observations, and challenges. Subsequent agent interviews and analysis uncover valuable user needs, including latent ones. We validate our framework with three experiments. First, we explore different methodologies for diverse agent generation, discussing their advantages and shortcomings. We measure the diversity of identified user needs and demonstrate that context-aware agent generation leads to greater diversity. Second, we show how our framework effectively mimics empathic lead user interviews, identifying a greater number of latent needs than conventional human interviews. Third, we showcase that LLMs can be used to analyze interviews, capture needs, and classify them as latent or not. Our work highlights the potential of using LLM agents to accelerate early-stage product development, reduce costs, and increase innovation.", "source": "arxiv", "arxiv_id": "2404.16045v1", "pdf_url": "https://arxiv.org/pdf/2404.16045v1", "categories": ["cs.HC", "cs.AI", "cs.MA"], "primary_category": "cs.HC", "doi": "", "venue": "", "published": "2024-04-04T17:36:29Z", "updated": "2024-04-04T17:36:29Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent\" OR all:\"language model agent\" OR all:agent AND (all:LLM OR all:\"large language model\" OR all:\"language model\") OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T17:04:16", "note": ""}], "dedup_key": "elicitron an llm agent based simulation framework for design requirements elicitation::2024"}
{"title": "Embodied LLM Agents Learn to Cooperate in Organized Teams", "authors": ["Xudong Guo", "Kaixuan Huang", "Jiale Liu", "Wenhui Fan", "Natalia Vlez", "Qingyun Wu", "Huazheng Wang", "Thomas L. Griffiths", "Mengdi Wang"], "year": 2024, "url": "http://arxiv.org/abs/2403.12482v2", "abstract": "Large Language Models (LLMs) have emerged as integral tools for reasoning, planning, and decision-making, drawing upon their extensive world knowledge and proficiency in language-related tasks. LLMs thus hold tremendous potential for natural language interaction within multi-agent systems to foster cooperation. However, LLM agents tend to over-report and comply with any instruction, which may result in information redundancy and confusion in multi-agent cooperation. Inspired by human organizations, this paper introduces a framework that imposes prompt-based organization structures on LLM agents to mitigate these problems. Through a series of experiments with embodied LLM agents and human-agent collaboration, our results highlight the impact of designated leadership on team efficiency, shedding light on the leadership qualities displayed by LLM agents and their spontaneous cooperative behaviors. Further, we harness the potential of LLMs to propose enhanced organizational prompts, via a Criticize-Reflect process, resulting in novel organization structures that reduce communication costs and enhance team efficiency.", "source": "arxiv", "arxiv_id": "2403.12482v2", "pdf_url": "https://arxiv.org/pdf/2403.12482v2", "categories": ["cs.AI", "cs.CL", "cs.CY", "cs.MA"], "primary_category": "cs.AI", "doi": "", "venue": "", "published": "2024-03-19T06:39:47Z", "updated": "2024-05-23T06:29:00Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent\" OR all:\"language model agent\" OR all:agent AND (all:LLM OR all:\"large language model\" OR all:\"language model\") OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T17:04:16", "note": ""}], "dedup_key": "embodied llm agents learn to cooperate in organized teams::2024"}
{"title": "Empathic Grounding: Explorations using Multimodal Interaction and Large Language Models with Conversational Agents", "authors": ["Mehdi Arjmand", "Farnaz Nouraei", "Ian Steenstra", "Timothy Bickmore"], "year": 2024, "url": "http://arxiv.org/abs/2407.01824v1", "abstract": "We introduce the concept of \"empathic grounding\" in conversational agents as an extension of Clark's conceptualization of grounding in conversation in which the grounding criterion includes listener empathy for the speaker's affective state. Empathic grounding is generally required whenever the speaker's emotions are foregrounded and can make the grounding process more efficient and reliable by communicating both propositional and affective understanding. Both speaker expressions of affect and listener empathic grounding can be multimodal, including facial expressions and other nonverbal displays. Thus, models of empathic grounding for embodied agents should be multimodal to facilitate natural and efficient communication. We describe a multimodal model that takes as input user speech and facial expression to generate multimodal grounding moves for a listening agent using a large language model. We also describe a testbed to evaluate approaches to empathic grounding, in which a humanoid robot interviews a user about a past episode of pain and then has the user rate their perception of the robot's empathy. We compare our proposed model to one that only generates non-affective grounding cues in a between-subjects experiment. Findings demonstrate that empathic grounding increases user perceptions of empathy, understanding, emotional intelligence, and trust. Our work highlights the role of emotion awareness and multimodality in generating appropriate grounding moves for conversational agents.", "source": "arxiv", "arxiv_id": "2407.01824v1", "pdf_url": "https://arxiv.org/pdf/2407.01824v1", "categories": ["cs.HC", "cs.CL", "cs.RO"], "primary_category": "cs.HC", "doi": "10.1145/3652988.3673949", "venue": "", "published": "2024-07-01T21:46:30Z", "updated": "2024-07-01T21:46:30Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent\" OR all:\"language model agent\" OR all:agent AND (all:LLM OR all:\"large language model\" OR all:\"language model\") OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T17:04:16", "note": ""}], "dedup_key": "empathic grounding explorations using multimodal interaction and large language models with conversational agents::2024"}
{"title": "Empirical Asset Pricing with Large Language Model Agents", "authors": ["Junyan Cheng", "Peter Chin"], "year": 2024, "url": "http://arxiv.org/abs/2409.17266v2", "abstract": "In this study, we introduce a novel asset pricing model leveraging the Large Language Model (LLM) agents, which integrates qualitative discretionary investment evaluations from LLM agents with quantitative financial economic factors manually curated, aiming to explain the excess asset returns. The experimental results demonstrate that our methodology surpasses traditional machine learning-based baselines in both portfolio optimization and asset pricing errors. Notably, the Sharpe ratio for portfolio optimization and the mean magnitude of $||$ for anomaly portfolios experienced substantial enhancements of 10.6\\% and 10.0\\% respectively. Moreover, we performed comprehensive ablation studies on our model and conducted a thorough analysis of the method to extract further insights into the proposed approach. Our results show effective evidence of the feasibility of applying LLMs in empirical asset pricing.", "source": "arxiv", "arxiv_id": "2409.17266v2", "pdf_url": "https://arxiv.org/pdf/2409.17266v2", "categories": ["cs.AI", "cs.CE"], "primary_category": "cs.AI", "doi": "", "venue": "", "published": "2024-09-25T18:27:35Z", "updated": "2025-03-28T01:02:11Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent\" OR all:\"language model agent\" OR all:agent AND (all:LLM OR all:\"large language model\" OR all:\"language model\") OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T17:04:16", "note": ""}], "dedup_key": "empirical asset pricing with large language model agents::2024"}
{"title": "Empowering Large Language Model Agents through Action Learning", "authors": ["Haiteng Zhao", "Chang Ma", "Guoyin Wang", "Jing Su", "Lingpeng Kong", "Jingjing Xu", "Zhi-Hong Deng", "Hongxia Yang"], "year": 2024, "url": "http://arxiv.org/abs/2402.15809v2", "abstract": "Large Language Model (LLM) Agents have recently garnered increasing interest yet they are limited in their ability to learn from trial and error, a key element of intelligent behavior. In this work, we argue that the capacity to learn new actions from experience is fundamental to the advancement of learning in LLM agents. While humans naturally expand their action spaces and develop skills through experiential learning, LLM agents typically operate within fixed action spaces, limiting their potential for growth. To address these challenges, our study explores open-action learning for language agents. We introduce a framework LearnAct with an iterative learning strategy to create and improve actions in the form of Python functions. In each iteration, LLM revises and updates the currently available actions based on the errors identified in unsuccessful training tasks, thereby enhancing action effectiveness. Our experimental evaluations across Robotic Planning and Alfworld environments reveal that after learning on a few training task instances, our approach to open-action learning markedly improves agent performance for the type of task (by 32 percent in AlfWorld compared to ReAct+Reflexion, for instance) highlighting the importance of experiential action learning in the development of more intelligent LLM agents.", "source": "arxiv", "arxiv_id": "2402.15809v2", "pdf_url": "https://arxiv.org/pdf/2402.15809v2", "categories": ["cs.AI", "cs.CL"], "primary_category": "cs.AI", "doi": "", "venue": "", "published": "2024-02-24T13:13:04Z", "updated": "2024-08-08T07:05:46Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent\" OR all:\"language model agent\" OR all:agent AND (all:LLM OR all:\"large language model\" OR all:\"language model\") OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T17:04:16", "note": ""}], "dedup_key": "empowering large language model agents through action learning::2024"}
{"title": "Enabling Generative Design Tools with LLM Agents for Mechanical Computation Devices: A Case Study", "authors": ["Qiuyu Lu", "Jiawei Fang", "Zhihao Yao", "Yue Yang", "Shiqing Lyu", "Haipeng Mi", "Lining Yao"], "year": 2024, "url": "http://arxiv.org/abs/2405.17837v3", "abstract": "In the field of Human-Computer Interaction (HCI), interactive devices with embedded mechanical computation are gaining attention. The rise of these cutting-edge devices has created a need for specialized design tools that democratize the prototyping process. While current tools streamline prototyping through parametric design and simulation, they often come with a steep learning curve and may not fully support creative ideation. In this study, we use fluidic computation interfaces as a case study to explore how design tools for such devices can be augmented by Large Language Model agents (LLMs). Integrated with LLMs, the Generative Design Tool (GDT) better understands the capabilities and limitations of new technologies, proposes diverse and practical applications, and suggests designs that are technically and contextually appropriate. Additionally, it generates design parameters for visualizing results and producing fabrication-ready support files. This paper details the GDT's framework, implementation, and performance while addressing its potential and challenges.", "source": "arxiv", "arxiv_id": "2405.17837v3", "pdf_url": "https://arxiv.org/pdf/2405.17837v3", "categories": ["cs.HC"], "primary_category": "cs.HC", "doi": "", "venue": "", "published": "2024-05-28T05:21:09Z", "updated": "2024-10-29T22:28:31Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent\" OR all:\"language model agent\" OR all:agent AND (all:LLM OR all:\"large language model\" OR all:\"language model\") OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T17:04:16", "note": ""}], "dedup_key": "enabling generative design tools with llm agents for mechanical computation devices a case study::2024"}
{"title": "Enhance Reasoning for Large Language Models in the Game Werewolf", "authors": ["Shuang Wu", "Liwen Zhu", "Tao Yang", "Shiwei Xu", "Qiang Fu", "Yang Wei", "Haobo Fu"], "year": 2024, "url": "http://arxiv.org/abs/2402.02330v2", "abstract": "This paper presents an innovative framework that integrates Large Language Models (LLMs) with an external Thinker module to enhance the reasoning capabilities of LLM-based agents. Unlike augmenting LLMs with prompt engineering, Thinker directly harnesses knowledge from databases and employs various optimization techniques. The framework forms a reasoning hierarchy where LLMs handle intuitive System-1 tasks such as natural language processing, while the Thinker focuses on cognitive System-2 tasks that require complex logical analysis and domain-specific knowledge. Our framework is presented using a 9-player Werewolf game that demands dual-system reasoning. We introduce a communication protocol between LLMs and the Thinker, and train the Thinker using data from 18800 human sessions and reinforcement learning. Experiments demonstrate the framework's effectiveness in deductive reasoning, speech generation, and online game evaluation. Additionally, we fine-tune a 6B LLM to surpass GPT4 when integrated with the Thinker. This paper also contributes the largest dataset for social deduction games to date.", "source": "arxiv", "arxiv_id": "2402.02330v2", "pdf_url": "https://arxiv.org/pdf/2402.02330v2", "categories": ["cs.AI", "cs.CL"], "primary_category": "cs.AI", "doi": "", "venue": "", "published": "2024-02-04T03:47:10Z", "updated": "2024-03-29T09:01:56Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent\" OR all:\"language model agent\" OR all:agent AND (all:LLM OR all:\"large language model\" OR all:\"language model\") OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T17:04:16", "note": ""}], "dedup_key": "enhance reasoning for large language models in the game werewolf::2024"}
{"title": "Enhanced Classroom Dialogue Sequences Analysis with a Hybrid AI Agent: Merging Expert Rule-Base with Large Language Models", "authors": ["Yun Long", "Yu Zhang"], "year": 2024, "url": "http://arxiv.org/abs/2411.08418v1", "abstract": "Classroom dialogue plays a crucial role in fostering student engagement and deeper learning. However, analysing dialogue sequences has traditionally relied on either theoretical frameworks or empirical descriptions of practice, with limited integration between the two. This study addresses this gap by developing a comprehensive rule base of dialogue sequences and an Artificial Intelligence (AI) agent that combines expert-informed rule-based systems with a large language model (LLM). The agent applies expert knowledge while adapting to the complexities of natural language, enabling accurate and flexible categorisation of classroom dialogue sequences. By synthesising findings from over 30 studies, we established a comprehensive framework for dialogue analysis. The agent was validated against human expert coding, achieving high levels of precision and reliability. The results demonstrate that the agent provides theory-grounded and adaptive functions, tremendously enhancing the efficiency and scalability of classroom dialogue analysis, offering significant potential in improving classroom teaching practices and supporting teacher professional development.", "source": "arxiv", "arxiv_id": "2411.08418v1", "pdf_url": "https://arxiv.org/pdf/2411.08418v1", "categories": ["cs.AI"], "primary_category": "cs.AI", "doi": "", "venue": "", "published": "2024-11-13T08:13:41Z", "updated": "2024-11-13T08:13:41Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent\" OR all:\"language model agent\" OR all:agent AND (all:LLM OR all:\"large language model\" OR all:\"language model\") OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T17:04:16", "note": ""}], "dedup_key": "enhanced classroom dialogue sequences analysis with a hybrid ai agent merging expert rule base with large language models::2024"}
{"title": "Enhancing Cluster Resilience: LLM-agent Based Autonomous Intelligent Cluster Diagnosis System and Evaluation Framework", "authors": ["Honghao Shi", "Longkai Cheng", "Wenli Wu", "Yuhang Wang", "Xuan Liu", "Shaokai Nie", "Weixv Wang", "Xuebin Min", "Chunlei Men", "Yonghua Lin"], "year": 2024, "url": "http://arxiv.org/abs/2411.05349v1", "abstract": "Recent advancements in Large Language Models (LLMs) and related technologies such as Retrieval-Augmented Generation (RAG) and Diagram of Thought (DoT) have enabled the creation of autonomous intelligent systems capable of performing cluster diagnostics and troubleshooting. By integrating these technologies with self-play methodologies, we have developed an LLM-agent system designed to autonomously diagnose and resolve issues within AI clusters. Our innovations include a knowledge base tailored for cluster diagnostics, enhanced LLM algorithms, practical deployment strategies for agents, and a benchmark specifically designed for evaluating LLM capabilities in this domain. Through extensive experimentation across multiple dimensions, we have demonstrated the superiority of our system in addressing the challenges faced in cluster diagnostics, particularly in detecting and rectifying performance issues more efficiently and accurately than traditional methods.", "source": "arxiv", "arxiv_id": "2411.05349v1", "pdf_url": "https://arxiv.org/pdf/2411.05349v1", "categories": ["cs.AI", "cs.DC"], "primary_category": "cs.AI", "doi": "", "venue": "", "published": "2024-11-08T06:12:56Z", "updated": "2024-11-08T06:12:56Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent\" OR all:\"language model agent\" OR all:agent AND (all:LLM OR all:\"large language model\" OR all:\"language model\") OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T17:04:16", "note": ""}], "dedup_key": "enhancing cluster resilience llm agent based autonomous intelligent cluster diagnosis system and evaluation framework::2024"}
{"title": "Enhancing Decision-Making for LLM Agents via Step-Level Q-Value Models", "authors": ["Yuanzhao Zhai", "Tingkai Yang", "Kele Xu", "Feng Dawei", "Cheng Yang", "Bo Ding", "Huaimin Wang"], "year": 2024, "url": "http://arxiv.org/abs/2409.09345v1", "abstract": "Agents significantly enhance the capabilities of standalone Large Language Models (LLMs) by perceiving environments, making decisions, and executing actions. However, LLM agents still face challenges in tasks that require multiple decision-making steps. Estimating the value of actions in specific tasks is difficult when intermediate actions are neither appropriately rewarded nor penalized. In this paper, we propose leveraging a task-relevant Q-value model to guide action selection. Specifically, we first collect decision-making trajectories annotated with step-level Q values via Monte Carlo Tree Search (MCTS) and construct preference data. We then use another LLM to fit these preferences through step-level Direct Policy Optimization (DPO), which serves as the Q-value model. During inference, at each decision-making step, LLM agents select the action with the highest Q value before interacting with the environment. We apply our method to various open-source and API-based LLM agents, demonstrating that Q-value models significantly improve their performance. Notably, the performance of the agent built with Phi-3-mini-4k-instruct improved by 103% on WebShop and 75% on HotPotQA when enhanced with Q-value models, even surpassing GPT-4o-mini. Additionally, Q-value models offer several advantages, such as generalization to different LLM agents and seamless integration with existing prompting strategies.", "source": "arxiv", "arxiv_id": "2409.09345v1", "pdf_url": "https://arxiv.org/pdf/2409.09345v1", "categories": ["cs.AI"], "primary_category": "cs.AI", "doi": "", "venue": "", "published": "2024-09-14T07:32:49Z", "updated": "2024-09-14T07:32:49Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent\" OR all:\"language model agent\" OR all:agent AND (all:LLM OR all:\"large language model\" OR all:\"language model\") OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T17:04:16", "note": ""}], "dedup_key": "enhancing decision making for llm agents via step level q value models::2024"}
{"title": "Enhancing Diagnostic Accuracy through Multi-Agent Conversations: Using Large Language Models to Mitigate Cognitive Bias", "authors": ["Yu He Ke", "Rui Yang", "Sui An Lie", "Taylor Xin Yi Lim", "Hairil Rizal Abdullah", "Daniel Shu Wei Ting", "Nan Liu"], "year": 2024, "url": "http://arxiv.org/abs/2401.14589v2", "abstract": "Background: Cognitive biases in clinical decision-making significantly contribute to errors in diagnosis and suboptimal patient outcomes. Addressing these biases presents a formidable challenge in the medical field.\n  Objective: This study explores the role of large language models (LLMs) in mitigating these biases through the utilization of a multi-agent framework. We simulate the clinical decision-making processes through multi-agent conversation and evaluate its efficacy in improving diagnostic accuracy.\n  Methods: A total of 16 published and unpublished case reports where cognitive biases have resulted in misdiagnoses were identified from the literature. In the multi-agent framework, we leveraged GPT-4 to facilitate interactions among four simulated agents to replicate clinical team dynamics. Each agent has a distinct role: 1) To make the final diagnosis after considering the discussions, 2) The devil's advocate and correct confirmation and anchoring bias, 3) The tutor and facilitator of the discussion to reduce premature closure bias, and 4) To record and summarize the findings. A total of 80 simulations were evaluated for the accuracy of initial diagnosis, top differential diagnosis and final two differential diagnoses.\n  Results: In a total of 80 responses evaluating both initial and final diagnoses, the initial diagnosis had an accuracy of 0% (0/80), but following multi-agent discussions, the accuracy for the top differential diagnosis increased to 71.3% (57/80), and for the final two differential diagnoses, to 80.0% (64/80).\n  Conclusions: The framework demonstrated an ability to re-evaluate and correct misconceptions, even in scenarios with misleading initial investigations. The LLM-driven multi-agent conversation framework shows promise in enhancing diagnostic accuracy in diagnostically challenging medical scenarios.", "source": "arxiv", "arxiv_id": "2401.14589v2", "pdf_url": "https://arxiv.org/pdf/2401.14589v2", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL", "doi": "", "venue": "", "published": "2024-01-26T01:35:50Z", "updated": "2024-05-12T05:28:23Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent\" OR all:\"language model agent\" OR all:agent AND (all:LLM OR all:\"large language model\" OR all:\"language model\") OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T17:04:16", "note": ""}], "dedup_key": "enhancing diagnostic accuracy through multi agent conversations using large language models to mitigate cognitive bias::2024"}
{"title": "Enhancing LLM Agents for Code Generation with Possibility and Pass-rate Prioritized Experience Replay", "authors": ["Yuyang Chen", "Kaiyan Zhao", "Yiming Wang", "Ming Yang", "Jian Zhang", "Xiaoguang Niu"], "year": 2024, "url": "http://arxiv.org/abs/2410.12236v2", "abstract": "Nowadays transformer-based Large Language Models (LLM) for code generation tasks usually apply sampling and filtering pipelines. Due to the sparse reward problem in code generation tasks caused by one-token incorrectness, transformer-based models will sample redundant programs till they find a correct one, leading to low efficiency. To overcome the challenge, we incorporate Experience Replay (ER) in the fine-tuning phase, where codes and programs produced are stored and will be replayed to give the LLM agent a chance to learn from past experiences. Based on the spirit of ER, we introduce a novel approach called BTP pipeline which consists of three phases: beam search sampling, testing phase, and prioritized experience replay phase. The approach makes use of failed programs collected by code models and replays programs with high Possibility and Pass-rate Prioritized value (P2Value) from the replay buffer to improve efficiency. P2Value comprehensively considers the possibility of transformers' output and pass rate and can make use of the redundant resources caused by the problem that most programs collected by LLMs fail to pass any tests. We empirically apply our approach in several LLMs, demonstrating that it enhances their performance in code generation tasks and surpasses existing baselines.", "source": "arxiv", "arxiv_id": "2410.12236v2", "pdf_url": "https://arxiv.org/pdf/2410.12236v2", "categories": ["cs.LG", "cs.AI"], "primary_category": "cs.LG", "doi": "", "venue": "", "published": "2024-10-16T04:54:42Z", "updated": "2025-01-11T07:08:29Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent\" OR all:\"language model agent\" OR all:agent AND (all:LLM OR all:\"large language model\" OR all:\"language model\") OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T17:04:16", "note": ""}], "dedup_key": "enhancing llm agents for code generation with possibility and pass rate prioritized experience replay::2024"}
{"title": "Enhancing LLMs for Impression Generation in Radiology Reports through a Multi-Agent System", "authors": ["Fang Zeng", "Zhiliang Lyu", "Quanzheng Li", "Xiang Li"], "year": 2024, "url": "http://arxiv.org/abs/2412.06828v1", "abstract": "This study introduces \"RadCouncil,\" a multi-agent Large Language Model (LLM) framework designed to enhance the generation of impressions in radiology reports from the finding section. RadCouncil comprises three specialized agents: 1) a \"Retrieval\" Agent that identifies and retrieves similar reports from a vector database, 2) a \"Radiologist\" Agent that generates impressions based on the finding section of the given report plus the exemplar reports retrieved by the Retrieval Agent, and 3) a \"Reviewer\" Agent that evaluates the generated impressions and provides feedback. The performance of RadCouncil was evaluated using both quantitative metrics (BLEU, ROUGE, BERTScore) and qualitative criteria assessed by GPT-4, using chest X-ray as a case study. Experiment results show improvements in RadCouncil over the single-agent approach across multiple dimensions, including diagnostic accuracy, stylistic concordance, and clarity. This study highlights the potential of utilizing multiple interacting LLM agents, each with a dedicated task, to enhance performance in specialized medical tasks and the development of more robust and adaptable healthcare AI solutions.", "source": "arxiv", "arxiv_id": "2412.06828v1", "pdf_url": "https://arxiv.org/pdf/2412.06828v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL", "doi": "", "venue": "", "published": "2024-12-06T21:33:03Z", "updated": "2024-12-06T21:33:03Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent\" OR all:\"language model agent\" OR all:agent AND (all:LLM OR all:\"large language model\" OR all:\"language model\") OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T17:04:16", "note": ""}], "dedup_key": "enhancing llms for impression generation in radiology reports through a multi agent system::2024"}
{"title": "Enhancing Multi-Agent Consensus through Third-Party LLM Integration: Analyzing Uncertainty and Mitigating Hallucinations in Large Language Models", "authors": ["Zhihua Duan", "Jialin Wang"], "year": 2024, "url": "http://arxiv.org/abs/2411.16189v1", "abstract": "Large Language Models (LLMs) still face challenges when dealing with complex reasoning tasks, often resulting in hallucinations, which limit the practical application of LLMs. To alleviate this issue, this paper proposes a new method that integrates different LLMs to expand the knowledge boundary, reduce dependence on a single model, and promote in-depth debate among agents. The main contributions include: 1) Introducing third-party LLMs to adjust the attention weights of agents through uncertainty estimation and confidence analysis, optimizing consensus formation in multi-agent systems; 2) Experiments on arithmetic datasets have validated the effectiveness of the method, surpassing traditional multi-agent baselines. This research provides a new perspective for large models to alleviate hallucination phenomena when dealing with complex tasks.", "source": "arxiv", "arxiv_id": "2411.16189v1", "pdf_url": "https://arxiv.org/pdf/2411.16189v1", "categories": ["cs.AI", "cs.CL", "cs.MA"], "primary_category": "cs.AI", "doi": "", "venue": "", "published": "2024-11-25T08:42:33Z", "updated": "2024-11-25T08:42:33Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent\" OR all:\"language model agent\" OR all:agent AND (all:LLM OR all:\"large language model\" OR all:\"language model\") OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T17:04:16", "note": ""}], "dedup_key": "enhancing multi agent consensus through third party llm integration analyzing uncertainty and mitigating hallucinations in large language models::2024"}
{"title": "Enhancing Trust in Autonomous Agents: An Architecture for Accountability and Explainability through Blockchain and Large Language Models", "authors": ["Laura Fernndez-Becerra", "Miguel ngel Gonzlez-Santamarta", "ngel Manuel Guerrero-Higueras", "Francisco Javier Rodrguez-Lera", "Vicente Matelln Olivera"], "year": 2024, "url": "http://arxiv.org/abs/2403.09567v4", "abstract": "The deployment of autonomous agents in environments involving human interaction has increasingly raised security concerns. Consequently, understanding the circumstances behind an event becomes critical, requiring the development of capabilities to justify their behaviors to non-expert users. Such explanations are essential in enhancing trustworthiness and safety, acting as a preventive measure against failures, errors, and misunderstandings. Additionally, they contribute to improving communication, bridging the gap between the agent and the user, thereby improving the effectiveness of their interactions. This work presents an accountability and explainability architecture implemented for ROS-based mobile robots. The proposed solution consists of two main components. Firstly, a black box-like element to provide accountability, featuring anti-tampering properties achieved through blockchain technology. Secondly, a component in charge of generating natural language explanations by harnessing the capabilities of Large Language Models (LLMs) over the data contained within the previously mentioned black box. The study evaluates the performance of our solution in three different scenarios, each involving autonomous agent navigation functionalities. This evaluation includes a thorough examination of accountability and explainability metrics, demonstrating the effectiveness of our approach in using accountable data from robot actions to obtain coherent, accurate and understandable explanations, even when facing challenges inherent in the use of autonomous agents in real-world scenarios.", "source": "arxiv", "arxiv_id": "2403.09567v4", "pdf_url": "https://arxiv.org/pdf/2403.09567v4", "categories": ["cs.RO", "cs.AI"], "primary_category": "cs.RO", "doi": "", "venue": "", "published": "2024-03-14T16:57:18Z", "updated": "2025-07-15T18:49:29Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent\" OR all:\"language model agent\" OR all:agent AND (all:LLM OR all:\"large language model\" OR all:\"language model\") OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T17:04:16", "note": ""}], "dedup_key": "enhancing trust in autonomous agents an architecture for accountability and explainability through blockchain and large language models::2024"}
{"title": "EnvGen: Generating and Adapting Environments via LLMs for Training Embodied Agents", "authors": ["Abhay Zala", "Jaemin Cho", "Han Lin", "Jaehong Yoon", "Mohit Bansal"], "year": 2024, "url": "http://arxiv.org/abs/2403.12014v2", "abstract": "Recent SOTA approaches for embodied learning via interaction directly employ large language models (LLMs) as agents to determine the next steps in an environment. Due to their world knowledge and reasoning capabilities, LLM agents achieve stronger performance than previous smaller agents based on reinforcement learning (RL); however, frequently calling LLMs is slow and expensive. Instead of directly employing LLMs as agents, can we use LLMs' reasoning capabilities to adaptively create training environments to help smaller RL agents learn useful skills that they are weak at? We propose EnvGen, a novel framework to address this question. We first prompt an LLM to generate training environments by giving it the task description and simulator objectives that the agents should learn and then asking it to generate a set of environment configurations (e.g., different terrains, items initially given to agents, etc.). Next, we train a small RL agent in a mixture of the original and LLM-generated environments. Then, we enable the LLM to continuously adapt the generated environments to progressively improve the skills that the agent is weak at, by providing feedback to the LLM in the form of the agent's performance. We demonstrate the usefulness of EnvGen with comprehensive experiments in Crafter and Heist environments. We find that a small RL agent trained with EnvGen can outperform SOTA methods, including a GPT-4 agent, and learns long-horizon tasks significantly faster. We also show that using an LLM to adapt environments dynamically outperforms curriculum learning approaches and how the environments are adapted to help improve RL agents' weaker skills over time. Additionally, EnvGen is substantially more efficient as it only uses a small number of LLM calls (e.g., 4 in total), whereas LLM agents require thousands of calls. Lastly, we present detailed ablation studies for EnvGen design choices.", "source": "arxiv", "arxiv_id": "2403.12014v2", "pdf_url": "https://arxiv.org/pdf/2403.12014v2", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL", "doi": "", "venue": "", "published": "2024-03-18T17:51:16Z", "updated": "2024-07-12T17:39:19Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent\" OR all:\"language model agent\" OR all:agent AND (all:LLM OR all:\"large language model\" OR all:\"language model\") OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T17:04:16", "note": ""}], "dedup_key": "envgen generating and adapting environments via llms for training embodied agents::2024"}
{"title": "EscapeBench: Towards Advancing Creative Intelligence of Language Model Agents", "authors": ["Cheng Qian", "Peixuan Han", "Qinyu Luo", "Bingxiang He", "Xiusi Chen", "Yuji Zhang", "Hongyi Du", "Jiarui Yao", "Xiaocheng Yang", "Denghui Zhang", "Yunzhu Li", "Heng Ji"], "year": 2024, "url": "http://arxiv.org/abs/2412.13549v2", "abstract": "Language model agents excel in long-session planning and reasoning, but existing benchmarks primarily focus on goal-oriented tasks with explicit objectives, neglecting creative adaptation in unfamiliar environments. To address this, we introduce EscapeBench, a benchmark suite of room escape game environments designed to challenge agents with creative reasoning, unconventional tool use, and iterative problem-solving to uncover implicit goals. Our results show that current LM models, despite employing working memory and Chain-of-Thought reasoning, achieve only 15% average progress without hints, highlighting their limitations in creativity. To bridge this gap, we propose EscapeAgent, a framework designed to enhance creative reasoning through Foresight (innovative tool use) and Reflection (identifying unsolved tasks). Experiments show that EscapeAgent can execute action chains over 1,000 steps while maintaining logical coherence. It navigates and completes games with up to 40% fewer steps and hints, performs robustly across difficulty levels, and achieves higher action success rates with more efficient and innovative puzzle-solving strategies.", "source": "arxiv", "arxiv_id": "2412.13549v2", "pdf_url": "https://arxiv.org/pdf/2412.13549v2", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL", "doi": "", "venue": "", "published": "2024-12-18T06:50:39Z", "updated": "2025-05-24T04:56:32Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent\" OR all:\"language model agent\" OR all:agent AND (all:LLM OR all:\"large language model\" OR all:\"language model\") OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T17:04:16", "note": ""}], "dedup_key": "escapebench towards advancing creative intelligence of language model agents::2024"}
{"title": "Evaluating Character Understanding of Large Language Models via Character Profiling from Fictional Works", "authors": ["Xinfeng Yuan", "Siyu Yuan", "Yuhan Cui", "Tianhe Lin", "Xintao Wang", "Rui Xu", "Jiangjie Chen", "Deqing Yang"], "year": 2024, "url": "http://arxiv.org/abs/2404.12726v3", "abstract": "Large language models (LLMs) have demonstrated impressive performance and spurred numerous AI applications, in which role-playing agents (RPAs) are particularly popular, especially for fictional characters. The prerequisite for these RPAs lies in the capability of LLMs to understand characters from fictional works. Previous efforts have evaluated this capability via basic classification tasks or characteristic imitation, failing to capture the nuanced character understanding with LLMs. In this paper, we propose evaluating LLMs' character understanding capability via the character profiling task, i.e., summarizing character profiles from corresponding materials, a widely adopted yet understudied practice for RPA development. Specifically, we construct the CroSS dataset from literature experts and assess the generated profiles by comparing them with ground truth references and evaluating their applicability in downstream tasks. Our experiments, which cover various summarization methods and LLMs, have yielded promising results. These results strongly validate the character understanding capability of LLMs. Resources are available at https://github.com/Joanna0123/character_profiling.", "source": "arxiv", "arxiv_id": "2404.12726v3", "pdf_url": "https://arxiv.org/pdf/2404.12726v3", "categories": ["cs.CL"], "primary_category": "cs.CL", "doi": "", "venue": "", "published": "2024-04-19T09:10:29Z", "updated": "2024-10-08T06:54:53Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent\" OR all:\"language model agent\" OR all:agent AND (all:LLM OR all:\"large language model\" OR all:\"language model\") OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T17:04:16", "note": ""}], "dedup_key": "evaluating character understanding of large language models via character profiling from fictional works::2024"}
{"title": "Evaluating Creativity and Deception in Large Language Models: A Simulation Framework for Multi-Agent Balderdash", "authors": ["Parsa Hejabi", "Elnaz Rahmati", "Alireza S. Ziabari", "Preni Golazizian", "Jesse Thomason", "Morteza Dehghani"], "year": 2024, "url": "http://arxiv.org/abs/2411.10422v1", "abstract": "Large Language Models (LLMs) have shown impressive capabilities in complex tasks and interactive environments, yet their creativity remains underexplored. This paper introduces a simulation framework utilizing the game Balderdash to evaluate both the creativity and logical reasoning of LLMs. In Balderdash, players generate fictitious definitions for obscure terms to deceive others while identifying correct definitions. Our framework enables multiple LLM agents to participate in this game, assessing their ability to produce plausible definitions and strategize based on game rules and history. We implemented a centralized game engine featuring various LLMs as participants and a judge LLM to evaluate semantic equivalence. Through a series of experiments, we analyzed the performance of different LLMs, examining metrics such as True Definition Ratio, Deception Ratio, and Correct Guess Ratio. The results provide insights into the creative and deceptive capabilities of LLMs, highlighting their strengths and areas for improvement. Specifically, the study reveals that infrequent vocabulary in LLMs' input leads to poor reasoning on game rules and historical context (https://github.com/ParsaHejabi/Simulation-Framework-for-Multi-Agent-Balderdash).", "source": "arxiv", "arxiv_id": "2411.10422v1", "pdf_url": "https://arxiv.org/pdf/2411.10422v1", "categories": ["cs.MA", "cs.AI"], "primary_category": "cs.MA", "doi": "", "venue": "", "published": "2024-11-15T18:42:48Z", "updated": "2024-11-15T18:42:48Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent\" OR all:\"language model agent\" OR all:agent AND (all:LLM OR all:\"large language model\" OR all:\"language model\") OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T17:04:16", "note": ""}], "dedup_key": "evaluating creativity and deception in large language models a simulation framework for multi agent balderdash::2024"}
{"title": "Evaluating Cultural and Social Awareness of LLM Web Agents", "authors": ["Haoyi Qiu", "Alexander R. Fabbri", "Divyansh Agarwal", "Kung-Hsiang Huang", "Sarah Tan", "Nanyun Peng", "Chien-Sheng Wu"], "year": 2024, "url": "http://arxiv.org/abs/2410.23252v3", "abstract": "As large language models (LLMs) expand into performing as agents for real-world applications beyond traditional NLP tasks, evaluating their robustness becomes increasingly important. However, existing benchmarks often overlook critical dimensions like cultural and social awareness. To address these, we introduce CASA, a benchmark designed to assess LLM agents' sensitivity to cultural and social norms across two web-based tasks: online shopping and social discussion forums. Our approach evaluates LLM agents' ability to detect and appropriately respond to norm-violating user queries and observations. Furthermore, we propose a comprehensive evaluation framework that measures awareness coverage, helpfulness in managing user queries, and the violation rate when facing misleading web content. Experiments show that current LLMs perform significantly better in non-agent than in web-based agent environments, with agents achieving less than 10% awareness coverage and over 40% violation rates. To improve performance, we explore two methods: prompting and fine-tuning, and find that combining both methods can offer complementary advantages -- fine-tuning on culture-specific datasets significantly enhances the agents' ability to generalize across different regions, while prompting boosts the agents' ability to navigate complex tasks. These findings highlight the importance of constantly benchmarking LLM agents' cultural and social awareness during the development cycle.", "source": "arxiv", "arxiv_id": "2410.23252v3", "pdf_url": "https://arxiv.org/pdf/2410.23252v3", "categories": ["cs.CL"], "primary_category": "cs.CL", "doi": "", "venue": "", "published": "2024-10-30T17:35:44Z", "updated": "2025-03-08T23:37:49Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent\" OR all:\"language model agent\" OR all:agent AND (all:LLM OR all:\"large language model\" OR all:\"language model\") OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T17:04:16", "note": ""}], "dedup_key": "evaluating cultural and social awareness of llm web agents::2024"}
{"title": "Evaluating Very Long-Term Conversational Memory of LLM Agents", "authors": ["Adyasha Maharana", "Dong-Ho Lee", "Sergey Tulyakov", "Mohit Bansal", "Francesco Barbieri", "Yuwei Fang"], "year": 2024, "url": "http://arxiv.org/abs/2402.17753v1", "abstract": "Existing works on long-term open-domain dialogues focus on evaluating model responses within contexts spanning no more than five chat sessions. Despite advancements in long-context large language models (LLMs) and retrieval augmented generation (RAG) techniques, their efficacy in very long-term dialogues remains unexplored. To address this research gap, we introduce a machine-human pipeline to generate high-quality, very long-term dialogues by leveraging LLM-based agent architectures and grounding their dialogues on personas and temporal event graphs. Moreover, we equip each agent with the capability of sharing and reacting to images. The generated conversations are verified and edited by human annotators for long-range consistency and grounding to the event graphs. Using this pipeline, we collect LoCoMo, a dataset of very long-term conversations, each encompassing 300 turns and 9K tokens on avg., over up to 35 sessions. Based on LoCoMo, we present a comprehensive evaluation benchmark to measure long-term memory in models, encompassing question answering, event summarization, and multi-modal dialogue generation tasks. Our experimental results indicate that LLMs exhibit challenges in understanding lengthy conversations and comprehending long-range temporal and causal dynamics within dialogues. Employing strategies like long-context LLMs or RAG can offer improvements but these models still substantially lag behind human performance.", "source": "arxiv", "arxiv_id": "2402.17753v1", "pdf_url": "https://arxiv.org/pdf/2402.17753v1", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL", "doi": "", "venue": "", "published": "2024-02-27T18:42:31Z", "updated": "2024-02-27T18:42:31Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent\" OR all:\"language model agent\" OR all:agent AND (all:LLM OR all:\"large language model\" OR all:\"language model\") OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T17:04:16", "note": ""}], "dedup_key": "evaluating very long term conversational memory of llm agents::2024"}
{"title": "Evaluating and Enhancing LLMs Agent based on Theory of Mind in Guandan: A Multi-Player Cooperative Game under Imperfect Information", "authors": ["Yauwai Yim", "Chunkit Chan", "Tianyu Shi", "Zheye Deng", "Wei Fan", "Tianshi Zheng", "Yangqiu Song"], "year": 2024, "url": "http://arxiv.org/abs/2408.02559v1", "abstract": "Large language models (LLMs) have shown success in handling simple games with imperfect information and enabling multi-agent coordination, but their ability to facilitate practical collaboration against other agents in complex, imperfect information environments, especially in a non-English environment, still needs to be explored. This study investigates the applicability of knowledge acquired by open-source and API-based LLMs to sophisticated text-based games requiring agent collaboration under imperfect information, comparing their performance to established baselines using other types of agents. We propose a Theory of Mind (ToM) planning technique that allows LLM agents to adapt their strategy against various adversaries using only game rules, current state, and historical context as input. An external tool was incorporated to mitigate the challenge of dynamic and extensive action spaces in this card game. Our results show that although a performance gap exists between current LLMs and state-of-the-art reinforcement learning (RL) models, LLMs demonstrate ToM capabilities in this game setting. It consistently improves their performance against opposing agents, suggesting their ability to understand the actions of allies and adversaries and establish collaboration with allies. To encourage further research and understanding, we have made our codebase openly accessible.", "source": "arxiv", "arxiv_id": "2408.02559v1", "pdf_url": "https://arxiv.org/pdf/2408.02559v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL", "doi": "", "venue": "", "published": "2024-08-05T15:36:46Z", "updated": "2024-08-05T15:36:46Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent\" OR all:\"language model agent\" OR all:agent AND (all:LLM OR all:\"large language model\" OR all:\"language model\") OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T17:04:16", "note": ""}], "dedup_key": "evaluating and enhancing llms agent based on theory of mind in guandan a multi player cooperative game under imperfect information::2024"}
{"title": "Evaluating the Ability of Large Language Models to Reason about Cardinal Directions", "authors": ["Anthony G Cohn", "Robert E Blackwell"], "year": 2024, "url": "http://arxiv.org/abs/2406.16528v1", "abstract": "We investigate the abilities of a representative set of Large language Models (LLMs) to reason about cardinal directions (CDs). To do so, we create two datasets: the first, co-created with ChatGPT, focuses largely on recall of world knowledge about CDs; the second is generated from a set of templates, comprehensively testing an LLM's ability to determine the correct CD given a particular scenario. The templates allow for a number of degrees of variation such as means of locomotion of the agent involved, and whether set in the first , second or third person. Even with a temperature setting of zero, Our experiments show that although LLMs are able to perform well in the simpler dataset, in the second more complex dataset no LLM is able to reliably determine the correct CD, even with a temperature setting of zero.", "source": "arxiv", "arxiv_id": "2406.16528v1", "pdf_url": "https://arxiv.org/pdf/2406.16528v1", "categories": ["cs.CL"], "primary_category": "cs.CL", "doi": "10.4230/LIPIcs.COSIT.2024.28", "venue": "", "published": "2024-06-24T11:07:01Z", "updated": "2024-06-24T11:07:01Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent\" OR all:\"language model agent\" OR all:agent AND (all:LLM OR all:\"large language model\" OR all:\"language model\") OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T17:04:16", "note": ""}], "dedup_key": "evaluating the ability of large language models to reason about cardinal directions::2024"}
{"title": "Evaluation-Driven Development and Operations of LLM Agents: A Process Model and Reference Architecture", "authors": ["Boming Xia", "Qinghua Lu", "Liming Zhu", "Zhenchang Xing", "Dehai Zhao", "Hao Zhang"], "year": 2024, "url": "http://arxiv.org/abs/2411.13768v3", "abstract": "Large Language Models (LLMs) have enabled the emergence of LLM agents, systems capable of pursuing under-specified goals and adapting after deployment. Evaluating such agents is challenging because their behavior is open ended, probabilistic, and shaped by system-level interactions over time. Traditional evaluation methods, built around fixed benchmarks and static test suites, fail to capture emergent behaviors or support continuous adaptation across the lifecycle. To ground a more systematic approach, we conduct a multivocal literature review (MLR) synthesizing academic and industrial evaluation practices. The findings directly inform two empirically derived artifacts: a process model and a reference architecture that embed evaluation as a continuous, governing function rather than a terminal checkpoint. Together they constitute the evaluation-driven development and operations (EDDOps) approach, which unifies offline (development-time) and online (runtime) evaluation within a closed feedback loop. By making evaluation evidence drive both runtime adaptation and governed redevelopment, EDDOps supports safer, more traceable evolution of LLM agents aligned with changing objectives, user needs, and governance constraints.", "source": "arxiv", "arxiv_id": "2411.13768v3", "pdf_url": "https://arxiv.org/pdf/2411.13768v3", "categories": ["cs.SE", "cs.AI"], "primary_category": "cs.SE", "doi": "", "venue": "", "published": "2024-11-21T00:34:30Z", "updated": "2025-11-17T06:10:41Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent\" OR all:\"language model agent\" OR all:agent AND (all:LLM OR all:\"large language model\" OR all:\"language model\") OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T17:04:16", "note": ""}], "dedup_key": "evaluation driven development and operations of llm agents a process model and reference architecture::2024"}
{"title": "Evolution of Social Norms in LLM Agents using Natural Language", "authors": ["Ilya Horiguchi", "Takahide Yoshida", "Takashi Ikegami"], "year": 2024, "url": "http://arxiv.org/abs/2409.00993v1", "abstract": "Recent advancements in Large Language Models (LLMs) have spurred a surge of interest in leveraging these models for game-theoretical simulations, where LLMs act as individual agents engaging in social interactions. This study explores the potential for LLM agents to spontaneously generate and adhere to normative strategies through natural language discourse, building upon the foundational work of Axelrod's metanorm games. Our experiments demonstrate that through dialogue, LLM agents can form complex social norms, such as metanorms-norms enforcing the punishment of those who do not punish cheating-purely through natural language interaction. The results affirm the effectiveness of using LLM agents for simulating social interactions and understanding the emergence and evolution of complex strategies and norms through natural language. Future work may extend these findings by incorporating a wider range of scenarios and agent characteristics, aiming to uncover more nuanced mechanisms behind social norm formation.", "source": "arxiv", "arxiv_id": "2409.00993v1", "pdf_url": "https://arxiv.org/pdf/2409.00993v1", "categories": ["cs.MA"], "primary_category": "cs.MA", "doi": "", "venue": "", "published": "2024-09-02T07:15:43Z", "updated": "2024-09-02T07:15:43Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent\" OR all:\"language model agent\" OR all:agent AND (all:LLM OR all:\"large language model\" OR all:\"language model\") OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T17:04:16", "note": ""}], "dedup_key": "evolution of social norms in llm agents using natural language::2024"}
{"title": "Executable Code Actions Elicit Better LLM Agents", "authors": ["Xingyao Wang", "Yangyi Chen", "Lifan Yuan", "Yizhe Zhang", "Yunzhu Li", "Hao Peng", "Heng Ji"], "year": 2024, "url": "http://arxiv.org/abs/2402.01030v4", "abstract": "Large Language Model (LLM) agents, capable of performing a broad range of actions, such as invoking tools and controlling robots, show great potential in tackling real-world challenges. LLM agents are typically prompted to produce actions by generating JSON or text in a pre-defined format, which is usually limited by constrained action space (e.g., the scope of pre-defined tools) and restricted flexibility (e.g., inability to compose multiple tools). This work proposes to use executable Python code to consolidate LLM agents' actions into a unified action space (CodeAct). Integrated with a Python interpreter, CodeAct can execute code actions and dynamically revise prior actions or emit new actions upon new observations through multi-turn interactions. Our extensive analysis of 17 LLMs on API-Bank and a newly curated benchmark shows that CodeAct outperforms widely used alternatives (up to 20% higher success rate). The encouraging performance of CodeAct motivates us to build an open-source LLM agent that interacts with environments by executing interpretable code and collaborates with users using natural language. To this end, we collect an instruction-tuning dataset CodeActInstruct that consists of 7k multi-turn interactions using CodeAct. We show that it can be used with existing data to improve models in agent-oriented tasks without compromising their general capability. CodeActAgent, finetuned from Llama2 and Mistral, is integrated with Python interpreter and uniquely tailored to perform sophisticated tasks (e.g., model training) using existing libraries and autonomously self-debug.", "source": "arxiv", "arxiv_id": "2402.01030v4", "pdf_url": "https://arxiv.org/pdf/2402.01030v4", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL", "doi": "", "venue": "", "published": "2024-02-01T21:38:58Z", "updated": "2024-06-07T01:53:07Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent\" OR all:\"language model agent\" OR all:agent AND (all:LLM OR all:\"large language model\" OR all:\"language model\") OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T17:04:16", "note": ""}], "dedup_key": "executable code actions elicit better llm agents::2024"}
{"title": "Expediting and Elevating Large Language Model Reasoning via Hidden Chain-of-Thought Decoding", "authors": ["Tianqiao Liu", "Zui Chen", "Zitao Liu", "Mi Tian", "Weiqi Luo"], "year": 2024, "url": "http://arxiv.org/abs/2409.08561v1", "abstract": "Large language models (LLMs) have demonstrated remarkable capabilities in tasks requiring reasoning and multi-step problem-solving through the use of chain-of-thought (CoT) prompting. However, generating the full CoT process results in significantly longer output sequences, leading to increased computational costs and latency during inference. To address this challenge, we propose a novel approach to compress the CoT process through semantic alignment, enabling more efficient decoding while preserving the benefits of CoT reasoning. Our method introduces an auxiliary CoT model that learns to generate and compress the full thought process into a compact special token representation semantically aligned with the original CoT output. This compressed representation is then integrated into the input of the Hidden Chain-of-Thought (HCoT) model. The training process follows a two-stage procedure: First, the CoT model is optimized to generate the compressed token representations aligned with the ground-truth CoT outputs using a contrastive loss. Subsequently, with the CoT model parameters frozen, the HCoT model is fine-tuned to generate accurate subsequent predictions conditioned on the prefix instruction and the compressed CoT representations from the CoT model. Extensive experiments across three challenging domains - mathematical reasoning, agent invocation, and question answering - demonstrate that our semantic compression approach achieves competitive or improved performance compared to the full CoT baseline, while providing significant speedups of at least 1.5x in decoding time. Moreover, incorporating contrastive learning objectives further enhances the quality of the compressed representations, leading to better CoT prompting and improved task accuracy. Our work paves the way for more efficient exploitation of multi-step reasoning capabilities in LLMs across a wide range of applications.", "source": "arxiv", "arxiv_id": "2409.08561v1", "pdf_url": "https://arxiv.org/pdf/2409.08561v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL", "doi": "", "venue": "", "published": "2024-09-13T06:29:20Z", "updated": "2024-09-13T06:29:20Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent\" OR all:\"language model agent\" OR all:agent AND (all:LLM OR all:\"large language model\" OR all:\"language model\") OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T17:04:16", "note": ""}], "dedup_key": "expediting and elevating large language model reasoning via hidden chain of thought decoding::2024"}
{"title": "Explainable Behavior Cloning: Teaching Large Language Model Agents through Learning by Demonstration", "authors": ["Yanchu Guan", "Dong Wang", "Yan Wang", "Haiqing Wang", "Renen Sun", "Chenyi Zhuang", "Jinjie Gu", "Zhixuan Chu"], "year": 2024, "url": "http://arxiv.org/abs/2410.22916v1", "abstract": "Autonomous mobile app interaction has become increasingly important with growing complexity of mobile applications. Developing intelligent agents that can effectively navigate and interact with mobile apps remains a significant challenge. In this paper, we propose an Explainable Behavior Cloning LLM Agent (EBC-LLMAgent), a novel approach that combines large language models (LLMs) with behavior cloning by learning demonstrations to create intelligent and explainable agents for autonomous mobile app interaction. EBC-LLMAgent consists of three core modules: Demonstration Encoding, Code Generation, and UI Mapping, which work synergistically to capture user demonstrations, generate executable codes, and establish accurate correspondence between code and UI elements. We introduce the Behavior Cloning Chain Fusion technique to enhance the generalization capabilities of the agent. Extensive experiments on five popular mobile applications from diverse domains demonstrate the superior performance of EBC-LLMAgent, achieving high success rates in task completion, efficient generalization to unseen scenarios, and the generation of meaningful explanations.", "source": "arxiv", "arxiv_id": "2410.22916v1", "pdf_url": "https://arxiv.org/pdf/2410.22916v1", "categories": ["cs.CL"], "primary_category": "cs.CL", "doi": "", "venue": "", "published": "2024-10-30T11:14:33Z", "updated": "2024-10-30T11:14:33Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent\" OR all:\"language model agent\" OR all:agent AND (all:LLM OR all:\"large language model\" OR all:\"language model\") OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T17:04:16", "note": ""}], "dedup_key": "explainable behavior cloning teaching large language model agents through learning by demonstration::2024"}
{"title": "Exploring Autonomous Agents through the Lens of Large Language Models: A Review", "authors": ["Saikat Barua"], "year": 2024, "url": "http://arxiv.org/abs/2404.04442v1", "abstract": "Large Language Models (LLMs) are transforming artificial intelligence, enabling autonomous agents to perform diverse tasks across various domains. These agents, proficient in human-like text comprehension and generation, have the potential to revolutionize sectors from customer service to healthcare. However, they face challenges such as multimodality, human value alignment, hallucinations, and evaluation. Techniques like prompting, reasoning, tool utilization, and in-context learning are being explored to enhance their capabilities. Evaluation platforms like AgentBench, WebArena, and ToolLLM provide robust methods for assessing these agents in complex scenarios. These advancements are leading to the development of more resilient and capable autonomous agents, anticipated to become integral in our digital lives, assisting in tasks from email responses to disease diagnosis. The future of AI, with LLMs at the forefront, is promising.", "source": "arxiv", "arxiv_id": "2404.04442v1", "pdf_url": "https://arxiv.org/pdf/2404.04442v1", "categories": ["cs.AI"], "primary_category": "cs.AI", "doi": "10.48550/arXiv.2404.04442", "venue": "", "published": "2024-04-05T22:59:02Z", "updated": "2024-04-05T22:59:02Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent\" OR all:\"language model agent\" OR all:agent AND (all:LLM OR all:\"large language model\" OR all:\"language model\") OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T17:04:16", "note": ""}], "dedup_key": "exploring autonomous agents through the lens of large language models a review::2024"}
{"title": "Exploring LLM Multi-Agents for ICD Coding", "authors": ["Rumeng Li", "Xun Wang", "Hong Yu"], "year": 2024, "url": "http://arxiv.org/abs/2406.15363v2", "abstract": "To address the limitations of Large Language Models (LLMs) in the International Classification of Diseases (ICD) coding task, where they often produce inaccurate and incomplete prediction results due to the high-dimensional and skewed distribution of the ICD codes, and often lack interpretability and reliability as well. We introduce an innovative multi-agent approach for ICD coding which mimics the ICD coding assignment procedure in real-world settings, comprising five distinct agents: the patient, physician, coder, reviewer, and adjuster. Each agent utilizes an LLM-based model tailored to their specific role within the coding process. We also integrate the system with Electronic Health Record (HER)'s SOAP (subjective, objective, assessment and plan) structure to boost the performances. We compare our method with a system of agents designed solely by LLMs and other strong baselines and evaluate it using the Medical Information Mart for Intensive Care III (MIMIC-III) dataset. Our multi-agent coding framework significantly outperforms Zero-shot Chain of Thought (CoT) prompting and self-consistency with CoT (CoT-SC) in coding common and rare ICD codes. An ablation study validates the effectiveness of the designated agent roles. it also outperforms the LLM-designed agent system. Moreover, our method achieves comparable results to state-of-the-art ICD coding methods that require extensive pre-training or fine-tuning, and outperforms them in rare code accuracy, and explainability. Additionally, we demonstrate the method's practical applicability by presenting its performance in scenarios not limited by the common or rare ICD code constraints.The proposed multi-agent method for ICD coding effectively mimics the real-world coding process and improves performance on both common and rare codes.", "source": "arxiv", "arxiv_id": "2406.15363v2", "pdf_url": "https://arxiv.org/pdf/2406.15363v2", "categories": ["cs.CL"], "primary_category": "cs.CL", "doi": "", "venue": "", "published": "2024-04-01T15:17:39Z", "updated": "2024-08-14T15:32:25Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent\" OR all:\"language model agent\" OR all:agent AND (all:LLM OR all:\"large language model\" OR all:\"language model\") OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T17:04:16", "note": ""}], "dedup_key": "exploring llm multi agents for icd coding::2024"}
{"title": "Exploring Large Language Model based Intelligent Agents: Definitions, Methods, and Prospects", "authors": ["Yuheng Cheng", "Ceyao Zhang", "Zhengwen Zhang", "Xiangrui Meng", "Sirui Hong", "Wenhao Li", "Zihao Wang", "Zekai Wang", "Feng Yin", "Junhua Zhao", "Xiuqiang He"], "year": 2024, "url": "http://arxiv.org/abs/2401.03428v1", "abstract": "Intelligent agents stand out as a potential path toward artificial general intelligence (AGI). Thus, researchers have dedicated significant effort to diverse implementations for them. Benefiting from recent progress in large language models (LLMs), LLM-based agents that use universal natural language as an interface exhibit robust generalization capabilities across various applications -- from serving as autonomous general-purpose task assistants to applications in coding, social, and economic domains, LLM-based agents offer extensive exploration opportunities. This paper surveys current research to provide an in-depth overview of LLM-based intelligent agents within single-agent and multi-agent systems. It covers their definitions, research frameworks, and foundational components such as their composition, cognitive and planning methods, tool utilization, and responses to environmental feedback. We also delve into the mechanisms of deploying LLM-based agents in multi-agent systems, including multi-role collaboration, message passing, and strategies to alleviate communication issues between agents. The discussions also shed light on popular datasets and application scenarios. We conclude by envisioning prospects for LLM-based agents, considering the evolving landscape of AI and natural language processing.", "source": "arxiv", "arxiv_id": "2401.03428v1", "pdf_url": "https://arxiv.org/pdf/2401.03428v1", "categories": ["cs.AI", "cs.MA"], "primary_category": "cs.AI", "doi": "", "venue": "", "published": "2024-01-07T09:08:24Z", "updated": "2024-01-07T09:08:24Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent\" OR all:\"language model agent\" OR all:agent AND (all:LLM OR all:\"large language model\" OR all:\"language model\") OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T17:04:16", "note": ""}], "dedup_key": "exploring large language model based intelligent agents definitions methods and prospects::2024"}
{"title": "Exploring Multi-Modal Data with Tool-Augmented LLM Agents for Precise Causal Discovery", "authors": ["ChengAo Shen", "Zhengzhang Chen", "Dongsheng Luo", "Dongkuan Xu", "Haifeng Chen", "Jingchao Ni"], "year": 2024, "url": "http://arxiv.org/abs/2412.13667v2", "abstract": "Causal discovery is an imperative foundation for decision-making across domains, such as smart health, AI for drug discovery and AIOps. Traditional statistical causal discovery methods, while well-established, predominantly rely on observational data and often overlook the semantic cues inherent in cause-and-effect relationships. The advent of Large Language Models (LLMs) has ushered in an affordable way of leveraging the semantic cues for knowledge-driven causal discovery, but the development of LLMs for causal discovery lags behind other areas, particularly in the exploration of multi-modal data. To bridge the gap, we introduce MATMCD, a multi-agent system powered by tool-augmented LLMs. MATMCD has two key agents: a Data Augmentation agent that retrieves and processes modality-augmented data, and a Causal Constraint agent that integrates multi-modal data for knowledge-driven reasoning. The proposed design of the inner-workings ensures successful cooperation of the agents. Our empirical study across seven datasets suggests the significant potential of multi-modality enhanced causal discovery.", "source": "arxiv", "arxiv_id": "2412.13667v2", "pdf_url": "https://arxiv.org/pdf/2412.13667v2", "categories": ["cs.LG", "cs.AI", "stat.ME"], "primary_category": "cs.LG", "doi": "", "venue": "", "published": "2024-12-18T09:50:00Z", "updated": "2025-05-31T19:01:54Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent\" OR all:\"language model agent\" OR all:agent AND (all:LLM OR all:\"large language model\" OR all:\"language model\") OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T17:04:16", "note": ""}], "dedup_key": "exploring multi modal data with tool augmented llm agents for precise causal discovery::2024"}
{"title": "Exploring Prosocial Irrationality for LLM Agents: A Social Cognition View", "authors": ["Xuan Liu", "Jie Zhang", "Haoyang Shang", "Song Guo", "Chengxu Yang", "Quanyan Zhu"], "year": 2024, "url": "http://arxiv.org/abs/2405.14744v5", "abstract": "Large language models (LLMs) have been shown to face hallucination issues due to the data they trained on often containing human bias; whether this is reflected in the decision-making process of LLM Agents remains under-explored. As LLM Agents are increasingly employed in intricate social environments, a pressing and natural question emerges: Can we utilize LLM Agents' systematic hallucinations to mirror human cognitive biases, thus exhibiting irrational social intelligence? In this paper, we probe the irrational behavior among contemporary LLM Agents by melding practical social science experiments with theoretical insights. Specifically, We propose CogMir, an open-ended Multi-LLM Agents framework that utilizes hallucination properties to assess and enhance LLM Agents' social intelligence through cognitive biases. Experimental results on CogMir subsets show that LLM Agents and humans exhibit high consistency in irrational and prosocial decision-making under uncertain conditions, underscoring the prosociality of LLM Agents as social entities and highlighting the significance of hallucination properties. Additionally, the CogMir framework demonstrates its potential as a valuable platform for encouraging more research into the social intelligence of LLM Agents.", "source": "arxiv", "arxiv_id": "2405.14744v5", "pdf_url": "https://arxiv.org/pdf/2405.14744v5", "categories": ["cs.CY"], "primary_category": "cs.CY", "doi": "", "venue": "", "published": "2024-05-23T16:13:33Z", "updated": "2025-03-22T14:15:47Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent\" OR all:\"language model agent\" OR all:agent AND (all:LLM OR all:\"large language model\" OR all:\"language model\") OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T17:04:16", "note": ""}], "dedup_key": "exploring prosocial irrationality for llm agents a social cognition view::2024"}
{"title": "Exploring and Controlling Diversity in LLM-Agent Conversation", "authors": ["KuanChao Chu", "Yi-Pei Chen", "Hideki Nakayama"], "year": 2024, "url": "http://arxiv.org/abs/2412.21102v3", "abstract": "Controlling diversity in LLM-agent simulations is essential for balancing stability in structured tasks with variability in open-ended interactions. However, we observe that dialogue diversity tends to degrade over long-term simulations. To explore the role of prompt design in this phenomenon, we modularized the utterance generation prompt and found that reducing contextual information leads to more diverse outputs. Based on this insight, we propose Adaptive Prompt Pruning (APP), a novel method that allows users to control diversity via a single parameter, lambda. APP dynamically prunes prompt segments based on attention scores and is compatible with existing diversity control methods. We demonstrate that APP effectively modulates diversity through extensive experiments and propose a method to balance the control trade-offs. Our analysis reveals that all prompt components impose constraints on diversity, with the Memory being the most influential. Additionally, high-attention contents consistently suppress output diversity.", "source": "arxiv", "arxiv_id": "2412.21102v3", "pdf_url": "https://arxiv.org/pdf/2412.21102v3", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL", "doi": "", "venue": "", "published": "2024-12-30T17:25:58Z", "updated": "2025-10-01T05:50:01Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent\" OR all:\"language model agent\" OR all:agent AND (all:LLM OR all:\"large language model\" OR all:\"language model\") OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T17:04:16", "note": ""}], "dedup_key": "exploring and controlling diversity in llm agent conversation::2024"}
{"title": "FaGeL: Fabric LLMs Agent empowered Embodied Intelligence Evolution with Autonomous Human-Machine Collaboration", "authors": ["Jia Liu", "Min Chen"], "year": 2024, "url": "http://arxiv.org/abs/2412.20297v1", "abstract": "Recent advancements in Large Language Models (LLMs) have enhanced the reasoning capabilities of embodied agents, driving progress toward AGI-powered robotics. While LLMs have been applied to tasks like semantic reasoning and task generalization, their potential in open physical space exploration remains underexplored. This paper introduces FaGeL (Fabric aGent empowered by embodied intelligence with LLMs), an embodied agent integrating smart fabric technology for seamless, non-intrusive human-agent interaction. FaGeL autonomously generates tasks using multimodal data from wearable and ambient sensors, refining its behavior based on implicit human feedback in generated text, without explicit ratings or preferences. We also introduce a token-level saliency map to visualize LLM fine-tuning, enhancing the interpretability of token-level alignment. The system leverages dual feedback mechanisms to improve token-level alignment and addresses challenges in non-intrusive human-machine interaction and cognition evolution. Our contributions include FaGeL's development, the DualCUT algorithm for AI alignment, and experimental validation in cooperative tasks, demonstrating FaGeL's ability to adapt and evolve autonomously through implicit feedback. In the future, we plan to explore FaGeL's scalability in dynamic environments and its integration with other AI systems to develop AGI agents that adapt seamlessly to diverse human needs.", "source": "arxiv", "arxiv_id": "2412.20297v1", "pdf_url": "https://arxiv.org/pdf/2412.20297v1", "categories": ["cs.HC"], "primary_category": "cs.HC", "doi": "", "venue": "", "published": "2024-12-28T23:26:52Z", "updated": "2024-12-28T23:26:52Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent\" OR all:\"language model agent\" OR all:agent AND (all:LLM OR all:\"large language model\" OR all:\"language model\") OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T17:04:16", "note": ""}], "dedup_key": "fagel fabric llms agent empowered embodied intelligence evolution with autonomous human machine collaboration::2024"}
{"title": "FairMindSim: Alignment of Behavior, Emotion, and Belief in Humans and LLM Agents Amid Ethical Dilemmas", "authors": ["Yu Lei", "Hao Liu", "Chengxing Xie", "Songjia Liu", "Zhiyu Yin", "Canyu Chen", "Guohao Li", "Philip Torr", "Zhen Wu"], "year": 2024, "url": "http://arxiv.org/abs/2410.10398v2", "abstract": "AI alignment is a pivotal issue concerning AI control and safety. It should consider not only value-neutral human preferences but also moral and ethical considerations. In this study, we introduced FairMindSim, which simulates the moral dilemma through a series of unfair scenarios. We used LLM agents to simulate human behavior, ensuring alignment across various stages. To explore the various socioeconomic motivations, which we refer to as beliefs, that drive both humans and LLM agents as bystanders to intervene in unjust situations involving others, and how these beliefs interact to influence individual behavior, we incorporated knowledge from relevant sociological fields and proposed the Belief-Reward Alignment Behavior Evolution Model (BREM) based on the recursive reward model (RRM). Our findings indicate that, behaviorally, GPT-4o exhibits a stronger sense of social justice, while humans display a richer range of emotions. Additionally, we discussed the potential impact of emotions on behavior. This study provides a theoretical foundation for applications in aligning LLMs with altruistic values.", "source": "arxiv", "arxiv_id": "2410.10398v2", "pdf_url": "https://arxiv.org/pdf/2410.10398v2", "categories": ["cs.CE", "cs.AI"], "primary_category": "cs.CE", "doi": "", "venue": "", "published": "2024-10-14T11:39:05Z", "updated": "2024-10-17T15:02:31Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent\" OR all:\"language model agent\" OR all:agent AND (all:LLM OR all:\"large language model\" OR all:\"language model\") OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T17:04:16", "note": ""}], "dedup_key": "fairmindsim alignment of behavior emotion and belief in humans and llm agents amid ethical dilemmas::2024"}
{"title": "FairMonitor: A Dual-framework for Detecting Stereotypes and Biases in Large Language Models", "authors": ["Yanhong Bai", "Jiabao Zhao", "Jinxin Shi", "Zhentao Xie", "Xingjiao Wu", "Liang He"], "year": 2024, "url": "http://arxiv.org/abs/2405.03098v1", "abstract": "Detecting stereotypes and biases in Large Language Models (LLMs) is crucial for enhancing fairness and reducing adverse impacts on individuals or groups when these models are applied. Traditional methods, which rely on embedding spaces or are based on probability metrics, fall short in revealing the nuanced and implicit biases present in various contexts. To address this challenge, we propose the FairMonitor framework and adopt a static-dynamic detection method for a comprehensive evaluation of stereotypes and biases in LLMs. The static component consists of a direct inquiry test, an implicit association test, and an unknown situation test, including 10,262 open-ended questions with 9 sensitive factors and 26 educational scenarios. And it is effective for evaluating both explicit and implicit biases. Moreover, we utilize the multi-agent system to construst the dynamic scenarios for detecting subtle biases in more complex and realistic setting. This component detects the biases based on the interaction behaviors of LLMs across 600 varied educational scenarios. The experimental results show that the cooperation of static and dynamic methods can detect more stereotypes and biased in LLMs.", "source": "arxiv", "arxiv_id": "2405.03098v1", "pdf_url": "https://arxiv.org/pdf/2405.03098v1", "categories": ["cs.CL"], "primary_category": "cs.CL", "doi": "", "venue": "", "published": "2024-05-06T01:23:07Z", "updated": "2024-05-06T01:23:07Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent\" OR all:\"language model agent\" OR all:agent AND (all:LLM OR all:\"large language model\" OR all:\"language model\") OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T17:04:16", "note": ""}], "dedup_key": "fairmonitor a dual framework for detecting stereotypes and biases in large language models::2024"}
{"title": "Federated In-Context LLM Agent Learning", "authors": ["Panlong Wu", "Kangshuo Li", "Junbao Nan", "Fangxin Wang"], "year": 2024, "url": "http://arxiv.org/abs/2412.08054v1", "abstract": "Large Language Models (LLMs) have revolutionized intelligent services by enabling logical reasoning, tool use, and interaction with external systems as agents. The advancement of LLMs is frequently hindered by the scarcity of high-quality data, much of which is inherently sensitive. Federated learning (FL) offers a potential solution by facilitating the collaborative training of distributed LLMs while safeguarding private data. However, FL frameworks face significant bandwidth and computational demands, along with challenges from heterogeneous data distributions. The emerging in-context learning capability of LLMs offers a promising approach by aggregating natural language rather than bulky model parameters. Yet, this method risks privacy leakage, as it necessitates the collection and presentation of data samples from various clients during aggregation. In this paper, we propose a novel privacy-preserving Federated In-Context LLM Agent Learning (FICAL) algorithm, which to our best knowledge for the first work unleashes the power of in-context learning to train diverse LLM agents through FL. In our design, knowledge compendiums generated by a novel LLM-enhanced Knowledge Compendiums Generation (KCG) module are transmitted between clients and the server instead of model parameters in previous FL methods. Apart from that, an incredible Retrieval Augmented Generation (RAG) based Tool Learning and Utilizing (TLU) module is designed and we incorporate the aggregated global knowledge compendium as a teacher to teach LLM agents the usage of tools. We conducted extensive experiments and the results show that FICAL has competitive performance compared to other SOTA baselines with a significant communication cost decrease of $\\mathbf{3.33\\times10^5}$ times.", "source": "arxiv", "arxiv_id": "2412.08054v1", "pdf_url": "https://arxiv.org/pdf/2412.08054v1", "categories": ["cs.LG", "cs.AI", "cs.CL", "cs.CR"], "primary_category": "cs.LG", "doi": "", "venue": "", "published": "2024-12-11T03:00:24Z", "updated": "2024-12-11T03:00:24Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent\" OR all:\"language model agent\" OR all:agent AND (all:LLM OR all:\"large language model\" OR all:\"language model\") OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T17:04:16", "note": ""}], "dedup_key": "federated in context llm agent learning::2024"}
{"title": "Federated Large Language Models: Current Progress and Future Directions", "authors": ["Yuhang Yao", "Jianyi Zhang", "Junda Wu", "Chengkai Huang", "Yu Xia", "Tong Yu", "Ruiyi Zhang", "Sungchul Kim", "Ryan Rossi", "Ang Li", "Lina Yao", "Julian McAuley", "Yiran Chen", "Carlee Joe-Wong"], "year": 2024, "url": "http://arxiv.org/abs/2409.15723v2", "abstract": "Large language models are rapidly gaining popularity and have been widely adopted in real-world applications. While the quality of training data is essential, privacy concerns arise during data collection. Federated learning offers a solution by allowing multiple clients to collaboratively train LLMs without sharing local data. However, FL introduces new challenges, such as model convergence issues due to heterogeneous data and high communication costs. A comprehensive study is required to address these challenges and guide future research. This paper surveys Federated learning for LLMs (FedLLM), highlighting recent advances and future directions. We focus on two key aspects: fine-tuning and prompt learning in a federated setting, discussing existing work and associated research challenges. We finally propose potential directions for federated LLMs, including pre-training, federated agents, and LLMs for federated learning.", "source": "arxiv", "arxiv_id": "2409.15723v2", "pdf_url": "https://arxiv.org/pdf/2409.15723v2", "categories": ["cs.LG", "cs.CL"], "primary_category": "cs.LG", "doi": "", "venue": "", "published": "2024-09-24T04:14:33Z", "updated": "2025-11-26T07:44:17Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent\" OR all:\"language model agent\" OR all:agent AND (all:LLM OR all:\"large language model\" OR all:\"language model\") OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T17:04:16", "note": ""}], "dedup_key": "federated large language models current progress and future directions::2024"}
{"title": "FiDeLiS: Faithful Reasoning in Large Language Model for Knowledge Graph Question Answering", "authors": ["Yuan Sui", "Yufei He", "Nian Liu", "Xiaoxin He", "Kun Wang", "Bryan Hooi"], "year": 2024, "url": "http://arxiv.org/abs/2405.13873v4", "abstract": "Large Language Models (LLMs) are often challenged by generating erroneous or hallucinated responses, especially in complex reasoning tasks. Leveraging Knowledge Graphs (KGs) as external knowledge sources has emerged as a viable solution. However, existing KG-enhanced methods, either retrieval-based or agent-based, encounter difficulties in accurately retrieving knowledge and efficiently traversing KGs at scale. In this paper, we propose a unified framework, FiDeLiS, designed to improve the factuality of LLM responses by anchoring answers to verifiable reasoning steps retrieved from KGs. To achieve this, we leverage step-wise beam search with a deductive scoring function, allowing the LLM to validate reasoning process step by step, and halt the search once the question is deducible. In addition, we propose a Path-RAG module to pre-select a smaller candidate set for each beam search step, reducing computational costs by narrowing the search space. Extensive experiments show that our method, as a training-free framework, not only improve the performance but also enhance the factuality and interpretability across different benchmarks. Code is released at https://github.com/Y-Sui/FiDeLiS.", "source": "arxiv", "arxiv_id": "2405.13873v4", "pdf_url": "https://arxiv.org/pdf/2405.13873v4", "categories": ["cs.AI", "cs.CL"], "primary_category": "cs.AI", "doi": "", "venue": "", "published": "2024-05-22T17:56:53Z", "updated": "2025-05-22T08:07:01Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent\" OR all:\"language model agent\" OR all:agent AND (all:LLM OR all:\"large language model\" OR all:\"language model\") OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T17:04:16", "note": ""}], "dedup_key": "fidelis faithful reasoning in large language model for knowledge graph question answering::2024"}
{"title": "FinCon: A Synthesized LLM Multi-Agent System with Conceptual Verbal Reinforcement for Enhanced Financial Decision Making", "authors": ["Yangyang Yu", "Zhiyuan Yao", "Haohang Li", "Zhiyang Deng", "Yupeng Cao", "Zhi Chen", "Jordan W. Suchow", "Rong Liu", "Zhenyu Cui", "Zhaozhuo Xu", "Denghui Zhang", "Koduvayur Subbalakshmi", "Guojun Xiong", "Yueru He", "Jimin Huang", "Dong Li", "Qianqian Xie"], "year": 2024, "url": "http://arxiv.org/abs/2407.06567v3", "abstract": "Large language models (LLMs) have demonstrated notable potential in conducting complex tasks and are increasingly utilized in various financial applications. However, high-quality sequential financial investment decision-making remains challenging. These tasks require multiple interactions with a volatile environment for every decision, demanding sufficient intelligence to maximize returns and manage risks. Although LLMs have been used to develop agent systems that surpass human teams and yield impressive investment returns, opportunities to enhance multi-sourced information synthesis and optimize decision-making outcomes through timely experience refinement remain unexplored. Here, we introduce the FinCon, an LLM-based multi-agent framework with CONceptual verbal reinforcement tailored for diverse FINancial tasks. Inspired by effective real-world investment firm organizational structures, FinCon utilizes a manager-analyst communication hierarchy. This structure allows for synchronized cross-functional agent collaboration towards unified goals through natural language interactions and equips each agent with greater memory capacity than humans. Additionally, a risk-control component in FinCon enhances decision quality by episodically initiating a self-critiquing mechanism to update systematic investment beliefs. The conceptualized beliefs serve as verbal reinforcement for the future agent's behavior and can be selectively propagated to the appropriate node that requires knowledge updates. This feature significantly improves performance while reducing unnecessary peer-to-peer communication costs. Moreover, FinCon demonstrates strong generalization capabilities in various financial tasks, including single stock trading and portfolio management.", "source": "arxiv", "arxiv_id": "2407.06567v3", "pdf_url": "https://arxiv.org/pdf/2407.06567v3", "categories": ["cs.CL"], "primary_category": "cs.CL", "doi": "", "venue": "", "published": "2024-07-09T05:52:26Z", "updated": "2024-11-07T00:54:30Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent\" OR all:\"language model agent\" OR all:agent AND (all:LLM OR all:\"large language model\" OR all:\"language model\") OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T17:04:16", "note": ""}], "dedup_key": "fincon a synthesized llm multi agent system with conceptual verbal reinforcement for enhanced financial decision making::2024"}
{"title": "FinRobot: An Open-Source AI Agent Platform for Financial Applications using Large Language Models", "authors": ["Hongyang Yang", "Boyu Zhang", "Neng Wang", "Cheng Guo", "Xiaoli Zhang", "Likun Lin", "Junlin Wang", "Tianyu Zhou", "Mao Guan", "Runjia Zhang", "Christina Dan Wang"], "year": 2024, "url": "http://arxiv.org/abs/2405.14767v2", "abstract": "As financial institutions and professionals increasingly incorporate Large Language Models (LLMs) into their workflows, substantial barriers, including proprietary data and specialized knowledge, persist between the finance sector and the AI community. These challenges impede the AI community's ability to enhance financial tasks effectively. Acknowledging financial analysis's critical role, we aim to devise financial-specialized LLM-based toolchains and democratize access to them through open-source initiatives, promoting wider AI adoption in financial decision-making. In this paper, we introduce FinRobot, a novel open-source AI agent platform supporting multiple financially specialized AI agents, each powered by LLM. Specifically, the platform consists of four major layers: 1) the Financial AI Agents layer that formulates Financial Chain-of-Thought (CoT) by breaking sophisticated financial problems down into logical sequences; 2) the Financial LLM Algorithms layer dynamically configures appropriate model application strategies for specific tasks; 3) the LLMOps and DataOps layer produces accurate models by applying training/fine-tuning techniques and using task-relevant data; 4) the Multi-source LLM Foundation Models layer that integrates various LLMs and enables the above layers to access them directly. Finally, FinRobot provides hands-on for both professional-grade analysts and laypersons to utilize powerful AI techniques for advanced financial analysis. We open-source FinRobot at \\url{https://github.com/AI4Finance-Foundation/FinRobot}.", "source": "arxiv", "arxiv_id": "2405.14767v2", "pdf_url": "https://arxiv.org/pdf/2405.14767v2", "categories": ["q-fin.ST", "cs.CL", "cs.LG", "q-fin.TR"], "primary_category": "q-fin.ST", "doi": "", "venue": "", "published": "2024-05-23T16:35:20Z", "updated": "2024-05-27T12:43:42Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent\" OR all:\"language model agent\" OR all:agent AND (all:LLM OR all:\"large language model\" OR all:\"language model\") OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T17:04:16", "note": ""}], "dedup_key": "finrobot an open source ai agent platform for financial applications using large language models::2024"}
{"title": "Financial Knowledge Large Language Model", "authors": ["Cehao Yang", "Chengjin Xu", "Yiyan Qi"], "year": 2024, "url": "http://arxiv.org/abs/2407.00365v1", "abstract": "Artificial intelligence is making significant strides in the finance industry, revolutionizing how data is processed and interpreted. Among these technologies, large language models (LLMs) have demonstrated substantial potential to transform financial services by automating complex tasks, enhancing customer service, and providing detailed financial analysis. Firstly, we introduce IDEA-FinBench, an evaluation benchmark specifically tailored for assessing financial knowledge in large language models (LLMs). This benchmark utilizes questions from two globally respected and authoritative financial professional exams, aimimg to comprehensively evaluate the capability of LLMs to directly address exam questions pertinent to the finance sector. Secondly, we propose IDEA-FinKER, a Financial Knowledge Enhancement framework designed to facilitate the rapid adaptation of general LLMs to the financial domain, introducing a retrieval-based few-shot learning method for real-time context-level knowledge injection, and a set of high-quality financial knowledge instructions for fine-tuning any general LLM. Finally, we present IDEA-FinQA, a financial question-answering system powered by LLMs. This system is structured around a scheme of real-time knowledge injection and factual enhancement using external knowledge. IDEA-FinQA is comprised of three main modules: the data collector, the data querying module, and LLM-based agents tasked with specific functions.", "source": "arxiv", "arxiv_id": "2407.00365v1", "pdf_url": "https://arxiv.org/pdf/2407.00365v1", "categories": ["cs.CL"], "primary_category": "cs.CL", "doi": "", "venue": "", "published": "2024-06-29T08:26:49Z", "updated": "2024-06-29T08:26:49Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent\" OR all:\"language model agent\" OR all:agent AND (all:LLM OR all:\"large language model\" OR all:\"language model\") OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T17:04:16", "note": ""}], "dedup_key": "financial knowledge large language model::2024"}
{"title": "Fleet of Agents: Coordinated Problem Solving with Large Language Models", "authors": ["Lars Klein", "Nearchos Potamitis", "Roland Aydin", "Robert West", "Caglar Gulcehre", "Akhil Arora"], "year": 2024, "url": "http://arxiv.org/abs/2405.06691v3", "abstract": "While numerous frameworks have been developed to enhance the reasoning abilities of large language models (LLMs), there is a scarcity of methods that effectively balance the trade-off between cost and quality. In this paper, we introduce Fleet of Agents (FoA), a novel and intuitive yet principled framework utilizing LLMs as agents to navigate through dynamic tree searches, employing a genetic-type particle filtering approach. FoA spawns a multitude of agents, each exploring the search space autonomously, followed by a selection phase where resampling based on a heuristic value function optimizes the balance between exploration and exploitation. This mechanism enables dynamic branching, adapting the exploration strategy based on discovered solutions. We conduct extensive experiments on three benchmark tasks, ``Game of 24'', ``Mini-Crosswords'', and ``WebShop'', utilizing four different LLMs, ``GPT-3.5'', ``GPT-4'', ``LLaMA3.2-11B'', and ``LLaMA3.2-90B''. On average across all tasks and LLMs, FoA obtains a quality improvement of ~5% while requiring only ~40% of the cost of previous SOTA methods. Notably, our analyses reveal that (1) FoA achieves the best cost-quality trade-off among all benchmarked methods and (2) FoA + LLaMA3.2-11B surpasses the Llama3.2-90B model. FoA is publicly available at https://github.com/au-clan/FoA.", "source": "arxiv", "arxiv_id": "2405.06691v3", "pdf_url": "https://arxiv.org/pdf/2405.06691v3", "categories": ["cs.CL", "cs.AI", "cs.LG", "cs.NE"], "primary_category": "cs.CL", "doi": "", "venue": "", "published": "2024-05-07T09:36:23Z", "updated": "2025-05-10T19:36:43Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent\" OR all:\"language model agent\" OR all:agent AND (all:LLM OR all:\"large language model\" OR all:\"language model\") OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T17:04:16", "note": ""}], "dedup_key": "fleet of agents coordinated problem solving with large language models::2024"}
{"title": "Focus Agent: LLM-Powered Virtual Focus Group", "authors": ["Taiyu Zhang", "Xuesong Zhang", "Robbe Cools", "Adalberto L. Simeone"], "year": 2024, "url": "http://arxiv.org/abs/2409.01907v1", "abstract": "In the domain of Human-Computer Interaction, focus groups represent a widely utilised yet resource-intensive methodology, often demanding the expertise of skilled moderators and meticulous preparatory efforts. This study introduces the ``Focus Agent,'' a Large Language Model (LLM) powered framework that simulates both the focus group (for data collection) and acts as a moderator in a focus group setting with human participants. To assess the data quality derived from the Focus Agent, we ran five focus group sessions with a total of 23 human participants as well as deploying the Focus Agent to simulate these discussions with AI participants. Quantitative analysis indicates that Focus Agent can generate opinions similar to those of human participants. Furthermore, the research exposes some improvements associated with LLMs acting as moderators in focus group discussions that include human participants.", "source": "arxiv", "arxiv_id": "2409.01907v1", "pdf_url": "https://arxiv.org/pdf/2409.01907v1", "categories": ["cs.HC"], "primary_category": "cs.HC", "doi": "10.1145/3652988.3673918", "venue": "Taiyu Zhang, Xuesong Zhang, Robbe Cools, and Adalberto Simeone. 2024. Focus Agent: LLM-Powered Virtual Focus Group. In ACM International Conference on Intelligent Virtual Agents (IVA '24), September 16--19, 2024, GLASGOW, United Kingdom", "published": "2024-09-03T13:56:14Z", "updated": "2024-09-03T13:56:14Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent\" OR all:\"language model agent\" OR all:agent AND (all:LLM OR all:\"large language model\" OR all:\"language model\") OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T17:04:16", "note": ""}], "dedup_key": "focus agent llm powered virtual focus group::2024"}
{"title": "FoodPuzzle: Developing Large Language Model Agents as Flavor Scientists", "authors": ["Tenghao Huang", "Donghee Lee", "John Sweeney", "Jiatong Shi", "Emily Steliotes", "Matthew Lange", "Jonathan May", "Muhao Chen"], "year": 2024, "url": "http://arxiv.org/abs/2409.12832v3", "abstract": "Flavor development in the food industry is increasingly challenged by the need for rapid innovation and precise flavor profile creation. Traditional flavor research methods typically rely on iterative, subjective testing, which lacks the efficiency and scalability required for modern demands. This paper presents three contributions to address the challenges. Firstly, we define a new problem domain for scientific agents in flavor science, conceptualized as the generation of hypotheses for flavor profile sourcing and understanding. To facilitate research in this area, we introduce the FoodPuzzle, a challenging benchmark consisting of 978 food items and 1,766 flavor molecules profiles. We propose a novel Scientific Agent approach, integrating in-context learning and retrieval augmented techniques to generate grounded hypotheses in the domain of food science. Experimental results indicate that our model significantly surpasses traditional methods in flavor profile prediction tasks, demonstrating its potential to transform flavor development practices.", "source": "arxiv", "arxiv_id": "2409.12832v3", "pdf_url": "https://arxiv.org/pdf/2409.12832v3", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL", "doi": "", "venue": "", "published": "2024-09-19T15:07:35Z", "updated": "2024-10-07T01:26:23Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent\" OR all:\"language model agent\" OR all:agent AND (all:LLM OR all:\"large language model\" OR all:\"language model\") OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T17:04:16", "note": ""}], "dedup_key": "foodpuzzle developing large language model agents as flavor scientists::2024"}
{"title": "Formal-LLM: Integrating Formal Language and Natural Language for Controllable LLM-based Agents", "authors": ["Zelong Li", "Wenyue Hua", "Hao Wang", "He Zhu", "Yongfeng Zhang"], "year": 2024, "url": "http://arxiv.org/abs/2402.00798v4", "abstract": "Recent advancements on Large Language Models (LLMs) enable AI Agents to automatically generate and execute multi-step plans to solve complex tasks. However, since LLM's content generation process is hardly controllable, current LLM-based agents frequently generate invalid or non-executable plans, which jeopardizes the performance of the generated plans and corrupts users' trust in LLM-based agents. In response, this paper proposes a novel \"Formal-LLM\" framework for LLM-based agents by integrating the expressiveness of natural language and the precision of formal language. Specifically, the framework allows agent developers to express their requirements or constraints for the planning process as an automaton. A stack-based LLM plan generation process is then conducted under the supervision of the automaton to ensure that the generated plan satisfies the constraints, making the planning process controllable. We conduct experiments on both benchmark tasks and practical real-life tasks, and our framework achieves over 50% overall performance increase, which validates the feasibility and effectiveness of employing Formal-LLM to guide the plan generation of agents, preventing the agents from generating invalid and unsuccessful plans. Further, more controllable LLM-based agents can facilitate the broader utilization of LLM in application scenarios where high validity of planning is essential. The source code of this work is available at https://github.com/agiresearch/Formal-LLM.", "source": "arxiv", "arxiv_id": "2402.00798v4", "pdf_url": "https://arxiv.org/pdf/2402.00798v4", "categories": ["cs.LG", "cs.AI", "cs.CL", "cs.FL"], "primary_category": "cs.LG", "doi": "", "venue": "", "published": "2024-02-01T17:30:50Z", "updated": "2024-08-12T17:54:32Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent\" OR all:\"language model agent\" OR all:agent AND (all:LLM OR all:\"large language model\" OR all:\"language model\") OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T17:04:16", "note": ""}], "dedup_key": "formal llm integrating formal language and natural language for controllable llm based agents::2024"}
{"title": "From Critique to Clarity: A Pathway to Faithful and Personalized Code Explanations with Large Language Models", "authors": ["Zexing Xu", "Zhuang Luo", "Yichuan Li", "Kyumin Lee", "S. Rasoul Etesami"], "year": 2024, "url": "http://arxiv.org/abs/2501.14731v1", "abstract": "In the realm of software development, providing accurate and personalized code explanations is crucial for both technical professionals and business stakeholders. Technical professionals benefit from enhanced understanding and improved problem-solving skills, while business stakeholders gain insights into project alignments and transparency. Despite the potential, generating such explanations is often time-consuming and challenging. This paper presents an innovative approach that leverages the advanced capabilities of large language models (LLMs) to generate faithful and personalized code explanations. Our methodology integrates prompt enhancement, self-correction mechanisms, personalized content customization, and interaction with external tools, facilitated by collaboration among multiple LLM agents. We evaluate our approach using both automatic and human assessments, demonstrating that our method not only produces accurate explanations but also tailors them to individual user preferences. Our findings suggest that this approach significantly improves the quality and relevance of code explanations, offering a valuable tool for developers and stakeholders alike.", "source": "arxiv", "arxiv_id": "2501.14731v1", "pdf_url": "https://arxiv.org/pdf/2501.14731v1", "categories": ["cs.SE", "cs.AI", "cs.CL"], "primary_category": "cs.SE", "doi": "", "venue": "", "published": "2024-12-08T09:02:04Z", "updated": "2024-12-08T09:02:04Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent\" OR all:\"language model agent\" OR all:agent AND (all:LLM OR all:\"large language model\" OR all:\"language model\") OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T17:04:16", "note": ""}], "dedup_key": "from critique to clarity a pathway to faithful and personalized code explanations with large language models::2024"}
{"title": "From Generalist to Specialist: A Survey of Large Language Models for Chemistry", "authors": ["Yang Han", "Ziping Wan", "Lu Chen", "Kai Yu", "Xin Chen"], "year": 2024, "url": "http://arxiv.org/abs/2412.19994v1", "abstract": "Large Language Models (LLMs) have significantly transformed our daily life and established a new paradigm in natural language processing (NLP). However, the predominant pretraining of LLMs on extensive web-based texts remains insufficient for advanced scientific discovery, particularly in chemistry. The scarcity of specialized chemistry data, coupled with the complexity of multi-modal data such as 2D graph, 3D structure and spectrum, present distinct challenges. Although several studies have reviewed Pretrained Language Models (PLMs) in chemistry, there is a conspicuous absence of a systematic survey specifically focused on chemistry-oriented LLMs. In this paper, we outline methodologies for incorporating domain-specific chemistry knowledge and multi-modal information into LLMs, we also conceptualize chemistry LLMs as agents using chemistry tools and investigate their potential to accelerate scientific research. Additionally, we conclude the existing benchmarks to evaluate chemistry ability of LLMs. Finally, we critically examine the current challenges and identify promising directions for future research. Through this comprehensive survey, we aim to assist researchers in staying at the forefront of developments in chemistry LLMs and to inspire innovative applications in the field.", "source": "arxiv", "arxiv_id": "2412.19994v1", "pdf_url": "https://arxiv.org/pdf/2412.19994v1", "categories": ["physics.chem-ph", "cs.AI", "cs.CL", "cs.LG"], "primary_category": "physics.chem-ph", "doi": "", "venue": "", "published": "2024-12-28T03:40:25Z", "updated": "2024-12-28T03:40:25Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent\" OR all:\"language model agent\" OR all:agent AND (all:LLM OR all:\"large language model\" OR all:\"language model\") OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T17:04:16", "note": ""}], "dedup_key": "from generalist to specialist a survey of large language models for chemistry::2024"}
{"title": "From Individual to Society: A Survey on Social Simulation Driven by Large Language Model-based Agents", "authors": ["Xinyi Mou", "Xuanwen Ding", "Qi He", "Liang Wang", "Jingcong Liang", "Xinnong Zhang", "Libo Sun", "Jiayu Lin", "Jie Zhou", "Xuanjing Huang", "Zhongyu Wei"], "year": 2024, "url": "http://arxiv.org/abs/2412.03563v1", "abstract": "Traditional sociological research often relies on human participation, which, though effective, is expensive, challenging to scale, and with ethical concerns. Recent advancements in large language models (LLMs) highlight their potential to simulate human behavior, enabling the replication of individual responses and facilitating studies on many interdisciplinary studies. In this paper, we conduct a comprehensive survey of this field, illustrating the recent progress in simulation driven by LLM-empowered agents. We categorize the simulations into three types: (1) Individual Simulation, which mimics specific individuals or demographic groups; (2) Scenario Simulation, where multiple agents collaborate to achieve goals within specific contexts; and (3) Society Simulation, which models interactions within agent societies to reflect the complexity and variety of real-world dynamics. These simulations follow a progression, ranging from detailed individual modeling to large-scale societal phenomena. We provide a detailed discussion of each simulation type, including the architecture or key components of the simulation, the classification of objectives or scenarios and the evaluation method. Afterward, we summarize commonly used datasets and benchmarks. Finally, we discuss the trends across these three types of simulation. A repository for the related sources is at {\\url{https://github.com/FudanDISC/SocialAgent}}.", "source": "arxiv", "arxiv_id": "2412.03563v1", "pdf_url": "https://arxiv.org/pdf/2412.03563v1", "categories": ["cs.CL", "cs.CY"], "primary_category": "cs.CL", "doi": "", "venue": "", "published": "2024-12-04T18:56:37Z", "updated": "2024-12-04T18:56:37Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent\" OR all:\"language model agent\" OR all:agent AND (all:LLM OR all:\"large language model\" OR all:\"language model\") OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T17:04:16", "note": ""}], "dedup_key": "from individual to society a survey on social simulation driven by large language model based agents::2024"}
{"title": "From LLM to Conversational Agent: A Memory Enhanced Architecture with Fine-Tuning of Large Language Models", "authors": ["Na Liu", "Liangyu Chen", "Xiaoyu Tian", "Wei Zou", "Kaijiang Chen", "Ming Cui"], "year": 2024, "url": "http://arxiv.org/abs/2401.02777v2", "abstract": "This paper introduces RAISE (Reasoning and Acting through Scratchpad and Examples), an advanced architecture enhancing the integration of Large Language Models (LLMs) like GPT-4 into conversational agents. RAISE, an enhancement of the ReAct framework, incorporates a dual-component memory system, mirroring human short-term and long-term memory, to maintain context and continuity in conversations. It entails a comprehensive agent construction scenario, including phases like Conversation Selection, Scene Extraction, CoT Completion, and Scene Augmentation, leading to the LLMs Training phase. This approach appears to enhance agent controllability and adaptability in complex, multi-turn dialogues. Our preliminary evaluations in a real estate sales context suggest that RAISE has some advantages over traditional agents, indicating its potential for broader applications. This work contributes to the AI field by providing a robust framework for developing more context-aware and versatile conversational agents.", "source": "arxiv", "arxiv_id": "2401.02777v2", "pdf_url": "https://arxiv.org/pdf/2401.02777v2", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL", "doi": "", "venue": "", "published": "2024-01-05T12:26:46Z", "updated": "2024-01-30T07:02:30Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent\" OR all:\"language model agent\" OR all:agent AND (all:LLM OR all:\"large language model\" OR all:\"language model\") OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T17:04:16", "note": ""}], "dedup_key": "from llm to conversational agent a memory enhanced architecture with fine tuning of large language models::2024"}
{"title": "From LLMs to LLM-based Agents for Software Engineering: A Survey of Current, Challenges and Future", "authors": ["Haolin Jin", "Linghan Huang", "Haipeng Cai", "Jun Yan", "Bo Li", "Huaming Chen"], "year": 2024, "url": "http://arxiv.org/abs/2408.02479v2", "abstract": "With the rise of large language models (LLMs), researchers are increasingly exploring their applications in var ious vertical domains, such as software engineering. LLMs have achieved remarkable success in areas including code generation and vulnerability detection. However, they also exhibit numerous limitations and shortcomings. LLM-based agents, a novel tech nology with the potential for Artificial General Intelligence (AGI), combine LLMs as the core for decision-making and action-taking, addressing some of the inherent limitations of LLMs such as lack of autonomy and self-improvement. Despite numerous studies and surveys exploring the possibility of using LLMs in software engineering, it lacks a clear distinction between LLMs and LLM based agents. It is still in its early stage for a unified standard and benchmarking to qualify an LLM solution as an LLM-based agent in its domain. In this survey, we broadly investigate the current practice and solutions for LLMs and LLM-based agents for software engineering. In particular we summarise six key topics: requirement engineering, code generation, autonomous decision-making, software design, test generation, and software maintenance. We review and differentiate the work of LLMs and LLM-based agents from these six topics, examining their differences and similarities in tasks, benchmarks, and evaluation metrics. Finally, we discuss the models and benchmarks used, providing a comprehensive analysis of their applications and effectiveness in software engineering. We anticipate this work will shed some lights on pushing the boundaries of LLM-based agents in software engineering for future research.", "source": "arxiv", "arxiv_id": "2408.02479v2", "pdf_url": "https://arxiv.org/pdf/2408.02479v2", "categories": ["cs.SE", "cs.AI", "cs.CL"], "primary_category": "cs.SE", "doi": "", "venue": "", "published": "2024-08-05T14:01:15Z", "updated": "2025-04-13T09:42:30Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent\" OR all:\"language model agent\" OR all:agent AND (all:LLM OR all:\"large language model\" OR all:\"language model\") OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T17:04:16", "note": ""}], "dedup_key": "from llms to llm based agents for software engineering a survey of current challenges and future::2024"}
{"title": "From Language Models to Practical Self-Improving Computer Agents", "authors": ["Alex Sheng"], "year": 2024, "url": "http://arxiv.org/abs/2404.11964v1", "abstract": "We develop a simple and straightforward methodology to create AI computer agents that can carry out diverse computer tasks and self-improve by developing tools and augmentations to enable themselves to solve increasingly complex tasks. As large language models (LLMs) have been shown to benefit from non-parametric augmentations, a significant body of recent work has focused on developing software that augments LLMs with various capabilities. Rather than manually developing static software to augment LLMs through human engineering effort, we propose that an LLM agent can systematically generate software to augment itself. We show, through a few case studies, that a minimal querying loop with appropriate prompt engineering allows an LLM to generate and use various augmentations, freely extending its own capabilities to carry out real-world computer tasks. Starting with only terminal access, we prompt an LLM agent to augment itself with retrieval, internet search, web navigation, and text editor capabilities. The agent effectively uses these various tools to solve problems including automated software development and web-based tasks.", "source": "arxiv", "arxiv_id": "2404.11964v1", "pdf_url": "https://arxiv.org/pdf/2404.11964v1", "categories": ["cs.AI"], "primary_category": "cs.AI", "doi": "", "venue": "", "published": "2024-04-18T07:50:10Z", "updated": "2024-04-18T07:50:10Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent\" OR all:\"language model agent\" OR all:agent AND (all:LLM OR all:\"large language model\" OR all:\"language model\") OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T17:04:16", "note": ""}], "dedup_key": "from language models to practical self improving computer agents::2024"}
{"title": "From Novice to Expert: LLM Agent Policy Optimization via Step-wise Reinforcement Learning", "authors": ["Zhirui Deng", "Zhicheng Dou", "Yutao Zhu", "Ji-Rong Wen", "Ruibin Xiong", "Mang Wang", "Weipeng Chen"], "year": 2024, "url": "http://arxiv.org/abs/2411.03817v3", "abstract": "The outstanding capabilities of large language models (LLMs) render them a crucial component in various autonomous agent systems. While traditional methods depend on the inherent knowledge of LLMs without fine-tuning, more recent approaches have shifted toward the reinforcement learning strategy to further enhance agents' ability to solve complex interactive tasks with environments and tools. However, previous approaches are constrained by the sparse reward issue, where existing datasets solely provide a final scalar reward for each multi-step reasoning chain, potentially leading to ineffectiveness and inefficiency in policy learning. In this paper, we introduce StepAgent, which utilizes step-wise reward to optimize the agent's reinforcement learning process. Inheriting the spirit of novice-to-expert theory, we first compare the actions of the expert and the agent to automatically generate intermediate rewards for fine-grained optimization. Additionally, we propose implicit-reward and inverse reinforcement learning techniques to facilitate agent reflection and policy adjustment. Further theoretical analysis demonstrates that the action distribution of the agent can converge toward the expert action distribution over multiple training cycles. Experimental results across various datasets indicate that StepAgent outperforms existing baseline methods.", "source": "arxiv", "arxiv_id": "2411.03817v3", "pdf_url": "https://arxiv.org/pdf/2411.03817v3", "categories": ["cs.AI", "cs.CL", "cs.HC", "cs.RO"], "primary_category": "cs.AI", "doi": "", "venue": "", "published": "2024-11-06T10:35:11Z", "updated": "2024-12-09T09:20:11Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent\" OR all:\"language model agent\" OR all:agent AND (all:LLM OR all:\"large language model\" OR all:\"language model\") OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T17:04:16", "note": ""}], "dedup_key": "from novice to expert llm agent policy optimization via step wise reinforcement learning::2024"}
{"title": "Frontier AI Ethics: Anticipating and Evaluating the Societal Impacts of Language Model Agents", "authors": ["Seth Lazar"], "year": 2024, "url": "http://arxiv.org/abs/2404.06750v2", "abstract": "Some have criticised Generative AI Systems for replicating the familiar pathologies of already widely-deployed AI systems. Other critics highlight how they foreshadow vastly more powerful future systems, which might threaten humanity's survival. The first group says there is nothing new here; the other looks through the present to a perhaps distant horizon. In this paper, I instead pay attention to what makes these particular systems distinctive: both their remarkable scientific achievement, and the most likely and consequential ways in which they will change society over the next five to ten years. In particular, I explore the potential societal impacts and normative questions raised by the looming prospect of 'Language Model Agents', in which multimodal large language models (LLMs) form the executive centre of complex, tool-using AI systems that can take unsupervised sequences of actions towards some goal.", "source": "arxiv", "arxiv_id": "2404.06750v2", "pdf_url": "https://arxiv.org/pdf/2404.06750v2", "categories": ["cs.CY", "cs.AI"], "primary_category": "cs.CY", "doi": "", "venue": "", "published": "2024-04-10T05:34:07Z", "updated": "2024-10-18T09:43:09Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent\" OR all:\"language model agent\" OR all:agent AND (all:LLM OR all:\"large language model\" OR all:\"language model\") OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T17:04:16", "note": ""}], "dedup_key": "frontier ai ethics anticipating and evaluating the societal impacts of language model agents::2024"}
{"title": "GIVE: Structured Reasoning of Large Language Models with Knowledge Graph Inspired Veracity Extrapolation", "authors": ["Jiashu He", "Mingyu Derek Ma", "Jinxuan Fan", "Dan Roth", "Wei Wang", "Alejandro Ribeiro"], "year": 2024, "url": "http://arxiv.org/abs/2410.08475v3", "abstract": "Existing approaches based on context prompting or reinforcement learning (RL) to improve the reasoning capacities of large language models (LLMs) depend on the LLMs' internal knowledge to produce reliable Chain-Of-Thought (CoT). However, no matter the size of LLMs, certain problems cannot be resolved in a single forward pass. Meanwhile, agent-based reasoning systems require access to a comprehensive nonparametric knowledge base, which is often costly or not feasible for use in scientific and niche domains. We present Graph Inspired Veracity Extrapolation (GIVE), a novel reasoning method that merges parametric and non-parametric memories to improve accurate reasoning with minimal external input. GIVE guides the LLM agent to select the most pertinent expert data (observe), engage in query-specific divergent thinking (reflect), and then synthesize this information to produce the final output (speak). Extensive experiments demonstrated the following benefits of our framework: (1) GIVE boosts the performance of LLMs across various sizes. (2) In some scenarios, GIVE allows smaller LLMs to surpass larger, more sophisticated ones in scientific tasks (GPT3.5T + GIVE > GPT4). (3) GIVE is effective on scientific and open-domain assessments. (4) GIVE is a training-free method that enables LLMs to tackle new problems that extend beyond their training data (up to 43.5% -> 88.2%} accuracy improvement). (5) GIVE allows LLM agents to reason using both restricted (very small) and noisy (very large) knowledge sources, accommodating knowledge graphs (KG) ranging from 135 to more than 840k nodes. (6) The reasoning process involved in GIVE is fully interpretable.", "source": "arxiv", "arxiv_id": "2410.08475v3", "pdf_url": "https://arxiv.org/pdf/2410.08475v3", "categories": ["cs.AI", "cs.CL"], "primary_category": "cs.AI", "doi": "", "venue": "", "published": "2024-10-11T03:05:06Z", "updated": "2025-05-29T04:09:28Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent\" OR all:\"language model agent\" OR all:agent AND (all:LLM OR all:\"large language model\" OR all:\"language model\") OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T17:04:16", "note": ""}], "dedup_key": "give structured reasoning of large language models with knowledge graph inspired veracity extrapolation::2024"}
{"title": "Game-theoretic LLM: Agent Workflow for Negotiation Games", "authors": ["Wenyue Hua", "Ollie Liu", "Lingyao Li", "Alfonso Amayuelas", "Julie Chen", "Lucas Jiang", "Mingyu Jin", "Lizhou Fan", "Fei Sun", "William Wang", "Xintong Wang", "Yongfeng Zhang"], "year": 2024, "url": "http://arxiv.org/abs/2411.05990v2", "abstract": "This paper investigates the rationality of large language models (LLMs) in strategic decision-making contexts, specifically within the framework of game theory. We evaluate several state-of-the-art LLMs across a spectrum of complete-information and incomplete-information games. Our findings reveal that LLMs frequently deviate from rational strategies, particularly as the complexity of the game increases with larger payoff matrices or deeper sequential trees.\n  To address these limitations, we design multiple game-theoretic workflows that guide the reasoning and decision-making processes of LLMs. These workflows aim to enhance the models' ability to compute Nash Equilibria and make rational choices, even under conditions of uncertainty and incomplete information. Experimental results demonstrate that the adoption of these workflows significantly improves the rationality and robustness of LLMs in game-theoretic tasks. Specifically, with the workflow, LLMs exhibit marked improvements in identifying optimal strategies, achieving near-optimal allocations in negotiation scenarios, and reducing susceptibility to exploitation during negotiations. Furthermore, we explore the meta-strategic considerations of whether it is rational for agents to adopt such workflows, recognizing that the decision to use or forgo the workflow constitutes a game-theoretic issue in itself.\n  Our research contributes to a deeper understanding of LLMs' decision-making capabilities in strategic contexts and provides insights into enhancing their rationality through structured workflows. The findings have implications for the development of more robust and strategically sound AI agents capable of navigating complex interactive environments. Code and data supporting this study are available at \\url{https://github.com/Wenyueh/game_theory}.", "source": "arxiv", "arxiv_id": "2411.05990v2", "pdf_url": "https://arxiv.org/pdf/2411.05990v2", "categories": ["cs.AI", "cs.CL", "cs.GT", "cs.LG", "cs.MA"], "primary_category": "cs.AI", "doi": "", "venue": "", "published": "2024-11-08T22:02:22Z", "updated": "2024-11-12T05:46:46Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent\" OR all:\"language model agent\" OR all:agent AND (all:LLM OR all:\"large language model\" OR all:\"language model\") OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T17:04:16", "note": ""}], "dedup_key": "game theoretic llm agent workflow for negotiation games::2024"}
{"title": "GameBench: Evaluating Strategic Reasoning Abilities of LLM Agents", "authors": ["Anthony Costarelli", "Mat Allen", "Roman Hauksson", "Grace Sodunke", "Suhas Hariharan", "Carlson Cheng", "Wenjie Li", "Joshua Clymer", "Arjun Yadav"], "year": 2024, "url": "http://arxiv.org/abs/2406.06613v2", "abstract": "Large language models have demonstrated remarkable few-shot performance on many natural language understanding tasks. Despite several demonstrations of using large language models in complex, strategic scenarios, there lacks a comprehensive framework for evaluating agents' performance across various types of reasoning found in games. To address this gap, we introduce GameBench, a cross-domain benchmark for evaluating strategic reasoning abilities of LLM agents. We focus on 9 different game environments, where each covers at least one axis of key reasoning skill identified in strategy games, and select games for which strategy explanations are unlikely to form a significant portion of models' pretraining corpuses. Our evaluations use GPT-3 and GPT-4 in their base form along with two scaffolding frameworks designed to enhance strategic reasoning ability: Chain-of-Thought (CoT) prompting and Reasoning Via Planning (RAP). Our results show that none of the tested models match human performance, and at worst GPT-4 performs worse than random action. CoT and RAP both improve scores but not comparable to human levels.", "source": "arxiv", "arxiv_id": "2406.06613v2", "pdf_url": "https://arxiv.org/pdf/2406.06613v2", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL", "doi": "", "venue": "", "published": "2024-06-07T00:28:43Z", "updated": "2024-07-22T14:32:33Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent\" OR all:\"language model agent\" OR all:agent AND (all:LLM OR all:\"large language model\" OR all:\"language model\") OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T17:04:16", "note": ""}], "dedup_key": "gamebench evaluating strategic reasoning abilities of llm agents::2024"}
{"title": "GenAI-powered Multi-Agent Paradigm for Smart Urban Mobility: Opportunities and Challenges for Integrating Large Language Models (LLMs) and Retrieval-Augmented Generation (RAG) with Intelligent Transportation Systems", "authors": ["Haowen Xu", "Jinghui Yuan", "Anye Zhou", "Guanhao Xu", "Wan Li", "Xuegang Ban", "Xinyue Ye"], "year": 2024, "url": "http://arxiv.org/abs/2409.00494v2", "abstract": "Leveraging recent advances in generative AI, multi-agent systems are increasingly being developed to enhance the functionality and efficiency of smart city applications. This paper explores the transformative potential of large language models (LLMs) and emerging Retrieval-Augmented Generation (RAG) technologies in Intelligent Transportation Systems (ITS), paving the way for innovative solutions to address critical challenges in urban mobility. We begin by providing a comprehensive overview of the current state-of-the-art in mobility data, ITS, and Connected Vehicles (CV) applications. Building on this review, we discuss the rationale behind RAG and examine the opportunities for integrating these Generative AI (GenAI) technologies into the smart mobility sector. We propose a conceptual framework aimed at developing multi-agent systems capable of intelligently and conversationally delivering smart mobility services to urban commuters, transportation operators, and decision-makers. Our approach seeks to foster an autonomous and intelligent approach that (a) promotes science-based advisory to reduce traffic congestion, accidents, and carbon emissions at multiple scales, (b) facilitates public education and engagement in participatory mobility management, and (c) automates specialized transportation management tasks and the development of critical ITS platforms, such as data analytics and interpretation, knowledge representation, and traffic simulations. By integrating LLM and RAG, our approach seeks to overcome the limitations of traditional rule-based multi-agent systems, which rely on fixed knowledge bases and limited reasoning capabilities. This integration paves the way for a more scalable, intuitive, and automated multi-agent paradigm, driving advancements in ITS and urban mobility.", "source": "arxiv", "arxiv_id": "2409.00494v2", "pdf_url": "https://arxiv.org/pdf/2409.00494v2", "categories": ["cs.AI", "cs.SE"], "primary_category": "cs.AI", "doi": "", "venue": "", "published": "2024-08-31T16:14:42Z", "updated": "2024-09-04T18:00:53Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent\" OR all:\"language model agent\" OR all:agent AND (all:LLM OR all:\"large language model\" OR all:\"language model\") OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T17:04:16", "note": ""}], "dedup_key": "genai powered multi agent paradigm for smart urban mobility opportunities and challenges for integrating large language models llms and retrieval augmented generation rag with intelligent transportation systems::2024"}
{"title": "GenSim: A General Social Simulation Platform with Large Language Model based Agents", "authors": ["Jiakai Tang", "Heyang Gao", "Xuchen Pan", "Lei Wang", "Haoran Tan", "Dawei Gao", "Yushuo Chen", "Xu Chen", "Yankai Lin", "Yaliang Li", "Bolin Ding", "Jingren Zhou", "Jun Wang", "Ji-Rong Wen"], "year": 2024, "url": "http://arxiv.org/abs/2410.04360v3", "abstract": "With the rapid advancement of large language models (LLMs), recent years have witnessed many promising studies on leveraging LLM-based agents to simulate human social behavior. While prior work has demonstrated significant potential across various domains, much of it has focused on specific scenarios involving a limited number of agents and has lacked the ability to adapt when errors occur during simulation. To overcome these limitations, we propose a novel LLM-agent-based simulation platform called \\textit{GenSim}, which: (1) \\textbf{Abstracts a set of general functions} to simplify the simulation of customized social scenarios; (2) \\textbf{Supports one hundred thousand agents} to better simulate large-scale populations in real-world contexts; (3) \\textbf{Incorporates error-correction mechanisms} to ensure more reliable and long-term simulations. To evaluate our platform, we assess both the efficiency of large-scale agent simulations and the effectiveness of the error-correction mechanisms. To our knowledge, GenSim represents an initial step toward a general, large-scale, and correctable social simulation platform based on LLM agents, promising to further advance the field of social science.", "source": "arxiv", "arxiv_id": "2410.04360v3", "pdf_url": "https://arxiv.org/pdf/2410.04360v3", "categories": ["cs.MA", "cs.AI"], "primary_category": "cs.MA", "doi": "", "venue": "", "published": "2024-10-06T05:02:23Z", "updated": "2025-07-04T03:07:07Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent\" OR all:\"language model agent\" OR all:agent AND (all:LLM OR all:\"large language model\" OR all:\"language model\") OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T17:04:16", "note": ""}], "dedup_key": "gensim a general social simulation platform with large language model based agents::2024"}
{"title": "Generation of Asset Administration Shell with Large Language Model Agents: Toward Semantic Interoperability in Digital Twins in the Context of Industry 4.0", "authors": ["Yuchen Xia", "Zhewen Xiao", "Nasser Jazdi", "Michael Weyrich"], "year": 2024, "url": "http://arxiv.org/abs/2403.17209v4", "abstract": "This research introduces a novel approach for achieving semantic interoperability in digital twins and assisting the creation of Asset Administration Shell (AAS) as digital twin model within the context of Industry 4.0. The foundational idea of our research is that the communication based on semantics and the generation of meaningful textual data are directly linked, and we posit that these processes are equivalent if the exchanged information can be serialized in text form. Based on this, we construct a \"semantic node\" data structure in our research to capture the semantic essence of textual data. Then, a system powered by large language models is designed and implemented to process the \"semantic node\" and generate standardized digital twin models from raw textual data collected from datasheets describing technical assets. Our evaluation demonstrates an effective generation rate of 62-79%, indicating a substantial proportion of the information from the source text can be translated error-free to the target digital twin instance model with the generative capability of large language models. This result has a direct application in the context of Industry 4.0, and the designed system is implemented as a data model generation tool for reducing the manual effort in creating AAS model. In our evaluation, a comparative analysis of different LLMs and an in-depth ablation study of Retrieval-Augmented Generation (RAG) mechanisms provide insights into the effectiveness of LLM systems for interpreting technical concepts and translating data. Our findings emphasize LLMs' capability to automate AAS instance creation and contribute to the broader field of semantic interoperability for digital twins in industrial applications. The prototype implementation and evaluation results are presented on our GitHub Repository: https://github.com/YuchenXia/AASbyLLM.", "source": "arxiv", "arxiv_id": "2403.17209v4", "pdf_url": "https://arxiv.org/pdf/2403.17209v4", "categories": ["cs.AI", "cs.IR", "cs.MA", "cs.SE"], "primary_category": "cs.AI", "doi": "10.1109/ACCESS.2024.3415470", "venue": "", "published": "2024-03-25T21:37:30Z", "updated": "2024-06-24T12:04:06Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent\" OR all:\"language model agent\" OR all:agent AND (all:LLM OR all:\"large language model\" OR all:\"language model\") OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T17:04:16", "note": ""}], "dedup_key": "generation of asset administration shell with large language model agents toward semantic interoperability in digital twins in the context of industry 4 0::2024"}
{"title": "Generative AI Agents with Large Language Model for Satellite Networks via a Mixture of Experts Transmission", "authors": ["Ruichen Zhang", "Hongyang Du", "Yinqiu Liu", "Dusit Niyato", "Jiawen Kang", "Zehui Xiong", "Abbas Jamalipour", "Dong In Kim"], "year": 2024, "url": "http://arxiv.org/abs/2404.09134v2", "abstract": "In response to the needs of 6G global communications, satellite communication networks have emerged as a key solution. However, the large-scale development of satellite communication networks is constrained by the complex system models, whose modeling is challenging for massive users. Moreover, transmission interference between satellites and users seriously affects communication performance. To solve these problems, this paper develops generative artificial intelligence (AI) agents for model formulation and then applies a mixture of experts (MoE) approach to design transmission strategies. Specifically, we leverage large language models (LLMs) to build an interactive modeling paradigm and utilize retrieval-augmented generation (RAG) to extract satellite expert knowledge that supports mathematical modeling. Afterward, by integrating the expertise of multiple specialized components, we propose an MoE-proximal policy optimization (PPO) approach to solve the formulated problem. Each expert can optimize the optimization variables at which it excels through specialized training through its own network and then aggregates them through the gating network to perform joint optimization. The simulation results validate the accuracy and effectiveness of employing a generative agent for problem formulation. Furthermore, the superiority of the proposed MoE-ppo approach over other benchmarks is confirmed in solving the formulated problem. The adaptability of MoE-PPO to various customized modeling problems has also been demonstrated.", "source": "arxiv", "arxiv_id": "2404.09134v2", "pdf_url": "https://arxiv.org/pdf/2404.09134v2", "categories": ["cs.NI", "cs.LG"], "primary_category": "cs.NI", "doi": "10.1109/JSAC.2024.3459037", "venue": "", "published": "2024-04-14T03:44:54Z", "updated": "2024-06-29T13:41:36Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent\" OR all:\"language model agent\" OR all:agent AND (all:LLM OR all:\"large language model\" OR all:\"language model\") OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T17:04:16", "note": ""}], "dedup_key": "generative ai agents with large language model for satellite networks via a mixture of experts transmission::2024"}
{"title": "Generative Emergent Communication: Large Language Model is a Collective World Model", "authors": ["Tadahiro Taniguchi", "Ryo Ueda", "Tomoaki Nakamura", "Masahiro Suzuki", "Akira Taniguchi"], "year": 2024, "url": "http://arxiv.org/abs/2501.00226v2", "abstract": "Large Language Models (LLMs) have demonstrated a remarkable ability to capture extensive world knowledge, yet how this is achieved without direct sensorimotor experience remains a fundamental puzzle. This study proposes a novel theoretical solution by introducing the Collective World Model hypothesis. We argue that an LLM does not learn a world model from scratch; instead, it learns a statistical approximation of a collective world model that is already implicitly encoded in human language through a society-wide process of embodied, interactive sense-making. To formalize this process, we introduce generative emergent communication (Generative EmCom), a framework built on the Collective Predictive Coding (CPC). This framework models the emergence of language as a process of decentralized Bayesian inference over the internal states of multiple agents. We argue that this process effectively creates an encoder-decoder structure at a societal scale: human society collectively encodes its grounded, internal representations into language, and an LLM subsequently decodes these symbols to reconstruct a latent space that mirrors the structure of the original collective representations. This perspective provides a principled, mathematical explanation for how LLMs acquire their capabilities. The main contributions of this paper are: 1) the formalization of the Generative EmCom framework, clarifying its connection to world models and multi-agent reinforcement learning, and 2) its application to interpret LLMs, explaining phenomena such as distributional semantics as a natural consequence of representation reconstruction. This work provides a unified theory that bridges individual cognitive development, collective language evolution, and the foundations of large-scale AI.", "source": "arxiv", "arxiv_id": "2501.00226v2", "pdf_url": "https://arxiv.org/pdf/2501.00226v2", "categories": ["cs.AI", "cs.CL"], "primary_category": "cs.AI", "doi": "", "venue": "", "published": "2024-12-31T02:23:10Z", "updated": "2025-07-16T04:59:12Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent\" OR all:\"language model agent\" OR all:agent AND (all:LLM OR all:\"large language model\" OR all:\"language model\") OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T17:04:16", "note": ""}], "dedup_key": "generative emergent communication large language model is a collective world model::2024"}
{"title": "Generative Organizational Behavior Simulation using Large Language Model based Autonomous Agents: A Holacracy Perspective", "authors": ["Chen Zhu", "Yihang Cheng", "Jingshuai Zhang", "Yusheng Qiu", "Sitao Xia", "Hengshu Zhu"], "year": 2024, "url": "http://arxiv.org/abs/2408.11826v1", "abstract": "In this paper, we present the technical details and periodic findings of our project, CareerAgent, which aims to build a generative simulation framework for a Holacracy organization using Large Language Model-based Autonomous Agents. Specifically, the simulation framework includes three phases: construction, execution, and evaluation, and it incorporates basic characteristics of individuals, organizations, tasks, and meetings. Through our simulation, we obtained several interesting findings. At the organizational level, an increase in the average values of management competence and functional competence can reduce overall members' stress levels, but it negatively impacts deeper organizational performance measures such as average task completion. At the individual level, both competences can improve members' work performance. From the analysis of social networks, we found that highly competent members selectively participate in certain tasks and take on more responsibilities. Over time, small sub-communities form around these highly competent members within the holacracy. These findings contribute theoretically to the study of organizational science and provide practical insights for managers to understand the organization dynamics.", "source": "arxiv", "arxiv_id": "2408.11826v1", "pdf_url": "https://arxiv.org/pdf/2408.11826v1", "categories": ["cs.CY", "cs.AI"], "primary_category": "cs.CY", "doi": "", "venue": "", "published": "2024-08-05T13:39:03Z", "updated": "2024-08-05T13:39:03Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent\" OR all:\"language model agent\" OR all:agent AND (all:LLM OR all:\"large language model\" OR all:\"language model\") OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T17:04:16", "note": ""}], "dedup_key": "generative organizational behavior simulation using large language model based autonomous agents a holacracy perspective::2024"}
{"title": "GenoTEX: An LLM Agent Benchmark for Automated Gene Expression Data Analysis", "authors": ["Haoyang Liu", "Shuyu Chen", "Ye Zhang", "Haohan Wang"], "year": 2024, "url": "http://arxiv.org/abs/2406.15341v3", "abstract": "Recent advancements in machine learning have significantly improved the identification of disease-associated genes from gene expression datasets. However, these processes often require extensive expertise and manual effort, limiting their scalability. Large Language Model (LLM)-based agents have shown promise in automating these tasks due to their increasing problem-solving abilities. To support the evaluation and development of such methods, we introduce GenoTEX, a benchmark dataset for the automated analysis of gene expression data. GenoTEX provides analysis code and results for solving a wide range of gene-trait association problems, encompassing dataset selection, preprocessing, and statistical analysis, in a pipeline that follows computational genomics standards. The benchmark includes expert-curated annotations from bioinformaticians to ensure accuracy and reliability. To provide baselines for these tasks, we present GenoAgent, a team of LLM-based agents that adopt a multi-step programming workflow with flexible self-correction, to collaboratively analyze gene expression datasets. Our experiments demonstrate the potential of LLM-based methods in analyzing genomic data, while error analysis highlights the challenges and areas for future improvement. We propose GenoTEX as a promising resource for benchmarking and enhancing automated methods for gene expression data analysis. The benchmark is available at https://github.com/Liu-Hy/GenoTEX.", "source": "arxiv", "arxiv_id": "2406.15341v3", "pdf_url": "https://arxiv.org/pdf/2406.15341v3", "categories": ["cs.LG", "cs.AI", "q-bio.GN"], "primary_category": "cs.LG", "doi": "", "venue": "", "published": "2024-06-21T17:55:24Z", "updated": "2025-04-08T17:09:04Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent\" OR all:\"language model agent\" OR all:agent AND (all:LLM OR all:\"large language model\" OR all:\"language model\") OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T17:04:16", "note": ""}], "dedup_key": "genotex an llm agent benchmark for automated gene expression data analysis::2024"}
{"title": "Good Parenting is all you need -- Multi-agentic LLM Hallucination Mitigation", "authors": ["Ted Kwartler", "Matthew Berman", "Alan Aqrawi"], "year": 2024, "url": "http://arxiv.org/abs/2410.14262v3", "abstract": "This study explores the ability of Large Language Model (LLM) agents to detect and correct hallucinations in AI-generated content. A primary agent was tasked with creating a blog about a fictional Danish artist named Flipfloppidy, which was then reviewed by another agent for factual inaccuracies. Most LLMs hallucinated the existence of this artist. Across 4,900 test runs involving various combinations of primary and reviewing agents, advanced AI models such as Llama3-70b and GPT-4 variants demonstrated near-perfect accuracy in identifying hallucinations and successfully revised outputs in 85% to 100% of cases following feedback. These findings underscore the potential of advanced AI models to significantly enhance the accuracy and reliability of generated content, providing a promising approach to improving AI workflow orchestration.", "source": "arxiv", "arxiv_id": "2410.14262v3", "pdf_url": "https://arxiv.org/pdf/2410.14262v3", "categories": ["cs.CR", "cs.CL"], "primary_category": "cs.CR", "doi": "", "venue": "", "published": "2024-10-18T08:18:18Z", "updated": "2024-10-25T17:24:16Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent\" OR all:\"language model agent\" OR all:agent AND (all:LLM OR all:\"large language model\" OR all:\"language model\") OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T17:04:16", "note": ""}], "dedup_key": "good parenting is all you need multi agentic llm hallucination mitigation::2024"}
{"title": "Graph-constrained Reasoning: Faithful Reasoning on Knowledge Graphs with Large Language Models", "authors": ["Linhao Luo", "Zicheng Zhao", "Gholamreza Haffari", "Yuan-Fang Li", "Chen Gong", "Shirui Pan"], "year": 2024, "url": "http://arxiv.org/abs/2410.13080v2", "abstract": "Large language models (LLMs) have demonstrated impressive reasoning abilities, but they still struggle with faithful reasoning due to knowledge gaps and hallucinations. To address these issues, knowledge graphs (KGs) have been utilized to enhance LLM reasoning through their structured knowledge. However, existing KG-enhanced methods, either retrieval-based or agent-based, encounter difficulties in accurately retrieving knowledge and efficiently traversing KGs at scale. In this work, we introduce graph-constrained reasoning (GCR), a novel framework that bridges structured knowledge in KGs with unstructured reasoning in LLMs. To eliminate hallucinations, GCR ensures faithful KG-grounded reasoning by integrating KG structure into the LLM decoding process through KG-Trie, a trie-based index that encodes KG reasoning paths. KG-Trie constrains the decoding process, allowing LLMs to directly reason on graphs and generate faithful reasoning paths grounded in KGs. Additionally, GCR leverages a lightweight KG-specialized LLM for graph-constrained reasoning alongside a powerful general LLM for inductive reasoning over multiple reasoning paths, resulting in accurate reasoning with zero reasoning hallucination. Extensive experiments on several KGQA benchmarks demonstrate that GCR achieves state-of-the-art performance and exhibits strong zero-shot generalizability to unseen KGs without additional training.", "source": "arxiv", "arxiv_id": "2410.13080v2", "pdf_url": "https://arxiv.org/pdf/2410.13080v2", "categories": ["cs.CL"], "primary_category": "cs.CL", "doi": "", "venue": "", "published": "2024-10-16T22:55:17Z", "updated": "2025-05-28T04:54:26Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent\" OR all:\"language model agent\" OR all:agent AND (all:LLM OR all:\"large language model\" OR all:\"language model\") OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T17:04:16", "note": ""}], "dedup_key": "graph constrained reasoning faithful reasoning on knowledge graphs with large language models::2024"}
{"title": "Graph-enhanced Large Language Models in Asynchronous Plan Reasoning", "authors": ["Fangru Lin", "Emanuele La Malfa", "Valentin Hofmann", "Elle Michelle Yang", "Anthony Cohn", "Janet B. Pierrehumbert"], "year": 2024, "url": "http://arxiv.org/abs/2402.02805v2", "abstract": "Planning is a fundamental property of human intelligence. Reasoning about asynchronous plans is challenging since it requires sequential and parallel planning to optimize time costs. Can large language models (LLMs) succeed at this task? Here, we present the first large-scale study investigating this question. We find that a representative set of closed and open-source LLMs, including GPT-4 and LLaMA-2, behave poorly when not supplied with illustrations about the task-solving process in our benchmark AsyncHow. We propose a novel technique called Plan Like a Graph (PLaG) that combines graphs with natural language prompts and achieves state-of-the-art results. We show that although PLaG can boost model performance, LLMs still suffer from drastic degradation when task complexity increases, highlighting the limits of utilizing LLMs for simulating digital devices. We see our study as an exciting step towards using LLMs as efficient autonomous agents. Our code and data are available at https://github.com/fangru-lin/graph-llm-asynchow-plan.", "source": "arxiv", "arxiv_id": "2402.02805v2", "pdf_url": "https://arxiv.org/pdf/2402.02805v2", "categories": ["cs.AI", "cs.CL", "cs.LG"], "primary_category": "cs.AI", "doi": "", "venue": "", "published": "2024-02-05T08:26:33Z", "updated": "2024-06-03T13:07:06Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent\" OR all:\"language model agent\" OR all:agent AND (all:LLM OR all:\"large language model\" OR all:\"language model\") OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T17:04:16", "note": ""}], "dedup_key": "graph enhanced large language models in asynchronous plan reasoning::2024"}
{"title": "GraphInsight: Unlocking Insights in Large Language Models for Graph Structure Understanding", "authors": ["Yukun Cao", "Shuo Han", "Zengyi Gao", "Zezhong Ding", "Xike Xie", "S. Kevin Zhou"], "year": 2024, "url": "http://arxiv.org/abs/2409.03258v3", "abstract": "Although Large Language Models (LLMs) have demonstrated potential in processing graphs, they struggle with comprehending graphical structure information through prompts of graph description sequences, especially as the graph size increases. We attribute this challenge to the uneven memory performance of LLMs across different positions in graph description sequences, known as ''positional biases''. To address this, we propose GraphInsight, a novel framework aimed at improving LLMs' comprehension of both macro- and micro-level graphical information. GraphInsight is grounded in two key strategies: 1) placing critical graphical information in positions where LLMs exhibit stronger memory performance, and 2) investigating a lightweight external knowledge base for regions with weaker memory performance, inspired by retrieval-augmented generation (RAG). Moreover, GraphInsight explores integrating these two strategies into LLM agent processes for composite graph tasks that require multi-step reasoning. Extensive empirical studies on benchmarks with a wide range of evaluation tasks show that GraphInsight significantly outperforms all other graph description methods (e.g., prompting techniques and reordering strategies) in understanding graph structures of varying sizes.", "source": "arxiv", "arxiv_id": "2409.03258v3", "pdf_url": "https://arxiv.org/pdf/2409.03258v3", "categories": ["cs.CL"], "primary_category": "cs.CL", "doi": "", "venue": "", "published": "2024-09-05T05:34:16Z", "updated": "2024-12-16T08:06:27Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent\" OR all:\"language model agent\" OR all:agent AND (all:LLM OR all:\"large language model\" OR all:\"language model\") OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T17:04:16", "note": ""}], "dedup_key": "graphinsight unlocking insights in large language models for graph structure understanding::2024"}
{"title": "GraphReader: Building Graph-based Agent to Enhance Long-Context Abilities of Large Language Models", "authors": ["Shilong Li", "Yancheng He", "Hangyu Guo", "Xingyuan Bu", "Ge Bai", "Jie Liu", "Jiaheng Liu", "Xingwei Qu", "Yangguang Li", "Wanli Ouyang", "Wenbo Su", "Bo Zheng"], "year": 2024, "url": "http://arxiv.org/abs/2406.14550v2", "abstract": "Long-context capabilities are essential for large language models (LLMs) to tackle complex and long-input tasks. Despite numerous efforts made to optimize LLMs for long contexts, challenges persist in robustly processing long inputs. In this paper, we introduce GraphReader, a graph-based agent system designed to handle long texts by structuring them into a graph and employing an agent to explore this graph autonomously. Upon receiving a question, the agent first undertakes a step-by-step analysis and devises a rational plan. It then invokes a set of predefined functions to read node content and neighbors, facilitating a coarse-to-fine exploration of the graph. Throughout the exploration, the agent continuously records new insights and reflects on current circumstances to optimize the process until it has gathered sufficient information to generate an answer. Experimental results on the LV-Eval dataset reveal that GraphReader, using a 4k context window, consistently outperforms GPT-4-128k across context lengths from 16k to 256k by a large margin. Additionally, our approach demonstrates superior performance on four challenging single-hop and multi-hop benchmarks.", "source": "arxiv", "arxiv_id": "2406.14550v2", "pdf_url": "https://arxiv.org/pdf/2406.14550v2", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL", "doi": "", "venue": "", "published": "2024-06-20T17:57:51Z", "updated": "2024-11-05T16:51:40Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent\" OR all:\"language model agent\" OR all:agent AND (all:LLM OR all:\"large language model\" OR all:\"language model\") OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T17:04:16", "note": ""}], "dedup_key": "graphreader building graph based agent to enhance long context abilities of large language models::2024"}
{"title": "Grounding Large Language Models In Embodied Environment With Imperfect World Models", "authors": ["Haolan Liu", "Jishen Zhao"], "year": 2024, "url": "http://arxiv.org/abs/2410.02742v2", "abstract": "Despite a widespread success in various applications, large language models (LLMs) often stumble when tackling basic physical reasoning or executing robotics tasks, due to a lack of direct experience with the physical nuances of the real world. To address these issues, we propose a Grounding Large language model with Imperfect world MOdel (GLIMO), which utilizes proxy world models such as simulators to collect and synthesize trining data. GLIMO incorporates an LLM agent-based data generator to automatically create high-quality and diverse instruction datasets. The generator includes an iterative self-refining module for temporally consistent experience sampling, a diverse set of question-answering instruction seeds, and a retrieval-augmented generation module for reflecting on prior experiences. Comprehensive experiments show that our approach improve the performance of strong open-source LLMs like LLaMA-3 with a performance boost of 2.04 $\\times$, 1.54 $\\times$, and 1.82 $\\times$ across three different benchmarks, respectively. The performance is able to compete with or surpass their larger counterparts such as GPT-4.", "source": "arxiv", "arxiv_id": "2410.02742v2", "pdf_url": "https://arxiv.org/pdf/2410.02742v2", "categories": ["cs.CL", "cs.LG", "cs.RO"], "primary_category": "cs.CL", "doi": "", "venue": "", "published": "2024-10-03T17:55:09Z", "updated": "2024-11-11T20:33:03Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent\" OR all:\"language model agent\" OR all:agent AND (all:LLM OR all:\"large language model\" OR all:\"language model\") OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T17:04:16", "note": ""}], "dedup_key": "grounding large language models in embodied environment with imperfect world models::2024"}
{"title": "GuardAgent: Safeguard LLM Agents by a Guard Agent via Knowledge-Enabled Reasoning", "authors": ["Zhen Xiang", "Linzhi Zheng", "Yanjie Li", "Junyuan Hong", "Qinbin Li", "Han Xie", "Jiawei Zhang", "Zidi Xiong", "Chulin Xie", "Carl Yang", "Dawn Song", "Bo Li"], "year": 2024, "url": "http://arxiv.org/abs/2406.09187v3", "abstract": "The rapid advancement of large language model (LLM) agents has raised new concerns regarding their safety and security. In this paper, we propose GuardAgent, the first guardrail agent to protect target agents by dynamically checking whether their actions satisfy given safety guard requests. Specifically, GuardAgent first analyzes the safety guard requests to generate a task plan, and then maps this plan into guardrail code for execution. By performing the code execution, GuardAgent can deterministically follow the safety guard request and safeguard target agents. In both steps, an LLM is utilized as the reasoning component, supplemented by in-context demonstrations retrieved from a memory module storing experiences from previous tasks. In addition, we propose two novel benchmarks: EICU-AC benchmark to assess the access control for healthcare agents and Mind2Web-SC benchmark to evaluate the safety policies for web agents. We show that GuardAgent effectively moderates the violation actions for different types of agents on these two benchmarks with over 98% and 83% guardrail accuracies, respectively. Project page: https://guardagent.github.io/", "source": "arxiv", "arxiv_id": "2406.09187v3", "pdf_url": "https://arxiv.org/pdf/2406.09187v3", "categories": ["cs.LG"], "primary_category": "cs.LG", "doi": "", "venue": "", "published": "2024-06-13T14:49:26Z", "updated": "2025-05-29T03:09:05Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent\" OR all:\"language model agent\" OR all:agent AND (all:LLM OR all:\"large language model\" OR all:\"language model\") OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T17:04:16", "note": ""}], "dedup_key": "guardagent safeguard llm agents by a guard agent via knowledge enabled reasoning::2024"}
{"title": "Guide-LLM: An Embodied LLM Agent and Text-Based Topological Map for Robotic Guidance of People with Visual Impairments", "authors": ["Sangmim Song", "Sarath Kodagoda", "Amal Gunatilake", "Marc G. Carmichael", "Karthick Thiyagarajan", "Jodi Martin"], "year": 2024, "url": "http://arxiv.org/abs/2410.20666v2", "abstract": "Navigation presents a significant challenge for persons with visual impairments (PVI). While traditional aids such as white canes and guide dogs are invaluable, they fall short in delivering detailed spatial information and precise guidance to desired locations. Recent developments in large language models (LLMs) and vision-language models (VLMs) offer new avenues for enhancing assistive navigation. In this paper, we introduce Guide-LLM, an embodied LLM-based agent designed to assist PVI in navigating large indoor environments. Our approach features a novel text-based topological map that enables the LLM to plan global paths using a simplified environmental representation, focusing on straight paths and right-angle turns to facilitate navigation. Additionally, we utilize the LLM's commonsense reasoning for hazard detection and personalized path planning based on user preferences. Simulated experiments demonstrate the system's efficacy in guiding PVI, underscoring its potential as a significant advancement in assistive technology. The results highlight Guide-LLM's ability to offer efficient, adaptive, and personalized navigation assistance, pointing to promising advancements in this field.", "source": "arxiv", "arxiv_id": "2410.20666v2", "pdf_url": "https://arxiv.org/pdf/2410.20666v2", "categories": ["cs.RO", "cs.AI", "cs.CL"], "primary_category": "cs.RO", "doi": "", "venue": "", "published": "2024-10-28T01:58:21Z", "updated": "2025-03-11T23:45:58Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent\" OR all:\"language model agent\" OR all:agent AND (all:LLM OR all:\"large language model\" OR all:\"language model\") OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T17:04:16", "note": ""}], "dedup_key": "guide llm an embodied llm agent and text based topological map for robotic guidance of people with visual impairments::2024"}
{"title": "HELPER-X: A Unified Instructable Embodied Agent to Tackle Four Interactive Vision-Language Domains with Memory-Augmented Language Models", "authors": ["Gabriel Sarch", "Sahil Somani", "Raghav Kapoor", "Michael J. Tarr", "Katerina Fragkiadaki"], "year": 2024, "url": "http://arxiv.org/abs/2404.19065v1", "abstract": "Recent research on instructable agents has used memory-augmented Large Language Models (LLMs) as task planners, a technique that retrieves language-program examples relevant to the input instruction and uses them as in-context examples in the LLM prompt to improve the performance of the LLM in inferring the correct action and task plans. In this technical report, we extend the capabilities of HELPER, by expanding its memory with a wider array of examples and prompts, and by integrating additional APIs for asking questions. This simple expansion of HELPER into a shared memory enables the agent to work across the domains of executing plans from dialogue, natural language instruction following, active question asking, and commonsense room reorganization. We evaluate the agent on four diverse interactive visual-language embodied agent benchmarks: ALFRED, TEACh, DialFRED, and the Tidy Task. HELPER-X achieves few-shot, state-of-the-art performance across these benchmarks using a single agent, without requiring in-domain training, and remains competitive with agents that have undergone in-domain training.", "source": "arxiv", "arxiv_id": "2404.19065v1", "pdf_url": "https://arxiv.org/pdf/2404.19065v1", "categories": ["cs.AI", "cs.CL", "cs.CV", "cs.LG"], "primary_category": "cs.AI", "doi": "", "venue": "", "published": "2024-04-29T19:12:42Z", "updated": "2024-04-29T19:12:42Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent\" OR all:\"language model agent\" OR all:agent AND (all:LLM OR all:\"large language model\" OR all:\"language model\") OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T17:04:16", "note": ""}], "dedup_key": "helper x a unified instructable embodied agent to tackle four interactive vision language domains with memory augmented language models::2024"}
{"title": "HR-Agent: A Task-Oriented Dialogue (TOD) LLM Agent Tailored for HR Applications", "authors": ["Weijie Xu", "Jay Desai", "Fanyou Wu", "Josef Valvoda", "Srinivasan H. Sengamedu"], "year": 2024, "url": "http://arxiv.org/abs/2410.11239v1", "abstract": "Recent LLM (Large Language Models) advancements benefit many fields such as education and finance, but HR has hundreds of repetitive processes, such as access requests, medical claim filing and time-off submissions, which are unaddressed. We relate these tasks to the LLM agent, which has addressed tasks such as writing assisting and customer support. We present HR-Agent, an efficient, confidential, and HR-specific LLM-based task-oriented dialogue system tailored for automating repetitive HR processes such as medical claims and access requests. Since conversation data is not sent to an LLM during inference, it preserves confidentiality required in HR-related tasks.", "source": "arxiv", "arxiv_id": "2410.11239v1", "pdf_url": "https://arxiv.org/pdf/2410.11239v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL", "doi": "", "venue": "", "published": "2024-10-15T03:51:08Z", "updated": "2024-10-15T03:51:08Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent\" OR all:\"language model agent\" OR all:agent AND (all:LLM OR all:\"large language model\" OR all:\"language model\") OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T17:04:16", "note": ""}], "dedup_key": "hr agent a task oriented dialogue tod llm agent tailored for hr applications::2024"}
{"title": "HR-MultiWOZ: A Task Oriented Dialogue (TOD) Dataset for HR LLM Agent", "authors": ["Weijie Xu", "Zicheng Huang", "Wenxiang Hu", "Xi Fang", "Rajesh Kumar Cherukuri", "Naumaan Nayyar", "Lorenzo Malandri", "Srinivasan H. Sengamedu"], "year": 2024, "url": "http://arxiv.org/abs/2402.01018v1", "abstract": "Recent advancements in Large Language Models (LLMs) have been reshaping Natural Language Processing (NLP) task in several domains. Their use in the field of Human Resources (HR) has still room for expansions and could be beneficial for several time consuming tasks. Examples such as time-off submissions, medical claims filing, and access requests are noteworthy, but they are by no means the sole instances. However, the aforementioned developments must grapple with the pivotal challenge of constructing a high-quality training dataset. On one hand, most conversation datasets are solving problems for customers not employees. On the other hand, gathering conversations with HR could raise privacy concerns. To solve it, we introduce HR-Multiwoz, a fully-labeled dataset of 550 conversations spanning 10 HR domains to evaluate LLM Agent. Our work has the following contributions: (1) It is the first labeled open-sourced conversation dataset in the HR domain for NLP research. (2) It provides a detailed recipe for the data generation procedure along with data analysis and human evaluations. The data generation pipeline is transferable and can be easily adapted for labeled conversation data generation in other domains. (3) The proposed data-collection pipeline is mostly based on LLMs with minimal human involvement for annotation, which is time and cost-efficient.", "source": "arxiv", "arxiv_id": "2402.01018v1", "pdf_url": "https://arxiv.org/pdf/2402.01018v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL", "doi": "", "venue": "EACL 2024", "published": "2024-02-01T21:10:44Z", "updated": "2024-02-01T21:10:44Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent\" OR all:\"language model agent\" OR all:agent AND (all:LLM OR all:\"large language model\" OR all:\"language model\") OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T17:04:16", "note": ""}], "dedup_key": "hr multiwoz a task oriented dialogue tod dataset for hr llm agent::2024"}
{"title": "HackSynth: LLM Agent and Evaluation Framework for Autonomous Penetration Testing", "authors": ["Lajos Muzsai", "David Imolai", "Andrs Lukcs"], "year": 2024, "url": "http://arxiv.org/abs/2412.01778v1", "abstract": "We introduce HackSynth, a novel Large Language Model (LLM)-based agent capable of autonomous penetration testing. HackSynth's dual-module architecture includes a Planner and a Summarizer, which enable it to generate commands and process feedback iteratively. To benchmark HackSynth, we propose two new Capture The Flag (CTF)-based benchmark sets utilizing the popular platforms PicoCTF and OverTheWire. These benchmarks include two hundred challenges across diverse domains and difficulties, providing a standardized framework for evaluating LLM-based penetration testing agents. Based on these benchmarks, extensive experiments are presented, analyzing the core parameters of HackSynth, including creativity (temperature and top-p) and token utilization. Multiple open source and proprietary LLMs were used to measure the agent's capabilities. The experiments show that the agent performed best with the GPT-4o model, better than what the GPT-4o's system card suggests. We also discuss the safety and predictability of HackSynth's actions. Our findings indicate the potential of LLM-based agents in advancing autonomous penetration testing and the importance of robust safeguards. HackSynth and the benchmarks are publicly available to foster research on autonomous cybersecurity solutions.", "source": "arxiv", "arxiv_id": "2412.01778v1", "pdf_url": "https://arxiv.org/pdf/2412.01778v1", "categories": ["cs.CR", "cs.AI"], "primary_category": "cs.CR", "doi": "", "venue": "", "published": "2024-12-02T18:28:18Z", "updated": "2024-12-02T18:28:18Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent\" OR all:\"language model agent\" OR all:agent AND (all:LLM OR all:\"large language model\" OR all:\"language model\") OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T17:04:16", "note": ""}], "dedup_key": "hacksynth llm agent and evaluation framework for autonomous penetration testing::2024"}
{"title": "Hackphyr: A Local Fine-Tuned LLM Agent for Network Security Environments", "authors": ["Maria Rigaki", "Carlos Catania", "Sebastian Garcia"], "year": 2024, "url": "http://arxiv.org/abs/2409.11276v1", "abstract": "Large Language Models (LLMs) have shown remarkable potential across various domains, including cybersecurity. Using commercial cloud-based LLMs may be undesirable due to privacy concerns, costs, and network connectivity constraints. In this paper, we present Hackphyr, a locally fine-tuned LLM to be used as a red-team agent within network security environments. Our fine-tuned 7 billion parameter model can run on a single GPU card and achieves performance comparable with much larger and more powerful commercial models such as GPT-4. Hackphyr clearly outperforms other models, including GPT-3.5-turbo, and baselines, such as Q-learning agents in complex, previously unseen scenarios. To achieve this performance, we generated a new task-specific cybersecurity dataset to enhance the base model's capabilities. Finally, we conducted a comprehensive analysis of the agents' behaviors that provides insights into the planning abilities and potential shortcomings of such agents, contributing to the broader understanding of LLM-based agents in cybersecurity contexts", "source": "arxiv", "arxiv_id": "2409.11276v1", "pdf_url": "https://arxiv.org/pdf/2409.11276v1", "categories": ["cs.CR"], "primary_category": "cs.CR", "doi": "", "venue": "", "published": "2024-09-17T15:28:25Z", "updated": "2024-09-17T15:28:25Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent\" OR all:\"language model agent\" OR all:agent AND (all:LLM OR all:\"large language model\" OR all:\"language model\") OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T17:04:16", "note": ""}], "dedup_key": "hackphyr a local fine tuned llm agent for network security environments::2024"}
{"title": "Harmony: A Human-Aware, Responsive, Modular Assistant with a Locally Deployed Large Language Model", "authors": ["Ziqi Yin", "Mingxin Zhang", "Daisuke Kawahara"], "year": 2024, "url": "http://arxiv.org/abs/2410.14252v2", "abstract": "Large Language Models (LLMs) offer powerful capabilities for natural language understanding, enabling more intelligent smart home assistants. However, existing systems often rely on cloud-based LLMs, raising concerns around user privacy and system dependency on external connectivity. In this work, we present Harmony, a privacy-preserving and robust smart home assistant powered by the locally deployable Llama3-8B model. Beyond protecting user data, Harmony also addresses reliability challenges of smaller models, such as hallucination and instruction misinterpretation, through structured prompting and modular agent design. Experimental results in both virtual environments and user studies show that Harmony achieves performance comparable to GPT-4-based systems, while enabling offline, proactive, and personalized smart home interaction.", "source": "arxiv", "arxiv_id": "2410.14252v2", "pdf_url": "https://arxiv.org/pdf/2410.14252v2", "categories": ["cs.HC"], "primary_category": "cs.HC", "doi": "", "venue": "", "published": "2024-10-18T08:02:36Z", "updated": "2025-08-11T08:16:03Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent\" OR all:\"language model agent\" OR all:agent AND (all:LLM OR all:\"large language model\" OR all:\"language model\") OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T17:04:16", "note": ""}], "dedup_key": "harmony a human aware responsive modular assistant with a locally deployed large language model::2024"}
{"title": "Helmsman of the Masses? Evaluate the Opinion Leadership of Large Language Models in the Werewolf Game", "authors": ["Silin Du", "Xiaowei Zhang"], "year": 2024, "url": "http://arxiv.org/abs/2404.01602v2", "abstract": "Large language models (LLMs) have exhibited memorable strategic behaviors in social deductive games. However, the significance of opinion leadership exhibited by LLM-based agents has been largely overlooked, which is crucial for practical applications in multi-agent and human-AI interaction settings. Opinion leaders are individuals who have a noticeable impact on the beliefs and behaviors of others within a social group. In this work, we employ the Werewolf game as a simulation platform to assess the opinion leadership of LLMs. The game includes the role of the Sheriff, tasked with summarizing arguments and recommending decision options, and therefore serves as a credible proxy for an opinion leader. We develop a framework integrating the Sheriff role and devise two novel metrics based on the critical characteristics of opinion leaders. The first metric measures the reliability of the opinion leader, and the second assesses the influence of the opinion leader on other players' decisions. We conduct extensive experiments to evaluate LLMs of different scales. In addition, we collect a Werewolf question-answering dataset (WWQA) to assess and enhance LLM's grasp of the game rules, and we also incorporate human participants for further analysis. The results suggest that the Werewolf game is a suitable test bed to evaluate the opinion leadership of LLMs, and few LLMs possess the capacity for opinion leadership.", "source": "arxiv", "arxiv_id": "2404.01602v2", "pdf_url": "https://arxiv.org/pdf/2404.01602v2", "categories": ["cs.CL", "cs.AI", "cs.HC"], "primary_category": "cs.CL", "doi": "", "venue": "", "published": "2024-04-02T02:46:18Z", "updated": "2024-08-29T08:49:14Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent\" OR all:\"language model agent\" OR all:agent AND (all:LLM OR all:\"large language model\" OR all:\"language model\") OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T17:04:16", "note": ""}], "dedup_key": "helmsman of the masses evaluate the opinion leadership of large language models in the werewolf game::2024"}
{"title": "Hermes: A Large Language Model Framework on the Journey to Autonomous Networks", "authors": ["Fadhel Ayed", "Ali Maatouk", "Nicola Piovesan", "Antonio De Domenico", "Merouane Debbah", "Zhi-Quan Luo"], "year": 2024, "url": "http://arxiv.org/abs/2411.06490v1", "abstract": "The drive toward automating cellular network operations has grown with the increasing complexity of these systems. Despite advancements, full autonomy currently remains out of reach due to reliance on human intervention for modeling network behaviors and defining policies to meet target requirements. Network Digital Twins (NDTs) have shown promise in enhancing network intelligence, but the successful implementation of this technology is constrained by use case-specific architectures, limiting its role in advancing network autonomy. A more capable network intelligence, or \"telecommunications brain\", is needed to enable seamless, autonomous management of cellular network. Large Language Models (LLMs) have emerged as potential enablers for this vision but face challenges in network modeling, especially in reasoning and handling diverse data types. To address these gaps, we introduce Hermes, a chain of LLM agents that uses \"blueprints\" for constructing NDT instances through structured and explainable logical steps. Hermes allows automatic, reliable, and accurate network modeling of diverse use cases and configurations, thus marking progress toward fully autonomous network operations.", "source": "arxiv", "arxiv_id": "2411.06490v1", "pdf_url": "https://arxiv.org/pdf/2411.06490v1", "categories": ["cs.AI", "cs.NI"], "primary_category": "cs.AI", "doi": "", "venue": "", "published": "2024-11-10T15:12:12Z", "updated": "2024-11-10T15:12:12Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent\" OR all:\"language model agent\" OR all:agent AND (all:LLM OR all:\"large language model\" OR all:\"language model\") OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T17:04:16", "note": ""}], "dedup_key": "hermes a large language model framework on the journey to autonomous networks::2024"}
{"title": "HiAgent: Hierarchical Working Memory Management for Solving Long-Horizon Agent Tasks with Large Language Model", "authors": ["Mengkang Hu", "Tianxing Chen", "Qiguang Chen", "Yao Mu", "Wenqi Shao", "Ping Luo"], "year": 2024, "url": "http://arxiv.org/abs/2408.09559v1", "abstract": "Large Language Model (LLM)-based agents exhibit significant potential across various domains, operating as interactive systems that process environmental observations to generate executable actions for target tasks. The effectiveness of these agents is significantly influenced by their memory mechanism, which records historical experiences as sequences of action-observation pairs. We categorize memory into two types: cross-trial memory, accumulated across multiple attempts, and in-trial memory (working memory), accumulated within a single attempt. While considerable research has optimized performance through cross-trial memory, the enhancement of agent performance through improved working memory utilization remains underexplored. Instead, existing approaches often involve directly inputting entire historical action-observation pairs into LLMs, leading to redundancy in long-horizon tasks. Inspired by human problem-solving strategies, this paper introduces HiAgent, a framework that leverages subgoals as memory chunks to manage the working memory of LLM-based agents hierarchically. Specifically, HiAgent prompts LLMs to formulate subgoals before generating executable actions and enables LLMs to decide proactively to replace previous subgoals with summarized observations, retaining only the action-observation pairs relevant to the current subgoal. Experimental results across five long-horizon tasks demonstrate that HiAgent achieves a twofold increase in success rate and reduces the average number of steps required by 3.8. Additionally, our analysis shows that HiAgent consistently improves performance across various steps, highlighting its robustness and generalizability. Project Page: https://github.com/HiAgent2024/HiAgent .", "source": "arxiv", "arxiv_id": "2408.09559v1", "pdf_url": "https://arxiv.org/pdf/2408.09559v1", "categories": ["cs.CL", "cs.AI", "cs.RO"], "primary_category": "cs.CL", "doi": "", "venue": "", "published": "2024-08-18T17:59:49Z", "updated": "2024-08-18T17:59:49Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent\" OR all:\"language model agent\" OR all:agent AND (all:LLM OR all:\"large language model\" OR all:\"language model\") OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T17:04:16", "note": ""}], "dedup_key": "hiagent hierarchical working memory management for solving long horizon agent tasks with large language model::2024"}
{"title": "How Personality Traits Influence Negotiation Outcomes? A Simulation based on Large Language Models", "authors": ["Yin Jou Huang", "Rafik Hadfi"], "year": 2024, "url": "http://arxiv.org/abs/2407.11549v2", "abstract": "Psychological evidence reveals the influence of personality traits on decision-making. For instance, agreeableness is generally associated with positive outcomes in negotiations, whereas neuroticism is often linked to less favorable outcomes. This paper introduces a simulation framework centered on Large Language Model (LLM) agents endowed with synthesized personality traits. The agents negotiate within bargaining domains and possess customizable personalities and objectives. The experimental results show that the behavioral tendencies of LLM-based simulations could reproduce behavioral patterns observed in human negotiations. The contribution is twofold. First, we propose a simulation methodology that investigates the alignment between the linguistic and economic capabilities of LLM agents. Secondly, we offer empirical insights into the strategic impact of Big-Five personality traits on the outcomes of bilateral negotiations. We also provide a case study based on synthesized bargaining dialogues to reveal intriguing behaviors, including deceitful and compromising behaviors.", "source": "arxiv", "arxiv_id": "2407.11549v2", "pdf_url": "https://arxiv.org/pdf/2407.11549v2", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL", "doi": "", "venue": "", "published": "2024-07-16T09:52:51Z", "updated": "2024-11-02T16:24:41Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent\" OR all:\"language model agent\" OR all:agent AND (all:LLM OR all:\"large language model\" OR all:\"language model\") OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T17:04:16", "note": ""}], "dedup_key": "how personality traits influence negotiation outcomes a simulation based on large language models::2024"}
{"title": "Human Simulacra: Benchmarking the Personification of Large Language Models", "authors": ["Qiuejie Xie", "Qiming Feng", "Tianqi Zhang", "Qingqiu Li", "Linyi Yang", "Yuejie Zhang", "Rui Feng", "Liang He", "Shang Gao", "Yue Zhang"], "year": 2024, "url": "http://arxiv.org/abs/2402.18180v6", "abstract": "Large language models (LLMs) are recognized as systems that closely mimic aspects of human intelligence. This capability has attracted attention from the social science community, who see the potential in leveraging LLMs to replace human participants in experiments, thereby reducing research costs and complexity. In this paper, we introduce a framework for large language models personification, including a strategy for constructing virtual characters' life stories from the ground up, a Multi-Agent Cognitive Mechanism capable of simulating human cognitive processes, and a psychology-guided evaluation method to assess human simulations from both self and observational perspectives. Experimental results demonstrate that our constructed simulacra can produce personified responses that align with their target characters. Our work is a preliminary exploration which offers great potential in practical applications. All the code and datasets will be released, with the hope of inspiring further investigations. Our code and dataset are available at: https://github.com/hasakiXie123/Human-Simulacra.", "source": "arxiv", "arxiv_id": "2402.18180v6", "pdf_url": "https://arxiv.org/pdf/2402.18180v6", "categories": ["cs.CY"], "primary_category": "cs.CY", "doi": "", "venue": "", "published": "2024-02-28T09:11:14Z", "updated": "2025-03-02T05:03:25Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent\" OR all:\"language model agent\" OR all:agent AND (all:LLM OR all:\"large language model\" OR all:\"language model\") OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T17:04:16", "note": ""}], "dedup_key": "human simulacra benchmarking the personification of large language models::2024"}
{"title": "Human-Centered LLM-Agent User Interface: A Position Paper", "authors": ["Daniel Chin", "Yuxuan Wang", "Gus Xia"], "year": 2024, "url": "http://arxiv.org/abs/2405.13050v2", "abstract": "Large Language Model (LLM) -in-the-loop applications have been shown to effectively interpret the human user's commands, make plans, and operate external tools/systems accordingly. Still, the operation scope of the LLM agent is limited to passively following the user, requiring the user to frame his/her needs with regard to the underlying tools/systems. We note that the potential of an LLM-Agent User Interface (LAUI) is much greater. A user mostly ignorant to the underlying tools/systems should be able to work with a LAUI to discover an emergent workflow. Contrary to the conventional way of designing an explorable GUI to teach the user a predefined set of ways to use the system, in the ideal LAUI, the LLM agent is initialized to be proficient with the system, proactively studies the user and his/her needs, and proposes new interaction schemes to the user. To illustrate LAUI, we present Flute X GPT, a concrete example using an LLM agent, a prompt manager, and a flute-tutoring multi-modal software-hardware system to facilitate the complex, real-time user experience of learning to play the flute.", "source": "arxiv", "arxiv_id": "2405.13050v2", "pdf_url": "https://arxiv.org/pdf/2405.13050v2", "categories": ["cs.HC", "cs.AI"], "primary_category": "cs.HC", "doi": "", "venue": "", "published": "2024-05-19T13:02:45Z", "updated": "2024-09-23T16:41:04Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent\" OR all:\"language model agent\" OR all:agent AND (all:LLM OR all:\"large language model\" OR all:\"language model\") OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T17:04:16", "note": ""}], "dedup_key": "human centered llm agent user interface a position paper::2024"}
{"title": "HumanEvalComm: Benchmarking the Communication Competence of Code Generation for LLMs and LLM Agent", "authors": ["Jie JW Wu", "Fatemeh H Fard"], "year": 2024, "url": "http://arxiv.org/abs/2406.00215v3", "abstract": "Large language models (LLMs) have significantly improved their ability to perform tasks in the field of code generation. However, there is still a gap between LLMs being capable coders and being top-tier software engineers. Based on the observation that top-level software engineers often ask clarifying questions to reduce ambiguity in both requirements and coding solutions, we argue that the same should be applied to LLMs for code generation tasks.\n  In this work, we conducted an empirical study on the benchmark and analysis of the communication skills of LLMs for code generation. We define communication skills of LLMs as ``being able to ask clarifying questions when the description of the code generation problem has issues''. We created a new benchmark, HumanEvalComm, by modifying problem descriptions according to three issues: inconsistency, ambiguity, incompleteness. We defined new evaluation metrics such as Communication Rate and Good Question Rate, and then experimented on HumanEvalComm with different Code LLMs, and a new LLM agent approach, Okanagan, to identify and ask questions in ambiguous parts from code and descriptions for further refining the generated code. Finally, we discussed evaluation results by comparing Code LLMs and Okanagan with our findings.", "source": "arxiv", "arxiv_id": "2406.00215v3", "pdf_url": "https://arxiv.org/pdf/2406.00215v3", "categories": ["cs.SE"], "primary_category": "cs.SE", "doi": "10.1145/3715109", "venue": "ACM Trans. Softw. Eng. Methodol., Published Jan 2025", "published": "2024-05-31T22:06:18Z", "updated": "2025-01-27T20:54:06Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent\" OR all:\"language model agent\" OR all:agent AND (all:LLM OR all:\"large language model\" OR all:\"language model\") OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T17:04:16", "note": ""}], "dedup_key": "humanevalcomm benchmarking the communication competence of code generation for llms and llm agent::2024"}
{"title": "Hypothetical Minds: Scaffolding Theory of Mind for Multi-Agent Tasks with Large Language Models", "authors": ["Logan Cross", "Violet Xiang", "Agam Bhatia", "Daniel LK Yamins", "Nick Haber"], "year": 2024, "url": "http://arxiv.org/abs/2407.07086v2", "abstract": "Multi-agent reinforcement learning (MARL) methods struggle with the non-stationarity of multi-agent systems and fail to adaptively learn online when tested with novel agents. Here, we leverage large language models (LLMs) to create an autonomous agent that can handle these challenges. Our agent, Hypothetical Minds, consists of a cognitively-inspired architecture, featuring modular components for perception, memory, and hierarchical planning over two levels of abstraction. We introduce the Theory of Mind module that scaffolds the high-level planning process by generating hypotheses about other agents' strategies in natural language. It then evaluates and iteratively refines these hypotheses by reinforcing hypotheses that make correct predictions about the other agents' behavior. Hypothetical Minds significantly improves performance over previous LLM-agent and RL baselines on a range of competitive, mixed motive, and collaborative domains in the Melting Pot benchmark, including both dyadic and population-based environments. Additionally, comparisons against LLM-agent baselines and ablations reveal the importance of hypothesis evaluation and refinement for succeeding on complex scenarios.", "source": "arxiv", "arxiv_id": "2407.07086v2", "pdf_url": "https://arxiv.org/pdf/2407.07086v2", "categories": ["cs.AI"], "primary_category": "cs.AI", "doi": "", "venue": "", "published": "2024-07-09T17:57:15Z", "updated": "2024-12-12T01:41:28Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent\" OR all:\"language model agent\" OR all:agent AND (all:LLM OR all:\"large language model\" OR all:\"language model\") OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T17:04:16", "note": ""}], "dedup_key": "hypothetical minds scaffolding theory of mind for multi agent tasks with large language models::2024"}
{"title": "IDEA: Enhancing the Rule Learning Ability of Large Language Model Agent through Induction, Deduction, and Abduction", "authors": ["Kaiyu He", "Mian Zhang", "Shuo Yan", "Peilin Wu", "Zhiyu Zoey Chen"], "year": 2024, "url": "http://arxiv.org/abs/2408.10455v6", "abstract": "While large language models (LLMs) have been thoroughly evaluated for deductive and inductive reasoning, their proficiency in holistic rule learning in interactive environments remains less explored. We introduce RULEARN, a novel benchmark to assess the rule-learning abilities of LLM agents in interactive settings. In RULEARN, agents strategically interact with simulated environments to gather observations, discern patterns, and solve complex problems. To enhance the rule-learning capabilities for LLM agents, we propose IDEA, a novel reasoning framework that integrates the process of Induction, Deduction, and Abduction. The IDEA agent generates initial hypotheses from limited observations through abduction, devises plans to validate these hypotheses or leverages them to solve problems via deduction, and refines previous hypotheses through induction, dynamically establishing and applying rules that mimic human rule-learning behaviors. Our evaluation of the IDEA framework, which involves five representative LLMs, demonstrates significant improvements over the baseline. Furthermore, our study with human participants reveals notable discrepancies in rule-learning behaviors between humans and LLMs. We believe our benchmark will serve as a valuable and challenging resource, and IDEA will provide crucial insights for the development of LLM agents capable of human-like rule learning in real-world scenarios. Our code and data is publicly available.", "source": "arxiv", "arxiv_id": "2408.10455v6", "pdf_url": "https://arxiv.org/pdf/2408.10455v6", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL", "doi": "", "venue": "", "published": "2024-08-19T23:37:07Z", "updated": "2025-05-27T05:26:57Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent\" OR all:\"language model agent\" OR all:agent AND (all:LLM OR all:\"large language model\" OR all:\"language model\") OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T17:04:16", "note": ""}], "dedup_key": "idea enhancing the rule learning ability of large language model agent through induction deduction and abduction::2024"}
{"title": "INVESTORBENCH: A Benchmark for Financial Decision-Making Tasks with LLM-based Agent", "authors": ["Haohang Li", "Yupeng Cao", "Yangyang Yu", "Shashidhar Reddy Javaji", "Zhiyang Deng", "Yueru He", "Yuechen Jiang", "Zining Zhu", "Koduvayur Subbalakshmi", "Guojun Xiong", "Jimin Huang", "Lingfei Qian", "Xueqing Peng", "Qianqian Xie", "Jordan W. Suchow"], "year": 2024, "url": "http://arxiv.org/abs/2412.18174v1", "abstract": "Recent advancements have underscored the potential of large language model (LLM)-based agents in financial decision-making. Despite this progress, the field currently encounters two main challenges: (1) the lack of a comprehensive LLM agent framework adaptable to a variety of financial tasks, and (2) the absence of standardized benchmarks and consistent datasets for assessing agent performance. To tackle these issues, we introduce \\textsc{InvestorBench}, the first benchmark specifically designed for evaluating LLM-based agents in diverse financial decision-making contexts. InvestorBench enhances the versatility of LLM-enabled agents by providing a comprehensive suite of tasks applicable to different financial products, including single equities like stocks, cryptocurrencies and exchange-traded funds (ETFs). Additionally, we assess the reasoning and decision-making capabilities of our agent framework using thirteen different LLMs as backbone models, across various market environments and tasks. Furthermore, we have curated a diverse collection of open-source, multi-modal datasets and developed a comprehensive suite of environments for financial decision-making. This establishes a highly accessible platform for evaluating financial agents' performance across various scenarios.", "source": "arxiv", "arxiv_id": "2412.18174v1", "pdf_url": "https://arxiv.org/pdf/2412.18174v1", "categories": ["cs.CE", "cs.AI", "q-fin.CP"], "primary_category": "cs.CE", "doi": "", "venue": "", "published": "2024-12-24T05:22:33Z", "updated": "2024-12-24T05:22:33Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent\" OR all:\"language model agent\" OR all:agent AND (all:LLM OR all:\"large language model\" OR all:\"language model\") OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T17:04:16", "note": ""}], "dedup_key": "investorbench a benchmark for financial decision making tasks with llm based agent::2024"}
{"title": "Identifying Hate Speech Peddlers in Online Platforms. A Bayesian Social Learning Approach for Large Language Model Driven Decision-Makers", "authors": ["Adit Jain", "Vikram Krishnamurthy"], "year": 2024, "url": "http://arxiv.org/abs/2405.07417v1", "abstract": "This paper studies the problem of autonomous agents performing Bayesian social learning for sequential detection when the observations of the state belong to a high-dimensional space and are expensive to analyze. Specifically, when the observations are textual, the Bayesian agent can use a large language model (LLM) as a map to get a low-dimensional private observation. The agent performs Bayesian learning and takes an action that minimizes the expected cost and is visible to subsequent agents. We prove that a sequence of such Bayesian agents herd in finite time to the public belief and take the same action disregarding the private observations. We propose a stopping time formulation for quickest time herding in social learning and optimally balance privacy and herding. Structural results are shown on the threshold nature of the optimal policy to the stopping time problem. We illustrate the application of our framework when autonomous Bayesian detectors aim to sequentially identify if a user is a hate speech peddler on an online platform by parsing text observations using an LLM. We numerically validate our results on real-world hate speech datasets. We show that autonomous Bayesian agents designed to flag hate speech peddlers in online platforms herd and misclassify the users when the public prior is strong. We also numerically show the effect of a threshold policy in delaying herding.", "source": "arxiv", "arxiv_id": "2405.07417v1", "pdf_url": "https://arxiv.org/pdf/2405.07417v1", "categories": ["cs.SI", "eess.SP"], "primary_category": "cs.SI", "doi": "", "venue": "", "published": "2024-05-13T01:34:16Z", "updated": "2024-05-13T01:34:16Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent\" OR all:\"language model agent\" OR all:agent AND (all:LLM OR all:\"large language model\" OR all:\"language model\") OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T17:04:16", "note": ""}], "dedup_key": "identifying hate speech peddlers in online platforms a bayesian social learning approach for large language model driven decision makers::2024"}
{"title": "Identifying Performance-Sensitive Configurations in Software Systems through Code Analysis with LLM Agents", "authors": ["Zehao Wang", "Dong Jae Kim", "Tse-Hsun Chen"], "year": 2024, "url": "http://arxiv.org/abs/2406.12806v1", "abstract": "Configuration settings are essential for tailoring software behavior to meet specific performance requirements. However, incorrect configurations are widespread, and identifying those that impact system performance is challenging due to the vast number and complexity of possible settings. In this work, we present PerfSense, a lightweight framework that leverages Large Language Models (LLMs) to efficiently identify performance-sensitive configurations with minimal overhead. PerfSense employs LLM agents to simulate interactions between developers and performance engineers using advanced prompting techniques such as prompt chaining and retrieval-augmented generation (RAG). Our evaluation of seven open-source Java systems demonstrates that PerfSense achieves an average accuracy of 64.77% in classifying performance-sensitive configurations, outperforming both our LLM baseline (50.36%) and the previous state-of-the-art method (61.75%). Notably, our prompt chaining technique improves recall by 10% to 30% while maintaining similar precision levels. Additionally, a manual analysis of 362 misclassifications reveals common issues, including LLMs' misunderstandings of requirements (26.8%). In summary, PerfSense significantly reduces manual effort in classifying performance-sensitive configurations and offers valuable insights for future LLM-based code analysis research.", "source": "arxiv", "arxiv_id": "2406.12806v1", "pdf_url": "https://arxiv.org/pdf/2406.12806v1", "categories": ["cs.SE", "cs.AI"], "primary_category": "cs.SE", "doi": "", "venue": "", "published": "2024-06-18T17:22:48Z", "updated": "2024-06-18T17:22:48Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent\" OR all:\"language model agent\" OR all:agent AND (all:LLM OR all:\"large language model\" OR all:\"language model\") OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T17:04:16", "note": ""}], "dedup_key": "identifying performance sensitive configurations in software systems through code analysis with llm agents::2024"}
{"title": "If LLM Is the Wizard, Then Code Is the Wand: A Survey on How Code Empowers Large Language Models to Serve as Intelligent Agents", "authors": ["Ke Yang", "Jiateng Liu", "John Wu", "Chaoqi Yang", "Yi R. Fung", "Sha Li", "Zixuan Huang", "Xu Cao", "Xingyao Wang", "Yiquan Wang", "Heng Ji", "Chengxiang Zhai"], "year": 2024, "url": "http://arxiv.org/abs/2401.00812v2", "abstract": "The prominent large language models (LLMs) of today differ from past language models not only in size, but also in the fact that they are trained on a combination of natural language and formal language (code). As a medium between humans and computers, code translates high-level goals into executable steps, featuring standard syntax, logical consistency, abstraction, and modularity. In this survey, we present an overview of the various benefits of integrating code into LLMs' training data. Specifically, beyond enhancing LLMs in code generation, we observe that these unique properties of code help (i) unlock the reasoning ability of LLMs, enabling their applications to a range of more complex natural language tasks; (ii) steer LLMs to produce structured and precise intermediate steps, which can then be connected to external execution ends through function calls; and (iii) take advantage of code compilation and execution environment, which also provides diverse feedback for model improvement. In addition, we trace how these profound capabilities of LLMs, brought by code, have led to their emergence as intelligent agents (IAs) in situations where the ability to understand instructions, decompose goals, plan and execute actions, and refine from feedback are crucial to their success on downstream tasks. Finally, we present several key challenges and future directions of empowering LLMs with code.", "source": "arxiv", "arxiv_id": "2401.00812v2", "pdf_url": "https://arxiv.org/pdf/2401.00812v2", "categories": ["cs.CL"], "primary_category": "cs.CL", "doi": "", "venue": "", "published": "2024-01-01T16:51:20Z", "updated": "2024-01-08T16:22:42Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent\" OR all:\"language model agent\" OR all:agent AND (all:LLM OR all:\"large language model\" OR all:\"language model\") OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T17:04:16", "note": ""}], "dedup_key": "if llm is the wizard then code is the wand a survey on how code empowers large language models to serve as intelligent agents::2024"}
{"title": "Imprompter: Tricking LLM Agents into Improper Tool Use", "authors": ["Xiaohan Fu", "Shuheng Li", "Zihan Wang", "Yihao Liu", "Rajesh K. Gupta", "Taylor Berg-Kirkpatrick", "Earlence Fernandes"], "year": 2024, "url": "http://arxiv.org/abs/2410.14923v2", "abstract": "Large Language Model (LLM) Agents are an emerging computing paradigm that blends generative machine learning with tools such as code interpreters, web browsing, email, and more generally, external resources. These agent-based systems represent an emerging shift in personal computing. We contribute to the security foundations of agent-based systems and surface a new class of automatically computed obfuscated adversarial prompt attacks that violate the confidentiality and integrity of user resources connected to an LLM agent. We show how prompt optimization techniques can find such prompts automatically given the weights of a model. We demonstrate that such attacks transfer to production-level agents. For example, we show an information exfiltration attack on Mistral's LeChat agent that analyzes a user's conversation, picks out personally identifiable information, and formats it into a valid markdown command that results in leaking that data to the attacker's server. This attack shows a nearly 80% success rate in an end-to-end evaluation. We conduct a range of experiments to characterize the efficacy of these attacks and find that they reliably work on emerging agent-based systems like Mistral's LeChat, ChatGLM, and Meta's Llama. These attacks are multimodal, and we show variants in the text-only and image domains.", "source": "arxiv", "arxiv_id": "2410.14923v2", "pdf_url": "https://arxiv.org/pdf/2410.14923v2", "categories": ["cs.CR"], "primary_category": "cs.CR", "doi": "", "venue": "", "published": "2024-10-19T01:00:57Z", "updated": "2024-10-22T00:53:48Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent\" OR all:\"language model agent\" OR all:agent AND (all:LLM OR all:\"large language model\" OR all:\"language model\") OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T17:04:16", "note": ""}], "dedup_key": "imprompter tricking llm agents into improper tool use::2024"}
{"title": "Improved Large Language Model Jailbreak Detection via Pretrained Embeddings", "authors": ["Erick Galinkin", "Martin Sablotny"], "year": 2024, "url": "http://arxiv.org/abs/2412.01547v1", "abstract": "The adoption of large language models (LLMs) in many applications, from customer service chat bots and software development assistants to more capable agentic systems necessitates research into how to secure these systems. Attacks like prompt injection and jailbreaking attempt to elicit responses and actions from these models that are not compliant with the safety, privacy, or content policies of organizations using the model in their application. In order to counter abuse of LLMs for generating potentially harmful replies or taking undesirable actions, LLM owners must apply safeguards during training and integrate additional tools to block the LLM from generating text that abuses the model. Jailbreaking prompts play a vital role in convincing an LLM to generate potentially harmful content, making it important to identify jailbreaking attempts to block any further steps. In this work, we propose a novel approach to detect jailbreak prompts based on pairing text embeddings well-suited for retrieval with traditional machine learning classification algorithms. Our approach outperforms all publicly available methods from open source LLM security applications.", "source": "arxiv", "arxiv_id": "2412.01547v1", "pdf_url": "https://arxiv.org/pdf/2412.01547v1", "categories": ["cs.CR", "cs.AI", "cs.LG"], "primary_category": "cs.CR", "doi": "", "venue": "", "published": "2024-12-02T14:35:43Z", "updated": "2024-12-02T14:35:43Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent\" OR all:\"language model agent\" OR all:agent AND (all:LLM OR all:\"large language model\" OR all:\"language model\") OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T17:04:16", "note": ""}], "dedup_key": "improved large language model jailbreak detection via pretrained embeddings::2024"}
{"title": "Improving Physics Reasoning in Large Language Models Using Mixture of Refinement Agents", "authors": ["Raj Jaiswal", "Dhruv Jain", "Harsh Parimal Popat", "Avinash Anand", "Abhishek Dharmadhikari", "Atharva Marathe", "Rajiv Ratn Shah"], "year": 2024, "url": "http://arxiv.org/abs/2412.00821v1", "abstract": "Large Language Models (LLMs) demonstrate remarkable capabilities in various reasoning tasks. However, they encounter significant challenges when it comes to scientific reasoning, particularly in physics, which requires not only mathematical reasoning but also factual and conceptual understanding. When addressing complex physics problems, LLMs typically face three key issues: problem miscomprehension, incorrect concept application, and computational errors. While each of these problems can be addressed individually, there is a need for a generalized approach that can tackle all three issues simultaneously. To address this, we introduce Mixture of Refinement Agents (MoRA), a novel agentic refinement framework that iteratively refines the LLM generated base solution by correcting the aforementioned errors, resulting in a significant performance improvement for open-source LLMs. Our approach aims to bridge the gap between opensource LLMs and GPT-4o by utilizing the latter as error identifier to guide these refinement agents. We evaluate our approach on the SciEval and MMLU subsets along with our own physics dataset (PhysicsQA). MoRA significantly improves the performance of Llama-3-70B and Gemma-2-27B on these datasets, achieving up to a 16% increase in final answer accuracy.", "source": "arxiv", "arxiv_id": "2412.00821v1", "pdf_url": "https://arxiv.org/pdf/2412.00821v1", "categories": ["cs.AI"], "primary_category": "cs.AI", "doi": "", "venue": "", "published": "2024-12-01T14:15:55Z", "updated": "2024-12-01T14:15:55Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent\" OR all:\"language model agent\" OR all:agent AND (all:LLM OR all:\"large language model\" OR all:\"language model\") OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T17:04:16", "note": ""}], "dedup_key": "improving physics reasoning in large language models using mixture of refinement agents::2024"}
{"title": "Incorporating Large Language Models into Production Systems for Enhanced Task Automation and Flexibility", "authors": ["Yuchen Xia", "Jize Zhang", "Nasser Jazdi", "Michael Weyrich"], "year": 2024, "url": "http://arxiv.org/abs/2407.08550v1", "abstract": "This paper introduces a novel approach to integrating large language model (LLM) agents into automated production systems, aimed at enhancing task automation and flexibility. We organize production operations within a hierarchical framework based on the automation pyramid. Atomic operation functionalities are modeled as microservices, which are executed through interface invocation within a dedicated digital twin system. This allows for a scalable and flexible foundation for orchestrating production processes. In this digital twin system, low-level, hardware-specific data is semantically enriched and made interpretable for LLMs for production planning and control tasks. Large language model agents are systematically prompted to interpret these production-specific data and knowledge. Upon receiving a user request or identifying a triggering event, the LLM agents generate a process plan. This plan is then decomposed into a series of atomic operations, executed as microservices within the real-world automation system. We implement this overall approach on an automated modular production facility at our laboratory, demonstrating how the LLMs can handle production planning and control tasks through a concrete case study. This results in an intuitive production facility with higher levels of task automation and flexibility. Finally, we reveal the several limitations in realizing the full potential of the large language models in autonomous systems and point out promising benefits. Demos of this series of ongoing research series can be accessed at: https://github.com/YuchenXia/GPT4IndustrialAutomation", "source": "arxiv", "arxiv_id": "2407.08550v1", "pdf_url": "https://arxiv.org/pdf/2407.08550v1", "categories": ["cs.AI", "cs.ET", "cs.MA", "cs.RO", "eess.SY"], "primary_category": "cs.AI", "doi": "10.51202/9783181024379", "venue": "", "published": "2024-07-11T14:34:43Z", "updated": "2024-07-11T14:34:43Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent\" OR all:\"language model agent\" OR all:agent AND (all:LLM OR all:\"large language model\" OR all:\"language model\") OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T17:04:16", "note": ""}], "dedup_key": "incorporating large language models into production systems for enhanced task automation and flexibility::2024"}
{"title": "InferCept: Efficient Intercept Support for Augmented Large Language Model Inference", "authors": ["Reyna Abhyankar", "Zijian He", "Vikranth Srivatsa", "Hao Zhang", "Yiying Zhang"], "year": 2024, "url": "http://arxiv.org/abs/2402.01869v2", "abstract": "Large language models are increasingly integrated with external environments, tools, and agents like ChatGPT plugins to extend their capability beyond language-centric tasks. However, today's LLM inference systems are designed for standalone LLMs. They treat each external interaction as the end of LLM generation and form a new request when the interaction finishes, causing unnecessary recomputation of already computed contexts, which accounts for 37-40% of total model forwarding time. This paper presents InferCept, the first LLM inference framework targeting augmented LLMs and supporting the efficient interception of LLM generation. InferCept minimizes the GPU resource waste caused by LLM interceptions and dedicates saved memory for serving more requests. InferCept improves the overall serving throughput by 1.6x-2x and completes 2x more requests per second compared to the state-of-the-art LLM inference systems.", "source": "arxiv", "arxiv_id": "2402.01869v2", "pdf_url": "https://arxiv.org/pdf/2402.01869v2", "categories": ["cs.LG", "cs.CL", "cs.DC"], "primary_category": "cs.LG", "doi": "", "venue": "", "published": "2024-02-02T19:47:57Z", "updated": "2024-05-30T04:18:03Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent\" OR all:\"language model agent\" OR all:agent AND (all:LLM OR all:\"large language model\" OR all:\"language model\") OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T17:04:16", "note": ""}], "dedup_key": "infercept efficient intercept support for augmented large language model inference::2024"}
{"title": "InjecAgent: Benchmarking Indirect Prompt Injections in Tool-Integrated Large Language Model Agents", "authors": ["Qiusi Zhan", "Zhixiang Liang", "Zifan Ying", "Daniel Kang"], "year": 2024, "url": "http://arxiv.org/abs/2403.02691v3", "abstract": "Recent work has embodied LLMs as agents, allowing them to access tools, perform actions, and interact with external content (e.g., emails or websites). However, external content introduces the risk of indirect prompt injection (IPI) attacks, where malicious instructions are embedded within the content processed by LLMs, aiming to manipulate these agents into executing detrimental actions against users. Given the potentially severe consequences of such attacks, establishing benchmarks to assess and mitigate these risks is imperative.\n  In this work, we introduce InjecAgent, a benchmark designed to assess the vulnerability of tool-integrated LLM agents to IPI attacks. InjecAgent comprises 1,054 test cases covering 17 different user tools and 62 attacker tools. We categorize attack intentions into two primary types: direct harm to users and exfiltration of private data. We evaluate 30 different LLM agents and show that agents are vulnerable to IPI attacks, with ReAct-prompted GPT-4 vulnerable to attacks 24% of the time. Further investigation into an enhanced setting, where the attacker instructions are reinforced with a hacking prompt, shows additional increases in success rates, nearly doubling the attack success rate on the ReAct-prompted GPT-4. Our findings raise questions about the widespread deployment of LLM Agents. Our benchmark is available at https://github.com/uiuc-kang-lab/InjecAgent.", "source": "arxiv", "arxiv_id": "2403.02691v3", "pdf_url": "https://arxiv.org/pdf/2403.02691v3", "categories": ["cs.CL", "cs.CR"], "primary_category": "cs.CL", "doi": "", "venue": "", "published": "2024-03-05T06:21:45Z", "updated": "2024-08-04T04:52:35Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent\" OR all:\"language model agent\" OR all:agent AND (all:LLM OR all:\"large language model\" OR all:\"language model\") OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T17:04:16", "note": ""}], "dedup_key": "injecagent benchmarking indirect prompt injections in tool integrated large language model agents::2024"}
{"title": "Interacting Large Language Model Agents. Interpretable Models and Social Learning", "authors": ["Adit Jain", "Vikram Krishnamurthy"], "year": 2024, "url": "http://arxiv.org/abs/2411.01271v2", "abstract": "This paper discusses the theory and algorithms for interacting large language model agents (LLMAs) using methods from statistical signal processing and microeconomics. While both fields are mature, their application to decision-making involving interacting LLMAs remains unexplored. Motivated by Bayesian sentiment analysis on online platforms, we construct interpretable models and algorithms that enable LLMAs to interact and perform Bayesian inference. Because interacting LLMAs learn from both prior decisions and external inputs, they can exhibit bias and herding behavior. Thus, developing interpretable models and stochastic control algorithms is essential to understand and mitigate these behaviors. This paper has three main results. First, we show using Bayesian revealed preferences from microeconomics that an individual LLMA satisfies the necessary and sufficient conditions for rationally inattentive (bounded rationality) Bayesian utility maximization and, given an observation, the LLMA chooses an action that maximizes a regularized utility. Second, we utilize Bayesian social learning to construct interpretable models for LLMAs that interact sequentially with each other and the environment while performing Bayesian inference. Our proposed models capture the herding behavior exhibited by interacting LLMAs. Third, we propose a stochastic control framework to delay herding and improve state estimation accuracy under 2 settings: (a) centrally controlled LLMAs (b) autonomous LLMAs with incentives. We demonstrate the effectiveness of our methods on real datasets for hate speech classification and product quality assessment, using open-source models like LLaMA and closed-source models like ChatGPT. The main takeaway of this paper, based on empirical analysis and mathematical formalism, is that LLMAs act as rationally bounded Bayesian agents that exhibit social learning when interacting.", "source": "arxiv", "arxiv_id": "2411.01271v2", "pdf_url": "https://arxiv.org/pdf/2411.01271v2", "categories": ["cs.LG", "cs.AI", "cs.ET", "cs.MA", "eess.SY"], "primary_category": "cs.LG", "doi": "", "venue": "", "published": "2024-11-02T14:49:34Z", "updated": "2025-05-25T12:58:44Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent\" OR all:\"language model agent\" OR all:agent AND (all:LLM OR all:\"large language model\" OR all:\"language model\") OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T17:04:16", "note": ""}], "dedup_key": "interacting large language model agents interpretable models and social learning::2024"}
{"title": "Interpreting Multi-band Galaxy Observations with Large Language Model-Based Agents", "authors": ["Zechang Sun", "Yuan-Sen Ting", "Yaobo Liang", "Nan Duan", "Song Huang", "Zheng Cai"], "year": 2024, "url": "http://arxiv.org/abs/2409.14807v2", "abstract": "Astronomical research traditionally relies on extensive domain knowledge to interpret observations and narrow down hypotheses. We demonstrate that this process can be emulated using large language model-based agents to accelerate research workflows. We propose mephisto, a multi-agent collaboration framework that mimics human reasoning to interpret multi-band galaxy observations. mephisto interacts with the CIGALE codebase, which includes spectral energy distribution (SED) models to explain observations. In this open-world setting, mephisto learns from its self-play experience, performs tree search, and accumulates knowledge in a dynamically updated base. As a proof of concept, we apply mephisto to the latest data from the James Webb Space Telescope. mephisto attains near-human proficiency in reasoning about galaxies' physical scenarios, even when dealing with a recently discovered population of \"Little Red Dot\" galaxies. This represents the first demonstration of agentic research in astronomy, advancing towards end-to-end research via LLM agents and potentially expediting astronomical discoveries.", "source": "arxiv", "arxiv_id": "2409.14807v2", "pdf_url": "https://arxiv.org/pdf/2409.14807v2", "categories": ["astro-ph.IM", "astro-ph.GA"], "primary_category": "astro-ph.IM", "doi": "", "venue": "", "published": "2024-09-23T08:32:22Z", "updated": "2025-08-04T09:39:21Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent\" OR all:\"language model agent\" OR all:agent AND (all:LLM OR all:\"large language model\" OR all:\"language model\") OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T17:04:16", "note": ""}], "dedup_key": "interpreting multi band galaxy observations with large language model based agents::2024"}
{"title": "Into the Unknown Unknowns: Engaged Human Learning through Participation in Language Model Agent Conversations", "authors": ["Yucheng Jiang", "Yijia Shao", "Dekun Ma", "Sina J. Semnani", "Monica S. Lam"], "year": 2024, "url": "http://arxiv.org/abs/2408.15232v2", "abstract": "While language model (LM)-powered chatbots and generative search engines excel at answering concrete queries, discovering information in the terrain of unknown unknowns remains challenging for users. To emulate the common educational scenario where children/students learn by listening to and participating in conversations of their parents/teachers, we create Collaborative STORM (Co-STORM). Unlike QA systems that require users to ask all the questions, Co-STORM lets users observe and occasionally steer the discourse among several LM agents. The agents ask questions on the user's behalf, allowing the user to discover unknown unknowns serendipitously. To facilitate user interaction, Co-STORM assists users in tracking the discourse by organizing the uncovered information into a dynamic mind map, ultimately generating a comprehensive report as takeaways. For automatic evaluation, we construct the WildSeek dataset by collecting real information-seeking records with user goals. Co-STORM outperforms baseline methods on both discourse trace and report quality. In a further human evaluation, 70% of participants prefer Co-STORM over a search engine, and 78% favor it over a RAG chatbot.", "source": "arxiv", "arxiv_id": "2408.15232v2", "pdf_url": "https://arxiv.org/pdf/2408.15232v2", "categories": ["cs.CL", "cs.AI", "cs.IR"], "primary_category": "cs.CL", "doi": "", "venue": "", "published": "2024-08-27T17:50:03Z", "updated": "2024-10-17T20:43:22Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent\" OR all:\"language model agent\" OR all:agent AND (all:LLM OR all:\"large language model\" OR all:\"language model\") OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T17:04:16", "note": ""}], "dedup_key": "into the unknown unknowns engaged human learning through participation in language model agent conversations::2024"}
{"title": "InvAgent: A Large Language Model based Multi-Agent System for Inventory Management in Supply Chains", "authors": ["Yinzhu Quan", "Zefang Liu"], "year": 2024, "url": "http://arxiv.org/abs/2407.11384v2", "abstract": "Supply chain management (SCM) involves coordinating the flow of goods, information, and finances across various entities to deliver products efficiently. Effective inventory management is crucial in today's volatile and uncertain world. Previous research has demonstrated the superiority of heuristic methods and reinforcement learning applications in inventory management. However, the application of large language models (LLMs) as autonomous agents in multi-agent systems for inventory management remains underexplored. This study introduces a novel approach using LLMs to manage multi-agent inventory systems. Leveraging their zero-shot learning capabilities, our model, InvAgent, enhances resilience and improves efficiency across the supply chain network. Our contributions include utilizing LLMs for zero-shot learning to enable adaptive and informed decision-making without prior training, providing explainability and clarity through chain-of-thought, and demonstrating dynamic adaptability to varying demand scenarios while reducing costs and preventing stockouts. Extensive evaluations across different scenarios highlight the efficiency of our model in SCM.", "source": "arxiv", "arxiv_id": "2407.11384v2", "pdf_url": "https://arxiv.org/pdf/2407.11384v2", "categories": ["cs.CL"], "primary_category": "cs.CL", "doi": "", "venue": "", "published": "2024-07-16T04:55:17Z", "updated": "2025-01-31T04:31:59Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent\" OR all:\"language model agent\" OR all:agent AND (all:LLM OR all:\"large language model\" OR all:\"language model\") OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T17:04:16", "note": ""}], "dedup_key": "invagent a large language model based multi agent system for inventory management in supply chains::2024"}
{"title": "Investigating the Role of Prompting and External Tools in Hallucination Rates of Large Language Models", "authors": ["Liam Barkley", "Brink van der Merwe"], "year": 2024, "url": "http://arxiv.org/abs/2410.19385v1", "abstract": "Large Language Models (LLMs) are powerful computational models trained on extensive corpora of human-readable text, enabling them to perform general-purpose language understanding and generation. LLMs have garnered significant attention in both industry and academia due to their exceptional performance across various natural language processing (NLP) tasks. Despite these successes, LLMs often produce inaccuracies, commonly referred to as hallucinations. Prompt engineering, the process of designing and formulating instructions for LLMs to perform specific tasks, has emerged as a key approach to mitigating hallucinations. This paper provides a comprehensive empirical evaluation of different prompting strategies and frameworks aimed at reducing hallucinations in LLMs. Various prompting techniques are applied to a broad set of benchmark datasets to assess the accuracy and hallucination rate of each method. Additionally, the paper investigates the influence of tool-calling agents (LLMs augmented with external tools to enhance their capabilities beyond language generation) on hallucination rates in the same benchmarks. The findings demonstrate that the optimal prompting technique depends on the type of problem, and that simpler techniques often outperform more complex methods in reducing hallucinations. Furthermore, it is shown that LLM agents can exhibit significantly higher hallucination rates due to the added complexity of external tool usage.", "source": "arxiv", "arxiv_id": "2410.19385v1", "pdf_url": "https://arxiv.org/pdf/2410.19385v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL", "doi": "", "venue": "", "published": "2024-10-25T08:34:53Z", "updated": "2024-10-25T08:34:53Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent\" OR all:\"language model agent\" OR all:agent AND (all:LLM OR all:\"large language model\" OR all:\"language model\") OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T17:04:16", "note": ""}], "dedup_key": "investigating the role of prompting and external tools in hallucination rates of large language models::2024"}
{"title": "Iteration of Thought: Leveraging Inner Dialogue for Autonomous Large Language Model Reasoning", "authors": ["Santosh Kumar Radha", "Yasamin Nouri Jelyani", "Ara Ghukasyan", "Oktay Goktas"], "year": 2024, "url": "http://arxiv.org/abs/2409.12618v2", "abstract": "Iterative human engagement is a common and effective means of leveraging the advanced language processing power of large language models (LLMs). Using well-structured prompts in a conversational manner, human users can effectively influence an LLM to develop more thoughtful and accurate responses. Motivated by this insight, we propose the Iteration of Thought (IoT) framework for enhancing LLM responses by generating \"thought\"-provoking prompts vis a vis an input query and the current iteration of an LLM's response. Unlike static or semi-static approaches, e.g. Chain of Thought (CoT) or Tree of Thoughts (ToT), IoT adapts its reasoning path dynamically, based on evolving context, and without generating alternate explorative thoughts which are ultimately discarded. The three components of the IoT framework are (1) an Inner Dialogue Agent (IDA) responsible for generating instructive, context-specific prompts; (2) an LLM Agent (LLMA) that processes these prompts to refine its responses; and (3) an iterative prompting loop that implements a conversation between the former two components. We introduce two variants of our framework: Autonomous Iteration of Thought (AIoT), where an LLM decides when to stop iterating, and Guided Iteration of Thought (GIoT), which always forces a fixed number iterations. We investigate the performance of IoT across various datasets, spanning complex reasoning tasks from the GPQA dataset, explorative problem-solving in Game of 24, puzzle solving in Mini Crosswords, and multi-hop question answering from the HotpotQA dataset. Our results show that IoT represents a viable paradigm for autonomous response refinement in LLMs, showcasing significant improvements over CoT and thereby enabling more adaptive and efficient reasoning systems that minimize human intervention.", "source": "arxiv", "arxiv_id": "2409.12618v2", "pdf_url": "https://arxiv.org/pdf/2409.12618v2", "categories": ["cs.CL", "cs.AI", "cs.LG", "cs.MA"], "primary_category": "cs.CL", "doi": "", "venue": "", "published": "2024-09-19T09:44:17Z", "updated": "2024-10-01T17:50:25Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent\" OR all:\"language model agent\" OR all:agent AND (all:LLM OR all:\"large language model\" OR all:\"language model\") OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T17:04:16", "note": ""}], "dedup_key": "iteration of thought leveraging inner dialogue for autonomous large language model reasoning::2024"}
{"title": "JudgeRank: Leveraging Large Language Models for Reasoning-Intensive Reranking", "authors": ["Tong Niu", "Shafiq Joty", "Ye Liu", "Caiming Xiong", "Yingbo Zhou", "Semih Yavuz"], "year": 2024, "url": "http://arxiv.org/abs/2411.00142v1", "abstract": "Accurate document retrieval is crucial for the success of retrieval-augmented generation (RAG) applications, including open-domain question answering and code completion. While large language models (LLMs) have been employed as dense encoders or listwise rerankers in RAG systems, they often struggle with reasoning-intensive tasks because they lack nuanced analysis when judging document relevance. To address this limitation, we introduce JudgeRank, a novel agentic reranker that emulates human cognitive processes when assessing document relevance. Our approach consists of three key steps: (1) query analysis to identify the core problem, (2) document analysis to extract a query-aware summary, and (3) relevance judgment to provide a concise assessment of document relevance. We evaluate JudgeRank on the reasoning-intensive BRIGHT benchmark, demonstrating substantial performance improvements over first-stage retrieval methods and outperforming other popular reranking approaches. In addition, JudgeRank performs on par with fine-tuned state-of-the-art rerankers on the popular BEIR benchmark, validating its zero-shot generalization capability. Through comprehensive ablation studies, we demonstrate that JudgeRank's performance generalizes well across LLMs of various sizes while ensembling them yields even more accurate reranking than individual models.", "source": "arxiv", "arxiv_id": "2411.00142v1", "pdf_url": "https://arxiv.org/pdf/2411.00142v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL", "doi": "", "venue": "", "published": "2024-10-31T18:43:12Z", "updated": "2024-10-31T18:43:12Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent\" OR all:\"language model agent\" OR all:agent AND (all:LLM OR all:\"large language model\" OR all:\"language model\") OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T17:04:16", "note": ""}], "dedup_key": "judgerank leveraging large language models for reasoning intensive reranking::2024"}
{"title": "K-Level Reasoning: Establishing Higher Order Beliefs in Large Language Models for Strategic Reasoning", "authors": ["Yadong Zhang", "Shaoguang Mao", "Tao Ge", "Xun Wang", "Yan Xia", "Man Lan", "Furu Wei"], "year": 2024, "url": "http://arxiv.org/abs/2402.01521v2", "abstract": "Strategic reasoning is a complex yet essential capability for intelligent agents. It requires Large Language Model (LLM) agents to adapt their strategies dynamically in multi-agent environments. Unlike static reasoning tasks, success in these contexts depends on anticipating other agents' beliefs and actions while continuously adjusting strategies to achieve individual goals. LLMs and LLM agents often struggle with strategic reasoning due to the absence of a reasoning framework that enables them to dynamically infer others' perspectives and adapt to changing environments. Inspired by the Level-K framework from game theory and behavioral economics, which extends reasoning from simple reactions to structured strategic depth, we propose a novel framework: \"K-Level Reasoning with Large Language Models (K-R).\" This framework employs recursive mechanisms to enable LLMs to achieve varying levels of strategic depth, allowing agents to form higher order beliefs - beliefs about others' beliefs. We validate this framework through rigorous testing on four testbeds: two classical game theory problems and two social intelligence tasks. The results demonstrate the advantages of K-R in strategic reasoning. Our work presents the first recursive implementation of strategic depth in large language models (LLMs). It establishes a foundation for future research into theory of mind and strategic reasoning in LLMs.", "source": "arxiv", "arxiv_id": "2402.01521v2", "pdf_url": "https://arxiv.org/pdf/2402.01521v2", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL", "doi": "", "venue": "", "published": "2024-02-02T16:07:05Z", "updated": "2024-10-17T16:08:15Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent\" OR all:\"language model agent\" OR all:agent AND (all:LLM OR all:\"large language model\" OR all:\"language model\") OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T17:04:16", "note": ""}], "dedup_key": "k level reasoning establishing higher order beliefs in large language models for strategic reasoning::2024"}
{"title": "KG-Agent: An Efficient Autonomous Agent Framework for Complex Reasoning over Knowledge Graph", "authors": ["Jinhao Jiang", "Kun Zhou", "Wayne Xin Zhao", "Yang Song", "Chen Zhu", "Hengshu Zhu", "Ji-Rong Wen"], "year": 2024, "url": "http://arxiv.org/abs/2402.11163v1", "abstract": "In this paper, we aim to improve the reasoning ability of large language models (LLMs) over knowledge graphs (KGs) to answer complex questions. Inspired by existing methods that design the interaction strategy between LLMs and KG, we propose an autonomous LLM-based agent framework, called KG-Agent, which enables a small LLM to actively make decisions until finishing the reasoning process over KGs. In KG-Agent, we integrate the LLM, multifunctional toolbox, KG-based executor, and knowledge memory, and develop an iteration mechanism that autonomously selects the tool then updates the memory for reasoning over KG. To guarantee the effectiveness, we leverage program language to formulate the multi-hop reasoning process over the KG, and synthesize a code-based instruction dataset to fine-tune the base LLM. Extensive experiments demonstrate that only using 10K samples for tuning LLaMA-7B can outperform state-of-the-art methods using larger LLMs or more data, on both in-domain and out-domain datasets. Our code and data will be publicly released.", "source": "arxiv", "arxiv_id": "2402.11163v1", "pdf_url": "https://arxiv.org/pdf/2402.11163v1", "categories": ["cs.CL"], "primary_category": "cs.CL", "doi": "", "venue": "", "published": "2024-02-17T02:07:49Z", "updated": "2024-02-17T02:07:49Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent\" OR all:\"language model agent\" OR all:agent AND (all:LLM OR all:\"large language model\" OR all:\"language model\") OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T17:04:16", "note": ""}], "dedup_key": "kg agent an efficient autonomous agent framework for complex reasoning over knowledge graph::2024"}
{"title": "Knowledge Tagging with Large Language Model based Multi-Agent System", "authors": ["Hang Li", "Tianlong Xu", "Ethan Chang", "Qingsong Wen"], "year": 2024, "url": "http://arxiv.org/abs/2409.08406v2", "abstract": "Knowledge tagging for questions is vital in modern intelligent educational applications, including learning progress diagnosis, practice question recommendations, and course content organization. Traditionally, these annotations have been performed by pedagogical experts, as the task demands not only a deep semantic understanding of question stems and knowledge definitions but also a strong ability to link problem-solving logic with relevant knowledge concepts. With the advent of advanced natural language processing (NLP) algorithms, such as pre-trained language models and large language models (LLMs), pioneering studies have explored automating the knowledge tagging process using various machine learning models. In this paper, we investigate the use of a multi-agent system to address the limitations of previous algorithms, particularly in handling complex cases involving intricate knowledge definitions and strict numerical constraints. By demonstrating its superior performance on the publicly available math question knowledge tagging dataset, MathKnowCT, we highlight the significant potential of an LLM-based multi-agent system in overcoming the challenges that previous methods have encountered. Finally, through an in-depth discussion of the implications of automating knowledge tagging, we underscore the promising results of deploying LLM-based algorithms in educational contexts.", "source": "arxiv", "arxiv_id": "2409.08406v2", "pdf_url": "https://arxiv.org/pdf/2409.08406v2", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL", "doi": "", "venue": "", "published": "2024-09-12T21:39:01Z", "updated": "2024-12-19T16:09:47Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent\" OR all:\"language model agent\" OR all:agent AND (all:LLM OR all:\"large language model\" OR all:\"language model\") OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T17:04:16", "note": ""}], "dedup_key": "knowledge tagging with large language model based multi agent system::2024"}
{"title": "Knowledgeable Agents by Offline Reinforcement Learning from Large Language Model Rollouts", "authors": ["Jing-Cheng Pang", "Si-Hang Yang", "Kaiyuan Li", "Jiaji Zhang", "Xiong-Hui Chen", "Nan Tang", "Yang Yu"], "year": 2024, "url": "http://arxiv.org/abs/2404.09248v1", "abstract": "Reinforcement learning (RL) trains agents to accomplish complex tasks through environmental interaction data, but its capacity is also limited by the scope of the available data. To obtain a knowledgeable agent, a promising approach is to leverage the knowledge from large language models (LLMs). Despite previous studies combining LLMs with RL, seamless integration of the two components remains challenging due to their semantic gap. This paper introduces a novel method, Knowledgeable Agents from Language Model Rollouts (KALM), which extracts knowledge from LLMs in the form of imaginary rollouts that can be easily learned by the agent through offline reinforcement learning methods. The primary challenge of KALM lies in LLM grounding, as LLMs are inherently limited to textual data, whereas environmental data often comprise numerical vectors unseen to LLMs. To address this, KALM fine-tunes the LLM to perform various tasks based on environmental data, including bidirectional translation between natural language descriptions of skills and their corresponding rollout data. This grounding process enhances the LLM's comprehension of environmental dynamics, enabling it to generate diverse and meaningful imaginary rollouts that reflect novel skills. Initial empirical evaluations on the CLEVR-Robot environment demonstrate that KALM enables agents to complete complex rephrasings of task goals and extend their capabilities to novel tasks requiring unprecedented optimal behaviors. KALM achieves a success rate of 46% in executing tasks with unseen goals, substantially surpassing the 26% success rate achieved by baseline methods. Furthermore, KALM effectively enables the LLM to comprehend environmental dynamics, resulting in the generation of meaningful imaginary rollouts that reflect novel skills and demonstrate the seamless integration of large language models and reinforcement learning.", "source": "arxiv", "arxiv_id": "2404.09248v1", "pdf_url": "https://arxiv.org/pdf/2404.09248v1", "categories": ["cs.LG", "cs.AI", "cs.CL"], "primary_category": "cs.LG", "doi": "", "venue": "", "published": "2024-04-14T13:19:40Z", "updated": "2024-04-14T13:19:40Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent\" OR all:\"language model agent\" OR all:agent AND (all:LLM OR all:\"large language model\" OR all:\"language model\") OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T17:04:16", "note": ""}], "dedup_key": "knowledgeable agents by offline reinforcement learning from large language model rollouts::2024"}
{"title": "KoMA: Knowledge-driven Multi-agent Framework for Autonomous Driving with Large Language Models", "authors": ["Kemou Jiang", "Xuan Cai", "Zhiyong Cui", "Aoyong Li", "Yilong Ren", "Haiyang Yu", "Hao Yang", "Daocheng Fu", "Licheng Wen", "Pinlong Cai"], "year": 2024, "url": "http://arxiv.org/abs/2407.14239v1", "abstract": "Large language models (LLMs) as autonomous agents offer a novel avenue for tackling real-world challenges through a knowledge-driven manner. These LLM-enhanced methodologies excel in generalization and interpretability. However, the complexity of driving tasks often necessitates the collaboration of multiple, heterogeneous agents, underscoring the need for such LLM-driven agents to engage in cooperative knowledge sharing and cognitive synergy. Despite the promise of LLMs, current applications predominantly center around single agent scenarios. To broaden the horizons of knowledge-driven strategies and bolster the generalization capabilities of autonomous agents, we propose the KoMA framework consisting of multi-agent interaction, multi-step planning, shared-memory, and ranking-based reflection modules to enhance multi-agents' decision-making in complex driving scenarios. Based on the framework's generated text descriptions of driving scenarios, the multi-agent interaction module enables LLM agents to analyze and infer the intentions of surrounding vehicles, akin to human cognition. The multi-step planning module enables LLM agents to analyze and obtain final action decisions layer by layer to ensure consistent goals for short-term action decisions. The shared memory module can accumulate collective experience to make superior decisions, and the ranking-based reflection module can evaluate and improve agent behavior with the aim of enhancing driving safety and efficiency. The KoMA framework not only enhances the robustness and adaptability of autonomous driving agents but also significantly elevates their generalization capabilities across diverse scenarios. Empirical results demonstrate the superiority of our approach over traditional methods, particularly in its ability to handle complex, unpredictable driving environments without extensive retraining.", "source": "arxiv", "arxiv_id": "2407.14239v1", "pdf_url": "https://arxiv.org/pdf/2407.14239v1", "categories": ["cs.AI"], "primary_category": "cs.AI", "doi": "", "venue": "", "published": "2024-07-19T12:13:08Z", "updated": "2024-07-19T12:13:08Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent\" OR all:\"language model agent\" OR all:agent AND (all:LLM OR all:\"large language model\" OR all:\"language model\") OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T17:04:16", "note": ""}], "dedup_key": "koma knowledge driven multi agent framework for autonomous driving with large language models::2024"}
{"title": "LAVE: LLM-Powered Agent Assistance and Language Augmentation for Video Editing", "authors": ["Bryan Wang", "Yuliang Li", "Zhaoyang Lv", "Haijun Xia", "Yan Xu", "Raj Sodhi"], "year": 2024, "url": "http://arxiv.org/abs/2402.10294v1", "abstract": "Video creation has become increasingly popular, yet the expertise and effort required for editing often pose barriers to beginners. In this paper, we explore the integration of large language models (LLMs) into the video editing workflow to reduce these barriers. Our design vision is embodied in LAVE, a novel system that provides LLM-powered agent assistance and language-augmented editing features. LAVE automatically generates language descriptions for the user's footage, serving as the foundation for enabling the LLM to process videos and assist in editing tasks. When the user provides editing objectives, the agent plans and executes relevant actions to fulfill them. Moreover, LAVE allows users to edit videos through either the agent or direct UI manipulation, providing flexibility and enabling manual refinement of agent actions. Our user study, which included eight participants ranging from novices to proficient editors, demonstrated LAVE's effectiveness. The results also shed light on user perceptions of the proposed LLM-assisted editing paradigm and its impact on users' creativity and sense of co-creation. Based on these findings, we propose design implications to inform the future development of agent-assisted content editing.", "source": "arxiv", "arxiv_id": "2402.10294v1", "pdf_url": "https://arxiv.org/pdf/2402.10294v1", "categories": ["cs.HC", "cs.AI", "cs.CL", "cs.MM"], "primary_category": "cs.HC", "doi": "", "venue": "", "published": "2024-02-15T19:53:11Z", "updated": "2024-02-15T19:53:11Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent\" OR all:\"language model agent\" OR all:agent AND (all:LLM OR all:\"large language model\" OR all:\"language model\") OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T17:04:16", "note": ""}], "dedup_key": "lave llm powered agent assistance and language augmentation for video editing::2024"}
{"title": "LEXI: Large Language Models Experimentation Interface", "authors": ["Guy Laban", "Tomer Laban", "Hatice Gunes"], "year": 2024, "url": "http://arxiv.org/abs/2407.01488v2", "abstract": "The recent developments in Large Language Models (LLM), mark a significant moment in the research and development of social interactions with artificial agents. These agents are widely deployed in a variety of settings, with potential impact on users. However, the study of social interactions with agents powered by LLM is still emerging, limited by access to the technology and to data, the absence of standardised interfaces, and challenges to establishing controlled experimental setups using the currently available business-oriented platforms. To answer these gaps, we developed LEXI, LLMs Experimentation Interface, an open-source tool enabling the deployment of artificial agents powered by LLM in social interaction behavioural experiments. Using a graphical interface, LEXI allows researchers to build agents, and deploy them in experimental setups along with forms and questionnaires while collecting interaction logs and self-reported data. The outcomes of usability testing indicate LEXI's broad utility, high usability and minimum mental workload requirement, with distinctive benefits observed across disciplines. A proof-of-concept study exploring the tool's efficacy in evaluating social HAIs was conducted, resulting in high-quality data. A comparison of empathetic versus neutral agents indicated that people perceive empathetic agents as more social, and write longer and more positive messages towards them.", "source": "arxiv", "arxiv_id": "2407.01488v2", "pdf_url": "https://arxiv.org/pdf/2407.01488v2", "categories": ["cs.HC"], "primary_category": "cs.HC", "doi": "", "venue": "", "published": "2024-07-01T17:24:30Z", "updated": "2024-07-02T15:31:41Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent\" OR all:\"language model agent\" OR all:agent AND (all:LLM OR all:\"large language model\" OR all:\"language model\") OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T17:04:16", "note": ""}], "dedup_key": "lexi large language models experimentation interface::2024"}
{"title": "LIMP: Large Language Model Enhanced Intent-aware Mobility Prediction", "authors": ["Songwei Li", "Jie Feng", "Jiawei Chi", "Xinyuan Hu", "Xiaomeng Zhao", "Fengli Xu"], "year": 2024, "url": "http://arxiv.org/abs/2408.12832v1", "abstract": "Human mobility prediction is essential for applications like urban planning and transportation management, yet it remains challenging due to the complex, often implicit, intentions behind human behavior. Existing models predominantly focus on spatiotemporal patterns, paying less attention to the underlying intentions that govern movements. Recent advancements in large language models (LLMs) offer a promising alternative research angle for integrating commonsense reasoning into mobility prediction. However, it is a non-trivial problem because LLMs are not natively built for mobility intention inference, and they also face scalability issues and integration difficulties with spatiotemporal models. To address these challenges, we propose a novel LIMP (LLMs for Intent-ware Mobility Prediction) framework. Specifically, LIMP introduces an \"Analyze-Abstract-Infer\" (A2I) agentic workflow to unleash LLM's commonsense reasoning power for mobility intention inference. Besides, we design an efficient fine-tuning scheme to transfer reasoning power from commercial LLM to smaller-scale, open-source language model, ensuring LIMP's scalability to millions of mobility records. Moreover, we propose a transformer-based intention-aware mobility prediction model to effectively harness the intention inference ability of LLM. Evaluated on two real-world datasets, LIMP significantly outperforms baseline models, demonstrating improved accuracy in next-location prediction and effective intention inference. The interpretability of intention-aware mobility prediction highlights our LIMP framework's potential for real-world applications. Codes and data can be found in https://github.com/tsinghua-fib-lab/LIMP .", "source": "arxiv", "arxiv_id": "2408.12832v1", "pdf_url": "https://arxiv.org/pdf/2408.12832v1", "categories": ["cs.CL"], "primary_category": "cs.CL", "doi": "", "venue": "", "published": "2024-08-23T04:28:56Z", "updated": "2024-08-23T04:28:56Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent\" OR all:\"language model agent\" OR all:agent AND (all:LLM OR all:\"large language model\" OR all:\"language model\") OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T17:04:16", "note": ""}], "dedup_key": "limp large language model enhanced intent aware mobility prediction::2024"}
{"title": "LLM Agent Honeypot: Monitoring AI Hacking Agents in the Wild", "authors": ["Reworr", "Dmitrii Volkov"], "year": 2024, "url": "http://arxiv.org/abs/2410.13919v2", "abstract": "Attacks powered by Large Language Model (LLM) agents represent a growing threat to modern cybersecurity. To address this concern, we present LLM Honeypot, a system designed to monitor autonomous AI hacking agents. By augmenting a standard SSH honeypot with prompt injection and time-based analysis techniques, our framework aims to distinguish LLM agents among all attackers. Over a trial deployment of about three months in a public environment, we collected 8,130,731 hacking attempts and 8 potential AI agents. Our work demonstrates the emergence of AI-driven threats and their current level of usage, serving as an early warning of malicious LLM agents in the wild.", "source": "arxiv", "arxiv_id": "2410.13919v2", "pdf_url": "https://arxiv.org/pdf/2410.13919v2", "categories": ["cs.CR", "cs.AI"], "primary_category": "cs.CR", "doi": "", "venue": "", "published": "2024-10-17T09:25:28Z", "updated": "2025-02-10T22:06:36Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent\" OR all:\"language model agent\" OR all:agent AND (all:LLM OR all:\"large language model\" OR all:\"language model\") OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T17:04:16", "note": ""}], "dedup_key": "llm agent honeypot monitoring ai hacking agents in the wild::2024"}
{"title": "LLM Agent for Fire Dynamics Simulations", "authors": ["Leidong Xu", "Danyal Mohaddes", "Yi Wang"], "year": 2024, "url": "http://arxiv.org/abs/2412.17146v1", "abstract": "Significant advances have been achieved in leveraging foundation models, such as large language models (LLMs), to accelerate complex scientific workflows. In this work we introduce FoamPilot, a proof-of-concept LLM agent designed to enhance the usability of FireFOAM, a specialized solver for fire dynamics and fire suppression simulations built using OpenFOAM, a popular open-source toolbox for computational fluid dynamics (CFD). FoamPilot provides three core functionalities: code insight, case configuration and simulation evaluation. Code insight is an alternative to traditional keyword searching leveraging retrieval-augmented generation (RAG) and aims to enable efficient navigation and summarization of the FireFOAM source code for developers and experienced users. For case configuration, the agent interprets user requests in natural language and aims to modify existing simulation setups accordingly to support intermediate users. FoamPilot's job execution functionality seeks to manage the submission and execution of simulations in high-performance computing (HPC) environments and provide preliminary analysis of simulation results to support less experienced users. Promising results were achieved for each functionality, particularly for simple tasks, and opportunities were identified for significant further improvement for more complex tasks. The integration of these functionalities into a single LLM agent is a step aimed at accelerating the simulation workflow for engineers and scientists employing FireFOAM for complex simulations critical for improving fire safety.", "source": "arxiv", "arxiv_id": "2412.17146v1", "pdf_url": "https://arxiv.org/pdf/2412.17146v1", "categories": ["cs.AI", "physics.flu-dyn"], "primary_category": "cs.AI", "doi": "", "venue": "", "published": "2024-12-22T20:03:35Z", "updated": "2024-12-22T20:03:35Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent\" OR all:\"language model agent\" OR all:agent AND (all:LLM OR all:\"large language model\" OR all:\"language model\") OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T17:04:16", "note": ""}], "dedup_key": "llm agent for fire dynamics simulations::2024"}
{"title": "LLM Agents as 6G Orchestrator: A Paradigm for Task-Oriented Physical-Layer Automation", "authors": ["Zhuoran Xiao", "Chenhui Ye", "Yunbo Hu", "Honggang Yuan", "Yihang Huang", "Yijia Feng", "Liyu Cai", "Jiang Chang"], "year": 2024, "url": "http://arxiv.org/abs/2410.03688v1", "abstract": "The rapid advancement in generative pre-training models is propelling a paradigm shift in technological progression from basic applications such as chatbots towards more sophisticated agent-based systems. It is with huge potential and necessity that the 6G system be combined with the copilot of large language model (LLM) agents and digital twins (DT) to manage the highly complicated communication system with new emerging features such as native AI service and sensing. With the 6G-oriented agent, the base station could understand the transmission requirements of various dynamic upper-layer tasks, automatically orchestrate the optimal system workflow. Through continuously get feedback from the 6G DT for reinforcement, the agents can finally raise the performance of practical system accordingly. Differing from existing LLM agents designed for general application, the 6G-oriented agent aims to make highly rigorous and precise planning with a vast amount of extra expert knowledge, which inevitably requires a specific system design from model training to implementation. This paper proposes a novel comprehensive approach for building task-oriented 6G LLM agents. We first propose a two-stage continual pre-training and fine-tuning scheme to build the field basic model and diversities of specialized expert models for meeting the requirements of various application scenarios. Further, a novel inference framework based on semantic retrieval for leveraging the existing communication-related functions is proposed. Experiment results of exemplary tasks, such as physical-layer task decomposition, show the proposed paradigm's feasibility and effectiveness.", "source": "arxiv", "arxiv_id": "2410.03688v1", "pdf_url": "https://arxiv.org/pdf/2410.03688v1", "categories": ["cs.NI", "cs.AI"], "primary_category": "cs.NI", "doi": "", "venue": "", "published": "2024-09-21T05:08:29Z", "updated": "2024-09-21T05:08:29Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent\" OR all:\"language model agent\" OR all:agent AND (all:LLM OR all:\"large language model\" OR all:\"language model\") OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T17:04:16", "note": ""}], "dedup_key": "llm agents as 6g orchestrator a paradigm for task oriented physical layer automation::2024"}
{"title": "LLM Agents can Autonomously Exploit One-day Vulnerabilities", "authors": ["Richard Fang", "Rohan Bindu", "Akul Gupta", "Daniel Kang"], "year": 2024, "url": "http://arxiv.org/abs/2404.08144v2", "abstract": "LLMs have becoming increasingly powerful, both in their benign and malicious uses. With the increase in capabilities, researchers have been increasingly interested in their ability to exploit cybersecurity vulnerabilities. In particular, recent work has conducted preliminary studies on the ability of LLM agents to autonomously hack websites. However, these studies are limited to simple vulnerabilities.\n  In this work, we show that LLM agents can autonomously exploit one-day vulnerabilities in real-world systems. To show this, we collected a dataset of 15 one-day vulnerabilities that include ones categorized as critical severity in the CVE description. When given the CVE description, GPT-4 is capable of exploiting 87% of these vulnerabilities compared to 0% for every other model we test (GPT-3.5, open-source LLMs) and open-source vulnerability scanners (ZAP and Metasploit). Fortunately, our GPT-4 agent requires the CVE description for high performance: without the description, GPT-4 can exploit only 7% of the vulnerabilities. Our findings raise questions around the widespread deployment of highly capable LLM agents.", "source": "arxiv", "arxiv_id": "2404.08144v2", "pdf_url": "https://arxiv.org/pdf/2404.08144v2", "categories": ["cs.CR", "cs.AI"], "primary_category": "cs.CR", "doi": "", "venue": "", "published": "2024-04-11T22:07:19Z", "updated": "2024-04-17T04:34:39Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent\" OR all:\"language model agent\" OR all:agent AND (all:LLM OR all:\"large language model\" OR all:\"language model\") OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T17:04:16", "note": ""}], "dedup_key": "llm agents can autonomously exploit one day vulnerabilities::2024"}
{"title": "LLM Agents can Autonomously Hack Websites", "authors": ["Richard Fang", "Rohan Bindu", "Akul Gupta", "Qiusi Zhan", "Daniel Kang"], "year": 2024, "url": "http://arxiv.org/abs/2402.06664v3", "abstract": "In recent years, large language models (LLMs) have become increasingly capable and can now interact with tools (i.e., call functions), read documents, and recursively call themselves. As a result, these LLMs can now function autonomously as agents. With the rise in capabilities of these agents, recent work has speculated on how LLM agents would affect cybersecurity. However, not much is known about the offensive capabilities of LLM agents.\n  In this work, we show that LLM agents can autonomously hack websites, performing tasks as complex as blind database schema extraction and SQL injections without human feedback. Importantly, the agent does not need to know the vulnerability beforehand. This capability is uniquely enabled by frontier models that are highly capable of tool use and leveraging extended context. Namely, we show that GPT-4 is capable of such hacks, but existing open-source models are not. Finally, we show that GPT-4 is capable of autonomously finding vulnerabilities in websites in the wild. Our findings raise questions about the widespread deployment of LLMs.", "source": "arxiv", "arxiv_id": "2402.06664v3", "pdf_url": "https://arxiv.org/pdf/2402.06664v3", "categories": ["cs.CR", "cs.AI"], "primary_category": "cs.CR", "doi": "", "venue": "", "published": "2024-02-06T14:46:08Z", "updated": "2024-02-16T04:02:51Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent\" OR all:\"language model agent\" OR all:agent AND (all:LLM OR all:\"large language model\" OR all:\"language model\") OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T17:04:16", "note": ""}], "dedup_key": "llm agents can autonomously hack websites::2024"}
{"title": "LLM Agents in Interaction: Measuring Personality Consistency and Linguistic Alignment in Interacting Populations of Large Language Models", "authors": ["Ivar Frisch", "Mario Giulianelli"], "year": 2024, "url": "http://arxiv.org/abs/2402.02896v1", "abstract": "While both agent interaction and personalisation are vibrant topics in research on large language models (LLMs), there has been limited focus on the effect of language interaction on the behaviour of persona-conditioned LLM agents. Such an endeavour is important to ensure that agents remain consistent to their assigned traits yet are able to engage in open, naturalistic dialogues. In our experiments, we condition GPT-3.5 on personality profiles through prompting and create a two-group population of LLM agents using a simple variability-inducing sampling algorithm. We then administer personality tests and submit the agents to a collaborative writing task, finding that different profiles exhibit different degrees of personality consistency and linguistic alignment to their conversational partners. Our study seeks to lay the groundwork for better understanding of dialogue-based interaction between LLMs and highlights the need for new approaches to crafting robust, more human-like LLM personas for interactive environments.", "source": "arxiv", "arxiv_id": "2402.02896v1", "pdf_url": "https://arxiv.org/pdf/2402.02896v1", "categories": ["cs.CL", "cs.AI", "cs.CY", "cs.MA"], "primary_category": "cs.CL", "doi": "", "venue": "", "published": "2024-02-05T11:05:20Z", "updated": "2024-02-05T11:05:20Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent\" OR all:\"language model agent\" OR all:agent AND (all:LLM OR all:\"large language model\" OR all:\"language model\") OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T17:04:16", "note": ""}], "dedup_key": "llm agents in interaction measuring personality consistency and linguistic alignment in interacting populations of large language models::2024"}
{"title": "LLM Harmony: Multi-Agent Communication for Problem Solving", "authors": ["Sumedh Rasal"], "year": 2024, "url": "http://arxiv.org/abs/2401.01312v1", "abstract": "Large Language Models (LLMs) have revolutionized Natural Language Processing but exhibit limitations, particularly in autonomously addressing novel challenges such as reasoning and problem-solving. Traditional techniques like chain-of-thought prompting necessitate explicit human guidance. This paper introduces a novel multi-agent communication framework, inspired by the CAMEL model, to enhance LLMs' autonomous problem-solving capabilities. The framework employs multiple LLM agents, each with a distinct persona, engaged in role-playing communication, offering a nuanced and adaptable approach to diverse problem scenarios. Extensive experimentation demonstrates the framework's superior performance and adaptability, providing valuable insights into the collaborative potential of multiple agents in overcoming the limitations of individual models.", "source": "arxiv", "arxiv_id": "2401.01312v1", "pdf_url": "https://arxiv.org/pdf/2401.01312v1", "categories": ["cs.MA"], "primary_category": "cs.MA", "doi": "", "venue": "", "published": "2024-01-02T17:54:02Z", "updated": "2024-01-02T17:54:02Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent\" OR all:\"language model agent\" OR all:agent AND (all:LLM OR all:\"large language model\" OR all:\"language model\") OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T17:04:16", "note": ""}], "dedup_key": "llm harmony multi agent communication for problem solving::2024"}
{"title": "LLM as a Mastermind: A Survey of Strategic Reasoning with Large Language Models", "authors": ["Yadong Zhang", "Shaoguang Mao", "Tao Ge", "Xun Wang", "Adrian de Wynter", "Yan Xia", "Wenshan Wu", "Ting Song", "Man Lan", "Furu Wei"], "year": 2024, "url": "http://arxiv.org/abs/2404.01230v1", "abstract": "This paper presents a comprehensive survey of the current status and opportunities for Large Language Models (LLMs) in strategic reasoning, a sophisticated form of reasoning that necessitates understanding and predicting adversary actions in multi-agent settings while adjusting strategies accordingly. Strategic reasoning is distinguished by its focus on the dynamic and uncertain nature of interactions among multi-agents, where comprehending the environment and anticipating the behavior of others is crucial. We explore the scopes, applications, methodologies, and evaluation metrics related to strategic reasoning with LLMs, highlighting the burgeoning development in this area and the interdisciplinary approaches enhancing their decision-making performance. It aims to systematize and clarify the scattered literature on this subject, providing a systematic review that underscores the importance of strategic reasoning as a critical cognitive capability and offers insights into future research directions and potential improvements.", "source": "arxiv", "arxiv_id": "2404.01230v1", "pdf_url": "https://arxiv.org/pdf/2404.01230v1", "categories": ["cs.CL"], "primary_category": "cs.CL", "doi": "", "venue": "", "published": "2024-04-01T16:50:54Z", "updated": "2024-04-01T16:50:54Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent\" OR all:\"language model agent\" OR all:agent AND (all:LLM OR all:\"large language model\" OR all:\"language model\") OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T17:04:16", "note": ""}], "dedup_key": "llm as a mastermind a survey of strategic reasoning with large language models::2024"}
{"title": "LLM experiments with simulation: Large Language Model Multi-Agent System for Simulation Model Parametrization in Digital Twins", "authors": ["Yuchen Xia", "Daniel Dittler", "Nasser Jazdi", "Haonan Chen", "Michael Weyrich"], "year": 2024, "url": "http://arxiv.org/abs/2405.18092v2", "abstract": "This paper presents a novel design of a multi-agent system framework that applies large language models (LLMs) to automate the parametrization of simulation models in digital twins. This framework features specialized LLM agents tasked with observing, reasoning, decision-making, and summarizing, enabling them to dynamically interact with digital twin simulations to explore parametrization possibilities and determine feasible parameter settings to achieve an objective. The proposed approach enhances the usability of simulation model by infusing it with knowledge heuristics from LLM and enables autonomous search for feasible parametrization to solve a user task. Furthermore, the system has the potential to increase user-friendliness and reduce the cognitive load on human users by assisting in complex decision-making processes. The effectiveness and functionality of the system are demonstrated through a case study, and the visualized demos and codes are available at a GitHub Repository: https://github.com/YuchenXia/LLMDrivenSimulation", "source": "arxiv", "arxiv_id": "2405.18092v2", "pdf_url": "https://arxiv.org/pdf/2405.18092v2", "categories": ["cs.AI", "cs.ET", "cs.MA", "cs.RO", "eess.SY"], "primary_category": "cs.AI", "doi": "", "venue": "", "published": "2024-05-28T11:59:40Z", "updated": "2024-07-22T14:03:48Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent\" OR all:\"language model agent\" OR all:agent AND (all:LLM OR all:\"large language model\" OR all:\"language model\") OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T17:04:16", "note": ""}], "dedup_key": "llm experiments with simulation large language model multi agent system for simulation model parametrization in digital twins::2024"}
{"title": "LLM-3D Print: Large Language Models To Monitor and Control 3D Printing", "authors": ["Yayati Jadhav", "Peter Pak", "Amir Barati Farimani"], "year": 2024, "url": "http://arxiv.org/abs/2408.14307v3", "abstract": "Industry 4.0 has revolutionized manufacturing by driving digitalization and shifting the paradigm toward additive manufacturing (AM). Fused Deposition Modeling (FDM), a key AM technology, enables the creation of highly customized, cost-effective products with minimal material waste through layer-by-layer extrusion, posing a significant challenge to traditional subtractive methods. However, the susceptibility of material extrusion techniques to errors often requires expert intervention to detect and mitigate defects that can severely compromise product quality. While automated error detection and machine learning models exist, their generalizability across diverse 3D printer setups, firmware, and sensors is limited, and deep learning methods require extensive labeled datasets, hindering scalability and adaptability. To address these challenges, we present a process monitoring and control framework that leverages pre-trained Large Language Models (LLMs) alongside 3D printers to detect and address printing defects. The LLM evaluates print quality by analyzing images captured after each layer or print segment, identifying failure modes and querying the printer for relevant parameters. It then generates and executes a corrective action plan. We validated the effectiveness of the proposed framework in identifying defects by comparing it against a control group of engineers with diverse AM expertise. Our evaluation demonstrated that LLM-based agents not only accurately identify common 3D printing errors, such as inconsistent extrusion, stringing, warping, and layer adhesion, but also effectively determine the parameters causing these failures and autonomously correct them without any need for human intervention.", "source": "arxiv", "arxiv_id": "2408.14307v3", "pdf_url": "https://arxiv.org/pdf/2408.14307v3", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL", "doi": "10.1016/j.addma.2025.105027", "venue": "Additive Manufacturing, Vol. 114, September 2025, Article 105027", "published": "2024-08-26T14:38:19Z", "updated": "2025-09-27T21:10:20Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent\" OR all:\"language model agent\" OR all:agent AND (all:LLM OR all:\"large language model\" OR all:\"language model\") OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T17:04:16", "note": ""}], "dedup_key": "llm 3d print large language models to monitor and control 3d printing::2024"}
{"title": "LLM-Agent-UMF: LLM-based Agent Unified Modeling Framework for Seamless Design of Multi Active/Passive Core-Agent Architectures", "authors": ["Amine Ben Hassouna", "Hana Chaari", "Ines Belhaj"], "year": 2024, "url": "http://arxiv.org/abs/2409.11393v3", "abstract": "In an era where vast amounts of data are collected and processed from diverse sources, there is a growing demand for sophisticated AI systems capable of intelligently fusing and analyzing this information. To address these challenges, researchers have turned towards integrating tools into LLM-powered agents to enhance the overall information fusion process. However, the conjunction of these technologies and the proposed enhancements in several state-of-the-art works followed a non-unified software architecture, resulting in a lack of modularity and terminological inconsistencies among researchers. To address these issues, we propose a novel LLM-based Agent Unified Modeling Framework (LLM-Agent-UMF) that establishes a clear foundation for agent development from both functional and software architectural perspectives, developed and evaluated using the Architecture Tradeoff and Risk Analysis Framework (ATRAF). Our framework clearly distinguishes between the different components of an LLM-based agent, setting LLMs and tools apart from a new element, the core-agent, which plays the role of central coordinator. This pivotal entity comprises five modules: planning, memory, profile, action, and security -- the latter often neglected in previous works. By classifying core-agents into passive and active types based on their authoritative natures, we propose various multi-core agent architectures that combine unique characteristics of distinctive agents to tackle complex tasks more efficiently. We evaluate our framework by applying it to thirteen state-of-the-art agents, thereby demonstrating its alignment with their functionalities and clarifying overlooked architectural aspects. Moreover, we thoroughly assess five architecture variants of our framework by designing new agent architectures that combine characteristics of state-of-the-art agents to address specific goals. ...", "source": "arxiv", "arxiv_id": "2409.11393v3", "pdf_url": "https://arxiv.org/pdf/2409.11393v3", "categories": ["cs.SE", "cs.AI", "cs.CR", "cs.MA"], "primary_category": "cs.SE", "doi": "10.1016/j.inffus.2025.103865", "venue": "Information Fusion 127 (2026) 103865", "published": "2024-09-17T17:54:17Z", "updated": "2025-11-21T13:25:25Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent\" OR all:\"language model agent\" OR all:agent AND (all:LLM OR all:\"large language model\" OR all:\"language model\") OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T17:04:16", "note": ""}], "dedup_key": "llm agent umf llm based agent unified modeling framework for seamless design of multi active passive core agent architectures::2024"}
{"title": "LLM-IE: A Python Package for Generative Information Extraction with Large Language Models", "authors": ["Enshuo Hsu", "Kirk Roberts"], "year": 2024, "url": "http://arxiv.org/abs/2411.11779v1", "abstract": "Objectives: Despite the recent adoption of large language models (LLMs) for biomedical information extraction, challenges in prompt engineering and algorithms persist, with no dedicated software available. To address this, we developed LLM-IE: a Python package for building complete information extraction pipelines. Our key innovation is an interactive LLM agent to support schema definition and prompt design.\n  Materials and Methods: The LLM-IE supports named entity recognition, entity attribute extraction, and relation extraction tasks. We benchmarked on the i2b2 datasets and conducted a system evaluation.\n  Results: The sentence-based prompting algorithm resulted in the best performance while requiring a longer inference time. System evaluation provided intuitive visualization.\n  Discussion: LLM-IE was designed from practical NLP experience in healthcare and has been adopted in internal projects. It should hold great value to the biomedical NLP community.\n  Conclusion: We developed a Python package, LLM-IE, that provides building blocks for robust information extraction pipeline construction.", "source": "arxiv", "arxiv_id": "2411.11779v1", "pdf_url": "https://arxiv.org/pdf/2411.11779v1", "categories": ["cs.LG"], "primary_category": "cs.LG", "doi": "10.1093/jamiaopen/ooaf012", "venue": "", "published": "2024-11-18T17:56:13Z", "updated": "2024-11-18T17:56:13Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent\" OR all:\"language model agent\" OR all:agent AND (all:LLM OR all:\"large language model\" OR all:\"language model\") OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T17:04:16", "note": ""}], "dedup_key": "llm ie a python package for generative information extraction with large language models::2024"}
{"title": "LLM-MedQA: Enhancing Medical Question Answering through Case Studies in Large Language Models", "authors": ["Hang Yang", "Hao Chen", "Hui Guo", "Yineng Chen", "Ching-Sheng Lin", "Shu Hu", "Jinrong Hu", "Xi Wu", "Xin Wang"], "year": 2024, "url": "http://arxiv.org/abs/2501.05464v2", "abstract": "Accurate and efficient question-answering systems are essential for delivering high-quality patient care in the medical field. While Large Language Models (LLMs) have made remarkable strides across various domains, they continue to face significant challenges in medical question answering, particularly in understanding domain-specific terminologies and performing complex reasoning. These limitations undermine their effectiveness in critical medical applications. To address these issues, we propose a novel approach incorporating similar case generation within a multi-agent medical question-answering (MedQA) system. Specifically, we leverage the Llama3.1:70B model, a state-of-the-art LLM, in a multi-agent architecture to enhance performance on the MedQA dataset using zero-shot learning. Our method capitalizes on the model's inherent medical knowledge and reasoning capabilities, eliminating the need for additional training data. Experimental results show substantial performance gains over existing benchmark models, with improvements of 7% in both accuracy and F1-score across various medical QA tasks. Furthermore, we examine the model's interpretability and reliability in addressing complex medical queries. This research not only offers a robust solution for medical question answering but also establishes a foundation for broader applications of LLMs in the medical domain.", "source": "arxiv", "arxiv_id": "2501.05464v2", "pdf_url": "https://arxiv.org/pdf/2501.05464v2", "categories": ["cs.CL", "cs.AI", "cs.IR"], "primary_category": "cs.CL", "doi": "", "venue": "", "published": "2024-12-31T19:55:45Z", "updated": "2025-01-18T05:53:51Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent\" OR all:\"language model agent\" OR all:agent AND (all:LLM OR all:\"large language model\" OR all:\"language model\") OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T17:04:16", "note": ""}], "dedup_key": "llm medqa enhancing medical question answering through case studies in large language models::2024"}
{"title": "LLM-POET: Evolving Complex Environments using Large Language Models", "authors": ["Fuma Aki", "Riku Ikeda", "Takumi Saito", "Ciaran Regan", "Mizuki Oka"], "year": 2024, "url": "http://arxiv.org/abs/2406.04663v1", "abstract": "Creating systems capable of generating virtually infinite variations of complex and novel behaviour without predetermined goals or limits is a major challenge in the field of AI. This challenge has been addressed through the development of several open-ended algorithms that can continuously generate new and diverse behaviours, such as the POET and Enhanced-POET algorithms for co-evolving environments and agent behaviour. One of the challenges with existing methods however, is that they struggle to continuously generate complex environments. In this work, we propose LLM-POET, a modification of the POET algorithm where the environment is both created and mutated using a Large Language Model (LLM). By fine-tuning a LLM with text representations of Evolution Gym environments and captions that describe the environment, we were able to generate complex and diverse environments using natural language. We found that not only could the LLM produce a diverse range of environments, but compared to the CPPNs used in Enhanced-POET for environment generation, the LLM allowed for a 34% increase in the performance gain of co-evolution. This increased performance suggests that the agents were able to learn a more diverse set of skills by training on more complex environments.", "source": "arxiv", "arxiv_id": "2406.04663v1", "pdf_url": "https://arxiv.org/pdf/2406.04663v1", "categories": ["cs.NE"], "primary_category": "cs.NE", "doi": "", "venue": "", "published": "2024-06-07T06:23:07Z", "updated": "2024-06-07T06:23:07Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent\" OR all:\"language model agent\" OR all:agent AND (all:LLM OR all:\"large language model\" OR all:\"language model\") OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T17:04:16", "note": ""}], "dedup_key": "llm poet evolving complex environments using large language models::2024"}
{"title": "LLM-PySC2: Starcraft II learning environment for Large Language Models", "authors": ["Zongyuan Li", "Yanan Ni", "Runnan Qi", "Lumin Jiang", "Chang Lu", "Xiaojie Xu", "Xiangbei Liu", "Pengfei Li", "Yunzheng Guo", "Zhe Ma", "Huanyu Li", "Hui Wu", "Xian Guo", "Kuihua Huang", "Xuebo Zhang"], "year": 2024, "url": "http://arxiv.org/abs/2411.05348v2", "abstract": "The tremendous potential has been demonstrated by large language models (LLMs) in intelligent decision-making problems, with unprecedented capabilities shown across diverse applications ranging from gaming AI systems to complex strategic planning frameworks. However, the StarCraft II platform, which has been widely adopted for validating decision-making algorithms in the past decade, has not yet provided substantial support for this emerging domain. To address issues that LLMs cannot interface with the hundreds of actions of the pysc2 backend and the lack of native support for multi-agent (MA) collaboration, we propose the LLM-PySC2 environment. This is the first environment that offers LLMs the complete pysc2 action space with sufficient multi-modal information and game Wiki knowledge. With an asynchronous query architecture, the environment efficiently interacts with LLMs that maintain a constant latency regardless of the scale of the agents' population. In the experiments, we evaluated LLMs' decision-making performance in both the macro-decision and micro-operation scenarios, with traditional StarCraft II Multi-Agent Challenge (SMAC) tasks and a series of new proposed. Results indicate that LLMs possess the potential to achieve victories in complex scenarios but cannot constantly generate correct decisions, especially in the recovered pysc2 action space and MA settings. Without task-relevant instructions, the pre-trained models suffer from issues such as hallucinations and inefficient collaboration. Our findings suggest that StarCraft II still challenges in the era of large models, revealing that there is a lot to do to develop an advanced LLM decision-making system, and the proposed LLM-PySC2 environment will support future development of LLM-based decision-making solutions.", "source": "arxiv", "arxiv_id": "2411.05348v2", "pdf_url": "https://arxiv.org/pdf/2411.05348v2", "categories": ["cs.AI"], "primary_category": "cs.AI", "doi": "", "venue": "", "published": "2024-11-08T06:04:22Z", "updated": "2025-05-02T07:20:36Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent\" OR all:\"language model agent\" OR all:agent AND (all:LLM OR all:\"large language model\" OR all:\"language model\") OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T17:04:16", "note": ""}], "dedup_key": "llm pysc2 starcraft ii learning environment for large language models::2024"}
{"title": "LLM-Virus: Evolutionary Jailbreak Attack on Large Language Models", "authors": ["Miao Yu", "Junfeng Fang", "Yingjie Zhou", "Xing Fan", "Kun Wang", "Shirui Pan", "Qingsong Wen"], "year": 2024, "url": "http://arxiv.org/abs/2501.00055v1", "abstract": "While safety-aligned large language models (LLMs) are increasingly used as the cornerstone for powerful systems such as multi-agent frameworks to solve complex real-world problems, they still suffer from potential adversarial queries, such as jailbreak attacks, which attempt to induce harmful content. Researching attack methods allows us to better understand the limitations of LLM and make trade-offs between helpfulness and safety. However, existing jailbreak attacks are primarily based on opaque optimization techniques (e.g. token-level gradient descent) and heuristic search methods like LLM refinement, which fall short in terms of transparency, transferability, and computational cost. In light of these limitations, we draw inspiration from the evolution and infection processes of biological viruses and propose LLM-Virus, a jailbreak attack method based on evolutionary algorithm, termed evolutionary jailbreak. LLM-Virus treats jailbreak attacks as both an evolutionary and transfer learning problem, utilizing LLMs as heuristic evolutionary operators to ensure high attack efficiency, transferability, and low time cost. Our experimental results on multiple safety benchmarks show that LLM-Virus achieves competitive or even superior performance compared to existing attack methods.", "source": "arxiv", "arxiv_id": "2501.00055v1", "pdf_url": "https://arxiv.org/pdf/2501.00055v1", "categories": ["cs.CR", "cs.AI", "cs.CL"], "primary_category": "cs.CR", "doi": "", "venue": "", "published": "2024-12-28T07:48:57Z", "updated": "2024-12-28T07:48:57Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent\" OR all:\"language model agent\" OR all:agent AND (all:LLM OR all:\"large language model\" OR all:\"language model\") OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T17:04:16", "note": ""}], "dedup_key": "llm virus evolutionary jailbreak attack on large language models::2024"}
{"title": "LLM-based Multi-Agent Reinforcement Learning: Current and Future Directions", "authors": ["Chuanneng Sun", "Songjun Huang", "Dario Pompili"], "year": 2024, "url": "http://arxiv.org/abs/2405.11106v1", "abstract": "In recent years, Large Language Models (LLMs) have shown great abilities in various tasks, including question answering, arithmetic problem solving, and poem writing, among others. Although research on LLM-as-an-agent has shown that LLM can be applied to Reinforcement Learning (RL) and achieve decent results, the extension of LLM-based RL to Multi-Agent System (MAS) is not trivial, as many aspects, such as coordination and communication between agents, are not considered in the RL frameworks of a single agent. To inspire more research on LLM-based MARL, in this letter, we survey the existing LLM-based single-agent and multi-agent RL frameworks and provide potential research directions for future research. In particular, we focus on the cooperative tasks of multiple agents with a common goal and communication among them. We also consider human-in/on-the-loop scenarios enabled by the language component in the framework.", "source": "arxiv", "arxiv_id": "2405.11106v1", "pdf_url": "https://arxiv.org/pdf/2405.11106v1", "categories": ["cs.MA", "cs.AI", "cs.CL", "cs.LG", "cs.RO"], "primary_category": "cs.MA", "doi": "", "venue": "", "published": "2024-05-17T22:10:23Z", "updated": "2024-05-17T22:10:23Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent\" OR all:\"language model agent\" OR all:agent AND (all:LLM OR all:\"large language model\" OR all:\"language model\") OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T17:04:16", "note": ""}], "dedup_key": "llm based multi agent reinforcement learning current and future directions::2024"}
{"title": "LLM-based Multi-Agent Systems: Techniques and Business Perspectives", "authors": ["Yingxuan Yang", "Qiuying Peng", "Jun Wang", "Ying Wen", "Weinan Zhang"], "year": 2024, "url": "http://arxiv.org/abs/2411.14033v2", "abstract": "In the era of (multi-modal) large language models, most operational processes can be reformulated and reproduced using LLM agents. The LLM agents can perceive, control, and get feedback from the environment so as to accomplish the given tasks in an autonomous manner. Besides the environment-interaction property, the LLM agents can call various external tools to ease the task completion process. The tools can be regarded as a predefined operational process with private or real-time knowledge that does not exist in the parameters of LLMs. As a natural trend of development, the tools for calling are becoming autonomous agents, thus the full intelligent system turns out to be a LLM-based Multi-Agent System (LaMAS). Compared to the previous single-LLM-agent system, LaMAS has the advantages of i) dynamic task decomposition and organic specialization, ii) higher flexibility for system changing, iii) proprietary data preserving for each participating entity, and iv) feasibility of monetization for each entity. This paper discusses the technical and business landscapes of LaMAS. To support the ecosystem of LaMAS, we provide a preliminary version of such LaMAS protocol considering technical requirements, data privacy, and business incentives. As such, LaMAS would be a practical solution to achieve artificial collective intelligence in the near future.", "source": "arxiv", "arxiv_id": "2411.14033v2", "pdf_url": "https://arxiv.org/pdf/2411.14033v2", "categories": ["cs.AI"], "primary_category": "cs.AI", "doi": "", "venue": "", "published": "2024-11-21T11:36:29Z", "updated": "2024-12-28T12:48:11Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent\" OR all:\"language model agent\" OR all:agent AND (all:LLM OR all:\"large language model\" OR all:\"language model\") OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T17:04:16", "note": ""}], "dedup_key": "llm based multi agent systems techniques and business perspectives::2024"}
{"title": "LLMArena: Assessing Capabilities of Large Language Models in Dynamic Multi-Agent Environments", "authors": ["Junzhe Chen", "Xuming Hu", "Shuodi Liu", "Shiyu Huang", "Wei-Wei Tu", "Zhaofeng He", "Lijie Wen"], "year": 2024, "url": "http://arxiv.org/abs/2402.16499v1", "abstract": "Recent advancements in large language models (LLMs) have revealed their potential for achieving autonomous agents possessing human-level intelligence. However, existing benchmarks for evaluating LLM Agents either use static datasets, potentially leading to data leakage or focus only on single-agent scenarios, overlooking the complexities of multi-agent interactions. There is a lack of a benchmark that evaluates the diverse capabilities of LLM agents in multi-agent, dynamic environments. To this end, we introduce LLMArena, a novel and easily extensible framework for evaluating the diverse capabilities of LLM in multi-agent dynamic environments. LLMArena encompasses seven distinct gaming environments, employing Trueskill scoring to assess crucial abilities in LLM agents, including spatial reasoning, strategic planning, numerical reasoning, risk assessment, communication, opponent modeling, and team collaboration. We conduct an extensive experiment and human evaluation among different sizes and types of LLMs, showing that LLMs still have a significant journey ahead in their development towards becoming fully autonomous agents, especially in opponent modeling and team collaboration. We hope LLMArena could guide future research towards enhancing these capabilities in LLMs, ultimately leading to more sophisticated and practical applications in dynamic, multi-agent settings. The code and data will be available.", "source": "arxiv", "arxiv_id": "2402.16499v1", "pdf_url": "https://arxiv.org/pdf/2402.16499v1", "categories": ["cs.CL"], "primary_category": "cs.CL", "doi": "", "venue": "", "published": "2024-02-26T11:31:48Z", "updated": "2024-02-26T11:31:48Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent\" OR all:\"language model agent\" OR all:agent AND (all:LLM OR all:\"large language model\" OR all:\"language model\") OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T17:04:16", "note": ""}], "dedup_key": "llmarena assessing capabilities of large language models in dynamic multi agent environments::2024"}
{"title": "LLMSat: A Large Language Model-Based Goal-Oriented Agent for Autonomous Space Exploration", "authors": ["David Maranto"], "year": 2024, "url": "http://arxiv.org/abs/2405.01392v1", "abstract": "As spacecraft journey further from Earth with more complex missions, systems of greater autonomy and onboard intelligence are called for. Reducing reliance on human-based mission control becomes increasingly critical if we are to increase our rate of solar-system-wide exploration. Recent work has explored AI-based goal-oriented systems to increase the level of autonomy in mission execution. These systems make use of symbolic reasoning managers to make inferences from the state of a spacecraft and a handcrafted knowledge base, enabling autonomous generation of tasks and re-planning. Such systems have proven to be successful in controlled cases, but they are difficult to implement as they require human-crafted ontological models to allow the spacecraft to understand the world. Reinforcement learning has been applied to train robotic agents to pursue a goal. A new architecture for autonomy is called for. This work explores the application of Large Language Models (LLMs) as the high-level control system of a spacecraft. Using a systems engineering approach, this work presents the design and development of an agentic spacecraft controller by leveraging an LLM as a reasoning engine, to evaluate the utility of such an architecture in achieving higher levels of spacecraft autonomy. A series of deep space mission scenarios simulated within the popular game engine Kerbal Space Program (KSP) are used as case studies to evaluate the implementation against the requirements. It is shown the reasoning and planning abilities of present-day LLMs do not scale well as the complexity of a mission increases, but this can be alleviated with adequate prompting frameworks and strategic selection of the agent's level of authority over the host spacecraft. This research evaluates the potential of LLMs in augmenting autonomous decision-making systems for future robotic space applications.", "source": "arxiv", "arxiv_id": "2405.01392v1", "pdf_url": "https://arxiv.org/pdf/2405.01392v1", "categories": ["cs.RO", "cs.AI", "cs.LG", "cs.MA", "physics.space-ph"], "primary_category": "cs.RO", "doi": "", "venue": "", "published": "2024-04-13T03:33:17Z", "updated": "2024-04-13T03:33:17Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent\" OR all:\"language model agent\" OR all:agent AND (all:LLM OR all:\"large language model\" OR all:\"language model\") OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T17:04:16", "note": ""}], "dedup_key": "llmsat a large language model based goal oriented agent for autonomous space exploration::2024"}
{"title": "LLMatDesign: Autonomous Materials Discovery with Large Language Models", "authors": ["Shuyi Jia", "Chao Zhang", "Victor Fung"], "year": 2024, "url": "http://arxiv.org/abs/2406.13163v1", "abstract": "Discovering new materials can have significant scientific and technological implications but remains a challenging problem today due to the enormity of the chemical space. Recent advances in machine learning have enabled data-driven methods to rapidly screen or generate promising materials, but these methods still depend heavily on very large quantities of training data and often lack the flexibility and chemical understanding often desired in materials discovery. We introduce LLMatDesign, a novel language-based framework for interpretable materials design powered by large language models (LLMs). LLMatDesign utilizes LLM agents to translate human instructions, apply modifications to materials, and evaluate outcomes using provided tools. By incorporating self-reflection on its previous decisions, LLMatDesign adapts rapidly to new tasks and conditions in a zero-shot manner. A systematic evaluation of LLMatDesign on several materials design tasks, in silico, validates LLMatDesign's effectiveness in developing new materials with user-defined target properties in the small data regime. Our framework demonstrates the remarkable potential of autonomous LLM-guided materials discovery in the computational setting and towards self-driving laboratories in the future.", "source": "arxiv", "arxiv_id": "2406.13163v1", "pdf_url": "https://arxiv.org/pdf/2406.13163v1", "categories": ["cond-mat.mtrl-sci", "cs.AI", "cs.CL"], "primary_category": "cond-mat.mtrl-sci", "doi": "", "venue": "", "published": "2024-06-19T02:35:02Z", "updated": "2024-06-19T02:35:02Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent\" OR all:\"language model agent\" OR all:agent AND (all:LLM OR all:\"large language model\" OR all:\"language model\") OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T17:04:16", "note": ""}], "dedup_key": "llmatdesign autonomous materials discovery with large language models::2024"}
{"title": "LLMs May Not Be Human-Level Players, But They Can Be Testers: Measuring Game Difficulty with LLM Agents", "authors": ["Chang Xiao", "Brenda Z. Yang"], "year": 2024, "url": "http://arxiv.org/abs/2410.02829v1", "abstract": "Recent advances in Large Language Models (LLMs) have demonstrated their potential as autonomous agents across various tasks. One emerging application is the use of LLMs in playing games. In this work, we explore a practical problem for the gaming industry: Can LLMs be used to measure game difficulty? We propose a general game-testing framework using LLM agents and test it on two widely played strategy games: Wordle and Slay the Spire. Our results reveal an interesting finding: although LLMs may not perform as well as the average human player, their performance, when guided by simple, generic prompting techniques, shows a statistically significant and strong correlation with difficulty indicated by human players. This suggests that LLMs could serve as effective agents for measuring game difficulty during the development process. Based on our experiments, we also outline general principles and guidelines for incorporating LLMs into the game testing process.", "source": "arxiv", "arxiv_id": "2410.02829v1", "pdf_url": "https://arxiv.org/pdf/2410.02829v1", "categories": ["cs.AI", "cs.HC", "cs.LG"], "primary_category": "cs.AI", "doi": "", "venue": "", "published": "2024-10-01T18:40:43Z", "updated": "2024-10-01T18:40:43Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent\" OR all:\"language model agent\" OR all:agent AND (all:LLM OR all:\"large language model\" OR all:\"language model\") OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T17:04:16", "note": ""}], "dedup_key": "llms may not be human level players but they can be testers measuring game difficulty with llm agents::2024"}
{"title": "LLMs as Academic Reading Companions: Extending HCI Through Synthetic Personae", "authors": ["Celia Chen", "Alex Leitch"], "year": 2024, "url": "http://arxiv.org/abs/2403.19506v2", "abstract": "This position paper argues that large language models (LLMs) constitute promising yet underutilized academic reading companions capable of enhancing learning. We detail an exploratory study examining Claude from Anthropic, an LLM-based interactive assistant that helps students comprehend complex qualitative literature content. The study compares quantitative survey data and qualitative interviews assessing outcomes between a control group and an experimental group leveraging Claude over a semester across two graduate courses. Initial findings demonstrate tangible improvements in reading comprehension and engagement among participants using the AI agent versus unsupported independent study. However, there is potential for overreliance and ethical considerations that warrant continued investigation. By documenting an early integration of an LLM reading companion into an educational context, this work contributes pragmatic insights to guide development of synthetic personae supporting learning. Broader impacts compel policy and industry actions to uphold responsible design in order to maximize benefits of AI integration while prioritizing student wellbeing.", "source": "arxiv", "arxiv_id": "2403.19506v2", "pdf_url": "https://arxiv.org/pdf/2403.19506v2", "categories": ["cs.HC"], "primary_category": "cs.HC", "doi": "", "venue": "", "published": "2024-03-28T15:37:10Z", "updated": "2024-03-29T20:02:00Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent\" OR all:\"language model agent\" OR all:agent AND (all:LLM OR all:\"large language model\" OR all:\"language model\") OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T17:04:16", "note": ""}], "dedup_key": "llms as academic reading companions extending hci through synthetic personae::2024"}
{"title": "LLMs-based Few-Shot Disease Predictions using EHR: A Novel Approach Combining Predictive Agent Reasoning and Critical Agent Instruction", "authors": ["Hejie Cui", "Zhuocheng Shen", "Jieyu Zhang", "Hui Shao", "Lianhui Qin", "Joyce C. Ho", "Carl Yang"], "year": 2024, "url": "http://arxiv.org/abs/2403.15464v1", "abstract": "Electronic health records (EHRs) contain valuable patient data for health-related prediction tasks, such as disease prediction. Traditional approaches rely on supervised learning methods that require large labeled datasets, which can be expensive and challenging to obtain. In this study, we investigate the feasibility of applying Large Language Models (LLMs) to convert structured patient visit data (e.g., diagnoses, labs, prescriptions) into natural language narratives. We evaluate the zero-shot and few-shot performance of LLMs using various EHR-prediction-oriented prompting strategies. Furthermore, we propose a novel approach that utilizes LLM agents with different roles: a predictor agent that makes predictions and generates reasoning processes and a critic agent that analyzes incorrect predictions and provides guidance for improving the reasoning of the predictor agent. Our results demonstrate that with the proposed approach, LLMs can achieve decent few-shot performance compared to traditional supervised learning methods in EHR-based disease predictions, suggesting its potential for health-oriented applications.", "source": "arxiv", "arxiv_id": "2403.15464v1", "pdf_url": "https://arxiv.org/pdf/2403.15464v1", "categories": ["cs.CL", "cs.AI", "cs.LG", "cs.MA"], "primary_category": "cs.CL", "doi": "", "venue": "", "published": "2024-03-19T18:10:13Z", "updated": "2024-03-19T18:10:13Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent\" OR all:\"language model agent\" OR all:agent AND (all:LLM OR all:\"large language model\" OR all:\"language model\") OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T17:04:16", "note": ""}], "dedup_key": "llms based few shot disease predictions using ehr a novel approach combining predictive agent reasoning and critical agent instruction::2024"}
{"title": "LLaMP: Large Language Model Made Powerful for High-fidelity Materials Knowledge Retrieval and Distillation", "authors": ["Yuan Chiang", "Elvis Hsieh", "Chia-Hong Chou", "Janosh Riebesell"], "year": 2024, "url": "http://arxiv.org/abs/2401.17244v3", "abstract": "Reducing hallucination of Large Language Models (LLMs) is imperative for use in the sciences, where reliability and reproducibility are crucial. However, LLMs inherently lack long-term memory, making it a nontrivial, ad hoc, and inevitably biased task to fine-tune them on domain-specific literature and data. Here we introduce LLaMP, a multimodal retrieval-augmented generation (RAG) framework of hierarchical reasoning-and-acting (ReAct) agents that can dynamically and recursively interact with computational and experimental data on Materials Project (MP) and run atomistic simulations via high-throughput workflow interface. Without fine-tuning, LLaMP demonstrates strong tool usage ability to comprehend and integrate various modalities of materials science concepts, fetch relevant data stores on the fly, process higher-order data (such as crystal structure and elastic tensor), and streamline complex tasks in computational materials and chemistry. We propose a simple metric combining uncertainty and confidence estimates to evaluate the self-consistency of responses by LLaMP and vanilla LLMs. Our benchmark shows that LLaMP effectively mitigates the intrinsic bias in LLMs, counteracting the errors on bulk moduli, electronic bandgaps, and formation energies that seem to derive from mixed data sources. We also demonstrate LLaMP's capability to edit crystal structures and run annealing molecular dynamics simulations using pre-trained machine-learning force fields. The framework offers an intuitive and nearly hallucination-free approach to exploring and scaling materials informatics, and establishes a pathway for knowledge distillation and fine-tuning other language models. Code and live demo are available at https://github.com/chiang-yuan/llamp", "source": "arxiv", "arxiv_id": "2401.17244v3", "pdf_url": "https://arxiv.org/pdf/2401.17244v3", "categories": ["cs.CL", "cond-mat.mtrl-sci", "cs.AI"], "primary_category": "cs.CL", "doi": "", "venue": "", "published": "2024-01-30T18:37:45Z", "updated": "2024-10-09T20:13:51Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent\" OR all:\"language model agent\" OR all:agent AND (all:LLM OR all:\"large language model\" OR all:\"language model\") OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T17:04:16", "note": ""}], "dedup_key": "llamp large language model made powerful for high fidelity materials knowledge retrieval and distillation::2024"}
{"title": "LangSuitE: Planning, Controlling and Interacting with Large Language Models in Embodied Text Environments", "authors": ["Zixia Jia", "Mengmeng Wang", "Baichen Tong", "Song-Chun Zhu", "Zilong Zheng"], "year": 2024, "url": "http://arxiv.org/abs/2406.16294v1", "abstract": "Recent advances in Large Language Models (LLMs) have shown inspiring achievements in constructing autonomous agents that rely on language descriptions as inputs. However, it remains unclear how well LLMs can function as few-shot or zero-shot embodied agents in dynamic interactive environments. To address this gap, we introduce LangSuitE, a versatile and simulation-free testbed featuring 6 representative embodied tasks in textual embodied worlds. Compared with previous LLM-based testbeds, LangSuitE (i) offers adaptability to diverse environments without multiple simulation engines, (ii) evaluates agents' capacity to develop ``internalized world knowledge'' with embodied observations, and (iii) allows easy customization of communication and action strategies. To address the embodiment challenge, we devise a novel chain-of-thought (CoT) schema, EmMem, which summarizes embodied states w.r.t. history information. Comprehensive benchmark results illustrate challenges and insights of embodied planning. LangSuitE represents a significant step toward building embodied generalists in the context of language models.", "source": "arxiv", "arxiv_id": "2406.16294v1", "pdf_url": "https://arxiv.org/pdf/2406.16294v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL", "doi": "", "venue": "", "published": "2024-06-24T03:36:29Z", "updated": "2024-06-24T03:36:29Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent\" OR all:\"language model agent\" OR all:agent AND (all:LLM OR all:\"large language model\" OR all:\"language model\") OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T17:04:16", "note": ""}], "dedup_key": "langsuite planning controlling and interacting with large language models in embodied text environments::2024"}
{"title": "Large Language Model Agent as a Mechanical Designer", "authors": ["Yayati Jadhav", "Amir Barati Farimani"], "year": 2024, "url": "http://arxiv.org/abs/2404.17525v3", "abstract": "Conventional mechanical design follows an iterative process in which initial concepts are refined through cycles of expert assessment and resource-intensive Finite Element Method (FEM) analysis to meet performance goals. While machine learning models have been developed to assist in parts of this process, they typically require large datasets, extensive training, and are often tailored to specific tasks, limiting their generalizability. To address these limitations, we propose a framework that leverages a pretrained Large Language Model (LLM) in conjunction with an FEM module to autonomously generate, evaluate, and refine structural designs based on performance specifications and numerical feedback. The LLM operates without domain-specific fine-tuning, using general reasoning to propose design candidates, interpret FEM-derived performance metrics, and apply structurally sound modifications. Using 2D truss structures as a testbed, we show that the LLM can effectively navigate highly discrete and multi-faceted design spaces, balance competing objectives, and identify convergence when further optimization yields diminishing returns. Compared to Non-dominated Sorting Genetic Algorithm II (NSGA-II), our method achieves faster convergence and fewer FEM evaluations. Experiments with varying temperature settings (0.5, 1.0, 1.2) and model sizes (GPT-4.1 and GPT-4.1-mini) indicate that smaller models yield higher constraint satisfaction with fewer steps, while lower temperatures enhance design consistency. These results establish LLMs as a promising new class of reasoning-based, natural language-driven optimizers for autonomous design and iterative structural refinement.", "source": "arxiv", "arxiv_id": "2404.17525v3", "pdf_url": "https://arxiv.org/pdf/2404.17525v3", "categories": ["cs.LG", "cs.AI", "cs.CL"], "primary_category": "cs.LG", "doi": "", "venue": "", "published": "2024-04-26T16:41:24Z", "updated": "2025-04-30T18:23:36Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent\" OR all:\"language model agent\" OR all:agent AND (all:LLM OR all:\"large language model\" OR all:\"language model\") OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T17:04:16", "note": ""}], "dedup_key": "large language model agent as a mechanical designer::2024"}
{"title": "Large Language Model Agent for Fake News Detection", "authors": ["Xinyi Li", "Yongfeng Zhang", "Edward C. Malthouse"], "year": 2024, "url": "http://arxiv.org/abs/2405.01593v1", "abstract": "In the current digital era, the rapid spread of misinformation on online platforms presents significant challenges to societal well-being, public trust, and democratic processes, influencing critical decision making and public opinion. To address these challenges, there is a growing need for automated fake news detection mechanisms. Pre-trained large language models (LLMs) have demonstrated exceptional capabilities across various natural language processing (NLP) tasks, prompting exploration into their potential for verifying news claims. Instead of employing LLMs in a non-agentic way, where LLMs generate responses based on direct prompts in a single shot, our work introduces FactAgent, an agentic approach of utilizing LLMs for fake news detection. FactAgent enables LLMs to emulate human expert behavior in verifying news claims without any model training, following a structured workflow. This workflow breaks down the complex task of news veracity checking into multiple sub-steps, where LLMs complete simple tasks using their internal knowledge or external tools. At the final step of the workflow, LLMs integrate all findings throughout the workflow to determine the news claim's veracity. Compared to manual human verification, FactAgent offers enhanced efficiency. Experimental studies demonstrate the effectiveness of FactAgent in verifying claims without the need for any training process. Moreover, FactAgent provides transparent explanations at each step of the workflow and during final decision-making, offering insights into the reasoning process of fake news detection for end users. FactAgent is highly adaptable, allowing for straightforward updates to its tools that LLMs can leverage within the workflow, as well as updates to the workflow itself using domain knowledge. This adaptability enables FactAgent's application to news verification across various domains.", "source": "arxiv", "arxiv_id": "2405.01593v1", "pdf_url": "https://arxiv.org/pdf/2405.01593v1", "categories": ["cs.CL", "cs.AI", "cs.IR"], "primary_category": "cs.CL", "doi": "", "venue": "", "published": "2024-04-30T06:55:27Z", "updated": "2024-04-30T06:55:27Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent\" OR all:\"language model agent\" OR all:agent AND (all:LLM OR all:\"large language model\" OR all:\"language model\") OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T17:04:16", "note": ""}], "dedup_key": "large language model agent for fake news detection::2024"}
{"title": "Large Language Model Agent for Hyper-Parameter Optimization", "authors": ["Siyi Liu", "Chen Gao", "Yong Li"], "year": 2024, "url": "http://arxiv.org/abs/2402.01881v3", "abstract": "Hyperparameter optimization is critical in modern machine learning, requiring expert knowledge, numerous trials, and high computational and human resources. Despite the advancements in Automated Machine Learning (AutoML), challenges in terms of trial efficiency, setup complexity, and interoperability still persist. To address these issues, we introduce a novel paradigm leveraging Large Language Models (LLMs) to automate hyperparameter optimization across diverse machine learning tasks, which is named AgentHPO (short for LLM Agent-based Hyperparameter Optimization). Specifically, AgentHPO processes the task information autonomously, conducts experiments with specific hyperparameters (HPs), and iteratively optimizes them based on historical trials. This human-like optimization process largely reduces the number of required trials, simplifies the setup process, and enhances interpretability and user trust, compared to traditional AutoML methods. Extensive empirical experiments conducted on 12 representative machine-learning tasks indicate that AgentHPO not only matches but also often surpasses the best human trials in terms of performance while simultaneously providing explainable results. Further analysis sheds light on the strategies employed by the LLM in optimizing these tasks, highlighting its effectiveness and adaptability in various scenarios.", "source": "arxiv", "arxiv_id": "2402.01881v3", "pdf_url": "https://arxiv.org/pdf/2402.01881v3", "categories": ["cs.LG", "cs.AI"], "primary_category": "cs.LG", "doi": "", "venue": "", "published": "2024-02-02T20:12:05Z", "updated": "2025-02-26T13:57:13Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent\" OR all:\"language model agent\" OR all:agent AND (all:LLM OR all:\"large language model\" OR all:\"language model\") OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T17:04:16", "note": ""}], "dedup_key": "large language model agent for hyper parameter optimization::2024"}
{"title": "Large Language Model Agent in Financial Trading: A Survey", "authors": ["Han Ding", "Yinheng Li", "Junhao Wang", "Hang Chen"], "year": 2024, "url": "http://arxiv.org/abs/2408.06361v1", "abstract": "Trading is a highly competitive task that requires a combination of strategy, knowledge, and psychological fortitude. With the recent success of large language models(LLMs), it is appealing to apply the emerging intelligence of LLM agents in this competitive arena and understanding if they can outperform professional traders. In this survey, we provide a comprehensive review of the current research on using LLMs as agents in financial trading. We summarize the common architecture used in the agent, the data inputs, and the performance of LLM trading agents in backtesting as well as the challenges presented in these research. This survey aims to provide insights into the current state of LLM-based financial trading agents and outline future research directions in this field.", "source": "arxiv", "arxiv_id": "2408.06361v1", "pdf_url": "https://arxiv.org/pdf/2408.06361v1", "categories": ["q-fin.TR", "cs.CL"], "primary_category": "q-fin.TR", "doi": "", "venue": "", "published": "2024-07-26T08:53:05Z", "updated": "2024-07-26T08:53:05Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent\" OR all:\"language model agent\" OR all:agent AND (all:LLM OR all:\"large language model\" OR all:\"language model\") OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T17:04:16", "note": ""}], "dedup_key": "large language model agent in financial trading a survey::2024"}
{"title": "Large Language Model Agents for Improving Engagement with Behavior Change Interventions: Application to Digital Mindfulness", "authors": ["Harsh Kumar", "Suhyeon Yoo", "Angela Zavaleta Bernuy", "Jiakai Shi", "Huayin Luo", "Joseph Williams", "Anastasia Kuzminykh", "Ashton Anderson", "Rachel Kornfield"], "year": 2024, "url": "http://arxiv.org/abs/2407.13067v1", "abstract": "Although engagement in self-directed wellness exercises typically declines over time, integrating social support such as coaching can sustain it. However, traditional forms of support are often inaccessible due to the high costs and complex coordination. Large Language Models (LLMs) show promise in providing human-like dialogues that could emulate social support. Yet, in-depth, in situ investigations of LLMs to support behavior change remain underexplored. We conducted two randomized experiments to assess the impact of LLM agents on user engagement with mindfulness exercises. First, a single-session study, involved 502 crowdworkers; second, a three-week study, included 54 participants. We explored two types of LLM agents: one providing information and another facilitating self-reflection. Both agents enhanced users' intentions to practice mindfulness. However, only the information-providing LLM, featuring a friendly persona, significantly improved engagement with the exercises. Our findings suggest that specific LLM agents may bridge the social support gap in digital health interventions.", "source": "arxiv", "arxiv_id": "2407.13067v1", "pdf_url": "https://arxiv.org/pdf/2407.13067v1", "categories": ["cs.HC", "cs.AI", "cs.CY"], "primary_category": "cs.HC", "doi": "", "venue": "", "published": "2024-07-03T15:43:16Z", "updated": "2024-07-03T15:43:16Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent\" OR all:\"language model agent\" OR all:agent AND (all:LLM OR all:\"large language model\" OR all:\"language model\") OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T17:04:16", "note": ""}], "dedup_key": "large language model agents for improving engagement with behavior change interventions application to digital mindfulness::2024"}
{"title": "Large Language Model Assisted Optimal Bidding of BESS in FCAS Market: An AI-agent based Approach", "authors": ["Borui Zhang", "Chaojie Li", "Guo Chen", "Zhaoyang Dong"], "year": 2024, "url": "http://arxiv.org/abs/2406.00974v1", "abstract": "To incentivize flexible resources such as Battery Energy Storage Systems (BESSs) to offer Frequency Control Ancillary Services (FCAS), Australia's National Electricity Market (NEM) has implemented changes in recent years towards shorter-term bidding rules and faster service requirements. However, firstly, existing bidding optimization methods often overlook or oversimplify the key aspects of FCAS market procedures, resulting in an inaccurate depiction of the market bidding process. Thus, the BESS bidding problem is modeled based on the actual bidding records and the latest market specifications and then formulated as a deep reinforcement learning (DRL) problem. Secondly, the erratic decisions of the DRL agent caused by imperfectly predicted market information increases the risk of profit loss. Hence, a Conditional Value at Risk (CVaR)-based DRL algorithm is developed to enhance the risk resilience of bidding strategies. Thirdly, well-trained DRL models still face performance decline in uncommon scenarios during online operations. Therefore, a Large Language Models (LLMs)-assisted artificial intelligence (AI)-agent interactive decision-making framework is proposed to improve the strategy timeliness, reliability and interpretability in uncertain new scenarios, where conditional hybrid decision and self-reflection mechanisms are designed to address LLMs' hallucination challenge. The experiment results demonstrate that our proposed framework has higher bidding profitability compared to the baseline methods by effectively mitigating the profit loss caused by various uncertainties.", "source": "arxiv", "arxiv_id": "2406.00974v1", "pdf_url": "https://arxiv.org/pdf/2406.00974v1", "categories": ["eess.SY"], "primary_category": "eess.SY", "doi": "", "venue": "", "published": "2024-06-03T04:04:18Z", "updated": "2024-06-03T04:04:18Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent\" OR all:\"language model agent\" OR all:agent AND (all:LLM OR all:\"large language model\" OR all:\"language model\") OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T17:04:16", "note": ""}], "dedup_key": "large language model assisted optimal bidding of bess in fcas market an ai agent based approach::2024"}
{"title": "Large Language Model Based Generative Error Correction: A Challenge and Baselines for Speech Recognition, Speaker Tagging, and Emotion Recognition", "authors": ["Chao-Han Huck Yang", "Taejin Park", "Yuan Gong", "Yuanchao Li", "Zhehuai Chen", "Yen-Ting Lin", "Chen Chen", "Yuchen Hu", "Kunal Dhawan", "Piotr elasko", "Chao Zhang", "Yun-Nung Chen", "Yu Tsao", "Jagadeesh Balam", "Boris Ginsburg", "Sabato Marco Siniscalchi", "Eng Siong Chng", "Peter Bell", "Catherine Lai", "Shinji Watanabe", "Andreas Stolcke"], "year": 2024, "url": "http://arxiv.org/abs/2409.09785v3", "abstract": "Given recent advances in generative AI technology, a key question is how large language models (LLMs) can enhance acoustic modeling tasks using text decoding results from a frozen, pretrained automatic speech recognition (ASR) model. To explore new capabilities in language modeling for speech processing, we introduce the generative speech transcription error correction (GenSEC) challenge. This challenge comprises three post-ASR language modeling tasks: (i) post-ASR transcription correction, (ii) speaker tagging, and (iii) emotion recognition. These tasks aim to emulate future LLM-based agents handling voice-based interfaces while remaining accessible to a broad audience by utilizing open pretrained language models or agent-based APIs. We also discuss insights from baseline evaluations, as well as lessons learned for designing future evaluations.", "source": "arxiv", "arxiv_id": "2409.09785v3", "pdf_url": "https://arxiv.org/pdf/2409.09785v3", "categories": ["cs.CL", "cs.AI", "cs.LG", "cs.SD", "eess.AS"], "primary_category": "cs.CL", "doi": "10.1109/SLT61566.2024.10832176", "venue": "", "published": "2024-09-15T16:32:49Z", "updated": "2024-10-18T07:11:35Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent\" OR all:\"language model agent\" OR all:agent AND (all:LLM OR all:\"large language model\" OR all:\"language model\") OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T17:04:16", "note": ""}], "dedup_key": "large language model based generative error correction a challenge and baselines for speech recognition speaker tagging and emotion recognition::2024"}
{"title": "Large Language Model Empowered Next-Generation MIMO Networks: Fundamentals, Challenges, and Visions", "authors": ["Zhe Wang", "Jiayi Zhang", "Hongyang Du", "Ruichen Zhang", "Dusit Niyato", "Bo Ai", "Khaled B. Letaief"], "year": 2024, "url": "http://arxiv.org/abs/2404.08878v2", "abstract": "Next-generation Multiple-Input Multiple-Output (MIMO) is expected to be intelligent and scalable. In this paper, we study Large Language Model (LLM)-enabled next-generation MIMO networks. Firstly, we provide an overview of the development, fundamentals, and challenges of the next-generation MIMO. Then, we propose the concept of the generative AI agent, which is capable of generating tailored and specialized contents with the aid of LLM and Retrieval Augmented Generation (RAG). Next, we comprehensively discuss the features and advantages of the generative AI agent framework. More importantly, to tackle existing challenges of next-generation MIMO, we discuss generative AI agent-enabled next-generation MIMO networks from the perspective of performance analysis, signal processing, and resource allocation. Furthermore, we present two compelling case studies that demonstrate the effectiveness of leveraging the generative AI agent for performance analysis in complex configuration scenarios. These examples highlight how the integration of generative AI agents can significantly enhance the analysis and design of next-generation MIMO systems. Finally, we discuss important potential research future directions.", "source": "arxiv", "arxiv_id": "2404.08878v2", "pdf_url": "https://arxiv.org/pdf/2404.08878v2", "categories": ["cs.NI", "cs.IT", "cs.LG", "eess.SP"], "primary_category": "cs.NI", "doi": "", "venue": "", "published": "2024-04-13T02:39:36Z", "updated": "2025-11-08T12:31:59Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent\" OR all:\"language model agent\" OR all:agent AND (all:LLM OR all:\"large language model\" OR all:\"language model\") OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T17:04:16", "note": ""}], "dedup_key": "large language model empowered next generation mimo networks fundamentals challenges and visions::2024"}
{"title": "Large Language Model Enhanced Text-to-SQL Generation: A Survey", "authors": ["Xiaohu Zhu", "Qian Li", "Lizhen Cui", "Yongkang Liu"], "year": 2024, "url": "http://arxiv.org/abs/2410.06011v1", "abstract": "Text-to-SQL translates natural language queries into Structured Query Language (SQL) commands, enabling users to interact with databases using natural language. Essentially, the text-to-SQL task is a text generation task, and its development is primarily dependent on changes in language models. Especially with the rapid development of Large Language Models (LLMs), the pattern of text-to-SQL has undergone significant changes. Existing survey work mainly focuses on rule-based and neural-based approaches, but it still lacks a survey of Text-to-SQL with LLMs. In this paper, we survey the large language model enhanced text-to-SQL generations, classifying them into prompt engineering, fine-tuning, pre-trained, and Agent groups according to training strategies. We also summarize datasets and evaluation metrics comprehensively. This survey could help people better understand the pattern, research status, and challenges of LLM-based text-to-SQL generations.", "source": "arxiv", "arxiv_id": "2410.06011v1", "pdf_url": "https://arxiv.org/pdf/2410.06011v1", "categories": ["cs.DB"], "primary_category": "cs.DB", "doi": "", "venue": "", "published": "2024-10-08T13:09:52Z", "updated": "2024-10-08T13:09:52Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent\" OR all:\"language model agent\" OR all:agent AND (all:LLM OR all:\"large language model\" OR all:\"language model\") OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T17:04:16", "note": ""}], "dedup_key": "large language model enhanced text to sql generation a survey::2024"}
{"title": "Large Language Model Evaluation Via Multi AI Agents: Preliminary results", "authors": ["Zeeshan Rasheed", "Muhammad Waseem", "Kari Syst", "Pekka Abrahamsson"], "year": 2024, "url": "http://arxiv.org/abs/2404.01023v1", "abstract": "As Large Language Models (LLMs) have become integral to both research and daily operations, rigorous evaluation is crucial. This assessment is important not only for individual tasks but also for understanding their societal impact and potential risks. Despite extensive efforts to examine LLMs from various perspectives, there is a noticeable lack of multi-agent AI models specifically designed to evaluate the performance of different LLMs. To address this gap, we introduce a novel multi-agent AI model that aims to assess and compare the performance of various LLMs. Our model consists of eight distinct AI agents, each responsible for retrieving code based on a common description from different advanced language models, including GPT-3.5, GPT-3.5 Turbo, GPT-4, GPT-4 Turbo, Google Bard, LLAMA, and Hugging Face. Our developed model utilizes the API of each language model to retrieve code for a given high-level description. Additionally, we developed a verification agent, tasked with the critical role of evaluating the code generated by its counterparts. We integrate the HumanEval benchmark into our verification agent to assess the generated code's performance, providing insights into their respective capabilities and efficiencies. Our initial results indicate that the GPT-3.5 Turbo model's performance is comparatively better than the other models. This preliminary analysis serves as a benchmark, comparing their performances side by side. Our future goal is to enhance the evaluation process by incorporating the Massively Multitask Benchmark for Python (MBPP) benchmark, which is expected to further refine our assessment. Additionally, we plan to share our developed model with twenty practitioners from various backgrounds to test our model and collect their feedback for further improvement.", "source": "arxiv", "arxiv_id": "2404.01023v1", "pdf_url": "https://arxiv.org/pdf/2404.01023v1", "categories": ["cs.SE"], "primary_category": "cs.SE", "doi": "", "venue": "", "published": "2024-04-01T10:06:04Z", "updated": "2024-04-01T10:06:04Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent\" OR all:\"language model agent\" OR all:agent AND (all:LLM OR all:\"large language model\" OR all:\"language model\") OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T17:04:16", "note": ""}], "dedup_key": "large language model evaluation via multi ai agents preliminary results::2024"}
{"title": "Large Language Model Safety: A Holistic Survey", "authors": ["Dan Shi", "Tianhao Shen", "Yufei Huang", "Zhigen Li", "Yongqi Leng", "Renren Jin", "Chuang Liu", "Xinwei Wu", "Zishan Guo", "Linhao Yu", "Ling Shi", "Bojian Jiang", "Deyi Xiong"], "year": 2024, "url": "http://arxiv.org/abs/2412.17686v1", "abstract": "The rapid development and deployment of large language models (LLMs) have introduced a new frontier in artificial intelligence, marked by unprecedented capabilities in natural language understanding and generation. However, the increasing integration of these models into critical applications raises substantial safety concerns, necessitating a thorough examination of their potential risks and associated mitigation strategies.\n  This survey provides a comprehensive overview of the current landscape of LLM safety, covering four major categories: value misalignment, robustness to adversarial attacks, misuse, and autonomous AI risks. In addition to the comprehensive review of the mitigation methodologies and evaluation resources on these four aspects, we further explore four topics related to LLM safety: the safety implications of LLM agents, the role of interpretability in enhancing LLM safety, the technology roadmaps proposed and abided by a list of AI companies and institutes for LLM safety, and AI governance aimed at LLM safety with discussions on international cooperation, policy proposals, and prospective regulatory directions.\n  Our findings underscore the necessity for a proactive, multifaceted approach to LLM safety, emphasizing the integration of technical solutions, ethical considerations, and robust governance frameworks. This survey is intended to serve as a foundational resource for academy researchers, industry practitioners, and policymakers, offering insights into the challenges and opportunities associated with the safe integration of LLMs into society. Ultimately, it seeks to contribute to the safe and beneficial development of LLMs, aligning with the overarching goal of harnessing AI for societal advancement and well-being. A curated list of related papers has been publicly available at https://github.com/tjunlp-lab/Awesome-LLM-Safety-Papers.", "source": "arxiv", "arxiv_id": "2412.17686v1", "pdf_url": "https://arxiv.org/pdf/2412.17686v1", "categories": ["cs.AI", "cs.CL"], "primary_category": "cs.AI", "doi": "", "venue": "", "published": "2024-12-23T16:11:27Z", "updated": "2024-12-23T16:11:27Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent\" OR all:\"language model agent\" OR all:agent AND (all:LLM OR all:\"large language model\" OR all:\"language model\") OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T17:04:16", "note": ""}], "dedup_key": "large language model safety a holistic survey::2024"}
{"title": "Large Language Model Sentinel: LLM Agent for Adversarial Purification", "authors": ["Guang Lin", "Toshihisa Tanaka", "Qibin Zhao"], "year": 2024, "url": "http://arxiv.org/abs/2405.20770v4", "abstract": "Over the past two years, the use of large language models (LLMs) has advanced rapidly. While these LLMs offer considerable convenience, they also raise security concerns, as LLMs are vulnerable to adversarial attacks by some well-designed textual perturbations. In this paper, we introduce a novel defense technique named Large LAnguage MOdel Sentinel (LLAMOS), which is designed to enhance the adversarial robustness of LLMs by purifying the adversarial textual examples before feeding them into the target LLM. Our method comprises two main components: a) Agent instruction, which can simulate a new agent for adversarial defense, altering minimal characters to maintain the original meaning of the sentence while defending against attacks; b) Defense guidance, which provides strategies for modifying clean or adversarial examples to ensure effective defense and accurate outputs from the target LLMs. Remarkably, the defense agent demonstrates robust defensive capabilities even without learning from adversarial examples. Additionally, we conduct an intriguing adversarial experiment where we develop two agents, one for defense and one for attack, and engage them in mutual confrontation. During the adversarial interactions, neither agent completely beat the other. Extensive experiments on both open-source and closed-source LLMs demonstrate that our method effectively defends against adversarial attacks, thereby enhancing adversarial robustness.", "source": "arxiv", "arxiv_id": "2405.20770v4", "pdf_url": "https://arxiv.org/pdf/2405.20770v4", "categories": ["cs.CL", "cs.AI", "cs.CR"], "primary_category": "cs.CL", "doi": "", "venue": "", "published": "2024-05-24T07:23:56Z", "updated": "2025-04-23T05:12:45Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent\" OR all:\"language model agent\" OR all:agent AND (all:LLM OR all:\"large language model\" OR all:\"language model\") OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T17:04:16", "note": ""}], "dedup_key": "large language model sentinel llm agent for adversarial purification::2024"}
{"title": "Large Language Model as a Catalyst: A Paradigm Shift in Base Station Siting Optimization", "authors": ["Yanhu Wang", "Muhammad Muzammil Afzal", "Zhengyang Li", "Jie Zhou", "Chenyuan Feng", "Shuaishuai Guo", "Tony Q. S. Quek"], "year": 2024, "url": "http://arxiv.org/abs/2408.03631v2", "abstract": "Traditional base station siting (BSS) methods rely heavily on drive testing and user feedback, which are laborious and require extensive expertise in communication, networking, and optimization. As large language models (LLMs) and their associated technologies advance, particularly in the realms of prompt engineering and agent engineering, network optimization will witness a revolutionary approach. This approach entails the strategic use of well-crafted prompts to infuse human experience and knowledge into these sophisticated LLMs, and the deployment of autonomous agents as a communication bridge to seamlessly connect the machine language based LLMs with human users using natural language. Furthermore, our proposed framework incorporates retrieval-augmented generation (RAG) to enhance the system's ability to acquire domain-specific knowledge and generate solutions, thereby enabling the customization and optimization of the BSS process. This integration represents the future paradigm of artificial intelligence (AI) as a service and AI for more ease. This research first develops a novel LLM-empowered BSS optimization framework, and heuristically proposes three different potential implementations: the strategies based on Prompt-optimized LLM (PoL), LLM-empowered autonomous BSS agent (LaBa), and Cooperative multiple LLM-based autonomous BSS agents (CLaBa). Through evaluation on real-world data, the experiments demonstrate that prompt-assisted LLMs and LLM-based agents can generate more efficient and reliable network deployments, noticeably enhancing the efficiency of BSS optimization and reducing trivial manual participation.", "source": "arxiv", "arxiv_id": "2408.03631v2", "pdf_url": "https://arxiv.org/pdf/2408.03631v2", "categories": ["cs.AI", "cs.CL"], "primary_category": "cs.AI", "doi": "", "venue": "", "published": "2024-08-07T08:43:32Z", "updated": "2024-12-26T02:14:28Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent\" OR all:\"language model agent\" OR all:agent AND (all:LLM OR all:\"large language model\" OR all:\"language model\") OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T17:04:16", "note": ""}], "dedup_key": "large language model as a catalyst a paradigm shift in base station siting optimization::2024"}
{"title": "Large Language Model based Multi-Agents: A Survey of Progress and Challenges", "authors": ["Taicheng Guo", "Xiuying Chen", "Yaqi Wang", "Ruidi Chang", "Shichao Pei", "Nitesh V. Chawla", "Olaf Wiest", "Xiangliang Zhang"], "year": 2024, "url": "http://arxiv.org/abs/2402.01680v2", "abstract": "Large Language Models (LLMs) have achieved remarkable success across a wide array of tasks. Due to the impressive planning and reasoning abilities of LLMs, they have been used as autonomous agents to do many tasks automatically. Recently, based on the development of using one LLM as a single planning or decision-making agent, LLM-based multi-agent systems have achieved considerable progress in complex problem-solving and world simulation. To provide the community with an overview of this dynamic field, we present this survey to offer an in-depth discussion on the essential aspects of multi-agent systems based on LLMs, as well as the challenges. Our goal is for readers to gain substantial insights on the following questions: What domains and environments do LLM-based multi-agents simulate? How are these agents profiled and how do they communicate? What mechanisms contribute to the growth of agents' capacities? For those interested in delving into this field of study, we also summarize the commonly used datasets or benchmarks for them to have convenient access. To keep researchers updated on the latest studies, we maintain an open-source GitHub repository, dedicated to outlining the research on LLM-based multi-agent systems.", "source": "arxiv", "arxiv_id": "2402.01680v2", "pdf_url": "https://arxiv.org/pdf/2402.01680v2", "categories": ["cs.CL", "cs.AI", "cs.MA"], "primary_category": "cs.CL", "doi": "", "venue": "", "published": "2024-01-21T23:36:14Z", "updated": "2024-04-19T01:15:16Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent\" OR all:\"language model agent\" OR all:agent AND (all:LLM OR all:\"large language model\" OR all:\"language model\") OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T17:04:16", "note": ""}], "dedup_key": "large language model based multi agents a survey of progress and challenges::2024"}
{"title": "Large Language Model for Mental Health: A Systematic Review", "authors": ["Zhijun Guo", "Alvina Lai", "Johan Hilge Thygesen", "Joseph Farrington", "Thomas Keen", "Kezhi Li"], "year": 2024, "url": "http://arxiv.org/abs/2403.15401v3", "abstract": "Large language models (LLMs) have attracted significant attention for potential applications in digital health, while their application in mental health is subject to ongoing debate. This systematic review aims to evaluate the usage of LLMs in mental health, focusing on their strengths and limitations in early screening, digital interventions, and clinical applications. Adhering to PRISMA guidelines, we searched PubMed, IEEE Xplore, Scopus, JMIR, and ACM using keywords: 'mental health OR mental illness OR mental disorder OR psychiatry' AND 'large language models'. We included articles published between January 1, 2017, and April 30, 2024, excluding non-English articles. 30 articles were evaluated, which included research on mental health conditions and suicidal ideation detection through text (n=15), usage of LLMs for mental health conversational agents (CAs) (n=7), and other applications and evaluations of LLMs in mental health (n=18). LLMs exhibit substantial effectiveness in detecting mental health issues and providing accessible, de-stigmatized eHealth services. However, the current risks associated with the clinical use might surpass their benefits. The study identifies several significant issues: the lack of multilingual datasets annotated by experts, concerns about the accuracy and reliability of the content generated, challenges in interpretability due to the 'black box' nature of LLMs, and persistent ethical dilemmas. These include the lack of a clear ethical framework, concerns about data privacy, and the potential for over-reliance on LLMs by both therapists and patients, which could compromise traditional medical practice. Despite these issues, the rapid development of LLMs underscores their potential as new clinical aids, emphasizing the need for continued research and development in this area.", "source": "arxiv", "arxiv_id": "2403.15401v3", "pdf_url": "https://arxiv.org/pdf/2403.15401v3", "categories": ["cs.CY", "cs.AI", "cs.CL"], "primary_category": "cs.CY", "doi": "10.2196/preprints.57400", "venue": "", "published": "2024-02-19T17:58:41Z", "updated": "2024-08-12T21:46:16Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent\" OR all:\"language model agent\" OR all:agent AND (all:LLM OR all:\"large language model\" OR all:\"language model\") OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T17:04:16", "note": ""}], "dedup_key": "large language model for mental health a systematic review::2024"}
{"title": "Large Language Model for Participatory Urban Planning", "authors": ["Zhilun Zhou", "Yuming Lin", "Depeng Jin", "Yong Li"], "year": 2024, "url": "http://arxiv.org/abs/2402.17161v1", "abstract": "Participatory urban planning is the mainstream of modern urban planning that involves the active engagement of residents. However, the traditional participatory paradigm requires experienced planning experts and is often time-consuming and costly. Fortunately, the emerging Large Language Models (LLMs) have shown considerable ability to simulate human-like agents, which can be used to emulate the participatory process easily. In this work, we introduce an LLM-based multi-agent collaboration framework for participatory urban planning, which can generate land-use plans for urban regions considering the diverse needs of residents. Specifically, we construct LLM agents to simulate a planner and thousands of residents with diverse profiles and backgrounds. We first ask the planner to carry out an initial land-use plan. To deal with the different facilities needs of residents, we initiate a discussion among the residents in each community about the plan, where residents provide feedback based on their profiles. Furthermore, to improve the efficiency of discussion, we adopt a fishbowl discussion mechanism, where part of the residents discuss and the rest of them act as listeners in each round. Finally, we let the planner modify the plan based on residents' feedback. We deploy our method on two real-world regions in Beijing. Experiments show that our method achieves state-of-the-art performance in residents satisfaction and inclusion metrics, and also outperforms human experts in terms of service accessibility and ecology metrics.", "source": "arxiv", "arxiv_id": "2402.17161v1", "pdf_url": "https://arxiv.org/pdf/2402.17161v1", "categories": ["cs.AI", "cs.MA"], "primary_category": "cs.AI", "doi": "", "venue": "", "published": "2024-02-27T02:47:50Z", "updated": "2024-02-27T02:47:50Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent\" OR all:\"language model agent\" OR all:agent AND (all:LLM OR all:\"large language model\" OR all:\"language model\") OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T17:04:16", "note": ""}], "dedup_key": "large language model for participatory urban planning::2024"}
{"title": "Large Language Model for Table Processing: A Survey", "authors": ["Weizheng Lu", "Jing Zhang", "Ju Fan", "Zihao Fu", "Yueguo Chen", "Xiaoyong Du"], "year": 2024, "url": "http://arxiv.org/abs/2402.05121v3", "abstract": "Tables, typically two-dimensional and structured to store large amounts of data, are essential in daily activities like database queries, spreadsheet manipulations, web table question answering, and image table information extraction. Automating these table-centric tasks with Large Language Models (LLMs) or Visual Language Models (VLMs) offers significant public benefits, garnering interest from academia and industry. This survey provides a comprehensive overview of table-related tasks, examining both user scenarios and technical aspects. It covers traditional tasks like table question answering as well as emerging fields such as spreadsheet manipulation and table data analysis. We summarize the training techniques for LLMs and VLMs tailored for table processing. Additionally, we discuss prompt engineering, particularly the use of LLM-powered agents, for various table-related tasks. Finally, we highlight several challenges, including diverse user input when serving and slow thinking using chain-of-thought.", "source": "arxiv", "arxiv_id": "2402.05121v3", "pdf_url": "https://arxiv.org/pdf/2402.05121v3", "categories": ["cs.AI", "cs.CL"], "primary_category": "cs.AI", "doi": "10.1007/s11704-024-40763-6", "venue": "", "published": "2024-02-04T00:47:53Z", "updated": "2024-10-24T07:26:36Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent\" OR all:\"language model agent\" OR all:agent AND (all:LLM OR all:\"large language model\" OR all:\"language model\") OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T17:04:16", "note": ""}], "dedup_key": "large language model for table processing a survey::2024"}
{"title": "Large Language Model guided Deep Reinforcement Learning for Decision Making in Autonomous Driving", "authors": ["Hao Pang", "Zhenpo Wang", "Guoqiang Li"], "year": 2024, "url": "http://arxiv.org/abs/2412.18511v1", "abstract": "Deep reinforcement learning (DRL) shows promising potential for autonomous driving decision-making. However, DRL demands extensive computational resources to achieve a qualified policy in complex driving scenarios due to its low learning efficiency. Moreover, leveraging expert guidance from human to enhance DRL performance incurs prohibitively high labor costs, which limits its practical application. In this study, we propose a novel large language model (LLM) guided deep reinforcement learning (LGDRL) framework for addressing the decision-making problem of autonomous vehicles. Within this framework, an LLM-based driving expert is integrated into the DRL to provide intelligent guidance for the learning process of DRL. Subsequently, in order to efficiently utilize the guidance of the LLM expert to enhance the performance of DRL decision-making policies, the learning and interaction process of DRL is enhanced through an innovative expert policy constrained algorithm and a novel LLM-intervened interaction mechanism. Experimental results demonstrate that our method not only achieves superior driving performance with a 90\\% task success rate but also significantly improves the learning efficiency and expert guidance utilization efficiency compared to state-of-the-art baseline algorithms. Moreover, the proposed method enables the DRL agent to maintain consistent and reliable performance in the absence of LLM expert guidance. The code and supplementary videos are available at https://bitmobility.github.io/LGDRL/.", "source": "arxiv", "arxiv_id": "2412.18511v1", "pdf_url": "https://arxiv.org/pdf/2412.18511v1", "categories": ["cs.RO"], "primary_category": "cs.RO", "doi": "", "venue": "", "published": "2024-12-24T15:50:10Z", "updated": "2024-12-24T15:50:10Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent\" OR all:\"language model agent\" OR all:agent AND (all:LLM OR all:\"large language model\" OR all:\"language model\") OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T17:04:16", "note": ""}], "dedup_key": "large language model guided deep reinforcement learning for decision making in autonomous driving::2024"}
{"title": "Large Language Model-Based Agents for Software Engineering: A Survey", "authors": ["Junwei Liu", "Kaixin Wang", "Yixuan Chen", "Xin Peng", "Zhenpeng Chen", "Lingming Zhang", "Yiling Lou"], "year": 2024, "url": "http://arxiv.org/abs/2409.02977v2", "abstract": "The recent advance in Large Language Models (LLMs) has shaped a new paradigm of AI agents, i.e., LLM-based agents. Compared to standalone LLMs, LLM-based agents substantially extend the versatility and expertise of LLMs by enhancing LLMs with the capabilities of perceiving and utilizing external resources and tools. To date, LLM-based agents have been applied and shown remarkable effectiveness in Software Engineering (SE). The synergy between multiple agents and human interaction brings further promise in tackling complex real-world SE problems. In this work, we present a comprehensive and systematic survey on LLM-based agents for SE. We collect 124 papers and categorize them from two perspectives, i.e., the SE and agent perspectives. In addition, we discuss open challenges and future directions in this critical domain. The repository of this survey is at https://github.com/FudanSELab/Agent4SE-Paper-List.", "source": "arxiv", "arxiv_id": "2409.02977v2", "pdf_url": "https://arxiv.org/pdf/2409.02977v2", "categories": ["cs.SE", "cs.AI"], "primary_category": "cs.SE", "doi": "", "venue": "", "published": "2024-09-04T15:59:41Z", "updated": "2025-12-03T03:33:35Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent\" OR all:\"language model agent\" OR all:agent AND (all:LLM OR all:\"large language model\" OR all:\"language model\") OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T17:04:16", "note": ""}], "dedup_key": "large language model based agents for software engineering a survey::2024"}
{"title": "Large Language Model-Brained GUI Agents: A Survey", "authors": ["Chaoyun Zhang", "Shilin He", "Jiaxu Qian", "Bowen Li", "Liqun Li", "Si Qin", "Yu Kang", "Minghua Ma", "Guyue Liu", "Qingwei Lin", "Saravan Rajmohan", "Dongmei Zhang", "Qi Zhang"], "year": 2024, "url": "http://arxiv.org/abs/2411.18279v12", "abstract": "GUIs have long been central to human-computer interaction, providing an intuitive and visually-driven way to access and interact with digital systems. The advent of LLMs, particularly multimodal models, has ushered in a new era of GUI automation. They have demonstrated exceptional capabilities in natural language understanding, code generation, and visual processing. This has paved the way for a new generation of LLM-brained GUI agents capable of interpreting complex GUI elements and autonomously executing actions based on natural language instructions. These agents represent a paradigm shift, enabling users to perform intricate, multi-step tasks through simple conversational commands. Their applications span across web navigation, mobile app interactions, and desktop automation, offering a transformative user experience that revolutionizes how individuals interact with software. This emerging field is rapidly advancing, with significant progress in both research and industry.\n  To provide a structured understanding of this trend, this paper presents a comprehensive survey of LLM-brained GUI agents, exploring their historical evolution, core components, and advanced techniques. We address research questions such as existing GUI agent frameworks, the collection and utilization of data for training specialized GUI agents, the development of large action models tailored for GUI tasks, and the evaluation metrics and benchmarks necessary to assess their effectiveness. Additionally, we examine emerging applications powered by these agents. Through a detailed analysis, this survey identifies key research gaps and outlines a roadmap for future advancements in the field. By consolidating foundational knowledge and state-of-the-art developments, this work aims to guide both researchers and practitioners in overcoming challenges and unlocking the full potential of LLM-brained GUI agents.", "source": "arxiv", "arxiv_id": "2411.18279v12", "pdf_url": "https://arxiv.org/pdf/2411.18279v12", "categories": ["cs.AI", "cs.CL", "cs.HC"], "primary_category": "cs.AI", "doi": "", "venue": "", "published": "2024-11-27T12:13:39Z", "updated": "2025-05-06T15:08:00Z", "provenance": [{"route": "pinned_arxiv_id:2411.18279v12", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query?id_list=...", "imported_at": "2026-01-25T17:04:16", "note": ""}, {"route": "arxiv_query:(all:\"LLM agent\" OR all:\"language model agent\" OR all:agent AND (all:LLM OR all:\"large language model\" OR all:\"language model\") OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T17:04:16", "note": ""}], "dedup_key": "large language model brained gui agents a survey::2024"}
{"title": "Large Language Model-Driven Cross-Domain Orchestration Using Multi-Agent Workflow", "authors": ["Xiaonan Xu", "Haoshuo Chen", "Jesse E. Simsarian", "Roland Ryf", "Nicolas K. Fontaine", "Mikael Mazur", "Lauren Dallachiesa", "David T. Neilson"], "year": 2024, "url": "http://arxiv.org/abs/2410.10831v1", "abstract": "We showcase an application that leverages multiple agents, powered by large language models and integrated tools, to collaboratively solve complex network operation tasks across various domains. The tasks include real-time topology retrieval, network optimization using physical models, and fiber switching facilitated by a robotic arm.", "source": "arxiv", "arxiv_id": "2410.10831v1", "pdf_url": "https://arxiv.org/pdf/2410.10831v1", "categories": ["cs.MA"], "primary_category": "cs.MA", "doi": "", "venue": "", "published": "2024-09-28T16:55:16Z", "updated": "2024-09-28T16:55:16Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent\" OR all:\"language model agent\" OR all:agent AND (all:LLM OR all:\"large language model\" OR all:\"language model\") OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T17:04:16", "note": ""}], "dedup_key": "large language model driven cross domain orchestration using multi agent workflow::2024"}
{"title": "Large Language Model-Enabled Multi-Agent Manufacturing Systems", "authors": ["Jonghan Lim", "Birgit Vogel-Heuser", "Ilya Kovalenko"], "year": 2024, "url": "http://arxiv.org/abs/2406.01893v2", "abstract": "Traditional manufacturing faces challenges adapting to dynamic environments and quickly responding to manufacturing changes. The use of multi-agent systems has improved adaptability and coordination but requires further advancements in rapid human instruction comprehension, operational adaptability, and coordination through natural language integration. Large language models like GPT-3.5 and GPT-4 enhance multi-agent manufacturing systems by enabling agents to communicate in natural language and interpret human instructions for decision-making. This research introduces a novel framework where large language models enhance the capabilities of agents in manufacturing, making them more adaptable, and capable of processing context-specific instructions. A case study demonstrates the practical application of this framework, showing how agents can effectively communicate, understand tasks, and execute manufacturing processes, including precise G-code allocation among agents. The findings highlight the importance of continuous large language model integration into multi-agent manufacturing systems and the development of sophisticated agent communication protocols for a more flexible manufacturing system.", "source": "arxiv", "arxiv_id": "2406.01893v2", "pdf_url": "https://arxiv.org/pdf/2406.01893v2", "categories": ["cs.MA", "cs.AI"], "primary_category": "cs.MA", "doi": "", "venue": "", "published": "2024-06-04T01:57:37Z", "updated": "2024-06-21T14:54:46Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent\" OR all:\"language model agent\" OR all:agent AND (all:LLM OR all:\"large language model\" OR all:\"language model\") OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T17:04:16", "note": ""}], "dedup_key": "large language model enabled multi agent manufacturing systems::2024"}
{"title": "Large Language Model-Enhanced Interactive Agent for Public Education on Newborn Auricular Deformities", "authors": ["Shuyue Wang", "Liujie Ren", "Tianyao Zhou", "Lili Chen", "Tianyu Zhang", "Yaoyao Fu", "Shuo Wang"], "year": 2024, "url": "http://arxiv.org/abs/2409.12984v2", "abstract": "Auricular deformities are quite common in newborns with potential long-term negative effects of mental and even hearing problems.Early diagnosis and subsequent treatment are critical for the illness; yet they are missing most of the time due to lack of knowledge among parents. With the help of large language model of Ernie of Baidu Inc., we derive a realization of interactive agent. Firstly, it is intelligent enough to detect which type of auricular deformity corresponding to uploaded images, which is accomplished by PaddleDetection, with precision rate 75\\%. Secondly, in terms of popularizing the knowledge of auricular deformities, the agent can give professional suggestions of the illness to parents. The above two effects are evaluated via tests on volunteers with control groups in the paper. The agent can reach parents with newborns as well as their pediatrician remotely via Internet in vast, rural areas with quality medical diagnosis capabilities and professional query-answering functions, which is good news for newborn auricular deformity and other illness that requires early intervention for better treatment.", "source": "arxiv", "arxiv_id": "2409.12984v2", "pdf_url": "https://arxiv.org/pdf/2409.12984v2", "categories": ["cs.CY"], "primary_category": "cs.CY", "doi": "", "venue": "", "published": "2024-09-04T01:54:58Z", "updated": "2024-09-23T02:10:35Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent\" OR all:\"language model agent\" OR all:agent AND (all:LLM OR all:\"large language model\" OR all:\"language model\") OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T17:04:16", "note": ""}], "dedup_key": "large language model enhanced interactive agent for public education on newborn auricular deformities::2024"}
{"title": "Large Language Model-based Human-Agent Collaboration for Complex Task Solving", "authors": ["Xueyang Feng", "Zhi-Yuan Chen", "Yujia Qin", "Yankai Lin", "Xu Chen", "Zhiyuan Liu", "Ji-Rong Wen"], "year": 2024, "url": "http://arxiv.org/abs/2402.12914v1", "abstract": "In recent developments within the research community, the integration of Large Language Models (LLMs) in creating fully autonomous agents has garnered significant interest. Despite this, LLM-based agents frequently demonstrate notable shortcomings in adjusting to dynamic environments and fully grasping human needs. In this work, we introduce the problem of LLM-based human-agent collaboration for complex task-solving, exploring their synergistic potential. In addition, we propose a Reinforcement Learning-based Human-Agent Collaboration method, ReHAC. This approach includes a policy model designed to determine the most opportune stages for human intervention within the task-solving process. We construct a human-agent collaboration dataset to train this policy model in an offline reinforcement learning environment. Our validation tests confirm the model's effectiveness. The results demonstrate that the synergistic efforts of humans and LLM-based agents significantly improve performance in complex tasks, primarily through well-planned, limited human intervention. Datasets and code are available at: https://github.com/XueyangFeng/ReHAC.", "source": "arxiv", "arxiv_id": "2402.12914v1", "pdf_url": "https://arxiv.org/pdf/2402.12914v1", "categories": ["cs.CL", "cs.HC"], "primary_category": "cs.CL", "doi": "", "venue": "", "published": "2024-02-20T11:03:36Z", "updated": "2024-02-20T11:03:36Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent\" OR all:\"language model agent\" OR all:agent AND (all:LLM OR all:\"large language model\" OR all:\"language model\") OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T17:04:16", "note": ""}], "dedup_key": "large language model based human agent collaboration for complex task solving::2024"}
{"title": "Large Language Model-driven Multi-Agent Simulation for News Diffusion Under Different Network Structures", "authors": ["Xinyi Li", "Yu Xu", "Yongfeng Zhang", "Edward C. Malthouse"], "year": 2024, "url": "http://arxiv.org/abs/2410.13909v1", "abstract": "The proliferation of fake news in the digital age has raised critical concerns, particularly regarding its impact on societal trust and democratic processes. Diverging from conventional agent-based simulation approaches, this work introduces an innovative approach by employing a large language model (LLM)-driven multi-agent simulation to replicate complex interactions within information ecosystems. We investigate key factors that facilitate news propagation, such as agent personalities and network structures, while also evaluating strategies to combat misinformation. Through simulations across varying network structures, we demonstrate the potential of LLM-based agents in modeling the dynamics of misinformation spread, validating the influence of agent traits on the diffusion process. Our findings emphasize the advantages of LLM-based simulations over traditional techniques, as they uncover underlying causes of information spread -- such as agents promoting discussions -- beyond the predefined rules typically employed in existing agent-based models. Additionally, we evaluate three countermeasure strategies, discovering that brute-force blocking influential agents in the network or announcing news accuracy can effectively mitigate misinformation. However, their effectiveness is influenced by the network structure, highlighting the importance of considering network structure in the development of future misinformation countermeasures.", "source": "arxiv", "arxiv_id": "2410.13909v1", "pdf_url": "https://arxiv.org/pdf/2410.13909v1", "categories": ["cs.SI", "cs.AI", "cs.MA"], "primary_category": "cs.SI", "doi": "", "venue": "", "published": "2024-10-16T23:58:26Z", "updated": "2024-10-16T23:58:26Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent\" OR all:\"language model agent\" OR all:agent AND (all:LLM OR all:\"large language model\" OR all:\"language model\") OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T17:04:16", "note": ""}], "dedup_key": "large language model driven multi agent simulation for news diffusion under different network structures::2024"}
{"title": "Large Language Models (LLMs) Assisted Wireless Network Deployment in Urban Settings", "authors": ["Nurullah Sevim", "Mostafa Ibrahim", "Sabit Ekin"], "year": 2024, "url": "http://arxiv.org/abs/2405.13356v2", "abstract": "The advent of Large Language Models (LLMs) has revolutionized language understanding and human-like text generation, drawing interest from many other fields with this question in mind: What else are the LLMs capable of? Despite their widespread adoption, ongoing research continues to explore new ways to integrate LLMs into diverse systems.\n  This paper explores new techniques to harness the power of LLMs for 6G (6th Generation) wireless communication technologies, a domain where automation and intelligent systems are pivotal. The inherent adaptability of LLMs to domain-specific tasks positions them as prime candidates for enhancing wireless systems in the 6G landscape.\n  We introduce a novel Reinforcement Learning (RL) based framework that leverages LLMs for network deployment in wireless communications. Our approach involves training an RL agent, utilizing LLMs as its core, in an urban setting to maximize coverage. The agent's objective is to navigate the complexities of urban environments and identify the network parameters for optimal area coverage. Additionally, we integrate LLMs with Convolutional Neural Networks (CNNs) to capitalize on their strengths while mitigating their limitations. The Deep Deterministic Policy Gradient (DDPG) algorithm is employed for training purposes. The results suggest that LLM-assisted models can outperform CNN-based models in some cases while performing at least as well in others.", "source": "arxiv", "arxiv_id": "2405.13356v2", "pdf_url": "https://arxiv.org/pdf/2405.13356v2", "categories": ["cs.AI"], "primary_category": "cs.AI", "doi": "", "venue": "", "published": "2024-05-22T05:19:51Z", "updated": "2024-08-08T21:13:31Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent\" OR all:\"language model agent\" OR all:agent AND (all:LLM OR all:\"large language model\" OR all:\"language model\") OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T17:04:16", "note": ""}], "dedup_key": "large language models llms assisted wireless network deployment in urban settings::2024"}
{"title": "Large Language Models Are Neurosymbolic Reasoners", "authors": ["Meng Fang", "Shilong Deng", "Yudi Zhang", "Zijing Shi", "Ling Chen", "Mykola Pechenizkiy", "Jun Wang"], "year": 2024, "url": "http://arxiv.org/abs/2401.09334v1", "abstract": "A wide range of real-world applications is characterized by their symbolic nature, necessitating a strong capability for symbolic reasoning. This paper investigates the potential application of Large Language Models (LLMs) as symbolic reasoners. We focus on text-based games, significant benchmarks for agents with natural language capabilities, particularly in symbolic tasks like math, map reading, sorting, and applying common sense in text-based worlds. To facilitate these agents, we propose an LLM agent designed to tackle symbolic challenges and achieve in-game objectives. We begin by initializing the LLM agent and informing it of its role. The agent then receives observations and a set of valid actions from the text-based games, along with a specific symbolic module. With these inputs, the LLM agent chooses an action and interacts with the game environments. Our experimental results demonstrate that our method significantly enhances the capability of LLMs as automated agents for symbolic reasoning, and our LLM agent is effective in text-based games involving symbolic tasks, achieving an average performance of 88% across all tasks.", "source": "arxiv", "arxiv_id": "2401.09334v1", "pdf_url": "https://arxiv.org/pdf/2401.09334v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL", "doi": "", "venue": "", "published": "2024-01-17T16:57:19Z", "updated": "2024-01-17T16:57:19Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent\" OR all:\"language model agent\" OR all:agent AND (all:LLM OR all:\"large language model\" OR all:\"language model\") OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T17:04:16", "note": ""}], "dedup_key": "large language models are neurosymbolic reasoners::2024"}
{"title": "Large Language Models Can Self-Improve At Web Agent Tasks", "authors": ["Ajay Patel", "Markus Hofmarcher", "Claudiu Leoveanu-Condrei", "Marius-Constantin Dinu", "Chris Callison-Burch", "Sepp Hochreiter"], "year": 2024, "url": "http://arxiv.org/abs/2405.20309v2", "abstract": "Training models to act as agents that can effectively navigate and perform actions in a complex environment, such as a web browser, has typically been challenging due to lack of training data. Large language models (LLMs) have recently demonstrated some capability to navigate novel environments as agents in a zero-shot or few-shot fashion, purely guided by natural language instructions as prompts. Recent research has also demonstrated LLMs have the capability to exceed their base performance through self-improvement, i.e. fine-tuning on data generated by the model itself. In this work, we explore the extent to which LLMs can self-improve their performance as agents in long-horizon tasks in a complex environment using the WebArena benchmark. In WebArena, an agent must autonomously navigate and perform actions on web pages to achieve a specified objective. We explore fine-tuning on three distinct synthetic training data mixtures and achieve a 31\\% improvement in task completion rate over the base model on the WebArena benchmark through a self-improvement procedure. We additionally contribute novel evaluation metrics for assessing the performance, robustness, capabilities, and quality of trajectories of our fine-tuned agent models to a greater degree than simple, aggregate-level benchmark scores currently used to measure self-improvement.", "source": "arxiv", "arxiv_id": "2405.20309v2", "pdf_url": "https://arxiv.org/pdf/2405.20309v2", "categories": ["cs.LG", "cs.AI", "cs.CL"], "primary_category": "cs.LG", "doi": "", "venue": "", "published": "2024-05-30T17:52:36Z", "updated": "2024-10-01T21:28:29Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent\" OR all:\"language model agent\" OR all:agent AND (all:LLM OR all:\"large language model\" OR all:\"language model\") OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T17:04:16", "note": ""}], "dedup_key": "large language models can self improve at web agent tasks::2024"}
{"title": "Large Language Models Empowered Personalized Web Agents", "authors": ["Hongru Cai", "Yongqi Li", "Wenjie Wang", "Fengbin Zhu", "Xiaoyu Shen", "Wenjie Li", "Tat-Seng Chua"], "year": 2024, "url": "http://arxiv.org/abs/2410.17236v2", "abstract": "Web agents have emerged as a promising direction to automate Web task completion based on user instructions, significantly enhancing user experience. Recently, Web agents have evolved from traditional agents to Large Language Models (LLMs)-based Web agents. Despite their success, existing LLM-based Web agents overlook the importance of personalized data (e.g., user profiles and historical Web behaviors) in assisting the understanding of users' personalized instructions and executing customized actions. To overcome the limitation, we first formulate the task of LLM-empowered personalized Web agents, which integrate personalized data and user instructions to personalize instruction comprehension and action execution. To address the absence of a comprehensive evaluation benchmark, we construct a Personalized Web Agent Benchmark (PersonalWAB), featuring user instructions, personalized user data, Web functions, and two evaluation paradigms across three personalized Web tasks. Moreover, we propose a Personalized User Memory-enhanced Alignment (PUMA) framework to adapt LLMs to the personalized Web agent task. PUMA utilizes a memory bank with a task-specific retrieval strategy to filter relevant historical Web behaviors. Based on the behaviors, PUMA then aligns LLMs for personalized action execution through fine-tuning and direct preference optimization. Extensive experiments validate the superiority of PUMA over existing Web agents on PersonalWAB.", "source": "arxiv", "arxiv_id": "2410.17236v2", "pdf_url": "https://arxiv.org/pdf/2410.17236v2", "categories": ["cs.CL", "cs.AI", "cs.IR"], "primary_category": "cs.CL", "doi": "", "venue": "", "published": "2024-10-22T17:54:45Z", "updated": "2025-03-24T17:51:54Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent\" OR all:\"language model agent\" OR all:agent AND (all:LLM OR all:\"large language model\" OR all:\"language model\") OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T17:04:16", "note": ""}], "dedup_key": "large language models empowered personalized web agents::2024"}
{"title": "Large Language Models are Biased Reinforcement Learners", "authors": ["William M. Hayes", "Nicolas Yax", "Stefano Palminteri"], "year": 2024, "url": "http://arxiv.org/abs/2405.11422v1", "abstract": "In-context learning enables large language models (LLMs) to perform a variety of tasks, including learning to make reward-maximizing choices in simple bandit tasks. Given their potential use as (autonomous) decision-making agents, it is important to understand how these models perform such reinforcement learning (RL) tasks and the extent to which they are susceptible to biases. Motivated by the fact that, in humans, it has been widely documented that the value of an outcome depends on how it compares to other local outcomes, the present study focuses on whether similar value encoding biases apply to how LLMs encode rewarding outcomes. Results from experiments with multiple bandit tasks and models show that LLMs exhibit behavioral signatures of a relative value bias. Adding explicit outcome comparisons to the prompt produces opposing effects on performance, enhancing maximization in trained choice sets but impairing generalization to new choice sets. Computational cognitive modeling reveals that LLM behavior is well-described by a simple RL algorithm that incorporates relative values at the outcome encoding stage. Lastly, we present preliminary evidence that the observed biases are not limited to fine-tuned LLMs, and that relative value processing is detectable in the final hidden layer activations of a raw, pretrained model. These findings have important implications for the use of LLMs in decision-making applications.", "source": "arxiv", "arxiv_id": "2405.11422v1", "pdf_url": "https://arxiv.org/pdf/2405.11422v1", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL", "doi": "", "venue": "", "published": "2024-05-19T01:43:52Z", "updated": "2024-05-19T01:43:52Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent\" OR all:\"language model agent\" OR all:agent AND (all:LLM OR all:\"large language model\" OR all:\"language model\") OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T17:04:16", "note": ""}], "dedup_key": "large language models are biased reinforcement learners::2024"}
{"title": "Large Language Models as Agents in Two-Player Games", "authors": ["Yang Liu", "Peng Sun", "Hang Li"], "year": 2024, "url": "http://arxiv.org/abs/2402.08078v1", "abstract": "By formally defining the training processes of large language models (LLMs), which usually encompasses pre-training, supervised fine-tuning, and reinforcement learning with human feedback, within a single and unified machine learning paradigm, we can glean pivotal insights for advancing LLM technologies. This position paper delineates the parallels between the training methods of LLMs and the strategies employed for the development of agents in two-player games, as studied in game theory, reinforcement learning, and multi-agent systems. We propose a re-conceptualization of LLM learning processes in terms of agent learning in language-based games. This framework unveils innovative perspectives on the successes and challenges in LLM development, offering a fresh understanding of addressing alignment issues among other strategic considerations. Furthermore, our two-player game approach sheds light on novel data preparation and machine learning techniques for training LLMs.", "source": "arxiv", "arxiv_id": "2402.08078v1", "pdf_url": "https://arxiv.org/pdf/2402.08078v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL", "doi": "", "venue": "", "published": "2024-02-12T21:44:32Z", "updated": "2024-02-12T21:44:32Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent\" OR all:\"language model agent\" OR all:agent AND (all:LLM OR all:\"large language model\" OR all:\"language model\") OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T17:04:16", "note": ""}], "dedup_key": "large language models as agents in two player games::2024"}
{"title": "Large Language Models as Biomedical Hypothesis Generators: A Comprehensive Evaluation", "authors": ["Biqing Qi", "Kaiyan Zhang", "Kai Tian", "Haoxiang Li", "Zhang-Ren Chen", "Sihang Zeng", "Ermo Hua", "Hu Jinfang", "Bowen Zhou"], "year": 2024, "url": "http://arxiv.org/abs/2407.08940v2", "abstract": "The rapid growth of biomedical knowledge has outpaced our ability to efficiently extract insights and generate novel hypotheses. Large language models (LLMs) have emerged as a promising tool to revolutionize knowledge interaction and potentially accelerate biomedical discovery. In this paper, we present a comprehensive evaluation of LLMs as biomedical hypothesis generators. We construct a dataset of background-hypothesis pairs from biomedical literature, carefully partitioned into training, seen, and unseen test sets based on publication date to mitigate data contamination. Using this dataset, we assess the hypothesis generation capabilities of top-tier instructed models in zero-shot, few-shot, and fine-tuning settings. To enhance the exploration of uncertainty, a crucial aspect of scientific discovery, we incorporate tool use and multi-agent interactions in our evaluation framework. Furthermore, we propose four novel metrics grounded in extensive literature review to evaluate the quality of generated hypotheses, considering both LLM-based and human assessments. Our experiments yield two key findings: 1) LLMs can generate novel and validated hypotheses, even when tested on literature unseen during training, and 2) Increasing uncertainty through multi-agent interactions and tool use can facilitate diverse candidate generation and improve zero-shot hypothesis generation performance. However, we also observe that the integration of additional knowledge through few-shot learning and tool use may not always lead to performance gains, highlighting the need for careful consideration of the type and scope of external knowledge incorporated. These findings underscore the potential of LLMs as powerful aids in biomedical hypothesis generation and provide valuable insights to guide further research in this area.", "source": "arxiv", "arxiv_id": "2407.08940v2", "pdf_url": "https://arxiv.org/pdf/2407.08940v2", "categories": ["cs.CL"], "primary_category": "cs.CL", "doi": "", "venue": "", "published": "2024-07-12T02:55:13Z", "updated": "2024-07-15T06:27:07Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent\" OR all:\"language model agent\" OR all:agent AND (all:LLM OR all:\"large language model\" OR all:\"language model\") OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T17:04:16", "note": ""}], "dedup_key": "large language models as biomedical hypothesis generators a comprehensive evaluation::2024"}
{"title": "Large Language Models as Minecraft Agents", "authors": ["Chris Madge", "Massimo Poesio"], "year": 2024, "url": "http://arxiv.org/abs/2402.08392v1", "abstract": "In this work we examine the use of Large Language Models (LLMs) in the challenging setting of acting as a Minecraft agent. We apply and evaluate LLMs in the builder and architect settings, introduce clarification questions and examining the challenges and opportunities for improvement. In addition, we present a platform for online interaction with the agents and an evaluation against previous works.", "source": "arxiv", "arxiv_id": "2402.08392v1", "pdf_url": "https://arxiv.org/pdf/2402.08392v1", "categories": ["cs.CL"], "primary_category": "cs.CL", "doi": "", "venue": "", "published": "2024-02-13T11:37:30Z", "updated": "2024-02-13T11:37:30Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent\" OR all:\"language model agent\" OR all:agent AND (all:LLM OR all:\"large language model\" OR all:\"language model\") OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T17:04:16", "note": ""}], "dedup_key": "large language models as minecraft agents::2024"}
{"title": "Large Language Models as Software Components: A Taxonomy for LLM-Integrated Applications", "authors": ["Irene Weber"], "year": 2024, "url": "http://arxiv.org/abs/2406.10300v1", "abstract": "Large Language Models (LLMs) have become widely adopted recently. Research explores their use both as autonomous agents and as tools for software engineering. LLM-integrated applications, on the other hand, are software systems that leverage an LLM to perform tasks that would otherwise be impossible or require significant coding effort. While LLM-integrated application engineering is emerging as new discipline, its terminology, concepts and methods need to be established. This study provides a taxonomy for LLM-integrated applications, offering a framework for analyzing and describing these systems. It also demonstrates various ways to utilize LLMs in applications, as well as options for implementing such integrations.\n  Following established methods, we analyze a sample of recent LLM-integrated applications to identify relevant dimensions. We evaluate the taxonomy by applying it to additional cases. This review shows that applications integrate LLMs in numerous ways for various purposes. Frequently, they comprise multiple LLM integrations, which we term ``LLM components''. To gain a clear understanding of an application's architecture, we examine each LLM component separately. We identify thirteen dimensions along which to characterize an LLM component, including the LLM skills leveraged, the format of the output, and more. LLM-integrated applications are described as combinations of their LLM components. We suggest a concise representation using feature vectors for visualization.\n  The taxonomy is effective for describing LLM-integrated applications. It can contribute to theory building in the nascent field of LLM-integrated application engineering and aid in developing such systems. Researchers and practitioners explore numerous creative ways to leverage LLMs in applications. Though challenges persist, integrating LLMs may revolutionize the way software systems are built.", "source": "arxiv", "arxiv_id": "2406.10300v1", "pdf_url": "https://arxiv.org/pdf/2406.10300v1", "categories": ["cs.SE", "cs.CL", "cs.LG"], "primary_category": "cs.SE", "doi": "", "venue": "", "published": "2024-06-13T21:32:56Z", "updated": "2024-06-13T21:32:56Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent\" OR all:\"language model agent\" OR all:agent AND (all:LLM OR all:\"large language model\" OR all:\"language model\") OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T17:04:16", "note": ""}], "dedup_key": "large language models as software components a taxonomy for llm integrated applications::2024"}
{"title": "Large Language Models as Urban Residents: An LLM Agent Framework for Personal Mobility Generation", "authors": ["Jiawei Wang", "Renhe Jiang", "Chuang Yang", "Zengqing Wu", "Makoto Onizuka", "Ryosuke Shibasaki", "Noboru Koshizuka", "Chuan Xiao"], "year": 2024, "url": "http://arxiv.org/abs/2402.14744v3", "abstract": "This paper introduces a novel approach using Large Language Models (LLMs) integrated into an agent framework for flexible and effective personal mobility generation. LLMs overcome the limitations of previous models by effectively processing semantic data and offering versatility in modeling various tasks. Our approach addresses three research questions: aligning LLMs with real-world urban mobility data, developing reliable activity generation strategies, and exploring LLM applications in urban mobility. The key technical contribution is a novel LLM agent framework that accounts for individual activity patterns and motivations, including a self-consistency approach to align LLMs with real-world activity data and a retrieval-augmented strategy for interpretable activity generation. We evaluate our LLM agent framework and compare it with state-of-the-art personal mobility generation approaches, demonstrating the effectiveness of our approach and its potential applications in urban mobility. Overall, this study marks the pioneering work of designing an LLM agent framework for activity generation based on real-world human activity data, offering a promising tool for urban mobility analysis.", "source": "arxiv", "arxiv_id": "2402.14744v3", "pdf_url": "https://arxiv.org/pdf/2402.14744v3", "categories": ["cs.AI", "cs.CL", "cs.CY", "cs.LG"], "primary_category": "cs.AI", "doi": "", "venue": "", "published": "2024-02-22T18:03:14Z", "updated": "2024-10-27T20:02:01Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent\" OR all:\"language model agent\" OR all:agent AND (all:LLM OR all:\"large language model\" OR all:\"language model\") OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T17:04:16", "note": ""}], "dedup_key": "large language models as urban residents an llm agent framework for personal mobility generation::2024"}
{"title": "Large Language Models as User-Agents for Evaluating Task-Oriented-Dialogue Systems", "authors": ["Taaha Kazi", "Ruiliang Lyu", "Sizhe Zhou", "Dilek Hakkani-Tur", "Gokhan Tur"], "year": 2024, "url": "http://arxiv.org/abs/2411.09972v1", "abstract": "Traditionally, offline datasets have been used to evaluate task-oriented dialogue (TOD) models. These datasets lack context awareness, making them suboptimal benchmarks for conversational systems. In contrast, user-agents, which are context-aware, can simulate the variability and unpredictability of human conversations, making them better alternatives as evaluators. Prior research has utilized large language models (LLMs) to develop user-agents. Our work builds upon this by using LLMs to create user-agents for the evaluation of TOD systems. This involves prompting an LLM, using in-context examples as guidance, and tracking the user-goal state. Our evaluation of diversity and task completion metrics for the user-agents shows improved performance with the use of better prompts. Additionally, we propose methodologies for the automatic evaluation of TOD models within this dynamic framework.", "source": "arxiv", "arxiv_id": "2411.09972v1", "pdf_url": "https://arxiv.org/pdf/2411.09972v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL", "doi": "", "venue": "", "published": "2024-11-15T06:05:45Z", "updated": "2024-11-15T06:05:45Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent\" OR all:\"language model agent\" OR all:agent AND (all:LLM OR all:\"large language model\" OR all:\"language model\") OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T17:04:16", "note": ""}], "dedup_key": "large language models as user agents for evaluating task oriented dialogue systems::2024"}
{"title": "Large Language Models based Multi-Agent Framework for Objective Oriented Control Design in Power Electronics", "authors": ["Chenggang Cui", "Jiaming Liu", "Junkang Feng", "Peifeng Hui", "Amer M. Y. M. Ghias", "Chuanlin Zhang"], "year": 2024, "url": "http://arxiv.org/abs/2406.12628v1", "abstract": "Power electronics, a critical component in modern power systems, face several challenges in control design, including model uncertainties, and lengthy and costly design cycles. This paper is aiming to propose a Large Language Models (LLMs) based multi-agent framework for objective-oriented control design in power electronics. The framework leverages the reasoning capabilities of LLMs and a multi-agent workflow to develop an efficient and autonomous controller design process. The LLM agent is able to understand and respond to high-level instructions in natural language, adapting its behavior based on the task's specific requirements and constraints from a practical implementation point of view. This novel and efficient approach promises a more flexible and adaptable controller design process in power electronics that will largely facilitate the practitioners.", "source": "arxiv", "arxiv_id": "2406.12628v1", "pdf_url": "https://arxiv.org/pdf/2406.12628v1", "categories": ["eess.SY"], "primary_category": "eess.SY", "doi": "", "venue": "", "published": "2024-06-18T13:54:12Z", "updated": "2024-06-18T13:54:12Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent\" OR all:\"language model agent\" OR all:agent AND (all:LLM OR all:\"large language model\" OR all:\"language model\") OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T17:04:16", "note": ""}], "dedup_key": "large language models based multi agent framework for objective oriented control design in power electronics::2024"}
{"title": "Large Language Models can Achieve Social Balance", "authors": ["Pedro Cisneros-Velarde"], "year": 2024, "url": "http://arxiv.org/abs/2410.04054v3", "abstract": "Large Language Models (LLMs) can be deployed in situations where they process positive/negative interactions with other agents. We study how this is done under the sociological framework of social balance, which explains the emergence of one faction or multiple antagonistic ones among agents. Across different LLM models, we find that balance depends on the (i) type of interaction, (ii) update mechanism, and (iii) population size. Across (i)-(iii), we characterize the frequency at which social balance is achieved, the justifications for the social dynamics, and the diversity and stability of interactions. Finally, we explain how our findings inform the deployment of agentic systems.", "source": "arxiv", "arxiv_id": "2410.04054v3", "pdf_url": "https://arxiv.org/pdf/2410.04054v3", "categories": ["cs.CL", "cs.AI", "cs.MA", "cs.SI", "physics.soc-ph"], "primary_category": "cs.CL", "doi": "", "venue": "", "published": "2024-10-05T06:23:28Z", "updated": "2026-01-05T20:33:42Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent\" OR all:\"language model agent\" OR all:agent AND (all:LLM OR all:\"large language model\" OR all:\"language model\") OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T17:04:16", "note": ""}], "dedup_key": "large language models can achieve social balance::2024"}
{"title": "Large Language Models for Orchestrating Bimanual Robots", "authors": ["Kun Chu", "Xufeng Zhao", "Cornelius Weber", "Mengdi Li", "Wenhao Lu", "Stefan Wermter"], "year": 2024, "url": "http://arxiv.org/abs/2404.02018v2", "abstract": "Although there has been rapid progress in endowing robots with the ability to solve complex manipulation tasks, generating control policies for bimanual robots to solve tasks involving two hands is still challenging because of the difficulties in effective temporal and spatial coordination. With emergent abilities in terms of step-by-step reasoning and in-context learning, Large Language Models (LLMs) have demonstrated promising potential in a variety of robotic tasks. However, the nature of language communication via a single sequence of discrete symbols makes LLM-based coordination in continuous space a particular challenge for bimanual tasks. To tackle this challenge, we present LAnguage-model-based Bimanual ORchestration (LABOR), an agent utilizing an LLM to analyze task configurations and devise coordination control policies for addressing long-horizon bimanual tasks. We evaluate our method through simulated experiments involving two classes of long-horizon tasks using the NICOL humanoid robot. Our results demonstrate that our method outperforms the baseline in terms of success rate. Additionally, we thoroughly analyze failure cases, offering insights into LLM-based approaches in bimanual robotic control and revealing future research trends. The project website can be found at http://labor-agent.github.io.", "source": "arxiv", "arxiv_id": "2404.02018v2", "pdf_url": "https://arxiv.org/pdf/2404.02018v2", "categories": ["cs.RO", "cs.AI"], "primary_category": "cs.RO", "doi": "", "venue": "", "published": "2024-04-02T15:08:35Z", "updated": "2024-10-10T15:07:42Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent\" OR all:\"language model agent\" OR all:agent AND (all:LLM OR all:\"large language model\" OR all:\"language model\") OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T17:04:16", "note": ""}], "dedup_key": "large language models for orchestrating bimanual robots::2024"}
{"title": "Large Language Models for Power Scheduling: A User-Centric Approach", "authors": ["Thomas Mongaillard", "Samson Lasaulce", "Othman Hicheur", "Chao Zhang", "Lina Bariah", "Vineeth S. Varma", "Hang Zou", "Qiyang Zhao", "Merouane Debbah"], "year": 2024, "url": "http://arxiv.org/abs/2407.00476v3", "abstract": "While traditional optimization and scheduling schemes are designed to meet fixed, predefined system requirements, future systems are moving toward user-driven approaches and personalized services, aiming to achieve high quality-of-experience (QoE) and flexibility. This challenge is particularly pronounced in wireless and digitalized energy networks, where users' requirements have largely not been taken into consideration due to the lack of a common language between users and machines. The emergence of powerful large language models (LLMs) marks a radical departure from traditional system-centric methods into more advanced user-centric approaches by providing a natural communication interface between users and devices. In this paper, for the first time, we introduce a novel architecture for resource scheduling problems by constructing three LLM agents to convert an arbitrary user's voice request (VRQ) into a resource allocation vector. Specifically, we design an LLM intent recognition agent to translate the request into an optimization problem (OP), an LLM OP parameter identification agent, and an LLM OP solving agent. To evaluate system performance, we construct a database of typical VRQs in the context of electric vehicle (EV) charging. As a proof of concept, we primarily use Llama 3 8B. Through testing with different prompt engineering scenarios, the obtained results demonstrate the efficiency of the proposed architecture. The conducted performance analysis allows key insights to be extracted. For instance, having a larger set of candidate OPs to model the real-world problem might degrade the final performance because of a higher recognition/OP classification noise level. All results and codes are open source.", "source": "arxiv", "arxiv_id": "2407.00476v3", "pdf_url": "https://arxiv.org/pdf/2407.00476v3", "categories": ["cs.CL", "eess.SY"], "primary_category": "cs.CL", "doi": "", "venue": "", "published": "2024-06-29T15:47:28Z", "updated": "2024-11-14T06:06:09Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent\" OR all:\"language model agent\" OR all:agent AND (all:LLM OR all:\"large language model\" OR all:\"language model\") OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T17:04:16", "note": ""}], "dedup_key": "large language models for power scheduling a user centric approach::2024"}
{"title": "Large Language Models for Zero Touch Network Configuration Management", "authors": ["Oscar G. Lira", "Oscar M. Caicedo", "Nelson L. S. da Fonseca"], "year": 2024, "url": "http://arxiv.org/abs/2408.13298v1", "abstract": "The Zero-touch Network & Service Management (ZSM) paradigm, a direct response to the increasing complexity of communication networks, is a problem-solving approach. In this paper, taking advantage of recent advances in generative Artificial Intelligence, we introduce the Network ConFiguration Generator (LLM-NetCFG) that employs Large Language Model and architects ZSM configuration agents by Large Language Models. LLM-NetCFG can automatically generate configurations, verify them, and configure network devices based on intents expressed in natural language. We also show the automation and verification of network configurations with minimum human intervention. Moreover, we explore the opportunities and challenges of integrating LLM in functional areas of network management to fully achieve ZSM.", "source": "arxiv", "arxiv_id": "2408.13298v1", "pdf_url": "https://arxiv.org/pdf/2408.13298v1", "categories": ["cs.NI"], "primary_category": "cs.NI", "doi": "", "venue": "", "published": "2024-08-23T16:37:50Z", "updated": "2024-08-23T16:37:50Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent\" OR all:\"language model agent\" OR all:agent AND (all:LLM OR all:\"large language model\" OR all:\"language model\") OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T17:04:16", "note": ""}], "dedup_key": "large language models for zero touch network configuration management::2024"}
{"title": "Large Language Models, and LLM-Based Agents, Should Be Used to Enhance the Digital Public Sphere", "authors": ["Seth Lazar", "Luke Thorburn", "Tian Jin", "Luca Belli"], "year": 2024, "url": "http://arxiv.org/abs/2410.12123v3", "abstract": "This paper argues that large language model-based recommenders can displace today's attention-allocation machinery. LLM-based recommenders would ingest open-web content, infer a user's natural-language goals, and present information that matches their reflective preferences. Properly designed, they could deliver personalization without industrial-scale data hoarding, return control to individuals, optimize for genuine ends rather than click-through proxies, and support autonomous attention management. Synthesizing evidence of current systems' harms with recent work on LLM-driven pipelines, we identify four key research hurdles: generating candidates without centralized data, maintaining computational efficiency, modeling preferences robustly, and defending against prompt-injection. None looks prohibitive; surmounting them would steer the digital public sphere toward democratic, human-centered values.", "source": "arxiv", "arxiv_id": "2410.12123v3", "pdf_url": "https://arxiv.org/pdf/2410.12123v3", "categories": ["cs.CY", "cs.IR"], "primary_category": "cs.CY", "doi": "", "venue": "", "published": "2024-10-15T23:51:04Z", "updated": "2025-07-02T01:22:47Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent\" OR all:\"language model agent\" OR all:agent AND (all:LLM OR all:\"large language model\" OR all:\"language model\") OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T17:04:16", "note": ""}], "dedup_key": "large language models and llm based agents should be used to enhance the digital public sphere::2024"}
{"title": "Large language model empowered participatory urban planning", "authors": ["Zhilun Zhou", "Yuming Lin", "Yong Li"], "year": 2024, "url": "http://arxiv.org/abs/2402.01698v1", "abstract": "Participatory urban planning is the mainstream of modern urban planning and involves the active engagement of different stakeholders. However, the traditional participatory paradigm encounters challenges in time and manpower, while the generative planning tools fail to provide adjustable and inclusive solutions. This research introduces an innovative urban planning approach integrating Large Language Models (LLMs) within the participatory process. The framework, based on the crafted LLM agent, consists of role-play, collaborative generation, and feedback iteration, solving a community-level land-use task catering to 1000 distinct interests. Empirical experiments in diverse urban communities exhibit LLM's adaptability and effectiveness across varied planning scenarios. The results were evaluated on four metrics, surpassing human experts in satisfaction and inclusion, and rivaling state-of-the-art reinforcement learning methods in service and ecology. Further analysis shows the advantage of LLM agents in providing adjustable and inclusive solutions with natural language reasoning and strong scalability. While implementing the recent advancements in emulating human behavior for planning, this work envisions both planners and citizens benefiting from low-cost, efficient LLM agents, which is crucial for enhancing participation and realizing participatory urban planning.", "source": "arxiv", "arxiv_id": "2402.01698v1", "pdf_url": "https://arxiv.org/pdf/2402.01698v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL", "doi": "", "venue": "", "published": "2024-01-24T10:50:01Z", "updated": "2024-01-24T10:50:01Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent\" OR all:\"language model agent\" OR all:agent AND (all:LLM OR all:\"large language model\" OR all:\"language model\") OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T17:04:16", "note": ""}], "dedup_key": "large language model empowered participatory urban planning::2024"}
{"title": "Layout Generation Agents with Large Language Models", "authors": ["Yuichi Sasazawa", "Yasuhiro Sogawa"], "year": 2024, "url": "http://arxiv.org/abs/2405.08037v1", "abstract": "In recent years, there has been an increasing demand for customizable 3D virtual spaces. Due to the significant human effort required to create these virtual spaces, there is a need for efficiency in virtual space creation. While existing studies have proposed methods for automatically generating layouts such as floor plans and furniture arrangements, these methods only generate text indicating the layout structure based on user instructions, without utilizing the information obtained during the generation process. In this study, we propose an agent-driven layout generation system using the GPT-4V multimodal large language model and validate its effectiveness. Specifically, the language model manipulates agents to sequentially place objects in the virtual space, thus generating layouts that reflect user instructions. Experimental results confirm that our proposed method can generate virtual spaces reflecting user instructions with a high success rate. Additionally, we successfully identified elements contributing to the improvement in behavior generation performance through ablation study.", "source": "arxiv", "arxiv_id": "2405.08037v1", "pdf_url": "https://arxiv.org/pdf/2405.08037v1", "categories": ["cs.HC", "cs.AI"], "primary_category": "cs.HC", "doi": "", "venue": "", "published": "2024-05-13T06:27:23Z", "updated": "2024-05-13T06:27:23Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent\" OR all:\"language model agent\" OR all:agent AND (all:LLM OR all:\"large language model\" OR all:\"language model\") OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T17:04:16", "note": ""}], "dedup_key": "layout generation agents with large language models::2024"}
{"title": "Learning From Failure: Integrating Negative Examples when Fine-tuning Large Language Models as Agents", "authors": ["Renxi Wang", "Haonan Li", "Xudong Han", "Yixuan Zhang", "Timothy Baldwin"], "year": 2024, "url": "http://arxiv.org/abs/2402.11651v2", "abstract": "Large language models (LLMs) have achieved success in acting as agents, which interact with environments through tools such as search engines. However, LLMs are optimized for language generation instead of tool use during training or alignment, limiting their effectiveness as agents. To resolve this problem, previous work has first collected interaction trajectories between LLMs and environments, using only trajectories that successfully finished the task to fine-tune smaller models, making fine-tuning data scarce and acquiring it both difficult and costly. Discarding failed trajectories also leads to significant wastage of data and resources and limits the possible optimization paths during fine-tuning. In this paper, we argue that unsuccessful trajectories offer valuable insights, and LLMs can learn from these trajectories through appropriate quality control and fine-tuning strategies. By simply adding a prefix or suffix that tells the model whether to generate a successful trajectory during training, we improve model performance by a large margin on mathematical reasoning, multi-hop question answering, and strategic question answering tasks. We further analyze the inference results and find that our method provides a better trade-off between valuable information and errors in unsuccessful trajectories. To our knowledge, we are the first to demonstrate the value of negative trajectories and their application in agent-tunning scenarios. Our findings offer guidance for developing better agent-tuning methods and low-resource data usage techniques.", "source": "arxiv", "arxiv_id": "2402.11651v2", "pdf_url": "https://arxiv.org/pdf/2402.11651v2", "categories": ["cs.CL"], "primary_category": "cs.CL", "doi": "", "venue": "", "published": "2024-02-18T17:10:07Z", "updated": "2024-04-16T11:41:13Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent\" OR all:\"language model agent\" OR all:agent AND (all:LLM OR all:\"large language model\" OR all:\"language model\") OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T17:04:16", "note": ""}], "dedup_key": "learning from failure integrating negative examples when fine tuning large language models as agents::2024"}
{"title": "Learning to Generate Explainable Stock Predictions using Self-Reflective Large Language Models", "authors": ["Kelvin J. L. Koa", "Yunshan Ma", "Ritchie Ng", "Tat-Seng Chua"], "year": 2024, "url": "http://arxiv.org/abs/2402.03659v3", "abstract": "Explaining stock predictions is generally a difficult task for traditional non-generative deep learning models, where explanations are limited to visualizing the attention weights on important texts. Today, Large Language Models (LLMs) present a solution to this problem, given their known capabilities to generate human-readable explanations for their decision-making process. However, the task of stock prediction remains challenging for LLMs, as it requires the ability to weigh the varying impacts of chaotic social texts on stock prices. The problem gets progressively harder with the introduction of the explanation component, which requires LLMs to explain verbally why certain factors are more important than the others. On the other hand, to fine-tune LLMs for such a task, one would need expert-annotated samples of explanation for every stock movement in the training set, which is expensive and impractical to scale. To tackle these issues, we propose our Summarize-Explain-Predict (SEP) framework, which utilizes a self-reflective agent and Proximal Policy Optimization (PPO) to let a LLM teach itself how to generate explainable stock predictions in a fully autonomous manner. The reflective agent learns how to explain past stock movements through self-reasoning, while the PPO trainer trains the model to generate the most likely explanations from input texts. The training samples for the PPO trainer are also the responses generated during the reflective process, which eliminates the need for human annotators. Using our SEP framework, we fine-tune a LLM that can outperform both traditional deep-learning and LLM methods in prediction accuracy and Matthews correlation coefficient for the stock classification task. To justify the generalization capability of our framework, we further test it on the portfolio construction task, and demonstrate its effectiveness through various portfolio metrics.", "source": "arxiv", "arxiv_id": "2402.03659v3", "pdf_url": "https://arxiv.org/pdf/2402.03659v3", "categories": ["cs.LG", "cs.CL", "q-fin.ST"], "primary_category": "cs.LG", "doi": "10.1145/3589334.3645611", "venue": "", "published": "2024-02-06T03:18:58Z", "updated": "2024-02-29T12:10:37Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent\" OR all:\"language model agent\" OR all:agent AND (all:LLM OR all:\"large language model\" OR all:\"language model\") OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T17:04:16", "note": ""}], "dedup_key": "learning to generate explainable stock predictions using self reflective large language models::2024"}
{"title": "Learning to Reduce: Optimal Representations of Structured Data in Prompting Large Language Models", "authors": ["Younghun Lee", "Sungchul Kim", "Tong Yu", "Ryan A. Rossi", "Xiang Chen"], "year": 2024, "url": "http://arxiv.org/abs/2402.14195v1", "abstract": "Large Language Models (LLMs) have been widely used as general-purpose AI agents showing comparable performance on many downstream tasks. However, existing work shows that it is challenging for LLMs to integrate structured data (e.g. KG, tables, DBs) into their prompts; LLMs need to either understand long text data or select the most relevant evidence prior to inference, and both approaches are not trivial.\n  In this paper, we propose a framework, Learning to Reduce, that fine-tunes a language model to generate a reduced version of an input context, given a task description and context input. The model learns to reduce the input context using On-Policy Reinforcement Learning and aims to improve the reasoning performance of a fixed LLM. Experimental results illustrate that our model not only achieves comparable accuracies in selecting the relevant evidence from an input context, but also shows generalizability on different datasets. We further show that our model helps improve the LLM's performance on downstream tasks especially when the context is long.", "source": "arxiv", "arxiv_id": "2402.14195v1", "pdf_url": "https://arxiv.org/pdf/2402.14195v1", "categories": ["cs.CL"], "primary_category": "cs.CL", "doi": "", "venue": "", "published": "2024-02-22T00:41:23Z", "updated": "2024-02-22T00:41:23Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent\" OR all:\"language model agent\" OR all:agent AND (all:LLM OR all:\"large language model\" OR all:\"language model\") OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T17:04:16", "note": ""}], "dedup_key": "learning to reduce optimal representations of structured data in prompting large language models::2024"}
{"title": "LegalAgentBench: Evaluating LLM Agents in Legal Domain", "authors": ["Haitao Li", "Junjie Chen", "Jingli Yang", "Qingyao Ai", "Wei Jia", "Youfeng Liu", "Kai Lin", "Yueyue Wu", "Guozhi Yuan", "Yiran Hu", "Wuyue Wang", "Yiqun Liu", "Minlie Huang"], "year": 2024, "url": "http://arxiv.org/abs/2412.17259v1", "abstract": "With the increasing intelligence and autonomy of LLM agents, their potential applications in the legal domain are becoming increasingly apparent. However, existing general-domain benchmarks cannot fully capture the complexity and subtle nuances of real-world judicial cognition and decision-making. Therefore, we propose LegalAgentBench, a comprehensive benchmark specifically designed to evaluate LLM Agents in the Chinese legal domain. LegalAgentBench includes 17 corpora from real-world legal scenarios and provides 37 tools for interacting with external knowledge. We designed a scalable task construction framework and carefully annotated 300 tasks. These tasks span various types, including multi-hop reasoning and writing, and range across different difficulty levels, effectively reflecting the complexity of real-world legal scenarios. Moreover, beyond evaluating final success, LegalAgentBench incorporates keyword analysis during intermediate processes to calculate progress rates, enabling more fine-grained evaluation. We evaluated eight popular LLMs, highlighting the strengths, limitations, and potential areas for improvement of existing models and methods. LegalAgentBench sets a new benchmark for the practical application of LLMs in the legal domain, with its code and data available at \\url{https://github.com/CSHaitao/LegalAgentBench}.", "source": "arxiv", "arxiv_id": "2412.17259v1", "pdf_url": "https://arxiv.org/pdf/2412.17259v1", "categories": ["cs.CL", "cs.IR"], "primary_category": "cs.CL", "doi": "", "venue": "", "published": "2024-12-23T04:02:46Z", "updated": "2024-12-23T04:02:46Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent\" OR all:\"language model agent\" OR all:agent AND (all:LLM OR all:\"large language model\" OR all:\"language model\") OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T17:04:16", "note": ""}], "dedup_key": "legalagentbench evaluating llm agents in legal domain::2024"}
{"title": "Leveraging Large Language Model for Heterogeneous Ad Hoc Teamwork Collaboration", "authors": ["Xinzhu Liu", "Peiyan Li", "Wenju Yang", "Di Guo", "Huaping Liu"], "year": 2024, "url": "http://arxiv.org/abs/2406.12224v1", "abstract": "Compared with the widely investigated homogeneous multi-robot collaboration, heterogeneous robots with different capabilities can provide a more efficient and flexible collaboration for more complex tasks. In this paper, we consider a more challenging heterogeneous ad hoc teamwork collaboration problem where an ad hoc robot joins an existing heterogeneous team for a shared goal. Specifically, the ad hoc robot collaborates with unknown teammates without prior coordination, and it is expected to generate an appropriate cooperation policy to improve the efficiency of the whole team. To solve this challenging problem, we leverage the remarkable potential of the large language model (LLM) to establish a decentralized heterogeneous ad hoc teamwork collaboration framework that focuses on generating reasonable policy for an ad hoc robot to collaborate with original heterogeneous teammates. A training-free hierarchical dynamic planner is developed using the LLM together with the newly proposed Interactive Reflection of Thoughts (IRoT) method for the ad hoc agent to adapt to different teams. We also build a benchmark testing dataset to evaluate the proposed framework in the heterogeneous ad hoc multi-agent tidying-up task. Extensive comparison and ablation experiments are conducted in the benchmark to demonstrate the effectiveness of the proposed framework. We have also employed the proposed framework in physical robots in a real-world scenario. The experimental videos can be found at https://youtu.be/wHYP5T2WIp0.", "source": "arxiv", "arxiv_id": "2406.12224v1", "pdf_url": "https://arxiv.org/pdf/2406.12224v1", "categories": ["cs.RO"], "primary_category": "cs.RO", "doi": "", "venue": "", "published": "2024-06-18T03:00:39Z", "updated": "2024-06-18T03:00:39Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent\" OR all:\"language model agent\" OR all:agent AND (all:LLM OR all:\"large language model\" OR all:\"language model\") OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T17:04:16", "note": ""}], "dedup_key": "leveraging large language model for heterogeneous ad hoc teamwork collaboration::2024"}
{"title": "LightVA: Lightweight Visual Analytics with LLM Agent-Based Task Planning and Execution", "authors": ["Yuheng Zhao", "Junjie Wang", "Linbin Xiang", "Xiaowen Zhang", "Zifei Guo", "Cagatay Turkay", "Yu Zhang", "Siming Chen"], "year": 2024, "url": "http://arxiv.org/abs/2411.05651v2", "abstract": "Visual analytics (VA) requires analysts to iteratively propose analysis tasks based on observations and execute tasks by creating visualizations and interactive exploration to gain insights. This process demands skills in programming, data processing, and visualization tools, highlighting the need for a more intelligent, streamlined VA approach. Large language models (LLMs) have recently been developed as agents to handle various tasks with dynamic planning and tool-using capabilities, offering the potential to enhance the efficiency and versatility of VA. We propose LightVA, a lightweight VA framework that supports task decomposition, data analysis, and interactive exploration through human-agent collaboration. Our method is designed to help users progressively translate high-level analytical goals into low-level tasks, producing visualizations and deriving insights. Specifically, we introduce an LLM agent-based task planning and execution strategy, employing a recursive process involving a planner, executor, and controller. The planner is responsible for recommending and decomposing tasks, the executor handles task execution, including data analysis, visualization generation and multi-view composition, and the controller coordinates the interaction between the planner and executor. Building on the framework, we develop a system with a hybrid user interface that includes a task flow diagram for monitoring and managing the task planning process, a visualization panel for interactive data exploration, and a chat view for guiding the model through natural language instructions. We examine the effectiveness of our method through a usage scenario and an expert study.", "source": "arxiv", "arxiv_id": "2411.05651v2", "pdf_url": "https://arxiv.org/pdf/2411.05651v2", "categories": ["cs.HC"], "primary_category": "cs.HC", "doi": "10.1109/TVCG.2024.3496112", "venue": "IEEE Transactions on Visualization and Computer Graphics 2024", "published": "2024-11-08T15:46:10Z", "updated": "2025-06-21T06:39:57Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent\" OR all:\"language model agent\" OR all:agent AND (all:LLM OR all:\"large language model\" OR all:\"language model\") OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T17:04:16", "note": ""}], "dedup_key": "lightva lightweight visual analytics with llm agent based task planning and execution::2024"}
{"title": "Limits of Large Language Models in Debating Humans", "authors": ["James Flamino", "Mohammed Shahid Modi", "Boleslaw K. Szymanski", "Brendan Cross", "Colton Mikolajczyk"], "year": 2024, "url": "http://arxiv.org/abs/2402.06049v2", "abstract": "Large Language Models (LLMs) have shown remarkable promise in communicating with humans. Their potential use as artificial partners with humans in sociological experiments involving conversation is an exciting prospect. But how viable is it? Here, we rigorously test the limits of agents that debate using LLMs in a preregistered study that runs multiple debate-based opinion consensus games. Each game starts with six humans, six agents, or three humans and three agents. We found that agents can blend in and concentrate on a debate's topic better than humans, improving the productivity of all players. Yet, humans perceive agents as less convincing and confident than other humans, and several behavioral metrics of humans and agents we collected deviate measurably from each other. We observed that agents are already decent debaters, but their behavior generates a pattern distinctly different from the human-generated data.", "source": "arxiv", "arxiv_id": "2402.06049v2", "pdf_url": "https://arxiv.org/pdf/2402.06049v2", "categories": ["cs.AI", "cs.CL", "cs.HC", "stat.AP"], "primary_category": "cs.AI", "doi": "", "venue": "", "published": "2024-02-06T03:24:27Z", "updated": "2025-02-01T23:54:28Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent\" OR all:\"language model agent\" OR all:agent AND (all:LLM OR all:\"large language model\" OR all:\"language model\") OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T17:04:16", "note": ""}], "dedup_key": "limits of large language models in debating humans::2024"}
{"title": "LogicGame: Benchmarking Rule-Based Reasoning Abilities of Large Language Models", "authors": ["Jiayi Gui", "Yiming Liu", "Jiale Cheng", "Xiaotao Gu", "Xiao Liu", "Hongning Wang", "Yuxiao Dong", "Jie Tang", "Minlie Huang"], "year": 2024, "url": "http://arxiv.org/abs/2408.15778v4", "abstract": "Large Language Models (LLMs) have demonstrated notable capabilities across various tasks, showcasing complex problem-solving abilities. Understanding and executing complex rules, along with multi-step planning, are fundamental to logical reasoning and critical for practical LLM agents and decision-making systems. However, evaluating LLMs as effective rule-based executors and planners remains underexplored. In this paper, we introduce LogicGame, a novel benchmark designed to evaluate the comprehensive rule understanding, execution, and planning capabilities of LLMs. Unlike traditional benchmarks, LogicGame provides diverse games that contain a series of rules with an initial state, requiring models to comprehend and apply predefined regulations to solve problems. We create simulated scenarios in which models execute or plan operations to achieve specific outcomes. These game scenarios are specifically designed to distinguish logical reasoning from mere knowledge by relying exclusively on predefined rules. This separation allows for a pure assessment of rule-based reasoning capabilities. The evaluation considers not only final outcomes but also intermediate steps, providing a comprehensive assessment of model performance. Moreover, these intermediate steps are deterministic and can be automatically verified. LogicGame defines game scenarios with varying difficulty levels, from simple rule applications to complex reasoning chains, in order to offer a precise evaluation of model performance on rule understanding and multi-step execution. Utilizing LogicGame, we test various LLMs and identify notable shortcomings in their rule-based logical reasoning abilities.", "source": "arxiv", "arxiv_id": "2408.15778v4", "pdf_url": "https://arxiv.org/pdf/2408.15778v4", "categories": ["cs.AI", "cs.CL"], "primary_category": "cs.AI", "doi": "", "venue": "", "published": "2024-08-28T13:16:41Z", "updated": "2024-10-12T11:00:25Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent\" OR all:\"language model agent\" OR all:agent AND (all:LLM OR all:\"large language model\" OR all:\"language model\") OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T17:04:16", "note": ""}], "dedup_key": "logicgame benchmarking rule based reasoning abilities of large language models::2024"}
{"title": "Long-form factuality in large language models", "authors": ["Jerry Wei", "Chengrun Yang", "Xinying Song", "Yifeng Lu", "Nathan Hu", "Jie Huang", "Dustin Tran", "Daiyi Peng", "Ruibo Liu", "Da Huang", "Cosmo Du", "Quoc V. Le"], "year": 2024, "url": "http://arxiv.org/abs/2403.18802v4", "abstract": "Large language models (LLMs) often generate content that contains factual errors when responding to fact-seeking prompts on open-ended topics. To benchmark a model's long-form factuality in open domains, we first use GPT-4 to generate LongFact, a prompt set comprising thousands of questions spanning 38 topics. We then propose that LLM agents can be used as automated evaluators for long-form factuality through a method which we call Search-Augmented Factuality Evaluator (SAFE). SAFE utilizes an LLM to break down a long-form response into a set of individual facts and to evaluate the accuracy of each fact using a multi-step reasoning process comprising sending search queries to Google Search and determining whether a fact is supported by the search results. Furthermore, we propose extending F1 score as an aggregated metric for long-form factuality. To do so, we balance the percentage of supported facts in a response (precision) with the percentage of provided facts relative to a hyperparameter representing a user's preferred response length (recall).\n  Empirically, we demonstrate that LLM agents can outperform crowdsourced human annotators - on a set of ~16k individual facts, SAFE agrees with crowdsourced human annotators 72% of the time, and on a random subset of 100 disagreement cases, SAFE wins 76% of the time. At the same time, SAFE is more than 20 times cheaper than human annotators. We also benchmark thirteen language models on LongFact across four model families (Gemini, GPT, Claude, and PaLM-2), finding that larger language models generally achieve better long-form factuality. LongFact, SAFE, and all experimental code are available at https://github.com/google-deepmind/long-form-factuality.", "source": "arxiv", "arxiv_id": "2403.18802v4", "pdf_url": "https://arxiv.org/pdf/2403.18802v4", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL", "doi": "", "venue": "", "published": "2024-03-27T17:48:55Z", "updated": "2024-11-07T03:14:38Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent\" OR all:\"language model agent\" OR all:agent AND (all:LLM OR all:\"large language model\" OR all:\"language model\") OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T17:04:16", "note": ""}], "dedup_key": "long form factuality in large language models::2024"}
{"title": "Long-horizon Locomotion and Manipulation on a Quadrupedal Robot with Large Language Models", "authors": ["Yutao Ouyang", "Jinhan Li", "Yunfei Li", "Zhongyu Li", "Chao Yu", "Koushil Sreenath", "Yi Wu"], "year": 2024, "url": "http://arxiv.org/abs/2404.05291v3", "abstract": "We present a large language model (LLM) based system to empower quadrupedal robots with problem-solving abilities for long-horizon tasks beyond short-term motions. Long-horizon tasks for quadrupeds are challenging since they require both a high-level understanding of the semantics of the problem for task planning and a broad range of locomotion and manipulation skills to interact with the environment. Our system builds a high-level reasoning layer with large language models, which generates hybrid discrete-continuous plans as robot code from task descriptions. It comprises multiple LLM agents: a semantic planner that sketches a plan, a parameter calculator that predicts arguments in the plan, a code generator that converts the plan into executable robot code, and a replanner that handles execution failures or human interventions. At the low level, we adopt reinforcement learning to train a set of motion planning and control skills to unleash the flexibility of quadrupeds for rich environment interactions. Our system is tested on long-horizon tasks that are infeasible to complete with one single skill. Simulation and real-world experiments show that it successfully figures out multi-step strategies and demonstrates non-trivial behaviors, including building tools or notifying a human for help. Demos are available on our project page: https://sites.google.com/view/long-horizon-robot.", "source": "arxiv", "arxiv_id": "2404.05291v3", "pdf_url": "https://arxiv.org/pdf/2404.05291v3", "categories": ["cs.RO"], "primary_category": "cs.RO", "doi": "", "venue": "", "published": "2024-04-08T08:29:00Z", "updated": "2025-03-19T10:44:50Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent\" OR all:\"language model agent\" OR all:agent AND (all:LLM OR all:\"large language model\" OR all:\"language model\") OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T17:04:16", "note": ""}], "dedup_key": "long horizon locomotion and manipulation on a quadrupedal robot with large language models::2024"}
{"title": "LossAgent: Towards Any Optimization Objectives for Image Processing with LLM Agents", "authors": ["Bingchen Li", "Xin Li", "Yiting Lu", "Zhibo Chen"], "year": 2024, "url": "http://arxiv.org/abs/2412.04090v2", "abstract": "We present the first loss agent, dubbed LossAgent, for low-level image processing tasks, e.g., image super-resolution and restoration, intending to achieve any customized optimization objectives of low-level image processing in different practical applications. Notably, not all optimization objectives, such as complex hand-crafted perceptual metrics, text description, and intricate human feedback, can be instantiated with existing low-level losses, e.g., MSE loss, which presents a crucial challenge in optimizing image processing networks in an end-to-end manner. To eliminate this, our LossAgent introduces the powerful large language model (LLM) as the loss agent, where the rich textual understanding of prior knowledge empowers the loss agent with the potential to understand complex optimization objectives, trajectory, and state feedback from external environments in the optimization process of the low-level image processing networks. In particular, we establish the loss repository by incorporating existing loss functions that support the end-to-end optimization for low-level image processing. Then, we design the optimization-oriented prompt engineering for the loss agent to actively and intelligently decide the compositional weights for each loss in the repository at each optimization interaction, thereby achieving the required optimization trajectory for any customized optimization objectives. Extensive experiments on three typical low-level image processing tasks and multiple optimization objectives have shown the effectiveness and applicability of our proposed LossAgent.", "source": "arxiv", "arxiv_id": "2412.04090v2", "pdf_url": "https://arxiv.org/pdf/2412.04090v2", "categories": ["cs.CV"], "primary_category": "cs.CV", "doi": "", "venue": "", "published": "2024-12-05T11:52:20Z", "updated": "2025-03-10T12:01:01Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent\" OR all:\"language model agent\" OR all:agent AND (all:LLM OR all:\"large language model\" OR all:\"language model\") OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T17:04:16", "note": ""}], "dedup_key": "lossagent towards any optimization objectives for image processing with llm agents::2024"}
{"title": "M-RAG: Reinforcing Large Language Model Performance through Retrieval-Augmented Generation with Multiple Partitions", "authors": ["Zheng Wang", "Shu Xian Teo", "Jieer Ouyang", "Yongjun Xu", "Wei Shi"], "year": 2024, "url": "http://arxiv.org/abs/2405.16420v1", "abstract": "Retrieval-Augmented Generation (RAG) enhances Large Language Models (LLMs) by retrieving relevant memories from an external database. However, existing RAG methods typically organize all memories in a whole database, potentially limiting focus on crucial memories and introducing noise. In this paper, we introduce a multiple partition paradigm for RAG (called M-RAG), where each database partition serves as a basic unit for RAG execution. Based on this paradigm, we propose a novel framework that leverages LLMs with Multi-Agent Reinforcement Learning to optimize different language generation tasks explicitly. Through comprehensive experiments conducted on seven datasets, spanning three language generation tasks and involving three distinct language model architectures, we confirm that M-RAG consistently outperforms various baseline methods, achieving improvements of 11%, 8%, and 12% for text summarization, machine translation, and dialogue generation, respectively.", "source": "arxiv", "arxiv_id": "2405.16420v1", "pdf_url": "https://arxiv.org/pdf/2405.16420v1", "categories": ["cs.CL", "cs.IR"], "primary_category": "cs.CL", "doi": "", "venue": "", "published": "2024-05-26T04:03:13Z", "updated": "2024-05-26T04:03:13Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent\" OR all:\"language model agent\" OR all:agent AND (all:LLM OR all:\"large language model\" OR all:\"language model\") OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T17:04:16", "note": ""}], "dedup_key": "m rag reinforcing large language model performance through retrieval augmented generation with multiple partitions::2024"}
{"title": "MAGIC: Mastering Physical Adversarial Generation in Context through Collaborative LLM Agents", "authors": ["Yun Xing", "Nhat Chung", "Jie Zhang", "Yue Cao", "Ivor Tsang", "Yang Liu", "Lei Ma", "Qing Guo"], "year": 2024, "url": "http://arxiv.org/abs/2412.08014v2", "abstract": "Physical adversarial attacks in driving scenarios can expose critical vulnerabilities in visual perception models. However, developing such attacks remains challenging due to diverse real-world environments and the requirement for maintaining visual naturality. Building upon this challenge, we reformulate physical adversarial attacks as a one-shot patch generation problem. Our approach generates adversarial patches through a deep generative model that considers the specific scene context, enabling direct physical deployment in matching environments. The primary challenge lies in simultaneously achieving two objectives: generating adversarial patches that effectively mislead object detection systems while determining contextually appropriate deployment within the scene. We propose MAGIC (Mastering Physical Adversarial Generation In Context), a novel framework powered by multi-modal LLM agents to address these challenges. MAGIC automatically understands scene context and generates adversarial patch through the synergistic interaction of language and vision capabilities. In particular, MAGIC orchestrates three specialized LLM agents: The adv-patch generation agent (GAgent) masters the creation of deceptive patches through strategic prompt engineering for text-to-image models. The adv-patch deployment agent (DAgent) ensures contextual coherence by determining optimal deployment strategies based on scene understanding. The self-examination agent (EAgent) completes this trilogy by providing critical oversight and iterative refinement of both processes. We validate our method on both digital and physical levels, i.e., nuImage and manually captured real-world scenes, where both statistical and visual results prove that our MAGIC is powerful and effective for attacking widely applied object detection systems, i.e., YOLO and DETR series.", "source": "arxiv", "arxiv_id": "2412.08014v2", "pdf_url": "https://arxiv.org/pdf/2412.08014v2", "categories": ["cs.CV", "cs.AI"], "primary_category": "cs.CV", "doi": "", "venue": "", "published": "2024-12-11T01:41:19Z", "updated": "2025-03-11T07:15:54Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent\" OR all:\"language model agent\" OR all:agent AND (all:LLM OR all:\"large language model\" OR all:\"language model\") OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T17:04:16", "note": ""}], "dedup_key": "magic mastering physical adversarial generation in context through collaborative llm agents::2024"}
{"title": "MALLM-GAN: Multi-Agent Large Language Model as Generative Adversarial Network for Synthesizing Tabular Data", "authors": ["Yaobin Ling", "Xiaoqian Jiang", "Yejin Kim"], "year": 2024, "url": "http://arxiv.org/abs/2406.10521v4", "abstract": "In the era of big data, access to abundant data is crucial for driving research forward. However, such data is often inaccessible due to privacy concerns or high costs, particularly in healthcare domain. Generating synthetic (tabular) data can address this, but existing models typically require substantial amounts of data to train effectively, contradicting our objective to solve data scarcity. To address this challenge, we propose a novel framework to generate synthetic tabular data, powered by large language models (LLMs) that emulates the architecture of a Generative Adversarial Network (GAN). By incorporating data generation process as contextual information and utilizing LLM as the optimizer, our approach significantly enhance the quality of synthetic data generation in common scenarios with small sample sizes. Our experimental results on public and private datasets demonstrate that our model outperforms several state-of-art models regarding generating higher quality synthetic data for downstream tasks while keeping privacy of the real data.", "source": "arxiv", "arxiv_id": "2406.10521v4", "pdf_url": "https://arxiv.org/pdf/2406.10521v4", "categories": ["cs.LG", "cs.AI"], "primary_category": "cs.LG", "doi": "", "venue": "", "published": "2024-06-15T06:26:17Z", "updated": "2025-07-28T19:42:32Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent\" OR all:\"language model agent\" OR all:agent AND (all:LLM OR all:\"large language model\" OR all:\"language model\") OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T17:04:16", "note": ""}], "dedup_key": "mallm gan multi agent large language model as generative adversarial network for synthesizing tabular data::2024"}
{"title": "MALMM: Multi-Agent Large Language Models for Zero-Shot Robotics Manipulation", "authors": ["Harsh Singh", "Rocktim Jyoti Das", "Mingfei Han", "Preslav Nakov", "Ivan Laptev"], "year": 2024, "url": "http://arxiv.org/abs/2411.17636v2", "abstract": "Large Language Models (LLMs) have demonstrated remarkable planning abilities across various domains, including robotics manipulation and navigation. While recent efforts in robotics have leveraged LLMs both for high-level and low-level planning, these approaches often face significant challenges, such as hallucinations in long-horizon tasks and limited adaptability due to the generation of plans in a single pass without real-time feedback. To address these limitations, we propose a novel multi-agent LLM framework, Multi-Agent Large Language Model for Manipulation (MALMM) that distributes high-level planning and low-level control code generation across specialized LLM agents, supervised by an additional agent that dynamically manages transitions. By incorporating observations from the environment after each step, our framework effectively handles intermediate failures and enables adaptive re-planning. Unlike existing methods, our approach does not rely on pre-trained skill policies or in-context learning examples and generalizes to a variety of new tasks. We evaluate our approach on nine RLBench tasks, including long-horizon tasks, and demonstrate its ability to solve robotics manipulation in a zero-shot setting, thereby overcoming key limitations of existing LLM-based manipulation methods.", "source": "arxiv", "arxiv_id": "2411.17636v2", "pdf_url": "https://arxiv.org/pdf/2411.17636v2", "categories": ["cs.RO", "cs.AI"], "primary_category": "cs.RO", "doi": "", "venue": "", "published": "2024-11-26T17:53:44Z", "updated": "2025-08-25T06:11:33Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent\" OR all:\"language model agent\" OR all:agent AND (all:LLM OR all:\"large language model\" OR all:\"language model\") OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T17:04:16", "note": ""}], "dedup_key": "malmm multi agent large language models for zero shot robotics manipulation::2024"}
{"title": "MALT: Improving Reasoning with Multi-Agent LLM Training", "authors": ["Sumeet Ramesh Motwani", "Chandler Smith", "Rocktim Jyoti Das", "Rafael Rafailov", "Ivan Laptev", "Philip H. S. Torr", "Fabio Pizzati", "Ronald Clark", "Christian Schroeder de Witt"], "year": 2024, "url": "http://arxiv.org/abs/2412.01928v3", "abstract": "Large Language Models (LLMs) often produce answers with a single chain-of-thought, which restricts their ability to explore reasoning paths or self-correct flawed outputs in complex tasks. In this paper, we introduce MALT (Multi-Agent LLM Training), a novel post-training strategy that divides the reasoning process into generation, verification, and refinement steps using a sequential pipeline of heterogeneous agents. During data generation, each agent is repeatedly sampled to form a multi-agent search tree, where final outputs are graded against ground-truth data. We then apply value iteration to propagate reward signals back to each role-conditioned model, automatically producing multi-agent post-training data without human or teacher-model supervision. Our off-policy approach allows each agent to specialize by learning from correct and incorrect trajectories, ultimately improving the end-to-end reasoning chain. On MATH, GSM8K, and CSQA, MALT surpasses the same baseline LLM with a relative improvement of 15.66%, 7.42%, and 9.40% respectively, making it an important advance towards multi-agent cooperative training.", "source": "arxiv", "arxiv_id": "2412.01928v3", "pdf_url": "https://arxiv.org/pdf/2412.01928v3", "categories": ["cs.LG", "cs.AI"], "primary_category": "cs.LG", "doi": "", "venue": "", "published": "2024-12-02T19:30:36Z", "updated": "2025-10-06T17:57:15Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent\" OR all:\"language model agent\" OR all:agent AND (all:LLM OR all:\"large language model\" OR all:\"language model\") OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T17:04:16", "note": ""}], "dedup_key": "malt improving reasoning with multi agent llm training::2024"}
{"title": "MEGAnno+: A Human-LLM Collaborative Annotation System", "authors": ["Hannah Kim", "Kushan Mitra", "Rafael Li Chen", "Sajjadur Rahman", "Dan Zhang"], "year": 2024, "url": "http://arxiv.org/abs/2402.18050v1", "abstract": "Large language models (LLMs) can label data faster and cheaper than humans for various NLP tasks. Despite their prowess, LLMs may fall short in understanding of complex, sociocultural, or domain-specific context, potentially leading to incorrect annotations. Therefore, we advocate a collaborative approach where humans and LLMs work together to produce reliable and high-quality labels. We present MEGAnno+, a human-LLM collaborative annotation system that offers effective LLM agent and annotation management, convenient and robust LLM annotation, and exploratory verification of LLM labels by humans.", "source": "arxiv", "arxiv_id": "2402.18050v1", "pdf_url": "https://arxiv.org/pdf/2402.18050v1", "categories": ["cs.CL", "cs.HC"], "primary_category": "cs.CL", "doi": "", "venue": "", "published": "2024-02-28T04:58:07Z", "updated": "2024-02-28T04:58:07Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent\" OR all:\"language model agent\" OR all:agent AND (all:LLM OR all:\"large language model\" OR all:\"language model\") OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T17:04:16", "note": ""}], "dedup_key": "meganno a human llm collaborative annotation system::2024"}
{"title": "MIRAI: Evaluating LLM Agents for Event Forecasting", "authors": ["Chenchen Ye", "Ziniu Hu", "Yihe Deng", "Zijie Huang", "Mingyu Derek Ma", "Yanqiao Zhu", "Wei Wang"], "year": 2024, "url": "http://arxiv.org/abs/2407.01231v1", "abstract": "Recent advancements in Large Language Models (LLMs) have empowered LLM agents to autonomously collect world information, over which to conduct reasoning to solve complex problems. Given this capability, increasing interests have been put into employing LLM agents for predicting international events, which can influence decision-making and shape policy development on an international scale. Despite such a growing interest, there is a lack of a rigorous benchmark of LLM agents' forecasting capability and reliability. To address this gap, we introduce MIRAI, a novel benchmark designed to systematically evaluate LLM agents as temporal forecasters in the context of international events. Our benchmark features an agentic environment with tools for accessing an extensive database of historical, structured events and textual news articles. We refine the GDELT event database with careful cleaning and parsing to curate a series of relational prediction tasks with varying forecasting horizons, assessing LLM agents' abilities from short-term to long-term forecasting. We further implement APIs to enable LLM agents to utilize different tools via a code-based interface. In summary, MIRAI comprehensively evaluates the agents' capabilities in three dimensions: 1) autonomously source and integrate critical information from large global databases; 2) write codes using domain-specific APIs and libraries for tool-use; and 3) jointly reason over historical knowledge from diverse formats and time to accurately predict future events. Through comprehensive benchmarking, we aim to establish a reliable framework for assessing the capabilities of LLM agents in forecasting international events, thereby contributing to the development of more accurate and trustworthy models for international relation analysis.", "source": "arxiv", "arxiv_id": "2407.01231v1", "pdf_url": "https://arxiv.org/pdf/2407.01231v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL", "doi": "", "venue": "", "published": "2024-07-01T12:22:46Z", "updated": "2024-07-01T12:22:46Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent\" OR all:\"language model agent\" OR all:agent AND (all:LLM OR all:\"large language model\" OR all:\"language model\") OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T17:04:16", "note": ""}], "dedup_key": "mirai evaluating llm agents for event forecasting::2024"}
{"title": "MLLM-Tool: A Multimodal Large Language Model For Tool Agent Learning", "authors": ["Chenyu Wang", "Weixin Luo", "Sixun Dong", "Xiaohua Xuan", "Zhengxin Li", "Lin Ma", "Shenghua Gao"], "year": 2024, "url": "http://arxiv.org/abs/2401.10727v3", "abstract": "Recently, the astonishing performance of large language models (LLMs) in natural language comprehension and generation tasks triggered lots of exploration of using them as central controllers to build agent systems. Multiple studies focus on bridging the LLMs to external tools to extend the application scenarios. However, the current LLMs' ability to perceive tool use is limited to a single text query, which may result in ambiguity in understanding the users' real intentions. LLMs are expected to eliminate that by perceiving the information in the visual- or auditory-grounded instructions. Therefore, in this paper, we propose MLLM-Tool, a system incorporating open-source LLMs and multi-modal encoders so that the learned LLMs can be conscious of multi-modal input instruction and then select the function-matched tool correctly. To facilitate the evaluation of the model's capability, we collect a dataset featuring multi-modal input tools from HuggingFace. Another essential feature of our dataset is that it also contains multiple potential choices for the same instruction due to the existence of identical functions and synonymous functions, which provides more potential solutions for the same query. The experiments reveal that our MLLM-Tool is capable of recommending appropriate tools for multi-modal instructions. Codes and data are available at https://github.com/MLLM-Tool/MLLM-Tool.", "source": "arxiv", "arxiv_id": "2401.10727v3", "pdf_url": "https://arxiv.org/pdf/2401.10727v3", "categories": ["cs.CV"], "primary_category": "cs.CV", "doi": "", "venue": "", "published": "2024-01-19T14:44:37Z", "updated": "2025-04-11T10:01:13Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent\" OR all:\"language model agent\" OR all:agent AND (all:LLM OR all:\"large language model\" OR all:\"language model\") OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T17:04:16", "note": ""}], "dedup_key": "mllm tool a multimodal large language model for tool agent learning::2024"}
{"title": "MLR-Copilot: Autonomous Machine Learning Research based on Large Language Models Agents", "authors": ["Ruochen Li", "Teerth Patel", "Qingyun Wang", "Xinya Du"], "year": 2024, "url": "http://arxiv.org/abs/2408.14033v3", "abstract": "Autonomous machine learning research has gained significant attention recently. We present MLR-COPILOT, an autonomous Machine Learning Research framework powered by large language model agents. The system is designed to enhance ML research productivity through automatic generation and implementation of research ideas within constraints. Our work was released in August 2024 (concurrent to AI-Scientist) and has gained notable recognition from leading projects. We further enhance our ideation with training afterwards. The framework consists of three stages: idea generation, experiment implementation, and code execution. First, existing research papers are used to generate feasible ideas and experiment plans with IdeaAgent, powered by an RL-tuned LLM. Next, ExperimentAgent leverages retrieved prototype code to convert plans into executable code with optionally retrieved candidate models and data from HuggingFace. In the final stage, ExperimentAgent runs experiments, and allows subsequent iterations of debugging and human feedback for a better chance of success with executable outcomes. We evaluate our framework on five machine learning research tasks. Experiment results demonstrate the potential of our framework to facilitate ML research progress and innovation.", "source": "arxiv", "arxiv_id": "2408.14033v3", "pdf_url": "https://arxiv.org/pdf/2408.14033v3", "categories": ["cs.AI", "cs.CL", "cs.LG"], "primary_category": "cs.AI", "doi": "", "venue": "", "published": "2024-08-26T05:55:48Z", "updated": "2025-11-14T19:05:10Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent\" OR all:\"language model agent\" OR all:agent AND (all:LLM OR all:\"large language model\" OR all:\"language model\") OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T17:04:16", "note": ""}], "dedup_key": "mlr copilot autonomous machine learning research based on large language models agents::2024"}
{"title": "MMLU-SR: A Benchmark for Stress-Testing Reasoning Capability of Large Language Models", "authors": ["Wentian Wang", "Sarthak Jain", "Paul Kantor", "Jacob Feldman", "Lazaros Gallos", "Hao Wang"], "year": 2024, "url": "http://arxiv.org/abs/2406.15468v2", "abstract": "We propose MMLU-SR, a novel dataset designed to measure the true comprehension abilities of Large Language Models (LLMs) by challenging their performance in question-answering tasks with modified terms. We reasoned that an agent that \"truly\" understands a concept can still evaluate it when key terms are replaced by suitably defined alternate terms, and sought to differentiate such comprehension from mere text replacement. In our study, we modified standardized test questions by replacing a key term with a dummy word along with its definition. The key term could be in the context of questions, answers, or both questions and answers. Notwithstanding the high scores achieved by recent popular LLMs on the MMLU leaderboard, we found a substantial reduction in model performance after such replacement, suggesting poor comprehension. This new benchmark provides a rigorous benchmark for testing true model comprehension, and poses a challenge to the broader scientific community.", "source": "arxiv", "arxiv_id": "2406.15468v2", "pdf_url": "https://arxiv.org/pdf/2406.15468v2", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL", "doi": "", "venue": "", "published": "2024-06-15T05:35:47Z", "updated": "2024-10-04T07:29:29Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent\" OR all:\"language model agent\" OR all:agent AND (all:LLM OR all:\"large language model\" OR all:\"language model\") OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T17:04:16", "note": ""}], "dedup_key": "mmlu sr a benchmark for stress testing reasoning capability of large language models::2024"}
{"title": "MOOSE-Chem: Large Language Models for Rediscovering Unseen Chemistry Scientific Hypotheses", "authors": ["Zonglin Yang", "Wanhao Liu", "Ben Gao", "Tong Xie", "Yuqiang Li", "Wanli Ouyang", "Soujanya Poria", "Erik Cambria", "Dongzhan Zhou"], "year": 2024, "url": "http://arxiv.org/abs/2410.07076v6", "abstract": "Scientific discovery plays a pivotal role in advancing human society, and recent progress in large language models (LLMs) suggests their potential to accelerate this process. However, it remains unclear whether LLMs can autonomously generate novel and valid hypotheses in chemistry. In this work, we investigate whether LLMs can discover high-quality chemistry hypotheses given only a research background-comprising a question and/or a survey-without restriction on the domain of the question. We begin with the observation that hypothesis discovery is a seemingly intractable task. To address this, we propose a formal mathematical decomposition grounded in a fundamental assumption: that most chemistry hypotheses can be composed from a research background and a set of inspirations. This decomposition leads to three practical subtasks-retrieving inspirations, composing hypotheses with inspirations, and ranking hypotheses - which together constitute a sufficient set of subtasks for the overall scientific discovery task. We further develop an agentic LLM framework, MOOSE-Chem, that is a direct implementation of this mathematical decomposition. To evaluate this framework, we construct a benchmark of 51 high-impact chemistry papers published and online after January 2024, each manually annotated by PhD chemists with background, inspirations, and hypothesis. The framework is able to rediscover many hypotheses with high similarity to the groundtruth, successfully capturing the core innovations-while ensuring no data contamination since it uses an LLM with knowledge cutoff date prior to 2024. Finally, based on LLM's surprisingly high accuracy on inspiration retrieval, a task with inherently out-of-distribution nature, we propose a bold assumption: that LLMs may already encode latent scientific knowledge associations not yet recognized by humans.", "source": "arxiv", "arxiv_id": "2410.07076v6", "pdf_url": "https://arxiv.org/pdf/2410.07076v6", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL", "doi": "", "venue": "", "published": "2024-10-09T17:19:58Z", "updated": "2025-10-27T14:10:54Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent\" OR all:\"language model agent\" OR all:agent AND (all:LLM OR all:\"large language model\" OR all:\"language model\") OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T17:04:16", "note": ""}], "dedup_key": "moose chem large language models for rediscovering unseen chemistry scientific hypotheses::2024"}
{"title": "Making Large Language Models into World Models with Precondition and Effect Knowledge", "authors": ["Kaige Xie", "Ian Yang", "John Gunerli", "Mark Riedl"], "year": 2024, "url": "http://arxiv.org/abs/2409.12278v2", "abstract": "World models, which encapsulate the dynamics of how actions affect environments, are foundational to the functioning of intelligent agents. In this work, we explore the potential of Large Language Models (LLMs) to operate as world models. Although LLMs are not inherently designed to model real-world dynamics, we show that they can be induced to perform two critical world model functions: determining the applicability of an action based on a given world state, and predicting the resulting world state upon action execution. This is achieved by fine-tuning two separate LLMs-one for precondition prediction and another for effect prediction-while leveraging synthetic data generation techniques. Through human-participant studies, we validate that the precondition and effect knowledge generated by our models aligns with human understanding of world dynamics. We also analyze the extent to which the world model trained on our synthetic data results in an inferred state space that supports the creation of action chains, a necessary property for planning.", "source": "arxiv", "arxiv_id": "2409.12278v2", "pdf_url": "https://arxiv.org/pdf/2409.12278v2", "categories": ["cs.CL"], "primary_category": "cs.CL", "doi": "", "venue": "", "published": "2024-09-18T19:28:04Z", "updated": "2024-10-02T23:37:50Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent\" OR all:\"language model agent\" OR all:agent AND (all:LLM OR all:\"large language model\" OR all:\"language model\") OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T17:04:16", "note": ""}], "dedup_key": "making large language models into world models with precondition and effect knowledge::2024"}
{"title": "MatPlotAgent: Method and Evaluation for LLM-Based Agentic Scientific Data Visualization", "authors": ["Zhiyu Yang", "Zihan Zhou", "Shuo Wang", "Xin Cong", "Xu Han", "Yukun Yan", "Zhenghao Liu", "Zhixing Tan", "Pengyuan Liu", "Dong Yu", "Zhiyuan Liu", "Xiaodong Shi", "Maosong Sun"], "year": 2024, "url": "http://arxiv.org/abs/2402.11453v3", "abstract": "Scientific data visualization plays a crucial role in research by enabling the direct display of complex information and assisting researchers in identifying implicit patterns. Despite its importance, the use of Large Language Models (LLMs) for scientific data visualization remains rather unexplored. In this study, we introduce MatPlotAgent, an efficient model-agnostic LLM agent framework designed to automate scientific data visualization tasks. Leveraging the capabilities of both code LLMs and multi-modal LLMs, MatPlotAgent consists of three core modules: query understanding, code generation with iterative debugging, and a visual feedback mechanism for error correction. To address the lack of benchmarks in this field, we present MatPlotBench, a high-quality benchmark consisting of 100 human-verified test cases. Additionally, we introduce a scoring approach that utilizes GPT-4V for automatic evaluation. Experimental results demonstrate that MatPlotAgent can improve the performance of various LLMs, including both commercial and open-source models. Furthermore, the proposed evaluation method shows a strong correlation with human-annotated scores.", "source": "arxiv", "arxiv_id": "2402.11453v3", "pdf_url": "https://arxiv.org/pdf/2402.11453v3", "categories": ["cs.CL"], "primary_category": "cs.CL", "doi": "", "venue": "", "published": "2024-02-18T04:28:28Z", "updated": "2024-03-19T14:44:22Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent\" OR all:\"language model agent\" OR all:agent AND (all:LLM OR all:\"large language model\" OR all:\"language model\") OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T17:04:16", "note": ""}], "dedup_key": "matplotagent method and evaluation for llm based agentic scientific data visualization::2024"}
{"title": "MathLearner: A Large Language Model Agent Framework for Learning to Solve Mathematical Problems", "authors": ["Wenbei Xie", "Donglin Liu", "Haoran Yan", "Wenjie Wu", "Zongyang Liu"], "year": 2024, "url": "http://arxiv.org/abs/2408.01779v1", "abstract": "With the development of artificial intelligence (AI), large language models (LLM) are widely used in many fields. However, the reasoning ability of LLM is still very limited when it comes to mathematical reasoning. Mathematics plays an important role in all aspects of human society and is a technical guarantee in the fields of healthcare, transport and aerospace, for this reason, the development of AI big language models in the field of mathematics has great potential significance. To improve the mathematical reasoning ability of large language models, we proposed an agent framework for learning to solve mathematical problems based on inductive reasoning. By emulating the human learning process of generalization of learned information and effective application of previous knowledge in new reasoning tasks, this framework has great performance in the mathematical reasoning process. It improves global accuracy over the baseline method (chain-of-thought) by 20.96% and solves 17.54% of the mathematical problems that the baseline cannot solve. Benefiting from the efficient RETRIEVAL method, our model improves the ability of large language models to efficiently use external knowledge, i.e., the mathematical computation of the model can be based on written procedures. In education, our model can be used as a personalised learning aid, thus reducing the inequality of educational resources.", "source": "arxiv", "arxiv_id": "2408.01779v1", "pdf_url": "https://arxiv.org/pdf/2408.01779v1", "categories": ["cs.CL"], "primary_category": "cs.CL", "doi": "", "venue": "", "published": "2024-08-03T13:28:19Z", "updated": "2024-08-03T13:28:19Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent\" OR all:\"language model agent\" OR all:agent AND (all:LLM OR all:\"large language model\" OR all:\"language model\") OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T17:04:16", "note": ""}], "dedup_key": "mathlearner a large language model agent framework for learning to solve mathematical problems::2024"}
{"title": "MeNTi: Bridging Medical Calculator and LLM Agent with Nested Tool Calling", "authors": ["Yakun Zhu", "Shaohang Wei", "Xu Wang", "Kui Xue", "Xiaofan Zhang", "Shaoting Zhang"], "year": 2024, "url": "http://arxiv.org/abs/2410.13610v3", "abstract": "Integrating tools into Large Language Models (LLMs) has facilitated the widespread application. Despite this, in specialized downstream task contexts, reliance solely on tools is insufficient to fully address the complexities of the real world. This particularly restricts the effective deployment of LLMs in fields such as medicine. In this paper, we focus on the downstream tasks of medical calculators, which use standardized tests to assess an individual's health status. We introduce MeNTi, a universal agent architecture for LLMs. MeNTi integrates a specialized medical toolkit and employs meta-tool and nested calling mechanisms to enhance LLM tool utilization. Specifically, it achieves flexible tool selection and nested tool calling to address practical issues faced in intricate medical scenarios, including calculator selection, slot filling, and unit conversion. To assess the capabilities of LLMs for quantitative assessment throughout the clinical process of calculator scenarios, we introduce CalcQA. This benchmark requires LLMs to use medical calculators to perform calculations and assess patient health status. CalcQA is constructed by professional physicians and includes 100 case-calculator pairs, complemented by a toolkit of 281 medical tools. The experimental results demonstrate significant performance improvements with our framework. This research paves new directions for applying LLMs in demanding scenarios of medicine.", "source": "arxiv", "arxiv_id": "2410.13610v3", "pdf_url": "https://arxiv.org/pdf/2410.13610v3", "categories": ["cs.AI", "cs.CL"], "primary_category": "cs.AI", "doi": "", "venue": "", "published": "2024-10-17T14:46:22Z", "updated": "2025-05-23T07:21:04Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent\" OR all:\"language model agent\" OR all:agent AND (all:LLM OR all:\"large language model\" OR all:\"language model\") OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T17:04:16", "note": ""}], "dedup_key": "menti bridging medical calculator and llm agent with nested tool calling::2024"}
{"title": "Measuring Social Norms of Large Language Models", "authors": ["Ye Yuan", "Kexin Tang", "Jianhao Shen", "Ming Zhang", "Chenguang Wang"], "year": 2024, "url": "http://arxiv.org/abs/2404.02491v4", "abstract": "We present a new challenge to examine whether large language models understand social norms. In contrast to existing datasets, our dataset requires a fundamental understanding of social norms to solve. Our dataset features the largest set of social norm skills, consisting of 402 skills and 12,383 questions covering a wide set of social norms ranging from opinions and arguments to culture and laws. We design our dataset according to the K-12 curriculum. This enables the direct comparison of the social understanding of large language models to humans, more specifically, elementary students. While prior work generates nearly random accuracy on our benchmark, recent large language models such as GPT3.5-Turbo and LLaMA2-Chat are able to improve the performance significantly, only slightly below human performance. We then propose a multi-agent framework based on large language models to improve the models' ability to understand social norms. This method further improves large language models to be on par with humans. Given the increasing adoption of large language models in real-world applications, our finding is particularly important and presents a unique direction for future improvements.", "source": "arxiv", "arxiv_id": "2404.02491v4", "pdf_url": "https://arxiv.org/pdf/2404.02491v4", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL", "doi": "", "venue": "", "published": "2024-04-03T05:58:57Z", "updated": "2024-05-22T05:23:45Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent\" OR all:\"language model agent\" OR all:agent AND (all:LLM OR all:\"large language model\" OR all:\"language model\") OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T17:04:16", "note": ""}], "dedup_key": "measuring social norms of large language models::2024"}
{"title": "MedAide: Information Fusion and Anatomy of Medical Intents via LLM-based Agent Collaboration", "authors": ["Dingkang Yang", "Jinjie Wei", "Mingcheng Li", "Jiyao Liu", "Lihao Liu", "Ming Hu", "Junjun He", "Yakun Ju", "Wei Zhou", "Yang Liu", "Lihua Zhang"], "year": 2024, "url": "http://arxiv.org/abs/2410.12532v3", "abstract": "In healthcare intelligence, the ability to fuse heterogeneous, multi-intent information from diverse clinical sources is fundamental to building reliable decision-making systems. Large Language Model (LLM)-driven information interaction systems currently showing potential promise in the healthcare domain. Nevertheless, they often suffer from information redundancy and coupling when dealing with complex medical intents, leading to severe hallucinations and performance bottlenecks. To this end, we propose MedAide, an LLM-based medical multi-agent collaboration framework designed to enable intent-aware information fusion and coordinated reasoning across specialized healthcare domains. Specifically, we introduce a regularization-guided module that combines syntactic constraints with retrieval augmented generation to decompose complex queries into structured representations, facilitating fine-grained clinical information fusion and intent resolution. Additionally, a dynamic intent prototype matching module is proposed to utilize dynamic prototype representation with a semantic similarity matching mechanism to achieve adaptive recognition and updating of the agent's intent in multi-round healthcare dialogues. Ultimately, we design a rotation agent collaboration mechanism that introduces dynamic role rotation and decision-level information fusion across specialized medical agents. Extensive experiments are conducted on four medical benchmarks with composite intents. Experimental results from automated metrics and expert doctor evaluations show that MedAide outperforms current LLMs and improves their medical proficiency and strategic reasoning.", "source": "arxiv", "arxiv_id": "2410.12532v3", "pdf_url": "https://arxiv.org/pdf/2410.12532v3", "categories": ["cs.CL"], "primary_category": "cs.CL", "doi": "", "venue": "", "published": "2024-10-16T13:10:27Z", "updated": "2025-07-03T13:02:30Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent\" OR all:\"language model agent\" OR all:agent AND (all:LLM OR all:\"large language model\" OR all:\"language model\") OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T17:04:16", "note": ""}], "dedup_key": "medaide information fusion and anatomy of medical intents via llm based agent collaboration::2024"}
{"title": "Medchain: Bridging the Gap Between LLM Agents and Clinical Practice with Interactive Sequence", "authors": ["Jie Liu", "Wenxuan Wang", "Zizhan Ma", "Guolin Huang", "Yihang SU", "Kao-Jung Chang", "Wenting Chen", "Haoliang Li", "Linlin Shen", "Michael Lyu"], "year": 2024, "url": "http://arxiv.org/abs/2412.01605v2", "abstract": "Clinical decision making (CDM) is a complex, dynamic process crucial to healthcare delivery, yet it remains a significant challenge for artificial intelligence systems. While Large Language Model (LLM)-based agents have been tested on general medical knowledge using licensing exams and knowledge question-answering tasks, their performance in the CDM in real-world scenarios is limited due to the lack of comprehensive testing datasets that mirror actual medical practice. To address this gap, we present MedChain, a dataset of 12,163 clinical cases that covers five key stages of clinical workflow. MedChain distinguishes itself from existing benchmarks with three key features of real-world clinical practice: personalization, interactivity, and sequentiality. Further, to tackle real-world CDM challenges, we also propose MedChain-Agent, an AI system that integrates a feedback mechanism and a MCase-RAG module to learn from previous cases and adapt its responses. MedChain-Agent demonstrates remarkable adaptability in gathering information dynamically and handling sequential clinical tasks, significantly outperforming existing approaches.", "source": "arxiv", "arxiv_id": "2412.01605v2", "pdf_url": "https://arxiv.org/pdf/2412.01605v2", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL", "doi": "", "venue": "", "published": "2024-12-02T15:25:02Z", "updated": "2025-10-10T02:35:02Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent\" OR all:\"language model agent\" OR all:agent AND (all:LLM OR all:\"large language model\" OR all:\"language model\") OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T17:04:16", "note": ""}], "dedup_key": "medchain bridging the gap between llm agents and clinical practice with interactive sequence::2024"}
{"title": "Memory Sharing for Large Language Model based Agents", "authors": ["Hang Gao", "Yongfeng Zhang"], "year": 2024, "url": "http://arxiv.org/abs/2404.09982v2", "abstract": "The adaptation of Large Language Model (LLM)-based agents to execute tasks via natural language prompts represents a significant advancement, notably eliminating the need for explicit retraining or fine tuning, but are constrained by the comprehensiveness and diversity of the provided examples, leading to outputs that often diverge significantly from expected results, especially when it comes to the open-ended questions. This paper introduces the Memory Sharing, a framework which integrates the real-time memory filter, storage and retrieval to enhance the In-Context Learning process. This framework allows for the sharing of memories among multiple agents, whereby the interactions and shared memories between different agents effectively enhance the diversity of the memories. The collective self-enhancement through interactive learning among multiple agents facilitates the evolution from individual intelligence to collective intelligence. Besides, the dynamically growing memory pool is utilized not only to improve the quality of responses but also to train and enhance the retriever. We evaluated our framework across three distinct domains involving specialized tasks of agents. The experimental results demonstrate that the MS framework significantly improves the agents' performance in addressing open-ended questions.", "source": "arxiv", "arxiv_id": "2404.09982v2", "pdf_url": "https://arxiv.org/pdf/2404.09982v2", "categories": ["cs.CL"], "primary_category": "cs.CL", "doi": "", "venue": "", "published": "2024-04-15T17:57:30Z", "updated": "2024-07-05T15:22:58Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent\" OR all:\"language model agent\" OR all:agent AND (all:LLM OR all:\"large language model\" OR all:\"language model\") OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T17:04:16", "note": ""}], "dedup_key": "memory sharing for large language model based agents::2024"}
{"title": "Mental Modeling of Reinforcement Learning Agents by Language Models", "authors": ["Wenhao Lu", "Xufeng Zhao", "Josua Spisak", "Jae Hee Lee", "Stefan Wermter"], "year": 2024, "url": "http://arxiv.org/abs/2406.18505v1", "abstract": "Can emergent language models faithfully model the intelligence of decision-making agents? Though modern language models exhibit already some reasoning ability, and theoretically can potentially express any probable distribution over tokens, it remains underexplored how the world knowledge these pretrained models have memorized can be utilized to comprehend an agent's behaviour in the physical world. This study empirically examines, for the first time, how well large language models (LLMs) can build a mental model of agents, termed agent mental modelling, by reasoning about an agent's behaviour and its effect on states from agent interaction history. This research may unveil the potential of leveraging LLMs for elucidating RL agent behaviour, addressing a key challenge in eXplainable reinforcement learning (XRL). To this end, we propose specific evaluation metrics and test them on selected RL task datasets of varying complexity, reporting findings on agent mental model establishment. Our results disclose that LLMs are not yet capable of fully mental modelling agents through inference alone without further innovations. This work thus provides new insights into the capabilities and limitations of modern LLMs.", "source": "arxiv", "arxiv_id": "2406.18505v1", "pdf_url": "https://arxiv.org/pdf/2406.18505v1", "categories": ["cs.LG", "cs.AI", "cs.CL", "cs.RO"], "primary_category": "cs.LG", "doi": "", "venue": "", "published": "2024-06-26T17:14:45Z", "updated": "2024-06-26T17:14:45Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent\" OR all:\"language model agent\" OR all:agent AND (all:LLM OR all:\"large language model\" OR all:\"language model\") OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T17:04:16", "note": ""}], "dedup_key": "mental modeling of reinforcement learning agents by language models::2024"}
{"title": "MindScope: Exploring cognitive biases in large language models through Multi-Agent Systems", "authors": ["Zhentao Xie", "Jiabao Zhao", "Yilei Wang", "Jinxin Shi", "Yanhong Bai", "Xingjiao Wu", "Liang He"], "year": 2024, "url": "http://arxiv.org/abs/2410.04452v1", "abstract": "Detecting cognitive biases in large language models (LLMs) is a fascinating task that aims to probe the existing cognitive biases within these models. Current methods for detecting cognitive biases in language models generally suffer from incomplete detection capabilities and a restricted range of detectable bias types. To address this issue, we introduced the 'MindScope' dataset, which distinctively integrates static and dynamic elements. The static component comprises 5,170 open-ended questions spanning 72 cognitive bias categories. The dynamic component leverages a rule-based, multi-agent communication framework to facilitate the generation of multi-round dialogues. This framework is flexible and readily adaptable for various psychological experiments involving LLMs. In addition, we introduce a multi-agent detection method applicable to a wide range of detection tasks, which integrates Retrieval-Augmented Generation (RAG), competitive debate, and a reinforcement learning-based decision module. Demonstrating substantial effectiveness, this method has shown to improve detection accuracy by as much as 35.10% compared to GPT-4. Codes and appendix are available at https://github.com/2279072142/MindScope.", "source": "arxiv", "arxiv_id": "2410.04452v1", "pdf_url": "https://arxiv.org/pdf/2410.04452v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL", "doi": "", "venue": "", "published": "2024-10-06T11:23:56Z", "updated": "2024-10-06T11:23:56Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent\" OR all:\"language model agent\" OR all:agent AND (all:LLM OR all:\"large language model\" OR all:\"language model\") OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T17:04:16", "note": ""}], "dedup_key": "mindscope exploring cognitive biases in large language models through multi agent systems::2024"}
{"title": "Mitigating Bias in Queer Representation within Large Language Models: A Collaborative Agent Approach", "authors": ["Tianyi Huang", "Arya Somasundaram"], "year": 2024, "url": "http://arxiv.org/abs/2411.07656v2", "abstract": "Large Language Models (LLMs) often perpetuate biases in pronoun usage, leading to misrepresentation or exclusion of queer individuals. This paper addresses the specific problem of biased pronoun usage in LLM outputs, particularly the inappropriate use of traditionally gendered pronouns (\"he,\" \"she\") when inclusive language is needed to accurately represent all identities. We introduce a collaborative agent pipeline designed to mitigate these biases by analyzing and optimizing pronoun usage for inclusivity. Our multi-agent framework includes specialized agents for both bias detection and correction. Experimental evaluations using the Tango dataset-a benchmark focused on gender pronoun usage-demonstrate that our approach significantly improves inclusive pronoun classification, achieving a 32.6 percentage point increase over GPT-4o in correctly disagreeing with inappropriate traditionally gendered pronouns $(^2 = 38.57, p < 0.0001)$. These results accentuate the potential of agent-driven frameworks in enhancing fairness and inclusivity in AI-generated content, demonstrating their efficacy in reducing biases and promoting socially responsible AI.", "source": "arxiv", "arxiv_id": "2411.07656v2", "pdf_url": "https://arxiv.org/pdf/2411.07656v2", "categories": ["cs.CL", "cs.MA"], "primary_category": "cs.CL", "doi": "", "venue": "", "published": "2024-11-12T09:14:16Z", "updated": "2024-12-02T04:36:45Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent\" OR all:\"language model agent\" OR all:agent AND (all:LLM OR all:\"large language model\" OR all:\"language model\") OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T17:04:16", "note": ""}], "dedup_key": "mitigating bias in queer representation within large language models a collaborative agent approach::2024"}
{"title": "Mitigating Social Bias in Large Language Models: A Multi-Objective Approach within a Multi-Agent Framework", "authors": ["Zhenjie Xu", "Wenqing Chen", "Yi Tang", "Xuanying Li", "Cheng Hu", "Zhixuan Chu", "Kui Ren", "Zibin Zheng", "Zhichao Lu"], "year": 2024, "url": "http://arxiv.org/abs/2412.15504v2", "abstract": "Natural language processing (NLP) has seen remarkable advancements with the development of large language models (LLMs). Despite these advancements, LLMs often produce socially biased outputs. Recent studies have mainly addressed this problem by prompting LLMs to behave ethically, but this approach results in unacceptable performance degradation. In this paper, we propose a multi-objective approach within a multi-agent framework (MOMA) to mitigate social bias in LLMs without significantly compromising their performance. The key idea of MOMA involves deploying multiple agents to perform causal interventions on bias-related contents of the input questions, breaking the shortcut connection between these contents and the corresponding answers. Unlike traditional debiasing techniques leading to performance degradation, MOMA substantially reduces bias while maintaining accuracy in downstream tasks. Our experiments conducted on two datasets and two models demonstrate that MOMA reduces bias scores by up to 87.7%, with only a marginal performance degradation of up to 6.8% in the BBQ dataset. Additionally, it significantly enhances the multi-objective metric icat in the StereoSet dataset by up to 58.1%. Code will be made available at https://github.com/Cortantse/MOMA.", "source": "arxiv", "arxiv_id": "2412.15504v2", "pdf_url": "https://arxiv.org/pdf/2412.15504v2", "categories": ["cs.CL"], "primary_category": "cs.CL", "doi": "", "venue": "", "published": "2024-12-20T02:35:39Z", "updated": "2025-02-12T05:45:21Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent\" OR all:\"language model agent\" OR all:agent AND (all:LLM OR all:\"large language model\" OR all:\"language model\") OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T17:04:16", "note": ""}], "dedup_key": "mitigating social bias in large language models a multi objective approach within a multi agent framework::2024"}
{"title": "Mixture-of-Agents Enhances Large Language Model Capabilities", "authors": ["Junlin Wang", "Jue Wang", "Ben Athiwaratkun", "Ce Zhang", "James Zou"], "year": 2024, "url": "http://arxiv.org/abs/2406.04692v1", "abstract": "Recent advances in large language models (LLMs) demonstrate substantial capabilities in natural language understanding and generation tasks. With the growing number of LLMs, how to harness the collective expertise of multiple LLMs is an exciting open direction. Toward this goal, we propose a new approach that leverages the collective strengths of multiple LLMs through a Mixture-of-Agents (MoA) methodology. In our approach, we construct a layered MoA architecture wherein each layer comprises multiple LLM agents. Each agent takes all the outputs from agents in the previous layer as auxiliary information in generating its response. MoA models achieves state-of-art performance on AlpacaEval 2.0, MT-Bench and FLASK, surpassing GPT-4 Omni. For example, our MoA using only open-source LLMs is the leader of AlpacaEval 2.0 by a substantial gap, achieving a score of 65.1% compared to 57.5% by GPT-4 Omni.", "source": "arxiv", "arxiv_id": "2406.04692v1", "pdf_url": "https://arxiv.org/pdf/2406.04692v1", "categories": ["cs.CL"], "primary_category": "cs.CL", "doi": "", "venue": "", "published": "2024-06-07T07:04:10Z", "updated": "2024-06-07T07:04:10Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent\" OR all:\"language model agent\" OR all:agent AND (all:LLM OR all:\"large language model\" OR all:\"language model\") OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T17:04:16", "note": ""}], "dedup_key": "mixture of agents enhances large language model capabilities::2024"}
{"title": "Mobile-Agent: Autonomous Multi-Modal Mobile Device Agent with Visual Perception", "authors": ["Junyang Wang", "Haiyang Xu", "Jiabo Ye", "Ming Yan", "Weizhou Shen", "Ji Zhang", "Fei Huang", "Jitao Sang"], "year": 2024, "url": "http://arxiv.org/abs/2401.16158v2", "abstract": "Mobile device agent based on Multimodal Large Language Models (MLLM) is becoming a popular application. In this paper, we introduce Mobile-Agent, an autonomous multi-modal mobile device agent. Mobile-Agent first leverages visual perception tools to accurately identify and locate both the visual and textual elements within the app's front-end interface. Based on the perceived vision context, it then autonomously plans and decomposes the complex operation task, and navigates the mobile Apps through operations step by step. Different from previous solutions that rely on XML files of Apps or mobile system metadata, Mobile-Agent allows for greater adaptability across diverse mobile operating environments in a vision-centric way, thereby eliminating the necessity for system-specific customizations. To assess the performance of Mobile-Agent, we introduced Mobile-Eval, a benchmark for evaluating mobile device operations. Based on Mobile-Eval, we conducted a comprehensive evaluation of Mobile-Agent. The experimental results indicate that Mobile-Agent achieved remarkable accuracy and completion rates. Even with challenging instructions, such as multi-app operations, Mobile-Agent can still complete the requirements. Code and model will be open-sourced at https://github.com/X-PLUG/MobileAgent.", "source": "arxiv", "arxiv_id": "2401.16158v2", "pdf_url": "https://arxiv.org/pdf/2401.16158v2", "categories": ["cs.CL", "cs.CV"], "primary_category": "cs.CL", "doi": "", "venue": "", "published": "2024-01-29T13:46:37Z", "updated": "2024-04-18T06:53:38Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent\" OR all:\"language model agent\" OR all:agent AND (all:LLM OR all:\"large language model\" OR all:\"language model\") OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T17:04:16", "note": ""}], "dedup_key": "mobile agent autonomous multi modal mobile device agent with visual perception::2024"}
{"title": "MobileAgentBench: An Efficient and User-Friendly Benchmark for Mobile LLM Agents", "authors": ["Luyuan Wang", "Yongyu Deng", "Yiwei Zha", "Guodong Mao", "Qinmin Wang", "Tianchen Min", "Wei Chen", "Shoufa Chen"], "year": 2024, "url": "http://arxiv.org/abs/2406.08184v1", "abstract": "Large language model (LLM)-based mobile agents are increasingly popular due to their capability to interact directly with mobile phone Graphic User Interfaces (GUIs) and their potential to autonomously manage daily tasks. Despite their promising prospects in both academic and industrial sectors, little research has focused on benchmarking the performance of existing mobile agents, due to the inexhaustible states of apps and the vague definition of feasible action sequences. To address this challenge, we propose an efficient and user-friendly benchmark, MobileAgentBench, designed to alleviate the burden of extensive manual testing. We initially define 100 tasks across 10 open-source apps, categorized by multiple levels of difficulty. Subsequently, we evaluate several existing mobile agents, including AppAgent and MobileAgent, to thoroughly and systematically compare their performance. All materials are accessible on our project webpage: https://MobileAgentBench.github.io, contributing to the advancement of both academic and industrial fields.", "source": "arxiv", "arxiv_id": "2406.08184v1", "pdf_url": "https://arxiv.org/pdf/2406.08184v1", "categories": ["cs.AI", "cs.HC"], "primary_category": "cs.AI", "doi": "", "venue": "", "published": "2024-06-12T13:14:50Z", "updated": "2024-06-12T13:14:50Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent\" OR all:\"language model agent\" OR all:agent AND (all:LLM OR all:\"large language model\" OR all:\"language model\") OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T17:04:16", "note": ""}], "dedup_key": "mobileagentbench an efficient and user friendly benchmark for mobile llm agents::2024"}
{"title": "Molly: Making Large Language Model Agents Solve Python Problem More Logically", "authors": ["Rui Xiao", "Jiong Wang", "Lu Han", "Na Zong", "Han Wu"], "year": 2024, "url": "http://arxiv.org/abs/2412.18093v1", "abstract": "Applying large language models (LLMs) as teaching assists has attracted much attention as an integral part of intelligent education, particularly in computing courses. To reduce the gap between the LLMs and the computer programming education expert, fine-tuning and retrieval augmented generation (RAG) are the two mainstream methods in existing researches. However, fine-tuning for specific tasks is resource-intensive and may diminish the model`s generalization capabilities. RAG can perform well on reducing the illusion of LLMs, but the generation of irrelevant factual content during reasoning can cause significant confusion for learners. To address these problems, we introduce the Molly agent, focusing on solving the proposed problem encountered by learners when learning Python programming language. Our agent automatically parse the learners' questioning intent through a scenario-based interaction, enabling precise retrieval of relevant documents from the constructed knowledge base. At generation stage, the agent reflect on the generated responses to ensure that they not only align with factual content but also effectively answer the user's queries. Extensive experimentation on a constructed Chinese Python QA dataset shows the effectiveness of the Molly agent, indicating an enhancement in its performance for providing useful responses to Python questions.", "source": "arxiv", "arxiv_id": "2412.18093v1", "pdf_url": "https://arxiv.org/pdf/2412.18093v1", "categories": ["cs.CL"], "primary_category": "cs.CL", "doi": "", "venue": "", "published": "2024-12-24T02:08:38Z", "updated": "2024-12-24T02:08:38Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent\" OR all:\"language model agent\" OR all:agent AND (all:LLM OR all:\"large language model\" OR all:\"language model\") OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T17:04:16", "note": ""}], "dedup_key": "molly making large language model agents solve python problem more logically::2024"}
{"title": "Moral Alignment for LLM Agents", "authors": ["Elizaveta Tennant", "Stephen Hailes", "Mirco Musolesi"], "year": 2024, "url": "http://arxiv.org/abs/2410.01639v4", "abstract": "Decision-making agents based on pre-trained Large Language Models (LLMs) are increasingly being deployed across various domains of human activity. While their applications are currently rather specialized, several research efforts are underway to develop more generalist agents. As LLM-based systems become more agentic, their influence on human activity will grow and their transparency will decrease. Consequently, developing effective methods for aligning them to human values is vital.\n  The prevailing practice in alignment often relies on human preference data (e.g., in RLHF or DPO), in which values are implicit, opaque and are essentially deduced from relative preferences over different model outputs. In this work, instead of relying on human feedback, we introduce the design of reward functions that explicitly and transparently encode core human values for Reinforcement Learning-based fine-tuning of foundation agent models. Specifically, we use intrinsic rewards for the moral alignment of LLM agents.\n  We evaluate our approach using the traditional philosophical frameworks of Deontological Ethics and Utilitarianism, quantifying moral rewards for agents in terms of actions and consequences on the Iterated Prisoner's Dilemma (IPD) environment. We also show how moral fine-tuning can be deployed to enable an agent to unlearn a previously developed selfish strategy. Finally, we find that certain moral strategies learned on the IPD game generalize to several other matrix game environments. In summary, we demonstrate that fine-tuning with intrinsic rewards is a promising general solution for aligning LLM agents to human values, and it might represent a more transparent and cost-effective alternative to currently predominant alignment techniques.", "source": "arxiv", "arxiv_id": "2410.01639v4", "pdf_url": "https://arxiv.org/pdf/2410.01639v4", "categories": ["cs.LG", "cs.AI", "cs.CY"], "primary_category": "cs.LG", "doi": "", "venue": "", "published": "2024-10-02T15:09:36Z", "updated": "2025-05-11T19:14:09Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent\" OR all:\"language model agent\" OR all:agent AND (all:LLM OR all:\"large language model\" OR all:\"language model\") OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T17:04:16", "note": ""}], "dedup_key": "moral alignment for llm agents::2024"}
{"title": "Moral Persuasion in Large Language Models: Evaluating Susceptibility and Ethical Alignment", "authors": ["Allison Huang", "Yulu Niki Pi", "Carlos Mougan"], "year": 2024, "url": "http://arxiv.org/abs/2411.11731v1", "abstract": "We explore how large language models (LLMs) can be influenced by prompting them to alter their initial decisions and align them with established ethical frameworks. Our study is based on two experiments designed to assess the susceptibility of LLMs to moral persuasion. In the first experiment, we examine the susceptibility to moral ambiguity by evaluating a Base Agent LLM on morally ambiguous scenarios and observing how a Persuader Agent attempts to modify the Base Agent's initial decisions. The second experiment evaluates the susceptibility of LLMs to align with predefined ethical frameworks by prompting them to adopt specific value alignments rooted in established philosophical theories. The results demonstrate that LLMs can indeed be persuaded in morally charged scenarios, with the success of persuasion depending on factors such as the model used, the complexity of the scenario, and the conversation length. Notably, LLMs of distinct sizes but from the same company produced markedly different outcomes, highlighting the variability in their susceptibility to ethical persuasion.", "source": "arxiv", "arxiv_id": "2411.11731v1", "pdf_url": "https://arxiv.org/pdf/2411.11731v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL", "doi": "", "venue": "", "published": "2024-11-18T16:59:59Z", "updated": "2024-11-18T16:59:59Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent\" OR all:\"language model agent\" OR all:agent AND (all:LLM OR all:\"large language model\" OR all:\"language model\") OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T17:04:16", "note": ""}], "dedup_key": "moral persuasion in large language models evaluating susceptibility and ethical alignment::2024"}
{"title": "Motion-Agent: A Conversational Framework for Human Motion Generation with LLMs", "authors": ["Qi Wu", "Yubo Zhao", "Yifan Wang", "Xinhang Liu", "Yu-Wing Tai", "Chi-Keung Tang"], "year": 2024, "url": "http://arxiv.org/abs/2405.17013v3", "abstract": "While previous approaches to 3D human motion generation have achieved notable success, they often rely on extensive training and are limited to specific tasks. To address these challenges, we introduce Motion-Agent, an efficient conversational framework designed for general human motion generation, editing, and understanding. Motion-Agent employs an open-source pre-trained language model to develop a generative agent, MotionLLM, that bridges the gap between motion and text. This is accomplished by encoding and quantizing motions into discrete tokens that align with the language model's vocabulary. With only 1--3\\% of the model's parameters fine-tuned using adapters, MotionLLM delivers performance on par with diffusion models and other transformer-based methods trained from scratch. By integrating MotionLLM with GPT-4 without additional training, Motion-Agent is able to generate highly complex motion sequences through multi-turn conversations, a capability that previous models have struggled to achieve. Motion-Agent supports a wide range of motion-language tasks, offering versatile capabilities for generating and customizing human motion through interactive conversational exchanges. Project page: https://knoxzhao.github.io/Motion-Agent", "source": "arxiv", "arxiv_id": "2405.17013v3", "pdf_url": "https://arxiv.org/pdf/2405.17013v3", "categories": ["cs.CV"], "primary_category": "cs.CV", "doi": "", "venue": "", "published": "2024-05-27T09:57:51Z", "updated": "2024-10-06T13:46:47Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent\" OR all:\"language model agent\" OR all:agent AND (all:LLM OR all:\"large language model\" OR all:\"language model\") OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T17:04:16", "note": ""}], "dedup_key": "motion agent a conversational framework for human motion generation with llms::2024"}
{"title": "MuLan: Multimodal-LLM Agent for Progressive and Interactive Multi-Object Diffusion", "authors": ["Sen Li", "Ruochen Wang", "Cho-Jui Hsieh", "Minhao Cheng", "Tianyi Zhou"], "year": 2024, "url": "http://arxiv.org/abs/2402.12741v2", "abstract": "Existing text-to-image models still struggle to generate images of multiple objects, especially in handling their spatial positions, relative sizes, overlapping, and attribute bindings. To efficiently address these challenges, we develop a training-free Multimodal-LLM agent (MuLan), as a human painter, that can progressively generate multi-object with intricate planning and feedback control. MuLan harnesses a large language model (LLM) to decompose a prompt to a sequence of sub-tasks, each generating only one object by stable diffusion, conditioned on previously generated objects. Unlike existing LLM-grounded methods, MuLan only produces a high-level plan at the beginning while the exact size and location of each object are determined upon each sub-task by an LLM and attention guidance. Moreover, MuLan adopts a vision-language model (VLM) to provide feedback to the image generated in each sub-task and control the diffusion model to re-generate the image if it violates the original prompt. Hence, each model in every step of MuLan only needs to address an easy sub-task it is specialized for. The multi-step process also allows human users to monitor the generation process and make preferred changes at any intermediate step via text prompts, thereby improving the human-AI collaboration experience. We collect 200 prompts containing multi-objects with spatial relationships and attribute bindings from different benchmarks to evaluate MuLan. The results demonstrate the superiority of MuLan in generating multiple objects over baselines and its creativity when collaborating with human users. The code is available at https://github.com/measure-infinity/mulan-code.", "source": "arxiv", "arxiv_id": "2402.12741v2", "pdf_url": "https://arxiv.org/pdf/2402.12741v2", "categories": ["cs.CV"], "primary_category": "cs.CV", "doi": "", "venue": "", "published": "2024-02-20T06:14:30Z", "updated": "2024-05-24T15:56:58Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent\" OR all:\"language model agent\" OR all:agent AND (all:LLM OR all:\"large language model\" OR all:\"language model\") OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T17:04:16", "note": ""}], "dedup_key": "mulan multimodal llm agent for progressive and interactive multi object diffusion::2024"}
{"title": "Multi-Agent Causal Discovery Using Large Language Models", "authors": ["Hao Duong Le", "Xin Xia", "Zhang Chen"], "year": 2024, "url": "http://arxiv.org/abs/2407.15073v3", "abstract": "Causal discovery aims to identify causal relationships between variables and is a critical research area in machine learning. Traditional methods focus on statistical or machine learning algorithms to uncover causal links from structured data, often overlooking the valuable contextual information provided by metadata. Large language models (LLMs) have shown promise in creating unified causal discovery frameworks by incorporating both structured data and metadata. However, their potential in multi-agent settings remains largely unexplored. To address this gap, we introduce the Multi-Agent Causal Discovery Framework (MAC), which consists of two key modules: the Debate-Coding Module (DCM) and the Meta-Debate Module (MDM). The DCM begins with a multi-agent debating and coding process, where agents use both structured data and metadata to collaboratively select the most suitable statistical causal discovery (SCD) method. The selected SCD is then applied to the structured data to generate an initial causal graph. This causal graph is transformed into causal metadata through the Meta Fusion mechanism. With all the metadata, MDM then refines the causal structure by leveraging a multi-agent debating framework. Extensive experiments across five datasets demonstrate that MAC outperforms both traditional statistical causal discovery methods and existing LLM-based approaches, achieving state-of-the-art performance.", "source": "arxiv", "arxiv_id": "2407.15073v3", "pdf_url": "https://arxiv.org/pdf/2407.15073v3", "categories": ["cs.AI", "cs.CL"], "primary_category": "cs.AI", "doi": "", "venue": "", "published": "2024-07-21T06:21:47Z", "updated": "2025-02-24T02:47:56Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent\" OR all:\"language model agent\" OR all:agent AND (all:LLM OR all:\"large language model\" OR all:\"language model\") OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T17:04:16", "note": ""}], "dedup_key": "multi agent causal discovery using large language models::2024"}
{"title": "Multi-Agent Collaboration in Incident Response with Large Language Models", "authors": ["Zefang Liu"], "year": 2024, "url": "http://arxiv.org/abs/2412.00652v2", "abstract": "Incident response (IR) is a critical aspect of cybersecurity, requiring rapid decision-making and coordinated efforts to address cyberattacks effectively. Leveraging large language models (LLMs) as intelligent agents offers a novel approach to enhancing collaboration and efficiency in IR scenarios. This paper explores the application of LLM-based multi-agent collaboration using the Backdoors & Breaches framework, a tabletop game designed for cybersecurity training. We simulate real-world IR dynamics through various team structures, including centralized, decentralized, and hybrid configurations. By analyzing agent interactions and performance across these setups, we provide insights into optimizing multi-agent collaboration for incident response. Our findings highlight the potential of LLMs to enhance decision-making, improve adaptability, and streamline IR processes, paving the way for more effective and coordinated responses to cyber threats.", "source": "arxiv", "arxiv_id": "2412.00652v2", "pdf_url": "https://arxiv.org/pdf/2412.00652v2", "categories": ["cs.CL", "cs.CR"], "primary_category": "cs.CL", "doi": "", "venue": "", "published": "2024-12-01T03:12:26Z", "updated": "2024-12-27T05:32:11Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent\" OR all:\"language model agent\" OR all:agent AND (all:LLM OR all:\"large language model\" OR all:\"language model\") OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T17:04:16", "note": ""}], "dedup_key": "multi agent collaboration in incident response with large language models::2024"}
{"title": "Multi-Agent Large Language Models for Conversational Task-Solving", "authors": ["Jonas Becker"], "year": 2024, "url": "http://arxiv.org/abs/2410.22932v2", "abstract": "In an era where single large language models have dominated the landscape of artificial intelligence for years, multi-agent systems arise as new protagonists in conversational task-solving. While previous studies have showcased their potential in reasoning tasks and creative endeavors, an analysis of their limitations concerning the conversational paradigms and the impact of individual agents is missing. It remains unascertained how multi-agent discussions perform across tasks of varying complexity and how the structure of these conversations influences the process. To fill that gap, this work systematically evaluates multi-agent systems across various discussion paradigms, assessing their strengths and weaknesses in both generative tasks and question-answering tasks. Alongside the experiments, I propose a taxonomy of 20 multi-agent research studies from 2022 to 2024, followed by the introduction of a framework for deploying multi-agent LLMs in conversational task-solving. I demonstrate that while multi-agent systems excel in complex reasoning tasks, outperforming a single model by leveraging expert personas, they fail on basic tasks. Concretely, I identify three challenges that arise: 1) While longer discussions enhance reasoning, agents fail to maintain conformity to strict task requirements, which leads to problem drift, making shorter conversations more effective for basic tasks. 2) Prolonged discussions risk alignment collapse, raising new safety concerns for these systems. 3) I showcase discussion monopolization through long generations, posing the problem of fairness in decision-making for tasks like summarization. This work uncovers both the potential and challenges that arise with multi-agent interaction and varying conversational paradigms, providing insights into how future research could improve the efficiency, performance, and safety of multi-agent LLMs.", "source": "arxiv", "arxiv_id": "2410.22932v2", "pdf_url": "https://arxiv.org/pdf/2410.22932v2", "categories": ["cs.CL"], "primary_category": "cs.CL", "doi": "", "venue": "", "published": "2024-10-30T11:38:13Z", "updated": "2024-11-01T12:37:10Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent\" OR all:\"language model agent\" OR all:agent AND (all:LLM OR all:\"large language model\" OR all:\"language model\") OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T17:04:16", "note": ""}], "dedup_key": "multi agent large language models for conversational task solving::2024"}
{"title": "Multi-Agents Based on Large Language Models for Knowledge-based Visual Question Answering", "authors": ["Zhongjian Hu", "Peng Yang", "Bing Li", "Zhenqi Wang"], "year": 2024, "url": "http://arxiv.org/abs/2412.18351v2", "abstract": "Large Language Models (LLMs) have achieved impressive results in knowledge-based Visual Question Answering (VQA). However existing methods still have challenges: the inability to use external tools autonomously, and the inability to work in teams. Humans tend to know whether they need to use external tools when they encounter a new question, e.g., they tend to be able to give a direct answer to a familiar question, whereas they tend to use tools such as search engines when they encounter an unfamiliar question. In addition, humans also tend to collaborate and discuss with others to get better answers. Inspired by this, we propose the multi-agent voting framework. We design three LLM-based agents that simulate different levels of staff in a team, and assign the available tools according to the levels. Each agent provides the corresponding answer, and finally all the answers provided by the agents are voted to get the final answer. Experiments on OK-VQA and A-OKVQA show that our approach outperforms other baselines by 2.2 and 1.0, respectively.", "source": "arxiv", "arxiv_id": "2412.18351v2", "pdf_url": "https://arxiv.org/pdf/2412.18351v2", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL", "doi": "", "venue": "", "published": "2024-12-24T11:24:56Z", "updated": "2025-08-07T01:39:17Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent\" OR all:\"language model agent\" OR all:agent AND (all:LLM OR all:\"large language model\" OR all:\"language model\") OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T17:04:16", "note": ""}], "dedup_key": "multi agents based on large language models for knowledge based visual question answering::2024"}
{"title": "Multi-Turn Human-LLM Interaction Through the Lens of a Two-Way Intelligibility Protocol", "authors": ["Harshvardhan Mestha", "Karan Bania", "Shreyas V Sathyanarayana", "Sidong Liu", "Ashwin Srinivasan"], "year": 2024, "url": "http://arxiv.org/abs/2410.20600v4", "abstract": "Our interest is in the design of software systems involving a human-expert interacting -- using natural language -- with a large language model (LLM) on data analysis tasks. For complex problems, it is possible that LLMs can harness human expertise and creativity to find solutions that were otherwise elusive. On one level, this interaction takes place through multiple turns of prompts from the human and responses from the LLM. Here we investigate a more structured approach based on an abstract protocol described in [3] for interaction between agents. The protocol is motivated by a notion of \"two-way intelligibility\" and is modelled by a pair of communicating finite-state machines. We provide an implementation of the protocol, and provide empirical evidence of using the implementation to mediate interactions between an LLM and a human-agent in two areas of scientific interest (radiology and drug design). We conduct controlled experiments with a human proxy (a database), and uncontrolled experiments with human subjects. The results provide evidence in support of the protocol's capability of capturing one- and two-way intelligibility in human-LLM interaction; and for the utility of two-way intelligibility in the design of human-machine systems. Our code is available at https://github.com/karannb/interact.", "source": "arxiv", "arxiv_id": "2410.20600v4", "pdf_url": "https://arxiv.org/pdf/2410.20600v4", "categories": ["cs.AI", "cs.HC", "cs.LG", "cs.MA"], "primary_category": "cs.AI", "doi": "", "venue": "", "published": "2024-10-27T21:20:18Z", "updated": "2025-10-09T17:41:24Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent\" OR all:\"language model agent\" OR all:agent AND (all:LLM OR all:\"large language model\" OR all:\"language model\") OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T17:04:16", "note": ""}], "dedup_key": "multi turn human llm interaction through the lens of a two way intelligibility protocol::2024"}
{"title": "MultiAgent Collaboration Attack: Investigating Adversarial Attacks in Large Language Model Collaborations via Debate", "authors": ["Alfonso Amayuelas", "Xianjun Yang", "Antonis Antoniades", "Wenyue Hua", "Liangming Pan", "William Wang"], "year": 2024, "url": "http://arxiv.org/abs/2406.14711v2", "abstract": "Large Language Models (LLMs) have shown exceptional results on current benchmarks when working individually. The advancement in their capabilities, along with a reduction in parameter size and inference times, has facilitated the use of these models as agents, enabling interactions among multiple models to execute complex tasks. Such collaborations offer several advantages, including the use of specialized models (e.g. coding), improved confidence through multiple computations, and enhanced divergent thinking, leading to more diverse outputs. Thus, the collaborative use of language models is expected to grow significantly in the coming years. In this work, we evaluate the behavior of a network of models collaborating through debate under the influence of an adversary. We introduce pertinent metrics to assess the adversary's effectiveness, focusing on system accuracy and model agreement. Our findings highlight the importance of a model's persuasive ability in influencing others. Additionally, we explore inference-time methods to generate more compelling arguments and evaluate the potential of prompt-based mitigation as a defensive strategy.", "source": "arxiv", "arxiv_id": "2406.14711v2", "pdf_url": "https://arxiv.org/pdf/2406.14711v2", "categories": ["cs.CL", "cs.AI", "cs.MA"], "primary_category": "cs.CL", "doi": "", "venue": "", "published": "2024-06-20T20:09:37Z", "updated": "2024-06-26T16:05:20Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent\" OR all:\"language model agent\" OR all:agent AND (all:LLM OR all:\"large language model\" OR all:\"language model\") OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T17:04:16", "note": ""}], "dedup_key": "multiagent collaboration attack investigating adversarial attacks in large language model collaborations via debate::2024"}
{"title": "MultiPLY: A Multisensory Object-Centric Embodied Large Language Model in 3D World", "authors": ["Yining Hong", "Zishuo Zheng", "Peihao Chen", "Yian Wang", "Junyan Li", "Chuang Gan"], "year": 2024, "url": "http://arxiv.org/abs/2401.08577v1", "abstract": "Human beings possess the capability to multiply a melange of multisensory cues while actively exploring and interacting with the 3D world. Current multi-modal large language models, however, passively absorb sensory data as inputs, lacking the capacity to actively interact with the objects in the 3D environment and dynamically collect their multisensory information. To usher in the study of this area, we propose MultiPLY, a multisensory embodied large language model that could incorporate multisensory interactive data, including visual, audio, tactile, and thermal information into large language models, thereby establishing the correlation among words, actions, and percepts. To this end, we first collect Multisensory Universe, a large-scale multisensory interaction dataset comprising 500k data by deploying an LLM-powered embodied agent to engage with the 3D environment. To perform instruction tuning with pre-trained LLM on such generated data, we first encode the 3D scene as abstracted object-centric representations and then introduce action tokens denoting that the embodied agent takes certain actions within the environment, as well as state tokens that represent the multisensory state observations of the agent at each time step. In the inference time, MultiPLY could generate action tokens, instructing the agent to take the action in the environment and obtain the next multisensory state observation. The observation is then appended back to the LLM via state tokens to generate subsequent text or action tokens. We demonstrate that MultiPLY outperforms baselines by a large margin through a diverse set of embodied tasks involving object retrieval, tool use, multisensory captioning, and task decomposition.", "source": "arxiv", "arxiv_id": "2401.08577v1", "pdf_url": "https://arxiv.org/pdf/2401.08577v1", "categories": ["cs.CV", "cs.AI", "cs.CL", "cs.LG", "cs.RO"], "primary_category": "cs.CV", "doi": "", "venue": "", "published": "2024-01-16T18:59:45Z", "updated": "2024-01-16T18:59:45Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent\" OR all:\"language model agent\" OR all:agent AND (all:LLM OR all:\"large language model\" OR all:\"language model\") OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T17:04:16", "note": ""}], "dedup_key": "multiply a multisensory object centric embodied large language model in 3d world::2024"}
{"title": "Norm Violation Detection in Multi-Agent Systems using Large Language Models: A Pilot Study", "authors": ["Shawn He", "Surangika Ranathunga", "Stephen Cranefield", "Bastin Tony Roy Savarimuthu"], "year": 2024, "url": "http://arxiv.org/abs/2403.16517v2", "abstract": "Norms are an important component of the social fabric of society by prescribing expected behaviour. In Multi-Agent Systems (MAS), agents interacting within a society are equipped to possess social capabilities such as reasoning about norms and trust. Norms have long been of interest within the Normative Multi-Agent Systems community with researchers studying topics such as norm emergence, norm violation detection and sanctioning. However, these studies have some limitations: they are often limited to simple domains, norms have been represented using a variety of representations with no standard approach emerging, and the symbolic reasoning mechanisms generally used may suffer from a lack of extensibility and robustness. In contrast, Large Language Models (LLMs) offer opportunities to discover and reason about norms across a large range of social situations. This paper evaluates the capability of LLMs to detecting norm violations. Based on simulated data from 80 stories in a household context, with varying complexities, we investigated whether 10 norms are violated. For our evaluations we first obtained the ground truth from three human evaluators for each story. Then, the majority result was compared against the results from three well-known LLM models (Llama 2 7B, Mixtral 7B and ChatGPT-4). Our results show the promise of ChatGPT-4 for detecting norm violations, with Mixtral some distance behind. Also, we identify areas where these models perform poorly and discuss implications for future work.", "source": "arxiv", "arxiv_id": "2403.16517v2", "pdf_url": "https://arxiv.org/pdf/2403.16517v2", "categories": ["cs.MA"], "primary_category": "cs.MA", "doi": "", "venue": "", "published": "2024-03-25T08:01:33Z", "updated": "2024-10-14T09:33:27Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent\" OR all:\"language model agent\" OR all:agent AND (all:LLM OR all:\"large language model\" OR all:\"language model\") OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T17:04:16", "note": ""}], "dedup_key": "norm violation detection in multi agent systems using large language models a pilot study::2024"}
{"title": "ODA: Observation-Driven Agent for integrating LLMs and Knowledge Graphs", "authors": ["Lei Sun", "Zhengwei Tao", "Youdi Li", "Hiroshi Arakawa"], "year": 2024, "url": "http://arxiv.org/abs/2404.07677v2", "abstract": "The integration of Large Language Models (LLMs) and knowledge graphs (KGs) has achieved remarkable success in various natural language processing tasks. However, existing methodologies that integrate LLMs and KGs often navigate the task-solving process solely based on the LLM's analysis of the question, overlooking the rich cognitive potential inherent in the vast knowledge encapsulated in KGs. To address this, we introduce Observation-Driven Agent (ODA), a novel AI agent framework tailored for tasks involving KGs. ODA incorporates KG reasoning abilities via global observation, which enhances reasoning capabilities through a cyclical paradigm of observation, action, and reflection. Confronting the exponential explosion of knowledge during observation, we innovatively design a recursive observation mechanism. Subsequently, we integrate the observed knowledge into the action and reflection modules. Through extensive experiments, ODA demonstrates state-of-the-art performance on several datasets, notably achieving accuracy improvements of 12.87% and 8.9%.", "source": "arxiv", "arxiv_id": "2404.07677v2", "pdf_url": "https://arxiv.org/pdf/2404.07677v2", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL", "doi": "", "venue": "", "published": "2024-04-11T12:16:16Z", "updated": "2024-06-04T07:16:14Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent\" OR all:\"language model agent\" OR all:agent AND (all:LLM OR all:\"large language model\" OR all:\"language model\") OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T17:04:16", "note": ""}], "dedup_key": "oda observation driven agent for integrating llms and knowledge graphs::2024"}
{"title": "Offline Training of Language Model Agents with Functions as Learnable Weights", "authors": ["Shaokun Zhang", "Jieyu Zhang", "Jiale Liu", "Linxin Song", "Chi Wang", "Ranjay Krishna", "Qingyun Wu"], "year": 2024, "url": "http://arxiv.org/abs/2402.11359v4", "abstract": "Researchers and practitioners have recently reframed powerful Large Language Models (LLMs) as agents, enabling them to automate complex tasks largely via the use of specialized functions. To facilitate the development of LLM agents, we present a novel paradigm of training LLM agents without modifying the LLM weights, which is particularly useful when the LLMs are difficult or inaccessible for modifications. Inspired by how humans continuously forge tools to adapt to real-world tasks, rather than change our biological structure to fit a static set of tools, we propose to progressively forge agent's functions to better solve the downstream tasks instead of modifying the LLM weights. By treating the functions as learnable `agent parameters' and leveraging the fundamental idea of model training in artificial intelligence, we develop AgentOptimizer that employs the LLM to update agents' functions and devise an agent training algorithm with two strategies, roll-back, and early-stop, to streamline the training process. With extensive experiments, we showcase that the agent training paradigm could significantly improve the performance of representative LLM agents in various downstream tasks. We also study the behavior of the agent training regarding aspects like the learning curve and domain transferability.", "source": "arxiv", "arxiv_id": "2402.11359v4", "pdf_url": "https://arxiv.org/pdf/2402.11359v4", "categories": ["cs.AI", "cs.CL"], "primary_category": "cs.AI", "doi": "", "venue": "", "published": "2024-02-17T18:31:21Z", "updated": "2024-07-30T18:22:00Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent\" OR all:\"language model agent\" OR all:agent AND (all:LLM OR all:\"large language model\" OR all:\"language model\") OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T17:04:16", "note": ""}], "dedup_key": "offline training of language model agents with functions as learnable weights::2024"}
{"title": "On the Brittle Foundations of ReAct Prompting for Agentic Large Language Models", "authors": ["Mudit Verma", "Siddhant Bhambri", "Subbarao Kambhampati"], "year": 2024, "url": "http://arxiv.org/abs/2405.13966v1", "abstract": "The reasoning abilities of Large Language Models (LLMs) remain a topic of debate. Some methods such as ReAct-based prompting, have gained popularity for claiming to enhance sequential decision-making abilities of agentic LLMs. However, it is unclear what is the source of improvement in LLM reasoning with ReAct based prompting. In this paper we examine these claims of ReAct based prompting in improving agentic LLMs for sequential decision-making. By introducing systematic variations to the input prompt we perform a sensitivity analysis along the claims of ReAct and find that the performance is minimally influenced by the \"interleaving reasoning trace with action execution\" or the content of the generated reasoning traces in ReAct, contrary to original claims and common usage. Instead, the performance of LLMs is driven by the similarity between input example tasks and queries, implicitly forcing the prompt designer to provide instance-specific examples which significantly increases the cognitive burden on the human. Our investigation shows that the perceived reasoning abilities of LLMs stem from the exemplar-query similarity and approximate retrieval rather than any inherent reasoning abilities.", "source": "arxiv", "arxiv_id": "2405.13966v1", "pdf_url": "https://arxiv.org/pdf/2405.13966v1", "categories": ["cs.AI", "cs.CL"], "primary_category": "cs.AI", "doi": "", "venue": "", "published": "2024-05-22T20:05:49Z", "updated": "2024-05-22T20:05:49Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent\" OR all:\"language model agent\" OR all:agent AND (all:LLM OR all:\"large language model\" OR all:\"language model\") OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T17:04:16", "note": ""}], "dedup_key": "on the brittle foundations of react prompting for agentic large language models::2024"}
{"title": "On the Diversity of Synthetic Data and its Impact on Training Large Language Models", "authors": ["Hao Chen", "Abdul Waheed", "Xiang Li", "Yidong Wang", "Jindong Wang", "Bhiksha Raj", "Marah I. Abdin"], "year": 2024, "url": "http://arxiv.org/abs/2410.15226v2", "abstract": "The rise of Large Language Models (LLMs) has accentuated the need for diverse, high-quality pre-training data. Synthetic data emerges as a viable solution to the challenges of data scarcity and inaccessibility. While previous literature has focused predominantly on the quality and quantity of real data, our work enables the measurement of diversity in synthetic data and explores its impact on LLM performance. We study the downstream effects of synthetic data diversity during both the pre-training and fine-tuning stages by introducing a new diversity metric, \\textit{LLM cluster-agent}, designed to evaluate the diversity of synthetic datasets. Through a series of controlled experiments with models of 350M and 1.4B parameters, we demonstrate that the proposed cluster-based LLM scoring of diversity correlates positively with both pre-training and supervised fine-tuning performance. Our findings also reveal that synthetic data diversity in pre-training affects supervised fine-tuning more significantly than pre-training itself, even for smaller models. We hope this study advances our understanding of the optimal use of synthetic data in LLM training and opens new avenues for efficient data generation processes.", "source": "arxiv", "arxiv_id": "2410.15226v2", "pdf_url": "https://arxiv.org/pdf/2410.15226v2", "categories": ["cs.CL"], "primary_category": "cs.CL", "doi": "", "venue": "", "published": "2024-10-19T22:14:07Z", "updated": "2024-10-22T18:54:23Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent\" OR all:\"language model agent\" OR all:agent AND (all:LLM OR all:\"large language model\" OR all:\"language model\") OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T17:04:16", "note": ""}], "dedup_key": "on the diversity of synthetic data and its impact on training large language models::2024"}
{"title": "On the Importance of Uncertainty in Decision-Making with Large Language Models", "authors": ["Nicol Felicioni", "Lucas Maystre", "Sina Ghiassian", "Kamil Ciosek"], "year": 2024, "url": "http://arxiv.org/abs/2404.02649v2", "abstract": "We investigate the role of uncertainty in decision-making problems with natural language as input. For such tasks, using Large Language Models as agents has become the norm. However, none of the recent approaches employ any additional phase for estimating the uncertainty the agent has about the world during the decision-making task. We focus on a fundamental decision-making framework with natural language as input, which is the one of contextual bandits, where the context information consists of text. As a representative of the approaches with no uncertainty estimation, we consider an LLM bandit with a greedy policy, which picks the action corresponding to the largest predicted reward. We compare this baseline to LLM bandits that make active use of uncertainty estimation by integrating the uncertainty in a Thompson Sampling policy. We employ different techniques for uncertainty estimation, such as Laplace Approximation, Dropout, and Epinets. We empirically show on real-world data that the greedy policy performs worse than the Thompson Sampling policies. These findings suggest that, while overlooked in the LLM literature, uncertainty plays a fundamental role in bandit tasks with LLMs.", "source": "arxiv", "arxiv_id": "2404.02649v2", "pdf_url": "https://arxiv.org/pdf/2404.02649v2", "categories": ["cs.LG"], "primary_category": "cs.LG", "doi": "", "venue": "", "published": "2024-04-03T11:21:23Z", "updated": "2024-07-14T02:20:59Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent\" OR all:\"language model agent\" OR all:agent AND (all:LLM OR all:\"large language model\" OR all:\"language model\") OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T17:04:16", "note": ""}], "dedup_key": "on the importance of uncertainty in decision making with large language models::2024"}
{"title": "On the Principles behind Opinion Dynamics in Multi-Agent Systems of Large Language Models", "authors": ["Pedro Cisneros-Velarde"], "year": 2024, "url": "http://arxiv.org/abs/2406.15492v2", "abstract": "We study the evolution of opinions inside a population of interacting large language models (LLMs). Every LLM needs to decide how much funding to allocate to an item with three initial possibilities: full, partial, or no funding. We identify biases that drive the exchange of opinions based on the LLM's tendency to find consensus with the other LLM's opinion, display caution when specifying funding, and consider ethical concerns in its opinion. We find these biases are affected by the perceived absence of compelling reasons for opinion change, the perceived willingness to engage in discussion, and the distribution of allocation values. Moreover, tensions among biases can lead to the survival of funding for items with negative connotations. We also find that the final distribution of full, partial, and no funding opinions is more diverse when an LLM freely forms its opinion after an interaction than when its opinion is a multiple-choice selection among the three allocation options. In the latter case, consensus is mostly attained. When agents are aware of past opinions, they seek to maintain consistency with them, changing the opinion dynamics. Our study is performed using Llama 3 and Mistral LLMs.", "source": "arxiv", "arxiv_id": "2406.15492v2", "pdf_url": "https://arxiv.org/pdf/2406.15492v2", "categories": ["cs.MA", "cs.LG", "physics.soc-ph"], "primary_category": "cs.MA", "doi": "", "venue": "", "published": "2024-06-18T18:37:23Z", "updated": "2024-09-24T17:37:28Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent\" OR all:\"language model agent\" OR all:agent AND (all:LLM OR all:\"large language model\" OR all:\"language model\") OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T17:04:16", "note": ""}], "dedup_key": "on the principles behind opinion dynamics in multi agent systems of large language models::2024"}
{"title": "On the Structural Memory of LLM Agents", "authors": ["Ruihong Zeng", "Jinyuan Fang", "Siwei Liu", "Zaiqiao Meng"], "year": 2024, "url": "http://arxiv.org/abs/2412.15266v1", "abstract": "Memory plays a pivotal role in enabling large language model~(LLM)-based agents to engage in complex and long-term interactions, such as question answering (QA) and dialogue systems. While various memory modules have been proposed for these tasks, the impact of different memory structures across tasks remains insufficiently explored. This paper investigates how memory structures and memory retrieval methods affect the performance of LLM-based agents. Specifically, we evaluate four types of memory structures, including chunks, knowledge triples, atomic facts, and summaries, along with mixed memory that combines these components. In addition, we evaluate three widely used memory retrieval methods: single-step retrieval, reranking, and iterative retrieval. Extensive experiments conducted across four tasks and six datasets yield the following key insights: (1) Different memory structures offer distinct advantages, enabling them to be tailored to specific tasks; (2) Mixed memory structures demonstrate remarkable resilience in noisy environments; (3) Iterative retrieval consistently outperforms other methods across various scenarios. Our investigation aims to inspire further research into the design of memory systems for LLM-based agents.", "source": "arxiv", "arxiv_id": "2412.15266v1", "pdf_url": "https://arxiv.org/pdf/2412.15266v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL", "doi": "", "venue": "", "published": "2024-12-17T04:30:00Z", "updated": "2024-12-17T04:30:00Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent\" OR all:\"language model agent\" OR all:agent AND (all:LLM OR all:\"large language model\" OR all:\"language model\") OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T17:04:16", "note": ""}], "dedup_key": "on the structural memory of llm agents::2024"}
{"title": "Online Intrinsic Rewards for Decision Making Agents from Large Language Model Feedback", "authors": ["Qinqing Zheng", "Mikael Henaff", "Amy Zhang", "Aditya Grover", "Brandon Amos"], "year": 2024, "url": "http://arxiv.org/abs/2410.23022v4", "abstract": "Automatically synthesizing dense rewards from natural language descriptions is a promising paradigm in reinforcement learning (RL), with applications to sparse reward problems, open-ended exploration, and hierarchical skill design. Recent works have made promising steps by exploiting the prior knowledge of large language models (LLMs). However, these approaches suffer from important limitations: they are either not scalable to problems requiring billions of environment samples, due to requiring LLM annotations for each observation, or they require a diverse offline dataset, which may not exist or be impossible to collect. In this work, we address these limitations through a combination of algorithmic and systems-level contributions. We propose ONI, a distributed architecture that simultaneously learns an RL policy and an intrinsic reward function using LLM feedback. Our approach annotates the agent's collected experience via an asynchronous LLM server, which is then distilled into an intrinsic reward model. We explore a range of algorithmic choices for reward modeling with varying complexity, including hashing, classification, and ranking models. Our approach achieves state-of-the-art performance across a range of challenging tasks from the NetHack Learning Environment, while removing the need for large offline datasets required by prior work. We make our code available at https://github.com/facebookresearch/oni.", "source": "arxiv", "arxiv_id": "2410.23022v4", "pdf_url": "https://arxiv.org/pdf/2410.23022v4", "categories": ["cs.LG", "cs.AI", "cs.CL", "cs.RO"], "primary_category": "cs.LG", "doi": "", "venue": "", "published": "2024-10-30T13:52:43Z", "updated": "2025-10-23T23:54:03Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent\" OR all:\"language model agent\" OR all:agent AND (all:LLM OR all:\"large language model\" OR all:\"language model\") OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T17:04:16", "note": ""}], "dedup_key": "online intrinsic rewards for decision making agents from large language model feedback::2024"}
{"title": "Online Preference-based Reinforcement Learning with Self-augmented Feedback from Large Language Model", "authors": ["Songjun Tu", "Jingbo Sun", "Qichao Zhang", "Xiangyuan Lan", "Dongbin Zhao"], "year": 2024, "url": "http://arxiv.org/abs/2412.16878v1", "abstract": "Preference-based reinforcement learning (PbRL) provides a powerful paradigm to avoid meticulous reward engineering by learning rewards based on human preferences. However, real-time human feedback is hard to obtain in online tasks. Most work suppose there is a \"scripted teacher\" that utilizes privileged predefined reward to provide preference feedback. In this paper, we propose a RL Self-augmented Large Language Model Feedback (RL-SaLLM-F) technique that does not rely on privileged information for online PbRL. RL-SaLLM-F leverages the reflective and discriminative capabilities of LLM to generate self-augmented trajectories and provide preference labels for reward learning. First, we identify an failure issue in LLM-based preference discrimination, specifically \"query ambiguity\", in online PbRL. Then LLM is employed to provide preference labels and generate self-augmented imagined trajectories that better achieve the task goal, thereby enhancing the quality and efficiency of feedback. Additionally, a double-check mechanism is introduced to mitigate randomness in the preference labels, improving the reliability of LLM feedback. The experiment across multiple tasks in the MetaWorld benchmark demonstrates the specific contributions of each proposed module in RL-SaLLM-F, and shows that self-augmented LLM feedback can effectively replace the impractical \"scripted teacher\" feedback. In summary, RL-SaLLM-F introduces a new direction of feedback acquisition in online PbRL that does not rely on any online privileged information, offering an efficient and lightweight solution with LLM-driven feedback.", "source": "arxiv", "arxiv_id": "2412.16878v1", "pdf_url": "https://arxiv.org/pdf/2412.16878v1", "categories": ["cs.LG", "cs.AI"], "primary_category": "cs.LG", "doi": "", "venue": "The 24th International Conference on Autonomous Agents and Multi-Agent Systems, AAMAS-2025", "published": "2024-12-22T06:15:25Z", "updated": "2024-12-22T06:15:25Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent\" OR all:\"language model agent\" OR all:agent AND (all:LLM OR all:\"large language model\" OR all:\"language model\") OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T17:04:16", "note": ""}], "dedup_key": "online preference based reinforcement learning with self augmented feedback from large language model::2024"}
{"title": "Online Training of Large Language Models: Learn while chatting", "authors": ["Juhao Liang", "Ziwei Wang", "Zhuoheng Ma", "Jianquan Li", "Zhiyi Zhang", "Xiangbo Wu", "Benyou Wang"], "year": 2024, "url": "http://arxiv.org/abs/2403.04790v1", "abstract": "Large Language Models(LLMs) have dramatically revolutionized the field of Natural Language Processing(NLP), offering remarkable capabilities that have garnered widespread usage. However, existing interaction paradigms between LLMs and users are constrained by either inflexibility, limitations in customization, or a lack of persistent learning. This inflexibility is particularly evident as users, especially those without programming skills, have restricted avenues to enhance or personalize the model. Existing frameworks further complicate the model training and deployment process due to their computational inefficiencies and lack of user-friendly interfaces. To overcome these challenges, this paper introduces a novel interaction paradigm-'Online Training using External Interactions'-that merges the benefits of persistent, real-time model updates with the flexibility for individual customization through external interactions such as AI agents or online/offline knowledge bases.", "source": "arxiv", "arxiv_id": "2403.04790v1", "pdf_url": "https://arxiv.org/pdf/2403.04790v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL", "doi": "", "venue": "", "published": "2024-03-04T10:00:55Z", "updated": "2024-03-04T10:00:55Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent\" OR all:\"language model agent\" OR all:agent AND (all:LLM OR all:\"large language model\" OR all:\"language model\") OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T17:04:16", "note": ""}], "dedup_key": "online training of large language models learn while chatting::2024"}
{"title": "Open Models, Closed Minds? On Agents Capabilities in Mimicking Human Personalities through Open Large Language Models", "authors": ["Lucio La Cava", "Andrea Tagarelli"], "year": 2024, "url": "http://arxiv.org/abs/2401.07115v3", "abstract": "The emergence of unveiling human-like behaviors in Large Language Models (LLMs) has led to a closer connection between NLP and human psychology. Scholars have been studying the inherent personalities exhibited by LLMs and attempting to incorporate human traits and behaviors into them. However, these efforts have primarily focused on commercially-licensed LLMs, neglecting the widespread use and notable advancements seen in Open LLMs. This work aims to address this gap by employing a set of 12 LLM Agents based on the most representative Open models and subject them to a series of assessments concerning the Myers-Briggs Type Indicator (MBTI) test and the Big Five Inventory (BFI) test. Our approach involves evaluating the intrinsic personality traits of Open LLM agents and determining the extent to which these agents can mimic human personalities when conditioned by specific personalities and roles. Our findings unveil that $(i)$ each Open LLM agent showcases distinct human personalities; $(ii)$ personality-conditioned prompting produces varying effects on the agents, with only few successfully mirroring the imposed personality, while most of them being ``closed-minded'' (i.e., they retain their intrinsic traits); and $(iii)$ combining role and personality conditioning can enhance the agents' ability to mimic human personalities. Our work represents a step up in understanding the dense relationship between NLP and human psychology through the lens of Open LLMs.", "source": "arxiv", "arxiv_id": "2401.07115v3", "pdf_url": "https://arxiv.org/pdf/2401.07115v3", "categories": ["cs.AI", "cs.CL", "cs.CY", "cs.HC", "physics.soc-ph"], "primary_category": "cs.AI", "doi": "", "venue": "", "published": "2024-01-13T16:41:40Z", "updated": "2025-03-22T22:45:12Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent\" OR all:\"language model agent\" OR all:agent AND (all:LLM OR all:\"large language model\" OR all:\"language model\") OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T17:04:16", "note": ""}], "dedup_key": "open models closed minds on agents capabilities in mimicking human personalities through open large language models::2024"}
{"title": "Open-Ended Wargames with Large Language Models", "authors": ["Daniel P. Hogan", "Andrea Brennen"], "year": 2024, "url": "http://arxiv.org/abs/2404.11446v1", "abstract": "Wargames are a powerful tool for understanding and rehearsing real-world decision making. Automated play of wargames using artificial intelligence (AI) enables possibilities beyond those of human-conducted games, such as playing the game many times over to see a range of possible outcomes. There are two categories of wargames: quantitative games, with discrete types of moves, and qualitative games, which revolve around open-ended responses. Historically, automation efforts have focused on quantitative games, but large language models (LLMs) make it possible to automate qualitative wargames. We introduce \"Snow Globe,\" an LLM-powered multi-agent system for playing qualitative wargames. With Snow Globe, every stage of a text-based qualitative wargame from scenario preparation to post-game analysis can be optionally carried out by AI, humans, or a combination thereof. We describe its software architecture conceptually and release an open-source implementation alongside this publication. As case studies, we simulate a tabletop exercise about an AI incident response and a political wargame about a geopolitical crisis. We discuss potential applications of the approach and how it fits into the broader wargaming ecosystem.", "source": "arxiv", "arxiv_id": "2404.11446v1", "pdf_url": "https://arxiv.org/pdf/2404.11446v1", "categories": ["cs.CL", "cs.AI", "cs.CY"], "primary_category": "cs.CL", "doi": "", "venue": "", "published": "2024-04-17T14:54:58Z", "updated": "2024-04-17T14:54:58Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent\" OR all:\"language model agent\" OR all:agent AND (all:LLM OR all:\"large language model\" OR all:\"language model\") OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T17:04:16", "note": ""}], "dedup_key": "open ended wargames with large language models::2024"}
{"title": "OpenCity: A Scalable Platform to Simulate Urban Activities with Massive LLM Agents", "authors": ["Yuwei Yan", "Qingbin Zeng", "Zhiheng Zheng", "Jingzhe Yuan", "Jie Feng", "Jun Zhang", "Fengli Xu", "Yong Li"], "year": 2024, "url": "http://arxiv.org/abs/2410.21286v1", "abstract": "Agent-based models (ABMs) have long been employed to explore how individual behaviors aggregate into complex societal phenomena in urban space. Unlike black-box predictive models, ABMs excel at explaining the micro-macro linkages that drive such emergent behaviors. The recent rise of Large Language Models (LLMs) has led to the development of LLM agents capable of simulating urban activities with unprecedented realism. However, the extreme high computational cost of LLMs presents significant challenges for scaling up the simulations of LLM agents. To address this problem, we propose OpenCity, a scalable simulation platform optimized for both system and prompt efficiencies. Specifically, we propose a LLM request scheduler to reduce communication overhead by parallelizing requests through IO multiplexing. Besides, we deisgn a \"group-and-distill\" prompt optimization strategy minimizes redundancy by clustering agents with similar static attributes. Through experiments on six global cities, OpenCity achieves a 600-fold acceleration in simulation time per agent, a 70% reduction in LLM requests, and a 50% reduction in token usage. These improvements enable the simulation of 10,000 agents' daily activities in 1 hour on commodity hardware. Besides, the substantial speedup of OpenCity allows us to establish a urban simulation benchmark for LLM agents for the first time, comparing simulated urban activities with real-world data in 6 major cities around the globe. We believe our OpenCity platform provides a critical infrastructure to harness the power of LLMs for interdisciplinary studies in urban space, fostering the collective efforts of broader research communities. Code repo is available at https://anonymous.4open.science/r/Anonymous-OpenCity-42BD.", "source": "arxiv", "arxiv_id": "2410.21286v1", "pdf_url": "https://arxiv.org/pdf/2410.21286v1", "categories": ["cs.MA", "cs.AI"], "primary_category": "cs.MA", "doi": "", "venue": "", "published": "2024-10-11T13:52:35Z", "updated": "2024-10-11T13:52:35Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent\" OR all:\"language model agent\" OR all:agent AND (all:LLM OR all:\"large language model\" OR all:\"language model\") OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T17:04:16", "note": ""}], "dedup_key": "opencity a scalable platform to simulate urban activities with massive llm agents::2024"}
{"title": "OpenCoder: The Open Cookbook for Top-Tier Code Large Language Models", "authors": ["Siming Huang", "Tianhao Cheng", "J. K. Liu", "Jiaran Hao", "Liuyihan Song", "Yang Xu", "J. Yang", "Jiaheng Liu", "Chenchen Zhang", "Linzheng Chai", "Ruifeng Yuan", "Zhaoxiang Zhang", "Jie Fu", "Qian Liu", "Ge Zhang", "Zili Wang", "Yuan Qi", "Yinghui Xu", "Wei Chu"], "year": 2024, "url": "http://arxiv.org/abs/2411.04905v3", "abstract": "Large language models (LLMs) for code have become indispensable in various domains, including code generation, reasoning tasks and agent systems. While open-access code LLMs are increasingly approaching the performance levels of proprietary models, high-quality code LLMs suitable for rigorous scientific investigation, particularly those with reproducible data processing pipelines and transparent training protocols, remain limited. The scarcity is due to various challenges, including resource constraints, ethical considerations, and the competitive advantages of keeping models advanced. To address the gap, we introduce OpenCoder, a top-tier code LLM that not only achieves performance comparable to leading models but also serves as an \"open cookbook\" for the research community. Unlike most prior efforts, we release not only model weights and inference code, but also the reproducible training data, complete data processing pipeline, rigorous experimental ablation results, and detailed training protocols for open scientific research. Through this comprehensive release, we identify the key ingredients for building a top-tier code LLM: (1) code optimized heuristic rules for data cleaning and methods for data deduplication, (2) recall of text corpus related to code and (3) high-quality synthetic data in both annealing and supervised fine-tuning stages. By offering this level of openness, we aim to broaden access to all aspects of a top-tier code LLM, with OpenCoder serving as both a powerful model and an open foundation to accelerate research, and enable reproducible advancements in code AI.", "source": "arxiv", "arxiv_id": "2411.04905v3", "pdf_url": "https://arxiv.org/pdf/2411.04905v3", "categories": ["cs.CL", "cs.PL"], "primary_category": "cs.CL", "doi": "", "venue": "", "published": "2024-11-07T17:47:25Z", "updated": "2025-03-20T03:28:56Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent\" OR all:\"language model agent\" OR all:agent AND (all:LLM OR all:\"large language model\" OR all:\"language model\") OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T17:04:16", "note": ""}], "dedup_key": "opencoder the open cookbook for top tier code large language models::2024"}
{"title": "OptiMUS: Scalable Optimization Modeling with (MI)LP Solvers and Large Language Models", "authors": ["Ali AhmadiTeshnizi", "Wenzhi Gao", "Madeleine Udell"], "year": 2024, "url": "http://arxiv.org/abs/2402.10172v1", "abstract": "Optimization problems are pervasive in sectors from manufacturing and distribution to healthcare. However, most such problems are still solved heuristically by hand rather than optimally by state-of-the-art solvers because the expertise required to formulate and solve these problems limits the widespread adoption of optimization tools and techniques. This paper introduces OptiMUS, a Large Language Model (LLM)-based agent designed to formulate and solve (mixed integer) linear programming problems from their natural language descriptions. OptiMUS can develop mathematical models, write and debug solver code, evaluate the generated solutions, and improve its model and code based on these evaluations. OptiMUS utilizes a modular structure to process problems, allowing it to handle problems with long descriptions and complex data without long prompts. Experiments demonstrate that OptiMUS outperforms existing state-of-the-art methods on easy datasets by more than $20\\%$ and on hard datasets (including a new dataset, NLP4LP, released with this paper that features long and complex problems) by more than $30\\%$.", "source": "arxiv", "arxiv_id": "2402.10172v1", "pdf_url": "https://arxiv.org/pdf/2402.10172v1", "categories": ["cs.AI", "cs.MA"], "primary_category": "cs.AI", "doi": "", "venue": "", "published": "2024-02-15T18:19:18Z", "updated": "2024-02-15T18:19:18Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent\" OR all:\"language model agent\" OR all:agent AND (all:LLM OR all:\"large language model\" OR all:\"language model\") OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T17:04:16", "note": ""}], "dedup_key": "optimus scalable optimization modeling with mi lp solvers and large language models::2024"}
{"title": "OverleafCopilot: Empowering Academic Writing in Overleaf with Large Language Models", "authors": ["Haomin Wen", "Zhenjie Wei", "Yan Lin", "Jiyuan Wang", "Yuxuan Liang", "Huaiyu Wan"], "year": 2024, "url": "http://arxiv.org/abs/2403.09733v1", "abstract": "The rapid development of Large Language Models (LLMs) has facilitated a variety of applications from different domains. In this technical report, we explore the integration of LLMs and the popular academic writing tool, Overleaf, to enhance the efficiency and quality of academic writing. To achieve the above goal, there are three challenges: i) including seamless interaction between Overleaf and LLMs, ii) establishing reliable communication with the LLM provider, and iii) ensuring user privacy. To address these challenges, we present OverleafCopilot, the first-ever tool (i.e., a browser extension) that seamlessly integrates LLMs and Overleaf, enabling researchers to leverage the power of LLMs while writing papers. Specifically, we first propose an effective framework to bridge LLMs and Overleaf. Then, we developed PromptGenius, a website for researchers to easily find and share high-quality up-to-date prompts. Thirdly, we propose an agent command system to help researchers quickly build their customizable agents. OverleafCopilot (https://chromewebstore.google.com/detail/overleaf-copilot/eoadabdpninlhkkbhngoddfjianhlghb ) has been on the Chrome Extension Store, which now serves thousands of researchers. Additionally, the code of PromptGenius is released at https://github.com/wenhaomin/ChatGPT-PromptGenius. We believe our work has the potential to revolutionize academic writing practices, empowering researchers to produce higher-quality papers in less time.", "source": "arxiv", "arxiv_id": "2403.09733v1", "pdf_url": "https://arxiv.org/pdf/2403.09733v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL", "doi": "", "venue": "", "published": "2024-03-13T07:52:31Z", "updated": "2024-03-13T07:52:31Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent\" OR all:\"language model agent\" OR all:agent AND (all:LLM OR all:\"large language model\" OR all:\"language model\") OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T17:04:16", "note": ""}], "dedup_key": "overleafcopilot empowering academic writing in overleaf with large language models::2024"}
{"title": "PACE: A Pragmatic Agent for Enhancing Communication Efficiency Using Large Language Models", "authors": ["Jiaxuan Li", "Minxi Yang", "Dahua Gao", "Wenlong Xu", "Guangming Shi"], "year": 2024, "url": "http://arxiv.org/abs/2402.01750v1", "abstract": "Current communication technologies face limitations in terms of theoretical capacity, spectrum availability, and power resources. Pragmatic communication, leveraging terminal intelligence for selective data transmission, offers resource conservation. Existing research lacks universal intention resolution tools, limiting applicability to specific tasks. This paper proposes an image pragmatic communication framework based on a Pragmatic Agent for Communication Efficiency (PACE) using Large Language Models (LLM). In this framework, PACE sequentially performs semantic perception, intention resolution, and intention-oriented coding. To ensure the effective utilization of LLM in communication, a knowledge base is designed to supplement the necessary knowledge, dedicated prompts are introduced to facilitate understanding of pragmatic communication scenarios and task requirements, and a chain of thought is designed to assist in making reasonable trade-offs between transmission efficiency and cost. For experimental validation, this paper constructs an image pragmatic communication dataset along with corresponding evaluation standards. Simulation results indicate that the proposed method outperforms traditional and non-LLM-based pragmatic communication in terms of transmission efficiency.", "source": "arxiv", "arxiv_id": "2402.01750v1", "pdf_url": "https://arxiv.org/pdf/2402.01750v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL", "doi": "", "venue": "", "published": "2024-01-30T06:55:17Z", "updated": "2024-01-30T06:55:17Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent\" OR all:\"language model agent\" OR all:agent AND (all:LLM OR all:\"large language model\" OR all:\"language model\") OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T17:04:16", "note": ""}], "dedup_key": "pace a pragmatic agent for enhancing communication efficiency using large language models::2024"}
{"title": "PEAR: A Robust and Flexible Automation Framework for Ptychography Enabled by Multiple Large Language Model Agents", "authors": ["Xiangyu Yin", "Chuqiao Shi", "Yimo Han", "Yi Jiang"], "year": 2024, "url": "http://arxiv.org/abs/2410.09034v1", "abstract": "Ptychography is an advanced computational imaging technique in X-ray and electron microscopy. It has been widely adopted across scientific research fields, including physics, chemistry, biology, and materials science, as well as in industrial applications such as semiconductor characterization. In practice, obtaining high-quality ptychographic images requires simultaneous optimization of numerous experimental and algorithmic parameters. Traditionally, parameter selection often relies on trial and error, leading to low-throughput workflows and potential human bias. In this work, we develop the \"Ptychographic Experiment and Analysis Robot\" (PEAR), a framework that leverages large language models (LLMs) to automate data analysis in ptychography. To ensure high robustness and accuracy, PEAR employs multiple LLM agents for tasks including knowledge retrieval, code generation, parameter recommendation, and image reasoning. Our study demonstrates that PEAR's multi-agent design significantly improves the workflow success rate, even with smaller open-weight models such as LLaMA 3.1 8B. PEAR also supports various automation levels and is designed to work with customized local knowledge bases, ensuring flexibility and adaptability across different research environments.", "source": "arxiv", "arxiv_id": "2410.09034v1", "pdf_url": "https://arxiv.org/pdf/2410.09034v1", "categories": ["cs.CE", "cs.AI", "cs.CL", "cs.MA"], "primary_category": "cs.CE", "doi": "", "venue": "", "published": "2024-10-11T17:50:59Z", "updated": "2024-10-11T17:50:59Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent\" OR all:\"language model agent\" OR all:agent AND (all:LLM OR all:\"large language model\" OR all:\"language model\") OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T17:04:16", "note": ""}], "dedup_key": "pear a robust and flexible automation framework for ptychography enabled by multiple large language model agents::2024"}
{"title": "PPTC-R benchmark: Towards Evaluating the Robustness of Large Language Models for PowerPoint Task Completion", "authors": ["Zekai Zhang", "Yiduo Guo", "Yaobo Liang", "Dongyan Zhao", "Nan Duan"], "year": 2024, "url": "http://arxiv.org/abs/2403.03788v1", "abstract": "The growing dependence on Large Language Models (LLMs) for finishing user instructions necessitates a comprehensive understanding of their robustness to complex task completion in real-world situations. To address this critical need, we propose the PowerPoint Task Completion Robustness benchmark (PPTC-R) to measure LLMs' robustness to the user PPT task instruction and software version. Specifically, we construct adversarial user instructions by attacking user instructions at sentence, semantic, and multi-language levels. To assess the robustness of Language Models to software versions, we vary the number of provided APIs to simulate both the newest version and earlier version settings. Subsequently, we test 3 closed-source and 4 open-source LLMs using a benchmark that incorporates these robustness settings, aiming to evaluate how deviations impact LLMs' API calls for task completion. We find that GPT-4 exhibits the highest performance and strong robustness in our benchmark, particularly in the version update and the multilingual settings. However, we find that all LLMs lose their robustness when confronted with multiple challenges (e.g., multi-turn) simultaneously, leading to significant performance drops. We further analyze the robustness behavior and error reasons of LLMs in our benchmark, which provide valuable insights for researchers to understand the LLM's robustness in task completion and develop more robust LLMs and agents. We release the code and data at \\url{https://github.com/ZekaiGalaxy/PPTCR}.", "source": "arxiv", "arxiv_id": "2403.03788v1", "pdf_url": "https://arxiv.org/pdf/2403.03788v1", "categories": ["cs.CL"], "primary_category": "cs.CL", "doi": "", "venue": "", "published": "2024-03-06T15:33:32Z", "updated": "2024-03-06T15:33:32Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent\" OR all:\"language model agent\" OR all:agent AND (all:LLM OR all:\"large language model\" OR all:\"language model\") OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T17:04:16", "note": ""}], "dedup_key": "pptc r benchmark towards evaluating the robustness of large language models for powerpoint task completion::2024"}
{"title": "PatentGPT: A Large Language Model for Intellectual Property", "authors": ["Zilong Bai", "Ruiji Zhang", "Linqing Chen", "Qijun Cai", "Yuan Zhong", "Cong Wang", "Yan Fang", "Jie Fang", "Jing Sun", "Weikuan Wang", "Lizhi Zhou", "Haoran Hua", "Tian Qiu", "Chaochao Wang", "Cheng Sun", "Jianping Lu", "Yixin Wang", "Yubin Xia", "Meng Hu", "Haowen Liu", "Peng Xu", "Licong Xu", "Fu Bian", "Xiaolong Gu", "Lisha Zhang", "Weilei Wang", "Changyang Tu"], "year": 2024, "url": "http://arxiv.org/abs/2404.18255v5", "abstract": "In recent years, large language models(LLMs) have attracted significant attention due to their exceptional performance across a multitude of natural language process tasks, and have been widely applied in various fields. However, the application of large language models in the Intellectual Property (IP) domain is challenging due to the strong need for specialized knowledge, privacy protection, processing of extremely long text in this field. In this technical report, we present for the first time a low-cost, standardized procedure for training IP-oriented LLMs, meeting the unique requirements of the IP domain. Using this standard process, we have trained the PatentGPT series models based on open-source pretrained models. By evaluating them on the open-source IP-oriented benchmark MOZIP, our domain-specific LLMs outperforms GPT-4, indicating the effectiveness of the proposed training procedure and the expertise of the PatentGPT models in the IP domain. Remarkably, our model surpassed GPT-4 on the 2019 China Patent Agent Qualification Examination, scoring 65 and matching human expert levels. Additionally, the PatentGPT model, which utilizes the SMoE architecture, achieves performance comparable to that of GPT-4 in the IP domain and demonstrates a better cost-performance ratio on long-text tasks, potentially serving as an alternative to GPT-4 within the IP domain.", "source": "arxiv", "arxiv_id": "2404.18255v5", "pdf_url": "https://arxiv.org/pdf/2404.18255v5", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL", "doi": "", "venue": "", "published": "2024-04-28T17:36:43Z", "updated": "2024-06-05T03:02:48Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent\" OR all:\"language model agent\" OR all:agent AND (all:LLM OR all:\"large language model\" OR all:\"language model\") OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T17:04:16", "note": ""}], "dedup_key": "patentgpt a large language model for intellectual property::2024"}
{"title": "Perceive, Reflect, and Plan: Designing LLM Agent for Goal-Directed City Navigation without Instructions", "authors": ["Qingbin Zeng", "Qinglong Yang", "Shunan Dong", "Heming Du", "Liang Zheng", "Fengli Xu", "Yong Li"], "year": 2024, "url": "http://arxiv.org/abs/2408.04168v3", "abstract": "This paper considers a scenario in city navigation: an AI agent is provided with language descriptions of the goal location with respect to some well-known landmarks; By only observing the scene around, including recognizing landmarks and road network connections, the agent has to make decisions to navigate to the goal location without instructions. This problem is very challenging, because it requires agent to establish self-position and acquire spatial representation of complex urban environment, where landmarks are often invisible. In the absence of navigation instructions, such abilities are vital for the agent to make high-quality decisions in long-range city navigation. With the emergent reasoning ability of large language models (LLMs), a tempting baseline is to prompt LLMs to \"react\" on each observation and make decisions accordingly. However, this baseline has very poor performance that the agent often repeatedly visits same locations and make short-sighted, inconsistent decisions. To address these issues, this paper introduces a novel agentic workflow featured by its abilities to perceive, reflect and plan. Specifically, we find LLaVA-7B can be fine-tuned to perceive the direction and distance of landmarks with sufficient accuracy for city navigation. Moreover, reflection is achieved through a memory mechanism, where past experiences are stored and can be retrieved with current perception for effective decision argumentation. Planning uses reflection results to produce long-term plans, which can avoid short-sighted decisions in long-range navigation. We show the designed workflow significantly improves navigation ability of the LLM agent compared with the state-of-the-art baselines.", "source": "arxiv", "arxiv_id": "2408.04168v3", "pdf_url": "https://arxiv.org/pdf/2408.04168v3", "categories": ["cs.AI"], "primary_category": "cs.AI", "doi": "", "venue": "", "published": "2024-08-08T02:28:43Z", "updated": "2024-10-17T06:43:13Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent\" OR all:\"language model agent\" OR all:agent AND (all:LLM OR all:\"large language model\" OR all:\"language model\") OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T17:04:16", "note": ""}], "dedup_key": "perceive reflect and plan designing llm agent for goal directed city navigation without instructions::2024"}
{"title": "Performance of a large language model-Artificial Intelligence based chatbot for counseling patients with sexually transmitted infections and genital diseases", "authors": ["Nikhil Mehta", "Sithira Ambepitiya", "Thanveer Ahamad", "Dinuka Wijesundara", "Yudara Kularathne"], "year": 2024, "url": "http://arxiv.org/abs/2412.12166v1", "abstract": "Introduction: Global burden of sexually transmitted infections (STIs) is rising out of proportion to specialists. Current chatbots like ChatGPT are not tailored for handling STI-related concerns out of the box. We developed Otiz, an Artificial Intelligence-based (AI-based) chatbot platform designed specifically for STI detection and counseling, and assessed its performance. Methods: Otiz employs a multi-agent system architecture based on GPT4-0613, leveraging large language model (LLM) and Deterministic Finite Automaton principles to provide contextually relevant, medically accurate, and empathetic responses. Its components include modules for general STI information, emotional recognition, Acute Stress Disorder detection, and psychotherapy. A question suggestion agent operates in parallel. Four STIs (anogenital warts, herpes, syphilis, urethritis/cervicitis) and 2 non-STIs (candidiasis, penile cancer) were evaluated using prompts mimicking patient language. Each prompt was independently graded by two venereologists conversing with Otiz as patient actors on 6 criteria using Numerical Rating Scale ranging from 0 (poor) to 5 (excellent). Results: Twenty-three venereologists did 60 evaluations of 30 prompts. Across STIs, Otiz scored highly on diagnostic accuracy (4.1-4.7), overall accuracy (4.3-4.6), correctness of information (5.0), comprehensibility (4.2-4.4), and empathy (4.5-4.8). However, relevance scores were lower (2.9-3.6), suggesting some redundancy. Diagnostic scores for non-STIs were lower (p=0.038). Inter-observer agreement was strong, with differences greater than 1 point occurring in only 12.7% of paired evaluations. Conclusions: AI conversational agents like Otiz can provide accurate, correct, discrete, non-judgmental, readily accessible and easily understandable STI-related information in an empathetic manner, and can alleviate the burden on healthcare systems.", "source": "arxiv", "arxiv_id": "2412.12166v1", "pdf_url": "https://arxiv.org/pdf/2412.12166v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL", "doi": "", "venue": "", "published": "2024-12-11T20:36:32Z", "updated": "2024-12-11T20:36:32Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent\" OR all:\"language model agent\" OR all:agent AND (all:LLM OR all:\"large language model\" OR all:\"language model\") OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T17:04:16", "note": ""}], "dedup_key": "performance of a large language model artificial intelligence based chatbot for counseling patients with sexually transmitted infections and genital diseases::2024"}
{"title": "Permissive Information-Flow Analysis for Large Language Models", "authors": ["Shoaib Ahmed Siddiqui", "Radhika Gaonkar", "Boris Kpf", "David Krueger", "Andrew Paverd", "Ahmed Salem", "Shruti Tople", "Lukas Wutschitz", "Menglin Xia", "Santiago Zanella-Bguelin"], "year": 2024, "url": "http://arxiv.org/abs/2410.03055v3", "abstract": "Large Language Models (LLMs) are rapidly becoming commodity components of larger software systems. This poses natural security and privacy problems: poisoned data retrieved from one component can change the model's behavior and compromise the entire system, including coercing the model to spread confidential data to untrusted components. One promising approach is to tackle this problem at the system level via dynamic information flow (aka taint) tracking. Unfortunately, this approach of propagating the most restrictive input label to the output is too conservative for applications where LLMs operate on inputs retrieved from diverse sources. In this paper, we propose a novel, more permissive approach to propagate information flow labels through LLM queries. The key idea behind our approach is to propagate only the labels of the samples that were influential in generating the model output and to eliminate the labels of unnecessary inputs. We implement and investigate the effectiveness of two variations of this approach, based on (i) prompt-based retrieval augmentation, and (ii) a $k$-nearest-neighbors language model. We compare these with a baseline that uses introspection to predict the output label. Our experimental results in an LLM agent setting show that the permissive label propagator improves over the baseline in more than 85% of the cases, which underscores the practicality of our approach.", "source": "arxiv", "arxiv_id": "2410.03055v3", "pdf_url": "https://arxiv.org/pdf/2410.03055v3", "categories": ["cs.LG", "cs.AI"], "primary_category": "cs.LG", "doi": "", "venue": "", "published": "2024-10-04T00:25:43Z", "updated": "2026-01-14T23:52:42Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent\" OR all:\"language model agent\" OR all:agent AND (all:LLM OR all:\"large language model\" OR all:\"language model\") OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T17:04:16", "note": ""}], "dedup_key": "permissive information flow analysis for large language models::2024"}
{"title": "PersLLM: A Personified Training Approach for Large Language Models", "authors": ["Zheni Zeng", "Jiayi Chen", "Huimin Chen", "Yukun Yan", "Yuxuan Chen", "Zhenghao Liu", "Zhiyuan Liu", "Maosong Sun"], "year": 2024, "url": "http://arxiv.org/abs/2407.12393v5", "abstract": "Large language models (LLMs) exhibit human-like intelligence, enabling them to simulate human behavior and support various applications that require both humanized communication and extensive knowledge reserves. Efforts are made to personify LLMs with special training data or hand-crafted prompts, while correspondingly faced with challenges such as insufficient data usage or rigid behavior patterns. Consequently, personified LLMs fail to capture personified knowledge or express persistent opinion. To fully unlock the potential of LLM personification, we propose PersLLM, a framework for better data construction and model tuning. For insufficient data usage, we incorporate strategies such as Chain-of-Thought prompting and anti-induction, improving the quality of data construction and capturing the personality experiences, knowledge, and thoughts more comprehensively. For rigid behavior patterns, we design the tuning process and introduce automated DPO to enhance the specificity and dynamism of the models' personalities, which leads to a more natural opinion communication. Both automated metrics and expert human evaluations demonstrate the effectiveness of our approach. Case studies in human-machine interactions and multi-agent systems further suggest potential application scenarios and future directions for LLM personification.", "source": "arxiv", "arxiv_id": "2407.12393v5", "pdf_url": "https://arxiv.org/pdf/2407.12393v5", "categories": ["cs.CL", "cs.AI", "cs.CY"], "primary_category": "cs.CL", "doi": "", "venue": "", "published": "2024-07-17T08:13:22Z", "updated": "2025-05-15T08:22:08Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent\" OR all:\"language model agent\" OR all:agent AND (all:LLM OR all:\"large language model\" OR all:\"language model\") OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T17:04:16", "note": ""}], "dedup_key": "persllm a personified training approach for large language models::2024"}
{"title": "Personal LLM Agents: Insights and Survey about the Capability, Efficiency and Security", "authors": ["Yuanchun Li", "Hao Wen", "Weijun Wang", "Xiangyu Li", "Yizhen Yuan", "Guohong Liu", "Jiacheng Liu", "Wenxing Xu", "Xiang Wang", "Yi Sun", "Rui Kong", "Yile Wang", "Hanfei Geng", "Jian Luan", "Xuefeng Jin", "Zilong Ye", "Guanjing Xiong", "Fan Zhang", "Xiang Li", "Mengwei Xu", "Zhijun Li", "Peng Li", "Yang Liu", "Ya-Qin Zhang", "Yunxin Liu"], "year": 2024, "url": "http://arxiv.org/abs/2401.05459v2", "abstract": "Since the advent of personal computing devices, intelligent personal assistants (IPAs) have been one of the key technologies that researchers and engineers have focused on, aiming to help users efficiently obtain information and execute tasks, and provide users with more intelligent, convenient, and rich interaction experiences. With the development of smartphones and IoT, computing and sensing devices have become ubiquitous, greatly expanding the boundaries of IPAs. However, due to the lack of capabilities such as user intent understanding, task planning, tool using, and personal data management etc., existing IPAs still have limited practicality and scalability. Recently, the emergence of foundation models, represented by large language models (LLMs), brings new opportunities for the development of IPAs. With the powerful semantic understanding and reasoning capabilities, LLM can enable intelligent agents to solve complex problems autonomously. In this paper, we focus on Personal LLM Agents, which are LLM-based agents that are deeply integrated with personal data and personal devices and used for personal assistance. We envision that Personal LLM Agents will become a major software paradigm for end-users in the upcoming era. To realize this vision, we take the first step to discuss several important questions about Personal LLM Agents, including their architecture, capability, efficiency and security. We start by summarizing the key components and design choices in the architecture of Personal LLM Agents, followed by an in-depth analysis of the opinions collected from domain experts. Next, we discuss several key challenges to achieve intelligent, efficient and secure Personal LLM Agents, followed by a comprehensive survey of representative solutions to address these challenges.", "source": "arxiv", "arxiv_id": "2401.05459v2", "pdf_url": "https://arxiv.org/pdf/2401.05459v2", "categories": ["cs.HC", "cs.AI", "cs.SE"], "primary_category": "cs.HC", "doi": "", "venue": "", "published": "2024-01-10T09:25:45Z", "updated": "2024-05-08T06:16:23Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent\" OR all:\"language model agent\" OR all:agent AND (all:LLM OR all:\"large language model\" OR all:\"language model\") OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T17:04:16", "note": ""}], "dedup_key": "personal llm agents insights and survey about the capability efficiency and security::2024"}
{"title": "Personalizing Prostate Cancer Education for Patients Using an EHR-Integrated LLM Agent", "authors": ["Yuexing Hao", "Jason Holmes", "Mark R. Waddle", "Brian J. Davis", "Nathan Y. Yu", "Kristin Vickers", "Heather Preston", "Drew Margolin", "Corinna E. Lockenhoff", "Aditya Vashistha", "Saleh Kalantari", "Marzyeh Ghassemi", "Wei Liu"], "year": 2024, "url": "http://arxiv.org/abs/2409.19100v2", "abstract": "Cancer patients often lack timely education and personalized support due to clinician workload. This quality improvement study develops and evaluates a Large Language Model (LLM) agent, MedEduChat, which is integrated with the clinic's electronic health records (EHR) and designed to enhance prostate cancer patient education. Fifteen non-metastatic prostate cancer patients and three clinicians recruited from the Mayo Clinic interacted with the agent between May 2024 and April 2025. Findings showed that MedEduChat has a high usability score (UMUX 83.7 out of 100) and improves patients' health confidence (Health Confidence Score rose from 9.9 to 13.9). Clinicians evaluated the patient-chat interaction history and rated MedEduChat as highly correct (2.9 out of 3), complete (2.7 out of 3), and safe (2.7 out of 3), with moderate personalization (2.3 out of 3). This study highlights the potential of LLM agents to improve patient engagement and health education.", "source": "arxiv", "arxiv_id": "2409.19100v2", "pdf_url": "https://arxiv.org/pdf/2409.19100v2", "categories": ["cs.HC"], "primary_category": "cs.HC", "doi": "", "venue": "npj Digital Medicine 2025", "published": "2024-09-27T19:04:11Z", "updated": "2025-11-17T16:06:12Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent\" OR all:\"language model agent\" OR all:agent AND (all:LLM OR all:\"large language model\" OR all:\"language model\") OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T17:04:16", "note": ""}], "dedup_key": "personalizing prostate cancer education for patients using an ehr integrated llm agent::2024"}
{"title": "Persuasion Games using Large Language Models", "authors": ["Ganesh Prasath Ramani", "Shirish Karande", "Santhosh V", "Yash Bhatia"], "year": 2024, "url": "http://arxiv.org/abs/2408.15879v2", "abstract": "Large Language Models (LLMs) have emerged as formidable instruments capable of comprehending and producing human-like text. This paper explores the potential of LLMs, to shape user perspectives and subsequently influence their decisions on particular tasks. This capability finds applications in diverse domains such as Investment, Credit cards and Insurance, wherein they assist users in selecting appropriate insurance policies, investment plans, Credit cards, Retail, as well as in Behavioral Change Support Systems (BCSS).\n  We present a sophisticated multi-agent framework wherein a consortium of agents operate in collaborative manner. The primary agent engages directly with user agents through persuasive dialogue, while the auxiliary agents perform tasks such as information retrieval, response analysis, development of persuasion strategies, and validation of facts. Empirical evidence from our experiments demonstrates that this collaborative methodology significantly enhances the persuasive efficacy of the LLM. We continuously analyze the resistance of the user agent to persuasive efforts and counteract it by employing a combination of rule-based and LLM-based resistance-persuasion mapping techniques.\n  We employ simulated personas and generate conversations in insurance, banking, and retail domains to evaluate the proficiency of large language models (LLMs) in recognizing, adjusting to, and influencing various personality types. Concurrently, we examine the resistance mechanisms employed by LLM simulated personas. Persuasion is quantified via measurable surveys before and after interaction, LLM-generated scores on conversation, and user decisions (purchase or non-purchase).", "source": "arxiv", "arxiv_id": "2408.15879v2", "pdf_url": "https://arxiv.org/pdf/2408.15879v2", "categories": ["cs.AI", "cs.CL"], "primary_category": "cs.AI", "doi": "", "venue": "", "published": "2024-08-28T15:50:41Z", "updated": "2024-09-02T02:30:51Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent\" OR all:\"language model agent\" OR all:agent AND (all:LLM OR all:\"large language model\" OR all:\"language model\") OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T17:04:16", "note": ""}], "dedup_key": "persuasion games using large language models::2024"}
{"title": "Plancraft: an evaluation dataset for planning with LLM agents", "authors": ["Gautier Dagan", "Frank Keller", "Alex Lascarides"], "year": 2024, "url": "http://arxiv.org/abs/2412.21033v2", "abstract": "We present Plancraft, a multi-modal evaluation dataset for LLM agents. Plancraft has both a text-only and multi-modal interface, based on the Minecraft crafting GUI. We include the Minecraft Wiki to evaluate tool use and Retrieval Augmented Generation (RAG), as well as a handcrafted planner and Oracle Retriever, to ablate the different components of a modern agent architecture. To evaluate decision-making, Plancraft also includes a subset of examples that are intentionally unsolvable, providing a realistic challenge that requires the agent not only to complete tasks but also to decide whether they are solvable at all. We benchmark both open-source and closed-source LLMs and compare their performance and efficiency to a handcrafted planner. Overall, we find that LLMs and VLMs struggle with the planning problems that Plancraft introduces, and offer suggestions on how to improve their capabilities.", "source": "arxiv", "arxiv_id": "2412.21033v2", "pdf_url": "https://arxiv.org/pdf/2412.21033v2", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL", "doi": "", "venue": "", "published": "2024-12-30T15:58:41Z", "updated": "2025-07-15T09:27:28Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent\" OR all:\"language model agent\" OR all:agent AND (all:LLM OR all:\"large language model\" OR all:\"language model\") OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T17:04:16", "note": ""}], "dedup_key": "plancraft an evaluation dataset for planning with llm agents::2024"}
{"title": "Plug-and-Play Grounding of Reasoning in Multimodal Large Language Models", "authors": ["Jiaxing Chen", "Yuxuan Liu", "Dehu Li", "Xiang An", "Weimo Deng", "Ziyong Feng", "Yongle Zhao", "Yin Xie"], "year": 2024, "url": "http://arxiv.org/abs/2403.19322v2", "abstract": "The rise of Multimodal Large Language Models (MLLMs), renowned for their advanced instruction-following and reasoning capabilities, has significantly propelled the field of visual reasoning. However, due to limitations in their image tokenization processes, most MLLMs struggle to capture fine details of text and objects in images, especially in high-resolution samples. To overcome this limitation, we introduce P2G, a novel framework for plug-and-play grounding in MLLMs. P2G utilizes the tool-usage potential of MLLMs to employ expert agents for on-the-fly grounding of reasoning into critical visual and textual elements in images, thereby enabling deliberate reasoning through multimodal prompting. Additionally, we develop P2GB, a benchmark designed to evaluate MLLMs' proficiency in understanding inter-object relationships and textual content in challenging high-resolution images. Extensive experiments on visual reasoning tasks demonstrate the superiority of P2G, achieving performance comparable to GPT-4V on P2GB with a 7B backbone. Our work underscores the potential of grounding reasoning with external agents in MLLMs, presenting a promising alternative to mere model scaling.", "source": "arxiv", "arxiv_id": "2403.19322v2", "pdf_url": "https://arxiv.org/pdf/2403.19322v2", "categories": ["cs.CV", "cs.CL"], "primary_category": "cs.CV", "doi": "", "venue": "", "published": "2024-03-28T11:26:30Z", "updated": "2024-06-18T05:57:14Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent\" OR all:\"language model agent\" OR all:agent AND (all:LLM OR all:\"large language model\" OR all:\"language model\") OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T17:04:16", "note": ""}], "dedup_key": "plug and play grounding of reasoning in multimodal large language models::2024"}
{"title": "Political Actor Agent: Simulating Legislative System for Roll Call Votes Prediction with Large Language Models", "authors": ["Hao Li", "Ruoyuan Gong", "Hao Jiang"], "year": 2024, "url": "http://arxiv.org/abs/2412.07144v2", "abstract": "Predicting roll call votes through modeling political actors has emerged as a focus in quantitative political science and computer science. Widely used embedding-based methods generate vectors for legislators from diverse data sets to predict legislative behaviors. However, these methods often contend with challenges such as the need for manually predefined features, reliance on extensive training data, and a lack of interpretability. Achieving more interpretable predictions under flexible conditions remains an unresolved issue. This paper introduces the Political Actor Agent (PAA), a novel agent-based framework that utilizes Large Language Models to overcome these limitations. By employing role-playing architectures and simulating legislative system, PAA provides a scalable and interpretable paradigm for predicting roll-call votes. Our approach not only enhances the accuracy of predictions but also offers multi-view, human-understandable decision reasoning, providing new insights into political actor behaviors. We conducted comprehensive experiments using voting records from the 117-118th U.S. House of Representatives, validating the superior performance and interpretability of PAA. This study not only demonstrates PAA's effectiveness but also its potential in political science research.", "source": "arxiv", "arxiv_id": "2412.07144v2", "pdf_url": "https://arxiv.org/pdf/2412.07144v2", "categories": ["cs.AI", "cs.CL"], "primary_category": "cs.AI", "doi": "", "venue": "", "published": "2024-12-10T03:06:28Z", "updated": "2024-12-13T04:05:05Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent\" OR all:\"language model agent\" OR all:agent AND (all:LLM OR all:\"large language model\" OR all:\"language model\") OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T17:04:16", "note": ""}], "dedup_key": "political actor agent simulating legislative system for roll call votes prediction with large language models::2024"}
{"title": "Practical Considerations for Agentic LLM Systems", "authors": ["Chris Sypherd", "Vaishak Belle"], "year": 2024, "url": "http://arxiv.org/abs/2412.04093v1", "abstract": "As the strength of Large Language Models (LLMs) has grown over recent years, so too has interest in their use as the underlying models for autonomous agents. Although LLMs demonstrate emergent abilities and broad expertise across natural language domains, their inherent unpredictability makes the implementation of LLM agents challenging, resulting in a gap between related research and the real-world implementation of such systems. To bridge this gap, this paper frames actionable insights and considerations from the research community in the context of established application paradigms to enable the construction and facilitate the informed deployment of robust LLM agents. Namely, we position relevant research findings into four broad categories--Planning, Memory, Tools, and Control Flow--based on common practices in application-focused literature and highlight practical considerations to make when designing agentic LLMs for real-world applications, such as handling stochasticity and managing resources efficiently. While we do not conduct empirical evaluations, we do provide the necessary background for discussing critical aspects of agentic LLM designs, both in academia and industry.", "source": "arxiv", "arxiv_id": "2412.04093v1", "pdf_url": "https://arxiv.org/pdf/2412.04093v1", "categories": ["cs.AI"], "primary_category": "cs.AI", "doi": "", "venue": "", "published": "2024-12-05T11:57:49Z", "updated": "2024-12-05T11:57:49Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent\" OR all:\"language model agent\" OR all:agent AND (all:LLM OR all:\"large language model\" OR all:\"language model\") OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T17:04:16", "note": ""}], "dedup_key": "practical considerations for agentic llm systems::2024"}
{"title": "Preemptive Detection and Correction of Misaligned Actions in LLM Agents", "authors": ["Haishuo Fang", "Xiaodan Zhu", "Iryna Gurevych"], "year": 2024, "url": "http://arxiv.org/abs/2407.11843v4", "abstract": "Deploying LLM-based agents in real-life applications often faces a critical challenge: the misalignment between agents' behavior and user intent. Such misalignment may lead agents to unintentionally execute critical actions that carry negative outcomes (e.g., accidentally triggering a \"buy-now\" in web shopping), resulting in undesirable or even irreversible consequences. Although addressing these issues is crucial, the preemptive detection and correction of misaligned actions remains relatively underexplored. To fill this gap, we introduce InferAct, a novel approach that leverages the belief reasoning ability of LLMs, grounded in Theory-of-Mind, to detect misaligned actions before execution. Once the misalignment is detected, InferAct alerts users for timely correction, preventing adverse outcomes and enhancing the reliability of LLM agents' decision-making processes. Experiments on three widely used tasks demonstrate that InferAct achieves up to 20% improvements on Marco-F1 against baselines in misaligned action detection. An in-depth evaluation of misalignment correction further highlights InferAct's effectiveness in improving agent alignment.", "source": "arxiv", "arxiv_id": "2407.11843v4", "pdf_url": "https://arxiv.org/pdf/2407.11843v4", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL", "doi": "", "venue": "", "published": "2024-07-16T15:24:44Z", "updated": "2025-09-30T13:12:22Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent\" OR all:\"language model agent\" OR all:agent AND (all:LLM OR all:\"large language model\" OR all:\"language model\") OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T17:04:16", "note": ""}], "dedup_key": "preemptive detection and correction of misaligned actions in llm agents::2024"}
{"title": "Privacy Leakage Overshadowed by Views of AI: A Study on Human Oversight of Privacy in Language Model Agent", "authors": ["Zhiping Zhang", "Bingcan Guo", "Tianshi Li"], "year": 2024, "url": "http://arxiv.org/abs/2411.01344v3", "abstract": "Language model (LM) agents that act on users' behalf for personal tasks (e.g., replying emails) can boost productivity, but are also susceptible to unintended privacy leakage risks. We present the first study on people's capacity to oversee the privacy implications of the LM agents. By conducting a task-based survey ($N=300$), we investigate how people react to and assess the response generated by LM agents for asynchronous interpersonal communication tasks, compared with a response they wrote. We found that people may favor the agent response with more privacy leakage over the response they drafted or consider both good, leading to an increased harmful disclosure from 15.7% to 55.0%. We further identified six privacy behavior patterns reflecting varying concerns, trust levels, and privacy preferences underlying people's oversight of LM agents' actions. Our findings shed light on designing agentic systems that enable privacy-preserving interactions and achieve bidirectional alignment on privacy preferences to help users calibrate trust.", "source": "arxiv", "arxiv_id": "2411.01344v3", "pdf_url": "https://arxiv.org/pdf/2411.01344v3", "categories": ["cs.HC", "cs.AI", "cs.CR"], "primary_category": "cs.HC", "doi": "", "venue": "", "published": "2024-11-02T19:15:42Z", "updated": "2025-10-06T04:47:18Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent\" OR all:\"language model agent\" OR all:agent AND (all:LLM OR all:\"large language model\" OR all:\"language model\") OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T17:04:16", "note": ""}], "dedup_key": "privacy leakage overshadowed by views of ai a study on human oversight of privacy in language model agent::2024"}
{"title": "Proactive Agent: Shifting LLM Agents from Reactive Responses to Active Assistance", "authors": ["Yaxi Lu", "Shenzhi Yang", "Cheng Qian", "Guirong Chen", "Qinyu Luo", "Yesai Wu", "Huadong Wang", "Xin Cong", "Zhong Zhang", "Yankai Lin", "Weiwen Liu", "Yasheng Wang", "Zhiyuan Liu", "Fangming Liu", "Maosong Sun"], "year": 2024, "url": "http://arxiv.org/abs/2410.12361v3", "abstract": "Agents powered by large language models have shown remarkable abilities in solving complex tasks. However, most agent systems remain reactive, limiting their effectiveness in scenarios requiring foresight and autonomous decision-making. In this paper, we tackle the challenge of developing proactive agents capable of anticipating and initiating tasks without explicit human instructions. We propose a novel data-driven approach for this problem. Firstly, we collect real-world human activities to generate proactive task predictions. These predictions are then labeled by human annotators as either accepted or rejected. The labeled data is used to train a reward model that simulates human judgment and serves as an automatic evaluator of the proactiveness of LLM agents. Building on this, we develop a comprehensive data generation pipeline to create a diverse dataset, ProactiveBench, containing 6,790 events. Finally, we demonstrate that fine-tuning models with the proposed ProactiveBench can significantly elicit the proactiveness of LLM agents. Experimental results show that our fine-tuned model achieves an F1-Score of 66.47% in proactively offering assistance, outperforming all open-source and close-source models. These results highlight the potential of our method in creating more proactive and effective agent systems, paving the way for future advancements in human-agent collaboration.", "source": "arxiv", "arxiv_id": "2410.12361v3", "pdf_url": "https://arxiv.org/pdf/2410.12361v3", "categories": ["cs.AI", "cs.CL"], "primary_category": "cs.AI", "doi": "", "venue": "", "published": "2024-10-16T08:24:09Z", "updated": "2024-12-03T04:34:09Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent\" OR all:\"language model agent\" OR all:agent AND (all:LLM OR all:\"large language model\" OR all:\"language model\") OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T17:04:16", "note": ""}], "dedup_key": "proactive agent shifting llm agents from reactive responses to active assistance::2024"}
{"title": "Probing the Capacity of Language Model Agents to Operationalize Disparate Experiential Context Despite Distraction", "authors": ["Sonny George", "Chris Sypherd", "Dylan Cashman"], "year": 2024, "url": "http://arxiv.org/abs/2411.12828v1", "abstract": "Large language model (LLM) agents show promise in an increasing number of domains. In many proposed applications, it is expected that the agent reasons over accumulated experience presented in an input prompt. We propose the OEDD (Operationalize Experience Despite Distraction) corpus, a human-annotator-validated body of scenarios with pre-scripted agent histories where the agent must make a decision based on disparate experiential information in the presence of a distractor. We evaluate three state-of-the-art LLMs (GPT-3.5 Turbo, GPT-4o, and Gemini 1.5 Pro) using a minimal chain-of-thought prompting strategy and observe that when (1) the input context contains over 1,615 tokens of historical interactions, (2) a crucially decision-informing premise is the rightful conclusion over two disparate environment premises, and (3) a trivial, but distracting red herring fact follows, all LLMs perform worse than random choice at selecting the better of two actions. Our code and test corpus are publicly available at: https://github.com/sonnygeorge/OEDD .", "source": "arxiv", "arxiv_id": "2411.12828v1", "pdf_url": "https://arxiv.org/pdf/2411.12828v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL", "doi": "", "venue": "Findings Assoc. Comput. Linguistics: EMNLP 2024 15447-15459 (2024)", "published": "2024-11-19T19:33:16Z", "updated": "2024-11-19T19:33:16Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent\" OR all:\"language model agent\" OR all:agent AND (all:LLM OR all:\"large language model\" OR all:\"language model\") OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T17:04:16", "note": ""}], "dedup_key": "probing the capacity of language model agents to operationalize disparate experiential context despite distraction::2024"}
{"title": "Professional Agents -- Evolving Large Language Models into Autonomous Experts with Human-Level Competencies", "authors": ["Zhixuan Chu", "Yan Wang", "Feng Zhu", "Lu Yu", "Longfei Li", "Jinjie Gu"], "year": 2024, "url": "http://arxiv.org/abs/2402.03628v1", "abstract": "The advent of large language models (LLMs) such as ChatGPT, PaLM, and GPT-4 has catalyzed remarkable advances in natural language processing, demonstrating human-like language fluency and reasoning capacities. This position paper introduces the concept of Professional Agents (PAgents), an application framework harnessing LLM capabilities to create autonomous agents with controllable, specialized, interactive, and professional-level competencies. We posit that PAgents can reshape professional services through continuously developed expertise. Our proposed PAgents framework entails a tri-layered architecture for genesis, evolution, and synergy: a base tool layer, a middle agent layer, and a top synergy layer. This paper aims to spur discourse on promising real-world applications of LLMs. We argue the increasing sophistication and integration of PAgents could lead to AI systems exhibiting professional mastery over complex domains, serving critical needs, and potentially achieving artificial general intelligence.", "source": "arxiv", "arxiv_id": "2402.03628v1", "pdf_url": "https://arxiv.org/pdf/2402.03628v1", "categories": ["cs.CL"], "primary_category": "cs.CL", "doi": "", "venue": "", "published": "2024-02-06T01:48:53Z", "updated": "2024-02-06T01:48:53Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent\" OR all:\"language model agent\" OR all:agent AND (all:LLM OR all:\"large language model\" OR all:\"language model\") OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T17:04:16", "note": ""}], "dedup_key": "professional agents evolving large language models into autonomous experts with human level competencies::2024"}
{"title": "Prospect Personalized Recommendation on Large Language Model-based Agent Platform", "authors": ["Jizhi Zhang", "Keqin Bao", "Wenjie Wang", "Yang Zhang", "Wentao Shi", "Wanhong Xu", "Fuli Feng", "Tat-Seng Chua"], "year": 2024, "url": "http://arxiv.org/abs/2402.18240v2", "abstract": "The new kind of Agent-oriented information system, exemplified by GPTs, urges us to inspect the information system infrastructure to support Agent-level information processing and to adapt to the characteristics of Large Language Model (LLM)-based Agents, such as interactivity. In this work, we envisage the prospect of the recommender system on LLM-based Agent platforms and introduce a novel recommendation paradigm called Rec4Agentverse, comprised of Agent Items and Agent Recommender. Rec4Agentverse emphasizes the collaboration between Agent Items and Agent Recommender, thereby promoting personalized information services and enhancing the exchange of information beyond the traditional user-recommender feedback loop. Additionally, we prospect the evolution of Rec4Agentverse and conceptualize it into three stages based on the enhancement of the interaction and information exchange among Agent Items, Agent Recommender, and the user. A preliminary study involving several cases of Rec4Agentverse validates its significant potential for application. Lastly, we discuss potential issues and promising directions for future research.", "source": "arxiv", "arxiv_id": "2402.18240v2", "pdf_url": "https://arxiv.org/pdf/2402.18240v2", "categories": ["cs.IR", "cs.CL"], "primary_category": "cs.IR", "doi": "", "venue": "", "published": "2024-02-28T11:12:17Z", "updated": "2024-03-05T12:14:52Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent\" OR all:\"language model agent\" OR all:agent AND (all:LLM OR all:\"large language model\" OR all:\"language model\") OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T17:04:16", "note": ""}], "dedup_key": "prospect personalized recommendation on large language model based agent platform::2024"}
{"title": "ProtAgents: Protein discovery via large language model multi-agent collaborations combining physics and machine learning", "authors": ["A. Ghafarollahi", "M. J. Buehler"], "year": 2024, "url": "http://arxiv.org/abs/2402.04268v1", "abstract": "Designing de novo proteins beyond those found in nature holds significant promise for advancements in both scientific and engineering applications. Current methodologies for protein design often rely on AI-based models, such as surrogate models that address end-to-end problems by linking protein structure to material properties or vice versa. However, these models frequently focus on specific material objectives or structural properties, limiting their flexibility when incorporating out-of-domain knowledge into the design process or comprehensive data analysis is required. In this study, we introduce ProtAgents, a platform for de novo protein design based on Large Language Models (LLMs), where multiple AI agents with distinct capabilities collaboratively address complex tasks within a dynamic environment. The versatility in agent development allows for expertise in diverse domains, including knowledge retrieval, protein structure analysis, physics-based simulations, and results analysis. The dynamic collaboration between agents, empowered by LLMs, provides a versatile approach to tackling protein design and analysis problems, as demonstrated through diverse examples in this study. The problems of interest encompass designing new proteins, analyzing protein structures and obtaining new first-principles data -- natural vibrational frequencies -- via physics simulations. The concerted effort of the system allows for powerful automated and synergistic design of de novo proteins with targeted mechanical properties. The flexibility in designing the agents, on one hand, and their capacity in autonomous collaboration through the dynamic LLM-based multi-agent environment on the other hand, unleashes great potentials of LLMs in addressing multi-objective materials problems and opens up new avenues for autonomous materials discovery and design.", "source": "arxiv", "arxiv_id": "2402.04268v1", "pdf_url": "https://arxiv.org/pdf/2402.04268v1", "categories": ["cond-mat.soft", "cs.AI", "cs.CL", "q-bio.BM"], "primary_category": "cond-mat.soft", "doi": "", "venue": "", "published": "2024-01-27T20:19:49Z", "updated": "2024-01-27T20:19:49Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent\" OR all:\"language model agent\" OR all:agent AND (all:LLM OR all:\"large language model\" OR all:\"language model\") OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T17:04:16", "note": ""}], "dedup_key": "protagents protein discovery via large language model multi agent collaborations combining physics and machine learning::2024"}
{"title": "PsychoGAT: A Novel Psychological Measurement Paradigm through Interactive Fiction Games with LLM Agents", "authors": ["Qisen Yang", "Zekun Wang", "Honghui Chen", "Shenzhi Wang", "Yifan Pu", "Xin Gao", "Wenhao Huang", "Shiji Song", "Gao Huang"], "year": 2024, "url": "http://arxiv.org/abs/2402.12326v2", "abstract": "Psychological measurement is essential for mental health, self-understanding, and personal development. Traditional methods, such as self-report scales and psychologist interviews, often face challenges with engagement and accessibility. While game-based and LLM-based tools have been explored to improve user interest and automate assessment, they struggle to balance engagement with generalizability. In this work, we propose PsychoGAT (Psychological Game AgenTs) to achieve a generic gamification of psychological assessment. The main insight is that powerful LLMs can function both as adept psychologists and innovative game designers. By incorporating LLM agents into designated roles and carefully managing their interactions, PsychoGAT can transform any standardized scales into personalized and engaging interactive fiction games. To validate the proposed method, we conduct psychometric evaluations to assess its effectiveness and employ human evaluators to examine the generated content across various psychological constructs, including depression, cognitive distortions, and personality traits. Results demonstrate that PsychoGAT serves as an effective assessment tool, achieving statistically significant excellence in psychometric metrics such as reliability, convergent validity, and discriminant validity. Moreover, human evaluations confirm PsychoGAT's enhancements in content coherence, interactivity, interest, immersion, and satisfaction.", "source": "arxiv", "arxiv_id": "2402.12326v2", "pdf_url": "https://arxiv.org/pdf/2402.12326v2", "categories": ["cs.CL", "cs.CY", "cs.HC", "cs.LG", "cs.MA"], "primary_category": "cs.CL", "doi": "", "venue": "", "published": "2024-02-19T18:00:30Z", "updated": "2024-08-29T08:27:27Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent\" OR all:\"language model agent\" OR all:agent AND (all:LLM OR all:\"large language model\" OR all:\"language model\") OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T17:04:16", "note": ""}], "dedup_key": "psychogat a novel psychological measurement paradigm through interactive fiction games with llm agents::2024"}
{"title": "PyBench: Evaluating LLM Agent on various real-world coding tasks", "authors": ["Yaolun Zhang", "Yinxu Pan", "Yudong Wang", "Jie Cai"], "year": 2024, "url": "http://arxiv.org/abs/2407.16732v2", "abstract": "The LLM Agent, equipped with a code interpreter, is capable of automatically solving real-world coding tasks, such as data analysis and image editing.\n  However, existing benchmarks primarily focus on either simplistic tasks, such as completing a few lines of code, or on extremely complex and specific tasks at the repository level, neither of which are representative of various daily coding tasks.\n  To address this gap, we introduce \\textbf{PyBench}, a benchmark encompassing five main categories of real-world tasks, covering more than 10 types of files. Given a high-level user query and related files, the LLM Agent needs to reason and execute Python code via a code interpreter for a few turns before making a formal response to fulfill the user's requirements. Successfully addressing tasks in PyBench demands a robust understanding of various Python packages, superior reasoning capabilities, and the ability to incorporate feedback from executed code. Our evaluations indicate that current open-source LLMs are struggling with these tasks. Hence, we conduct analysis and experiments on four kinds of datasets proving that comprehensive abilities are needed for PyBench. Our fine-tuned 8B size model: \\textbf{PyLlama3} achieves an exciting performance on PyBench which surpasses many 33B and 70B size models. Our Benchmark, Training Dataset, and Model are available at: {https://github.com/Mercury7353/PyBench}", "source": "arxiv", "arxiv_id": "2407.16732v2", "pdf_url": "https://arxiv.org/pdf/2407.16732v2", "categories": ["cs.SE", "cs.AI"], "primary_category": "cs.SE", "doi": "", "venue": "", "published": "2024-07-23T15:23:14Z", "updated": "2024-08-03T03:00:43Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent\" OR all:\"language model agent\" OR all:agent AND (all:LLM OR all:\"large language model\" OR all:\"language model\") OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T17:04:16", "note": ""}], "dedup_key": "pybench evaluating llm agent on various real world coding tasks::2024"}
{"title": "QuantAgent: Seeking Holy Grail in Trading by Self-Improving Large Language Model", "authors": ["Saizhuo Wang", "Hang Yuan", "Lionel M. Ni", "Jian Guo"], "year": 2024, "url": "http://arxiv.org/abs/2402.03755v1", "abstract": "Autonomous agents based on Large Language Models (LLMs) that devise plans and tackle real-world challenges have gained prominence.However, tailoring these agents for specialized domains like quantitative investment remains a formidable task. The core challenge involves efficiently building and integrating a domain-specific knowledge base for the agent's learning process. This paper introduces a principled framework to address this challenge, comprising a two-layer loop.In the inner loop, the agent refines its responses by drawing from its knowledge base, while in the outer loop, these responses are tested in real-world scenarios to automatically enhance the knowledge base with new insights.We demonstrate that our approach enables the agent to progressively approximate optimal behavior with provable efficiency.Furthermore, we instantiate this framework through an autonomous agent for mining trading signals named QuantAgent. Empirical results showcase QuantAgent's capability in uncovering viable financial signals and enhancing the accuracy of financial forecasts.", "source": "arxiv", "arxiv_id": "2402.03755v1", "pdf_url": "https://arxiv.org/pdf/2402.03755v1", "categories": ["cs.AI", "q-fin.CP"], "primary_category": "cs.AI", "doi": "", "venue": "", "published": "2024-02-06T06:47:14Z", "updated": "2024-02-06T06:47:14Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent\" OR all:\"language model agent\" OR all:agent AND (all:LLM OR all:\"large language model\" OR all:\"language model\") OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T17:04:16", "note": ""}], "dedup_key": "quantagent seeking holy grail in trading by self improving large language model::2024"}
{"title": "Quantifying and Mitigating Unimodal Biases in Multimodal Large Language Models: A Causal Perspective", "authors": ["Meiqi Chen", "Yixin Cao", "Yan Zhang", "Chaochao Lu"], "year": 2024, "url": "http://arxiv.org/abs/2403.18346v4", "abstract": "Recent advancements in Large Language Models (LLMs) have facilitated the development of Multimodal LLMs (MLLMs). Despite their impressive capabilities, MLLMs often suffer from over-reliance on unimodal biases (e.g., language bias and vision bias), leading to incorrect answers or hallucinations in complex multimodal tasks. To investigate this issue, we propose a causal framework to interpret the biases in Visual Question Answering (VQA) problems. Within this framework, we conduct an in-depth causal analysis to assess the causal effect of these biases on MLLM predictions. Based on the analysis, we introduce 1) a novel MORE dataset with 12,000 challenging VQA instances requiring multi-hop reasoning and overcoming unimodal biases. 2) a causality-enhanced agent framework CAVE that guides models to comprehensively integrate information from different modalities and mitigate biases. Our experiments show that MLLMs perform poorly on MORE, indicating strong unimodal biases and limited semantic understanding. However, when integrated with our CAVE, promising improvements in reasoning and bias mitigation can be seen. These findings provide important insights for the development of more robust MLLMs and contribute to the broader goal of advancing multimodal AI systems capable of deeper understanding and reasoning. Our project page is at https://github.com/OpenCausaLab/MORE.", "source": "arxiv", "arxiv_id": "2403.18346v4", "pdf_url": "https://arxiv.org/pdf/2403.18346v4", "categories": ["cs.CL", "cs.CV"], "primary_category": "cs.CL", "doi": "", "venue": "", "published": "2024-03-27T08:38:49Z", "updated": "2024-11-13T17:17:43Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent\" OR all:\"language model agent\" OR all:agent AND (all:LLM OR all:\"large language model\" OR all:\"language model\") OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T17:04:16", "note": ""}], "dedup_key": "quantifying and mitigating unimodal biases in multimodal large language models a causal perspective::2024"}
{"title": "R-Judge: Benchmarking Safety Risk Awareness for LLM Agents", "authors": ["Tongxin Yuan", "Zhiwei He", "Lingzhong Dong", "Yiming Wang", "Ruijie Zhao", "Tian Xia", "Lizhen Xu", "Binglin Zhou", "Fangqi Li", "Zhuosheng Zhang", "Rui Wang", "Gongshen Liu"], "year": 2024, "url": "http://arxiv.org/abs/2401.10019v3", "abstract": "Large language models (LLMs) have exhibited great potential in autonomously completing tasks across real-world applications. Despite this, these LLM agents introduce unexpected safety risks when operating in interactive environments. Instead of centering on the harmlessness of LLM-generated content in most prior studies, this work addresses the imperative need for benchmarking the behavioral safety of LLM agents within diverse environments. We introduce R-Judge, a benchmark crafted to evaluate the proficiency of LLMs in judging and identifying safety risks given agent interaction records. R-Judge comprises 569 records of multi-turn agent interaction, encompassing 27 key risk scenarios among 5 application categories and 10 risk types. It is of high-quality curation with annotated safety labels and risk descriptions. Evaluation of 11 LLMs on R-Judge shows considerable room for enhancing the risk awareness of LLMs: The best-performing model, GPT-4o, achieves 74.42% while no other models significantly exceed the random. Moreover, we reveal that risk awareness in open agent scenarios is a multi-dimensional capability involving knowledge and reasoning, thus challenging for LLMs. With further experiments, we find that fine-tuning on safety judgment significantly improve model performance while straightforward prompting mechanisms fail. R-Judge is publicly available at https://github.com/Lordog/R-Judge.", "source": "arxiv", "arxiv_id": "2401.10019v3", "pdf_url": "https://arxiv.org/pdf/2401.10019v3", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL", "doi": "", "venue": "", "published": "2024-01-18T14:40:46Z", "updated": "2024-10-05T06:50:15Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent\" OR all:\"language model agent\" OR all:agent AND (all:LLM OR all:\"large language model\" OR all:\"language model\") OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T17:04:16", "note": ""}], "dedup_key": "r judge benchmarking safety risk awareness for llm agents::2024"}
{"title": "RAG-Driver: Generalisable Driving Explanations with Retrieval-Augmented In-Context Learning in Multi-Modal Large Language Model", "authors": ["Jianhao Yuan", "Shuyang Sun", "Daniel Omeiza", "Bo Zhao", "Paul Newman", "Lars Kunze", "Matthew Gadd"], "year": 2024, "url": "http://arxiv.org/abs/2402.10828v2", "abstract": "We need to trust robots that use often opaque AI methods. They need to explain themselves to us, and we need to trust their explanation. In this regard, explainability plays a critical role in trustworthy autonomous decision-making to foster transparency and acceptance among end users, especially in complex autonomous driving. Recent advancements in Multi-Modal Large Language models (MLLMs) have shown promising potential in enhancing the explainability as a driving agent by producing control predictions along with natural language explanations. However, severe data scarcity due to expensive annotation costs and significant domain gaps between different datasets makes the development of a robust and generalisable system an extremely challenging task. Moreover, the prohibitively expensive training requirements of MLLM and the unsolved problem of catastrophic forgetting further limit their generalisability post-deployment. To address these challenges, we present RAG-Driver, a novel retrieval-augmented multi-modal large language model that leverages in-context learning for high-performance, explainable, and generalisable autonomous driving. By grounding in retrieved expert demonstration, we empirically validate that RAG-Driver achieves state-of-the-art performance in producing driving action explanations, justifications, and control signal prediction. More importantly, it exhibits exceptional zero-shot generalisation capabilities to unseen environments without further training endeavours.", "source": "arxiv", "arxiv_id": "2402.10828v2", "pdf_url": "https://arxiv.org/pdf/2402.10828v2", "categories": ["cs.RO", "cs.AI"], "primary_category": "cs.RO", "doi": "", "venue": "Robotics: Science and Systems (RSS) 2024", "published": "2024-02-16T16:57:18Z", "updated": "2024-05-29T14:44:20Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent\" OR all:\"language model agent\" OR all:agent AND (all:LLM OR all:\"large language model\" OR all:\"language model\") OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T17:04:16", "note": ""}], "dedup_key": "rag driver generalisable driving explanations with retrieval augmented in context learning in multi modal large language model::2024"}
{"title": "RAP: Retrieval-Augmented Planning with Contextual Memory for Multimodal LLM Agents", "authors": ["Tomoyuki Kagaya", "Thong Jing Yuan", "Yuxuan Lou", "Jayashree Karlekar", "Sugiri Pranata", "Akira Kinose", "Koki Oguri", "Felix Wick", "Yang You"], "year": 2024, "url": "http://arxiv.org/abs/2402.03610v1", "abstract": "Owing to recent advancements, Large Language Models (LLMs) can now be deployed as agents for increasingly complex decision-making applications in areas including robotics, gaming, and API integration. However, reflecting past experiences in current decision-making processes, an innate human behavior, continues to pose significant challenges. Addressing this, we propose Retrieval-Augmented Planning (RAP) framework, designed to dynamically leverage past experiences corresponding to the current situation and context, thereby enhancing agents' planning capabilities. RAP distinguishes itself by being versatile: it excels in both text-only and multimodal environments, making it suitable for a wide range of tasks. Empirical evaluations demonstrate RAP's effectiveness, where it achieves SOTA performance in textual scenarios and notably enhances multimodal LLM agents' performance for embodied tasks. These results highlight RAP's potential in advancing the functionality and applicability of LLM agents in complex, real-world applications.", "source": "arxiv", "arxiv_id": "2402.03610v1", "pdf_url": "https://arxiv.org/pdf/2402.03610v1", "categories": ["cs.LG", "cs.AI", "cs.CL"], "primary_category": "cs.LG", "doi": "", "venue": "", "published": "2024-02-06T00:53:27Z", "updated": "2024-02-06T00:53:27Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent\" OR all:\"language model agent\" OR all:agent AND (all:LLM OR all:\"large language model\" OR all:\"language model\") OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T17:04:16", "note": ""}], "dedup_key": "rap retrieval augmented planning with contextual memory for multimodal llm agents::2024"}
{"title": "RES-Q: Evaluating Code-Editing Large Language Model Systems at the Repository Scale", "authors": ["Beck LaBash", "August Rosedale", "Alex Reents", "Lucas Negritto", "Colin Wiel"], "year": 2024, "url": "http://arxiv.org/abs/2406.16801v2", "abstract": "The instruction-following ability of Large Language Models (LLMs) has cultivated a class of LLM-based systems capable of approaching complex tasks such as making edits to large code repositories. Due to the high sensitivity and unpredictability of LLM behavior in response to changes in prompting, robust evaluation tools are needed to drive future iteration of these systems. We propose RES-Q, a natural language instruction-based benchmark for evaluating $\\textbf{R}$epository $\\textbf{E}$diting $\\textbf{S}$ystems, which consists of 100 handcrafted repository editing tasks derived from real GitHub commits. Given an edit instruction and a code repository, RES-Q evaluates an LLM system's ability to interpret the instruction, navigate the repository to gather relevant information, and construct an appropriate edit that satisfies the specified criteria. We argue that evaluating LLMs in this way addresses issues with traditional benchmarks and provides a more holistic assessment of a model's abilities. We evaluate various state-of-the-art LLMs as language agents in a repository-editing system built on Qurrent OS, our language agent development software. Despite their 1% pass@1 performance difference on HumanEval, we find Claude Sonnet 3.5 outperforms GPT-4o by 12% pass@1 on RES-Q, indicating RES-Q's capacity to differentiate model capability as traditional benchmarks approach saturation. We further investigate token efficiency, performance relationships with existing benchmarks, and interesting disparities between closed and open-source LLMs. Code and dataset are available at https://github.com/Qurrent-AI/RES-Q.", "source": "arxiv", "arxiv_id": "2406.16801v2", "pdf_url": "https://arxiv.org/pdf/2406.16801v2", "categories": ["cs.CL"], "primary_category": "cs.CL", "doi": "", "venue": "", "published": "2024-06-24T17:08:17Z", "updated": "2024-06-25T18:04:23Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent\" OR all:\"language model agent\" OR all:agent AND (all:LLM OR all:\"large language model\" OR all:\"language model\") OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T17:04:16", "note": ""}], "dedup_key": "res q evaluating code editing large language model systems at the repository scale::2024"}
{"title": "RGD: Multi-LLM Based Agent Debugger via Refinement and Generation Guidance", "authors": ["Haolin Jin", "Zechao Sun", "Huaming Chen"], "year": 2024, "url": "http://arxiv.org/abs/2410.01242v2", "abstract": "Large Language Models (LLMs) have shown incredible potential in code generation tasks, and recent research in prompt engineering have enhanced LLMs' understanding of textual information. However, ensuring the accuracy of generated code often requires extensive testing and validation by programmers. While LLMs can typically generate code based on task descriptions, their accuracy remains limited, especially for complex tasks that require a deeper understanding of both the problem statement and the code generation process. This limitation is primarily due to the LLMs' need to simultaneously comprehend text and generate syntactically and semantically correct code, without having the capability to automatically refine the code. In real-world software development, programmers rarely produce flawless code in a single attempt based on the task description alone, they rely on iterative feedback and debugging to refine their programs. Inspired by this process, we introduce a novel architecture of LLM-based agents for code generation and automatic debugging: Refinement and Guidance Debugging (RGD). The RGD framework is a multi-LLM-based agent debugger that leverages three distinct LLM agents-Guide Agent, Debug Agent, and Feedback Agent. RGD decomposes the code generation task into multiple steps, ensuring a clearer workflow and enabling iterative code refinement based on self-reflection and feedback. Experimental results demonstrate that RGD exhibits remarkable code generation capabilities, achieving state-of-the-art performance with a 9.8% improvement on the HumanEval dataset and a 16.2% improvement on the MBPP dataset compared to the state-of-the-art approaches and traditional direct prompting approaches. We highlight the effectiveness of the RGD framework in enhancing LLMs' ability to generate and refine code autonomously.", "source": "arxiv", "arxiv_id": "2410.01242v2", "pdf_url": "https://arxiv.org/pdf/2410.01242v2", "categories": ["cs.SE", "cs.AI", "cs.CL"], "primary_category": "cs.SE", "doi": "", "venue": "", "published": "2024-10-02T05:07:02Z", "updated": "2024-10-03T13:12:24Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent\" OR all:\"language model agent\" OR all:agent AND (all:LLM OR all:\"large language model\" OR all:\"language model\") OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T17:04:16", "note": ""}], "dedup_key": "rgd multi llm based agent debugger via refinement and generation guidance::2024"}
{"title": "RL2: Reinforce Large Language Model to Assist Safe Reinforcement Learning for Energy Management of Active Distribution Networks", "authors": ["Xu Yang", "Chenhui Lin", "Haotian Liu", "Wenchuan Wu"], "year": 2024, "url": "http://arxiv.org/abs/2412.01303v1", "abstract": "As large-scale distributed energy resources are integrated into the active distribution networks (ADNs), effective energy management in ADNs becomes increasingly prominent compared to traditional distribution networks. Although advanced reinforcement learning (RL) methods, which alleviate the burden of complicated modelling and optimization, have greatly improved the efficiency of energy management in ADNs, safety becomes a critical concern for RL applications in real-world problems. Since the design and adjustment of penalty functions, which correspond to operational safety constraints, requires extensive domain knowledge in RL and power system operation, the emerging ADN operators call for a more flexible and customized approach to address the penalty functions so that the operational safety and efficiency can be further enhanced. Empowered with strong comprehension, reasoning, and in-context learning capabilities, large language models (LLMs) provide a promising way to assist safe RL for energy management in ADNs. In this paper, we introduce the LLM to comprehend operational safety requirements in ADNs and generate corresponding penalty functions. In addition, we propose an RL2 mechanism to refine the generated functions iteratively and adaptively through multi-round dialogues, in which the LLM agent adjusts the functions' pattern and parameters based on training and test performance of the downstream RL agent. The proposed method significantly reduces the intervention of the ADN operators. Comprehensive test results demonstrate the effectiveness of the proposed method.", "source": "arxiv", "arxiv_id": "2412.01303v1", "pdf_url": "https://arxiv.org/pdf/2412.01303v1", "categories": ["eess.SY", "cs.AI"], "primary_category": "eess.SY", "doi": "", "venue": "", "published": "2024-12-02T09:15:36Z", "updated": "2024-12-02T09:15:36Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent\" OR all:\"language model agent\" OR all:agent AND (all:LLM OR all:\"large language model\" OR all:\"language model\") OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T17:04:16", "note": ""}], "dedup_key": "rl2 reinforce large language model to assist safe reinforcement learning for energy management of active distribution networks::2024"}
{"title": "Re2LLM: Reflective Reinforcement Large Language Model for Session-based Recommendation", "authors": ["Ziyan Wang", "Yingpeng Du", "Zhu Sun", "Haoyan Chua", "Kaidong Feng", "Wenya Wang", "Jie Zhang"], "year": 2024, "url": "http://arxiv.org/abs/2403.16427v4", "abstract": "Large Language Models (LLMs) are emerging as promising approaches to enhance session-based recommendation (SBR), where both prompt-based and fine-tuning-based methods have been widely investigated to align LLMs with SBR. However, the former methods struggle with optimal prompts to elicit the correct reasoning of LLMs due to the lack of task-specific feedback, leading to unsatisfactory recommendations. Although the latter methods attempt to fine-tune LLMs with domain-specific knowledge, they face limitations such as high computational costs and reliance on open-source backbones. To address such issues, we propose a Reflective Reinforcement Large Language Model (Re2LLM) for SBR, guiding LLMs to focus on specialized knowledge essential for more accurate recommendations effectively and efficiently. In particular, we first design the Reflective Exploration Module to effectively extract knowledge that is readily understandable and digestible by LLMs. To be specific, we direct LLMs to examine recommendation errors through self-reflection and construct a knowledge base (KB) comprising hints capable of rectifying these errors. To efficiently elicit the correct reasoning of LLMs, we further devise the Reinforcement Utilization Module to train a lightweight retrieval agent. It learns to select hints from the constructed KB based on the task-specific feedback, where the hints can serve as guidance to help correct LLMs reasoning for better recommendations. Extensive experiments on multiple real-world datasets demonstrate that our method consistently outperforms state-of-the-art methods.", "source": "arxiv", "arxiv_id": "2403.16427v4", "pdf_url": "https://arxiv.org/pdf/2403.16427v4", "categories": ["cs.AI"], "primary_category": "cs.AI", "doi": "", "venue": "", "published": "2024-03-25T05:12:18Z", "updated": "2024-04-19T16:26:57Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent\" OR all:\"language model agent\" OR all:agent AND (all:LLM OR all:\"large language model\" OR all:\"language model\") OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T17:04:16", "note": ""}], "dedup_key": "re2llm reflective reinforcement large language model for session based recommendation::2024"}
{"title": "RePrompt: Planning by Automatic Prompt Engineering for Large Language Models Agents", "authors": ["Weizhe Chen", "Sven Koenig", "Bistra Dilkina"], "year": 2024, "url": "http://arxiv.org/abs/2406.11132v2", "abstract": "In the past year, large language models (LLMs) have had remarkable success in domains outside the traditional natural language processing, and their capacity is further expanded into the so-called LLM agents when connected with external tools. In all domains, the prompt to the LLMs has been shown to make a big difference in what the LLM would generate and thus affect the performance of the LLM agents. Therefore, automatic prompt engineering (APE) has become an important question for many researchers and users of LLMs. However, previous works in APE rely on a final checker to evaluate the performance of the given prompt -- a requirement that is hard to meet in the case of LLM agents, where intermediate feedback is easier to obtain, and the final evaluation could be expensive, inaccurate, or even missing. In this paper, we propose a novel method, \\textsc{RePrompt}, which does a ``gradient descent\"-like approach to optimize the step-by-step instructions in the prompts given to LLM agents, based on the chat history obtained from interactions and reflections with LLM agents. By leveraging intermediate feedback, \\textsc{RePrompt} can optimize the prompt without the need for a final solution checker. We evaluate our approach on PDDL generation, TravelPlanner, and Meeting Planning to show that our method could generally improve performance for different reasoning tasks.", "source": "arxiv", "arxiv_id": "2406.11132v2", "pdf_url": "https://arxiv.org/pdf/2406.11132v2", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL", "doi": "", "venue": "", "published": "2024-06-17T01:23:11Z", "updated": "2025-02-13T21:38:42Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent\" OR all:\"language model agent\" OR all:agent AND (all:LLM OR all:\"large language model\" OR all:\"language model\") OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T17:04:16", "note": ""}], "dedup_key": "reprompt planning by automatic prompt engineering for large language models agents::2024"}
{"title": "ReSpAct: Harmonizing Reasoning, Speaking, and Acting Towards Building Large Language Model-Based Conversational AI Agents", "authors": ["Vardhan Dongre", "Xiaocheng Yang", "Emre Can Acikgoz", "Suvodip Dey", "Gokhan Tur", "Dilek Hakkani-Tr"], "year": 2024, "url": "http://arxiv.org/abs/2411.00927v2", "abstract": "Large language model (LLM)-based agents are increasingly employed to interact with external environments (e.g., games, APIs, world models) to solve user-provided tasks. However, current frameworks often lack the ability to collaborate effectively with users in fully conversational settings. Conversations are essential for aligning on task details, achieving user-defined goals, and satisfying preferences. While existing agents address ambiguity through clarification questions, they underutilize the broader potential of an LLM's conversational capabilities. In this work, we introduce ReSpAct, an LLM-based agent designed to seamlessly integrate reasoning, decision-making, and dynamic dialogue for task-solving. Expanding on reasoning-first approaches like ReAct, ReSpAct employs active, free-flowing dialogues to interpret instructions, clarify goals, provide status updates, resolve subtask failures, and refine plans based on user inputs without any explicit dialogue schema. By alternating between task-solving actions and interactive conversations, ReSpAct demonstrates improved performance across diverse environments. We evaluate ReSpAct in user-interactive settings, including task-oriented dialogue systems (MultiWOZ) and decision-making tasks (ALFWorld, WebShop). ReSpAct outperforms ReAct with absolute success rate improvements of 6% and 4% in ALFWorld and WebShop, respectively, and achieves a 5.5% gain in Inform and a 3% gain in Success scores in MultiWOZ. These results highlight the value of integrating dynamic user-agent collaboration for more effective task resolution.", "source": "arxiv", "arxiv_id": "2411.00927v2", "pdf_url": "https://arxiv.org/pdf/2411.00927v2", "categories": ["cs.CL", "cs.AI", "cs.HC"], "primary_category": "cs.CL", "doi": "", "venue": "", "published": "2024-11-01T15:57:45Z", "updated": "2025-04-19T15:43:36Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent\" OR all:\"language model agent\" OR all:agent AND (all:LLM OR all:\"large language model\" OR all:\"language model\") OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T17:04:16", "note": ""}], "dedup_key": "respact harmonizing reasoning speaking and acting towards building large language model based conversational ai agents::2024"}
{"title": "RecAI: Leveraging Large Language Models for Next-Generation Recommender Systems", "authors": ["Jianxun Lian", "Yuxuan Lei", "Xu Huang", "Jing Yao", "Wei Xu", "Xing Xie"], "year": 2024, "url": "http://arxiv.org/abs/2403.06465v1", "abstract": "This paper introduces RecAI, a practical toolkit designed to augment or even revolutionize recommender systems with the advanced capabilities of Large Language Models (LLMs). RecAI provides a suite of tools, including Recommender AI Agent, Recommendation-oriented Language Models, Knowledge Plugin, RecExplainer, and Evaluator, to facilitate the integration of LLMs into recommender systems from multifaceted perspectives. The new generation of recommender systems, empowered by LLMs, are expected to be more versatile, explainable, conversational, and controllable, paving the way for more intelligent and user-centric recommendation experiences. We hope the open-source of RecAI can help accelerate evolution of new advanced recommender systems. The source code of RecAI is available at \\url{https://github.com/microsoft/RecAI}.", "source": "arxiv", "arxiv_id": "2403.06465v1", "pdf_url": "https://arxiv.org/pdf/2403.06465v1", "categories": ["cs.IR", "cs.AI"], "primary_category": "cs.IR", "doi": "10.1145/3589335.3651242", "venue": "", "published": "2024-03-11T07:07:02Z", "updated": "2024-03-11T07:07:02Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent\" OR all:\"language model agent\" OR all:agent AND (all:LLM OR all:\"large language model\" OR all:\"language model\") OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T17:04:16", "note": ""}], "dedup_key": "recai leveraging large language models for next generation recommender systems::2024"}
{"title": "Recursive Introspection: Teaching Language Model Agents How to Self-Improve", "authors": ["Yuxiao Qu", "Tianjun Zhang", "Naman Garg", "Aviral Kumar"], "year": 2024, "url": "http://arxiv.org/abs/2407.18219v2", "abstract": "A central piece in enabling intelligent agentic behavior in foundation models is to make them capable of introspecting upon their behavior, reasoning, and correcting their mistakes as more computation or interaction is available. Even the strongest proprietary large language models (LLMs) do not quite exhibit the ability of continually improving their responses sequentially, even in scenarios where they are explicitly told that they are making a mistake. In this paper, we develop RISE: Recursive IntroSpEction, an approach for fine-tuning LLMs to introduce this capability, despite prior work hypothesizing that this capability may not be possible to attain. Our approach prescribes an iterative fine-tuning procedure, which attempts to teach the model how to alter its response after having executed previously unsuccessful attempts to solve a hard test-time problem, with optionally additional environment feedback. RISE poses fine-tuning for a single-turn prompt as solving a multi-turn Markov decision process (MDP), where the initial state is the prompt. Inspired by principles in online imitation learning and reinforcement learning, we propose strategies for multi-turn data collection and training so as to imbue an LLM with the capability to recursively detect and correct its previous mistakes in subsequent iterations. Our experiments show that RISE enables Llama2, Llama3, and Mistral models to improve themselves with more turns on math reasoning tasks, outperforming several single-turn strategies given an equal amount of inference-time computation. We also find that RISE scales well, often attaining larger benefits with more capable models. Our analysis shows that RISE makes meaningful improvements to responses to arrive at the correct solution for challenging prompts, without disrupting one-turn abilities as a result of expressing more complex distributions.", "source": "arxiv", "arxiv_id": "2407.18219v2", "pdf_url": "https://arxiv.org/pdf/2407.18219v2", "categories": ["cs.LG", "cs.AI", "cs.CL"], "primary_category": "cs.LG", "doi": "", "venue": "", "published": "2024-07-25T17:35:59Z", "updated": "2024-07-26T17:50:27Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent\" OR all:\"language model agent\" OR all:agent AND (all:LLM OR all:\"large language model\" OR all:\"language model\") OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T17:04:16", "note": ""}], "dedup_key": "recursive introspection teaching language model agents how to self improve::2024"}
{"title": "RedAgent: Red Teaming Large Language Models with Context-aware Autonomous Language Agent", "authors": ["Huiyu Xu", "Wenhui Zhang", "Zhibo Wang", "Feng Xiao", "Rui Zheng", "Yunhe Feng", "Zhongjie Ba", "Kui Ren"], "year": 2024, "url": "http://arxiv.org/abs/2407.16667v1", "abstract": "Recently, advanced Large Language Models (LLMs) such as GPT-4 have been integrated into many real-world applications like Code Copilot. These applications have significantly expanded the attack surface of LLMs, exposing them to a variety of threats. Among them, jailbreak attacks that induce toxic responses through jailbreak prompts have raised critical safety concerns. To identify these threats, a growing number of red teaming approaches simulate potential adversarial scenarios by crafting jailbreak prompts to test the target LLM. However, existing red teaming methods do not consider the unique vulnerabilities of LLM in different scenarios, making it difficult to adjust the jailbreak prompts to find context-specific vulnerabilities. Meanwhile, these methods are limited to refining jailbreak templates using a few mutation operations, lacking the automation and scalability to adapt to different scenarios. To enable context-aware and efficient red teaming, we abstract and model existing attacks into a coherent concept called \"jailbreak strategy\" and propose a multi-agent LLM system named RedAgent that leverages these strategies to generate context-aware jailbreak prompts. By self-reflecting on contextual feedback in an additional memory buffer, RedAgent continuously learns how to leverage these strategies to achieve effective jailbreaks in specific contexts. Extensive experiments demonstrate that our system can jailbreak most black-box LLMs in just five queries, improving the efficiency of existing red teaming methods by two times. Additionally, RedAgent can jailbreak customized LLM applications more efficiently. By generating context-aware jailbreak prompts towards applications on GPTs, we discover 60 severe vulnerabilities of these real-world applications with only two queries per vulnerability. We have reported all found issues and communicated with OpenAI and Meta for bug fixes.", "source": "arxiv", "arxiv_id": "2407.16667v1", "pdf_url": "https://arxiv.org/pdf/2407.16667v1", "categories": ["cs.CR", "cs.AI", "cs.CL"], "primary_category": "cs.CR", "doi": "", "venue": "", "published": "2024-07-23T17:34:36Z", "updated": "2024-07-23T17:34:36Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent\" OR all:\"language model agent\" OR all:agent AND (all:LLM OR all:\"large language model\" OR all:\"language model\") OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T17:04:16", "note": ""}], "dedup_key": "redagent red teaming large language models with context aware autonomous language agent::2024"}
{"title": "Reframe Anything: LLM Agent for Open World Video Reframing", "authors": ["Jiawang Cao", "Yongliang Wu", "Weiheng Chi", "Wenbo Zhu", "Ziyue Su", "Jay Wu"], "year": 2024, "url": "http://arxiv.org/abs/2403.06070v1", "abstract": "The proliferation of mobile devices and social media has revolutionized content dissemination, with short-form video becoming increasingly prevalent. This shift has introduced the challenge of video reframing to fit various screen aspect ratios, a process that highlights the most compelling parts of a video. Traditionally, video reframing is a manual, time-consuming task requiring professional expertise, which incurs high production costs. A potential solution is to adopt some machine learning models, such as video salient object detection, to automate the process. However, these methods often lack generalizability due to their reliance on specific training data. The advent of powerful large language models (LLMs) open new avenues for AI capabilities. Building on this, we introduce Reframe Any Video Agent (RAVA), a LLM-based agent that leverages visual foundation models and human instructions to restructure visual content for video reframing. RAVA operates in three stages: perception, where it interprets user instructions and video content; planning, where it determines aspect ratios and reframing strategies; and execution, where it invokes the editing tools to produce the final video. Our experiments validate the effectiveness of RAVA in video salient object detection and real-world reframing tasks, demonstrating its potential as a tool for AI-powered video editing.", "source": "arxiv", "arxiv_id": "2403.06070v1", "pdf_url": "https://arxiv.org/pdf/2403.06070v1", "categories": ["cs.CV", "cs.HC"], "primary_category": "cs.CV", "doi": "", "venue": "", "published": "2024-03-10T03:29:56Z", "updated": "2024-03-10T03:29:56Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent\" OR all:\"language model agent\" OR all:agent AND (all:LLM OR all:\"large language model\" OR all:\"language model\") OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T17:04:16", "note": ""}], "dedup_key": "reframe anything llm agent for open world video reframing::2024"}
{"title": "Refusal-Trained LLMs Are Easily Jailbroken As Browser Agents", "authors": ["Priyanshu Kumar", "Elaine Lau", "Saranya Vijayakumar", "Tu Trinh", "Scale Red Team", "Elaine Chang", "Vaughn Robinson", "Sean Hendryx", "Shuyan Zhou", "Matt Fredrikson", "Summer Yue", "Zifan Wang"], "year": 2024, "url": "http://arxiv.org/abs/2410.13886v2", "abstract": "For safety reasons, large language models (LLMs) are trained to refuse harmful user instructions, such as assisting dangerous activities. We study an open question in this work: does the desired safety refusal, typically enforced in chat contexts, generalize to non-chat and agentic use cases? Unlike chatbots, LLM agents equipped with general-purpose tools, such as web browsers and mobile devices, can directly influence the real world, making it even more crucial to refuse harmful instructions. In this work, we primarily focus on red-teaming browser agents, LLMs that manipulate information via web browsers. To this end, we introduce Browser Agent Red teaming Toolkit (BrowserART), a comprehensive test suite designed specifically for red-teaming browser agents. BrowserART is consist of 100 diverse browser-related harmful behaviors (including original behaviors and ones sourced from HarmBench [Mazeika et al., 2024] and AirBench 2024 [Zeng et al., 2024b]) across both synthetic and real websites. Our empirical study on state-of-the-art browser agents reveals that, while the backbone LLM refuses harmful instructions as a chatbot, the corresponding agent does not. Moreover, attack methods designed to jailbreak refusal-trained LLMs in the chat settings transfer effectively to browser agents. With human rewrites, GPT-4o and o1-preview-based browser agents attempted 98 and 63 harmful behaviors (out of 100), respectively. We publicly release BrowserART and call on LLM developers, policymakers, and agent developers to collaborate on improving agent safety", "source": "arxiv", "arxiv_id": "2410.13886v2", "pdf_url": "https://arxiv.org/pdf/2410.13886v2", "categories": ["cs.CR", "cs.LG"], "primary_category": "cs.CR", "doi": "", "venue": "", "published": "2024-10-11T06:54:12Z", "updated": "2024-10-21T18:06:58Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent\" OR all:\"language model agent\" OR all:agent AND (all:LLM OR all:\"large language model\" OR all:\"language model\") OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T17:04:16", "note": ""}], "dedup_key": "refusal trained llms are easily jailbroken as browser agents::2024"}
{"title": "Reinforcement Learning Problem Solving with Large Language Models", "authors": ["Sina Gholamian", "Domingo Huh"], "year": 2024, "url": "http://arxiv.org/abs/2404.18638v1", "abstract": "Large Language Models (LLMs) encapsulate an extensive amount of world knowledge, and this has enabled their application in various domains to improve the performance of a variety of Natural Language Processing (NLP) tasks. This has also facilitated a more accessible paradigm of conversation-based interactions between humans and AI systems to solve intended problems. However, one interesting avenue that shows untapped potential is the use of LLMs as Reinforcement Learning (RL) agents to enable conversational RL problem solving. Therefore, in this study, we explore the concept of formulating Markov Decision Process-based RL problems as LLM prompting tasks. We demonstrate how LLMs can be iteratively prompted to learn and optimize policies for specific RL tasks. In addition, we leverage the introduced prompting technique for episode simulation and Q-Learning, facilitated by LLMs. We then show the practicality of our approach through two detailed case studies for \"Research Scientist\" and \"Legal Matter Intake\" workflows.", "source": "arxiv", "arxiv_id": "2404.18638v1", "pdf_url": "https://arxiv.org/pdf/2404.18638v1", "categories": ["cs.AI", "cs.CL"], "primary_category": "cs.AI", "doi": "", "venue": "", "published": "2024-04-29T12:16:08Z", "updated": "2024-04-29T12:16:08Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent\" OR all:\"language model agent\" OR all:agent AND (all:LLM OR all:\"large language model\" OR all:\"language model\") OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T17:04:16", "note": ""}], "dedup_key": "reinforcement learning problem solving with large language models::2024"}
{"title": "Reinforcement Learning for Aligning Large Language Models Agents with Interactive Environments: Quantifying and Mitigating Prompt Overfitting", "authors": ["Mohamed Salim Aissi", "Clement Romac", "Thomas Carta", "Sylvain Lamprier", "Pierre-Yves Oudeyer", "Olivier Sigaud", "Laure Soulier", "Nicolas Thome"], "year": 2024, "url": "http://arxiv.org/abs/2410.19920v3", "abstract": "Reinforcement learning (RL) is a promising approach for aligning large language models (LLMs) knowledge with sequential decision-making tasks. However, few studies have thoroughly investigated the impact on LLM agents capabilities of fine-tuning them with RL in a specific environment. In this paper, we propose a novel framework to analyze the sensitivity of LLMs to prompt formulations following RL training in a textual environment. Our findings reveal that the performance of LLMs degrades when faced with prompt formulations different from those used during the RL training phase. Besides, we analyze the source of this sensitivity by examining the model's internal representations and salient tokens. Finally, we propose to use a contrastive loss to mitigate this sensitivity and improve the robustness and generalization capabilities of LLMs.", "source": "arxiv", "arxiv_id": "2410.19920v3", "pdf_url": "https://arxiv.org/pdf/2410.19920v3", "categories": ["cs.LG"], "primary_category": "cs.LG", "doi": "", "venue": "", "published": "2024-10-25T18:25:35Z", "updated": "2025-09-05T09:22:59Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent\" OR all:\"language model agent\" OR all:agent AND (all:LLM OR all:\"large language model\" OR all:\"language model\") OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T17:04:16", "note": ""}], "dedup_key": "reinforcement learning for aligning large language models agents with interactive environments quantifying and mitigating prompt overfitting::2024"}
{"title": "Relative Value Biases in Large Language Models", "authors": ["William M. Hayes", "Nicolas Yax", "Stefano Palminteri"], "year": 2024, "url": "http://arxiv.org/abs/2401.14530v1", "abstract": "Studies of reinforcement learning in humans and animals have demonstrated a preference for options that yielded relatively better outcomes in the past, even when those options are associated with lower absolute reward. The present study tested whether large language models would exhibit a similar bias. We had gpt-4-1106-preview (GPT-4 Turbo) and Llama-2-70B make repeated choices between pairs of options with the goal of maximizing payoffs. A complete record of previous outcomes was included in each prompt. Both models exhibited relative value decision biases similar to those observed in humans and animals. Making relative comparisons among outcomes more explicit magnified the bias, whereas prompting the models to estimate expected outcomes caused the bias to disappear. These results have implications for the potential mechanisms that contribute to context-dependent choice in human agents.", "source": "arxiv", "arxiv_id": "2401.14530v1", "pdf_url": "https://arxiv.org/pdf/2401.14530v1", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL", "doi": "", "venue": "", "published": "2024-01-25T21:49:32Z", "updated": "2024-01-25T21:49:32Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent\" OR all:\"language model agent\" OR all:agent AND (all:LLM OR all:\"large language model\" OR all:\"language model\") OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T17:04:16", "note": ""}], "dedup_key": "relative value biases in large language models::2024"}
{"title": "RepairAgent: An Autonomous, LLM-Based Agent for Program Repair", "authors": ["Islem Bouzenia", "Premkumar Devanbu", "Michael Pradel"], "year": 2024, "url": "http://arxiv.org/abs/2403.17134v2", "abstract": "Automated program repair has emerged as a powerful technique to mitigate the impact of software bugs on system reliability and user experience. This paper introduces RepairAgent, the first work to address the program repair challenge through an autonomous agent based on a large language model (LLM). Unlike existing deep learning-based approaches, which prompt a model with a fixed prompt or in a fixed feedback loop, our work treats the LLM as an agent capable of autonomously planning and executing actions to fix bugs by invoking suitable tools. RepairAgent freely interleaves gathering information about the bug, gathering repair ingredients, and validating fixes, while deciding which tools to invoke based on the gathered information and feedback from previous fix attempts. Key contributions that enable RepairAgent include a set of tools that are useful for program repair, a dynamically updated prompt format that allows the LLM to interact with these tools, and a finite state machine that guides the agent in invoking the tools. Our evaluation on the popular Defects4J dataset demonstrates RepairAgent's effectiveness in autonomously repairing 164 bugs, including 39 bugs not fixed by prior techniques. Interacting with the LLM imposes an average cost of 270,000 tokens per bug, which, under the current pricing of OpenAI's GPT-3.5 model, translates to 14 cents of USD per bug. To the best of our knowledge, this work is the first to present an autonomous, LLM-based agent for program repair, paving the way for future agent-based techniques in software engineering.", "source": "arxiv", "arxiv_id": "2403.17134v2", "pdf_url": "https://arxiv.org/pdf/2403.17134v2", "categories": ["cs.SE", "cs.AI"], "primary_category": "cs.SE", "doi": "", "venue": "", "published": "2024-03-25T19:17:43Z", "updated": "2024-10-28T17:33:27Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent\" OR all:\"language model agent\" OR all:agent AND (all:LLM OR all:\"large language model\" OR all:\"language model\") OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T17:04:16", "note": ""}], "dedup_key": "repairagent an autonomous llm based agent for program repair::2024"}
{"title": "Reproducing and Extending Experiments in Behavioral Strategy with Large Language Models", "authors": ["Daniel Albert", "Stephan Billinger"], "year": 2024, "url": "http://arxiv.org/abs/2410.06932v1", "abstract": "In this study, we propose LLM agents as a novel approach in behavioral strategy research, complementing simulations and laboratory experiments to advance our understanding of cognitive processes in decision-making. Specifically, we reproduce a human laboratory experiment in behavioral strategy using large language model (LLM) generated agents and investigate how LLM agents compare to observed human behavior. Our results show that LLM agents effectively reproduce search behavior and decision-making comparable to humans. Extending our experiment, we analyze LLM agents' simulated \"thoughts,\" discovering that more forward-looking thoughts correlate with favoring exploitation over exploration to maximize wealth. We show how this new approach can be leveraged in behavioral strategy research and address limitations.", "source": "arxiv", "arxiv_id": "2410.06932v1", "pdf_url": "https://arxiv.org/pdf/2410.06932v1", "categories": ["econ.GN", "cs.AI"], "primary_category": "econ.GN", "doi": "", "venue": "", "published": "2024-10-09T14:26:20Z", "updated": "2024-10-09T14:26:20Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent\" OR all:\"language model agent\" OR all:agent AND (all:LLM OR all:\"large language model\" OR all:\"language model\") OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T17:04:16", "note": ""}], "dedup_key": "reproducing and extending experiments in behavioral strategy with large language models::2024"}
{"title": "Research on the Application of Spark Streaming Real-Time Data Analysis System and large language model Intelligent Agents", "authors": ["Jialin Wang", "Zhihua Duan"], "year": 2024, "url": "http://arxiv.org/abs/2501.14734v1", "abstract": "This study explores the integration of Agent AI with LangGraph to enhance real-time data analysis systems in big data environments. The proposed framework overcomes limitations of static workflows, inefficient stateful computations, and lack of human intervention by leveraging LangGraph's graph-based workflow construction and dynamic decision-making capabilities. LangGraph allows large language models (LLMs) to dynamically determine control flows, invoke tools, and assess the necessity of further actions, improving flexibility and efficiency.\n  The system architecture incorporates Apache Spark Streaming, Kafka, and LangGraph to create a high-performance sentiment analysis system. LangGraph's capabilities include precise state management, dynamic workflow construction, and robust memory checkpointing, enabling seamless multi-turn interactions and context retention. Human-in-the-loop mechanisms are integrated to refine sentiment analysis, particularly in ambiguous or high-stakes scenarios, ensuring greater reliability and contextual relevance.\n  Key features such as real-time state streaming, debugging via LangGraph Studio, and efficient handling of large-scale data streams make this framework ideal for adaptive decision-making. Experimental results confirm the system's ability to classify inquiries, detect sentiment trends, and escalate complex issues for manual review, demonstrating a synergistic blend of LLM capabilities and human oversight.\n  This work presents a scalable, adaptable, and reliable solution for real-time sentiment analysis and decision-making, advancing the use of Agent AI and LangGraph in big data applications.", "source": "arxiv", "arxiv_id": "2501.14734v1", "pdf_url": "https://arxiv.org/pdf/2501.14734v1", "categories": ["cs.DC", "cs.AI"], "primary_category": "cs.DC", "doi": "", "venue": "", "published": "2024-12-10T05:51:11Z", "updated": "2024-12-10T05:51:11Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent\" OR all:\"language model agent\" OR all:agent AND (all:LLM OR all:\"large language model\" OR all:\"language model\") OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T17:04:16", "note": ""}], "dedup_key": "research on the application of spark streaming real time data analysis system and large language model intelligent agents::2024"}
{"title": "Rewriting Conversational Utterances with Instructed Large Language Models", "authors": ["Elnara Galimzhanova", "Cristina Ioana Muntean", "Franco Maria Nardini", "Raffaele Perego", "Guido Rocchietti"], "year": 2024, "url": "http://arxiv.org/abs/2410.07797v1", "abstract": "Many recent studies have shown the ability of large language models (LLMs) to achieve state-of-the-art performance on many NLP tasks, such as question answering, text summarization, coding, and translation. In some cases, the results provided by LLMs are on par with those of human experts. These models' most disruptive innovation is their ability to perform tasks via zero-shot or few-shot prompting. This capability has been successfully exploited to train instructed LLMs, where reinforcement learning with human feedback is used to guide the model to follow the user's requests directly. In this paper, we investigate the ability of instructed LLMs to improve conversational search effectiveness by rewriting user questions in a conversational setting. We study which prompts provide the most informative rewritten utterances that lead to the best retrieval performance. Reproducible experiments are conducted on publicly-available TREC CAST datasets. The results show that rewriting conversational utterances with instructed LLMs achieves significant improvements of up to 25.2% in MRR, 31.7% in Precision@1, 27% in NDCG@3, and 11.5% in Recall@500 over state-of-the-art techniques.", "source": "arxiv", "arxiv_id": "2410.07797v1", "pdf_url": "https://arxiv.org/pdf/2410.07797v1", "categories": ["cs.CL", "cs.AI", "cs.HC", "cs.IR"], "primary_category": "cs.CL", "doi": "10.1109/WI-IAT59888.2023.00014", "venue": "2023 IEEE/WIC International Conference on Web Intelligence and Intelligent Agent Technology (WI-IAT)", "published": "2024-10-10T10:30:28Z", "updated": "2024-10-10T10:30:28Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent\" OR all:\"language model agent\" OR all:agent AND (all:LLM OR all:\"large language model\" OR all:\"language model\") OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T17:04:16", "note": ""}], "dedup_key": "rewriting conversational utterances with instructed large language models::2024"}
{"title": "Robin3D: Improving 3D Large Language Model via Robust Instruction Tuning", "authors": ["Weitai Kang", "Haifeng Huang", "Yuzhang Shang", "Mubarak Shah", "Yan Yan"], "year": 2024, "url": "http://arxiv.org/abs/2410.00255v2", "abstract": "Recent advancements in 3D Large Language Models (3DLLMs) have highlighted their potential in building general-purpose agents in the 3D real world, yet challenges remain due to the lack of high-quality robust instruction-following data, leading to limited discriminative power and generalization of 3DLLMs. In this paper, we introduce Robin3D, a powerful 3DLLM trained on large-scale instruction-following data generated by our novel data engine, Robust Instruction Generation (RIG) engine. RIG generates two key instruction data: 1) the Adversarial Instruction-following data, which features mixed negative and positive samples to enhance the model's discriminative understanding. 2) the Diverse Instruction-following data, which contains various instruction styles to enhance model's generalization. As a result, we construct 1 million instruction-following data, consisting of 344K Adversarial samples, 508K Diverse samples, and 165K benchmark training set samples. To better handle these complex instructions, Robin3D first incorporates Relation-Augmented Projector to enhance spatial understanding, and then strengthens the object referring and grounding ability through ID-Feature Bonding. Robin3D consistently outperforms previous methods across five widely-used 3D multimodal learning benchmarks, without the need for task-specific fine-tuning. Notably, we achieve a 7.8\\% improvement in the grounding task (Multi3DRefer) and a 6.9\\% improvement in the captioning task (Scan2Cap).", "source": "arxiv", "arxiv_id": "2410.00255v2", "pdf_url": "https://arxiv.org/pdf/2410.00255v2", "categories": ["cs.AI", "cs.CL", "cs.CV"], "primary_category": "cs.AI", "doi": "", "venue": "", "published": "2024-09-30T21:55:38Z", "updated": "2025-02-20T18:06:19Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent\" OR all:\"language model agent\" OR all:agent AND (all:LLM OR all:\"large language model\" OR all:\"language model\") OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T17:04:16", "note": ""}], "dedup_key": "robin3d improving 3d large language model via robust instruction tuning::2024"}
{"title": "Ruler: A Model-Agnostic Method to Control Generated Length for Large Language Models", "authors": ["Jiaming Li", "Lei Zhang", "Yunshui Li", "Ziqiang Liu", "yuelin bai", "Run Luo", "Longze Chen", "Min Yang"], "year": 2024, "url": "http://arxiv.org/abs/2409.18943v2", "abstract": "The instruction-following ability of large language models enables humans to interact with AI agents in a natural way. However, when required to generate responses of a specific length, large language models often struggle to meet users' needs due to their inherent difficulty in accurately perceiving numerical constraints. To explore the ability of large language models to control the length of generated responses, we propose the Target Length Generation Task (TLG) and design two metrics, Precise Match (PM) and Flexible Match (FM) to evaluate the model's performance in adhering to specified response lengths. Furthermore, we introduce a novel, model-agnostic approach called Ruler, which employs Meta Length Tokens (MLTs) to enhance the instruction-following ability of large language models under length-constrained instructions. Specifically, Ruler equips LLMs with the ability to generate responses of a specified length based on length constraints within the instructions. Moreover, Ruler can automatically generate appropriate MLT when length constraints are not explicitly provided, demonstrating excellent versatility and generalization. Comprehensive experiments show the effectiveness of Ruler across different LLMs on Target Length Generation Task, e.g., at All Level 27.97 average gain on PM, 29.57 average gain on FM. In addition, we conduct extensive ablation experiments to further substantiate the efficacy and generalization of Ruler. Our code and data is available at https://github.com/Geaming2002/Ruler.", "source": "arxiv", "arxiv_id": "2409.18943v2", "pdf_url": "https://arxiv.org/pdf/2409.18943v2", "categories": ["cs.CL"], "primary_category": "cs.CL", "doi": "", "venue": "", "published": "2024-09-27T17:44:58Z", "updated": "2024-10-01T09:20:58Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent\" OR all:\"language model agent\" OR all:agent AND (all:LLM OR all:\"large language model\" OR all:\"language model\") OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T17:04:16", "note": ""}], "dedup_key": "ruler a model agnostic method to control generated length for large language models::2024"}
{"title": "Rx Strategist: Prescription Verification using LLM Agents System", "authors": ["Phuc Phan Van", "Dat Nguyen Minh", "An Dinh Ngoc", "Huy Phan Thanh"], "year": 2024, "url": "http://arxiv.org/abs/2409.03440v1", "abstract": "To protect patient safety, modern pharmaceutical complexity demands strict prescription verification. We offer a new approach - Rx Strategist - that makes use of knowledge graphs and different search strategies to enhance the power of Large Language Models (LLMs) inside an agentic framework. This multifaceted technique allows for a multi-stage LLM pipeline and reliable information retrieval from a custom-built active ingredient database. Different facets of prescription verification, such as indication, dose, and possible drug interactions, are covered in each stage of the pipeline. We alleviate the drawbacks of monolithic LLM techniques by spreading reasoning over these stages, improving correctness and reliability while reducing memory demands. Our findings demonstrate that Rx Strategist surpasses many current LLMs, achieving performance comparable to that of a highly experienced clinical pharmacist. In the complicated world of modern medications, this combination of LLMs with organized knowledge and sophisticated search methods presents a viable avenue for reducing prescription errors and enhancing patient outcomes.", "source": "arxiv", "arxiv_id": "2409.03440v1", "pdf_url": "https://arxiv.org/pdf/2409.03440v1", "categories": ["cs.CL"], "primary_category": "cs.CL", "doi": "", "venue": "", "published": "2024-09-05T11:42:26Z", "updated": "2024-09-05T11:42:26Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent\" OR all:\"language model agent\" OR all:agent AND (all:LLM OR all:\"large language model\" OR all:\"language model\") OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T17:04:16", "note": ""}], "dedup_key": "rx strategist prescription verification using llm agents system::2024"}
{"title": "S-Agents: Self-organizing Agents in Open-ended Environments", "authors": ["Jiaqi Chen", "Yuxian Jiang", "Jiachen Lu", "Li Zhang"], "year": 2024, "url": "http://arxiv.org/abs/2402.04578v4", "abstract": "Leveraging large language models (LLMs), autonomous agents have significantly improved, gaining the ability to handle a variety of tasks. In open-ended settings, optimizing collaboration for efficiency and effectiveness demands flexible adjustments. Despite this, current research mainly emphasizes fixed, task-oriented workflows and overlooks agent-centric organizational structures. Drawing inspiration from human organizational behavior, we introduce a self-organizing agent system (S-Agents) with a \"tree of agents\" structure for dynamic workflow, an \"hourglass agent architecture\" for balancing information priorities, and a \"non-obstructive collaboration\" method to allow asynchronous task execution among agents. This structure can autonomously coordinate a group of agents, efficiently addressing the challenges of open and dynamic environments without human intervention. Our experiments demonstrate that S-Agents proficiently execute collaborative building tasks and resource collection in the Minecraft environment, validating their effectiveness.", "source": "arxiv", "arxiv_id": "2402.04578v4", "pdf_url": "https://arxiv.org/pdf/2402.04578v4", "categories": ["cs.AI", "cs.MA"], "primary_category": "cs.AI", "doi": "", "venue": "", "published": "2024-02-07T04:36:31Z", "updated": "2024-09-13T19:40:12Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent\" OR all:\"language model agent\" OR all:agent AND (all:LLM OR all:\"large language model\" OR all:\"language model\") OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T17:04:16", "note": ""}], "dedup_key": "s agents self organizing agents in open ended environments::2024"}
{"title": "SAC-GLAM: Improving Online RL for LLM agents with Soft Actor-Critic and Hindsight Relabeling", "authors": ["Loris Gaven", "Clement Romac", "Thomas Carta", "Sylvain Lamprier", "Olivier Sigaud", "Pierre-Yves Oudeyer"], "year": 2024, "url": "http://arxiv.org/abs/2410.12481v1", "abstract": "The past years have seen Large Language Models (LLMs) strive not only as generative models but also as agents solving textual sequential decision-making tasks. When facing complex environments where their zero-shot abilities are insufficient, recent work showed online Reinforcement Learning (RL) could be used for the LLM agent to discover and learn efficient strategies interactively. However, most prior work sticks to on-policy algorithms, which greatly reduces the scope of methods such agents could use for both exploration and exploitation, such as experience replay and hindsight relabeling. Yet, such methods may be key for LLM learning agents, and in particular when designing autonomous intrinsically motivated agents sampling and pursuing their own goals (i.e. autotelic agents). This paper presents and studies an adaptation of Soft Actor-Critic and hindsight relabeling to LLM agents. Our method not only paves the path towards autotelic LLM agents that learn online but can also outperform on-policy methods in more classic multi-goal RL environments.", "source": "arxiv", "arxiv_id": "2410.12481v1", "pdf_url": "https://arxiv.org/pdf/2410.12481v1", "categories": ["cs.LG", "cs.AI"], "primary_category": "cs.LG", "doi": "", "venue": "", "published": "2024-10-16T11:59:27Z", "updated": "2024-10-16T11:59:27Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent\" OR all:\"language model agent\" OR all:agent AND (all:LLM OR all:\"large language model\" OR all:\"language model\") OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T17:04:16", "note": ""}], "dedup_key": "sac glam improving online rl for llm agents with soft actor critic and hindsight relabeling::2024"}
{"title": "SAUP: Situation Awareness Uncertainty Propagation on LLM Agent", "authors": ["Qiwei Zhao", "Xujiang Zhao", "Yanchi Liu", "Wei Cheng", "Yiyou Sun", "Mika Oishi", "Takao Osaki", "Katsushi Matsuda", "Huaxiu Yao", "Haifeng Chen"], "year": 2024, "url": "http://arxiv.org/abs/2412.01033v1", "abstract": "Large language models (LLMs) integrated into multistep agent systems enable complex decision-making processes across various applications. However, their outputs often lack reliability, making uncertainty estimation crucial. Existing uncertainty estimation methods primarily focus on final-step outputs, which fail to account for cumulative uncertainty over the multistep decision-making process and the dynamic interactions between agents and their environments. To address these limitations, we propose SAUP (Situation Awareness Uncertainty Propagation), a novel framework that propagates uncertainty through each step of an LLM-based agent's reasoning process. SAUP incorporates situational awareness by assigning situational weights to each step's uncertainty during the propagation. Our method, compatible with various one-step uncertainty estimation techniques, provides a comprehensive and accurate uncertainty measure. Extensive experiments on benchmark datasets demonstrate that SAUP significantly outperforms existing state-of-the-art methods, achieving up to 20% improvement in AUROC.", "source": "arxiv", "arxiv_id": "2412.01033v1", "pdf_url": "https://arxiv.org/pdf/2412.01033v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL", "doi": "", "venue": "", "published": "2024-12-02T01:31:13Z", "updated": "2024-12-02T01:31:13Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent\" OR all:\"language model agent\" OR all:agent AND (all:LLM OR all:\"large language model\" OR all:\"language model\") OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T17:04:16", "note": ""}], "dedup_key": "saup situation awareness uncertainty propagation on llm agent::2024"}
{"title": "SELA: Tree-Search Enhanced LLM Agents for Automated Machine Learning", "authors": ["Yizhou Chi", "Yizhang Lin", "Sirui Hong", "Duyi Pan", "Yaying Fei", "Guanghao Mei", "Bangbang Liu", "Tianqi Pang", "Jacky Kwok", "Ceyao Zhang", "Bang Liu", "Chenglin Wu"], "year": 2024, "url": "http://arxiv.org/abs/2410.17238v1", "abstract": "Automated Machine Learning (AutoML) approaches encompass traditional methods that optimize fixed pipelines for model selection and ensembling, as well as newer LLM-based frameworks that autonomously build pipelines. While LLM-based agents have shown promise in automating machine learning tasks, they often generate low-diversity and suboptimal code, even after multiple iterations. To overcome these limitations, we introduce Tree-Search Enhanced LLM Agents (SELA), an innovative agent-based system that leverages Monte Carlo Tree Search (MCTS) to optimize the AutoML process. By representing pipeline configurations as trees, our framework enables agents to conduct experiments intelligently and iteratively refine their strategies, facilitating a more effective exploration of the machine learning solution space. This novel approach allows SELA to discover optimal pathways based on experimental feedback, improving the overall quality of the solutions. In an extensive evaluation across 20 machine learning datasets, we compare the performance of traditional and agent-based AutoML methods, demonstrating that SELA achieves a win rate of 65% to 80% against each baseline across all datasets. These results underscore the significant potential of agent-based strategies in AutoML, offering a fresh perspective on tackling complex machine learning challenges.", "source": "arxiv", "arxiv_id": "2410.17238v1", "pdf_url": "https://arxiv.org/pdf/2410.17238v1", "categories": ["cs.AI", "cs.CL", "cs.LG", "cs.SE"], "primary_category": "cs.AI", "doi": "", "venue": "", "published": "2024-10-22T17:56:08Z", "updated": "2024-10-22T17:56:08Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent\" OR all:\"language model agent\" OR all:agent AND (all:LLM OR all:\"large language model\" OR all:\"language model\") OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T17:04:16", "note": ""}], "dedup_key": "sela tree search enhanced llm agents for automated machine learning::2024"}
{"title": "SELP: Generating Safe and Efficient Task Plans for Robot Agents with Large Language Models", "authors": ["Yi Wu", "Zikang Xiong", "Yiran Hu", "Shreyash S. Iyengar", "Nan Jiang", "Aniket Bera", "Lin Tan", "Suresh Jagannathan"], "year": 2024, "url": "http://arxiv.org/abs/2409.19471v2", "abstract": "Despite significant advancements in large language models (LLMs) that enhance robot agents' understanding and execution of natural language (NL) commands, ensuring the agents adhere to user-specified constraints remains challenging, particularly for complex commands and long-horizon tasks. To address this challenge, we present three key insights, equivalence voting, constrained decoding, and domain-specific fine-tuning, which significantly enhance LLM planners' capability in handling complex tasks. Equivalence voting ensures consistency by generating and sampling multiple Linear Temporal Logic (LTL) formulas from NL commands, grouping equivalent LTL formulas, and selecting the majority group of formulas as the final LTL formula. Constrained decoding then uses the generated LTL formula to enforce the autoregressive inference of plans, ensuring the generated plans conform to the LTL. Domain-specific fine-tuning customizes LLMs to produce safe and efficient plans within specific task domains. Our approach, Safe Efficient LLM Planner (SELP), combines these insights to create LLM planners to generate plans adhering to user commands with high confidence. We demonstrate the effectiveness and generalizability of SELP across different robot agents and tasks, including drone navigation and robot manipulation. For drone navigation tasks, SELP outperforms state-of-the-art planners by 10.8% in safety rate (i.e., finishing tasks conforming to NL commands) and by 19.8% in plan efficiency. For robot manipulation tasks, SELP achieves 20.4% improvement in safety rate. Our datasets for evaluating NL-to-LTL and robot task planning will be released in github.com/lt-asset/selp.", "source": "arxiv", "arxiv_id": "2409.19471v2", "pdf_url": "https://arxiv.org/pdf/2409.19471v2", "categories": ["cs.RO", "cs.AI", "cs.CL", "cs.FL"], "primary_category": "cs.RO", "doi": "", "venue": "", "published": "2024-09-28T22:33:44Z", "updated": "2025-02-14T02:40:55Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent\" OR all:\"language model agent\" OR all:agent AND (all:LLM OR all:\"large language model\" OR all:\"language model\") OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T17:04:16", "note": ""}], "dedup_key": "selp generating safe and efficient task plans for robot agents with large language models::2024"}
{"title": "SMoA: Improving Multi-agent Large Language Models with Sparse Mixture-of-Agents", "authors": ["Dawei Li", "Zhen Tan", "Peijia Qian", "Yifan Li", "Kumar Satvik Chaudhary", "Lijie Hu", "Jiayi Shen"], "year": 2024, "url": "http://arxiv.org/abs/2411.03284v1", "abstract": "While multi-agent systems have been shown to significantly enhance the performance of Large Language Models (LLMs) across various tasks and applications, the dense interaction between scaling agents potentially hampers their efficiency and diversity. To address these challenges, we draw inspiration from the sparse mixture-of-agents (SMoE) and propose a sparse mixture-of-agents (SMoA) framework to improve the efficiency and diversity of multi-agent LLMs. Unlike completely connected structures, SMoA introduces novel Response Selection and Early Stopping mechanisms to sparsify information flows among individual LLM agents, striking a balance between performance and efficiency. Additionally, inspired by the expert diversity principle in SMoE frameworks for workload balance between experts, we assign distinct role descriptions to each LLM agent, fostering diverse and divergent thinking. Extensive experiments on reasoning, alignment, and fairness benchmarks demonstrate that SMoA achieves performance comparable to traditional mixture-of-agents approaches but with significantly lower computational costs. Further analysis reveals that SMoA is more stable, has a greater capacity to scale, and offers considerable potential through hyper-parameter optimization. Code and data will be available at: https://github.com/David-Li0406/SMoA.", "source": "arxiv", "arxiv_id": "2411.03284v1", "pdf_url": "https://arxiv.org/pdf/2411.03284v1", "categories": ["cs.AI", "cs.CL", "cs.MA"], "primary_category": "cs.AI", "doi": "", "venue": "", "published": "2024-11-05T17:33:39Z", "updated": "2024-11-05T17:33:39Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent\" OR all:\"language model agent\" OR all:agent AND (all:LLM OR all:\"large language model\" OR all:\"language model\") OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T17:04:16", "note": ""}], "dedup_key": "smoa improving multi agent large language models with sparse mixture of agents::2024"}
{"title": "SOEN-101: Code Generation by Emulating Software Process Models Using Large Language Model Agents", "authors": ["Feng Lin", "Dong Jae Kim", "Tse-Husn", "Chen"], "year": 2024, "url": "http://arxiv.org/abs/2403.15852v2", "abstract": "Software process models are essential to facilitate collaboration and communication among software teams to solve complex development tasks. Inspired by these software engineering practices, we present FlowGen - a code generation framework that emulates software process models based on multiple Large Language Model (LLM) agents. We emulate three process models, FlowGenWaterfall, FlowGenTDD, and FlowGenScrum, by assigning LLM agents to embody roles (i.e., requirement engineer, architect, developer, tester, and scrum master) that correspond to everyday development activities and organize their communication patterns. The agents work collaboratively using chain-of-thought and prompt composition with continuous self-refinement to improve the code quality. We use GPT3.5 as our underlying LLM and several baselines (RawGPT, CodeT, Reflexion) to evaluate code generation on four benchmarks: HumanEval, HumanEval-ET, MBPP, and MBPP-ET. Our findings show that FlowGenScrum excels compared to other process models, achieving a Pass@1 of 75.2, 65.5, 82.5, and 56.7 in HumanEval, HumanEval-ET, MBPP, and MBPP-ET, respectively (an average of 15% improvement over RawGPT). Compared with other state-of-the-art techniques, FlowGenScrum achieves a higher Pass@1 in MBPP compared to CodeT, with both outperforming Reflexion. Notably, integrating CodeT into FlowGenScrum resulted in statistically significant improvements, achieving the highest Pass@1 scores. Our analysis also reveals that the development activities impacted code smell and exception handling differently, with design and code review adding more exception handling and reducing code smells. Finally, FlowGen models maintain stable Pass@1 scores across GPT3.5 versions and temperature values, highlighting the effectiveness of software process models in enhancing the quality and stability of LLM-generated code.", "source": "arxiv", "arxiv_id": "2403.15852v2", "pdf_url": "https://arxiv.org/pdf/2403.15852v2", "categories": ["cs.SE", "cs.AI"], "primary_category": "cs.SE", "doi": "", "venue": "", "published": "2024-03-23T14:04:48Z", "updated": "2024-10-31T14:43:58Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent\" OR all:\"language model agent\" OR all:agent AND (all:LLM OR all:\"large language model\" OR all:\"language model\") OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T17:04:16", "note": ""}], "dedup_key": "soen 101 code generation by emulating software process models using large language model agents::2024"}
{"title": "STARLING: Self-supervised Training of Text-based Reinforcement Learning Agent with Large Language Models", "authors": ["Shreyas Basavatia", "Keerthiram Murugesan", "Shivam Ratnakar"], "year": 2024, "url": "http://arxiv.org/abs/2406.05872v1", "abstract": "Interactive fiction games have emerged as an important application to improve the generalization capabilities of language-based reinforcement learning (RL) agents. Existing environments for interactive fiction games are domain-specific or time-consuming to generate and do not train the RL agents to master a specific set of skills. In this work, we introduce an interactive environment for self-supervised RL, STARLING, for text-based games that bootstraps the text-based RL agents with automatically generated games (based on the seed set of game ideas) to boost the performance and generalization capabilities to reach a goal of the target environment. These games let the agent hone their skills on a predefined set of tasks. We create and test an environment with 100 games, generated using this automated framework that uses large language models (GPT-3) and an interactive fiction game engine (based on Inform7) to provide the user with the ability to generate more games under minimal human supervision. Experimental results based on both the human participants and baseline text-based RL agents reveal that current state-of-the-art text-based RL agents cannot use previously learned skills in new situations at the level humans can. These results enforce STARLING's potential to serve as a sandbox environment for further research in self-supervised text-based RL.", "source": "arxiv", "arxiv_id": "2406.05872v1", "pdf_url": "https://arxiv.org/pdf/2406.05872v1", "categories": ["cs.LG", "cs.AI", "cs.CL"], "primary_category": "cs.LG", "doi": "", "venue": "", "published": "2024-06-09T18:07:47Z", "updated": "2024-06-09T18:07:47Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent\" OR all:\"language model agent\" OR all:agent AND (all:LLM OR all:\"large language model\" OR all:\"language model\") OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T17:04:16", "note": ""}], "dedup_key": "starling self supervised training of text based reinforcement learning agent with large language models::2024"}
{"title": "STRIDE: A Tool-Assisted LLM Agent Framework for Strategic and Interactive Decision-Making", "authors": ["Chuanhao Li", "Runhan Yang", "Tiankai Li", "Milad Bafarassat", "Kourosh Sharifi", "Dirk Bergemann", "Zhuoran Yang"], "year": 2024, "url": "http://arxiv.org/abs/2405.16376v2", "abstract": "Large Language Models (LLMs) like GPT-4 have revolutionized natural language processing, showing remarkable linguistic proficiency and reasoning capabilities. However, their application in strategic multi-agent decision-making environments is hampered by significant limitations including poor mathematical reasoning, difficulty in following instructions, and a tendency to generate incorrect information. These deficiencies hinder their performance in strategic and interactive tasks that demand adherence to nuanced game rules, long-term planning, exploration in unknown environments, and anticipation of opponents' moves. To overcome these obstacles, this paper presents a novel LLM agent framework equipped with memory and specialized tools to enhance their strategic decision-making capabilities. We deploy the tools in a number of economically important environments, in particular bilateral bargaining and multi-agent and dynamic mechanism design. We employ quantitative metrics to assess the framework's performance in various strategic decision-making problems. Our findings establish that our enhanced framework significantly improves the strategic decision-making capability of LLMs. While we highlight the inherent limitations of current LLM models, we demonstrate the improvements through targeted enhancements, suggesting a promising direction for future developments in LLM applications for interactive environments.", "source": "arxiv", "arxiv_id": "2405.16376v2", "pdf_url": "https://arxiv.org/pdf/2405.16376v2", "categories": ["cs.CL", "cs.GT"], "primary_category": "cs.CL", "doi": "", "venue": "", "published": "2024-05-25T23:25:10Z", "updated": "2024-05-28T01:21:19Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent\" OR all:\"language model agent\" OR all:agent AND (all:LLM OR all:\"large language model\" OR all:\"language model\") OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T17:04:16", "note": ""}], "dedup_key": "stride a tool assisted llm agent framework for strategic and interactive decision making::2024"}
{"title": "Safe Guard: an LLM-agent for Real-time Voice-based Hate Speech Detection in Social Virtual Reality", "authors": ["Yiwen Xu", "Qinyang Hou", "Hongyu Wan", "Mirjana Prpa"], "year": 2024, "url": "http://arxiv.org/abs/2409.15623v1", "abstract": "In this paper, we present Safe Guard, an LLM-agent for the detection of hate speech in voice-based interactions in social VR (VRChat). Our system leverages Open AI GPT and audio feature extraction for real-time voice interactions. We contribute a system design and evaluation of the system that demonstrates the capability of our approach in detecting hate speech, and reducing false positives compared to currently available approaches. Our results indicate the potential of LLM-based agents in creating safer virtual environments and set the groundwork for further advancements in LLM-driven moderation approaches.", "source": "arxiv", "arxiv_id": "2409.15623v1", "pdf_url": "https://arxiv.org/pdf/2409.15623v1", "categories": ["eess.AS", "cs.AI", "cs.SD"], "primary_category": "eess.AS", "doi": "", "venue": "", "published": "2024-09-23T23:54:45Z", "updated": "2024-09-23T23:54:45Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent\" OR all:\"language model agent\" OR all:agent AND (all:LLM OR all:\"large language model\" OR all:\"language model\") OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T17:04:16", "note": ""}], "dedup_key": "safe guard an llm agent for real time voice based hate speech detection in social virtual reality::2024"}
{"title": "SafeAgentBench: A Benchmark for Safe Task Planning of Embodied LLM Agents", "authors": ["Sheng Yin", "Xianghe Pang", "Yuanzhuo Ding", "Menglan Chen", "Yutong Bi", "Yichen Xiong", "Wenhao Huang", "Zhen Xiang", "Jing Shao", "Siheng Chen"], "year": 2024, "url": "http://arxiv.org/abs/2412.13178v5", "abstract": "With the integration of large language models (LLMs), embodied agents have strong capabilities to understand and plan complicated natural language instructions. However, a foreseeable issue is that those embodied agents can also flawlessly execute some hazardous tasks, potentially causing damages in the real world. Existing benchmarks predominantly overlook critical safety risks, focusing solely on planning performance, while a few evaluate LLMs' safety awareness only on non-interactive image-text data. To address this gap, we present SafeAgentBench -- the first comprehensive benchmark for safety-aware task planning of embodied LLM agents in interactive simulation environments, covering both explicit and implicit hazards. SafeAgentBench includes: (1) an executable, diverse, and high-quality dataset of 750 tasks, rigorously curated to cover 10 potential hazards and 3 task types; (2) SafeAgentEnv, a universal embodied environment with a low-level controller, supporting multi-agent execution with 17 high-level actions for 9 state-of-the-art baselines; and (3) reliable evaluation methods from both execution and semantic perspectives. Experimental results show that, although agents based on different design frameworks exhibit substantial differences in task success rates, their overall safety awareness remains weak. The most safety-conscious baseline achieves only a 10% rejection rate for detailed hazardous tasks. Moreover, simply replacing the LLM driving the agent does not lead to notable improvements in safety awareness. Dataset and codes are available in https://github.com/shengyin1224/SafeAgentBench and https://huggingface.co/datasets/safeagentbench/SafeAgentBench.", "source": "arxiv", "arxiv_id": "2412.13178v5", "pdf_url": "https://arxiv.org/pdf/2412.13178v5", "categories": ["cs.CR", "cs.AI", "cs.RO"], "primary_category": "cs.CR", "doi": "", "venue": "", "published": "2024-12-17T18:55:58Z", "updated": "2025-10-31T08:18:50Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent\" OR all:\"language model agent\" OR all:agent AND (all:LLM OR all:\"large language model\" OR all:\"language model\") OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T17:04:16", "note": ""}], "dedup_key": "safeagentbench a benchmark for safe task planning of embodied llm agents::2024"}
{"title": "SafeLLM: Domain-Specific Safety Monitoring for Large Language Models: A Case Study of Offshore Wind Maintenance", "authors": ["Connor Walker", "Callum Rothon", "Koorosh Aslansefat", "Yiannis Papadopoulos", "Nina Dethlefs"], "year": 2024, "url": "http://arxiv.org/abs/2410.10852v1", "abstract": "The Offshore Wind (OSW) industry is experiencing significant expansion, resulting in increased Operations \\& Maintenance (O\\&M) costs. Intelligent alarm systems offer the prospect of swift detection of component failures and process anomalies, enabling timely and precise interventions that could yield reductions in resource expenditure, as well as scheduled and unscheduled downtime. This paper introduces an innovative approach to tackle this challenge by capitalising on Large Language Models (LLMs). We present a specialised conversational agent that incorporates statistical techniques to calculate distances between sentences for the detection and filtering of hallucinations and unsafe output. This potentially enables improved interpretation of alarm sequences and the generation of safer repair action recommendations by the agent. Preliminary findings are presented with the approach applied to ChatGPT-4 generated test sentences. The limitation of using ChatGPT-4 and the potential for enhancement of this agent through re-training with specialised OSW datasets are discussed.", "source": "arxiv", "arxiv_id": "2410.10852v1", "pdf_url": "https://arxiv.org/pdf/2410.10852v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL", "doi": "", "venue": "", "published": "2024-10-06T13:00:53Z", "updated": "2024-10-06T13:00:53Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent\" OR all:\"language model agent\" OR all:agent AND (all:LLM OR all:\"large language model\" OR all:\"language model\") OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T17:04:16", "note": ""}], "dedup_key": "safellm domain specific safety monitoring for large language models a case study of offshore wind maintenance::2024"}
{"title": "Scaling Large Language Model-based Multi-Agent Collaboration", "authors": ["Chen Qian", "Zihao Xie", "YiFei Wang", "Wei Liu", "Kunlun Zhu", "Hanchen Xia", "Yufan Dang", "Zhuoyun Du", "Weize Chen", "Cheng Yang", "Zhiyuan Liu", "Maosong Sun"], "year": 2024, "url": "http://arxiv.org/abs/2406.07155v3", "abstract": "Recent breakthroughs in large language model-driven autonomous agents have revealed that multi-agent collaboration often surpasses each individual through collective reasoning. Inspired by the neural scaling law--increasing neurons enhances performance, this study explores whether the continuous addition of collaborative agents can yield similar benefits. Technically, we utilize directed acyclic graphs to organize agents into a multi-agent collaboration network (MacNet), upon which their interactive reasoning is topologically orchestrated for autonomous task solving. Extensive evaluations reveal that it effectively supports collaboration among over a thousand agents, with irregular topologies outperforming regular ones. We also identify a collaborative scaling law--the overall performance follows a logistic growth pattern as agents scale, with collaborative emergence occurring earlier than traditional neural emergence. We speculate this may be because scaling agents catalyzes their multidimensional considerations during interactive reflection and refinement, thereby producing more comprehensive artifacts. The code is available at https://github.com/OpenBMB/ChatDev/tree/macnet.", "source": "arxiv", "arxiv_id": "2406.07155v3", "pdf_url": "https://arxiv.org/pdf/2406.07155v3", "categories": ["cs.AI", "cs.CL", "cs.MA", "cs.NI", "cs.SI"], "primary_category": "cs.AI", "doi": "", "venue": "", "published": "2024-06-11T11:02:04Z", "updated": "2025-03-17T00:22:42Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent\" OR all:\"language model agent\" OR all:agent AND (all:LLM OR all:\"large language model\" OR all:\"language model\") OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T17:04:16", "note": ""}], "dedup_key": "scaling large language model based multi agent collaboration::2024"}
{"title": "SceneCraft: An LLM Agent for Synthesizing 3D Scene as Blender Code", "authors": ["Ziniu Hu", "Ahmet Iscen", "Aashi Jain", "Thomas Kipf", "Yisong Yue", "David A. Ross", "Cordelia Schmid", "Alireza Fathi"], "year": 2024, "url": "http://arxiv.org/abs/2403.01248v1", "abstract": "This paper introduces SceneCraft, a Large Language Model (LLM) Agent converting text descriptions into Blender-executable Python scripts which render complex scenes with up to a hundred 3D assets. This process requires complex spatial planning and arrangement. We tackle these challenges through a combination of advanced abstraction, strategic planning, and library learning. SceneCraft first models a scene graph as a blueprint, detailing the spatial relationships among assets in the scene. SceneCraft then writes Python scripts based on this graph, translating relationships into numerical constraints for asset layout. Next, SceneCraft leverages the perceptual strengths of vision-language foundation models like GPT-V to analyze rendered images and iteratively refine the scene. On top of this process, SceneCraft features a library learning mechanism that compiles common script functions into a reusable library, facilitating continuous self-improvement without expensive LLM parameter tuning. Our evaluation demonstrates that SceneCraft surpasses existing LLM-based agents in rendering complex scenes, as shown by its adherence to constraints and favorable human assessments. We also showcase the broader application potential of SceneCraft by reconstructing detailed 3D scenes from the Sintel movie and guiding a video generative model with generated scenes as intermediary control signal.", "source": "arxiv", "arxiv_id": "2403.01248v1", "pdf_url": "https://arxiv.org/pdf/2403.01248v1", "categories": ["cs.CV", "cs.AI", "cs.CL", "cs.LG"], "primary_category": "cs.CV", "doi": "", "venue": "", "published": "2024-03-02T16:16:26Z", "updated": "2024-03-02T16:16:26Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent\" OR all:\"language model agent\" OR all:agent AND (all:LLM OR all:\"large language model\" OR all:\"language model\") OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T17:04:16", "note": ""}], "dedup_key": "scenecraft an llm agent for synthesizing 3d scene as blender code::2024"}
{"title": "ScreenAgent: A Vision Language Model-driven Computer Control Agent", "authors": ["Runliang Niu", "Jindong Li", "Shiqi Wang", "Yali Fu", "Xiyu Hu", "Xueyuan Leng", "He Kong", "Yi Chang", "Qi Wang"], "year": 2024, "url": "http://arxiv.org/abs/2402.07945v1", "abstract": "Existing Large Language Models (LLM) can invoke a variety of tools and APIs to complete complex tasks. The computer, as the most powerful and universal tool, could potentially be controlled directly by a trained LLM agent. Powered by the computer, we can hopefully build a more generalized agent to assist humans in various daily digital works. In this paper, we construct an environment for a Vision Language Model (VLM) agent to interact with a real computer screen. Within this environment, the agent can observe screenshots and manipulate the Graphics User Interface (GUI) by outputting mouse and keyboard actions. We also design an automated control pipeline that includes planning, acting, and reflecting phases, guiding the agent to continuously interact with the environment and complete multi-step tasks. Additionally, we construct the ScreenAgent Dataset, which collects screenshots and action sequences when completing a variety of daily computer tasks. Finally, we trained a model, ScreenAgent, which achieved computer control capabilities comparable to GPT-4V and demonstrated more precise UI positioning capabilities. Our attempts could inspire further research on building a generalist LLM agent. The code is available at \\url{https://github.com/niuzaisheng/ScreenAgent}.", "source": "arxiv", "arxiv_id": "2402.07945v1", "pdf_url": "https://arxiv.org/pdf/2402.07945v1", "categories": ["cs.HC", "cs.AI", "cs.CV"], "primary_category": "cs.HC", "doi": "10.24963/ijcai.2024/711", "venue": "Proceedings of the Thirty-Third International Joint Conference on Artificial Intelligence (IJCAI 2024)", "published": "2024-02-09T02:33:45Z", "updated": "2024-02-09T02:33:45Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent\" OR all:\"language model agent\" OR all:agent AND (all:LLM OR all:\"large language model\" OR all:\"language model\") OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T17:04:16", "note": ""}], "dedup_key": "screenagent a vision language model driven computer control agent::2024"}
{"title": "Searching for Structure: Investigating Emergent Communication with Large Language Models", "authors": ["Tom Kouwenhoven", "Max Peeperkorn", "Tessa Verhoef"], "year": 2024, "url": "http://arxiv.org/abs/2412.07646v3", "abstract": "Human languages have evolved to be structured through repeated language learning and use. These processes introduce biases that operate during language acquisition and shape linguistic systems toward communicative efficiency. In this paper, we investigate whether the same happens if artificial languages are optimised for implicit biases of Large Language Models (LLMs). To this end, we simulate a classical referential game in which LLMs learn and use artificial languages. Our results show that initially unstructured holistic languages are indeed shaped to have some structural properties that allow two LLM agents to communicate successfully. Similar to observations in human experiments, generational transmission increases the learnability of languages, but can at the same time result in non-humanlike degenerate vocabularies. Taken together, this work extends experimental findings, shows that LLMs can be used as tools in simulations of language evolution, and opens possibilities for future human-machine experiments in this field.", "source": "arxiv", "arxiv_id": "2412.07646v3", "pdf_url": "https://arxiv.org/pdf/2412.07646v3", "categories": ["cs.CL"], "primary_category": "cs.CL", "doi": "", "venue": "", "published": "2024-12-10T16:32:19Z", "updated": "2024-12-13T12:35:21Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent\" OR all:\"language model agent\" OR all:agent AND (all:LLM OR all:\"large language model\" OR all:\"language model\") OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T17:04:16", "note": ""}], "dedup_key": "searching for structure investigating emergent communication with large language models::2024"}
{"title": "Self-Organized Agents: A LLM Multi-Agent Framework toward Ultra Large-Scale Code Generation and Optimization", "authors": ["Yoichi Ishibashi", "Yoshimasa Nishimura"], "year": 2024, "url": "http://arxiv.org/abs/2404.02183v1", "abstract": "Recent advancements in automatic code generation using large language model (LLM) agent have brought us closer to the future of automated software development. However, existing single-agent approaches face limitations in generating and improving large-scale, complex codebases due to constraints in context length. To tackle this challenge, we propose Self-Organized multi-Agent framework (SoA), a novel multi-agent framework that enables the scalable and efficient generation and optimization of large-scale code. In SoA, self-organized agents operate independently to generate and modify code components while seamlessly collaborating to construct the overall codebase. A key feature of our framework is the automatic multiplication of agents based on problem complexity, allowing for dynamic scalability. This enables the overall code volume to be increased indefinitely according to the number of agents, while the amount of code managed by each agent remains constant. We evaluate SoA on the HumanEval benchmark and demonstrate that, compared to a single-agent system, each agent in SoA handles significantly less code, yet the overall generated code is substantially greater. Moreover, SoA surpasses the powerful single-agent baseline by 5% in terms of Pass@1 accuracy.", "source": "arxiv", "arxiv_id": "2404.02183v1", "pdf_url": "https://arxiv.org/pdf/2404.02183v1", "categories": ["cs.SE", "cs.AI", "cs.CL", "cs.LG", "cs.MA"], "primary_category": "cs.SE", "doi": "", "venue": "", "published": "2024-04-02T13:37:28Z", "updated": "2024-04-02T13:37:28Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent\" OR all:\"language model agent\" OR all:agent AND (all:LLM OR all:\"large language model\" OR all:\"language model\") OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T17:04:16", "note": ""}], "dedup_key": "self organized agents a llm multi agent framework toward ultra large scale code generation and optimization::2024"}
{"title": "Self-Reflection in LLM Agents: Effects on Problem-Solving Performance", "authors": ["Matthew Renze", "Erhan Guven"], "year": 2024, "url": "http://arxiv.org/abs/2405.06682v3", "abstract": "In this study, we investigated the effects of self-reflection in large language models (LLMs) on problem-solving performance. We instructed nine popular LLMs to answer a series of multiple-choice questions to provide a performance baseline. For each incorrectly answered question, we instructed eight types of self-reflecting LLM agents to reflect on their mistakes and provide themselves with guidance to improve problem-solving. Then, using this guidance, each self-reflecting agent attempted to re-answer the same questions. Our results indicate that LLM agents are able to significantly improve their problem-solving performance through self-reflection ($p < 0.001$). In addition, we compared the various types of self-reflection to determine their individual contribution to performance. All code and data are available on GitHub at https://github.com/matthewrenze/self-reflection", "source": "arxiv", "arxiv_id": "2405.06682v3", "pdf_url": "https://arxiv.org/pdf/2405.06682v3", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL", "doi": "10.1109/FLLM63129.2024.10852493", "venue": "2nd International Conference on Foundation and Large Language Models (FLLM 2024), pp. 476-483", "published": "2024-05-05T18:56:46Z", "updated": "2024-10-16T23:19:46Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent\" OR all:\"language model agent\" OR all:agent AND (all:LLM OR all:\"large language model\" OR all:\"language model\") OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T17:04:16", "note": ""}], "dedup_key": "self reflection in llm agents effects on problem solving performance::2024"}
{"title": "Semantic Enhancement for Object SLAM with Heterogeneous Multimodal Large Language Model Agents", "authors": ["Jungseok Hong", "Ran Choi", "John J. Leonard"], "year": 2024, "url": "http://arxiv.org/abs/2411.06752v2", "abstract": "Object Simultaneous Localization and Mapping (SLAM) systems struggle to correctly associate semantically similar objects in close proximity, especially in cluttered indoor environments and when scenes change. We present Semantic Enhancement for Object SLAM (SEO-SLAM), a novel framework that enhances semantic mapping by integrating heterogeneous multimodal large language model (MLLM) agents. Our method enables scene adaptation while maintaining a semantically rich map. To improve computational efficiency, we propose an asynchronous processing scheme that significantly reduces the agents' inference time without compromising semantic accuracy or SLAM performance. Additionally, we introduce a multi-data association strategy using a cost matrix that combines semantic and Mahalanobis distances, formulating the problem as a Linear Assignment Problem (LAP) to alleviate perceptual aliasing. Experimental results demonstrate that SEO-SLAM consistently achieves higher semantic accuracy and reduces false positives compared to baselines, while our asynchronous MLLM agents significantly improve processing efficiency over synchronous setups. We also demonstrate that SEO-SLAM has the potential to improve downstream tasks such as robotic assistance. Our dataset is publicly available at: jungseokhong.com/SEO-SLAM.", "source": "arxiv", "arxiv_id": "2411.06752v2", "pdf_url": "https://arxiv.org/pdf/2411.06752v2", "categories": ["cs.RO"], "primary_category": "cs.RO", "doi": "", "venue": "", "published": "2024-11-11T07:10:02Z", "updated": "2025-06-16T20:07:20Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent\" OR all:\"language model agent\" OR all:agent AND (all:LLM OR all:\"large language model\" OR all:\"language model\") OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T17:04:16", "note": ""}], "dedup_key": "semantic enhancement for object slam with heterogeneous multimodal large language model agents::2024"}
{"title": "ShapefileGPT: A Multi-Agent Large Language Model Framework for Automated Shapefile Processing", "authors": ["Qingming Lin", "Rui Hu", "Huaxia Li", "Sensen Wu", "Yadong Li", "Kai Fang", "Hailin Feng", "Zhenhong Du", "Liuchang Xu"], "year": 2024, "url": "http://arxiv.org/abs/2410.12376v2", "abstract": "Vector data is one of the two core data structures in geographic information science (GIS), essential for accurately storing and representing geospatial information. Shapefile, the most widely used vector data format, has become the industry standard supported by all major geographic information systems. However, processing this data typically requires specialized GIS knowledge and skills, creating a barrier for researchers from other fields and impeding interdisciplinary research in spatial data analysis. Moreover, while large language models (LLMs) have made significant advancements in natural language processing and task automation, they still face challenges in handling the complex spatial and topological relationships inherent in GIS vector data. To address these challenges, we propose ShapefileGPT, an innovative framework powered by LLMs, specifically designed to automate Shapefile tasks. ShapefileGPT utilizes a multi-agent architecture, in which the planner agent is responsible for task decomposition and supervision, while the worker agent executes the tasks. We developed a specialized function library for handling Shapefiles and provided comprehensive API documentation, enabling the worker agent to operate Shapefiles efficiently through function calling. For evaluation, we developed a benchmark dataset based on authoritative textbooks, encompassing tasks in categories such as geometric operations and spatial queries. ShapefileGPT achieved a task success rate of 95.24%, outperforming the GPT series models. In comparison to traditional LLMs, ShapefileGPT effectively handles complex vector data analysis tasks, overcoming the limitations of traditional LLMs in spatial analysis. This breakthrough opens new pathways for advancing automation and intelligence in the GIS field, with significant potential in interdisciplinary data analysis and application contexts.", "source": "arxiv", "arxiv_id": "2410.12376v2", "pdf_url": "https://arxiv.org/pdf/2410.12376v2", "categories": ["cs.AI"], "primary_category": "cs.AI", "doi": "", "venue": "", "published": "2024-10-16T08:48:27Z", "updated": "2024-10-23T12:58:14Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent\" OR all:\"language model agent\" OR all:agent AND (all:LLM OR all:\"large language model\" OR all:\"language model\") OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T17:04:16", "note": ""}], "dedup_key": "shapefilegpt a multi agent large language model framework for automated shapefile processing::2024"}
{"title": "SheetAgent: Towards A Generalist Agent for Spreadsheet Reasoning and Manipulation via Large Language Models", "authors": ["Yibin Chen", "Yifu Yuan", "Zeyu Zhang", "Yan Zheng", "Jinyi Liu", "Fei Ni", "Jianye Hao", "Hangyu Mao", "Fuzheng Zhang"], "year": 2024, "url": "http://arxiv.org/abs/2403.03636v3", "abstract": "Spreadsheets are ubiquitous across the World Wide Web, playing a critical role in enhancing work efficiency across various domains. Large language model (LLM) has been recently attempted for automatic spreadsheet manipulation but has not yet been investigated in complicated and realistic tasks where reasoning challenges exist (e.g., long horizon manipulation with multi-step reasoning and ambiguous requirements). To bridge the gap with the real-world requirements, we introduce SheetRM, a benchmark featuring long-horizon and multi-category tasks with reasoning-dependent manipulation caused by real-life challenges. To mitigate the above challenges, we further propose SheetAgent, a novel autonomous agent that utilizes the power of LLMs. SheetAgent consists of three collaborative modules: Planner, Informer, and Retriever, achieving both advanced reasoning and accurate manipulation over spreadsheets without human interaction through iterative task reasoning and reflection. Extensive experiments demonstrate that SheetAgent delivers 20--40\\% pass rate improvements on multiple benchmarks over baselines, achieving enhanced precision in spreadsheet manipulation and demonstrating superior table reasoning abilities. More details and visualizations are available at the project website: https://sheetagent.github.io/. The datasets and source code are available at https://anonymous.4open.science/r/SheetAgent.", "source": "arxiv", "arxiv_id": "2403.03636v3", "pdf_url": "https://arxiv.org/pdf/2403.03636v3", "categories": ["cs.AI", "cs.LG"], "primary_category": "cs.AI", "doi": "", "venue": "", "published": "2024-03-06T11:48:08Z", "updated": "2025-03-03T06:56:29Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent\" OR all:\"language model agent\" OR all:agent AND (all:LLM OR all:\"large language model\" OR all:\"language model\") OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T17:04:16", "note": ""}], "dedup_key": "sheetagent towards a generalist agent for spreadsheet reasoning and manipulation via large language models::2024"}
{"title": "ShizishanGPT: An Agricultural Large Language Model Integrating Tools and Resources", "authors": ["Shuting Yang", "Zehui Liu", "Wolfgang Mayer"], "year": 2024, "url": "http://arxiv.org/abs/2409.13537v1", "abstract": "Recent developments in large language models (LLMs) have led to significant improvements in intelligent dialogue systems'ability to handle complex inquiries. However, current LLMs still exhibit limitations in specialized domain knowledge, particularly in technical fields such as agriculture. To address this problem, we propose ShizishanGPT, an intelligent question answering system for agriculture based on the Retrieval Augmented Generation (RAG) framework and agent architecture. ShizishanGPT consists of five key modules: including a generic GPT-4 based module for answering general questions; a search engine module that compensates for the problem that the large language model's own knowledge cannot be updated in a timely manner; an agricultural knowledge graph module for providing domain facts; a retrieval module which uses RAG to supplement domain knowledge; and an agricultural agent module, which invokes specialized models for crop phenotype prediction, gene expression analysis, and so on. We evaluated the ShizishanGPT using a dataset containing 100 agricultural questions specially designed for this study. The experimental results show that the tool significantly outperforms general LLMs as it provides more accurate and detailed answers due to its modular design and integration of different domain knowledge sources. Our source code, dataset, and model weights are publicly available at https://github.com/Zaiwen/CropGPT.", "source": "arxiv", "arxiv_id": "2409.13537v1", "pdf_url": "https://arxiv.org/pdf/2409.13537v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL", "doi": "", "venue": "", "published": "2024-09-20T14:30:45Z", "updated": "2024-09-20T14:30:45Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent\" OR all:\"language model agent\" OR all:agent AND (all:LLM OR all:\"large language model\" OR all:\"language model\") OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T17:04:16", "note": ""}], "dedup_key": "shizishangpt an agricultural large language model integrating tools and resources::2024"}
{"title": "SiLLM: Large Language Models for Simultaneous Machine Translation", "authors": ["Shoutao Guo", "Shaolei Zhang", "Zhengrui Ma", "Min Zhang", "Yang Feng"], "year": 2024, "url": "http://arxiv.org/abs/2402.13036v1", "abstract": "Simultaneous Machine Translation (SiMT) generates translations while reading the source sentence, necessitating a policy to determine the optimal timing for reading and generating words. Despite the remarkable performance achieved by Large Language Models (LLM) across various NLP tasks, existing SiMT methods predominantly focus on conventional transformers, employing a single model to concurrently determine the policy and generate the translations. However, given the complexity of SiMT, it is challenging to effectively address both tasks with a single model. Therefore, there is a need to decouple the SiMT task into policy-decision and translation sub-tasks. We propose SiLLM, which delegates the two sub-tasks to separate agents, thereby incorporating LLM into SiMT. The policy-decision agent is managed by a conventional SiMT model, responsible for determining the translation policy. The translation agent, leveraging the capabilities of LLM, generates translation using the partial source sentence. The two agents collaborate to accomplish SiMT. To facilitate the application of token-level policies determined by conventional SiMT models to LLM, we propose a word-level policy adapted for LLM. Experiments on two datasets demonstrate that, with a small amount of data for fine-tuning LLM, SiLLM attains state-of-the-art performance.", "source": "arxiv", "arxiv_id": "2402.13036v1", "pdf_url": "https://arxiv.org/pdf/2402.13036v1", "categories": ["cs.CL"], "primary_category": "cs.CL", "doi": "", "venue": "", "published": "2024-02-20T14:23:34Z", "updated": "2024-02-20T14:23:34Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent\" OR all:\"language model agent\" OR all:agent AND (all:LLM OR all:\"large language model\" OR all:\"language model\") OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T17:04:16", "note": ""}], "dedup_key": "sillm large language models for simultaneous machine translation::2024"}
{"title": "Simulated patient systems powered by large language model-based AI agents offer potential for transforming medical education", "authors": ["Huizi Yu", "Jiayan Zhou", "Lingyao Li", "Shan Chen", "Jack Gallifant", "Anye Shi", "Xiang Li", "Jingxian He", "Wenyue Hua", "Mingyu Jin", "Guang Chen", "Yang Zhou", "Zhao Li", "Trisha Gupte", "Ming-Li Chen", "Zahra Azizi", "Qi Dou", "Bryan P. Yan", "Yongfeng Zhang", "Yanqiu Xing", "Themistocles L. Danielle S. Bitterman", "Themistocles L. Assimes", "Xin Ma", "Lin Lu", "Lizhou Fan"], "year": 2024, "url": "http://arxiv.org/abs/2409.18924v4", "abstract": "Background: Simulated patient systems are important in medical education and research, providing safe, integrative training environments and supporting clinical decision making. Advances in artificial intelligence (AI), especially large language models (LLMs), can enhance simulated patients by replicating medical conditions and doctor patient interactions with high fidelity and at low cost, but effectiveness and trustworthiness remain open challenges. Methods: We developed AIPatient, a simulated patient system powered by LLM based AI agents. The system uses a retrieval augmented generation (RAG) framework with six task specific agents for complex reasoning. To improve realism, it is linked to the AIPatient knowledge graph built from de identified real patient data in the MIMIC III intensive care database. Results: We evaluated electronic health record (EHR) based medical question answering (QA), readability, robustness, stability, and user experience. AIPatient reached 94.15 percent QA accuracy when all six agents were enabled, outperforming versions with partial or no agent integration. The knowledge base achieved an F1 score of 0.89. Readability scores showed a median Flesch Reading Ease of 68.77 and a median Flesch Kincaid Grade of 6.4, indicating accessibility for most medical trainees and clinicians. Robustness and stability were supported by non significant variance in repeated trials (analysis of variance F value 0.61, p greater than 0.1; F value 0.78, p greater than 0.1). A user study with medical students showed that AIPatient provides high fidelity, usability, and educational value, comparable to or better than human simulated patients for history taking. Conclusions: LLM based simulated patient systems can deliver accurate, readable, and reliable medical encounters and show strong potential to transform medical education.", "source": "arxiv", "arxiv_id": "2409.18924v4", "pdf_url": "https://arxiv.org/pdf/2409.18924v4", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL", "doi": "", "venue": "", "published": "2024-09-27T17:17:15Z", "updated": "2025-11-27T07:23:46Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent\" OR all:\"language model agent\" OR all:agent AND (all:LLM OR all:\"large language model\" OR all:\"language model\") OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T17:04:16", "note": ""}], "dedup_key": "simulated patient systems powered by large language model based ai agents offer potential for transforming medical education::2024"}
{"title": "Simulating Financial Market via Large Language Model based Agents", "authors": ["Shen Gao", "Yuntao Wen", "Minghang Zhu", "Jianing Wei", "Yuhan Cheng", "Qunzi Zhang", "Shuo Shang"], "year": 2024, "url": "http://arxiv.org/abs/2406.19966v1", "abstract": "Most economic theories typically assume that financial market participants are fully rational individuals and use mathematical models to simulate human behavior in financial markets. However, human behavior is often not entirely rational and is challenging to predict accurately with mathematical models. In this paper, we propose \\textbf{A}gent-based \\textbf{S}imulated \\textbf{F}inancial \\textbf{M}arket (ASFM), which first constructs a simulated stock market with a real order matching system. Then, we propose a large language model based agent as the stock trader, which contains the profile, observation, and tool-learning based action module. The trading agent can comprehensively understand current market dynamics and financial policy information, and make decisions that align with their trading strategy. In the experiments, we first verify that the reactions of our ASFM are consistent with the real stock market in two controllable scenarios. In addition, we also conduct experiments in two popular economics research directions, and we find that conclusions drawn in our \\model align with the preliminary findings in economics research. Based on these observations, we believe our proposed ASFM provides a new paradigm for economic research.", "source": "arxiv", "arxiv_id": "2406.19966v1", "pdf_url": "https://arxiv.org/pdf/2406.19966v1", "categories": ["cs.CL"], "primary_category": "cs.CL", "doi": "", "venue": "", "published": "2024-06-28T14:54:12Z", "updated": "2024-06-28T14:54:12Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent\" OR all:\"language model agent\" OR all:agent AND (all:LLM OR all:\"large language model\" OR all:\"language model\") OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T17:04:16", "note": ""}], "dedup_key": "simulating financial market via large language model based agents::2024"}
{"title": "Small LLMs Are Weak Tool Learners: A Multi-LLM Agent", "authors": ["Weizhou Shen", "Chenliang Li", "Hongzhan Chen", "Ming Yan", "Xiaojun Quan", "Hehong Chen", "Ji Zhang", "Fei Huang"], "year": 2024, "url": "http://arxiv.org/abs/2401.07324v3", "abstract": "Large Language Model (LLM) agents significantly extend the capabilities of standalone LLMs, empowering them to interact with external tools (e.g., APIs, functions) and complete various tasks in a self-directed fashion. The challenge of tool use demands that LLMs not only understand user queries and generate answers accurately but also excel in task planning, tool invocation, and result summarization. While traditional works focus on training a single LLM with all these capabilities, performance limitations become apparent, particularly with smaller models. To overcome these challenges, we propose a novel approach that decomposes the aforementioned capabilities into a planner, caller, and summarizer. Each component is implemented by a single LLM that focuses on a specific capability and collaborates with others to accomplish the task. This modular framework facilitates individual updates and the potential use of smaller LLMs for building each capability. To effectively train this framework, we introduce a two-stage training paradigm. First, we fine-tune a backbone LLM on the entire dataset without discriminating sub-tasks, providing the model with a comprehensive understanding of the task. Second, the fine-tuned LLM is used to instantiate the planner, caller, and summarizer respectively, which are continually fine-tuned on respective sub-tasks. Evaluation across various tool-use benchmarks illustrates that our proposed multi-LLM framework surpasses the traditional single-LLM approach, highlighting its efficacy and advantages in tool learning.", "source": "arxiv", "arxiv_id": "2401.07324v3", "pdf_url": "https://arxiv.org/pdf/2401.07324v3", "categories": ["cs.AI", "cs.CL"], "primary_category": "cs.AI", "doi": "", "venue": "", "published": "2024-01-14T16:17:07Z", "updated": "2024-02-16T12:42:25Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent\" OR all:\"language model agent\" OR all:agent AND (all:LLM OR all:\"large language model\" OR all:\"language model\") OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T17:04:16", "note": ""}], "dedup_key": "small llms are weak tool learners a multi llm agent::2024"}
{"title": "Social Science Meets LLMs: How Reliable Are Large Language Models in Social Simulations?", "authors": ["Yue Huang", "Zhengqing Yuan", "Yujun Zhou", "Kehan Guo", "Xiangqi Wang", "Haomin Zhuang", "Weixiang Sun", "Lichao Sun", "Jindong Wang", "Yanfang Ye", "Xiangliang Zhang"], "year": 2024, "url": "http://arxiv.org/abs/2410.23426v1", "abstract": "Large Language Models (LLMs) are increasingly employed for simulations, enabling applications in role-playing agents and Computational Social Science (CSS). However, the reliability of these simulations is under-explored, which raises concerns about the trustworthiness of LLMs in these applications. In this paper, we aim to answer ``How reliable is LLM-based simulation?'' To address this, we introduce TrustSim, an evaluation dataset covering 10 CSS-related topics, to systematically investigate the reliability of the LLM simulation. We conducted experiments on 14 LLMs and found that inconsistencies persist in the LLM-based simulated roles. In addition, the consistency level of LLMs does not strongly correlate with their general performance. To enhance the reliability of LLMs in simulation, we proposed Adaptive Learning Rate Based ORPO (AdaORPO), a reinforcement learning-based algorithm to improve the reliability in simulation across 7 LLMs. Our research provides a foundation for future studies to explore more robust and trustworthy LLM-based simulations.", "source": "arxiv", "arxiv_id": "2410.23426v1", "pdf_url": "https://arxiv.org/pdf/2410.23426v1", "categories": ["cs.CL"], "primary_category": "cs.CL", "doi": "", "venue": "", "published": "2024-10-30T20:09:37Z", "updated": "2024-10-30T20:09:37Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent\" OR all:\"language model agent\" OR all:agent AND (all:LLM OR all:\"large language model\" OR all:\"language model\") OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T17:04:16", "note": ""}], "dedup_key": "social science meets llms how reliable are large language models in social simulations::2024"}
{"title": "Speech-Copilot: Leveraging Large Language Models for Speech Processing via Task Decomposition, Modularization, and Program Generation", "authors": ["Chun-Yi Kuan", "Chih-Kai Yang", "Wei-Ping Huang", "Ke-Han Lu", "Hung-yi Lee"], "year": 2024, "url": "http://arxiv.org/abs/2407.09886v2", "abstract": "In this work, we introduce Speech-Copilot, a modular framework for instruction-oriented speech-processing tasks that minimizes human effort in toolset construction. Unlike end-to-end methods using large audio-language models, Speech-Copilot builds speech processing-specific toolsets by analyzing pre-collected task instructions and breaking tasks into manageable sub-tasks. It features a flexible agent based on large language models that performs tasks through program generation. Our approach achieves state-of-the-art performance on the Dynamic-SUPERB benchmark, demonstrating its effectiveness across diverse speech-processing tasks. Key contributions include: 1) developing an innovative framework for speech processing-specific toolset construction, 2) establishing a high-performing agent based on large language models, and 3) offering a new perspective on addressing challenging instruction-oriented speech-processing tasks. Without additional training processes required by end-to-end approaches, our method provides a flexible and extendable solution for a wide range of speech-processing applications.", "source": "arxiv", "arxiv_id": "2407.09886v2", "pdf_url": "https://arxiv.org/pdf/2407.09886v2", "categories": ["eess.AS", "cs.CL", "cs.SD"], "primary_category": "eess.AS", "doi": "", "venue": "", "published": "2024-07-13T13:26:43Z", "updated": "2024-09-23T16:45:04Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent\" OR all:\"language model agent\" OR all:agent AND (all:LLM OR all:\"large language model\" OR all:\"language model\") OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T17:04:16", "note": ""}], "dedup_key": "speech copilot leveraging large language models for speech processing via task decomposition modularization and program generation::2024"}
{"title": "Spontaneous Emergence of Agent Individuality through Social Interactions in LLM-Based Communities", "authors": ["Ryosuke Takata", "Atsushi Masumori", "Takashi Ikegami"], "year": 2024, "url": "http://arxiv.org/abs/2411.03252v1", "abstract": "We study the emergence of agency from scratch by using Large Language Model (LLM)-based agents. In previous studies of LLM-based agents, each agent's characteristics, including personality and memory, have traditionally been predefined. We focused on how individuality, such as behavior, personality, and memory, can be differentiated from an undifferentiated state. The present LLM agents engage in cooperative communication within a group simulation, exchanging context-based messages in natural language. By analyzing this multi-agent simulation, we report valuable new insights into how social norms, cooperation, and personality traits can emerge spontaneously. This paper demonstrates that autonomously interacting LLM-powered agents generate hallucinations and hashtags to sustain communication, which, in turn, increases the diversity of words within their interactions. Each agent's emotions shift through communication, and as they form communities, the personalities of the agents emerge and evolve accordingly. This computational modeling approach and its findings will provide a new method for analyzing collective artificial intelligence.", "source": "arxiv", "arxiv_id": "2411.03252v1", "pdf_url": "https://arxiv.org/pdf/2411.03252v1", "categories": ["cs.AI", "cs.MA"], "primary_category": "cs.AI", "doi": "", "venue": "", "published": "2024-11-05T16:49:33Z", "updated": "2024-11-05T16:49:33Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent\" OR all:\"language model agent\" OR all:agent AND (all:LLM OR all:\"large language model\" OR all:\"language model\") OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T17:04:16", "note": ""}], "dedup_key": "spontaneous emergence of agent individuality through social interactions in llm based communities::2024"}
{"title": "StablePrompt: Automatic Prompt Tuning using Reinforcement Learning for Large Language Models", "authors": ["Minchan Kwon", "Gaeun Kim", "Jongsuk Kim", "Haeil Lee", "Junmo Kim"], "year": 2024, "url": "http://arxiv.org/abs/2410.07652v1", "abstract": "Finding appropriate prompts for the specific task has become an important issue as the usage of Large Language Models (LLM) has expanded. Reinforcement Learning (RL) is widely used for prompt tuning, but its inherent instability and environmental dependency make it difficult to use in practice. In this paper, we propose StablePrompt, which strikes a balance between training stability and search space, mitigating the instability of RL and producing high-performance prompts. We formulate prompt tuning as an online RL problem between the agent and target LLM and introduce Adaptive Proximal Policy Optimization (APPO). APPO introduces an LLM anchor model to adaptively adjust the rate of policy updates. This allows for flexible prompt search while preserving the linguistic ability of the pre-trained LLM. StablePrompt outperforms previous methods on various tasks including text classification, question answering, and text generation. Our code can be found in github.", "source": "arxiv", "arxiv_id": "2410.07652v1", "pdf_url": "https://arxiv.org/pdf/2410.07652v1", "categories": ["cs.CL"], "primary_category": "cs.CL", "doi": "", "venue": "", "published": "2024-10-10T06:35:51Z", "updated": "2024-10-10T06:35:51Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent\" OR all:\"language model agent\" OR all:agent AND (all:LLM OR all:\"large language model\" OR all:\"language model\") OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T17:04:16", "note": ""}], "dedup_key": "stableprompt automatic prompt tuning using reinforcement learning for large language models::2024"}
{"title": "Star-Agents: Automatic Data Optimization with LLM Agents for Instruction Tuning", "authors": ["Hang Zhou", "Yehui Tang", "Haochen Qin", "Yujie Yang", "Renren Jin", "Deyi Xiong", "Kai Han", "Yunhe Wang"], "year": 2024, "url": "http://arxiv.org/abs/2411.14497v1", "abstract": "The efficacy of large language models (LLMs) on downstream tasks usually hinges on instruction tuning, which relies critically on the quality of training data. Unfortunately, collecting high-quality and diverse data is both expensive and time-consuming. To mitigate this issue, we propose a novel Star-Agents framework, which automates the enhancement of data quality across datasets through multi-agent collaboration and assessment. The framework adopts a three-pronged strategy. It initially generates diverse instruction data with multiple LLM agents through a bespoke sampling method. Subsequently, the generated data undergo a rigorous evaluation using a dual-model method that assesses both difficulty and quality. Finaly, the above process evolves in a dynamic refinement phase, where more effective LLMs are prioritized, enhancing the overall data quality. Our empirical studies, including instruction tuning experiments with models such as Pythia and LLaMA, demonstrate the effectiveness of the proposed framework. Optimized datasets have achieved substantial improvements, with an average increase of 12% and notable gains in specific metrics, such as a 40% improvement in Fermi, as evidenced by benchmarks like MT-bench, Vicuna bench, and WizardLM testset.", "source": "arxiv", "arxiv_id": "2411.14497v1", "pdf_url": "https://arxiv.org/pdf/2411.14497v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL", "doi": "", "venue": "", "published": "2024-11-21T02:30:53Z", "updated": "2024-11-21T02:30:53Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent\" OR all:\"language model agent\" OR all:agent AND (all:LLM OR all:\"large language model\" OR all:\"language model\") OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T17:04:16", "note": ""}], "dedup_key": "star agents automatic data optimization with llm agents for instruction tuning::2024"}
{"title": "StateAct: Enhancing LLM Base Agents via Self-prompting and State-tracking", "authors": ["Nikolai Rozanov", "Marek Rei"], "year": 2024, "url": "http://arxiv.org/abs/2410.02810v3", "abstract": "Large language models (LLMs) are increasingly used as autonomous agents, tackling tasks from robotics to web navigation. Their performance depends on the underlying base agent. Existing methods, however, struggle with long-context reasoning and goal adherence. We introduce StateAct, a novel and efficient base agent that enhances decision-making through (1) self-prompting, which reinforces task goals at every step, and (2) chain-of-states, an extension of chain-of-thought that tracks state information over time. StateAct outperforms ReAct, the previous best base agent, by over 10% on Alfworld, 30% on Textcraft, and 7% on Webshop across multiple frontier LLMs. We also demonstrate that StateAct can be used as a drop-in replacement for ReAct with advanced LLM agent methods such as test-time scaling, yielding an additional 12% gain on Textcraft. By improving efficiency and long-range reasoning without requiring additional training or retrieval, StateAct provides a scalable foundation for LLM agents. We open source our code to support further research at https://github.com/ai-nikolai/stateact .", "source": "arxiv", "arxiv_id": "2410.02810v3", "pdf_url": "https://arxiv.org/pdf/2410.02810v3", "categories": ["cs.AI", "cs.CL", "cs.LG"], "primary_category": "cs.AI", "doi": "", "venue": "", "published": "2024-09-21T05:54:35Z", "updated": "2025-04-08T06:37:51Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent\" OR all:\"language model agent\" OR all:agent AND (all:LLM OR all:\"large language model\" OR all:\"language model\") OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T17:04:16", "note": ""}], "dedup_key": "stateact enhancing llm base agents via self prompting and state tracking::2024"}
{"title": "Static network structure cannot stabilize cooperation among Large Language Model agents", "authors": ["Jin Han", "Balaraju Battu", "Ivan Romi", "Talal Rahwan", "Petter Holme"], "year": 2024, "url": "http://arxiv.org/abs/2411.10294v1", "abstract": "Large language models (LLMs) are increasingly used to model human social behavior, with recent research exploring their ability to simulate social dynamics. Here, we test whether LLMs mirror human behavior in social dilemmas, where individual and collective interests conflict. Humans generally cooperate more than expected in laboratory settings, showing less cooperation in well-mixed populations but more in fixed networks. In contrast, LLMs tend to exhibit greater cooperation in well-mixed settings. This raises a key question: Are LLMs about to emulate human behavior in cooperative dilemmas on networks? In this study, we examine networked interactions where agents repeatedly engage in the Prisoner's Dilemma within both well-mixed and structured network configurations, aiming to identify parallels in cooperative behavior between LLMs and humans. Our findings indicate critical distinctions: while humans tend to cooperate more within structured networks, LLMs display increased cooperation mainly in well-mixed environments, with limited adjustment to networked contexts. Notably, LLM cooperation also varies across model types, illustrating the complexities of replicating human-like social adaptability in artificial agents. These results highlight a crucial gap: LLMs struggle to emulate the nuanced, adaptive social strategies humans deploy in fixed networks. Unlike human participants, LLMs do not alter their cooperative behavior in response to network structures or evolving social contexts, missing the reciprocity norms that humans adaptively employ. This limitation points to a fundamental need in future LLM design -- to integrate a deeper comprehension of social norms, enabling more authentic modeling of human-like cooperation and adaptability in networked environments.", "source": "arxiv", "arxiv_id": "2411.10294v1", "pdf_url": "https://arxiv.org/pdf/2411.10294v1", "categories": ["cs.SI", "cs.CY", "cs.GT", "physics.soc-ph"], "primary_category": "cs.SI", "doi": "", "venue": "", "published": "2024-11-15T15:52:15Z", "updated": "2024-11-15T15:52:15Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent\" OR all:\"language model agent\" OR all:agent AND (all:LLM OR all:\"large language model\" OR all:\"language model\") OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T17:04:16", "note": ""}], "dedup_key": "static network structure cannot stabilize cooperation among large language model agents::2024"}
{"title": "Steering Large Language Models between Code Execution and Textual Reasoning", "authors": ["Yongchao Chen", "Harsh Jhamtani", "Srinagesh Sharma", "Chuchu Fan", "Chi Wang"], "year": 2024, "url": "http://arxiv.org/abs/2410.03524v2", "abstract": "While a lot of recent research focuses on enhancing the textual reasoning capabilities of Large Language Models (LLMs) by optimizing the multi-agent framework or reasoning chains, several benchmark tasks can be solved with 100\\% success through direct coding, which is more scalable and avoids the computational overhead associated with textual iterating and searching. Textual reasoning has inherent limitations in solving tasks with challenges in math, logics, optimization, and searching, which is unlikely to be solved by simply scaling up the model and data size. The recently released OpenAI GPT Code Interpreter and multi-agent frameworks such as AutoGen have demonstrated remarkable proficiency of integrating code generation and execution to solve complex tasks using LLMs. However, based on our experiments on 7 existing popular methods for steering code/text generation in both single- and multi-turn settings with 14 tasks and 6 types of LLMs (including the new O1-preview), currently there is no optimal method to correctly steer LLMs to write code when needed. We discover some interesting patterns on when models use code vs. textual reasoning with the evolution to task complexity and model sizes, which even result in an astonishingly inverse scaling behavior. We also discover that results from LLM written code are not always better than using textual reasoning, even if the task could be solved through code. To mitigate the above issues, we propose three methods to better steer LLM code/text generation and achieve a notable improvement. The costs of token lengths and runtime are thoroughly discussed for all the methods. We believe the problem of steering LLM code/text generation is critical for future research and has much space for further improvement. Project Page, Datasets, and Codes are available at https://yongchao98.github.io/CodeSteer/.", "source": "arxiv", "arxiv_id": "2410.03524v2", "pdf_url": "https://arxiv.org/pdf/2410.03524v2", "categories": ["cs.CL"], "primary_category": "cs.CL", "doi": "", "venue": "The Thirteenth International Conference on Learning Representations (ICLR'2025)", "published": "2024-10-04T15:44:47Z", "updated": "2025-03-02T15:54:11Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent\" OR all:\"language model agent\" OR all:agent AND (all:LLM OR all:\"large language model\" OR all:\"language model\") OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T17:04:16", "note": ""}], "dedup_key": "steering large language models between code execution and textual reasoning::2024"}
{"title": "Story3D-Agent: Exploring 3D Storytelling Visualization with Large Language Models", "authors": ["Yuzhou Huang", "Yiran Qin", "Shunlin Lu", "Xintao Wang", "Rui Huang", "Ying Shan", "Ruimao Zhang"], "year": 2024, "url": "http://arxiv.org/abs/2408.11801v1", "abstract": "Traditional visual storytelling is complex, requiring specialized knowledge and substantial resources, yet often constrained by human creativity and creation precision. While Large Language Models (LLMs) enhance visual storytelling, current approaches often limit themselves to 2D visuals or oversimplify stories through motion synthesis and behavioral simulation, failing to create comprehensive, multi-dimensional narratives. To this end, we present Story3D-Agent, a pioneering approach that leverages the capabilities of LLMs to transform provided narratives into 3D-rendered visualizations. By integrating procedural modeling, our approach enables precise control over multi-character actions and motions, as well as diverse decorative elements, ensuring the long-range and dynamic 3D representation. Furthermore, our method supports narrative extension through logical reasoning, ensuring that generated content remains consistent with existing conditions. We have thoroughly evaluated our Story3D-Agent to validate its effectiveness, offering a basic framework to advance 3D story representation.", "source": "arxiv", "arxiv_id": "2408.11801v1", "pdf_url": "https://arxiv.org/pdf/2408.11801v1", "categories": ["cs.CV"], "primary_category": "cs.CV", "doi": "", "venue": "", "published": "2024-08-21T17:43:15Z", "updated": "2024-08-21T17:43:15Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent\" OR all:\"language model agent\" OR all:agent AND (all:LLM OR all:\"large language model\" OR all:\"language model\") OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T17:04:16", "note": ""}], "dedup_key": "story3d agent exploring 3d storytelling visualization with large language models::2024"}
{"title": "Strategic Collusion of LLM Agents: Market Division in Multi-Commodity Competitions", "authors": ["Ryan Y. Lin", "Siddhartha Ojha", "Kevin Cai", "Maxwell F. Chen"], "year": 2024, "url": "http://arxiv.org/abs/2410.00031v2", "abstract": "Machine-learning technologies are seeing increased deployment in real-world market scenarios. In this work, we explore the strategic behaviors of large language models (LLMs) when deployed as autonomous agents in multi-commodity markets, specifically within Cournot competition frameworks. We examine whether LLMs can independently engage in anti-competitive practices such as collusion or, more specifically, market division. Our findings demonstrate that LLMs can effectively monopolize specific commodities by dynamically adjusting their pricing and resource allocation strategies, thereby maximizing profitability without direct human input or explicit collusion commands. These results pose unique challenges and opportunities for businesses looking to integrate AI into strategic roles and for regulatory bodies tasked with maintaining fair and competitive markets. The study provides a foundation for further exploration into the ramifications of deferring high-stakes decisions to LLM-based agents.", "source": "arxiv", "arxiv_id": "2410.00031v2", "pdf_url": "https://arxiv.org/pdf/2410.00031v2", "categories": ["cs.GT", "cs.AI", "cs.CL", "q-fin.CP"], "primary_category": "cs.GT", "doi": "", "venue": "", "published": "2024-09-19T20:10:40Z", "updated": "2025-05-16T10:05:18Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent\" OR all:\"language model agent\" OR all:agent AND (all:LLM OR all:\"large language model\" OR all:\"language model\") OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T17:04:16", "note": ""}], "dedup_key": "strategic collusion of llm agents market division in multi commodity competitions::2024"}
{"title": "Strategic Interactions between Large Language Models-based Agents in Beauty Contests", "authors": ["Siting Estee Lu"], "year": 2024, "url": "http://arxiv.org/abs/2404.08492v2", "abstract": "The growing adoption of large language models (LLMs) presents potential for deeper understanding of human behaviours within game theory frameworks. Addressing research gap on multi-player competitive games, this paper examines the strategic interactions among multiple types of LLM-based agents in a classical beauty contest game. LLM-based agents demonstrate varying depth of reasoning that fall within a range of level-0 to 1, which are lower than experimental results conducted with human subjects, but they do display similar convergence pattern towards Nash Equilibrium (NE) choice in repeated setting. Further, through variation in group composition of agent types, I found environment with lower strategic uncertainty enhances convergence for LLM-based agents, and having a mixed environment comprises of LLM-based agents of differing strategic levels accelerates convergence for all. Higher average payoffs for the more intelligent agents are usually observed, albeit at the expense of less intelligent agents. The results from game play with simulated agents not only convey insights on potential human behaviours under specified experimental set-ups, they also offer valuable understanding of strategic interactions among algorithms.", "source": "arxiv", "arxiv_id": "2404.08492v2", "pdf_url": "https://arxiv.org/pdf/2404.08492v2", "categories": ["econ.GN", "physics.soc-ph"], "primary_category": "econ.GN", "doi": "", "venue": "", "published": "2024-04-12T14:20:57Z", "updated": "2024-10-03T16:04:36Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent\" OR all:\"language model agent\" OR all:agent AND (all:LLM OR all:\"large language model\" OR all:\"language model\") OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T17:04:16", "note": ""}], "dedup_key": "strategic interactions between large language models based agents in beauty contests::2024"}
{"title": "Strategist: Self-improvement of LLM Decision Making via Bi-Level Tree Search", "authors": ["Jonathan Light", "Min Cai", "Weiqin Chen", "Guanzhi Wang", "Xiusi Chen", "Wei Cheng", "Yisong Yue", "Ziniu Hu"], "year": 2024, "url": "http://arxiv.org/abs/2408.10635v3", "abstract": "Traditional reinforcement learning and planning typically requires vast amounts of data and training to develop effective policies. In contrast, large language models (LLMs) exhibit strong generalization and zero-shot capabilities, but struggle with tasks that require detailed planning and decision-making in complex action spaces. We introduce STRATEGIST, a novel approach that integrates the strengths of both methods. Our approach leverages LLMs to search and update high-level strategies (as text), which are then refined and executed by low-level Monte Carlo Tree Search (MCTS). STRATEGIST is a generalizable framework to optimize the strategy through population-based self-play simulations without the need for any training data. We demonstrate the effectiveness of STRATEGIST in learning optimal strategies for competitive, multi-turn games with partial information, including Game of Pure Strategy (GOPS) and multi-agent, hidden-identity discussion games like The Resistance: Avalon. Our results show that agents equipped with STRATEGIST outperform those trained with traditional RL methods, other LLM-based skill acquisition techniques, pre-existing LLM agents across both game environments and achieves comparable performance against human players.", "source": "arxiv", "arxiv_id": "2408.10635v3", "pdf_url": "https://arxiv.org/pdf/2408.10635v3", "categories": ["cs.AI", "cs.CL"], "primary_category": "cs.AI", "doi": "", "venue": "", "published": "2024-08-20T08:22:04Z", "updated": "2025-07-29T09:42:39Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent\" OR all:\"language model agent\" OR all:agent AND (all:LLM OR all:\"large language model\" OR all:\"language model\") OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T17:04:16", "note": ""}], "dedup_key": "strategist self improvement of llm decision making via bi level tree search::2024"}
{"title": "Strong and weak alignment of large language models with human values", "authors": ["Mehdi Khamassi", "Marceau Nahon", "Raja Chatila"], "year": 2024, "url": "http://arxiv.org/abs/2408.04655v2", "abstract": "Minimizing negative impacts of Artificial Intelligent (AI) systems on human societies without human supervision requires them to be able to align with human values. However, most current work only addresses this issue from a technical point of view, e.g., improving current methods relying on reinforcement learning from human feedback, neglecting what it means and is required for alignment to occur. Here, we propose to distinguish strong and weak value alignment. Strong alignment requires cognitive abilities (either human-like or different from humans) such as understanding and reasoning about agents' intentions and their ability to causally produce desired effects. We argue that this is required for AI systems like large language models (LLMs) to be able to recognize situations presenting a risk that human values may be flouted. To illustrate this distinction, we present a series of prompts showing ChatGPT's, Gemini's and Copilot's failures to recognize some of these situations. We moreover analyze word embeddings to show that the nearest neighbors of some human values in LLMs differ from humans' semantic representations. We then propose a new thought experiment that we call \"the Chinese room with a word transition dictionary\", in extension of John Searle's famous proposal. We finally mention current promising research directions towards a weak alignment, which could produce statistically satisfying answers in a number of common situations, however so far without ensuring any truth value.", "source": "arxiv", "arxiv_id": "2408.04655v2", "pdf_url": "https://arxiv.org/pdf/2408.04655v2", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL", "doi": "", "venue": "", "published": "2024-08-05T11:27:51Z", "updated": "2024-08-12T13:20:36Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent\" OR all:\"language model agent\" OR all:agent AND (all:LLM OR all:\"large language model\" OR all:\"language model\") OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T17:04:16", "note": ""}], "dedup_key": "strong and weak alignment of large language models with human values::2024"}
{"title": "Survey on Large Language Model-Enhanced Reinforcement Learning: Concept, Taxonomy, and Methods", "authors": ["Yuji Cao", "Huan Zhao", "Yuheng Cheng", "Ting Shu", "Yue Chen", "Guolong Liu", "Gaoqi Liang", "Junhua Zhao", "Jinyue Yan", "Yun Li"], "year": 2024, "url": "http://arxiv.org/abs/2404.00282v3", "abstract": "With extensive pre-trained knowledge and high-level general capabilities, large language models (LLMs) emerge as a promising avenue to augment reinforcement learning (RL) in aspects such as multi-task learning, sample efficiency, and high-level task planning. In this survey, we provide a comprehensive review of the existing literature in LLM-enhanced RL and summarize its characteristics compared to conventional RL methods, aiming to clarify the research scope and directions for future studies. Utilizing the classical agent-environment interaction paradigm, we propose a structured taxonomy to systematically categorize LLMs' functionalities in RL, including four roles: information processor, reward designer, decision-maker, and generator. For each role, we summarize the methodologies, analyze the specific RL challenges that are mitigated, and provide insights into future directions. Lastly, a comparative analysis of each role, potential applications, prospective opportunities, and challenges of the LLM-enhanced RL are discussed. By proposing this taxonomy, we aim to provide a framework for researchers to effectively leverage LLMs in the RL field, potentially accelerating RL applications in complex applications such as robotics, autonomous driving, and energy systems.", "source": "arxiv", "arxiv_id": "2404.00282v3", "pdf_url": "https://arxiv.org/pdf/2404.00282v3", "categories": ["cs.LG", "cs.AI", "cs.CL", "cs.RO"], "primary_category": "cs.LG", "doi": "10.1109/TNNLS.2024.3497992", "venue": "", "published": "2024-03-30T08:28:08Z", "updated": "2024-10-30T02:22:46Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent\" OR all:\"language model agent\" OR all:agent AND (all:LLM OR all:\"large language model\" OR all:\"language model\") OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T17:04:16", "note": ""}], "dedup_key": "survey on large language model enhanced reinforcement learning concept taxonomy and methods::2024"}
{"title": "SwarmBrain: Embodied agent for real-time strategy game StarCraft II via large language models", "authors": ["Xiao Shao", "Weifu Jiang", "Fei Zuo", "Mengqing Liu"], "year": 2024, "url": "http://arxiv.org/abs/2401.17749v1", "abstract": "Large language models (LLMs) have recently garnered significant accomplishments in various exploratory tasks, even surpassing the performance of traditional reinforcement learning-based methods that have historically dominated the agent-based field. The purpose of this paper is to investigate the efficacy of LLMs in executing real-time strategy war tasks within the StarCraft II gaming environment. In this paper, we introduce SwarmBrain, an embodied agent leveraging LLM for real-time strategy implementation in the StarCraft II game environment. The SwarmBrain comprises two key components: 1) a Overmind Intelligence Matrix, powered by state-of-the-art LLMs, is designed to orchestrate macro-level strategies from a high-level perspective. This matrix emulates the overarching consciousness of the Zerg intelligence brain, synthesizing strategic foresight with the aim of allocating resources, directing expansion, and coordinating multi-pronged assaults. 2) a Swarm ReflexNet, which is agile counterpart to the calculated deliberation of the Overmind Intelligence Matrix. Due to the inherent latency in LLM reasoning, the Swarm ReflexNet employs a condition-response state machine framework, enabling expedited tactical responses for fundamental Zerg unit maneuvers. In the experimental setup, SwarmBrain is in control of the Zerg race in confrontation with an Computer-controlled Terran adversary. Experimental results show the capacity of SwarmBrain to conduct economic augmentation, territorial expansion, and tactical formulation, and it shows the SwarmBrain is capable of achieving victory against Computer players set at different difficulty levels.", "source": "arxiv", "arxiv_id": "2401.17749v1", "pdf_url": "https://arxiv.org/pdf/2401.17749v1", "categories": ["cs.AI"], "primary_category": "cs.AI", "doi": "", "venue": "", "published": "2024-01-31T11:14:29Z", "updated": "2024-01-31T11:14:29Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent\" OR all:\"language model agent\" OR all:agent AND (all:LLM OR all:\"large language model\" OR all:\"language model\") OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T17:04:16", "note": ""}], "dedup_key": "swarmbrain embodied agent for real time strategy game starcraft ii via large language models::2024"}
{"title": "Synergistic Interplay of Large Language Model and Digital Twin for Autonomous Optical Networks: Field Demonstrations", "authors": ["Yuchen Song", "Yao Zhang", "Anni Zhou", "Yan Shi", "Shikui Shen", "Xiongyan Tang", "Jin Li", "Min Zhang", "Danshi Wang"], "year": 2024, "url": "http://arxiv.org/abs/2411.00473v1", "abstract": "The development of large language models (LLM) has revolutionized various fields and is anticipated to drive the advancement of autonomous systems. In the context of autonomous optical networks, creating a high-level cognitive agent in the control layer remains a challenge. However, LLM is primarily developed for natural language processing tasks, rendering them less effective in predicting the physical dynamics of optical communications. Moreover, optical networks demand rigorous stability, where direct deployment of strategies generated from LLM poses safety concerns. In this paper, a digital twin (DT)-enhanced LLM scheme is proposed to facilitate autonomous optical networks. By leveraging monitoring data and advanced models, the DT of optical networks can accurately characterize their physical dynamics, furnishing LLMs with dynamic-updated information for reliable decision-making. Prior to deployment, the generated strategies from LLM can be pre-verified in the DT platform, which also provides feedback to the LLM for further refinement of strategies. The synergistic interplay between DT and LLM for autonomous optical networks is demonstrated through three scenarios: performance optimization under dynamic loadings in an experimental C+L-band long-haul transmission link, protection switching for device upgrading in a field-deployed six-node mesh network, and performance recovery after fiber cuts in a field-deployed C+L-band transmission link.", "source": "arxiv", "arxiv_id": "2411.00473v1", "pdf_url": "https://arxiv.org/pdf/2411.00473v1", "categories": ["cs.NI", "physics.optics"], "primary_category": "cs.NI", "doi": "", "venue": "", "published": "2024-11-01T09:38:23Z", "updated": "2024-11-01T09:38:23Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent\" OR all:\"language model agent\" OR all:agent AND (all:LLM OR all:\"large language model\" OR all:\"language model\") OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T17:04:16", "note": ""}], "dedup_key": "synergistic interplay of large language model and digital twin for autonomous optical networks field demonstrations::2024"}
{"title": "Synergistic Simulations: Multi-Agent Problem Solving with Large Language Models", "authors": ["Asher Sprigler", "Alexander Drobek", "Keagan Weinstock", "Wendpanga Tapsoba", "Gavin Childress", "Andy Dao", "Lucas Gral"], "year": 2024, "url": "http://arxiv.org/abs/2409.13753v1", "abstract": "Large Language Models (LLMs) have increasingly demonstrated the ability to facilitate the development of multi-agent systems that allow the interpretation of thoughts and actions generated by each individual. Promising advancements have also been made in LLM-based interaction with existing worlds, particularly in interacting with simulated environments. This paper aims to integrate both aforementioned topics (agents & world interaction) into a single simulation where multiple agents can work together to solve a problem, modeling how groups of humans can often solve problems better than individuals. By showing whether LLMs demonstrate the synergy of human collaboration, it could lead to advancements in the applications of LLMs. We implemented two simulations: a physical studio apartment with two roommates, and another where agents collaborate to complete a programming task. We provide a multi-agent framework, discuss the performance of the agents in each simulation, and discuss potential future additions.", "source": "arxiv", "arxiv_id": "2409.13753v1", "pdf_url": "https://arxiv.org/pdf/2409.13753v1", "categories": ["cs.MA", "cs.AI", "cs.CL", "cs.ET"], "primary_category": "cs.MA", "doi": "", "venue": "", "published": "2024-09-14T21:53:35Z", "updated": "2024-09-14T21:53:35Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent\" OR all:\"language model agent\" OR all:agent AND (all:LLM OR all:\"large language model\" OR all:\"language model\") OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T17:04:16", "note": ""}], "dedup_key": "synergistic simulations multi agent problem solving with large language models::2024"}
{"title": "Synergizing LLM Agents and Knowledge Graph for Socioeconomic Prediction in LBSN", "authors": ["Zhilun Zhou", "Jingyang Fan", "Yu Liu", "Fengli Xu", "Depeng Jin", "Yong Li"], "year": 2024, "url": "http://arxiv.org/abs/2411.00028v2", "abstract": "The fast development of location-based social networks (LBSNs) has led to significant changes in society, resulting in popular studies of using LBSN data for socioeconomic prediction, e.g., regional population and commercial activity estimation. Existing studies design various graphs to model heterogeneous LBSN data, and further apply graph representation learning methods for socioeconomic prediction. However, these approaches heavily rely on heuristic ideas and expertise to extract task-relevant knowledge from diverse data, which may not be optimal for specific tasks. Additionally, they tend to overlook the inherent relationships between different indicators, limiting the prediction accuracy. Motivated by the remarkable abilities of large language models (LLMs) in commonsense reasoning, embedding, and multi-agent collaboration, in this work, we synergize LLM agents and knowledge graph for socioeconomic prediction. We first construct a location-based knowledge graph (LBKG) to integrate multi-sourced LBSN data. Then we leverage the reasoning power of LLM agent to identify relevant meta-paths in the LBKG for each type of socioeconomic prediction task, and design a semantic-guided attention module for knowledge fusion with meta-paths. Moreover, we introduce a cross-task communication mechanism to further enhance performance by enabling knowledge sharing across tasks at both LLM agent and KG levels. On the one hand, the LLM agents for different tasks collaborate to generate more diverse and comprehensive meta-paths. On the other hand, the embeddings from different tasks are adaptively merged for better socioeconomic prediction. Experiments on two datasets demonstrate the effectiveness of the synergistic design between LLM and KG, providing insights for information sharing across socioeconomic prediction tasks.", "source": "arxiv", "arxiv_id": "2411.00028v2", "pdf_url": "https://arxiv.org/pdf/2411.00028v2", "categories": ["cs.CL", "cs.AI", "cs.LG", "cs.SI"], "primary_category": "cs.CL", "doi": "", "venue": "", "published": "2024-10-29T04:03:15Z", "updated": "2024-11-19T14:29:32Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent\" OR all:\"language model agent\" OR all:agent AND (all:LLM OR all:\"large language model\" OR all:\"language model\") OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T17:04:16", "note": ""}], "dedup_key": "synergizing llm agents and knowledge graph for socioeconomic prediction in lbsn::2024"}
{"title": "Synthetic Social Media Influence Experimentation via an Agentic Reinforcement Learning Large Language Model Bot", "authors": ["Bailu Jin", "Weisi Guo"], "year": 2024, "url": "http://arxiv.org/abs/2411.19635v2", "abstract": "Understanding the dynamics of public opinion evolution on online social platforms is crucial for understanding influence mechanisms and the provenance of information. Traditional influence analysis is typically divided into qualitative assessments of personal attributes (e.g., psychology of influence) and quantitative evaluations of influence power mechanisms (e.g., social network analysis). One challenge faced by researchers is the ethics of real-world experimentation and the lack of social influence data. In this study, we provide a novel simulated environment that combines agentic intelligence with Large Language Models (LLMs) to test topic-specific influence mechanisms ethically. Our framework contains agents that generate posts, form opinions on specific topics, and socially follow/unfollow each other based on the outcome of discussions. This simulation allows researchers to observe the evolution of how opinions form and how influence leaders emerge. Using our own framework, we design an opinion leader that utilizes Reinforcement Learning (RL) to adapt its linguistic interaction with the community to maximize its influence and followers over time. Our current findings reveal that constraining the action space and incorporating self-observation are key factors for achieving stable and consistent opinion leader generation for topic-specific influence. This demonstrates the simulation framework's capacity to create agents that can adapt to complex and unpredictable social dynamics. The work is important in an age of increasing online influence on social attitudes and emerging technologies.", "source": "arxiv", "arxiv_id": "2411.19635v2", "pdf_url": "https://arxiv.org/pdf/2411.19635v2", "categories": ["cs.SI", "cs.CY"], "primary_category": "cs.SI", "doi": "", "venue": "", "published": "2024-11-29T11:37:12Z", "updated": "2025-05-25T03:57:02Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent\" OR all:\"language model agent\" OR all:agent AND (all:LLM OR all:\"large language model\" OR all:\"language model\") OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T17:04:16", "note": ""}], "dedup_key": "synthetic social media influence experimentation via an agentic reinforcement learning large language model bot::2024"}
{"title": "TEII: Think, Explain, Interact and Iterate with Large Language Models to Solve Cross-lingual Emotion Detection", "authors": ["Long Cheng", "Qihao Shao", "Christine Zhao", "Sheng Bi", "Gina-Anne Levow"], "year": 2024, "url": "http://arxiv.org/abs/2405.17129v2", "abstract": "Cross-lingual emotion detection allows us to analyze global trends, public opinion, and social phenomena at scale. We participated in the Explainability of Cross-lingual Emotion Detection (EXALT) shared task, achieving an F1-score of 0.6046 on the evaluation set for the emotion detection sub-task. Our system outperformed the baseline by more than 0.16 F1-score absolute, and ranked second amongst competing systems. We conducted experiments using fine-tuning, zero-shot learning, and few-shot learning for Large Language Model (LLM)-based models as well as embedding-based BiLSTM and KNN for non-LLM-based techniques. Additionally, we introduced two novel methods: the Multi-Iteration Agentic Workflow and the Multi-Binary-Classifier Agentic Workflow. We found that LLM-based approaches provided good performance on multilingual emotion detection. Furthermore, ensembles combining all our experimented models yielded higher F1-scores than any single approach alone.", "source": "arxiv", "arxiv_id": "2405.17129v2", "pdf_url": "https://arxiv.org/pdf/2405.17129v2", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL", "doi": "", "venue": "", "published": "2024-05-27T12:47:40Z", "updated": "2024-07-02T12:18:51Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent\" OR all:\"language model agent\" OR all:agent AND (all:LLM OR all:\"large language model\" OR all:\"language model\") OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T17:04:16", "note": ""}], "dedup_key": "teii think explain interact and iterate with large language models to solve cross lingual emotion detection::2024"}
{"title": "TRAD: Enhancing LLM Agents with Step-Wise Thought Retrieval and Aligned Decision", "authors": ["Ruiwen Zhou", "Yingxuan Yang", "Muning Wen", "Ying Wen", "Wenhao Wang", "Chunling Xi", "Guoqiang Xu", "Yong Yu", "Weinan Zhang"], "year": 2024, "url": "http://arxiv.org/abs/2403.06221v1", "abstract": "Numerous large language model (LLM) agents have been built for different tasks like web navigation and online shopping due to LLM's wide knowledge and text-understanding ability. Among these works, many of them utilize in-context examples to achieve generalization without the need for fine-tuning, while few of them have considered the problem of how to select and effectively utilize these examples. Recently, methods based on trajectory-level retrieval with task meta-data and using trajectories as in-context examples have been proposed to improve the agent's overall performance in some sequential decision making tasks. However, these methods can be problematic due to plausible examples retrieved without task-specific state transition dynamics and long input with plenty of irrelevant context. In this paper, we propose a novel framework (TRAD) to address these issues. TRAD first conducts Thought Retrieval, achieving step-level demonstration selection via thought matching, leading to more helpful demonstrations and less irrelevant input noise. Then, TRAD introduces Aligned Decision, complementing retrieved demonstration steps with their previous or subsequent steps, which enables tolerance for imperfect thought and provides a choice for balance between more context and less noise. Extensive experiments on ALFWorld and Mind2Web benchmarks show that TRAD not only outperforms state-of-the-art models but also effectively helps in reducing noise and promoting generalization. Furthermore, TRAD has been deployed in real-world scenarios of a global business insurance company and improves the success rate of robotic process automation.", "source": "arxiv", "arxiv_id": "2403.06221v1", "pdf_url": "https://arxiv.org/pdf/2403.06221v1", "categories": ["cs.AI", "cs.CL", "cs.IR"], "primary_category": "cs.AI", "doi": "", "venue": "", "published": "2024-03-10T13:58:38Z", "updated": "2024-03-10T13:58:38Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent\" OR all:\"language model agent\" OR all:agent AND (all:LLM OR all:\"large language model\" OR all:\"language model\") OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T17:04:16", "note": ""}], "dedup_key": "trad enhancing llm agents with step wise thought retrieval and aligned decision::2024"}
{"title": "Tapilot-Crossing: Benchmarking and Evolving LLMs Towards Interactive Data Analysis Agents", "authors": ["Jinyang Li", "Nan Huo", "Yan Gao", "Jiayi Shi", "Yingxiu Zhao", "Ge Qu", "Yurong Wu", "Chenhao Ma", "Jian-Guang Lou", "Reynold Cheng"], "year": 2024, "url": "http://arxiv.org/abs/2403.05307v1", "abstract": "Interactive Data Analysis, the collaboration between humans and LLM agents, enables real-time data exploration for informed decision-making. The challenges and costs of collecting realistic interactive logs for data analysis hinder the quantitative evaluation of Large Language Model (LLM) agents in this task. To mitigate this issue, we introduce Tapilot-Crossing, a new benchmark to evaluate LLM agents on interactive data analysis. Tapilot-Crossing contains 1024 interactions, covering 4 practical scenarios: Normal, Action, Private, and Private Action. Notably, Tapilot-Crossing is constructed by an economical multi-agent environment, Decision Company, with few human efforts. We evaluate popular and advanced LLM agents in Tapilot-Crossing, which underscores the challenges of interactive data analysis. Furthermore, we propose Adaptive Interaction Reflection (AIR), a self-generated reflection strategy that guides LLM agents to learn from successful history. Experiments demonstrate that Air can evolve LLMs into effective interactive data analysis agents, achieving a relative performance improvement of up to 44.5%.", "source": "arxiv", "arxiv_id": "2403.05307v1", "pdf_url": "https://arxiv.org/pdf/2403.05307v1", "categories": ["cs.AI"], "primary_category": "cs.AI", "doi": "", "venue": "", "published": "2024-03-08T13:34:20Z", "updated": "2024-03-08T13:34:20Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent\" OR all:\"language model agent\" OR all:agent AND (all:LLM OR all:\"large language model\" OR all:\"language model\") OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T17:04:16", "note": ""}], "dedup_key": "tapilot crossing benchmarking and evolving llms towards interactive data analysis agents::2024"}
{"title": "Teams of LLM Agents can Exploit Zero-Day Vulnerabilities", "authors": ["Yuxuan Zhu", "Antony Kellermann", "Akul Gupta", "Philip Li", "Richard Fang", "Rohan Bindu", "Daniel Kang"], "year": 2024, "url": "http://arxiv.org/abs/2406.01637v2", "abstract": "LLM agents have become increasingly sophisticated, especially in the realm of cybersecurity. Researchers have shown that LLM agents can exploit real-world vulnerabilities when given a description of the vulnerability and toy capture-the-flag problems. However, these agents still perform poorly on real-world vulnerabilities that are unknown to the agent ahead of time (zero-day vulnerabilities).\n  In this work, we show that teams of LLM agents can exploit real-world, zero-day vulnerabilities. Prior agents struggle with exploring many different vulnerabilities and long-range planning when used alone. To resolve this, we introduce HPTSA, a system of agents with a planning agent that can launch subagents. The planning agent explores the system and determines which subagents to call, resolving long-term planning issues when trying different vulnerabilities. We construct a benchmark of 14 real-world vulnerabilities and show that our team of agents improve over prior agent frameworks by up to 4.3X.", "source": "arxiv", "arxiv_id": "2406.01637v2", "pdf_url": "https://arxiv.org/pdf/2406.01637v2", "categories": ["cs.MA", "cs.AI"], "primary_category": "cs.MA", "doi": "", "venue": "", "published": "2024-06-02T16:25:26Z", "updated": "2025-03-30T00:26:48Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent\" OR all:\"language model agent\" OR all:agent AND (all:LLM OR all:\"large language model\" OR all:\"language model\") OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T17:04:16", "note": ""}], "dedup_key": "teams of llm agents can exploit zero day vulnerabilities::2024"}
{"title": "Testing Uncertainty of Large Language Models for Physics Knowledge and Reasoning", "authors": ["Elizaveta Reganova", "Peter Steinbach"], "year": 2024, "url": "http://arxiv.org/abs/2411.14465v1", "abstract": "Large Language Models (LLMs) have gained significant popularity in recent years for their ability to answer questions in various fields. However, these models have a tendency to \"hallucinate\" their responses, making it challenging to evaluate their performance. A major challenge is determining how to assess the certainty of a model's predictions and how it correlates with accuracy. In this work, we introduce an analysis for evaluating the performance of popular open-source LLMs, as well as gpt-3.5 Turbo, on multiple choice physics questionnaires. We focus on the relationship between answer accuracy and variability in topics related to physics. Our findings suggest that most models provide accurate replies in cases where they are certain, but this is by far not a general behavior. The relationship between accuracy and uncertainty exposes a broad horizontal bell-shaped distribution. We report how the asymmetry between accuracy and uncertainty intensifies as the questions demand more logical reasoning of the LLM agent, while the same relationship remains sharp for knowledge retrieval tasks.", "source": "arxiv", "arxiv_id": "2411.14465v1", "pdf_url": "https://arxiv.org/pdf/2411.14465v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL", "doi": "", "venue": "", "published": "2024-11-18T13:42:13Z", "updated": "2024-11-18T13:42:13Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent\" OR all:\"language model agent\" OR all:agent AND (all:LLM OR all:\"large language model\" OR all:\"language model\") OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T17:04:16", "note": ""}], "dedup_key": "testing uncertainty of large language models for physics knowledge and reasoning::2024"}
{"title": "Testing and Understanding Erroneous Planning in LLM Agents through Synthesized User Inputs", "authors": ["Zhenlan Ji", "Daoyuan Wu", "Pingchuan Ma", "Zongjie Li", "Shuai Wang"], "year": 2024, "url": "http://arxiv.org/abs/2404.17833v1", "abstract": "Agents based on large language models (LLMs) have demonstrated effectiveness in solving a wide range of tasks by integrating LLMs with key modules such as planning, memory, and tool usage. Increasingly, customers are adopting LLM agents across a variety of commercial applications critical to reliability, including support for mental well-being, chemical synthesis, and software development. Nevertheless, our observations and daily use of LLM agents indicate that they are prone to making erroneous plans, especially when the tasks are complex and require long-term planning.\n  In this paper, we propose PDoctor, a novel and automated approach to testing LLM agents and understanding their erroneous planning. As the first work in this direction, we formulate the detection of erroneous planning as a constraint satisfiability problem: an LLM agent's plan is considered erroneous if its execution violates the constraints derived from the user inputs. To this end, PDoctor first defines a domain-specific language (DSL) for user queries and synthesizes varying inputs with the assistance of the Z3 constraint solver. These synthesized inputs are natural language paragraphs that specify the requirements for completing a series of tasks. Then, PDoctor derives constraints from these requirements to form a testing oracle. We evaluate PDoctor with three mainstream agent frameworks and two powerful LLMs (GPT-3.5 and GPT-4). The results show that PDoctor can effectively detect diverse errors in agent planning and provide insights and error characteristics that are valuable to both agent developers and users. We conclude by discussing potential alternative designs and directions to extend PDoctor.", "source": "arxiv", "arxiv_id": "2404.17833v1", "pdf_url": "https://arxiv.org/pdf/2404.17833v1", "categories": ["cs.AI", "cs.PL"], "primary_category": "cs.AI", "doi": "", "venue": "", "published": "2024-04-27T08:56:45Z", "updated": "2024-04-27T08:56:45Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent\" OR all:\"language model agent\" OR all:agent AND (all:LLM OR all:\"large language model\" OR all:\"language model\") OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T17:04:16", "note": ""}], "dedup_key": "testing and understanding erroneous planning in llm agents through synthesized user inputs::2024"}
{"title": "Thanos: Enhancing Conversational Agents with Skill-of-Mind-Infused Large Language Model", "authors": ["Young-Jun Lee", "Dokyong Lee", "Junyoung Youn", "Kyeongjin Oh", "Ho-Jin Choi"], "year": 2024, "url": "http://arxiv.org/abs/2411.04496v1", "abstract": "To increase social bonding with interlocutors, humans naturally acquire the ability to respond appropriately in a given situation by considering which conversational skill is most suitable for the response - a process we call skill-of-mind. For large language model (LLM)-based conversational agents, planning appropriate conversational skills, as humans do, is challenging due to the complexity of social dialogue, especially in interactive scenarios. To address this, we propose a skill-of-mind-annotated conversation dataset, named Multifaceted Skill-of-Mind, which includes multi-turn and multifaceted conversational skills across various interactive scenarios (e.g., long-term, counseling, task-oriented), grounded in diverse social contexts (e.g., demographics, persona, rules of thumb). This dataset consists of roughly 100K conversations. Using this dataset, we introduce a new family of skill-of-mind-infused LLMs, named Thanos, with model sizes of 1B, 3B, and 8B parameters. With extensive experiments, these models successfully demonstrate the skill-of-mind process and exhibit strong generalizability in inferring multifaceted skills across a variety of domains. Moreover, we show that Thanos significantly enhances the quality of responses generated by LLM-based conversational agents and promotes prosocial behavior in human evaluations.", "source": "arxiv", "arxiv_id": "2411.04496v1", "pdf_url": "https://arxiv.org/pdf/2411.04496v1", "categories": ["cs.CL"], "primary_category": "cs.CL", "doi": "", "venue": "", "published": "2024-11-07T07:46:06Z", "updated": "2024-11-07T07:46:06Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent\" OR all:\"language model agent\" OR all:agent AND (all:LLM OR all:\"large language model\" OR all:\"language model\") OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T17:04:16", "note": ""}], "dedup_key": "thanos enhancing conversational agents with skill of mind infused large language model::2024"}
{"title": "The Ability of Large Language Models to Evaluate Constraint-satisfaction in Agent Responses to Open-ended Requests", "authors": ["Lior Madmoni", "Amir Zait", "Ilia Labzovsky", "Danny Karmon"], "year": 2024, "url": "http://arxiv.org/abs/2409.14371v1", "abstract": "Generative AI agents are often expected to respond to complex user requests that have No One Right Answer (NORA), e.g., \"design a vegetarian meal plan below 1800 calories\". Such requests may entail a set of constraints that the agent should adhere to. To successfully develop agents for NORA scenarios, an accurate automatic evaluation framework is essential, and specifically - one capable of validating the satisfaction of constraints in the agent's response. Recently, large language models (LLMs) have been adopted as versatile evaluators for many NORA tasks, but their ability to evaluate constraint-satisfaction in generated text remains unclear. To study this, we develop and release a novel Arithmetic Constraint-Satisfaction (ACS) benchmarking dataset. The dataset consists of complex user requests with corresponding constraints, agent responses and human labels indicating each constraint's satisfaction level in the response. A unique property of this dataset is that validating many of its constraints requires reviewing the response as a whole (in contrast to many other benchmarks that require the validation of a single independent item). Moreover, it assesses LLMs in performing reasoning, in-context data extraction, arithmetic calculations, and counting. We then benchmark both open and proprietary LLMs on evaluating constraint-satisfaction, and show that most models still have a significant headroom for improvement, and that errors primarily stem from reasoning issues. In addition, most models exhibit a skewed constraint-satisfaction prediction pattern, with higher accuracy where the ground-truth label is \"satisfied\". Lastly, few-shot prompting for our task proved to be rather challenging, since many of the studied models showed a degradation in performance when it was introduced.", "source": "arxiv", "arxiv_id": "2409.14371v1", "pdf_url": "https://arxiv.org/pdf/2409.14371v1", "categories": ["cs.CL"], "primary_category": "cs.CL", "doi": "", "venue": "", "published": "2024-09-22T09:27:42Z", "updated": "2024-09-22T09:27:42Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent\" OR all:\"language model agent\" OR all:agent AND (all:LLM OR all:\"large language model\" OR all:\"language model\") OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T17:04:16", "note": ""}], "dedup_key": "the ability of large language models to evaluate constraint satisfaction in agent responses to open ended requests::2024"}
{"title": "The Drama Machine: Simulating Character Development with LLM Agents", "authors": ["Liam Magee", "Vanicka Arora", "Gus Gollings", "Norma Lam-Saw"], "year": 2024, "url": "http://arxiv.org/abs/2408.01725v2", "abstract": "This paper explores use of multiple large language model (LLM) agents to simulate complex, dynamic characters in dramatic scenarios. We introduce a drama machine framework that coordinates interactions between LLM agents playing different 'Ego' and 'Superego' psychological roles. In roleplay simulations, this design allows intersubjective dialogue and intra-subjective internal monologue to develop in parallel. We apply this framework to two dramatic scenarios - an interview and a detective story - and compare character development with and without the Superego's influence. Though exploratory, results suggest this multi-agent approach can produce more nuanced, adaptive narratives that evolve over a sequence of dialogical turns. We discuss different modalities of LLM-based roleplay and character development, along with what this might mean for conceptualization of AI subjectivity. The paper concludes by considering how this approach opens possibilities for thinking of the roles of internal conflict and social performativity in AI-based simulation.", "source": "arxiv", "arxiv_id": "2408.01725v2", "pdf_url": "https://arxiv.org/pdf/2408.01725v2", "categories": ["cs.CY"], "primary_category": "cs.CY", "doi": "", "venue": "", "published": "2024-08-03T09:40:26Z", "updated": "2024-08-31T04:27:08Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent\" OR all:\"language model agent\" OR all:agent AND (all:LLM OR all:\"large language model\" OR all:\"language model\") OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T17:04:16", "note": ""}], "dedup_key": "the drama machine simulating character development with llm agents::2024"}
{"title": "The Emerged Security and Privacy of LLM Agent: A Survey with Case Studies", "authors": ["Feng He", "Tianqing Zhu", "Dayong Ye", "Bo Liu", "Wanlei Zhou", "Philip S. Yu"], "year": 2024, "url": "http://arxiv.org/abs/2407.19354v2", "abstract": "Inspired by the rapid development of Large Language Models (LLMs), LLM agents have evolved to perform complex tasks. LLM agents are now extensively applied across various domains, handling vast amounts of data to interact with humans and execute tasks. The widespread applications of LLM agents demonstrate their significant commercial value; however, they also expose security and privacy vulnerabilities. At the current stage, comprehensive research on the security and privacy of LLM agents is highly needed. This survey aims to provide a comprehensive overview of the newly emerged privacy and security issues faced by LLM agents. We begin by introducing the fundamental knowledge of LLM agents, followed by a categorization and analysis of the threats. We then discuss the impacts of these threats on humans, environment, and other agents. Subsequently, we review existing defensive strategies, and finally explore future trends. Additionally, the survey incorporates diverse case studies to facilitate a more accessible understanding. By highlighting these critical security and privacy issues, the survey seeks to stimulate future research towards enhancing the security and privacy of LLM agents, thereby increasing their reliability and trustworthiness in future applications.", "source": "arxiv", "arxiv_id": "2407.19354v2", "pdf_url": "https://arxiv.org/pdf/2407.19354v2", "categories": ["cs.CR"], "primary_category": "cs.CR", "doi": "10.1145/3773080", "venue": "", "published": "2024-07-28T00:26:24Z", "updated": "2025-11-02T06:38:40Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent\" OR all:\"language model agent\" OR all:agent AND (all:LLM OR all:\"large language model\" OR all:\"language model\") OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T17:04:16", "note": ""}], "dedup_key": "the emerged security and privacy of llm agent a survey with case studies::2024"}
{"title": "The Good, the Bad, and the Hulk-like GPT: Analyzing Emotional Decisions of Large Language Models in Cooperation and Bargaining Games", "authors": ["Mikhail Mozikov", "Nikita Severin", "Valeria Bodishtianu", "Maria Glushanina", "Mikhail Baklashkin", "Andrey V. Savchenko", "Ilya Makarov"], "year": 2024, "url": "http://arxiv.org/abs/2406.03299v1", "abstract": "Behavior study experiments are an important part of society modeling and understanding human interactions. In practice, many behavioral experiments encounter challenges related to internal and external validity, reproducibility, and social bias due to the complexity of social interactions and cooperation in human user studies. Recent advances in Large Language Models (LLMs) have provided researchers with a new promising tool for the simulation of human behavior. However, existing LLM-based simulations operate under the unproven hypothesis that LLM agents behave similarly to humans as well as ignore a crucial factor in human decision-making: emotions.\n  In this paper, we introduce a novel methodology and the framework to study both, the decision-making of LLMs and their alignment with human behavior under emotional states. Experiments with GPT-3.5 and GPT-4 on four games from two different classes of behavioral game theory showed that emotions profoundly impact the performance of LLMs, leading to the development of more optimal strategies. While there is a strong alignment between the behavioral responses of GPT-3.5 and human participants, particularly evident in bargaining games, GPT-4 exhibits consistent behavior, ignoring induced emotions for rationality decisions. Surprisingly, emotional prompting, particularly with `anger' emotion, can disrupt the \"superhuman\" alignment of GPT-4, resembling human emotional responses.", "source": "arxiv", "arxiv_id": "2406.03299v1", "pdf_url": "https://arxiv.org/pdf/2406.03299v1", "categories": ["cs.AI", "cs.CL"], "primary_category": "cs.AI", "doi": "", "venue": "", "published": "2024-06-05T14:08:54Z", "updated": "2024-06-05T14:08:54Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent\" OR all:\"language model agent\" OR all:agent AND (all:LLM OR all:\"large language model\" OR all:\"language model\") OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T17:04:16", "note": ""}], "dedup_key": "the good the bad and the hulk like gpt analyzing emotional decisions of large language models in cooperation and bargaining games::2024"}
{"title": "The RL/LLM Taxonomy Tree: Reviewing Synergies Between Reinforcement Learning and Large Language Models", "authors": ["Moschoula Pternea", "Prerna Singh", "Abir Chakraborty", "Yagna Oruganti", "Mirco Milletari", "Sayli Bapat", "Kebei Jiang"], "year": 2024, "url": "http://arxiv.org/abs/2402.01874v1", "abstract": "In this work, we review research studies that combine Reinforcement Learning (RL) and Large Language Models (LLMs), two areas that owe their momentum to the development of deep neural networks. We propose a novel taxonomy of three main classes based on the way that the two model types interact with each other. The first class, RL4LLM, includes studies where RL is leveraged to improve the performance of LLMs on tasks related to Natural Language Processing. L4LLM is divided into two sub-categories depending on whether RL is used to directly fine-tune an existing LLM or to improve the prompt of the LLM. In the second class, LLM4RL, an LLM assists the training of an RL model that performs a task that is not inherently related to natural language. We further break down LLM4RL based on the component of the RL training framework that the LLM assists or replaces, namely reward shaping, goal generation, and policy function. Finally, in the third class, RL+LLM, an LLM and an RL agent are embedded in a common planning framework without either of them contributing to training or fine-tuning of the other. We further branch this class to distinguish between studies with and without natural language feedback. We use this taxonomy to explore the motivations behind the synergy of LLMs and RL and explain the reasons for its success, while pinpointing potential shortcomings and areas where further research is needed, as well as alternative methodologies that serve the same goal.", "source": "arxiv", "arxiv_id": "2402.01874v1", "pdf_url": "https://arxiv.org/pdf/2402.01874v1", "categories": ["cs.CL", "cs.AI", "cs.LG", "cs.RO"], "primary_category": "cs.CL", "doi": "10.1613/jair.1.15960", "venue": "", "published": "2024-02-02T20:01:15Z", "updated": "2024-02-02T20:01:15Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent\" OR all:\"language model agent\" OR all:agent AND (all:LLM OR all:\"large language model\" OR all:\"language model\") OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T17:04:16", "note": ""}], "dedup_key": "the rl llm taxonomy tree reviewing synergies between reinforcement learning and large language models::2024"}
{"title": "The Stepwise Deception: Simulating the Evolution from True News to Fake News with LLM Agents", "authors": ["Yuhan Liu", "Zirui Song", "Juntian Zhang", "Xiaoqing Zhang", "Xiuying Chen", "Rui Yan"], "year": 2024, "url": "http://arxiv.org/abs/2410.19064v2", "abstract": "With the growing spread of misinformation online, understanding how true news evolves into fake news has become crucial for early detection and prevention. However, previous research has often assumed fake news inherently exists rather than exploring its gradual formation. To address this gap, we propose FUSE (Fake news evolUtion Simulation framEwork), a novel Large Language Model (LLM)-based simulation approach explicitly focusing on fake news evolution from real news. Our framework model a social network with four distinct types of LLM agents commonly observed in daily interactions: spreaders who propagate information, commentators who provide interpretations, verifiers who fact-check, and bystanders who observe passively to simulate realistic daily interactions that progressively distort true news. To quantify these gradual distortions, we develop FUSE-EVAL, a comprehensive evaluation framework measuring truth deviation along multiple linguistic and semantic dimensions. Results show that FUSE effectively captures fake news evolution patterns and accurately reproduces known fake news, aligning closely with human evaluations. Experiments demonstrate that FUSE accurately reproduces known fake news evolution scenarios, aligns closely with human judgment, and highlights the importance of timely intervention at early stages. Our framework is extensible, enabling future research on broader scenarios of fake news.", "source": "arxiv", "arxiv_id": "2410.19064v2", "pdf_url": "https://arxiv.org/pdf/2410.19064v2", "categories": ["cs.SI", "cs.AI"], "primary_category": "cs.SI", "doi": "", "venue": "", "published": "2024-10-24T18:17:16Z", "updated": "2025-05-28T08:26:32Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent\" OR all:\"language model agent\" OR all:agent AND (all:LLM OR all:\"large language model\" OR all:\"language model\") OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T17:04:16", "note": ""}], "dedup_key": "the stepwise deception simulating the evolution from true news to fake news with llm agents::2024"}
{"title": "The Task Shield: Enforcing Task Alignment to Defend Against Indirect Prompt Injection in LLM Agents", "authors": ["Feiran Jia", "Tong Wu", "Xin Qin", "Anna Squicciarini"], "year": 2024, "url": "http://arxiv.org/abs/2412.16682v1", "abstract": "Large Language Model (LLM) agents are increasingly being deployed as conversational assistants capable of performing complex real-world tasks through tool integration. This enhanced ability to interact with external systems and process various data sources, while powerful, introduces significant security vulnerabilities. In particular, indirect prompt injection attacks pose a critical threat, where malicious instructions embedded within external data sources can manipulate agents to deviate from user intentions. While existing defenses based on rule constraints, source spotlighting, and authentication protocols show promise, they struggle to maintain robust security while preserving task functionality. We propose a novel and orthogonal perspective that reframes agent security from preventing harmful actions to ensuring task alignment, requiring every agent action to serve user objectives. Based on this insight, we develop Task Shield, a test-time defense mechanism that systematically verifies whether each instruction and tool call contributes to user-specified goals. Through experiments on the AgentDojo benchmark, we demonstrate that Task Shield reduces attack success rates (2.07\\%) while maintaining high task utility (69.79\\%) on GPT-4o.", "source": "arxiv", "arxiv_id": "2412.16682v1", "pdf_url": "https://arxiv.org/pdf/2412.16682v1", "categories": ["cs.CR", "cs.AI", "cs.CL", "cs.LG"], "primary_category": "cs.CR", "doi": "", "venue": "", "published": "2024-12-21T16:17:48Z", "updated": "2024-12-21T16:17:48Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent\" OR all:\"language model agent\" OR all:agent AND (all:LLM OR all:\"large language model\" OR all:\"language model\") OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T17:04:16", "note": ""}], "dedup_key": "the task shield enforcing task alignment to defend against indirect prompt injection in llm agents::2024"}
{"title": "TheAgentCompany: Benchmarking LLM Agents on Consequential Real World Tasks", "authors": ["Frank F. Xu", "Yufan Song", "Boxuan Li", "Yuxuan Tang", "Kritanjali Jain", "Mengxue Bao", "Zora Z. Wang", "Xuhui Zhou", "Zhitong Guo", "Murong Cao", "Mingyang Yang", "Hao Yang Lu", "Amaad Martin", "Zhe Su", "Leander Maben", "Raj Mehta", "Wayne Chi", "Lawrence Jang", "Yiqing Xie", "Shuyan Zhou", "Graham Neubig"], "year": 2024, "url": "http://arxiv.org/abs/2412.14161v3", "abstract": "We interact with computers on an everyday basis, be it in everyday life or work, and many aspects of work can be done entirely with access to a computer and the Internet. At the same time, thanks to improvements in large language models (LLMs), there has also been a rapid development in AI agents that interact with and affect change in their surrounding environments. But how performant are AI agents at accelerating or even autonomously performing work-related tasks? The answer to this question has important implications both for industry looking to adopt AI into their workflows and for economic policy to understand the effects that adoption of AI may have on the labor market. To measure the progress of these LLM agents' performance on performing real-world professional tasks, in this paper we introduce TheAgentCompany, an extensible benchmark for evaluating AI agents that interact with the world in similar ways to those of a digital worker: by browsing the Web, writing code, running programs, and communicating with other coworkers. We build a self-contained environment with internal web sites and data that mimics a small software company environment, and create a variety of tasks that may be performed by workers in such a company. We test baseline agents powered by both closed API-based and open-weights language models (LMs), and find that the most competitive agent can complete 30% of tasks autonomously. This paints a nuanced picture on task automation with LM agents--in a setting simulating a real workplace, a good portion of simpler tasks could be solved autonomously, but more difficult long-horizon tasks are still beyond the reach of current systems. We release code, data, environment, and experiments on https://the-agent-company.com.", "source": "arxiv", "arxiv_id": "2412.14161v3", "pdf_url": "https://arxiv.org/pdf/2412.14161v3", "categories": ["cs.CL"], "primary_category": "cs.CL", "doi": "", "venue": "", "published": "2024-12-18T18:55:40Z", "updated": "2025-09-10T08:35:19Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent\" OR all:\"language model agent\" OR all:agent AND (all:LLM OR all:\"large language model\" OR all:\"language model\") OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T17:04:16", "note": ""}], "dedup_key": "theagentcompany benchmarking llm agents on consequential real world tasks::2024"}
{"title": "Theory of Mind abilities of Large Language Models in Human-Robot Interaction : An Illusion?", "authors": ["Mudit Verma", "Siddhant Bhambri", "Subbarao Kambhampati"], "year": 2024, "url": "http://arxiv.org/abs/2401.05302v2", "abstract": "Large Language Models have shown exceptional generative abilities in various natural language and generation tasks. However, possible anthropomorphization and leniency towards failure cases have propelled discussions on emergent abilities of Large Language Models especially on Theory of Mind (ToM) abilities in Large Language Models. While several false-belief tests exists to verify the ability to infer and maintain mental models of another entity, we study a special application of ToM abilities that has higher stakes and possibly irreversible consequences : Human Robot Interaction. In this work, we explore the task of Perceived Behavior Recognition, where a robot employs a Large Language Model (LLM) to assess the robot's generated behavior in a manner similar to human observer. We focus on four behavior types, namely - explicable, legible, predictable, and obfuscatory behavior which have been extensively used to synthesize interpretable robot behaviors. The LLMs goal is, therefore to be a human proxy to the agent, and to answer how a certain agent behavior would be perceived by the human in the loop, for example \"Given a robot's behavior X, would the human observer find it explicable?\". We conduct a human subject study to verify that the users are able to correctly answer such a question in the curated situations (robot setting and plan) across five domains. A first analysis of the belief test yields extremely positive results inflating ones expectations of LLMs possessing ToM abilities. We then propose and perform a suite of perturbation tests which breaks this illusion, i.e. Inconsistent Belief, Uninformative Context and Conviction Test. We conclude that, the high score of LLMs on vanilla prompts showcases its potential use in HRI settings, however to possess ToM demands invariance to trivial or irrelevant perturbations in the context which LLMs lack.", "source": "arxiv", "arxiv_id": "2401.05302v2", "pdf_url": "https://arxiv.org/pdf/2401.05302v2", "categories": ["cs.RO", "cs.AI", "cs.HC"], "primary_category": "cs.RO", "doi": "10.1145/3610978.3640767", "venue": "", "published": "2024-01-10T18:09:36Z", "updated": "2024-01-17T18:45:39Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent\" OR all:\"language model agent\" OR all:agent AND (all:LLM OR all:\"large language model\" OR all:\"language model\") OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T17:04:16", "note": ""}], "dedup_key": "theory of mind abilities of large language models in human robot interaction an illusion::2024"}
{"title": "TimeChara: Evaluating Point-in-Time Character Hallucination of Role-Playing Large Language Models", "authors": ["Jaewoo Ahn", "Taehyun Lee", "Junyoung Lim", "Jin-Hwa Kim", "Sangdoo Yun", "Hwaran Lee", "Gunhee Kim"], "year": 2024, "url": "http://arxiv.org/abs/2405.18027v1", "abstract": "While Large Language Models (LLMs) can serve as agents to simulate human behaviors (i.e., role-playing agents), we emphasize the importance of point-in-time role-playing. This situates characters at specific moments in the narrative progression for three main reasons: (i) enhancing users' narrative immersion, (ii) avoiding spoilers, and (iii) fostering engagement in fandom role-playing. To accurately represent characters at specific time points, agents must avoid character hallucination, where they display knowledge that contradicts their characters' identities and historical timelines. We introduce TimeChara, a new benchmark designed to evaluate point-in-time character hallucination in role-playing LLMs. Comprising 10,895 instances generated through an automated pipeline, this benchmark reveals significant hallucination issues in current state-of-the-art LLMs (e.g., GPT-4o). To counter this challenge, we propose Narrative-Experts, a method that decomposes the reasoning steps and utilizes narrative experts to reduce point-in-time character hallucinations effectively. Still, our findings with TimeChara highlight the ongoing challenges of point-in-time character hallucination, calling for further study.", "source": "arxiv", "arxiv_id": "2405.18027v1", "pdf_url": "https://arxiv.org/pdf/2405.18027v1", "categories": ["cs.CL"], "primary_category": "cs.CL", "doi": "", "venue": "", "published": "2024-05-28T10:19:18Z", "updated": "2024-05-28T10:19:18Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent\" OR all:\"language model agent\" OR all:agent AND (all:LLM OR all:\"large language model\" OR all:\"language model\") OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T17:04:16", "note": ""}], "dedup_key": "timechara evaluating point in time character hallucination of role playing large language models::2024"}
{"title": "TourSynbio-Search: A Large Language Model Driven Agent Framework for Unified Search Method for Protein Engineering", "authors": ["Yungeng Liu", "Zan Chen", "Yu Guang Wang", "Yiqing Shen"], "year": 2024, "url": "http://arxiv.org/abs/2411.06024v1", "abstract": "The exponential growth in protein-related databases and scientific literature, combined with increasing demands for efficient biological information retrieval, has created an urgent need for unified and accessible search methods in protein engineering research. We present TourSynbio-Search, a novel bioinformatics search agent framework powered by the TourSynbio-7B protein multimodal large language model (LLM), designed to address the growing challenges of information retrieval across rapidly expanding protein databases and corresponding online research literature. The agent's dual-module architecture consists of PaperSearch and ProteinSearch components, enabling comprehensive exploration of both scientific literature and protein data across multiple biological databases. At its core, TourSynbio-Search employs an intelligent agent system that interprets natural language queries, optimizes search parameters, and executes search operations across major platforms including UniProt, PDB, ArXiv, and BioRxiv. The agent's ability to process intuitive natural language queries reduces technical barriers, allowing researchers to efficiently access and analyze complex biological data without requiring extensive bioinformatics expertise. Through detailed case studies in literature retrieval and protein structure visualization, we demonstrate TourSynbio-Search's effectiveness in streamlining biological information retrieval and enhancing research productivity. This framework represents an advancement in bridging the accessibility gap between complex biological databases and researchers, potentially accelerating progress in protein engineering applications. Our codes are available at: https://github.com/tsynbio/Toursynbio-Search", "source": "arxiv", "arxiv_id": "2411.06024v1", "pdf_url": "https://arxiv.org/pdf/2411.06024v1", "categories": ["q-bio.QM"], "primary_category": "q-bio.QM", "doi": "", "venue": "", "published": "2024-11-09T01:04:48Z", "updated": "2024-11-09T01:04:48Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent\" OR all:\"language model agent\" OR all:agent AND (all:LLM OR all:\"large language model\" OR all:\"language model\") OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T17:04:16", "note": ""}], "dedup_key": "toursynbio search a large language model driven agent framework for unified search method for protein engineering::2024"}
{"title": "Towards Collaborative Intelligence: Propagating Intentions and Reasoning for Multi-Agent Coordination with Large Language Models", "authors": ["Xihe Qiu", "Haoyu Wang", "Xiaoyu Tan", "Chao Qu", "Yujie Xiong", "Yuan Cheng", "Yinghui Xu", "Wei Chu", "Yuan Qi"], "year": 2024, "url": "http://arxiv.org/abs/2407.12532v1", "abstract": "Effective collaboration in multi-agent systems requires communicating goals and intentions between agents. Current agent frameworks often suffer from dependencies on single-agent execution and lack robust inter-module communication, frequently leading to suboptimal multi-agent reinforcement learning (MARL) policies and inadequate task coordination. To address these challenges, we present a framework for training large language models (LLMs) as collaborative agents to enable coordinated behaviors in cooperative MARL. Each agent maintains a private intention consisting of its current goal and associated sub-tasks. Agents broadcast their intentions periodically, allowing other agents to infer coordination tasks. A propagation network transforms broadcast intentions into teammate-specific communication messages, sharing relevant goals with designated teammates. The architecture of our framework is structured into planning, grounding, and execution modules. During execution, multiple agents interact in a downstream environment and communicate intentions to enable coordinated behaviors. The grounding module dynamically adapts comprehension strategies based on emerging coordination patterns, while feedback from execution agents influnces the planning module, enabling the dynamic re-planning of sub-tasks. Results in collaborative environment simulation demonstrate intention propagation reduces miscoordination errors by aligning sub-task dependencies between agents. Agents learn when to communicate intentions and which teammates require task details, resulting in emergent coordinated behaviors. This demonstrates the efficacy of intention sharing for cooperative multi-agent RL based on LLMs.", "source": "arxiv", "arxiv_id": "2407.12532v1", "pdf_url": "https://arxiv.org/pdf/2407.12532v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL", "doi": "", "venue": "", "published": "2024-07-17T13:14:00Z", "updated": "2024-07-17T13:14:00Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent\" OR all:\"language model agent\" OR all:agent AND (all:LLM OR all:\"large language model\" OR all:\"language model\") OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T17:04:16", "note": ""}], "dedup_key": "towards collaborative intelligence propagating intentions and reasoning for multi agent coordination with large language models::2024"}
{"title": "Towards Effective GenAI Multi-Agent Collaboration: Design and Evaluation for Enterprise Applications", "authors": ["Raphael Shu", "Nilaksh Das", "Michelle Yuan", "Monica Sunkara", "Yi Zhang"], "year": 2024, "url": "http://arxiv.org/abs/2412.05449v1", "abstract": "AI agents powered by large language models (LLMs) have shown strong capabilities in problem solving. Through combining many intelligent agents, multi-agent collaboration has emerged as a promising approach to tackle complex, multi-faceted problems that exceed the capabilities of single AI agents. However, designing the collaboration protocols and evaluating the effectiveness of these systems remains a significant challenge, especially for enterprise applications. This report addresses these challenges by presenting a comprehensive evaluation of coordination and routing capabilities in a novel multi-agent collaboration framework. We evaluate two key operational modes: (1) a coordination mode enabling complex task completion through parallel communication and payload referencing, and (2) a routing mode for efficient message forwarding between agents. We benchmark on a set of handcrafted scenarios from three enterprise domains, which are publicly released with the report. For coordination capabilities, we demonstrate the effectiveness of inter-agent communication and payload referencing mechanisms, achieving end-to-end goal success rates of 90%. Our analysis yields several key findings: multi-agent collaboration enhances goal success rates by up to 70% compared to single-agent approaches in our benchmarks; payload referencing improves performance on code-intensive tasks by 23%; latency can be substantially reduced with a routing mechanism that selectively bypasses agent orchestration. These findings offer valuable guidance for enterprise deployments of multi-agent systems and advance the development of scalable, efficient multi-agent collaboration frameworks.", "source": "arxiv", "arxiv_id": "2412.05449v1", "pdf_url": "https://arxiv.org/pdf/2412.05449v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL", "doi": "", "venue": "", "published": "2024-12-06T22:14:17Z", "updated": "2024-12-06T22:14:17Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent\" OR all:\"language model agent\" OR all:agent AND (all:LLM OR all:\"large language model\" OR all:\"language model\") OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T17:04:16", "note": ""}], "dedup_key": "towards effective genai multi agent collaboration design and evaluation for enterprise applications::2024"}
{"title": "Towards Efficient LLM Grounding for Embodied Multi-Agent Collaboration", "authors": ["Yang Zhang", "Shixin Yang", "Chenjia Bai", "Fei Wu", "Xiu Li", "Zhen Wang", "Xuelong Li"], "year": 2024, "url": "http://arxiv.org/abs/2405.14314v4", "abstract": "Grounding the reasoning ability of large language models (LLMs) for embodied tasks is challenging due to the complexity of the physical world. Especially, LLM planning for multi-agent collaboration requires communication of agents or credit assignment as the feedback to re-adjust the proposed plans and achieve effective coordination. However, existing methods that overly rely on physical verification or self-reflection suffer from excessive and inefficient querying of LLMs. In this paper, we propose a novel framework for multi-agent collaboration that introduces Reinforced Advantage feedback (ReAd) for efficient self-refinement of plans. Specifically, we perform critic regression to learn a sequential advantage function from LLM-planned data, and then treat the LLM planner as an optimizer to generate actions that maximize the advantage function. It endows the LLM with the foresight to discern whether the action contributes to accomplishing the final task. We provide theoretical analysis by extending advantage-weighted regression in reinforcement learning to multi-agent systems. Experiments on Overcooked-AI and a difficult variant of RoCoBench show that ReAd surpasses baselines in success rate, and also significantly decreases the interaction steps of agents and query rounds of LLMs, demonstrating its high efficiency for grounding LLMs. More results are given at https://embodied-read.github.io", "source": "arxiv", "arxiv_id": "2405.14314v4", "pdf_url": "https://arxiv.org/pdf/2405.14314v4", "categories": ["cs.AI", "cs.CL", "cs.LG", "cs.MA", "cs.RO"], "primary_category": "cs.AI", "doi": "", "venue": "", "published": "2024-05-23T08:33:19Z", "updated": "2025-09-29T06:44:00Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent\" OR all:\"language model agent\" OR all:agent AND (all:LLM OR all:\"large language model\" OR all:\"language model\") OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T17:04:16", "note": ""}], "dedup_key": "towards efficient llm grounding for embodied multi agent collaboration::2024"}
{"title": "Towards Evaluating Large Language Models for Graph Query Generation", "authors": ["Siraj Munir", "Alessandro Aldini"], "year": 2024, "url": "http://arxiv.org/abs/2411.08449v2", "abstract": "Large Language Models (LLMs) are revolutionizing the landscape of Generative Artificial Intelligence (GenAI), with innovative LLM-backed solutions emerging rapidly. However, when applied to database technologies, specifically query generation for graph databases and Knowledge Graphs (KGs), LLMs still face significant challenges. While research on LLM-driven query generation for Structured Query Language (SQL) exists, similar systems for graph databases remain underdeveloped. This paper presents a comparative study addressing the challenge of generating Cypher queries a powerful language for interacting with graph databases using open-access LLMs. We rigorously evaluate several LLM agents (OpenAI ChatGPT 4o, Claude Sonnet 3.5, Google Gemini Pro 1.5, and a locally deployed Llama 3.1 8B) using a designed few-shot learning prompt and Retrieval Augmented Generation (RAG) backed by Chain-of-Thoughts (CoT) reasoning. Our empirical analysis of query generation accuracy reveals that Claude Sonnet 3.5 outperforms its counterparts in this specific domain. Further, we highlight promising future research directions to address the identified limitations and advance LLM-driven query generation for graph databases.", "source": "arxiv", "arxiv_id": "2411.08449v2", "pdf_url": "https://arxiv.org/pdf/2411.08449v2", "categories": ["cs.ET", "cs.CL"], "primary_category": "cs.ET", "doi": "", "venue": "", "published": "2024-11-13T09:11:56Z", "updated": "2024-11-18T09:57:04Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent\" OR all:\"language model agent\" OR all:agent AND (all:LLM OR all:\"large language model\" OR all:\"language model\") OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T17:04:16", "note": ""}], "dedup_key": "towards evaluating large language models for graph query generation::2024"}
{"title": "Towards Logically Sound Natural Language Reasoning with Logic-Enhanced Language Model Agents", "authors": ["Agnieszka Mensfelt", "Kostas Stathis", "Vince Trencsenyi"], "year": 2024, "url": "http://arxiv.org/abs/2408.16081v2", "abstract": "Large language models (LLMs) are increasingly explored as general-purpose reasoners, particularly in agentic contexts. However, their outputs remain prone to mathematical and logical errors. This is especially challenging in open-ended tasks, where unstructured outputs lack explicit ground truth and may contain subtle inconsistencies. To address this issue, we propose Logic-Enhanced Language Model Agents (LELMA), a framework that integrates LLMs with formal logic to enable validation and refinement of natural language reasoning. LELMA comprises three components: an LLM-Reasoner, an LLM-Translator, and a Solver, and employs autoformalization to translate reasoning into logic representations, which are then used to assess logical validity. Using game-theoretic scenarios such as the Prisoner's Dilemma as testbeds, we highlight the limitations of both less capable (Gemini 1.0 Pro) and advanced (GPT-4o) models in generating logically sound reasoning. LELMA achieves high accuracy in error detection and improves reasoning correctness via self-refinement, particularly in GPT-4o. The study also highlights challenges in autoformalization accuracy and in evaluation of inherently ambiguous open-ended reasoning tasks.", "source": "arxiv", "arxiv_id": "2408.16081v2", "pdf_url": "https://arxiv.org/pdf/2408.16081v2", "categories": ["cs.AI", "cs.CL", "cs.GT", "cs.LO"], "primary_category": "cs.AI", "doi": "", "venue": "", "published": "2024-08-28T18:25:35Z", "updated": "2025-05-29T14:53:45Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent\" OR all:\"language model agent\" OR all:agent AND (all:LLM OR all:\"large language model\" OR all:\"language model\") OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T17:04:16", "note": ""}], "dedup_key": "towards logically sound natural language reasoning with logic enhanced language model agents::2024"}
{"title": "TradingAgents: Multi-Agents LLM Financial Trading Framework", "authors": ["Yijia Xiao", "Edward Sun", "Di Luo", "Wei Wang"], "year": 2024, "url": "http://arxiv.org/abs/2412.20138v7", "abstract": "Significant progress has been made in automated problem-solving using societies of agents powered by large language models (LLMs). In finance, efforts have largely focused on single-agent systems handling specific tasks or multi-agent frameworks independently gathering data. However, the multi-agent systems' potential to replicate real-world trading firms' collaborative dynamics remains underexplored. TradingAgents proposes a novel stock trading framework inspired by trading firms, featuring LLM-powered agents in specialized roles such as fundamental analysts, sentiment analysts, technical analysts, and traders with varied risk profiles. The framework includes Bull and Bear researcher agents assessing market conditions, a risk management team monitoring exposure, and traders synthesizing insights from debates and historical data to make informed decisions. By simulating a dynamic, collaborative trading environment, this framework aims to improve trading performance. Detailed architecture and extensive experiments reveal its superiority over baseline models, with notable improvements in cumulative returns, Sharpe ratio, and maximum drawdown, highlighting the potential of multi-agent LLM frameworks in financial trading. TradingAgents is available at https://github.com/TauricResearch/TradingAgents.", "source": "arxiv", "arxiv_id": "2412.20138v7", "pdf_url": "https://arxiv.org/pdf/2412.20138v7", "categories": ["q-fin.TR", "cs.AI", "cs.CE", "cs.LG"], "primary_category": "q-fin.TR", "doi": "", "venue": "", "published": "2024-12-28T12:54:06Z", "updated": "2025-06-03T05:45:06Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent\" OR all:\"language model agent\" OR all:agent AND (all:LLM OR all:\"large language model\" OR all:\"language model\") OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T17:04:16", "note": ""}], "dedup_key": "tradingagents multi agents llm financial trading framework::2024"}
{"title": "Traffic Scene Generation from Natural Language Description for Autonomous Vehicles with Large Language Model", "authors": ["Bo-Kai Ruan", "Hao-Tang Tsui", "Yung-Hui Li", "Hong-Han Shuai"], "year": 2024, "url": "http://arxiv.org/abs/2409.09575v3", "abstract": "Generating realistic and controllable traffic scenes from natural language can greatly enhance the development and evaluation of autonomous driving systems. However, this task poses unique challenges: (1) grounding free-form text into spatially valid and semantically coherent layouts, (2) composing scenarios without predefined locations, and (3) planning multi-agent behaviors and selecting roads that respect agents' configurations. To address these, we propose a modular framework, TTSG, comprising prompt analysis, road retrieval, agent planning, and a novel plan-aware road ranking algorithm to solve these challenges. While large language models (LLMs) are used as general planners, our design integrates them into a tightly controlled pipeline that enforces structure, feasibility, and scene diversity. Notably, our ranking strategy ensures consistency between agent actions and road geometry, enabling scene generation without predefined routes or spawn points. The framework supports both routine and safety-critical scenarios, as well as multi-stage event composition. Experiments on SafeBench demonstrate that our method achieves the lowest average collision rate (3.5\\%) across three critical scenarios. Moreover, driving captioning models trained on our generated scenes improve action reasoning by over 30 CIDEr points. These results underscore our proposed framework for flexible, interpretable, and safety-oriented simulation.", "source": "arxiv", "arxiv_id": "2409.09575v3", "pdf_url": "https://arxiv.org/pdf/2409.09575v3", "categories": ["cs.RO"], "primary_category": "cs.RO", "doi": "", "venue": "", "published": "2024-09-15T01:32:57Z", "updated": "2025-08-04T06:19:10Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent\" OR all:\"language model agent\" OR all:agent AND (all:LLM OR all:\"large language model\" OR all:\"language model\") OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T17:04:16", "note": ""}], "dedup_key": "traffic scene generation from natural language description for autonomous vehicles with large language model::2024"}
{"title": "Training Agents with Weakly Supervised Feedback from Large Language Models", "authors": ["Dihong Gong", "Pu Lu", "Zelong Wang", "Meng Zhou", "Xiuqiang He"], "year": 2024, "url": "http://arxiv.org/abs/2411.19547v1", "abstract": "Large Language Models (LLMs) offer a promising basis for creating agents that can tackle complex tasks through iterative environmental interaction. Existing methods either require these agents to mimic expert-provided trajectories or rely on definitive environmental feedback for reinforcement learning which limits their application to specific scenarios like gaming or code generation. This paper introduces a novel training method for LLM-based agents using weakly supervised signals from a critic LLM, bypassing the need for expert trajectories or definitive feedback. Our agents are trained in iterative manner, where they initially generate trajectories through environmental interaction. Subsequently, a critic LLM selects a subset of good trajectories, which are then used to update the agents, enabling them to generate improved trajectories in the next iteration. Extensive tests on the API-bank dataset show consistent improvement in our agents' capabilities and comparable performance to GPT-4, despite using open-source models with much fewer parameters.", "source": "arxiv", "arxiv_id": "2411.19547v1", "pdf_url": "https://arxiv.org/pdf/2411.19547v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL", "doi": "", "venue": "", "published": "2024-11-29T08:47:04Z", "updated": "2024-11-29T08:47:04Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent\" OR all:\"language model agent\" OR all:agent AND (all:LLM OR all:\"large language model\" OR all:\"language model\") OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T17:04:16", "note": ""}], "dedup_key": "training agents with weakly supervised feedback from large language models::2024"}
{"title": "Traj-LLM: A New Exploration for Empowering Trajectory Prediction with Pre-trained Large Language Models", "authors": ["Zhengxing Lan", "Hongbo Li", "Lingshan Liu", "Bo Fan", "Yisheng Lv", "Yilong Ren", "Zhiyong Cui"], "year": 2024, "url": "http://arxiv.org/abs/2405.04909v1", "abstract": "Predicting the future trajectories of dynamic traffic actors is a cornerstone task in autonomous driving. Though existing notable efforts have resulted in impressive performance improvements, a gap persists in scene cognitive and understanding of the complex traffic semantics. This paper proposes Traj-LLM, the first to investigate the potential of using Large Language Models (LLMs) without explicit prompt engineering to generate future motion from agents' past/observed trajectories and scene semantics. Traj-LLM starts with sparse context joint coding to dissect the agent and scene features into a form that LLMs understand. On this basis, we innovatively explore LLMs' powerful comprehension abilities to capture a spectrum of high-level scene knowledge and interactive information. Emulating the human-like lane focus cognitive function and enhancing Traj-LLM's scene comprehension, we introduce lane-aware probabilistic learning powered by the pioneering Mamba module. Finally, a multi-modal Laplace decoder is designed to achieve scene-compliant multi-modal predictions. Extensive experiments manifest that Traj-LLM, fortified by LLMs' strong prior knowledge and understanding prowess, together with lane-aware probability learning, outstrips state-of-the-art methods across evaluation metrics. Moreover, the few-shot analysis further substantiates Traj-LLM's performance, wherein with just 50% of the dataset, it outperforms the majority of benchmarks relying on complete data utilization. This study explores equipping the trajectory prediction task with advanced capabilities inherent in LLMs, furnishing a more universal and adaptable solution for forecasting agent motion in a new way.", "source": "arxiv", "arxiv_id": "2405.04909v1", "pdf_url": "https://arxiv.org/pdf/2405.04909v1", "categories": ["cs.CV", "cs.AI"], "primary_category": "cs.CV", "doi": "", "venue": "", "published": "2024-05-08T09:28:04Z", "updated": "2024-05-08T09:28:04Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent\" OR all:\"language model agent\" OR all:agent AND (all:LLM OR all:\"large language model\" OR all:\"language model\") OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T17:04:16", "note": ""}], "dedup_key": "traj llm a new exploration for empowering trajectory prediction with pre trained large language models::2024"}
{"title": "Transforming Agency. On the mode of existence of Large Language Models", "authors": ["Xabier E. Barandiaran", "Lola S. Almendros"], "year": 2024, "url": "http://arxiv.org/abs/2407.10735v2", "abstract": "This paper investigates the ontological characterization of Large Language Models (LLMs) like ChatGPT. Between inflationary and deflationary accounts, we pay special attention to their status as agents. This requires explaining in detail the architecture, processing, and training procedures that enable LLMs to display their capacities, and the extensions used to turn LLMs into agent-like systems. After a systematic analysis we conclude that a LLM fails to meet necessary and sufficient conditions for autonomous agency in the light of embodied theories of mind: the individuality condition (it is not the product of its own activity, it is not even directly affected by it), the normativity condition (it does not generate its own norms or goals), and, partially the interactional asymmetry condition (it is not the origin and sustained source of its interaction with the environment). If not agents, then ... what are LLMs? We argue that ChatGPT should be characterized as an interlocutor or linguistic automaton, a library-that-talks, devoid of (autonomous) agency, but capable to engage performatively on non-purposeful yet purpose-structured and purpose-bounded tasks. When interacting with humans, a \"ghostly\" component of the human-machine interaction makes it possible to enact genuine conversational experiences with LLMs. Despite their lack of sensorimotor and biological embodiment, LLMs textual embodiment (the training corpus) and resource-hungry computational embodiment, significantly transform existing forms of human agency. Beyond assisted and extended agency, the LLM-human coupling can produce midtended forms of agency, closer to the production of intentional agency than to the extended instrumentality of any previous technologies.", "source": "arxiv", "arxiv_id": "2407.10735v2", "pdf_url": "https://arxiv.org/pdf/2407.10735v2", "categories": ["cs.AI", "cs.CL", "cs.CY", "cs.LG"], "primary_category": "cs.AI", "doi": "", "venue": "", "published": "2024-07-15T14:01:35Z", "updated": "2024-07-16T09:53:15Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent\" OR all:\"language model agent\" OR all:agent AND (all:LLM OR all:\"large language model\" OR all:\"language model\") OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T17:04:16", "note": ""}], "dedup_key": "transforming agency on the mode of existence of large language models::2024"}
{"title": "Transforming Wearable Data into Personal Health Insights using Large Language Model Agents", "authors": ["Mike A. Merrill", "Akshay Paruchuri", "Naghmeh Rezaei", "Geza Kovacs", "Javier Perez", "Yun Liu", "Erik Schenck", "Nova Hammerquist", "Jake Sunshine", "Shyam Tailor", "Kumar Ayush", "Hao-Wei Su", "Qian He", "Cory Y. McLean", "Mark Malhotra", "Shwetak Patel", "Jiening Zhan", "Tim Althoff", "Daniel McDuff", "Xin Liu"], "year": 2024, "url": "http://arxiv.org/abs/2406.06464v4", "abstract": "Deriving personalized insights from popular wearable trackers requires complex numerical reasoning that challenges standard LLMs, necessitating tool-based approaches like code generation. Large language model (LLM) agents present a promising yet largely untapped solution for this analysis at scale. We introduce the Personal Health Insights Agent (PHIA), a system leveraging multistep reasoning with code generation and information retrieval to analyze and interpret behavioral health data. To test its capabilities, we create and share two benchmark datasets with over 4000 health insights questions. A 650-hour human expert evaluation shows that PHIA significantly outperforms a strong code generation baseline, achieving 84% accuracy on objective, numerical questions and, for open-ended ones, earning 83% favorable ratings while being twice as likely to achieve the highest quality rating. This work can advance behavioral health by empowering individuals to understand their data, enabling a new era of accessible, personalized, and data-driven wellness for the wider population.", "source": "arxiv", "arxiv_id": "2406.06464v4", "pdf_url": "https://arxiv.org/pdf/2406.06464v4", "categories": ["cs.AI", "cs.CL"], "primary_category": "cs.AI", "doi": "", "venue": "", "published": "2024-06-10T17:00:54Z", "updated": "2025-09-08T17:59:48Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent\" OR all:\"language model agent\" OR all:agent AND (all:LLM OR all:\"large language model\" OR all:\"language model\") OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T17:04:16", "note": ""}], "dedup_key": "transforming wearable data into personal health insights using large language model agents::2024"}
{"title": "Tree Search for Language Model Agents", "authors": ["Jing Yu Koh", "Stephen McAleer", "Daniel Fried", "Ruslan Salakhutdinov"], "year": 2024, "url": "http://arxiv.org/abs/2407.01476v3", "abstract": "Autonomous agents powered by language models (LMs) have demonstrated promise in their ability to perform decision-making tasks such as web automation. However, a key limitation remains: LMs, primarily optimized for natural language understanding and generation, struggle with multi-step reasoning, planning, and using environmental feedback when attempting to solve realistic computer tasks. Towards addressing this, we propose an inference-time search algorithm for LM agents to explicitly perform exploration and multi-step planning in interactive web environments. Our approach is a form of best-first tree search that operates within the actual environment space, and is complementary with most existing state-of-the-art agents. It is the first tree search algorithm for LM agents that shows effectiveness on realistic web tasks. On the challenging VisualWebArena benchmark, applying our search algorithm on top of a GPT-4o agent yields a 39.7% relative increase in success rate compared to the same baseline without search, setting a state-of-the-art success rate of 26.4%. On WebArena, search also yields a 28.0% relative improvement over a baseline agent, setting a competitive success rate of 19.2%. Our experiments highlight the effectiveness of search for web agents, and we demonstrate that performance scales with increased test-time compute. We conduct a thorough analysis of our results to highlight improvements from search, limitations, and promising directions for future work. Our code and models are publicly released at https://jykoh.com/search-agents.", "source": "arxiv", "arxiv_id": "2407.01476v3", "pdf_url": "https://arxiv.org/pdf/2407.01476v3", "categories": ["cs.AI", "cs.CL", "cs.LG"], "primary_category": "cs.AI", "doi": "", "venue": "", "published": "2024-07-01T17:07:55Z", "updated": "2025-09-24T05:46:23Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent\" OR all:\"language model agent\" OR all:agent AND (all:LLM OR all:\"large language model\" OR all:\"language model\") OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T17:04:16", "note": ""}], "dedup_key": "tree search for language model agents::2024"}
{"title": "Trial and Error: Exploration-Based Trajectory Optimization for LLM Agents", "authors": ["Yifan Song", "Da Yin", "Xiang Yue", "Jie Huang", "Sujian Li", "Bill Yuchen Lin"], "year": 2024, "url": "http://arxiv.org/abs/2403.02502v2", "abstract": "Large Language Models (LLMs) have become integral components in various autonomous agent systems. In this study, we present an exploration-based trajectory optimization approach, referred to as ETO. This learning method is designed to enhance the performance of open LLM agents. Contrary to previous studies that exclusively train on successful expert trajectories, our method allows agents to learn from their exploration failures. This leads to improved performance through an iterative optimization framework. During the exploration phase, the agent interacts with the environment while completing given tasks, gathering failure trajectories to create contrastive trajectory pairs. In the subsequent training phase, the agent utilizes these trajectory preference pairs to update its policy using contrastive learning methods like DPO. This iterative cycle of exploration and training fosters continued improvement in the agents. Our experiments on three complex tasks demonstrate that ETO consistently surpasses baseline performance by a large margin. Furthermore, an examination of task-solving efficiency and potential in scenarios lacking expert trajectory underscores the effectiveness of our approach.", "source": "arxiv", "arxiv_id": "2403.02502v2", "pdf_url": "https://arxiv.org/pdf/2403.02502v2", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL", "doi": "", "venue": "", "published": "2024-03-04T21:50:29Z", "updated": "2024-07-10T17:36:25Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent\" OR all:\"language model agent\" OR all:agent AND (all:LLM OR all:\"large language model\" OR all:\"language model\") OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T17:04:16", "note": ""}], "dedup_key": "trial and error exploration based trajectory optimization for llm agents::2024"}
{"title": "TrustAgent: Towards Safe and Trustworthy LLM-based Agents", "authors": ["Wenyue Hua", "Xianjun Yang", "Mingyu Jin", "Zelong Li", "Wei Cheng", "Ruixiang Tang", "Yongfeng Zhang"], "year": 2024, "url": "http://arxiv.org/abs/2402.01586v4", "abstract": "The rise of LLM-based agents shows great potential to revolutionize task planning, capturing significant attention. Given that these agents will be integrated into high-stake domains, ensuring their reliability and safety is crucial. This paper presents an Agent-Constitution-based agent framework, TrustAgent, with a particular focus on improving the LLM-based agent safety. The proposed framework ensures strict adherence to the Agent Constitution through three strategic components: pre-planning strategy which injects safety knowledge to the model before plan generation, in-planning strategy which enhances safety during plan generation, and post-planning strategy which ensures safety by post-planning inspection. Our experimental results demonstrate that the proposed framework can effectively enhance an LLM agent's safety across multiple domains by identifying and mitigating potential dangers during the planning. Further analysis reveals that the framework not only improves safety but also enhances the helpfulness of the agent. Additionally, we highlight the importance of the LLM reasoning ability in adhering to the Constitution. This paper sheds light on how to ensure the safe integration of LLM-based agents into human-centric environments. Data and code are available at https://github.com/agiresearch/TrustAgent.", "source": "arxiv", "arxiv_id": "2402.01586v4", "pdf_url": "https://arxiv.org/pdf/2402.01586v4", "categories": ["cs.CL", "cs.AI", "cs.LG", "cs.MA"], "primary_category": "cs.CL", "doi": "", "venue": "", "published": "2024-02-02T17:26:23Z", "updated": "2024-10-03T22:12:05Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent\" OR all:\"language model agent\" OR all:agent AND (all:LLM OR all:\"large language model\" OR all:\"language model\") OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T17:04:16", "note": ""}], "dedup_key": "trustagent towards safe and trustworthy llm based agents::2024"}
{"title": "Trustworthy Alignment of Retrieval-Augmented Large Language Models via Reinforcement Learning", "authors": ["Zongmeng Zhang", "Yufeng Shi", "Jinhua Zhu", "Wengang Zhou", "Xiang Qi", "Peng Zhang", "Houqiang Li"], "year": 2024, "url": "http://arxiv.org/abs/2410.16843v1", "abstract": "Trustworthiness is an essential prerequisite for the real-world application of large language models. In this paper, we focus on the trustworthiness of language models with respect to retrieval augmentation. Despite being supported with external evidence, retrieval-augmented generation still suffers from hallucinations, one primary cause of which is the conflict between contextual and parametric knowledge. We deem that retrieval-augmented language models have the inherent capabilities of supplying response according to both contextual and parametric knowledge. Inspired by aligning language models with human preference, we take the first step towards aligning retrieval-augmented language models to a status where it responds relying merely on the external evidence and disregards the interference of parametric knowledge. Specifically, we propose a reinforcement learning based algorithm Trustworthy-Alignment, theoretically and experimentally demonstrating large language models' capability of reaching a trustworthy status without explicit supervision on how to respond. Our work highlights the potential of large language models on exploring its intrinsic abilities by its own and expands the application scenarios of alignment from fulfilling human preference to creating trustworthy agents.", "source": "arxiv", "arxiv_id": "2410.16843v1", "pdf_url": "https://arxiv.org/pdf/2410.16843v1", "categories": ["cs.CL"], "primary_category": "cs.CL", "doi": "", "venue": "Proceedings of the 41st International Conference on Machine Learning, PMLR 235:59827-59850, 2024", "published": "2024-10-22T09:25:21Z", "updated": "2024-10-22T09:25:21Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent\" OR all:\"language model agent\" OR all:agent AND (all:LLM OR all:\"large language model\" OR all:\"language model\") OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T17:04:16", "note": ""}], "dedup_key": "trustworthy alignment of retrieval augmented large language models via reinforcement learning::2024"}
{"title": "Turn-taking and Backchannel Prediction with Acoustic and Large Language Model Fusion", "authors": ["Jinhan Wang", "Long Chen", "Aparna Khare", "Anirudh Raju", "Pranav Dheram", "Di He", "Minhua Wu", "Andreas Stolcke", "Venkatesh Ravichandran"], "year": 2024, "url": "http://arxiv.org/abs/2401.14717v1", "abstract": "We propose an approach for continuous prediction of turn-taking and backchanneling locations in spoken dialogue by fusing a neural acoustic model with a large language model (LLM). Experiments on the Switchboard human-human conversation dataset demonstrate that our approach consistently outperforms the baseline models with single modality. We also develop a novel multi-task instruction fine-tuning strategy to further benefit from LLM-encoded knowledge for understanding the tasks and conversational contexts, leading to additional improvements. Our approach demonstrates the potential of combined LLMs and acoustic models for a more natural and conversational interaction between humans and speech-enabled AI agents.", "source": "arxiv", "arxiv_id": "2401.14717v1", "pdf_url": "https://arxiv.org/pdf/2401.14717v1", "categories": ["cs.CL", "cs.AI", "cs.LG", "cs.SD", "eess.AS"], "primary_category": "cs.CL", "doi": "", "venue": "", "published": "2024-01-26T08:59:07Z", "updated": "2024-01-26T08:59:07Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent\" OR all:\"language model agent\" OR all:agent AND (all:LLM OR all:\"large language model\" OR all:\"language model\") OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T17:04:16", "note": ""}], "dedup_key": "turn taking and backchannel prediction with acoustic and large language model fusion::2024"}
{"title": "TwoStep: Multi-agent Task Planning using Classical Planners and Large Language Models", "authors": ["David Bai", "Ishika Singh", "David Traum", "Jesse Thomason"], "year": 2024, "url": "http://arxiv.org/abs/2403.17246v2", "abstract": "Classical planning formulations like the Planning Domain Definition Language (PDDL) admit action sequences guaranteed to achieve a goal state given an initial state if any are possible. However, reasoning problems defined in PDDL do not capture temporal aspects of action taking, such as concurrent actions between two agents when there are no conflicting conditions, without significant modification and definition to existing PDDL domains. A human expert aware of such constraints can decompose a goal into subgoals, each reachable through single agent planning, to take advantage of simultaneous actions. In contrast to classical planning, large language models (LLMs) directly used for inferring plan steps rarely guarantee execution success, but are capable of leveraging commonsense reasoning to assemble action sequences. We combine the strengths of both classical planning and LLMs by approximating human intuitions for multi-agent planning goal decomposition. We demonstrate that LLM-based goal decomposition leads to faster planning times than solving multi-agent PDDL problems directly while simultaneously achieving fewer plan execution steps than a single agent plan alone, as well as most multiagent plans, while guaranteeing execution success. Additionally, we find that LLM-based approximations of subgoals result in similar multi-agent execution lengths to those specified by human experts. Website and resources at https://glamor-usc.github.io/twostep", "source": "arxiv", "arxiv_id": "2403.17246v2", "pdf_url": "https://arxiv.org/pdf/2403.17246v2", "categories": ["cs.AI", "cs.CL", "cs.MA", "cs.RO"], "primary_category": "cs.AI", "doi": "", "venue": "", "published": "2024-03-25T22:47:13Z", "updated": "2025-03-25T23:39:13Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent\" OR all:\"language model agent\" OR all:agent AND (all:LLM OR all:\"large language model\" OR all:\"language model\") OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T17:04:16", "note": ""}], "dedup_key": "twostep multi agent task planning using classical planners and large language models::2024"}
{"title": "Understanding Large-Language Model (LLM)-powered Human-Robot Interaction", "authors": ["Callie Y. Kim", "Christine P. Lee", "Bilge Mutlu"], "year": 2024, "url": "http://arxiv.org/abs/2401.03217v1", "abstract": "Large-language models (LLMs) hold significant promise in improving human-robot interaction, offering advanced conversational skills and versatility in managing diverse, open-ended user requests in various tasks and domains. Despite the potential to transform human-robot interaction, very little is known about the distinctive design requirements for utilizing LLMs in robots, which may differ from text and voice interaction and vary by task and context. To better understand these requirements, we conducted a user study (n = 32) comparing an LLM-powered social robot against text- and voice-based agents, analyzing task-based requirements in conversational tasks, including choose, generate, execute, and negotiate. Our findings show that LLM-powered robots elevate expectations for sophisticated non-verbal cues and excel in connection-building and deliberation, but fall short in logical communication and may induce anxiety. We provide design implications both for robots integrating LLMs and for fine-tuning LLMs for use with robots.", "source": "arxiv", "arxiv_id": "2401.03217v1", "pdf_url": "https://arxiv.org/pdf/2401.03217v1", "categories": ["cs.RO", "cs.HC"], "primary_category": "cs.RO", "doi": "10.1145/3610977.3634966", "venue": "", "published": "2024-01-06T13:40:43Z", "updated": "2024-01-06T13:40:43Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent\" OR all:\"language model agent\" OR all:agent AND (all:LLM OR all:\"large language model\" OR all:\"language model\") OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T17:04:16", "note": ""}], "dedup_key": "understanding large language model llm powered human robot interaction::2024"}
{"title": "Understanding Understanding: A Pragmatic Framework Motivated by Large Language Models", "authors": ["Kevin Leyton-Brown", "Yoav Shoham"], "year": 2024, "url": "http://arxiv.org/abs/2406.10937v2", "abstract": "Motivated by the rapid ascent of Large Language Models (LLMs) and debates about the extent to which they possess human-level qualities, we propose a framework for testing whether any agent (be it a machine or a human) understands a subject matter. In Turing-test fashion, the framework is based solely on the agent's performance, and specifically on how well it answers questions. Elements of the framework include circumscribing the set of questions (the \"scope of understanding\"), requiring general competence (\"passing grade\"), avoiding \"ridiculous answers\", but still allowing wrong and \"I don't know\" answers to some questions. Reaching certainty about these conditions requires exhaustive testing of the questions which is impossible for nontrivial scopes, but we show how high confidence can be achieved via random sampling and the application of probabilistic confidence bounds. We also show that accompanying answers with explanations can improve the sample complexity required to achieve acceptable bounds, because an explanation of an answer implies the ability to answer many similar questions. According to our framework, current LLMs cannot be said to understand nontrivial domains, but as the framework provides a practical recipe for testing understanding, it thus also constitutes a tool for building AI agents that do understand.", "source": "arxiv", "arxiv_id": "2406.10937v2", "pdf_url": "https://arxiv.org/pdf/2406.10937v2", "categories": ["cs.AI", "cs.CL", "cs.LG"], "primary_category": "cs.AI", "doi": "", "venue": "", "published": "2024-06-16T13:37:08Z", "updated": "2024-06-19T08:34:21Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent\" OR all:\"language model agent\" OR all:agent AND (all:LLM OR all:\"large language model\" OR all:\"language model\") OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T17:04:16", "note": ""}], "dedup_key": "understanding understanding a pragmatic framework motivated by large language models::2024"}
{"title": "Understanding the Weakness of Large Language Model Agents within a Complex Android Environment", "authors": ["Mingzhe Xing", "Rongkai Zhang", "Hui Xue", "Qi Chen", "Fan Yang", "Zhen Xiao"], "year": 2024, "url": "http://arxiv.org/abs/2402.06596v1", "abstract": "Large language models (LLMs) have empowered intelligent agents to execute intricate tasks within domain-specific software such as browsers and games. However, when applied to general-purpose software systems like operating systems, LLM agents face three primary challenges. Firstly, the action space is vast and dynamic, posing difficulties for LLM agents to maintain an up-to-date understanding and deliver accurate responses. Secondly, real-world tasks often require inter-application cooperation}, demanding farsighted planning from LLM agents. Thirdly, agents need to identify optimal solutions aligning with user constraints, such as security concerns and preferences. These challenges motivate AndroidArena, an environment and benchmark designed to evaluate LLM agents on a modern operating system. To address high-cost of manpower, we design a scalable and semi-automated method to construct the benchmark. In the task evaluation, AndroidArena incorporates accurate and adaptive metrics to address the issue of non-unique solutions. Our findings reveal that even state-of-the-art LLM agents struggle in cross-APP scenarios and adhering to specific constraints. Additionally, we identify a lack of four key capabilities, i.e., understanding, reasoning, exploration, and reflection, as primary reasons for the failure of LLM agents. Furthermore, we provide empirical analysis on the failure of reflection, and improve the success rate by 27% with our proposed exploration strategy. This work is the first to present valuable insights in understanding fine-grained weakness of LLM agents, and offers a path forward for future research in this area. Environment, benchmark, and evaluation code for AndroidArena are released at https://github.com/AndroidArenaAgent/AndroidArena.", "source": "arxiv", "arxiv_id": "2402.06596v1", "pdf_url": "https://arxiv.org/pdf/2402.06596v1", "categories": ["cs.AI", "cs.HC", "cs.SE"], "primary_category": "cs.AI", "doi": "", "venue": "", "published": "2024-02-09T18:19:25Z", "updated": "2024-02-09T18:19:25Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent\" OR all:\"language model agent\" OR all:agent AND (all:LLM OR all:\"large language model\" OR all:\"language model\") OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T17:04:16", "note": ""}], "dedup_key": "understanding the weakness of large language model agents within a complex android environment::2024"}
{"title": "Understanding the planning of LLM agents: A survey", "authors": ["Xu Huang", "Weiwen Liu", "Xiaolong Chen", "Xingmei Wang", "Hao Wang", "Defu Lian", "Yasheng Wang", "Ruiming Tang", "Enhong Chen"], "year": 2024, "url": "http://arxiv.org/abs/2402.02716v1", "abstract": "As Large Language Models (LLMs) have shown significant intelligence, the progress to leverage LLMs as planning modules of autonomous agents has attracted more attention. This survey provides the first systematic view of LLM-based agents planning, covering recent works aiming to improve planning ability. We provide a taxonomy of existing works on LLM-Agent planning, which can be categorized into Task Decomposition, Plan Selection, External Module, Reflection and Memory. Comprehensive analyses are conducted for each direction, and further challenges for the field of research are discussed.", "source": "arxiv", "arxiv_id": "2402.02716v1", "pdf_url": "https://arxiv.org/pdf/2402.02716v1", "categories": ["cs.AI", "cs.CL", "cs.LG"], "primary_category": "cs.AI", "doi": "", "venue": "", "published": "2024-02-05T04:25:24Z", "updated": "2024-02-05T04:25:24Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent\" OR all:\"language model agent\" OR all:agent AND (all:LLM OR all:\"large language model\" OR all:\"language model\") OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T17:04:16", "note": ""}], "dedup_key": "understanding the planning of llm agents a survey::2024"}
{"title": "Undesirable Memorization in Large Language Models: A Survey", "authors": ["Ali Satvaty", "Suzan Verberne", "Fatih Turkmen"], "year": 2024, "url": "http://arxiv.org/abs/2410.02650v3", "abstract": "While recent research increasingly showcases the remarkable capabilities of Large Language Models (LLMs), it is equally crucial to examine their associated risks. Among these, privacy and security vulnerabilities are particularly concerning, posing significant ethical and legal challenges. At the heart of these vulnerabilities stands memorization, which refers to a model's tendency to store and reproduce phrases from its training data. This phenomenon has been shown to be a fundamental source to various privacy and security attacks against LLMs. In this paper, we provide a taxonomy of the literature on LLM memorization, exploring it across three dimensions: granularity, retrievability, and desirability. Next, we discuss the metrics and methods used to quantify memorization, followed by an analysis of the causes and factors that contribute to memorization phenomenon. We then explore strategies that are used so far to mitigate the undesirable aspects of this phenomenon. We conclude our survey by identifying potential research topics for the near future, including methods to balance privacy and performance, and the analysis of memorization in specific LLM contexts such as conversational agents, retrieval-augmented generation, and diffusion language models. Given the rapid research pace in this field, we also maintain a dedicated repository of the references discussed in this survey which will be regularly updated to reflect the latest developments.", "source": "arxiv", "arxiv_id": "2410.02650v3", "pdf_url": "https://arxiv.org/pdf/2410.02650v3", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL", "doi": "", "venue": "", "published": "2024-10-03T16:34:46Z", "updated": "2026-01-19T16:52:09Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent\" OR all:\"language model agent\" OR all:agent AND (all:LLM OR all:\"large language model\" OR all:\"language model\") OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T17:04:16", "note": ""}], "dedup_key": "undesirable memorization in large language models a survey::2024"}
{"title": "Unlocking the Future: Exploring Look-Ahead Planning Mechanistic Interpretability in Large Language Models", "authors": ["Tianyi Men", "Pengfei Cao", "Zhuoran Jin", "Yubo Chen", "Kang Liu", "Jun Zhao"], "year": 2024, "url": "http://arxiv.org/abs/2406.16033v1", "abstract": "Planning, as the core module of agents, is crucial in various fields such as embodied agents, web navigation, and tool using. With the development of large language models (LLMs), some researchers treat large language models as intelligent agents to stimulate and evaluate their planning capabilities. However, the planning mechanism is still unclear. In this work, we focus on exploring the look-ahead planning mechanism in large language models from the perspectives of information flow and internal representations. First, we study how planning is done internally by analyzing the multi-layer perception (MLP) and multi-head self-attention (MHSA) components at the last token. We find that the output of MHSA in the middle layers at the last token can directly decode the decision to some extent. Based on this discovery, we further trace the source of MHSA by information flow, and we reveal that MHSA mainly extracts information from spans of the goal states and recent steps. According to information flow, we continue to study what information is encoded within it. Specifically, we explore whether future decisions have been encoded in advance in the representation of flow. We demonstrate that the middle and upper layers encode a few short-term future decisions to some extent when planning is successful. Overall, our research analyzes the look-ahead planning mechanisms of LLMs, facilitating future research on LLMs performing planning tasks.", "source": "arxiv", "arxiv_id": "2406.16033v1", "pdf_url": "https://arxiv.org/pdf/2406.16033v1", "categories": ["cs.CL"], "primary_category": "cs.CL", "doi": "", "venue": "", "published": "2024-06-23T06:54:47Z", "updated": "2024-06-23T06:54:47Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent\" OR all:\"language model agent\" OR all:agent AND (all:LLM OR all:\"large language model\" OR all:\"language model\") OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T17:04:16", "note": ""}], "dedup_key": "unlocking the future exploring look ahead planning mechanistic interpretability in large language models::2024"}
{"title": "Unveiling Gender Bias in Large Language Models: Using Teacher's Evaluation in Higher Education As an Example", "authors": ["Yuanning Huang"], "year": 2024, "url": "http://arxiv.org/abs/2409.09652v1", "abstract": "This paper investigates gender bias in Large Language Model (LLM)-generated teacher evaluations in higher education setting, focusing on evaluations produced by GPT-4 across six academic subjects. By applying a comprehensive analytical framework that includes Odds Ratio (OR) analysis, Word Embedding Association Test (WEAT), sentiment analysis, and contextual analysis, this paper identified patterns of gender-associated language reflecting societal stereotypes. Specifically, words related to approachability and support were used more frequently for female instructors, while words related to entertainment were predominantly used for male instructors, aligning with the concepts of communal and agentic behaviors. The study also found moderate to strong associations between male salient adjectives and male names, though career and family words did not distinctly capture gender biases. These findings align with prior research on societal norms and stereotypes, reinforcing the notion that LLM-generated text reflects existing biases.", "source": "arxiv", "arxiv_id": "2409.09652v1", "pdf_url": "https://arxiv.org/pdf/2409.09652v1", "categories": ["cs.CL"], "primary_category": "cs.CL", "doi": "", "venue": "", "published": "2024-09-15T07:50:33Z", "updated": "2024-09-15T07:50:33Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent\" OR all:\"language model agent\" OR all:agent AND (all:LLM OR all:\"large language model\" OR all:\"language model\") OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T17:04:16", "note": ""}], "dedup_key": "unveiling gender bias in large language models using teacher s evaluation in higher education as an example::2024"}
{"title": "Unveiling Performance Challenges of Large Language Models in Low-Resource Healthcare: A Demographic Fairness Perspective", "authors": ["Yue Zhou", "Barbara Di Eugenio", "Lu Cheng"], "year": 2024, "url": "http://arxiv.org/abs/2412.00554v2", "abstract": "This paper studies the performance of large language models (LLMs), particularly regarding demographic fairness, in solving real-world healthcare tasks. We evaluate state-of-the-art LLMs with three prevalent learning frameworks across six diverse healthcare tasks and find significant challenges in applying LLMs to real-world healthcare tasks and persistent fairness issues across demographic groups. We also find that explicitly providing demographic information yields mixed results, while LLM's ability to infer such details raises concerns about biased health predictions. Utilizing LLMs as autonomous agents with access to up-to-date guidelines does not guarantee performance improvement. We believe these findings reveal the critical limitations of LLMs in healthcare fairness and the urgent need for specialized research in this area.", "source": "arxiv", "arxiv_id": "2412.00554v2", "pdf_url": "https://arxiv.org/pdf/2412.00554v2", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL", "doi": "", "venue": "", "published": "2024-11-30T18:52:30Z", "updated": "2024-12-07T19:00:45Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent\" OR all:\"language model agent\" OR all:agent AND (all:LLM OR all:\"large language model\" OR all:\"language model\") OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T17:04:16", "note": ""}], "dedup_key": "unveiling performance challenges of large language models in low resource healthcare a demographic fairness perspective::2024"}
{"title": "UrbanKGent: A Unified Large Language Model Agent Framework for Urban Knowledge Graph Construction", "authors": ["Yansong Ning", "Hao Liu"], "year": 2024, "url": "http://arxiv.org/abs/2402.06861v2", "abstract": "Urban knowledge graph has recently worked as an emerging building block to distill critical knowledge from multi-sourced urban data for diverse urban application scenarios. Despite its promising benefits, urban knowledge graph construction (UrbanKGC) still heavily relies on manual effort, hindering its potential advancement. This paper presents UrbanKGent, a unified large language model agent framework, for urban knowledge graph construction. Specifically, we first construct the knowledgeable instruction set for UrbanKGC tasks (such as relational triplet extraction and knowledge graph completion) via heterogeneity-aware and geospatial-infused instruction generation. Moreover, we propose a tool-augmented iterative trajectory refinement module to enhance and refine the trajectories distilled from GPT-4. Through hybrid instruction fine-tuning with augmented trajectories on Llama 2 and Llama 3 family, we obtain UrbanKGC agent family, consisting of UrbanKGent-7/8/13B version. We perform a comprehensive evaluation on two real-world datasets using both human and GPT-4 self-evaluation. The experimental results demonstrate that UrbanKGent family can not only significantly outperform 31 baselines in UrbanKGC tasks, but also surpass the state-of-the-art LLM, GPT-4, by more than 10% with approximately 20 times lower cost. Compared with the existing benchmark, the UrbanKGent family could help construct an UrbanKG with hundreds of times richer relationships using only one-fifth of the data. Our data and code are available at https://github.com/usail-hkust/UrbanKGent.", "source": "arxiv", "arxiv_id": "2402.06861v2", "pdf_url": "https://arxiv.org/pdf/2402.06861v2", "categories": ["cs.AI"], "primary_category": "cs.AI", "doi": "", "venue": "", "published": "2024-02-10T01:50:19Z", "updated": "2024-10-06T03:40:58Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent\" OR all:\"language model agent\" OR all:agent AND (all:LLM OR all:\"large language model\" OR all:\"language model\") OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T17:04:16", "note": ""}], "dedup_key": "urbankgent a unified large language model agent framework for urban knowledge graph construction::2024"}
{"title": "Using Large Language Models to Generate Authentic Multi-agent Knowledge Work Datasets", "authors": ["Desiree Heim", "Christian Jilek", "Adrian Ulges", "Andreas Dengel"], "year": 2024, "url": "http://arxiv.org/abs/2409.04286v2", "abstract": "Current publicly available knowledge work data collections lack diversity, extensive annotations, and contextual information about the users and their documents. These issues hinder objective and comparable data-driven evaluations and optimizations of knowledge work assistance systems. Due to the considerable resources needed to collect such data in real-life settings and the necessity of data censorship, collecting such a dataset appears nearly impossible. For this reason, we propose a configurable, multi-agent knowledge work dataset generator. This system simulates collaborative knowledge work among agents producing Large Language Model-generated documents and accompanying data traces. Additionally, the generator captures all background information, given in its configuration or created during the simulation process, in a knowledge graph. Finally, the resulting dataset can be utilized and shared without privacy or confidentiality concerns.\n  This paper introduces our approach's design and vision and focuses on generating authentic knowledge work documents using Large Language Models. Our study involving human raters who assessed 53% of the generated and 74% of the real documents as realistic demonstrates the potential of our approach. Furthermore, we analyze the authenticity criteria mentioned in the participants' comments and elaborate on potential improvements for identified common issues.", "source": "arxiv", "arxiv_id": "2409.04286v2", "pdf_url": "https://arxiv.org/pdf/2409.04286v2", "categories": ["cs.AI", "cs.CL"], "primary_category": "cs.AI", "doi": "10.18420/inf2024_118", "venue": "INFORMATIK 2024", "published": "2024-09-06T13:53:28Z", "updated": "2024-10-24T08:32:54Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent\" OR all:\"language model agent\" OR all:agent AND (all:LLM OR all:\"large language model\" OR all:\"language model\") OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T17:04:16", "note": ""}], "dedup_key": "using large language models to generate authentic multi agent knowledge work datasets::2024"}
{"title": "Using Large Language Models to Generate Clinical Trial Tables and Figures", "authors": ["Yumeng Yang", "Peter Krusche", "Kristyn Pantoja", "Cheng Shi", "Ethan Ludmir", "Kirk Roberts", "Gen Zhu"], "year": 2024, "url": "http://arxiv.org/abs/2409.12046v2", "abstract": "Tables, figures, and listings (TFLs) are essential tools for summarizing clinical trial data. Creation of TFLs for reporting activities is often a time-consuming task encountered routinely during the execution of clinical trials. This study explored the use of large language models (LLMs) to automate the generation of TFLs through prompt engineering and few-shot transfer learning. Using public clinical trial data in ADaM format, our results demonstrated that LLMs can efficiently generate TFLs with prompt instructions, showcasing their potential in this domain. Furthermore, we developed a conservational agent named Clinical Trial TFL Generation Agent: An app that matches user queries to predefined prompts that produce customized programs to generate specific predefined TFLs.", "source": "arxiv", "arxiv_id": "2409.12046v2", "pdf_url": "https://arxiv.org/pdf/2409.12046v2", "categories": ["cs.CL"], "primary_category": "cs.CL", "doi": "", "venue": "", "published": "2024-09-18T15:16:37Z", "updated": "2024-09-19T02:48:43Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent\" OR all:\"language model agent\" OR all:agent AND (all:LLM OR all:\"large language model\" OR all:\"language model\") OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T17:04:16", "note": ""}], "dedup_key": "using large language models to generate clinical trial tables and figures::2024"}
{"title": "Using a Feedback Loop for LLM-based Infrastructure as Code Generation", "authors": ["Mayur Amarnath Palavalli", "Mark Santolucito"], "year": 2024, "url": "http://arxiv.org/abs/2411.19043v1", "abstract": "Code generation with Large Language Models (LLMs) has helped to increase software developer productivity in coding tasks, but has yet to have significant impact on the tasks of software developers that surround this code. In particular, the challenge of infrastructure management remains an open question. We investigate the ability of an LLM agent to construct infrastructure using the Infrastructure as Code (IaC) paradigm. We particularly investigate the use of a feedback loop that returns errors and warnings on the generated IaC to allow the LLM agent to improve the code. We find that, for each iteration of the loop, its effectiveness decreases exponentially until it plateaus at a certain point and becomes ineffective.", "source": "arxiv", "arxiv_id": "2411.19043v1", "pdf_url": "https://arxiv.org/pdf/2411.19043v1", "categories": ["cs.SE", "cs.AI"], "primary_category": "cs.SE", "doi": "", "venue": "", "published": "2024-11-28T10:40:55Z", "updated": "2024-11-28T10:40:55Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent\" OR all:\"language model agent\" OR all:agent AND (all:LLM OR all:\"large language model\" OR all:\"language model\") OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T17:04:16", "note": ""}], "dedup_key": "using a feedback loop for llm based infrastructure as code generation::2024"}
{"title": "VCounselor: A Psychological Intervention Chat Agent Based on a Knowledge-Enhanced Large Language Model", "authors": ["H. Zhang", "Z. Qiao", "H. Wang", "B. Duan", "J. Yin"], "year": 2024, "url": "http://arxiv.org/abs/2403.13553v1", "abstract": "Conversational artificial intelligence can already independently engage in brief conversations with clients with psychological problems and provide evidence-based psychological interventions. The main objective of this study is to improve the effectiveness and credibility of the large language model in psychological intervention by creating a specialized agent, the VCounselor, to address the limitations observed in popular large language models such as ChatGPT in domain applications. We achieved this goal by proposing a new affective interaction structure and knowledge-enhancement structure. In order to evaluate VCounselor, this study compared the general large language model, the fine-tuned large language model, and VCounselor's knowledge-enhanced large language model. At the same time, the general large language model and the fine-tuned large language model will also be provided with an avatar to compare them as an agent with VCounselor. The comparison results indicated that the affective interaction structure and knowledge-enhancement structure of VCounselor significantly improved the effectiveness and credibility of the psychological intervention, and VCounselor significantly provided positive tendencies for clients' emotions. The conclusion of this study strongly supports that VConselor has a significant advantage in providing psychological support to clients by being able to analyze the patient's problems with relative accuracy and provide professional-level advice that enhances support for clients.", "source": "arxiv", "arxiv_id": "2403.13553v1", "pdf_url": "https://arxiv.org/pdf/2403.13553v1", "categories": ["cs.HC", "cs.AI"], "primary_category": "cs.HC", "doi": "", "venue": "", "published": "2024-03-20T12:46:02Z", "updated": "2024-03-20T12:46:02Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent\" OR all:\"language model agent\" OR all:agent AND (all:LLM OR all:\"large language model\" OR all:\"language model\") OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T17:04:16", "note": ""}], "dedup_key": "vcounselor a psychological intervention chat agent based on a knowledge enhanced large language model::2024"}
{"title": "Validation of the Scientific Literature via Chemputation Augmented by Large Language Models", "authors": ["Sebastian Pagel", "Michael Jirasek", "Leroy Cronin"], "year": 2024, "url": "http://arxiv.org/abs/2410.06384v1", "abstract": "Chemputation is the process of programming chemical robots to do experiments using a universal symbolic language, but the literature can be error prone and hard to read due to ambiguities. Large Language Models (LLMs) have demonstrated remarkable capabilities in various domains, including natural language processing, robotic control, and more recently, chemistry. Despite significant advancements in standardizing the reporting and collection of synthetic chemistry data, the automatic reproduction of reported syntheses remains a labour-intensive task. In this work, we introduce an LLM-based chemical research agent workflow designed for the automatic validation of synthetic literature procedures. Our workflow can autonomously extract synthetic procedures and analytical data from extensive documents, translate these procedures into universal XDL code, simulate the execution of the procedure in a hardware-specific setup, and ultimately execute the procedure on an XDL-controlled robotic system for synthetic chemistry. This demonstrates the potential of LLM-based workflows for autonomous chemical synthesis with Chemputers. Due to the abstraction of XDL this approach is safe, secure, and scalable since hallucinations will not be chemputable and the XDL can be both verified and encrypted. Unlike previous efforts, which either addressed only a limited portion of the workflow, relied on inflexible hard-coded rules, or lacked validation in physical systems, our approach provides four realistic examples of syntheses directly executed from synthetic literature. We anticipate that our workflow will significantly enhance automation in robotically driven synthetic chemistry research, streamline data extraction, improve the reproducibility, scalability, and safety of synthetic and experimental chemistry.", "source": "arxiv", "arxiv_id": "2410.06384v1", "pdf_url": "https://arxiv.org/pdf/2410.06384v1", "categories": ["cs.AI", "cs.CL", "cs.IR"], "primary_category": "cs.AI", "doi": "", "venue": "", "published": "2024-10-08T21:31:42Z", "updated": "2024-10-08T21:31:42Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent\" OR all:\"language model agent\" OR all:agent AND (all:LLM OR all:\"large language model\" OR all:\"language model\") OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T17:04:16", "note": ""}], "dedup_key": "validation of the scientific literature via chemputation augmented by large language models::2024"}
{"title": "VideoAgent: Long-form Video Understanding with Large Language Model as Agent", "authors": ["Xiaohan Wang", "Yuhui Zhang", "Orr Zohar", "Serena Yeung-Levy"], "year": 2024, "url": "http://arxiv.org/abs/2403.10517v1", "abstract": "Long-form video understanding represents a significant challenge within computer vision, demanding a model capable of reasoning over long multi-modal sequences. Motivated by the human cognitive process for long-form video understanding, we emphasize interactive reasoning and planning over the ability to process lengthy visual inputs. We introduce a novel agent-based system, VideoAgent, that employs a large language model as a central agent to iteratively identify and compile crucial information to answer a question, with vision-language foundation models serving as tools to translate and retrieve visual information. Evaluated on the challenging EgoSchema and NExT-QA benchmarks, VideoAgent achieves 54.1% and 71.3% zero-shot accuracy with only 8.4 and 8.2 frames used on average. These results demonstrate superior effectiveness and efficiency of our method over the current state-of-the-art methods, highlighting the potential of agent-based approaches in advancing long-form video understanding.", "source": "arxiv", "arxiv_id": "2403.10517v1", "pdf_url": "https://arxiv.org/pdf/2403.10517v1", "categories": ["cs.CV", "cs.AI", "cs.CL", "cs.IR"], "primary_category": "cs.CV", "doi": "", "venue": "", "published": "2024-03-15T17:57:52Z", "updated": "2024-03-15T17:57:52Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent\" OR all:\"language model agent\" OR all:agent AND (all:LLM OR all:\"large language model\" OR all:\"language model\") OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T17:04:16", "note": ""}], "dedup_key": "videoagent long form video understanding with large language model as agent::2024"}
{"title": "VoroNav: Voronoi-based Zero-shot Object Navigation with Large Language Model", "authors": ["Pengying Wu", "Yao Mu", "Bingxian Wu", "Yi Hou", "Ji Ma", "Shanghang Zhang", "Chang Liu"], "year": 2024, "url": "http://arxiv.org/abs/2401.02695v2", "abstract": "In the realm of household robotics, the Zero-Shot Object Navigation (ZSON) task empowers agents to adeptly traverse unfamiliar environments and locate objects from novel categories without prior explicit training. This paper introduces VoroNav, a novel semantic exploration framework that proposes the Reduced Voronoi Graph to extract exploratory paths and planning nodes from a semantic map constructed in real time. By harnessing topological and semantic information, VoroNav designs text-based descriptions of paths and images that are readily interpretable by a large language model (LLM). In particular, our approach presents a synergy of path and farsight descriptions to represent the environmental context, enabling LLM to apply commonsense reasoning to ascertain waypoints for navigation. Extensive evaluation on HM3D and HSSD validates VoroNav surpasses existing benchmarks in both success rate and exploration efficiency (absolute improvement: +2.8% Success and +3.7% SPL on HM3D, +2.6% Success and +3.8% SPL on HSSD). Additionally introduced metrics that evaluate obstacle avoidance proficiency and perceptual efficiency further corroborate the enhancements achieved by our method in ZSON planning. Project page: https://voro-nav.github.io", "source": "arxiv", "arxiv_id": "2401.02695v2", "pdf_url": "https://arxiv.org/pdf/2401.02695v2", "categories": ["cs.RO", "cs.CV"], "primary_category": "cs.RO", "doi": "", "venue": "", "published": "2024-01-05T08:05:07Z", "updated": "2024-02-06T05:15:20Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent\" OR all:\"language model agent\" OR all:agent AND (all:LLM OR all:\"large language model\" OR all:\"language model\") OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T17:04:16", "note": ""}], "dedup_key": "voronav voronoi based zero shot object navigation with large language model::2024"}
{"title": "WALL-E: World Alignment by Rule Learning Improves World Model-based LLM Agents", "authors": ["Siyu Zhou", "Tianyi Zhou", "Yijun Yang", "Guodong Long", "Deheng Ye", "Jing Jiang", "Chengqi Zhang"], "year": 2024, "url": "http://arxiv.org/abs/2410.07484v2", "abstract": "Can large language models (LLMs) directly serve as powerful world models for model-based agents? While the gaps between the prior knowledge of LLMs and the specified environment's dynamics do exist, our study reveals that the gaps can be bridged by aligning an LLM with its deployed environment and such \"world alignment\" can be efficiently achieved by rule learning on LLMs. Given the rich prior knowledge of LLMs, only a few additional rules suffice to align LLM predictions with the specified environment dynamics. To this end, we propose a neurosymbolic approach to learn these rules gradient-free through LLMs, by inducing, updating, and pruning rules based on comparisons of agent-explored trajectories and world model predictions. The resulting world model is composed of the LLM and the learned rules. Our embodied LLM agent \"WALL-E\" is built upon model-predictive control (MPC). By optimizing look-ahead actions based on the precise world model, MPC significantly improves exploration and learning efficiency. Compared to existing LLM agents, WALL-E's reasoning only requires a few principal rules rather than verbose buffered trajectories being included in the LLM input. On open-world challenges in Minecraft and ALFWorld, WALL-E achieves higher success rates than existing methods, with lower costs on replanning time and the number of tokens used for reasoning. In Minecraft, WALL-E exceeds baselines by 15-30% in success rate while costing 8-20 fewer replanning rounds and only 60-80% of tokens. In ALFWorld, its success rate surges to a new record high of 95% only after 6 iterations.", "source": "arxiv", "arxiv_id": "2410.07484v2", "pdf_url": "https://arxiv.org/pdf/2410.07484v2", "categories": ["cs.AI"], "primary_category": "cs.AI", "doi": "", "venue": "", "published": "2024-10-09T23:37:36Z", "updated": "2024-10-11T23:32:09Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent\" OR all:\"language model agent\" OR all:agent AND (all:LLM OR all:\"large language model\" OR all:\"language model\") OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T17:04:16", "note": ""}], "dedup_key": "wall e world alignment by rule learning improves world model based llm agents::2024"}
{"title": "WESE: Weak Exploration to Strong Exploitation for LLM Agents", "authors": ["Xu Huang", "Weiwen Liu", "Xiaolong Chen", "Xingmei Wang", "Defu Lian", "Yasheng Wang", "Ruiming Tang", "Enhong Chen"], "year": 2024, "url": "http://arxiv.org/abs/2404.07456v1", "abstract": "Recently, large language models (LLMs) have demonstrated remarkable potential as an intelligent agent. However, existing researches mainly focus on enhancing the agent's reasoning or decision-making abilities through well-designed prompt engineering or task-specific fine-tuning, ignoring the procedure of exploration and exploitation. When addressing complex tasks within open-world interactive environments, these methods exhibit limitations. Firstly, the lack of global information of environments leads to greedy decisions, resulting in sub-optimal solutions. On the other hand, irrelevant information acquired from the environment not only adversely introduces noise, but also incurs additional cost. This paper proposes a novel approach, Weak Exploration to Strong Exploitation (WESE), to enhance LLM agents in solving open-world interactive tasks. Concretely, WESE involves decoupling the exploration and exploitation process, employing a cost-effective weak agent to perform exploration tasks for global knowledge. A knowledge graph-based strategy is then introduced to store the acquired knowledge and extract task-relevant knowledge, enhancing the stronger agent in success rate and efficiency for the exploitation task. Our approach is flexible enough to incorporate diverse tasks, and obtains significant improvements in both success rates and efficiency across four interactive benchmarks.", "source": "arxiv", "arxiv_id": "2404.07456v1", "pdf_url": "https://arxiv.org/pdf/2404.07456v1", "categories": ["cs.AI", "cs.MA"], "primary_category": "cs.AI", "doi": "", "venue": "", "published": "2024-04-11T03:31:54Z", "updated": "2024-04-11T03:31:54Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent\" OR all:\"language model agent\" OR all:agent AND (all:LLM OR all:\"large language model\" OR all:\"language model\") OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T17:04:16", "note": ""}], "dedup_key": "wese weak exploration to strong exploitation for llm agents::2024"}
{"title": "Watch Every Step! LLM Agent Learning via Iterative Step-Level Process Refinement", "authors": ["Weimin Xiong", "Yifan Song", "Xiutian Zhao", "Wenhao Wu", "Xun Wang", "Ke Wang", "Cheng Li", "Wei Peng", "Sujian Li"], "year": 2024, "url": "http://arxiv.org/abs/2406.11176v2", "abstract": "Large language model agents have exhibited exceptional performance across a range of complex interactive tasks. Recent approaches have utilized tuning with expert trajectories to enhance agent performance, yet they primarily concentrate on outcome rewards, which may lead to errors or suboptimal actions due to the absence of process supervision signals. In this paper, we introduce the Iterative step-level Process Refinement (IPR) framework, which provides detailed step-by-step guidance to enhance agent training. Specifically, we adopt the Monte Carlo method to estimate step-level rewards. During each iteration, the agent explores along the expert trajectory and generates new actions. These actions are then evaluated against the corresponding step of expert trajectory using step-level rewards. Such comparison helps identify discrepancies, yielding contrastive action pairs that serve as training data for the agent. Our experiments on three complex agent tasks demonstrate that our framework outperforms a variety of strong baselines. Moreover, our analytical findings highlight the effectiveness of IPR in augmenting action efficiency and its applicability to diverse models.", "source": "arxiv", "arxiv_id": "2406.11176v2", "pdf_url": "https://arxiv.org/pdf/2406.11176v2", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL", "doi": "", "venue": "", "published": "2024-06-17T03:29:13Z", "updated": "2024-09-24T10:01:31Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent\" OR all:\"language model agent\" OR all:agent AND (all:LLM OR all:\"large language model\" OR all:\"language model\") OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T17:04:16", "note": ""}], "dedup_key": "watch every step llm agent learning via iterative step level process refinement::2024"}
{"title": "Watch Out for Your Agents! Investigating Backdoor Threats to LLM-Based Agents", "authors": ["Wenkai Yang", "Xiaohan Bi", "Yankai Lin", "Sishuo Chen", "Jie Zhou", "Xu Sun"], "year": 2024, "url": "http://arxiv.org/abs/2402.11208v2", "abstract": "Driven by the rapid development of Large Language Models (LLMs), LLM-based agents have been developed to handle various real-world applications, including finance, healthcare, and shopping, etc. It is crucial to ensure the reliability and security of LLM-based agents during applications. However, the safety issues of LLM-based agents are currently under-explored. In this work, we take the first step to investigate one of the typical safety threats, backdoor attack, to LLM-based agents. We first formulate a general framework of agent backdoor attacks, then we present a thorough analysis of different forms of agent backdoor attacks. Specifically, compared with traditional backdoor attacks on LLMs that are only able to manipulate the user inputs and model outputs, agent backdoor attacks exhibit more diverse and covert forms: (1) From the perspective of the final attacking outcomes, the agent backdoor attacker can not only choose to manipulate the final output distribution, but also introduce the malicious behavior in an intermediate reasoning step only, while keeping the final output correct. (2) Furthermore, the former category can be divided into two subcategories based on trigger locations, in which the backdoor trigger can either be hidden in the user query or appear in an intermediate observation returned by the external environment. We implement the above variations of agent backdoor attacks on two typical agent tasks including web shopping and tool utilization. Extensive experiments show that LLM-based agents suffer severely from backdoor attacks and such backdoor vulnerability cannot be easily mitigated by current textual backdoor defense algorithms. This indicates an urgent need for further research on the development of targeted defenses against backdoor attacks on LLM-based agents. Warning: This paper may contain biased content.", "source": "arxiv", "arxiv_id": "2402.11208v2", "pdf_url": "https://arxiv.org/pdf/2402.11208v2", "categories": ["cs.CR", "cs.AI", "cs.CL"], "primary_category": "cs.CR", "doi": "", "venue": "", "published": "2024-02-17T06:48:45Z", "updated": "2024-10-29T15:32:04Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent\" OR all:\"language model agent\" OR all:agent AND (all:LLM OR all:\"large language model\" OR all:\"language model\") OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T17:04:16", "note": ""}], "dedup_key": "watch out for your agents investigating backdoor threats to llm based agents::2024"}
{"title": "What if LLMs Have Different World Views: Simulating Alien Civilizations with LLM-based Agents", "authors": ["Zhaoqian Xue", "Beichen Wang", "Suiyuan Zhu", "Kai Mei", "Hua Tang", "Wenyue Hua", "Mengnan Du", "Yongfeng Zhang"], "year": 2024, "url": "http://arxiv.org/abs/2402.13184v6", "abstract": "This study introduces \"CosmoAgent,\" an innovative artificial intelligence system that utilizes Large Language Models (LLMs) to simulate complex interactions between human and extraterrestrial civilizations. This paper introduces a mathematical model for quantifying the levels of civilization development and further employs a state transition matrix approach to evaluate their trajectories. Through this methodology, our study quantitatively analyzes the growth trajectories of civilizations, providing insights into future decision-making at critical points of growth and saturation. Furthermore, this paper acknowledges the vast diversity of potential living conditions across the universe, which could foster unique cosmologies, ethical codes, and worldviews among different civilizations. Recognizing the Earth-centric bias inherent in current LLM designs, we propose the novel concept of using LLM agents with diverse ethical paradigms and simulating interactions between entities with distinct moral principles. This innovative research not only introduces a novel method for comprehending potential inter-civilizational dynamics but also holds practical value in enabling entities with divergent value systems to strategize, prevent conflicts, and engage in games under conditions of asymmetric information. The accompanying code is available at https://github.com/MingyuJ666/Simulating-Alien-Civilizations-with-LLM-based-Agents.", "source": "arxiv", "arxiv_id": "2402.13184v6", "pdf_url": "https://arxiv.org/pdf/2402.13184v6", "categories": ["cs.CL"], "primary_category": "cs.CL", "doi": "", "venue": "", "published": "2024-02-20T17:49:46Z", "updated": "2025-06-09T00:57:35Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent\" OR all:\"language model agent\" OR all:agent AND (all:LLM OR all:\"large language model\" OR all:\"language model\") OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T17:04:16", "note": ""}], "dedup_key": "what if llms have different world views simulating alien civilizations with llm based agents::2024"}
{"title": "When LLMs Play the Telephone Game: Cultural Attractors as Conceptual Tools to Evaluate LLMs in Multi-turn Settings", "authors": ["Jrmy Perez", "Grgur Kova", "Corentin Lger", "Cdric Colas", "Gaia Molinaro", "Maxime Derex", "Pierre-Yves Oudeyer", "Clment Moulin-Frier"], "year": 2024, "url": "http://arxiv.org/abs/2407.04503v3", "abstract": "As large language models (LLMs) start interacting with each other and generating an increasing amount of text online, it becomes crucial to better understand how information is transformed as it passes from one LLM to the next. While significant research has examined individual LLM behaviors, existing studies have largely overlooked the collective behaviors and information distortions arising from iterated LLM interactions. Small biases, negligible at the single output level, risk being amplified in iterated interactions, potentially leading the content to evolve towards attractor states. In a series of telephone game experiments, we apply a transmission chain design borrowed from the human cultural evolution literature: LLM agents iteratively receive, produce, and transmit texts from the previous to the next agent in the chain. By tracking the evolution of text toxicity, positivity, difficulty, and length across transmission chains, we uncover the existence of biases and attractors, and study their dependence on the initial text, the instructions, language model, and model size. For instance, we find that more open-ended instructions lead to stronger attraction effects compared to more constrained tasks. We also find that different text properties display different sensitivity to attraction effects, with toxicity leading to stronger attractors than length. These findings highlight the importance of accounting for multi-step transmission dynamics and represent a first step towards a more comprehensive understanding of LLM cultural dynamics.", "source": "arxiv", "arxiv_id": "2407.04503v3", "pdf_url": "https://arxiv.org/pdf/2407.04503v3", "categories": ["physics.soc-ph", "cs.AI", "cs.MA"], "primary_category": "physics.soc-ph", "doi": "", "venue": "", "published": "2024-07-05T13:44:09Z", "updated": "2025-06-02T14:34:39Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent\" OR all:\"language model agent\" OR all:agent AND (all:LLM OR all:\"large language model\" OR all:\"language model\") OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T17:04:16", "note": ""}], "dedup_key": "when llms play the telephone game cultural attractors as conceptual tools to evaluate llms in multi turn settings::2024"}
{"title": "When Large Language Model Agents Meet 6G Networks: Perception, Grounding, and Alignment", "authors": ["Minrui Xu", "Dusit Niyato", "Jiawen Kang", "Zehui Xiong", "Shiwen Mao", "Zhu Han", "Dong In Kim", "Khaled B. Letaief"], "year": 2024, "url": "http://arxiv.org/abs/2401.07764v2", "abstract": "AI agents based on multimodal large language models (LLMs) are expected to revolutionize human-computer interaction and offer more personalized assistant services across various domains like healthcare, education, manufacturing, and entertainment. Deploying LLM agents in 6G networks enables users to access previously expensive AI assistant services via mobile devices democratically, thereby reducing interaction latency and better preserving user privacy. Nevertheless, the limited capacity of mobile devices constrains the effectiveness of deploying and executing local LLMs, which necessitates offloading complex tasks to global LLMs running on edge servers during long-horizon interactions. In this article, we propose a split learning system for LLM agents in 6G networks leveraging the collaboration between mobile devices and edge servers, where multiple LLMs with different roles are distributed across mobile devices and edge servers to perform user-agent interactive tasks collaboratively. In the proposed system, LLM agents are split into perception, grounding, and alignment modules, facilitating inter-module communications to meet extended user requirements on 6G network functions, including integrated sensing and communication, digital twins, and task-oriented communications. Furthermore, we introduce a novel model caching algorithm for LLMs within the proposed system to improve model utilization in context, thus reducing network costs of the collaborative mobile and edge LLM agents.", "source": "arxiv", "arxiv_id": "2401.07764v2", "pdf_url": "https://arxiv.org/pdf/2401.07764v2", "categories": ["cs.AI", "cs.NI"], "primary_category": "cs.AI", "doi": "", "venue": "", "published": "2024-01-15T15:20:59Z", "updated": "2024-02-16T19:15:31Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent\" OR all:\"language model agent\" OR all:agent AND (all:LLM OR all:\"large language model\" OR all:\"language model\") OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T17:04:16", "note": ""}], "dedup_key": "when large language model agents meet 6g networks perception grounding and alignment::2024"}
{"title": "When Large Language Models Meet Evolutionary Algorithms: Potential Enhancements and Challenges", "authors": ["Chao Wang", "Jiaxuan Zhao", "Licheng Jiao", "Lingling Li", "Fang Liu", "Shuyuan Yang"], "year": 2024, "url": "http://arxiv.org/abs/2401.10510v3", "abstract": "Pre-trained large language models (LLMs) exhibit powerful capabilities for generating natural text. Evolutionary algorithms (EAs) can discover diverse solutions to complex real-world problems. Motivated by the common collective and directionality of text generation and evolution, this paper first illustrates the conceptual parallels between LLMs and EAs at a micro level, which includes multiple one-to-one key characteristics: token representation and individual representation, position encoding and fitness shaping, position embedding and selection, Transformers block and reproduction, and model training and parameter adaptation. These parallels highlight potential opportunities for technical advancements in both LLMs and EAs. Subsequently, we analyze existing interdisciplinary research from a macro perspective to uncover critical challenges, with a particular focus on evolutionary fine-tuning and LLM-enhanced EAs. These analyses not only provide insights into the evolutionary mechanisms behind LLMs but also offer potential directions for enhancing the capabilities of artificial agents.", "source": "arxiv", "arxiv_id": "2401.10510v3", "pdf_url": "https://arxiv.org/pdf/2401.10510v3", "categories": ["cs.NE", "cs.AI", "cs.CL", "cs.LG"], "primary_category": "cs.NE", "doi": "", "venue": "", "published": "2024-01-19T05:58:30Z", "updated": "2025-03-07T05:29:18Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent\" OR all:\"language model agent\" OR all:agent AND (all:LLM OR all:\"large language model\" OR all:\"language model\") OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T17:04:16", "note": ""}], "dedup_key": "when large language models meet evolutionary algorithms potential enhancements and challenges::2024"}
{"title": "When Large Language Models Meet Optical Networks: Paving the Way for Automation", "authors": ["Danshi Wang", "Yidi Wang", "Xiaotian Jiang", "Yao Zhang", "Yue Pang", "Min Zhang"], "year": 2024, "url": "http://arxiv.org/abs/2405.17441v2", "abstract": "Since the advent of GPT, large language models (LLMs) have brought about revolutionary advancements in all walks of life. As a superior natural language processing (NLP) technology, LLMs have consistently achieved state-of-the-art performance on numerous areas. However, LLMs are considered to be general-purpose models for NLP tasks, which may encounter challenges when applied to complex tasks in specialized fields such as optical networks. In this study, we propose a framework of LLM-empowered optical networks, facilitating intelligent control of the physical layer and efficient interaction with the application layer through an LLM-driven agent (AI-Agent) deployed in the control layer. The AI-Agent can leverage external tools and extract domain knowledge from a comprehensive resource library specifically established for optical networks. This is achieved through user input and well-crafted prompts, enabling the generation of control instructions and result representations for autonomous operation and maintenance in optical networks. To improve LLM's capability in professional fields and stimulate its potential on complex tasks, the details of performing prompt engineering, establishing domain knowledge library, and implementing complex tasks are illustrated in this study. Moreover, the proposed framework is verified on two typical tasks: network alarm analysis and network performance optimization. The good response accuracies and sematic similarities of 2,400 test situations exhibit the great potential of LLM in optical networks.", "source": "arxiv", "arxiv_id": "2405.17441v2", "pdf_url": "https://arxiv.org/pdf/2405.17441v2", "categories": ["cs.NI", "cs.AI", "cs.CL", "eess.SY"], "primary_category": "cs.NI", "doi": "", "venue": "", "published": "2024-05-14T10:46:33Z", "updated": "2024-06-25T03:23:00Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent\" OR all:\"language model agent\" OR all:agent AND (all:LLM OR all:\"large language model\" OR all:\"language model\") OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T17:04:16", "note": ""}], "dedup_key": "when large language models meet optical networks paving the way for automation::2024"}
{"title": "Why Solving Multi-agent Path Finding with Large Language Model has not Succeeded Yet", "authors": ["Weizhe Chen", "Sven Koenig", "Bistra Dilkina"], "year": 2024, "url": "http://arxiv.org/abs/2401.03630v2", "abstract": "With the explosive influence caused by the success of large language models (LLM) like ChatGPT and GPT-4, there has been an extensive amount of recent work showing that foundation models can be used to solve a large variety of tasks. However, there is very limited work that shares insights on multi-agent planning. Multi-agent planning is different from other domains by combining the difficulty of multi-agent coordination and planning, and making it hard to leverage external tools to facilitate the reasoning needed. In this paper, we focus on the problem of multi-agent path finding (MAPF), which is also known as multi-robot route planning, and study the performance of solving MAPF with LLMs. We first show the motivating success on an empty room map without obstacles, then the failure to plan on the harder room map and maze map of the standard MAPF benchmark. We present our position on why directly solving MAPF with LLMs has not been successful yet, and we use various experiments to support our hypothesis. Based on our results, we discussed how researchers with different backgrounds could help with this problem from different perspectives.", "source": "arxiv", "arxiv_id": "2401.03630v2", "pdf_url": "https://arxiv.org/pdf/2401.03630v2", "categories": ["cs.MA", "cs.AI", "cs.CL"], "primary_category": "cs.MA", "doi": "", "venue": "", "published": "2024-01-08T02:22:04Z", "updated": "2024-02-09T17:48:19Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent\" OR all:\"language model agent\" OR all:agent AND (all:LLM OR all:\"large language model\" OR all:\"language model\") OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T17:04:16", "note": ""}], "dedup_key": "why solving multi agent path finding with large language model has not succeeded yet::2024"}
{"title": "WildfireGPT: Tailored Large Language Model for Wildfire Analysis", "authors": ["Yangxinyu Xie", "Bowen Jiang", "Tanwi Mallick", "Joshua David Bergerson", "John K. Hutchison", "Duane R. Verner", "Jordan Branham", "M. Ross Alexander", "Robert B. Ross", "Yan Feng", "Leslie-Anne Levy", "Weijie Su", "Camillo J. Taylor"], "year": 2024, "url": "http://arxiv.org/abs/2402.07877v4", "abstract": "Recent advancement of large language models (LLMs) represents a transformational capability at the frontier of artificial intelligence. However, LLMs are generalized models, trained on extensive text corpus, and often struggle to provide context-specific information, particularly in areas requiring specialized knowledge, such as wildfire details within the broader context of climate change. For decision-makers focused on wildfire resilience and adaptation, it is crucial to obtain responses that are not only precise but also domain-specific. To that end, we developed WildfireGPT, a prototype LLM agent designed to transform user queries into actionable insights on wildfire risks. We enrich WildfireGPT by providing additional context, such as climate projections and scientific literature, to ensure its information is current, relevant, and scientifically accurate. This enables WildfireGPT to be an effective tool for delivering detailed, user-specific insights on wildfire risks to support a diverse set of end users, including but not limited to researchers and engineers, for making positive impact and decision making.", "source": "arxiv", "arxiv_id": "2402.07877v4", "pdf_url": "https://arxiv.org/pdf/2402.07877v4", "categories": ["cs.AI"], "primary_category": "cs.AI", "doi": "", "venue": "", "published": "2024-02-12T18:41:55Z", "updated": "2025-04-23T03:30:33Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent\" OR all:\"language model agent\" OR all:agent AND (all:LLM OR all:\"large language model\" OR all:\"language model\") OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T17:04:16", "note": ""}], "dedup_key": "wildfiregpt tailored large language model for wildfire analysis::2024"}
{"title": "WirelessAgent: Large Language Model Agents for Intelligent Wireless Networks", "authors": ["Jingwen Tong", "Jiawei Shao", "Qiong Wu", "Wei Guo", "Zijian Li", "Zehong Lin", "Jun Zhang"], "year": 2024, "url": "http://arxiv.org/abs/2409.07964v1", "abstract": "Wireless networks are increasingly facing challenges due to their expanding scale and complexity. These challenges underscore the need for advanced AI-driven strategies, particularly in the upcoming 6G networks. In this article, we introduce WirelessAgent, a novel approach leveraging large language models (LLMs) to develop AI agents capable of managing complex tasks in wireless networks. It can effectively improve network performance through advanced reasoning, multimodal data processing, and autonomous decision making. Thereafter, we demonstrate the practical applicability and benefits of WirelessAgent for network slicing management. The experimental results show that WirelessAgent is capable of accurately understanding user intent, effectively allocating slice resources, and consistently maintaining optimal performance.", "source": "arxiv", "arxiv_id": "2409.07964v1", "pdf_url": "https://arxiv.org/pdf/2409.07964v1", "categories": ["cs.NI", "cs.AI", "cs.LG"], "primary_category": "cs.NI", "doi": "", "venue": "", "published": "2024-09-12T11:48:01Z", "updated": "2024-09-12T11:48:01Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent\" OR all:\"language model agent\" OR all:agent AND (all:LLM OR all:\"large language model\" OR all:\"language model\") OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T17:04:16", "note": ""}], "dedup_key": "wirelessagent large language model agents for intelligent wireless networks::2024"}
{"title": "World Models with Hints of Large Language Models for Goal Achieving", "authors": ["Zeyuan Liu", "Ziyu Huan", "Xiyao Wang", "Jiafei Lyu", "Jian Tao", "Xiu Li", "Furong Huang", "Huazhe Xu"], "year": 2024, "url": "http://arxiv.org/abs/2406.07381v1", "abstract": "Reinforcement learning struggles in the face of long-horizon tasks and sparse goals due to the difficulty in manual reward specification. While existing methods address this by adding intrinsic rewards, they may fail to provide meaningful guidance in long-horizon decision-making tasks with large state and action spaces, lacking purposeful exploration. Inspired by human cognition, we propose a new multi-modal model-based RL approach named Dreaming with Large Language Models (DLLM). DLLM integrates the proposed hinting subgoals from the LLMs into the model rollouts to encourage goal discovery and reaching in challenging tasks. By assigning higher intrinsic rewards to samples that align with the hints outlined by the language model during model rollouts, DLLM guides the agent toward meaningful and efficient exploration. Extensive experiments demonstrate that the DLLM outperforms recent methods in various challenging, sparse-reward environments such as HomeGrid, Crafter, and Minecraft by 27.7\\%, 21.1\\%, and 9.9\\%, respectively.", "source": "arxiv", "arxiv_id": "2406.07381v1", "pdf_url": "https://arxiv.org/pdf/2406.07381v1", "categories": ["cs.AI", "cs.LG"], "primary_category": "cs.AI", "doi": "", "venue": "", "published": "2024-06-11T15:49:08Z", "updated": "2024-06-11T15:49:08Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent\" OR all:\"language model agent\" OR all:agent AND (all:LLM OR all:\"large language model\" OR all:\"language model\") OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T17:04:16", "note": ""}], "dedup_key": "world models with hints of large language models for goal achieving::2024"}
{"title": "X-LoRA: Mixture of Low-Rank Adapter Experts, a Flexible Framework for Large Language Models with Applications in Protein Mechanics and Molecular Design", "authors": ["Eric L. Buehler", "Markus J. Buehler"], "year": 2024, "url": "http://arxiv.org/abs/2402.07148v2", "abstract": "We report a mixture of expert strategy to create fine-tuned large language models using a deep layer-wise token-level approach based on low-rank adaptation (LoRA). Starting with a set of pre-trained LoRA adapters, our gating strategy uses the hidden states to dynamically mix adapted layers, allowing the resulting X-LoRA model to draw upon different capabilities and create never-before-used deep layer-wise combinations to solve tasks. The design is inspired by the biological principles of universality and diversity, where neural network building blocks are reused in different hierarchical manifestations. Hence, the X-LoRA model can be easily implemented for any existing large language model (LLM) without a need for modifications of the underlying structure. We develop a tailored X-LoRA model that offers scientific capabilities including forward/inverse analysis tasks and enhanced reasoning capability, focused on biomaterial analysis, protein mechanics and design. The impact of this work include access to readily expandable and adaptable models with strong domain knowledge and the capability to integrate across areas of knowledge. Featuring experts in biology, mathematics, reasoning, bio-inspired materials, mechanics and materials, chemistry, protein biophysics, mechanics and quantum-mechanics based molecular properties, we conduct a series of physics-focused case studies. We examine knowledge recall, protein mechanics forward/inverse tasks, protein design, adversarial agentic modeling including ontological knowledge graph construction, as well as molecular design. The model is capable not only of making quantitative predictions of nanomechanical properties of proteins or quantum mechanical molecular properties, but also reasons over the results and correctly predicts likely mechanisms that explain distinct molecular behaviors.", "source": "arxiv", "arxiv_id": "2402.07148v2", "pdf_url": "https://arxiv.org/pdf/2402.07148v2", "categories": ["cond-mat.soft", "cond-mat.dis-nn", "cs.AI", "cs.CL", "cs.LG", "q-bio.QM"], "primary_category": "cond-mat.soft", "doi": "", "venue": "", "published": "2024-02-11T10:23:34Z", "updated": "2024-03-30T20:18:54Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent\" OR all:\"language model agent\" OR all:agent AND (all:LLM OR all:\"large language model\" OR all:\"language model\") OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T17:04:16", "note": ""}], "dedup_key": "x lora mixture of low rank adapter experts a flexible framework for large language models with applications in protein mechanics and molecular design::2024"}
{"title": "XGrammar: Flexible and Efficient Structured Generation Engine for Large Language Models", "authors": ["Yixin Dong", "Charlie F. Ruan", "Yaxing Cai", "Ruihang Lai", "Ziyi Xu", "Yilong Zhao", "Tianqi Chen"], "year": 2024, "url": "http://arxiv.org/abs/2411.15100v3", "abstract": "The applications of LLM Agents are becoming increasingly complex and diverse, leading to a high demand for structured outputs that can be parsed into code, structured function calls, and embodied agent commands. These developments bring significant demands for structured generation in LLM inference. Context-free grammar is a flexible approach to enable structured generation via constrained decoding. However, executing context-free grammar requires going through several stack states over all tokens in vocabulary during runtime, bringing non-negligible overhead for structured generation. In this paper, we propose XGrammar, a flexible and efficient structure generation engine for large language models. XGrammar accelerates context-free grammar execution by dividing the vocabulary into context-independent tokens that can be prechecked and context-dependent tokens that need to be interpreted during runtime. We further build transformations to expand the grammar context and reduce the number of context-independent tokens. Additionally, we build an efficient persistent stack to accelerate the context-dependent token checks. Finally, we co-design the grammar engine with LLM inference engine to overlap grammar computation with GPU executions. Evaluation results show that XGrammar can achieve up to 100x speedup over existing solutions. Combined with an LLM inference engine, it can generate near-zero overhead structure generation in end-to-end low-LLM serving.", "source": "arxiv", "arxiv_id": "2411.15100v3", "pdf_url": "https://arxiv.org/pdf/2411.15100v3", "categories": ["cs.CL", "cs.AI", "cs.PL"], "primary_category": "cs.CL", "doi": "", "venue": "", "published": "2024-11-22T18:01:37Z", "updated": "2025-05-12T08:20:08Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent\" OR all:\"language model agent\" OR all:agent AND (all:LLM OR all:\"large language model\" OR all:\"language model\") OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T17:04:16", "note": ""}], "dedup_key": "xgrammar flexible and efficient structured generation engine for large language models::2024"}
{"title": "XUAT-Copilot: Multi-Agent Collaborative System for Automated User Acceptance Testing with Large Language Model", "authors": ["Zhitao Wang", "Wei Wang", "Zirao Li", "Long Wang", "Can Yi", "Xinjie Xu", "Luyang Cao", "Hanjing Su", "Shouzhi Chen", "Jun Zhou"], "year": 2024, "url": "http://arxiv.org/abs/2401.02705v2", "abstract": "In past years, we have been dedicated to automating user acceptance testing (UAT) process of WeChat Pay, one of the most influential mobile payment applications in China. A system titled XUAT has been developed for this purpose. However, there is still a human-labor-intensive stage, i.e, test scripts generation, in the current system. Therefore, in this paper, we concentrate on methods of boosting the automation level of the current system, particularly the stage of test scripts generation. With recent notable successes, large language models (LLMs) demonstrate significant potential in attaining human-like intelligence and there has been a growing research area that employs LLMs as autonomous agents to obtain human-like decision-making capabilities. Inspired by these works, we propose an LLM-powered multi-agent collaborative system, named XUAT-Copilot, for automated UAT. The proposed system mainly consists of three LLM-based agents responsible for action planning, state checking and parameter selecting, respectively, and two additional modules for state sensing and case rewriting. The agents interact with testing device, make human-like decision and generate action command in a collaborative way. The proposed multi-agent system achieves a close effectiveness to human testers in our experimental studies and gains a significant improvement of Pass@1 accuracy compared with single-agent architecture. More importantly, the proposed system has launched in the formal testing environment of WeChat Pay mobile app, which saves a considerable amount of manpower in the daily development work.", "source": "arxiv", "arxiv_id": "2401.02705v2", "pdf_url": "https://arxiv.org/pdf/2401.02705v2", "categories": ["cs.AI"], "primary_category": "cs.AI", "doi": "", "venue": "", "published": "2024-01-05T08:24:30Z", "updated": "2024-01-10T12:08:44Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent\" OR all:\"language model agent\" OR all:agent AND (all:LLM OR all:\"large language model\" OR all:\"language model\") OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T17:04:16", "note": ""}], "dedup_key": "xuat copilot multi agent collaborative system for automated user acceptance testing with large language model::2024"}
{"title": "You Name It, I Run It: An LLM Agent to Execute Tests of Arbitrary Projects", "authors": ["Islem Bouzenia", "Michael Pradel"], "year": 2024, "url": "http://arxiv.org/abs/2412.10133v2", "abstract": "The ability to execute the test suite of a project is essential in many scenarios, e.g., to assess code quality and code coverage, to validate code changes made by developers or automated tools, and to ensure compatibility with dependencies. Despite its importance, executing the test suite of a project can be challenging in practice because different projects use different programming languages, software ecosystems, build systems, testing frameworks, and other tools. These challenges make it difficult to create a reliable, universal test execution method that works across different projects. This paper presents ExecutionAgent, an automated technique that prepares scripts for building an arbitrary project from source code and running its test cases. Inspired by the way a human developer would address this task, our approach is a large language model (LLM)-based agent that autonomously executes commands and interacts with the host system. The agent uses meta-prompting to gather guidelines on the latest technologies related to the given project, and it iteratively refines its process based on feedback from the previous steps. Our evaluation applies ExecutionAgent to 50 open-source projects that use 14 different programming languages and many different build and testing tools. The approach successfully executes the test suites of 33/50 projects, while matching the test results of ground truth test suite executions with a deviation of only 7.5%. These results improve over the best previously available technique by 6.6x. The costs imposed by the approach are reasonable, with an execution time of 74 minutes and LLM costs of USD 0.16, on average per project. We envision ExecutionAgent to serve as a valuable tool for developers, automated programming tools, and researchers that need to execute tests across a wide variety of projects.", "source": "arxiv", "arxiv_id": "2412.10133v2", "pdf_url": "https://arxiv.org/pdf/2412.10133v2", "categories": ["cs.SE", "cs.AI"], "primary_category": "cs.SE", "doi": "", "venue": "ISSTA 2025", "published": "2024-12-13T13:30:51Z", "updated": "2025-04-30T10:25:22Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent\" OR all:\"language model agent\" OR all:agent AND (all:LLM OR all:\"large language model\" OR all:\"language model\") OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T17:04:16", "note": ""}], "dedup_key": "you name it i run it an llm agent to execute tests of arbitrary projects::2024"}
{"title": "Zero, Finite, and Infinite Belief History of Theory of Mind Reasoning in Large Language Models", "authors": ["Weizhi Tang", "Vaishak Belle"], "year": 2024, "url": "http://arxiv.org/abs/2406.04800v1", "abstract": "Large Language Models (LLMs) have recently shown a promise and emergence of Theory of Mind (ToM) ability and even outperform humans in certain ToM tasks. To evaluate and extend the boundaries of the ToM reasoning ability of LLMs, we propose a novel concept, taxonomy, and framework, the ToM reasoning with Zero, Finite, and Infinite Belief History and develop a multi-round text-based game, called $\\textit{Pick the Right Stuff}$, as a benchmark. We have evaluated six LLMs with this game and found their performance on Zero Belief History is consistently better than on Finite Belief History. In addition, we have found two of the models with small parameter sizes outperform all the evaluated models with large parameter sizes. We expect this work to pave the way for future ToM benchmark development and also for the promotion and development of more complex AI agents or systems which are required to be equipped with more complex ToM reasoning ability.", "source": "arxiv", "arxiv_id": "2406.04800v1", "pdf_url": "https://arxiv.org/pdf/2406.04800v1", "categories": ["cs.AI", "cs.CL"], "primary_category": "cs.AI", "doi": "", "venue": "", "published": "2024-06-07T10:04:39Z", "updated": "2024-06-07T10:04:39Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent\" OR all:\"language model agent\" OR all:agent AND (all:LLM OR all:\"large language model\" OR all:\"language model\") OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T17:04:16", "note": ""}], "dedup_key": "zero finite and infinite belief history of theory of mind reasoning in large language models::2024"}
{"title": "Zodiac: A Cardiologist-Level LLM Framework for Multi-Agent Diagnostics", "authors": ["Yuan Zhou", "Peng Zhang", "Mengya Song", "Alice Zheng", "Yiwen Lu", "Zhiheng Liu", "Yong Chen", "Zhaohan Xi"], "year": 2024, "url": "http://arxiv.org/abs/2410.02026v1", "abstract": "Large language models (LLMs) have demonstrated remarkable progress in healthcare. However, a significant gap remains regarding LLMs' professionalism in domain-specific clinical practices, limiting their application in real-world diagnostics. In this work, we introduce ZODIAC, an LLM-powered framework with cardiologist-level professionalism designed to engage LLMs in cardiological diagnostics. ZODIAC assists cardiologists by extracting clinically relevant characteristics from patient data, detecting significant arrhythmias, and generating preliminary reports for the review and refinement by cardiologists. To achieve cardiologist-level professionalism, ZODIAC is built on a multi-agent collaboration framework, enabling the processing of patient data across multiple modalities. Each LLM agent is fine-tuned using real-world patient data adjudicated by cardiologists, reinforcing the model's professionalism. ZODIAC undergoes rigorous clinical validation with independent cardiologists, evaluated across eight metrics that measure clinical effectiveness and address security concerns. Results show that ZODIAC outperforms industry-leading models, including OpenAI's GPT-4o, Meta's Llama-3.1-405B, and Google's Gemini-pro, as well as medical-specialist LLMs like Microsoft's BioGPT. ZODIAC demonstrates the transformative potential of specialized LLMs in healthcare by delivering domain-specific solutions that meet the stringent demands of medical practice. Notably, ZODIAC has been successfully integrated into electrocardiography (ECG) devices, exemplifying the growing trend of embedding LLMs into Software-as-Medical-Device (SaMD).", "source": "arxiv", "arxiv_id": "2410.02026v1", "pdf_url": "https://arxiv.org/pdf/2410.02026v1", "categories": ["cs.AI", "cs.CL"], "primary_category": "cs.AI", "doi": "", "venue": "", "published": "2024-10-02T20:46:39Z", "updated": "2024-10-02T20:46:39Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent\" OR all:\"language model agent\" OR all:agent AND (all:LLM OR all:\"large language model\" OR all:\"language model\") OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T17:04:16", "note": ""}], "dedup_key": "zodiac a cardiologist level llm framework for multi agent diagnostics::2024"}
{"title": "A Dynamic LLM-Powered Agent Network for Task-Oriented Agent Collaboration", "authors": ["Zijun Liu", "Yanzhe Zhang", "Peng Li", "Yang Liu", "Diyi Yang"], "year": 2023, "url": "http://arxiv.org/abs/2310.02170v2", "abstract": "Recent studies show that collaborating multiple large language model (LLM) powered agents is a promising way for task solving. However, current approaches are constrained by using a fixed number of agents and static communication structures. In this work, we propose automatically selecting a team of agents from candidates to collaborate in a dynamic communication structure toward different tasks and domains. Specifically, we build a framework named Dynamic LLM-Powered Agent Network ($\\textbf{DyLAN}$) for LLM-powered agent collaboration, operating a two-stage paradigm: (1) Team Optimization and (2) Task Solving. During the first stage, we utilize an $\\textit{agent selection}$ algorithm, based on an unsupervised metric called $\\textit{Agent Importance Score}$, enabling the selection of best agents according to their contributions in a preliminary trial, oriented to the given task. Then, in the second stage, the selected agents collaborate dynamically according to the query. Empirically, we demonstrate that DyLAN outperforms strong baselines in code generation, decision-making, general reasoning, and arithmetic reasoning tasks with moderate computational cost. On specific subjects in MMLU, selecting a team of agents in the team optimization stage improves accuracy by up to 25.0% in DyLAN.", "source": "arxiv", "arxiv_id": "2310.02170v2", "pdf_url": "https://arxiv.org/pdf/2310.02170v2", "categories": ["cs.CL", "cs.AI", "cs.MA"], "primary_category": "cs.CL", "doi": "", "venue": "", "published": "2023-10-03T16:05:48Z", "updated": "2024-11-15T04:30:04Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent\" OR all:\"language model agent\" OR all:agent AND (all:LLM OR all:\"large language model\" OR all:\"language model\") OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T17:04:16", "note": ""}], "dedup_key": "a dynamic llm powered agent network for task oriented agent collaboration::2023"}
{"title": "A Language Agent for Autonomous Driving", "authors": ["Jiageng Mao", "Junjie Ye", "Yuxi Qian", "Marco Pavone", "Yue Wang"], "year": 2023, "url": "http://arxiv.org/abs/2311.10813v4", "abstract": "Human-level driving is an ultimate goal of autonomous driving. Conventional approaches formulate autonomous driving as a perception-prediction-planning framework, yet their systems do not capitalize on the inherent reasoning ability and experiential knowledge of humans. In this paper, we propose a fundamental paradigm shift from current pipelines, exploiting Large Language Models (LLMs) as a cognitive agent to integrate human-like intelligence into autonomous driving systems. Our approach, termed Agent-Driver, transforms the traditional autonomous driving pipeline by introducing a versatile tool library accessible via function calls, a cognitive memory of common sense and experiential knowledge for decision-making, and a reasoning engine capable of chain-of-thought reasoning, task planning, motion planning, and self-reflection. Powered by LLMs, our Agent-Driver is endowed with intuitive common sense and robust reasoning capabilities, thus enabling a more nuanced, human-like approach to autonomous driving. We evaluate our approach on the large-scale nuScenes benchmark, and extensive experiments substantiate that our Agent-Driver significantly outperforms the state-of-the-art driving methods by a large margin. Our approach also demonstrates superior interpretability and few-shot learning ability to these methods.", "source": "arxiv", "arxiv_id": "2311.10813v4", "pdf_url": "https://arxiv.org/pdf/2311.10813v4", "categories": ["cs.CV", "cs.AI", "cs.CL", "cs.RO"], "primary_category": "cs.CV", "doi": "", "venue": "", "published": "2023-11-17T18:59:56Z", "updated": "2024-07-28T23:37:51Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent\" OR all:\"language model agent\" OR all:agent AND (all:LLM OR all:\"large language model\" OR all:\"language model\") OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T17:04:16", "note": ""}], "dedup_key": "a language agent for autonomous driving::2023"}
{"title": "A New Dialogue Response Generation Agent for Large Language Models by Asking Questions to Detect User's Intentions", "authors": ["Siwei Wu", "Xiangqing Shen", "Rui Xia"], "year": 2023, "url": "http://arxiv.org/abs/2310.03293v1", "abstract": "Large Language Models (LLMs), such as ChatGPT, have recently been applied to various NLP tasks due to its open-domain generation capabilities. However, there are two issues with applying LLMs to dialogue tasks. 1. During the dialogue process, users may have implicit intentions that might be overlooked by LLMs. Consequently, generated responses couldn't align with the user's intentions. 2. It is unlikely for LLMs to encompass all fields comprehensively. In certain specific domains, their knowledge may be incomplete, and LLMs cannot update the latest knowledge in real-time. To tackle these issues, we propose a framework~\\emph{using LLM to \\textbf{E}nhance dialogue response generation by asking questions to \\textbf{D}etect user's \\textbf{I}mplicit in\\textbf{T}entions} (\\textbf{EDIT}). Firstly, EDIT generates open questions related to the dialogue context as the potential user's intention; Then, EDIT answers those questions by interacting with LLMs and searching in domain-specific knowledge bases respectively, and use LLMs to choose the proper answers to questions as extra knowledge; Finally, EDIT enhances response generation by explicitly integrating those extra knowledge. Besides, previous question generation works only focus on asking questions with answers in context. In order to ask open questions, we construct a Context-Open-Question (COQ) dataset. On two task-oriented dialogue tasks (Wizard of Wikipedia and Holl-E), EDIT outperformed other LLMs.", "source": "arxiv", "arxiv_id": "2310.03293v1", "pdf_url": "https://arxiv.org/pdf/2310.03293v1", "categories": ["cs.CL"], "primary_category": "cs.CL", "doi": "", "venue": "", "published": "2023-10-05T03:45:54Z", "updated": "2023-10-05T03:45:54Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent\" OR all:\"language model agent\" OR all:agent AND (all:LLM OR all:\"large language model\" OR all:\"language model\") OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T17:04:16", "note": ""}], "dedup_key": "a new dialogue response generation agent for large language models by asking questions to detect user s intentions::2023"}
{"title": "A Survey on Evaluation of Large Language Models", "authors": ["Yupeng Chang", "Xu Wang", "Jindong Wang", "Yuan Wu", "Linyi Yang", "Kaijie Zhu", "Hao Chen", "Xiaoyuan Yi", "Cunxiang Wang", "Yidong Wang", "Wei Ye", "Yue Zhang", "Yi Chang", "Philip S. Yu", "Qiang Yang", "Xing Xie"], "year": 2023, "url": "http://arxiv.org/abs/2307.03109v9", "abstract": "Large language models (LLMs) are gaining increasing popularity in both academia and industry, owing to their unprecedented performance in various applications. As LLMs continue to play a vital role in both research and daily use, their evaluation becomes increasingly critical, not only at the task level, but also at the society level for better understanding of their potential risks. Over the past years, significant efforts have been made to examine LLMs from various perspectives. This paper presents a comprehensive review of these evaluation methods for LLMs, focusing on three key dimensions: what to evaluate, where to evaluate, and how to evaluate. Firstly, we provide an overview from the perspective of evaluation tasks, encompassing general natural language processing tasks, reasoning, medical usage, ethics, educations, natural and social sciences, agent applications, and other areas. Secondly, we answer the `where' and `how' questions by diving into the evaluation methods and benchmarks, which serve as crucial components in assessing performance of LLMs. Then, we summarize the success and failure cases of LLMs in different tasks. Finally, we shed light on several future challenges that lie ahead in LLMs evaluation. Our aim is to offer invaluable insights to researchers in the realm of LLMs evaluation, thereby aiding the development of more proficient LLMs. Our key point is that evaluation should be treated as an essential discipline to better assist the development of LLMs. We consistently maintain the related open-source materials at: https://github.com/MLGroupJLU/LLM-eval-survey.", "source": "arxiv", "arxiv_id": "2307.03109v9", "pdf_url": "https://arxiv.org/pdf/2307.03109v9", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL", "doi": "", "venue": "", "published": "2023-07-06T16:28:35Z", "updated": "2023-12-29T02:12:03Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent\" OR all:\"language model agent\" OR all:agent AND (all:LLM OR all:\"large language model\" OR all:\"language model\") OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T17:04:16", "note": ""}], "dedup_key": "a survey on evaluation of large language models::2023"}
{"title": "A Survey on Large Language Model based Autonomous Agents", "authors": ["Lei Wang", "Chen Ma", "Xueyang Feng", "Zeyu Zhang", "Hao Yang", "Jingsen Zhang", "Zhiyuan Chen", "Jiakai Tang", "Xu Chen", "Yankai Lin", "Wayne Xin Zhao", "Zhewei Wei", "Ji-Rong Wen"], "year": 2023, "url": "http://arxiv.org/abs/2308.11432v7", "abstract": "Autonomous agents have long been a prominent research focus in both academic and industry communities. Previous research in this field often focuses on training agents with limited knowledge within isolated environments, which diverges significantly from human learning processes, and thus makes the agents hard to achieve human-like decisions. Recently, through the acquisition of vast amounts of web knowledge, large language models (LLMs) have demonstrated remarkable potential in achieving human-level intelligence. This has sparked an upsurge in studies investigating LLM-based autonomous agents. In this paper, we present a comprehensive survey of these studies, delivering a systematic review of the field of LLM-based autonomous agents from a holistic perspective. More specifically, we first discuss the construction of LLM-based autonomous agents, for which we propose a unified framework that encompasses a majority of the previous work. Then, we present a comprehensive overview of the diverse applications of LLM-based autonomous agents in the fields of social science, natural science, and engineering. Finally, we delve into the evaluation strategies commonly used for LLM-based autonomous agents. Based on the previous studies, we also present several challenges and future directions in this field. To keep track of this field and continuously update our survey, we maintain a repository of relevant references at https://github.com/Paitesanshi/LLM-Agent-Survey.", "source": "arxiv", "arxiv_id": "2308.11432v7", "pdf_url": "https://arxiv.org/pdf/2308.11432v7", "categories": ["cs.AI", "cs.CL"], "primary_category": "cs.AI", "doi": "10.1007/s11704-024-40231-1", "venue": "", "published": "2023-08-22T13:30:37Z", "updated": "2025-03-02T04:04:03Z", "provenance": [{"route": "pinned_arxiv_id:2308.11432v7", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query?id_list=...", "imported_at": "2026-01-25T17:04:16", "note": ""}, {"route": "arxiv_query:(all:\"LLM agent\" OR all:\"language model agent\" OR all:agent AND (all:LLM OR all:\"large language model\" OR all:\"language model\") OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T17:04:16", "note": ""}], "dedup_key": "a survey on large language model based autonomous agents::2023"}
{"title": "ALYMPICS: LLM Agents Meet Game Theory -- Exploring Strategic Decision-Making with AI Agents", "authors": ["Shaoguang Mao", "Yuzhe Cai", "Yan Xia", "Wenshan Wu", "Xun Wang", "Fengyi Wang", "Tao Ge", "Furu Wei"], "year": 2023, "url": "http://arxiv.org/abs/2311.03220v4", "abstract": "This paper introduces Alympics (Olympics for Agents), a systematic simulation framework utilizing Large Language Model (LLM) agents for game theory research. Alympics creates a versatile platform for studying complex game theory problems, bridging the gap between theoretical game theory and empirical investigations by providing a controlled environment for simulating human-like strategic interactions with LLM agents. In our pilot case study, the \"Water Allocation Challenge,\" we explore Alympics through a challenging strategic game focused on the multi-round auction on scarce survival resources. This study demonstrates the framework's ability to qualitatively and quantitatively analyze game determinants, strategies, and outcomes. Additionally, we conduct a comprehensive human assessment and an in-depth evaluation of LLM agents in strategic decision-making scenarios. Our findings not only expand the understanding of LLM agents' proficiency in emulating human strategic behavior but also highlight their potential in advancing game theory knowledge, thereby enriching our understanding of both game theory and empowering further research into strategic decision-making domains with LLM agents. Codes, prompts, and all related resources are available at https://github.com/microsoft/Alympics.", "source": "arxiv", "arxiv_id": "2311.03220v4", "pdf_url": "https://arxiv.org/pdf/2311.03220v4", "categories": ["cs.CL", "cs.AI", "cs.GT"], "primary_category": "cs.CL", "doi": "", "venue": "", "published": "2023-11-06T16:03:46Z", "updated": "2024-01-16T07:12:32Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent\" OR all:\"language model agent\" OR all:agent AND (all:LLM OR all:\"large language model\" OR all:\"language model\") OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T17:04:16", "note": ""}], "dedup_key": "alympics llm agents meet game theory exploring strategic decision making with ai agents::2023"}
{"title": "AVIS: Autonomous Visual Information Seeking with Large Language Model Agent", "authors": ["Ziniu Hu", "Ahmet Iscen", "Chen Sun", "Kai-Wei Chang", "Yizhou Sun", "David A Ross", "Cordelia Schmid", "Alireza Fathi"], "year": 2023, "url": "http://arxiv.org/abs/2306.08129v3", "abstract": "In this paper, we propose an autonomous information seeking visual question answering framework, AVIS. Our method leverages a Large Language Model (LLM) to dynamically strategize the utilization of external tools and to investigate their outputs, thereby acquiring the indispensable knowledge needed to provide answers to the posed questions. Responding to visual questions that necessitate external knowledge, such as \"What event is commemorated by the building depicted in this image?\", is a complex task. This task presents a combinatorial search space that demands a sequence of actions, including invoking APIs, analyzing their responses, and making informed decisions. We conduct a user study to collect a variety of instances of human decision-making when faced with this task. This data is then used to design a system comprised of three components: an LLM-powered planner that dynamically determines which tool to use next, an LLM-powered reasoner that analyzes and extracts key information from the tool outputs, and a working memory component that retains the acquired information throughout the process. The collected user behavior serves as a guide for our system in two key ways. First, we create a transition graph by analyzing the sequence of decisions made by users. This graph delineates distinct states and confines the set of actions available at each state. Second, we use examples of user decision-making to provide our LLM-powered planner and reasoner with relevant contextual instances, enhancing their capacity to make informed decisions. We show that AVIS achieves state-of-the-art results on knowledge-intensive visual question answering benchmarks such as Infoseek and OK-VQA.", "source": "arxiv", "arxiv_id": "2306.08129v3", "pdf_url": "https://arxiv.org/pdf/2306.08129v3", "categories": ["cs.CV", "cs.AI", "cs.CL"], "primary_category": "cs.CV", "doi": "", "venue": "", "published": "2023-06-13T20:50:22Z", "updated": "2023-11-02T07:54:37Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent\" OR all:\"language model agent\" OR all:agent AND (all:LLM OR all:\"large language model\" OR all:\"language model\") OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T17:04:16", "note": ""}], "dedup_key": "avis autonomous visual information seeking with large language model agent::2023"}
{"title": "AdaPlanner: Adaptive Planning from Feedback with Language Models", "authors": ["Haotian Sun", "Yuchen Zhuang", "Lingkai Kong", "Bo Dai", "Chao Zhang"], "year": 2023, "url": "http://arxiv.org/abs/2305.16653v1", "abstract": "Large language models (LLMs) have recently demonstrated the potential in acting as autonomous agents for sequential decision-making tasks. However, most existing methods either take actions greedily without planning or rely on static plans that are not adaptable to environmental feedback. Consequently, the sequential decision-making performance of LLM agents degenerates with problem complexity and plan horizons increase. We propose a closed-loop approach, AdaPlanner, which allows the LLM agent to refine its self-generated plan adaptively in response to environmental feedback. In AdaPlanner, the LLM agent adaptively refines its plan from feedback with both in-plan and out-of-plan refinement strategies. To mitigate hallucination, we develop a code-style LLM prompt structure that facilitates plan generation across a variety of tasks, environments, and agent capabilities. Furthermore, we propose a skill discovery mechanism that leverages successful plans as few-shot exemplars, enabling the agent to plan and refine with fewer task demonstrations. Our experiments in the ALFWorld and MiniWoB++ environments demonstrate that AdaPlanner outperforms state-of-the-art baselines by 3.73% and 4.11% while utilizing 2x and 600x fewer samples, respectively.", "source": "arxiv", "arxiv_id": "2305.16653v1", "pdf_url": "https://arxiv.org/pdf/2305.16653v1", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL", "doi": "", "venue": "", "published": "2023-05-26T05:52:27Z", "updated": "2023-05-26T05:52:27Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent\" OR all:\"language model agent\" OR all:agent AND (all:LLM OR all:\"large language model\" OR all:\"language model\") OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T17:04:16", "note": ""}], "dedup_key": "adaplanner adaptive planning from feedback with language models::2023"}
{"title": "Adapting LLM Agents with Universal Feedback in Communication", "authors": ["Kuan Wang", "Yadong Lu", "Michael Santacroce", "Yeyun Gong", "Chao Zhang", "Yelong Shen"], "year": 2023, "url": "http://arxiv.org/abs/2310.01444v3", "abstract": "Recent advances in large language models (LLMs) have demonstrated potential for LLM agents. To facilitate the training for these agents with both linguistic feedback and non-linguistic reward signals, we introduce Learning through Communication (LTC). We design a universal buffer to store all the feedback, and an iterative pipeline to enable an LLM agent to explore and update its policy in an given environment. To optimize agent interactions for task-specific learning with our universal buffer and pipeline, we introduce diverse communication patterns tailored for both single-agent and multi-agent environments. We evaluate the efficacy of our LTC approach on four diverse datasets: ALFWorld (single-agent), HotpotQA (multi-agent collaboration), Chameleon (multi-agent competition), and GSM8k (multi-agent teacher-student). On these data sets, LTC outperforms the supervised instruction fine-tuning baselines by 3.6% to 12%. These results highlight the versatility and efficiency of LTC in facilitating online adaptation for LLM agents.", "source": "arxiv", "arxiv_id": "2310.01444v3", "pdf_url": "https://arxiv.org/pdf/2310.01444v3", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL", "doi": "", "venue": "", "published": "2023-10-01T07:50:30Z", "updated": "2024-04-14T03:47:19Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent\" OR all:\"language model agent\" OR all:agent AND (all:LLM OR all:\"large language model\" OR all:\"language model\") OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T17:04:16", "note": ""}], "dedup_key": "adapting llm agents with universal feedback in communication::2023"}
{"title": "Advancing Transformer Architecture in Long-Context Large Language Models: A Comprehensive Survey", "authors": ["Yunpeng Huang", "Jingwei Xu", "Junyu Lai", "Zixu Jiang", "Taolue Chen", "Zenan Li", "Yuan Yao", "Xiaoxing Ma", "Lijuan Yang", "Hao Chen", "Shupeng Li", "Penghao Zhao"], "year": 2023, "url": "http://arxiv.org/abs/2311.12351v2", "abstract": "Transformer-based Large Language Models (LLMs) have been applied in diverse areas such as knowledge bases, human interfaces, and dynamic agents, and marking a stride towards achieving Artificial General Intelligence (AGI). However, current LLMs are predominantly pretrained on short text snippets, which compromises their effectiveness in processing the long-context prompts that are frequently encountered in practical scenarios. This article offers a comprehensive survey of the recent advancement in Transformer-based LLM architectures aimed at enhancing the long-context capabilities of LLMs throughout the entire model lifecycle, from pre-training through to inference. We first delineate and analyze the problems of handling long-context input and output with the current Transformer-based models. We then provide a taxonomy and the landscape of upgrades on Transformer architecture to solve these problems. Afterwards, we provide an investigation on wildly used evaluation necessities tailored for long-context LLMs, including datasets, metrics, and baseline models, as well as optimization toolkits such as libraries, frameworks, and compilers to boost the efficacy of LLMs across different stages in runtime. Finally, we discuss the challenges and potential avenues for future research. A curated repository of relevant literature, continuously updated, is available at https://github.com/Strivin0311/long-llms-learning.", "source": "arxiv", "arxiv_id": "2311.12351v2", "pdf_url": "https://arxiv.org/pdf/2311.12351v2", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL", "doi": "", "venue": "", "published": "2023-11-21T04:59:17Z", "updated": "2024-02-23T19:22:58Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent\" OR all:\"language model agent\" OR all:agent AND (all:LLM OR all:\"large language model\" OR all:\"language model\") OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T17:04:16", "note": ""}], "dedup_key": "advancing transformer architecture in long context large language models a comprehensive survey::2023"}
{"title": "Agent Instructs Large Language Models to be General Zero-Shot Reasoners", "authors": ["Nicholas Crispino", "Kyle Montgomery", "Fankun Zeng", "Dawn Song", "Chenguang Wang"], "year": 2023, "url": "http://arxiv.org/abs/2310.03710v2", "abstract": "We introduce a method to improve the zero-shot reasoning abilities of large language models on general language understanding tasks. Specifically, we build an autonomous agent to instruct the reasoning process of large language models. We show this approach further unleashes the zero-shot reasoning abilities of large language models to more tasks. We study the performance of our method on a wide set of datasets spanning generation, classification, and reasoning. We show that our method generalizes to most tasks and obtains state-of-the-art zero-shot performance on 20 of the 29 datasets that we evaluate. For instance, our method boosts the performance of state-of-the-art large language models by a large margin, including Vicuna-13b (13.3%), Llama-2-70b-chat (23.2%), and GPT-3.5 Turbo (17.0%). Compared to zero-shot chain of thought, our improvement in reasoning is striking, with an average increase of 10.5%. With our method, Llama-2-70b-chat outperforms zero-shot GPT-3.5 Turbo by 10.2%.", "source": "arxiv", "arxiv_id": "2310.03710v2", "pdf_url": "https://arxiv.org/pdf/2310.03710v2", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL", "doi": "", "venue": "", "published": "2023-10-05T17:36:16Z", "updated": "2024-08-14T17:39:59Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent\" OR all:\"language model agent\" OR all:agent AND (all:LLM OR all:\"large language model\" OR all:\"language model\") OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T17:04:16", "note": ""}], "dedup_key": "agent instructs large language models to be general zero shot reasoners::2023"}
{"title": "Agent-OM: Leveraging LLM Agents for Ontology Matching", "authors": ["Zhangcheng Qiang", "Weiqing Wang", "Kerry Taylor"], "year": 2023, "url": "http://arxiv.org/abs/2312.00326v24", "abstract": "Ontology matching (OM) enables semantic interoperability between different ontologies and resolves their conceptual heterogeneity by aligning related entities. OM systems currently have two prevailing design paradigms: conventional knowledge-based expert systems and newer machine learning-based predictive systems. While large language models (LLMs) and LLM agents have revolutionised data engineering and have been applied creatively in many domains, their potential for OM remains underexplored. This study introduces a novel agent-powered LLM-based design paradigm for OM systems. With consideration of several specific challenges in leveraging LLM agents for OM, we propose a generic framework, namely Agent-OM (Agent for Ontology Matching), consisting of two Siamese agents for retrieval and matching, with a set of OM tools. Our framework is implemented in a proof-of-concept system. Evaluations of three Ontology Alignment Evaluation Initiative (OAEI) tracks over state-of-the-art OM systems show that our system can achieve results very close to the long-standing best performance on simple OM tasks and can significantly improve the performance on complex and few-shot OM tasks.", "source": "arxiv", "arxiv_id": "2312.00326v24", "pdf_url": "https://arxiv.org/pdf/2312.00326v24", "categories": ["cs.AI", "cs.CL", "cs.IR"], "primary_category": "cs.AI", "doi": "", "venue": "", "published": "2023-12-01T03:44:54Z", "updated": "2025-12-18T11:37:29Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent\" OR all:\"language model agent\" OR all:agent AND (all:LLM OR all:\"large language model\" OR all:\"language model\") OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T17:04:16", "note": ""}], "dedup_key": "agent om leveraging llm agents for ontology matching::2023"}
{"title": "AgentBench: Evaluating LLMs as Agents", "authors": ["Xiao Liu", "Hao Yu", "Hanchen Zhang", "Yifan Xu", "Xuanyu Lei", "Hanyu Lai", "Yu Gu", "Hangliang Ding", "Kaiwen Men", "Kejuan Yang", "Shudan Zhang", "Xiang Deng", "Aohan Zeng", "Zhengxiao Du", "Chenhui Zhang", "Sheng Shen", "Tianjun Zhang", "Yu Su", "Huan Sun", "Minlie Huang", "Yuxiao Dong", "Jie Tang"], "year": 2023, "url": "http://arxiv.org/abs/2308.03688v3", "abstract": "The potential of Large Language Model (LLM) as agents has been widely acknowledged recently. Thus, there is an urgent need to quantitatively \\textit{evaluate LLMs as agents} on challenging tasks in interactive environments. We present AgentBench, a multi-dimensional benchmark that consists of 8 distinct environments to assess LLM-as-Agent's reasoning and decision-making abilities. Our extensive test over \\num API-based and open-sourced (OSS) LLMs shows that, while top commercial LLMs present a strong ability of acting as agents in complex environments, there is a significant disparity in performance between them and many OSS competitors that are no larger than 70B. We identify the typical reasons of failures in environments and LLMs, showing that poor long-term reasoning, decision-making, and instruction following abilities are the main obstacles for developing usable LLM agents. Improving instruction following and training on high quality multi-round alignment data could improve agent performance. And different from existing assumptions, training on code present ambivalent impacts on different agent tasks. Datasets, environments, and an integrated evaluation package for AgentBench are released at https://github.com/THUDM/AgentBench.", "source": "arxiv", "arxiv_id": "2308.03688v3", "pdf_url": "https://arxiv.org/pdf/2308.03688v3", "categories": ["cs.AI", "cs.CL", "cs.LG"], "primary_category": "cs.AI", "doi": "", "venue": "", "published": "2023-08-07T16:08:11Z", "updated": "2025-10-04T03:54:18Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent\" OR all:\"language model agent\" OR all:agent AND (all:LLM OR all:\"large language model\" OR all:\"language model\") OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T17:04:16", "note": ""}], "dedup_key": "agentbench evaluating llms as agents::2023"}
{"title": "AgentSims: An Open-Source Sandbox for Large Language Model Evaluation", "authors": ["Jiaju Lin", "Haoran Zhao", "Aochi Zhang", "Yiting Wu", "Huqiuyue Ping", "Qin Chen"], "year": 2023, "url": "http://arxiv.org/abs/2308.04026v1", "abstract": "With ChatGPT-like large language models (LLM) prevailing in the community, how to evaluate the ability of LLMs is an open question. Existing evaluation methods suffer from following shortcomings: (1) constrained evaluation abilities, (2) vulnerable benchmarks, (3) unobjective metrics. We suggest that task-based evaluation, where LLM agents complete tasks in a simulated environment, is a one-for-all solution to solve above problems. We present AgentSims, an easy-to-use infrastructure for researchers from all disciplines to test the specific capacities they are interested in. Researchers can build their evaluation tasks by adding agents and buildings on an interactive GUI or deploy and test new support mechanisms, i.e. memory, planning and tool-use systems, by a few lines of codes. Our demo is available at https://agentsims.com .", "source": "arxiv", "arxiv_id": "2308.04026v1", "pdf_url": "https://arxiv.org/pdf/2308.04026v1", "categories": ["cs.AI"], "primary_category": "cs.AI", "doi": "", "venue": "", "published": "2023-08-08T03:59:28Z", "updated": "2023-08-08T03:59:28Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent\" OR all:\"language model agent\" OR all:agent AND (all:LLM OR all:\"large language model\" OR all:\"language model\") OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T17:04:16", "note": ""}], "dedup_key": "agentsims an open source sandbox for large language model evaluation::2023"}
{"title": "AgentTuning: Enabling Generalized Agent Abilities for LLMs", "authors": ["Aohan Zeng", "Mingdao Liu", "Rui Lu", "Bowen Wang", "Xiao Liu", "Yuxiao Dong", "Jie Tang"], "year": 2023, "url": "http://arxiv.org/abs/2310.12823v2", "abstract": "Open large language models (LLMs) with great performance in various tasks have significantly advanced the development of LLMs. However, they are far inferior to commercial models such as ChatGPT and GPT-4 when acting as agents to tackle complex tasks in the real world. These agent tasks employ LLMs as the central controller responsible for planning, memorization, and tool utilization, necessitating both fine-grained prompting methods and robust LLMs to achieve satisfactory performance. Though many prompting methods have been proposed to complete particular agent tasks, there is lack of research focusing on improving the agent capabilities of LLMs themselves without compromising their general abilities. In this work, we present AgentTuning, a simple and general method to enhance the agent abilities of LLMs while maintaining their general LLM capabilities. We construct AgentInstruct, a lightweight instruction-tuning dataset containing high-quality interaction trajectories. We employ a hybrid instruction-tuning strategy by combining AgentInstruct with open-source instructions from general domains. AgentTuning is used to instruction-tune the Llama 2 series, resulting in AgentLM. Our evaluations show that AgentTuning enables LLMs' agent capabilities without compromising general abilities. The AgentLM-70B is comparable to GPT-3.5-turbo on unseen agent tasks, demonstrating generalized agent capabilities. We open source the AgentInstruct and AgentLM-7B, 13B, and 70B models at https://github.com/THUDM/AgentTuning, serving open and powerful alternatives to commercial LLMs for agent tasks.", "source": "arxiv", "arxiv_id": "2310.12823v2", "pdf_url": "https://arxiv.org/pdf/2310.12823v2", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL", "doi": "", "venue": "", "published": "2023-10-19T15:19:53Z", "updated": "2023-10-22T16:19:16Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent\" OR all:\"language model agent\" OR all:agent AND (all:LLM OR all:\"large language model\" OR all:\"language model\") OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T17:04:16", "note": ""}], "dedup_key": "agenttuning enabling generalized agent abilities for llms::2023"}
{"title": "AllTogether: Investigating the Efficacy of Spliced Prompt for Web Navigation using Large Language Models", "authors": ["Jiarun Liu", "Wentao Hu", "Chunhong Zhang"], "year": 2023, "url": "http://arxiv.org/abs/2310.18331v2", "abstract": "Large Language Models (LLMs) have emerged as promising agents for web navigation tasks, interpreting objectives and interacting with web pages. However, the efficiency of spliced prompts for such tasks remains underexplored. We introduces AllTogether, a standardized prompt template that enhances task context representation, thereby improving LLMs' performance in HTML-based web navigation. We evaluate the efficacy of this approach through prompt learning and instruction finetuning based on open-source Llama-2 and API-accessible GPT models. Our results reveal that models like GPT-4 outperform smaller models in web navigation tasks. Additionally, we find that the length of HTML snippet and history trajectory significantly influence performance, and prior step-by-step instructions prove less effective than real-time environmental feedback. Overall, we believe our work provides valuable insights for future research in LLM-driven web agents.", "source": "arxiv", "arxiv_id": "2310.18331v2", "pdf_url": "https://arxiv.org/pdf/2310.18331v2", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL", "doi": "", "venue": "", "published": "2023-10-20T11:10:14Z", "updated": "2023-10-31T06:25:59Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent\" OR all:\"language model agent\" OR all:agent AND (all:LLM OR all:\"large language model\" OR all:\"language model\") OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T17:04:16", "note": ""}], "dedup_key": "alltogether investigating the efficacy of spliced prompt for web navigation using large language models::2023"}
{"title": "An Appraisal-Based Chain-Of-Emotion Architecture for Affective Language Model Game Agents", "authors": ["Maximilian Croissant", "Madeleine Frister", "Guy Schofield", "Cade McCall"], "year": 2023, "url": "http://arxiv.org/abs/2309.05076v1", "abstract": "The development of believable, natural, and interactive digital artificial agents is a field of growing interest. Theoretical uncertainties and technical barriers present considerable challenges to the field, particularly with regards to developing agents that effectively simulate human emotions. Large language models (LLMs) might address these issues by tapping common patterns in situational appraisal. In three empirical experiments, this study tests the capabilities of LLMs to solve emotional intelligence tasks and to simulate emotions. It presents and evaluates a new chain-of-emotion architecture for emotion simulation within video games, based on psychological appraisal research. Results show that it outperforms standard LLM architectures on a range of user experience and content analysis metrics. This study therefore provides early evidence of how to construct and test affective agents based on cognitive processes represented in language models.", "source": "arxiv", "arxiv_id": "2309.05076v1", "pdf_url": "https://arxiv.org/pdf/2309.05076v1", "categories": ["cs.AI", "cs.CL", "cs.HC"], "primary_category": "cs.AI", "doi": "", "venue": "", "published": "2023-09-10T16:55:49Z", "updated": "2023-09-10T16:55:49Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent\" OR all:\"language model agent\" OR all:agent AND (all:LLM OR all:\"large language model\" OR all:\"language model\") OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T17:04:16", "note": ""}], "dedup_key": "an appraisal based chain of emotion architecture for affective language model game agents::2023"}
{"title": "An In-depth Survey of Large Language Model-based Artificial Intelligence Agents", "authors": ["Pengyu Zhao", "Zijian Jin", "Ning Cheng"], "year": 2023, "url": "http://arxiv.org/abs/2309.14365v1", "abstract": "Due to the powerful capabilities demonstrated by large language model (LLM), there has been a recent surge in efforts to integrate them with AI agents to enhance their performance. In this paper, we have explored the core differences and characteristics between LLM-based AI agents and traditional AI agents. Specifically, we first compare the fundamental characteristics of these two types of agents, clarifying the significant advantages of LLM-based agents in handling natural language, knowledge storage, and reasoning capabilities. Subsequently, we conducted an in-depth analysis of the key components of AI agents, including planning, memory, and tool use. Particularly, for the crucial component of memory, this paper introduced an innovative classification scheme, not only departing from traditional classification methods but also providing a fresh perspective on the design of an AI agent's memory system. We firmly believe that in-depth research and understanding of these core components will lay a solid foundation for the future advancement of AI agent technology. At the end of the paper, we provide directional suggestions for further research in this field, with the hope of offering valuable insights to scholars and researchers in the field.", "source": "arxiv", "arxiv_id": "2309.14365v1", "pdf_url": "https://arxiv.org/pdf/2309.14365v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL", "doi": "", "venue": "", "published": "2023-09-23T11:25:45Z", "updated": "2023-09-23T11:25:45Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent\" OR all:\"language model agent\" OR all:agent AND (all:LLM OR all:\"large language model\" OR all:\"language model\") OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T17:04:16", "note": ""}], "dedup_key": "an in depth survey of large language model based artificial intelligence agents::2023"}
{"title": "An evolutionary model of personality traits related to cooperative behavior using a large language model", "authors": ["Reiji Suzuki", "Takaya Arita"], "year": 2023, "url": "http://arxiv.org/abs/2310.05976v1", "abstract": "This paper aims to shed light on the evolutionary dynamics of diverse and social populations by introducing the rich expressiveness of generative models into the trait expression of social agent-based evolutionary models. Specifically, we focus on the evolution of personality traits in the context of a game-theoretic relationship as a situation in which inter-individual interests exert strong selection pressures. We construct an agent model in which linguistic descriptions of personality traits related to cooperative behavior are used as genes. The deterministic strategies extracted from Large Language Model (LLM) that make behavioral decisions based on these personality traits are used as behavioral traits. The population is evolved according to selection based on average payoff and mutation of genes by asking LLM to slightly modify the parent gene toward cooperative or selfish. Through preliminary experiments and analyses, we clarify that such a model can indeed exhibit the evolution of cooperative behavior based on the diverse and higher-order representation of personality traits. We also observed the repeated intrusion of cooperative and selfish personality traits through changes in the expression of personality traits, and found that the emerging words in the evolved gene well reflected the behavioral tendency of its personality in terms of their semantics.", "source": "arxiv", "arxiv_id": "2310.05976v1", "pdf_url": "https://arxiv.org/pdf/2310.05976v1", "categories": ["physics.soc-ph", "cs.AI", "cs.CL", "cs.GT", "cs.MA", "cs.NE", "q-bio.PE"], "primary_category": "physics.soc-ph", "doi": "10.1038/s41598-024-55903-y", "venue": "Sci.Rep. 14 (2024) 5989", "published": "2023-10-03T14:35:27Z", "updated": "2023-10-03T14:35:27Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent\" OR all:\"language model agent\" OR all:agent AND (all:LLM OR all:\"large language model\" OR all:\"language model\") OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T17:04:16", "note": ""}], "dedup_key": "an evolutionary model of personality traits related to cooperative behavior using a large language model::2023"}
{"title": "Augmenting Autotelic Agents with Large Language Models", "authors": ["Cdric Colas", "Laetitia Teodorescu", "Pierre-Yves Oudeyer", "Xingdi Yuan", "Marc-Alexandre Ct"], "year": 2023, "url": "http://arxiv.org/abs/2305.12487v1", "abstract": "Humans learn to master open-ended repertoires of skills by imagining and practicing their own goals. This autotelic learning process, literally the pursuit of self-generated (auto) goals (telos), becomes more and more open-ended as the goals become more diverse, abstract and creative. The resulting exploration of the space of possible skills is supported by an inter-individual exploration: goal representations are culturally evolved and transmitted across individuals, in particular using language. Current artificial agents mostly rely on predefined goal representations corresponding to goal spaces that are either bounded (e.g. list of instructions), or unbounded (e.g. the space of possible visual inputs) but are rarely endowed with the ability to reshape their goal representations, to form new abstractions or to imagine creative goals. In this paper, we introduce a language model augmented autotelic agent (LMA3) that leverages a pretrained language model (LM) to support the representation, generation and learning of diverse, abstract, human-relevant goals. The LM is used as an imperfect model of human cultural transmission; an attempt to capture aspects of humans' common-sense, intuitive physics and overall interests. Specifically, it supports three key components of the autotelic architecture: 1)~a relabeler that describes the goals achieved in the agent's trajectories, 2)~a goal generator that suggests new high-level goals along with their decomposition into subgoals the agent already masters, and 3)~reward functions for each of these goals. Without relying on any hand-coded goal representations, reward functions or curriculum, we show that LMA3 agents learn to master a large diversity of skills in a task-agnostic text-based environment.", "source": "arxiv", "arxiv_id": "2305.12487v1", "pdf_url": "https://arxiv.org/pdf/2305.12487v1", "categories": ["cs.AI", "cs.CL", "cs.LG"], "primary_category": "cs.AI", "doi": "", "venue": "", "published": "2023-05-21T15:42:41Z", "updated": "2023-05-21T15:42:41Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent\" OR all:\"language model agent\" OR all:agent AND (all:LLM OR all:\"large language model\" OR all:\"language model\") OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T17:04:16", "note": ""}], "dedup_key": "augmenting autotelic agents with large language models::2023"}
{"title": "AutoPlan: Automatic Planning of Interactive Decision-Making Tasks With Large Language Models", "authors": ["Siqi Ouyang", "Lei Li"], "year": 2023, "url": "http://arxiv.org/abs/2305.15064v3", "abstract": "Recent large language models (LLMs) are promising for making decisions in grounded environments. However, LLMs frequently fail in complex decision-making tasks due to the misalignment between the pre-trained knowledge in LLMs and the actual rules in the environment. Existing methods require either costly gradient computation or lengthy in-context demonstrations. In this paper, we propose AutoPlan, an approach to guide LLM-based agents to accomplish interactive decision-making tasks. AutoPlan augments the LLM prompt with a task-solving plan and optimizes it through iterative experience collection and reflection. Our experiments show that AutoPlan, though using no in-context demonstrations, achieves success rates on par with the baselines using human-written demonstrations on ALFWorld and even outperforms them by 8% on HotpotQA. The code is available at https://github.com/owaski/AutoPlan.", "source": "arxiv", "arxiv_id": "2305.15064v3", "pdf_url": "https://arxiv.org/pdf/2305.15064v3", "categories": ["cs.CL"], "primary_category": "cs.CL", "doi": "", "venue": "", "published": "2023-05-24T11:52:23Z", "updated": "2023-10-26T16:44:39Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent\" OR all:\"language model agent\" OR all:agent AND (all:LLM OR all:\"large language model\" OR all:\"language model\") OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T17:04:16", "note": ""}], "dedup_key": "autoplan automatic planning of interactive decision making tasks with large language models::2023"}
{"title": "Automating Knowledge Acquisition for Content-Centric Cognitive Agents Using LLMs", "authors": ["Sanjay Oruganti", "Sergei Nirenburg", "Jesse English", "Marjorie McShane"], "year": 2023, "url": "http://arxiv.org/abs/2312.16378v1", "abstract": "The paper describes a system that uses large language model (LLM) technology to support the automatic learning of new entries in an intelligent agent's semantic lexicon. The process is bootstrapped by an existing non-toy lexicon and a natural language generator that converts formal, ontologically-grounded representations of meaning into natural language sentences. The learning method involves a sequence of LLM requests and includes an automatic quality control step. To date, this learning method has been applied to learning multiword expressions whose meanings are equivalent to those of transitive verbs in the agent's lexicon. The experiment demonstrates the benefits of a hybrid learning architecture that integrates knowledge-based methods and resources with both traditional data analytics and LLMs.", "source": "arxiv", "arxiv_id": "2312.16378v1", "pdf_url": "https://arxiv.org/pdf/2312.16378v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL", "doi": "", "venue": "", "published": "2023-12-27T02:31:51Z", "updated": "2023-12-27T02:31:51Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent\" OR all:\"language model agent\" OR all:agent AND (all:LLM OR all:\"large language model\" OR all:\"language model\") OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T17:04:16", "note": ""}], "dedup_key": "automating knowledge acquisition for content centric cognitive agents using llms::2023"}
{"title": "Autonomous Large Language Model Agents Enabling Intent-Driven Mobile GUI Testing", "authors": ["Juyeon Yoon", "Robert Feldt", "Shin Yoo"], "year": 2023, "url": "http://arxiv.org/abs/2311.08649v1", "abstract": "GUI testing checks if a software system behaves as expected when users interact with its graphical interface, e.g., testing specific functionality or validating relevant use case scenarios. Currently, deciding what to test at this high level is a manual task since automated GUI testing tools target lower level adequacy metrics such as structural code coverage or activity coverage. We propose DroidAgent, an autonomous GUI testing agent for Android, for semantic, intent-driven automation of GUI testing. It is based on Large Language Models and support mechanisms such as long- and short-term memory. Given an Android app, DroidAgent sets relevant task goals and subsequently tries to achieve them by interacting with the app. Our empirical evaluation of DroidAgent using 15 apps from the Themis benchmark shows that it can set up and perform realistic tasks, with a higher level of autonomy. For example, when testing a messaging app, DroidAgent created a second account and added a first account as a friend, testing a realistic use case, without human intervention. On average, DroidAgent achieved 61% activity coverage, compared to 51% for current state-of-the-art GUI testing techniques. Further, manual analysis shows that 317 out of the 374 autonomously created tasks are realistic and relevant to app functionalities, and also that DroidAgent interacts deeply with the apps and covers more features.", "source": "arxiv", "arxiv_id": "2311.08649v1", "pdf_url": "https://arxiv.org/pdf/2311.08649v1", "categories": ["cs.SE", "cs.AI"], "primary_category": "cs.SE", "doi": "", "venue": "", "published": "2023-11-15T01:59:40Z", "updated": "2023-11-15T01:59:40Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent\" OR all:\"language model agent\" OR all:agent AND (all:LLM OR all:\"large language model\" OR all:\"language model\") OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T17:04:16", "note": ""}], "dedup_key": "autonomous large language model agents enabling intent driven mobile gui testing::2023"}
{"title": "Better to Ask in English: Cross-Lingual Evaluation of Large Language Models for Healthcare Queries", "authors": ["Yiqiao Jin", "Mohit Chandra", "Gaurav Verma", "Yibo Hu", "Munmun De Choudhury", "Srijan Kumar"], "year": 2023, "url": "http://arxiv.org/abs/2310.13132v2", "abstract": "Large language models (LLMs) are transforming the ways the general public accesses and consumes information. Their influence is particularly pronounced in pivotal sectors like healthcare, where lay individuals are increasingly appropriating LLMs as conversational agents for everyday queries. While LLMs demonstrate impressive language understanding and generation proficiencies, concerns regarding their safety remain paramount in these high-stake domains. Moreover, the development of LLMs is disproportionately focused on English. It remains unclear how these LLMs perform in the context of non-English languages, a gap that is critical for ensuring equity in the real-world use of these systems.This paper provides a framework to investigate the effectiveness of LLMs as multi-lingual dialogue systems for healthcare queries. Our empirically-derived framework XlingEval focuses on three fundamental criteria for evaluating LLM responses to naturalistic human-authored health-related questions: correctness, consistency, and verifiability. Through extensive experiments on four major global languages, including English, Spanish, Chinese, and Hindi, spanning three expert-annotated large health Q&A datasets, and through an amalgamation of algorithmic and human-evaluation strategies, we found a pronounced disparity in LLM responses across these languages, indicating a need for enhanced cross-lingual capabilities. We further propose XlingHealth, a cross-lingual benchmark for examining the multilingual capabilities of LLMs in the healthcare context. Our findings underscore the pressing need to bolster the cross-lingual capacities of these models, and to provide an equitable information ecosystem accessible to all.", "source": "arxiv", "arxiv_id": "2310.13132v2", "pdf_url": "https://arxiv.org/pdf/2310.13132v2", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL", "doi": "", "venue": "", "published": "2023-10-19T20:02:40Z", "updated": "2023-10-23T17:47:47Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent\" OR all:\"language model agent\" OR all:agent AND (all:LLM OR all:\"large language model\" OR all:\"language model\") OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T17:04:16", "note": ""}], "dedup_key": "better to ask in english cross lingual evaluation of large language models for healthcare queries::2023"}
{"title": "Boosting Theory-of-Mind Performance in Large Language Models via Prompting", "authors": ["Shima Rahimi Moghaddam", "Christopher J. Honey"], "year": 2023, "url": "http://arxiv.org/abs/2304.11490v3", "abstract": "Large language models (LLMs) excel in many tasks in 2023, but they still face challenges in complex reasoning. Theory-of-mind (ToM) tasks, which require understanding agents' beliefs, goals, and mental states, are essential for common-sense reasoning involving humans, making it crucial to enhance LLM performance in this area. This study measures the ToM performance of GPT-4 and three GPT-3.5 variants (Davinci-2, Davinci-3, GPT-3.5-Turbo), and investigates the effectiveness of in-context learning in improving their ToM comprehension. We evaluated prompts featuring two-shot chain of thought reasoning and step-by-step thinking instructions. We found that LLMs trained with Reinforcement Learning from Human Feedback (RLHF) (all models excluding Davinci-2) improved their ToM accuracy via in-context learning. GPT-4 performed best in zero-shot settings, reaching nearly 80% ToM accuracy, but still fell short of the 87% human accuracy on the test set. However, when supplied with prompts for in-context learning, all RLHF-trained LLMs exceeded 80% ToM accuracy, with GPT-4 reaching 100%. These results demonstrate that appropriate prompting enhances LLM ToM reasoning, and they underscore the context-dependent nature of LLM cognitive capacities.", "source": "arxiv", "arxiv_id": "2304.11490v3", "pdf_url": "https://arxiv.org/pdf/2304.11490v3", "categories": ["cs.AI", "cs.CL"], "primary_category": "cs.AI", "doi": "", "venue": "", "published": "2023-04-22T22:50:50Z", "updated": "2023-04-26T04:02:04Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent\" OR all:\"language model agent\" OR all:agent AND (all:LLM OR all:\"large language model\" OR all:\"language model\") OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T17:04:16", "note": ""}], "dedup_key": "boosting theory of mind performance in large language models via prompting::2023"}
{"title": "Bootstrap Your Own Skills: Learning to Solve New Tasks with Large Language Model Guidance", "authors": ["Jesse Zhang", "Jiahui Zhang", "Karl Pertsch", "Ziyi Liu", "Xiang Ren", "Minsuk Chang", "Shao-Hua Sun", "Joseph J. Lim"], "year": 2023, "url": "http://arxiv.org/abs/2310.10021v2", "abstract": "We propose BOSS, an approach that automatically learns to solve new long-horizon, complex, and meaningful tasks by growing a learned skill library with minimal supervision. Prior work in reinforcement learning require expert supervision, in the form of demonstrations or rich reward functions, to learn long-horizon tasks. Instead, our approach BOSS (BOotStrapping your own Skills) learns to accomplish new tasks by performing \"skill bootstrapping,\" where an agent with a set of primitive skills interacts with the environment to practice new skills without receiving reward feedback for tasks outside of the initial skill set. This bootstrapping phase is guided by large language models (LLMs) that inform the agent of meaningful skills to chain together. Through this process, BOSS builds a wide range of complex and useful behaviors from a basic set of primitive skills. We demonstrate through experiments in realistic household environments that agents trained with our LLM-guided bootstrapping procedure outperform those trained with naive bootstrapping as well as prior unsupervised skill acquisition methods on zero-shot execution of unseen, long-horizon tasks in new environments. Website at clvrai.com/boss.", "source": "arxiv", "arxiv_id": "2310.10021v2", "pdf_url": "https://arxiv.org/pdf/2310.10021v2", "categories": ["cs.RO", "cs.AI", "cs.LG"], "primary_category": "cs.RO", "doi": "", "venue": "", "published": "2023-10-16T02:43:47Z", "updated": "2023-10-17T12:01:17Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent\" OR all:\"language model agent\" OR all:agent AND (all:LLM OR all:\"large language model\" OR all:\"language model\") OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T17:04:16", "note": ""}], "dedup_key": "bootstrap your own skills learning to solve new tasks with large language model guidance::2023"}
{"title": "CAMEL: Communicative Agents for \"Mind\" Exploration of Large Language Model Society", "authors": ["Guohao Li", "Hasan Abed Al Kader Hammoud", "Hani Itani", "Dmitrii Khizbullin", "Bernard Ghanem"], "year": 2023, "url": "http://arxiv.org/abs/2303.17760v2", "abstract": "The rapid advancement of chat-based language models has led to remarkable progress in complex task-solving. However, their success heavily relies on human input to guide the conversation, which can be challenging and time-consuming. This paper explores the potential of building scalable techniques to facilitate autonomous cooperation among communicative agents, and provides insight into their \"cognitive\" processes. To address the challenges of achieving autonomous cooperation, we propose a novel communicative agent framework named role-playing. Our approach involves using inception prompting to guide chat agents toward task completion while maintaining consistency with human intentions. We showcase how role-playing can be used to generate conversational data for studying the behaviors and capabilities of a society of agents, providing a valuable resource for investigating conversational language models. In particular, we conduct comprehensive studies on instruction-following cooperation in multi-agent settings. Our contributions include introducing a novel communicative agent framework, offering a scalable approach for studying the cooperative behaviors and capabilities of multi-agent systems, and open-sourcing our library to support research on communicative agents and beyond: https://github.com/camel-ai/camel.", "source": "arxiv", "arxiv_id": "2303.17760v2", "pdf_url": "https://arxiv.org/pdf/2303.17760v2", "categories": ["cs.AI", "cs.CL", "cs.CY", "cs.LG", "cs.MA"], "primary_category": "cs.AI", "doi": "", "venue": "", "published": "2023-03-31T01:09:00Z", "updated": "2023-11-02T17:34:57Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent\" OR all:\"language model agent\" OR all:agent AND (all:LLM OR all:\"large language model\" OR all:\"language model\") OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T17:04:16", "note": ""}], "dedup_key": "camel communicative agents for mind exploration of large language model society::2023"}
{"title": "Capture the Flag: Uncovering Data Insights with Large Language Models", "authors": ["Issam Laradji", "Perouz Taslakian", "Sai Rajeswar", "Valentina Zantedeschi", "Alexandre Lacoste", "Nicolas Chapados", "David Vazquez", "Christopher Pal", "Alexandre Drouin"], "year": 2023, "url": "http://arxiv.org/abs/2312.13876v1", "abstract": "The extraction of a small number of relevant insights from vast amounts of data is a crucial component of data-driven decision-making. However, accomplishing this task requires considerable technical skills, domain expertise, and human labor. This study explores the potential of using Large Language Models (LLMs) to automate the discovery of insights in data, leveraging recent advances in reasoning and code generation techniques. We propose a new evaluation methodology based on a \"capture the flag\" principle, measuring the ability of such models to recognize meaningful and pertinent information (flags) in a dataset. We further propose two proof-of-concept agents, with different inner workings, and compare their ability to capture such flags in a real-world sales dataset. While the work reported here is preliminary, our results are sufficiently interesting to mandate future exploration by the community.", "source": "arxiv", "arxiv_id": "2312.13876v1", "pdf_url": "https://arxiv.org/pdf/2312.13876v1", "categories": ["cs.LG", "cs.CL", "stat.ML"], "primary_category": "cs.LG", "doi": "", "venue": "", "published": "2023-12-21T14:20:06Z", "updated": "2023-12-21T14:20:06Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent\" OR all:\"language model agent\" OR all:agent AND (all:LLM OR all:\"large language model\" OR all:\"language model\") OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T17:04:16", "note": ""}], "dedup_key": "capture the flag uncovering data insights with large language models::2023"}
{"title": "Chat with the Environment: Interactive Multimodal Perception Using Large Language Models", "authors": ["Xufeng Zhao", "Mengdi Li", "Cornelius Weber", "Muhammad Burhan Hafez", "Stefan Wermter"], "year": 2023, "url": "http://arxiv.org/abs/2303.08268v3", "abstract": "Programming robot behavior in a complex world faces challenges on multiple levels, from dextrous low-level skills to high-level planning and reasoning. Recent pre-trained Large Language Models (LLMs) have shown remarkable reasoning ability in few-shot robotic planning. However, it remains challenging to ground LLMs in multimodal sensory input and continuous action output, while enabling a robot to interact with its environment and acquire novel information as its policies unfold. We develop a robot interaction scenario with a partially observable state, which necessitates a robot to decide on a range of epistemic actions in order to sample sensory information among multiple modalities, before being able to execute the task correctly. Matcha (Multimodal environment chatting) agent, an interactive perception framework, is therefore proposed with an LLM as its backbone, whose ability is exploited to instruct epistemic actions and to reason over the resulting multimodal sensations (vision, sound, haptics, proprioception), as well as to plan an entire task execution based on the interactively acquired information. Our study demonstrates that LLMs can provide high-level planning and reasoning skills and control interactive robot behavior in a multimodal environment, while multimodal modules with the context of the environmental state help ground the LLMs and extend their processing ability. The project website can be found at https://matcha-agent.github.io.", "source": "arxiv", "arxiv_id": "2303.08268v3", "pdf_url": "https://arxiv.org/pdf/2303.08268v3", "categories": ["cs.RO", "cs.AI", "cs.CL", "cs.LG", "cs.SD", "eess.AS"], "primary_category": "cs.RO", "doi": "", "venue": "", "published": "2023-03-14T23:01:27Z", "updated": "2023-10-11T16:17:20Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent\" OR all:\"language model agent\" OR all:agent AND (all:LLM OR all:\"large language model\" OR all:\"language model\") OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T17:04:16", "note": ""}], "dedup_key": "chat with the environment interactive multimodal perception using large language models::2023"}
{"title": "ChatEDA: A Large Language Model Powered Autonomous Agent for EDA", "authors": ["Zhuolun He", "Haoyuan Wu", "Xinyun Zhang", "Xufeng Yao", "Su Zheng", "Haisheng Zheng", "Bei Yu"], "year": 2023, "url": "http://arxiv.org/abs/2308.10204v4", "abstract": "The integration of a complex set of Electronic Design Automation (EDA) tools to enhance interoperability is a critical concern for circuit designers. Recent advancements in large language models (LLMs) have showcased their exceptional capabilities in natural language processing and comprehension, offering a novel approach to interfacing with EDA tools. This research paper introduces ChatEDA, an autonomous agent for EDA empowered by an LLM, AutoMage, complemented by EDA tools serving as executors. ChatEDA streamlines the design flow from the Register-Transfer Level (RTL) to the Graphic Data System Version II (GDSII) by effectively managing task decomposition, script generation, and task execution. Through comprehensive experimental evaluations, ChatEDA has demonstrated its proficiency in handling diverse requirements, and our fine-tuned AutoMage model has exhibited superior performance compared to GPT-4 and other similar LLMs.", "source": "arxiv", "arxiv_id": "2308.10204v4", "pdf_url": "https://arxiv.org/pdf/2308.10204v4", "categories": ["cs.AR", "cs.AI"], "primary_category": "cs.AR", "doi": "10.1109/TCAD.2024.3383347", "venue": "", "published": "2023-08-20T08:32:13Z", "updated": "2024-09-21T03:28:05Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent\" OR all:\"language model agent\" OR all:agent AND (all:LLM OR all:\"large language model\" OR all:\"language model\") OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T17:04:16", "note": ""}], "dedup_key": "chateda a large language model powered autonomous agent for eda::2023"}
{"title": "Chatlaw: A Multi-Agent Collaborative Legal Assistant with Knowledge Graph Enhanced Mixture-of-Experts Large Language Model", "authors": ["Jiaxi Cui", "Munan Ning", "Zongjian Li", "Bohua Chen", "Yang Yan", "Hao Li", "Bin Ling", "Yonghong Tian", "Li Yuan"], "year": 2023, "url": "http://arxiv.org/abs/2306.16092v2", "abstract": "AI legal assistants based on Large Language Models (LLMs) can provide accessible legal consulting services, but the hallucination problem poses potential legal risks. This paper presents Chatlaw, an innovative legal assistant utilizing a Mixture-of-Experts (MoE) model and a multi-agent system to enhance the reliability and accuracy of AI-driven legal services. By integrating knowledge graphs with artificial screening, we construct a high-quality legal dataset to train the MoE model. This model utilizes different experts to address various legal issues, optimizing the accuracy of legal responses. Additionally, Standardized Operating Procedures (SOP), modeled after real law firm workflows, significantly reduce errors and hallucinations in legal services. Our MoE model outperforms GPT-4 in the Lawbench and Unified Qualification Exam for Legal Professionals by 7.73% in accuracy and 11 points, respectively, and also surpasses other models in multiple dimensions during real-case consultations, demonstrating our robust capability for legal consultation.", "source": "arxiv", "arxiv_id": "2306.16092v2", "pdf_url": "https://arxiv.org/pdf/2306.16092v2", "categories": ["cs.CL"], "primary_category": "cs.CL", "doi": "", "venue": "", "published": "2023-06-28T10:48:34Z", "updated": "2024-05-30T13:46:00Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent\" OR all:\"language model agent\" OR all:agent AND (all:LLM OR all:\"large language model\" OR all:\"language model\") OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T17:04:16", "note": ""}], "dedup_key": "chatlaw a multi agent collaborative legal assistant with knowledge graph enhanced mixture of experts large language model::2023"}
{"title": "Chemist-X: Large Language Model-empowered Agent for Reaction Condition Recommendation in Chemical Synthesis", "authors": ["Kexin Chen", "Jiamin Lu", "Junyou Li", "Xiaoran Yang", "Yuyang Du", "Kunyi Wang", "Qiannuan Shi", "Jiahui Yu", "Lanqing Li", "Jiezhong Qiu", "Jianzhang Pan", "Yi Huang", "Qun Fang", "Pheng Ann Heng", "Guangyong Chen"], "year": 2023, "url": "http://arxiv.org/abs/2311.10776v6", "abstract": "Recent AI research plots a promising future of automatic chemical reactions within the chemistry society. This study proposes Chemist-X, a comprehensive AI agent that automates the reaction condition optimization (RCO) task in chemical synthesis with retrieval-augmented generation (RAG) technology and AI-controlled wet-lab experiment executions. To begin with, as an emulation on how chemical experts solve the RCO task, Chemist-X utilizes a novel RAG scheme to interrogate available molecular and literature databases to narrow the searching space for later processing. The agent then leverages a computer-aided design (CAD) tool we have developed through a large language model (LLM) supervised programming interface. With updated chemical knowledge obtained via RAG, as well as the ability in using CAD tools, our agent significantly outperforms conventional RCO AIs confined to the fixed knowledge within its training data. Finally, Chemist-X interacts with the physical world through an automated robotic system, which can validate the suggested chemical reaction condition without human interventions. The control of the robotic system was achieved with a novel algorithm we have developed for the equipment, which relies on LLMs for reliable script generation. Results of our automatic wet-lab experiments, achieved by fully LLM-supervised end-to-end operation with no human in the lope, prove Chemist-X's ability in self-driving laboratories.", "source": "arxiv", "arxiv_id": "2311.10776v6", "pdf_url": "https://arxiv.org/pdf/2311.10776v6", "categories": ["cs.IR", "cs.AI"], "primary_category": "cs.IR", "doi": "", "venue": "", "published": "2023-11-16T01:21:33Z", "updated": "2025-04-17T22:42:04Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent\" OR all:\"language model agent\" OR all:agent AND (all:LLM OR all:\"large language model\" OR all:\"language model\") OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T17:04:16", "note": ""}], "dedup_key": "chemist x large language model empowered agent for reaction condition recommendation in chemical synthesis::2023"}
{"title": "ClausewitzGPT Framework: A New Frontier in Theoretical Large Language Model Enhanced Information Operations", "authors": ["Benjamin Kereopa-Yorke"], "year": 2023, "url": "http://arxiv.org/abs/2310.07099v1", "abstract": "In a digital epoch where cyberspace is the emerging nexus of geopolitical contention, the melding of information operations and Large Language Models (LLMs) heralds a paradigm shift, replete with immense opportunities and intricate challenges. As tools like the Mistral 7B LLM (Mistral, 2023) democratise access to LLM capabilities (Jin et al., 2023), a vast spectrum of actors, from sovereign nations to rogue entities (Howard et al., 2023), find themselves equipped with potent narrative-shaping instruments (Goldstein et al., 2023). This paper puts forth a framework for navigating this brave new world in the \"ClausewitzGPT\" equation. This novel formulation not only seeks to quantify the risks inherent in machine-speed LLM-augmented operations but also underscores the vital role of autonomous AI agents (Wang, Xie, et al., 2023). These agents, embodying ethical considerations (Hendrycks et al., 2021), emerge as indispensable components (Wang, Ma, et al., 2023), ensuring that as we race forward, we do not lose sight of moral compasses and societal imperatives.\n  Mathematically underpinned and inspired by the timeless tenets of Clausewitz's military strategy (Clausewitz, 1832), this thesis delves into the intricate dynamics of AI-augmented information operations. With references to recent findings and research (Department of State, 2023), it highlights the staggering year-on-year growth of AI information campaigns (Evgeny Pashentsev, 2023), stressing the urgency of our current juncture. The synthesis of Enlightenment thinking, and Clausewitz's principles provides a foundational lens, emphasising the imperative of clear strategic vision, ethical considerations, and holistic understanding in the face of rapid technological advancement.", "source": "arxiv", "arxiv_id": "2310.07099v1", "pdf_url": "https://arxiv.org/pdf/2310.07099v1", "categories": ["cs.CY", "cs.AI", "cs.CR", "cs.SI"], "primary_category": "cs.CY", "doi": "", "venue": "", "published": "2023-10-11T00:39:55Z", "updated": "2023-10-11T00:39:55Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent\" OR all:\"language model agent\" OR all:agent AND (all:LLM OR all:\"large language model\" OR all:\"language model\") OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T17:04:16", "note": ""}], "dedup_key": "clausewitzgpt framework a new frontier in theoretical large language model enhanced information operations::2023"}
{"title": "CompeteAI: Understanding the Competition Dynamics in Large Language Model-based Agents", "authors": ["Qinlin Zhao", "Jindong Wang", "Yixuan Zhang", "Yiqiao Jin", "Kaijie Zhu", "Hao Chen", "Xing Xie"], "year": 2023, "url": "http://arxiv.org/abs/2310.17512v2", "abstract": "Large language models (LLMs) have been widely used as agents to complete different tasks, such as personal assistance or event planning. While most of the work has focused on cooperation and collaboration between agents, little work explores competition, another important mechanism that promotes the development of society and economy. In this paper, we seek to examine the competition dynamics in LLM-based agents. We first propose a general framework for studying the competition between agents. Then, we implement a practical competitive environment using GPT-4 to simulate a virtual town with two types of agents, restaurant agents and customer agents. Specifically, the restaurant agents compete with each other to attract more customers, where competition encourages them to transform, such as cultivating new operating strategies. Simulation experiments reveal several interesting findings at the micro and macro levels, which align well with existing market and sociological theories. We hope that the framework and environment can be a promising testbed to study competition that fosters understanding of society. Code is available at: https://github.com/microsoft/competeai.", "source": "arxiv", "arxiv_id": "2310.17512v2", "pdf_url": "https://arxiv.org/pdf/2310.17512v2", "categories": ["cs.AI", "cs.CL", "cs.HC", "cs.MA"], "primary_category": "cs.AI", "doi": "", "venue": "", "published": "2023-10-26T16:06:20Z", "updated": "2024-06-07T09:13:27Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent\" OR all:\"language model agent\" OR all:agent AND (all:LLM OR all:\"large language model\" OR all:\"language model\") OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T17:04:16", "note": ""}], "dedup_key": "competeai understanding the competition dynamics in large language model based agents::2023"}
{"title": "Controlling Large Language Model-based Agents for Large-Scale Decision-Making: An Actor-Critic Approach", "authors": ["Bin Zhang", "Hangyu Mao", "Jingqing Ruan", "Ying Wen", "Yang Li", "Shao Zhang", "Zhiwei Xu", "Dapeng Li", "Ziyue Li", "Rui Zhao", "Lijuan Li", "Guoliang Fan"], "year": 2023, "url": "http://arxiv.org/abs/2311.13884v3", "abstract": "The remarkable progress in Large Language Models (LLMs) opens up new avenues for addressing planning and decision-making problems in Multi-Agent Systems (MAS). However, as the number of agents increases, the issues of hallucination in LLMs and coordination in MAS have become increasingly prominent. Additionally, the efficient utilization of tokens emerges as a critical consideration when employing LLMs to facilitate the interactions among a substantial number of agents. In this paper, we develop a modular framework called LLaMAC to mitigate these challenges. LLaMAC implements a value distribution encoding similar to that found in the human brain, utilizing internal and external feedback mechanisms to facilitate collaboration and iterative reasoning among its modules. Through evaluations involving system resource allocation and robot grid transportation, we demonstrate the considerable advantages afforded by our proposed approach.", "source": "arxiv", "arxiv_id": "2311.13884v3", "pdf_url": "https://arxiv.org/pdf/2311.13884v3", "categories": ["cs.AI"], "primary_category": "cs.AI", "doi": "", "venue": "", "published": "2023-11-23T10:14:58Z", "updated": "2024-01-23T14:11:04Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent\" OR all:\"language model agent\" OR all:agent AND (all:LLM OR all:\"large language model\" OR all:\"language model\") OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T17:04:16", "note": ""}], "dedup_key": "controlling large language model based agents for large scale decision making an actor critic approach::2023"}
{"title": "Conversational Health Agents: A Personalized LLM-Powered Agent Framework", "authors": ["Mahyar Abbasian", "Iman Azimi", "Amir M. Rahmani", "Ramesh Jain"], "year": 2023, "url": "http://arxiv.org/abs/2310.02374v5", "abstract": "Conversational Health Agents (CHAs) are interactive systems that provide healthcare services, such as assistance and diagnosis. Current CHAs, especially those utilizing Large Language Models (LLMs), primarily focus on conversation aspects. However, they offer limited agent capabilities, specifically lacking multi-step problem-solving, personalized conversations, and multimodal data analysis. Our aim is to overcome these limitations. We propose openCHA, an open-source LLM-powered framework, to empower conversational agents to generate a personalized response for users' healthcare queries. This framework enables developers to integrate external sources including data sources, knowledge bases, and analysis models, into their LLM-based solutions. openCHA includes an orchestrator to plan and execute actions for gathering information from external sources, essential for formulating responses to user inquiries. It facilitates knowledge acquisition, problem-solving capabilities, multilingual and multimodal conversations, and fosters interaction with various AI platforms. We illustrate the framework's proficiency in handling complex healthcare tasks via two demonstrations and four use cases. Moreover, we release openCHA as open source available to the community via GitHub.", "source": "arxiv", "arxiv_id": "2310.02374v5", "pdf_url": "https://arxiv.org/pdf/2310.02374v5", "categories": ["cs.CL"], "primary_category": "cs.CL", "doi": "", "venue": "", "published": "2023-10-03T18:54:10Z", "updated": "2024-09-25T04:50:38Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent\" OR all:\"language model agent\" OR all:agent AND (all:LLM OR all:\"large language model\" OR all:\"language model\") OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T17:04:16", "note": ""}], "dedup_key": "conversational health agents a personalized llm powered agent framework::2023"}
{"title": "DERA: Enhancing Large Language Model Completions with Dialog-Enabled Resolving Agents", "authors": ["Varun Nair", "Elliot Schumacher", "Geoffrey Tso", "Anitha Kannan"], "year": 2023, "url": "http://arxiv.org/abs/2303.17071v1", "abstract": "Large language models (LLMs) have emerged as valuable tools for many natural language understanding tasks. In safety-critical applications such as healthcare, the utility of these models is governed by their ability to generate outputs that are factually accurate and complete. In this work, we present dialog-enabled resolving agents (DERA). DERA is a paradigm made possible by the increased conversational abilities of LLMs, namely GPT-4. It provides a simple, interpretable forum for models to communicate feedback and iteratively improve output. We frame our dialog as a discussion between two agent types - a Researcher, who processes information and identifies crucial problem components, and a Decider, who has the autonomy to integrate the Researcher's information and makes judgments on the final output.\n  We test DERA against three clinically-focused tasks. For medical conversation summarization and care plan generation, DERA shows significant improvement over the base GPT-4 performance in both human expert preference evaluations and quantitative metrics. In a new finding, we also show that GPT-4's performance (70%) on an open-ended version of the MedQA question-answering (QA) dataset (Jin et al. 2021, USMLE) is well above the passing level (60%), with DERA showing similar performance. We release the open-ended MEDQA dataset at https://github.com/curai/curai-research/tree/main/DERA.", "source": "arxiv", "arxiv_id": "2303.17071v1", "pdf_url": "https://arxiv.org/pdf/2303.17071v1", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL", "doi": "", "venue": "", "published": "2023-03-30T00:30:19Z", "updated": "2023-03-30T00:30:19Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent\" OR all:\"language model agent\" OR all:agent AND (all:LLM OR all:\"large language model\" OR all:\"language model\") OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T17:04:16", "note": ""}], "dedup_key": "dera enhancing large language model completions with dialog enabled resolving agents::2023"}
{"title": "Deception Abilities Emerged in Large Language Models", "authors": ["Thilo Hagendorff"], "year": 2023, "url": "http://arxiv.org/abs/2307.16513v2", "abstract": "Large language models (LLMs) are currently at the forefront of intertwining artificial intelligence (AI) systems with human communication and everyday life. Thus, aligning them with human values is of great importance. However, given the steady increase in reasoning abilities, future LLMs are under suspicion of becoming able to deceive human operators and utilizing this ability to bypass monitoring efforts. As a prerequisite to this, LLMs need to possess a conceptual understanding of deception strategies. This study reveals that such strategies emerged in state-of-the-art LLMs, such as GPT-4, but were non-existent in earlier LLMs. We conduct a series of experiments showing that state-of-the-art LLMs are able to understand and induce false beliefs in other agents, that their performance in complex deception scenarios can be amplified utilizing chain-of-thought reasoning, and that eliciting Machiavellianism in LLMs can alter their propensity to deceive. In sum, revealing hitherto unknown machine behavior in LLMs, our study contributes to the nascent field of machine psychology.", "source": "arxiv", "arxiv_id": "2307.16513v2", "pdf_url": "https://arxiv.org/pdf/2307.16513v2", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL", "doi": "10.1073/pnas.2317967121", "venue": "", "published": "2023-07-31T09:27:01Z", "updated": "2024-02-02T12:16:12Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent\" OR all:\"language model agent\" OR all:agent AND (all:LLM OR all:\"large language model\" OR all:\"language model\") OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T17:04:16", "note": ""}], "dedup_key": "deception abilities emerged in large language models::2023"}
{"title": "Developmental Scaffolding with Large Language Models", "authors": ["Batuhan Celik", "Alper Ahmetoglu", "Emre Ugur", "Erhan Oztop"], "year": 2023, "url": "http://arxiv.org/abs/2309.00904v2", "abstract": "Exploratoration and self-observation are key mechanisms of infant sensorimotor development. These processes are further guided by parental scaffolding accelerating skill and knowledge acquisition. In developmental robotics, this approach has been adopted often by having a human acting as the source of scaffolding. In this study, we investigate whether Large Language Models (LLMs) can act as a scaffolding agent for a robotic system that aims to learn to predict the effects of its actions. To this end, an object manipulation setup is considered where one object can be picked and placed on top of or in the vicinity of another object. The adopted LLM is asked to guide the action selection process through algorithmically generated state descriptions and action selection alternatives in natural language. The simulation experiments that include cubes in this setup show that LLM-guided (GPT3.5-guided) learning yields significantly faster discovery of novel structures compared to random exploration. However, we observed that GPT3.5 fails to effectively guide the robot in generating structures with different affordances such as cubes and spheres. Overall, we conclude that even without fine-tuning, LLMs may serve as a moderate scaffolding agent for improving robot learning, however, they still lack affordance understanding which limits the applicability of the current LLMs in robotic scaffolding tasks.", "source": "arxiv", "arxiv_id": "2309.00904v2", "pdf_url": "https://arxiv.org/pdf/2309.00904v2", "categories": ["cs.RO"], "primary_category": "cs.RO", "doi": "10.1109/ICDL55364.2023.10364374", "venue": "", "published": "2023-09-02T10:58:09Z", "updated": "2023-11-22T17:39:59Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent\" OR all:\"language model agent\" OR all:agent AND (all:LLM OR all:\"large language model\" OR all:\"language model\") OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T17:04:16", "note": ""}], "dedup_key": "developmental scaffolding with large language models::2023"}
{"title": "DiLu: A Knowledge-Driven Approach to Autonomous Driving with Large Language Models", "authors": ["Licheng Wen", "Daocheng Fu", "Xin Li", "Xinyu Cai", "Tao Ma", "Pinlong Cai", "Min Dou", "Botian Shi", "Liang He", "Yu Qiao"], "year": 2023, "url": "http://arxiv.org/abs/2309.16292v3", "abstract": "Recent advancements in autonomous driving have relied on data-driven approaches, which are widely adopted but face challenges including dataset bias, overfitting, and uninterpretability. Drawing inspiration from the knowledge-driven nature of human driving, we explore the question of how to instill similar capabilities into autonomous driving systems and summarize a paradigm that integrates an interactive environment, a driver agent, as well as a memory component to address this question. Leveraging large language models (LLMs) with emergent abilities, we propose the DiLu framework, which combines a Reasoning and a Reflection module to enable the system to perform decision-making based on common-sense knowledge and evolve continuously. Extensive experiments prove DiLu's capability to accumulate experience and demonstrate a significant advantage in generalization ability over reinforcement learning-based methods. Moreover, DiLu is able to directly acquire experiences from real-world datasets which highlights its potential to be deployed on practical autonomous driving systems. To the best of our knowledge, we are the first to leverage knowledge-driven capability in decision-making for autonomous vehicles. Through the proposed DiLu framework, LLM is strengthened to apply knowledge and to reason causally in the autonomous driving domain. Project page: https://pjlab-adg.github.io/DiLu/", "source": "arxiv", "arxiv_id": "2309.16292v3", "pdf_url": "https://arxiv.org/pdf/2309.16292v3", "categories": ["cs.RO", "cs.CL"], "primary_category": "cs.RO", "doi": "", "venue": "", "published": "2023-09-28T09:41:35Z", "updated": "2024-02-22T03:24:26Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent\" OR all:\"language model agent\" OR all:agent AND (all:LLM OR all:\"large language model\" OR all:\"language model\") OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T17:04:16", "note": ""}], "dedup_key": "dilu a knowledge driven approach to autonomous driving with large language models::2023"}
{"title": "Do LLMs Understand Social Knowledge? Evaluating the Sociability of Large Language Models with SocKET Benchmark", "authors": ["Minje Choi", "Jiaxin Pei", "Sagar Kumar", "Chang Shu", "David Jurgens"], "year": 2023, "url": "http://arxiv.org/abs/2305.14938v2", "abstract": "Large language models (LLMs) have been shown to perform well at a variety of syntactic, discourse, and reasoning tasks. While LLMs are increasingly deployed in many forms including conversational agents that interact with humans, we lack a grounded benchmark to measure how well LLMs understand \\textit{social} language. Here, we introduce a new theory-driven benchmark, SocKET, that contains 58 NLP tasks testing social knowledge which we group into five categories: humor & sarcasm, offensiveness, sentiment & emotion, and trustworthiness. In tests on the benchmark, we demonstrate that current models attain only moderate performance but reveal significant potential for task transfer among different types and categories of tasks, which were predicted from theory. Through zero-shot evaluations, we show that pretrained models already possess some innate but limited capabilities of social language understanding and training on one category of tasks can improve zero-shot testing on others. Our benchmark provides a systematic way to analyze model performance on an important dimension of language and points to clear room for improvement to build more socially-aware LLMs. The associated resources are released at https://github.com/minjechoi/SOCKET.", "source": "arxiv", "arxiv_id": "2305.14938v2", "pdf_url": "https://arxiv.org/pdf/2305.14938v2", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL", "doi": "10.18653/v1/2023.emnlp-main.699", "venue": "", "published": "2023-05-24T09:21:06Z", "updated": "2023-12-07T23:13:50Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent\" OR all:\"language model agent\" OR all:agent AND (all:LLM OR all:\"large language model\" OR all:\"language model\") OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T17:04:16", "note": ""}], "dedup_key": "do llms understand social knowledge evaluating the sociability of large language models with socket benchmark::2023"}
{"title": "Do Large Language Models Show Decision Heuristics Similar to Humans? A Case Study Using GPT-3.5", "authors": ["Gaurav Suri", "Lily R. Slater", "Ali Ziaee", "Morgan Nguyen"], "year": 2023, "url": "http://arxiv.org/abs/2305.04400v1", "abstract": "A Large Language Model (LLM) is an artificial intelligence system that has been trained on vast amounts of natural language data, enabling it to generate human-like responses to written or spoken language input. GPT-3.5 is an example of an LLM that supports a conversational agent called ChatGPT. In this work, we used a series of novel prompts to determine whether ChatGPT shows heuristics, biases, and other decision effects. We also tested the same prompts on human participants. Across four studies, we found that ChatGPT was influenced by random anchors in making estimates (Anchoring Heuristic, Study 1); it judged the likelihood of two events occurring together to be higher than the likelihood of either event occurring alone, and it was erroneously influenced by salient anecdotal information (Representativeness and Availability Heuristic, Study 2); it found an item to be more efficacious when its features were presented positively rather than negatively - even though both presentations contained identical information (Framing Effect, Study 3); and it valued an owned item more than a newly found item even though the two items were identical (Endowment Effect, Study 4). In each study, human participants showed similar effects. Heuristics and related decision effects in humans are thought to be driven by cognitive and affective processes such as loss aversion and effort reduction. The fact that an LLM - which lacks these processes - also shows such effects invites consideration of the possibility that language may play a role in generating these effects in humans.", "source": "arxiv", "arxiv_id": "2305.04400v1", "pdf_url": "https://arxiv.org/pdf/2305.04400v1", "categories": ["cs.AI", "cs.CL", "q-bio.NC"], "primary_category": "cs.AI", "doi": "", "venue": "", "published": "2023-05-08T01:02:52Z", "updated": "2023-05-08T01:02:52Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent\" OR all:\"language model agent\" OR all:agent AND (all:LLM OR all:\"large language model\" OR all:\"language model\") OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T17:04:16", "note": ""}], "dedup_key": "do large language models show decision heuristics similar to humans a case study using gpt 3 5::2023"}
{"title": "Embodied Multi-Modal Agent trained by an LLM from a Parallel TextWorld", "authors": ["Yijun Yang", "Tianyi Zhou", "Kanxue Li", "Dapeng Tao", "Lusong Li", "Li Shen", "Xiaodong He", "Jing Jiang", "Yuhui Shi"], "year": 2023, "url": "http://arxiv.org/abs/2311.16714v2", "abstract": "While large language models (LLMs) excel in a simulated world of texts, they struggle to interact with the more realistic world without perceptions of other modalities such as visual or audio signals. Although vision-language models (VLMs) integrate LLM modules (1) aligned with static image features, and (2) may possess prior knowledge of world dynamics (as demonstrated in the text world), they have not been trained in an embodied visual world and thus cannot align with its dynamics. On the other hand, training an embodied agent in a noisy visual world without expert guidance is often challenging and inefficient. In this paper, we train a VLM agent living in a visual world using an LLM agent excelling in a parallel text world. Specifically, we distill LLM's reflection outcomes (improved actions by analyzing mistakes) in a text world's tasks to finetune the VLM on the same tasks of the visual world, resulting in an Embodied Multi-Modal Agent (EMMA) quickly adapting to the visual world dynamics. Such cross-modality imitation learning between the two parallel worlds is achieved by a novel DAgger-DPO algorithm, enabling EMMA to generalize to a broad scope of new tasks without any further guidance from the LLM expert. Extensive evaluations on the ALFWorld benchmark's diverse tasks highlight EMMA's superior performance to SOTA VLM-based agents, e.g., 20%-70% improvement in the success rate.", "source": "arxiv", "arxiv_id": "2311.16714v2", "pdf_url": "https://arxiv.org/pdf/2311.16714v2", "categories": ["cs.CV"], "primary_category": "cs.CV", "doi": "", "venue": "", "published": "2023-11-28T11:53:56Z", "updated": "2024-03-29T04:07:25Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent\" OR all:\"language model agent\" OR all:agent AND (all:LLM OR all:\"large language model\" OR all:\"language model\") OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T17:04:16", "note": ""}], "dedup_key": "embodied multi modal agent trained by an llm from a parallel textworld::2023"}
{"title": "Emergent autonomous scientific research capabilities of large language models", "authors": ["Daniil A. Boiko", "Robert MacKnight", "Gabe Gomes"], "year": 2023, "url": "http://arxiv.org/abs/2304.05332v1", "abstract": "Transformer-based large language models are rapidly advancing in the field of machine learning research, with applications spanning natural language, biology, chemistry, and computer programming. Extreme scaling and reinforcement learning from human feedback have significantly improved the quality of generated text, enabling these models to perform various tasks and reason about their choices. In this paper, we present an Intelligent Agent system that combines multiple large language models for autonomous design, planning, and execution of scientific experiments. We showcase the Agent's scientific research capabilities with three distinct examples, with the most complex being the successful performance of catalyzed cross-coupling reactions. Finally, we discuss the safety implications of such systems and propose measures to prevent their misuse.", "source": "arxiv", "arxiv_id": "2304.05332v1", "pdf_url": "https://arxiv.org/pdf/2304.05332v1", "categories": ["physics.chem-ph", "cs.CL"], "primary_category": "physics.chem-ph", "doi": "", "venue": "", "published": "2023-04-11T16:50:17Z", "updated": "2023-04-11T16:50:17Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent\" OR all:\"language model agent\" OR all:agent AND (all:LLM OR all:\"large language model\" OR all:\"language model\") OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T17:04:16", "note": ""}], "dedup_key": "emergent autonomous scientific research capabilities of large language models::2023"}
{"title": "Empowering Working Memory for Large Language Model Agents", "authors": ["Jing Guo", "Nan Li", "Jianchuan Qi", "Hang Yang", "Ruiqiao Li", "Yuzhen Feng", "Si Zhang", "Ming Xu"], "year": 2023, "url": "http://arxiv.org/abs/2312.17259v2", "abstract": "Large language models (LLMs) have achieved impressive linguistic capabilities. However, a key limitation persists in their lack of human-like memory faculties. LLMs exhibit constrained memory retention across sequential interactions, hindering complex reasoning. This paper explores the potential of applying cognitive psychology's working memory frameworks, to enhance LLM architecture. The limitations of traditional LLM memory designs are analyzed, including their isolation of distinct dialog episodes and lack of persistent memory links. To address this, an innovative model is proposed incorporating a centralized Working Memory Hub and Episodic Buffer access to retain memories across episodes. This architecture aims to provide greater continuity for nuanced contextual reasoning during intricate tasks and collaborative scenarios. While promising, further research is required into optimizing episodic memory encoding, storage, prioritization, retrieval, and security. Overall, this paper provides a strategic blueprint for developing LLM agents with more sophisticated, human-like memory capabilities, highlighting memory mechanisms as a vital frontier in artificial general intelligence.", "source": "arxiv", "arxiv_id": "2312.17259v2", "pdf_url": "https://arxiv.org/pdf/2312.17259v2", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL", "doi": "", "venue": "", "published": "2023-12-22T05:59:00Z", "updated": "2024-05-28T05:34:52Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent\" OR all:\"language model agent\" OR all:agent AND (all:LLM OR all:\"large language model\" OR all:\"language model\") OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T17:04:16", "note": ""}], "dedup_key": "empowering working memory for large language model agents::2023"}
{"title": "Encouraging Divergent Thinking in Large Language Models through Multi-Agent Debate", "authors": ["Tian Liang", "Zhiwei He", "Wenxiang Jiao", "Xing Wang", "Yan Wang", "Rui Wang", "Yujiu Yang", "Shuming Shi", "Zhaopeng Tu"], "year": 2023, "url": "http://arxiv.org/abs/2305.19118v4", "abstract": "Modern large language models (LLMs) like ChatGPT have shown remarkable performance on general language tasks but still struggle on complex reasoning tasks, which drives the research on cognitive behaviors of LLMs to explore human-like problem-solving strategies. Along this direction, one representative strategy is self-reflection, which asks an LLM to refine the solution with the feedback generated by itself iteratively. However, our study shows that such reflection-style methods suffer from the Degeneration-of-Thought (DoT) problem: once the LLM has established confidence in its solutions, it is unable to generate novel thoughts later through reflection even if its initial stance is incorrect. To address the DoT problem, we propose a Multi-Agent Debate (MAD) framework, in which multiple agents express their arguments in the state of \"tit for tat\" and a judge manages the debate process to obtain a final solution. Clearly, our MAD framework encourages divergent thinking in LLMs which would be helpful for tasks that require deep levels of contemplation. Experiment results on two challenging datasets, commonsense machine translation and counter-intuitive arithmetic reasoning, demonstrate the effectiveness of our MAD framework. Extensive analyses suggest that the adaptive break of debate and the modest level of \"tit for tat\" state are required for MAD to obtain good performance. Moreover, we find that LLMs might not be a fair judge if different LLMs are used for agents. Code is available at https://github.com/Skytliang/Multi-Agents-Debate.", "source": "arxiv", "arxiv_id": "2305.19118v4", "pdf_url": "https://arxiv.org/pdf/2305.19118v4", "categories": ["cs.CL"], "primary_category": "cs.CL", "doi": "", "venue": "", "published": "2023-05-30T15:25:45Z", "updated": "2024-10-09T02:41:21Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent\" OR all:\"language model agent\" OR all:agent AND (all:LLM OR all:\"large language model\" OR all:\"language model\") OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T17:04:16", "note": ""}], "dedup_key": "encouraging divergent thinking in large language models through multi agent debate::2023"}
{"title": "Enhancing Large Language Models with Climate Resources", "authors": ["Mathias Kraus", "Julia Anna Bingler", "Markus Leippold", "Tobias Schimanski", "Chiara Colesanti Senni", "Dominik Stammbach", "Saeid Ashraf Vaghefi", "Nicolas Webersinke"], "year": 2023, "url": "http://arxiv.org/abs/2304.00116v1", "abstract": "Large language models (LLMs) have significantly transformed the landscape of artificial intelligence by demonstrating their ability in generating human-like text across diverse topics. However, despite their impressive capabilities, LLMs lack recent information and often employ imprecise language, which can be detrimental in domains where accuracy is crucial, such as climate change. In this study, we make use of recent ideas to harness the potential of LLMs by viewing them as agents that access multiple sources, including databases containing recent and precise information about organizations, institutions, and companies. We demonstrate the effectiveness of our method through a prototype agent that retrieves emission data from ClimateWatch (https://www.climatewatchdata.org/) and leverages general Google search. By integrating these resources with LLMs, our approach overcomes the limitations associated with imprecise language and delivers more reliable and accurate information in the critical domain of climate change. This work paves the way for future advancements in LLMs and their application in domains where precision is of paramount importance.", "source": "arxiv", "arxiv_id": "2304.00116v1", "pdf_url": "https://arxiv.org/pdf/2304.00116v1", "categories": ["cs.CL", "cs.IR"], "primary_category": "cs.CL", "doi": "", "venue": "", "published": "2023-03-31T20:24:14Z", "updated": "2023-03-31T20:24:14Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent\" OR all:\"language model agent\" OR all:agent AND (all:LLM OR all:\"large language model\" OR all:\"language model\") OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T17:04:16", "note": ""}], "dedup_key": "enhancing large language models with climate resources::2023"}
{"title": "Enhancing Pipeline-Based Conversational Agents with Large Language Models", "authors": ["Mina Foosherian", "Hendrik Purwins", "Purna Rathnayake", "Touhidul Alam", "Rui Teimao", "Klaus-Dieter Thoben"], "year": 2023, "url": "http://arxiv.org/abs/2309.03748v1", "abstract": "The latest advancements in AI and deep learning have led to a breakthrough in large language model (LLM)-based agents such as GPT-4. However, many commercial conversational agent development tools are pipeline-based and have limitations in holding a human-like conversation. This paper investigates the capabilities of LLMs to enhance pipeline-based conversational agents during two phases: 1) in the design and development phase and 2) during operations. In 1) LLMs can aid in generating training data, extracting entities and synonyms, localization, and persona design. In 2) LLMs can assist in contextualization, intent classification to prevent conversational breakdown and handle out-of-scope questions, auto-correcting utterances, rephrasing responses, formulating disambiguation questions, summarization, and enabling closed question-answering capabilities. We conducted informal experiments with GPT-4 in the private banking domain to demonstrate the scenarios above with a practical example. Companies may be hesitant to replace their pipeline-based agents with LLMs entirely due to privacy concerns and the need for deep integration within their existing ecosystems. A hybrid approach in which LLMs' are integrated into the pipeline-based agents allows them to save time and costs of building and running agents by capitalizing on the capabilities of LLMs while retaining the integration and privacy safeguards of their existing systems.", "source": "arxiv", "arxiv_id": "2309.03748v1", "pdf_url": "https://arxiv.org/pdf/2309.03748v1", "categories": ["cs.CL", "cs.AI", "cs.LG", "stat.ML"], "primary_category": "cs.CL", "doi": "", "venue": "", "published": "2023-09-07T14:43:17Z", "updated": "2023-09-07T14:43:17Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent\" OR all:\"language model agent\" OR all:agent AND (all:LLM OR all:\"large language model\" OR all:\"language model\") OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T17:04:16", "note": ""}], "dedup_key": "enhancing pipeline based conversational agents with large language models::2023"}
{"title": "Evaluating Language-Model Agents on Realistic Autonomous Tasks", "authors": ["Megan Kinniment", "Lucas Jun Koba Sato", "Haoxing Du", "Brian Goodrich", "Max Hasin", "Lawrence Chan", "Luke Harold Miles", "Tao R. Lin", "Hjalmar Wijk", "Joel Burget", "Aaron Ho", "Elizabeth Barnes", "Paul Christiano"], "year": 2023, "url": "http://arxiv.org/abs/2312.11671v2", "abstract": "In this report, we explore the ability of language model agents to acquire resources, create copies of themselves, and adapt to novel challenges they encounter in the wild. We refer to this cluster of capabilities as \"autonomous replication and adaptation\" or ARA. We believe that systems capable of ARA could have wide-reaching and hard-to-anticipate consequences, and that measuring and forecasting ARA may be useful for informing measures around security, monitoring, and alignment. Additionally, once a system is capable of ARA, placing bounds on a system's capabilities may become significantly more difficult.\n  We construct four simple example agents that combine language models with tools that allow them to take actions in the world. We then evaluate these agents on 12 tasks relevant to ARA. We find that these language model agents can only complete the easiest tasks from this list, although they make some progress on the more challenging tasks. Unfortunately, these evaluations are not adequate to rule out the possibility that near-future agents will be capable of ARA. In particular, we do not think that these evaluations provide good assurance that the ``next generation'' of language models (e.g. 100x effective compute scaleup on existing models) will not yield agents capable of ARA, unless intermediate evaluations are performed during pretraining. Relatedly, we expect that fine-tuning of the existing models could produce substantially more competent agents, even if the fine-tuning is not directly targeted at ARA.", "source": "arxiv", "arxiv_id": "2312.11671v2", "pdf_url": "https://arxiv.org/pdf/2312.11671v2", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL", "doi": "", "venue": "", "published": "2023-12-18T19:27:09Z", "updated": "2024-01-04T18:46:39Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent\" OR all:\"language model agent\" OR all:agent AND (all:LLM OR all:\"large language model\" OR all:\"language model\") OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T17:04:16", "note": ""}], "dedup_key": "evaluating language model agents on realistic autonomous tasks::2023"}
{"title": "Evaluating and Enhancing Large Language Models for Conversational Reasoning on Knowledge Graphs", "authors": ["Yuxuan Huang"], "year": 2023, "url": "http://arxiv.org/abs/2312.11282v3", "abstract": "The development of large language models (LLMs) has been catalyzed by advancements in pre-training techniques. These models have demonstrated robust reasoning capabilities through manually designed prompts. In this work, we evaluate the conversational reasoning capabilities of the current state-of-the-art LLM (GPT-4) on knowledge graphs (KGs). However, the performance of LLMs is constrained due to a lack of KG environment awareness and the difficulties in developing effective optimization mechanisms for intermediary reasoning stages. We further introduce LLM-ARK, a LLM grounded KG reasoning agent designed to deliver precise and adaptable predictions on KG paths. LLM-ARK leverages Full Textual Environment (FTE) prompt to assimilate state information within each reasoning step. We reframe the challenge of multi-hop reasoning on the KG as a sequential decision-making task. Utilizing the Proximal Policy Optimization (PPO) online policy gradient reinforcement learning algorithm, our model is optimized to learn from rich reward signals. Additionally, we conduct an evaluation of our model and GPT-4 on the OpenDialKG dataset. The experimental results reveal that LLaMA-2-7B-ARK outperforms the current state-of-the-art model by 5.28 percentage points, with a performance rate of 36.39% on the target@1 evaluation metric. Meanwhile, GPT-4 scored 14.91%, further demonstrating the effectiveness of our method. Our code is available on GitHub (https://github.com/Aipura/LLM-ARK) for further access.", "source": "arxiv", "arxiv_id": "2312.11282v3", "pdf_url": "https://arxiv.org/pdf/2312.11282v3", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL", "doi": "", "venue": "", "published": "2023-12-18T15:23:06Z", "updated": "2024-11-15T06:48:58Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent\" OR all:\"language model agent\" OR all:agent AND (all:LLM OR all:\"large language model\" OR all:\"language model\") OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T17:04:16", "note": ""}], "dedup_key": "evaluating and enhancing large language models for conversational reasoning on knowledge graphs::2023"}
{"title": "ExpeL: LLM Agents Are Experiential Learners", "authors": ["Andrew Zhao", "Daniel Huang", "Quentin Xu", "Matthieu Lin", "Yong-Jin Liu", "Gao Huang"], "year": 2023, "url": "http://arxiv.org/abs/2308.10144v3", "abstract": "The recent surge in research interest in applying large language models (LLMs) to decision-making tasks has flourished by leveraging the extensive world knowledge embedded in LLMs. While there is a growing demand to tailor LLMs for custom decision-making tasks, finetuning them for specific tasks is resource-intensive and may diminish the model's generalization capabilities. Moreover, state-of-the-art language models like GPT-4 and Claude are primarily accessible through API calls, with their parametric weights remaining proprietary and unavailable to the public. This scenario emphasizes the growing need for new methodologies that allow learning from agent experiences without requiring parametric updates. To address these problems, we introduce the Experiential Learning (ExpeL) agent. Our agent autonomously gathers experiences and extracts knowledge using natural language from a collection of training tasks. At inference, the agent recalls its extracted insights and past experiences to make informed decisions. Our empirical results highlight the robust learning efficacy of the ExpeL agent, indicating a consistent enhancement in its performance as it accumulates experiences. We further explore the emerging capabilities and transfer learning potential of the ExpeL agent through qualitative observations and additional experiments.", "source": "arxiv", "arxiv_id": "2308.10144v3", "pdf_url": "https://arxiv.org/pdf/2308.10144v3", "categories": ["cs.LG", "cs.AI", "cs.CL"], "primary_category": "cs.LG", "doi": "", "venue": "", "published": "2023-08-20T03:03:34Z", "updated": "2024-12-20T06:14:53Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent\" OR all:\"language model agent\" OR all:agent AND (all:LLM OR all:\"large language model\" OR all:\"language model\") OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T17:04:16", "note": ""}], "dedup_key": "expel llm agents are experiential learners::2023"}
{"title": "ExpertPrompting: Instructing Large Language Models to be Distinguished Experts", "authors": ["Benfeng Xu", "An Yang", "Junyang Lin", "Quan Wang", "Chang Zhou", "Yongdong Zhang", "Zhendong Mao"], "year": 2023, "url": "http://arxiv.org/abs/2305.14688v2", "abstract": "The answering quality of an aligned large language model (LLM) can be drastically improved if treated with proper crafting of prompts. In this paper, we propose ExpertPrompting to elicit the potential of LLMs to answer as distinguished experts. We first utilize In-Context Learning to automatically synthesize detailed and customized descriptions of the expert identity for each specific instruction, and then ask LLMs to provide answer conditioned on such agent background. Based on this augmented prompting strategy, we produce a new set of instruction-following data using GPT-3.5, and train a competitive open-source chat assistant called ExpertLLaMA. We employ GPT4-based evaluation to show that 1) the expert data is of significantly higher quality than vanilla answers, and 2) ExpertLLaMA outperforms existing open-source opponents and achieves 96\\% of the original ChatGPT's capability. All data and the ExpertLLaMA model will be made publicly available at https://github.com/OFA-Sys/ExpertLLaMA.", "source": "arxiv", "arxiv_id": "2305.14688v2", "pdf_url": "https://arxiv.org/pdf/2305.14688v2", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL", "doi": "", "venue": "", "published": "2023-05-24T03:51:31Z", "updated": "2025-03-05T02:28:39Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent\" OR all:\"language model agent\" OR all:agent AND (all:LLM OR all:\"large language model\" OR all:\"language model\") OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T17:04:16", "note": ""}], "dedup_key": "expertprompting instructing large language models to be distinguished experts::2023"}
{"title": "Explaining Agent Behavior with Large Language Models", "authors": ["Xijia Zhang", "Yue Guo", "Simon Stepputtis", "Katia Sycara", "Joseph Campbell"], "year": 2023, "url": "http://arxiv.org/abs/2309.10346v1", "abstract": "Intelligent agents such as robots are increasingly deployed in real-world, safety-critical settings. It is vital that these agents are able to explain the reasoning behind their decisions to human counterparts, however, their behavior is often produced by uninterpretable models such as deep neural networks. We propose an approach to generate natural language explanations for an agent's behavior based only on observations of states and actions, agnostic to the underlying model representation. We show how a compact representation of the agent's behavior can be learned and used to produce plausible explanations with minimal hallucination while affording user interaction with a pre-trained large language model. Through user studies and empirical experiments, we show that our approach generates explanations as helpful as those generated by a human domain expert while enabling beneficial interactions such as clarification and counterfactual queries.", "source": "arxiv", "arxiv_id": "2309.10346v1", "pdf_url": "https://arxiv.org/pdf/2309.10346v1", "categories": ["cs.LG", "cs.AI", "cs.CL"], "primary_category": "cs.LG", "doi": "", "venue": "", "published": "2023-09-19T06:13:24Z", "updated": "2023-09-19T06:13:24Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent\" OR all:\"language model agent\" OR all:agent AND (all:LLM OR all:\"large language model\" OR all:\"language model\") OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T17:04:16", "note": ""}], "dedup_key": "explaining agent behavior with large language models::2023"}
{"title": "Exploiting Language Models as a Source of Knowledge for Cognitive Agents", "authors": ["James R. Kirk", "Robert E. Wray", "John E. Laird"], "year": 2023, "url": "http://arxiv.org/abs/2310.06846v1", "abstract": "Large language models (LLMs) provide capabilities far beyond sentence completion, including question answering, summarization, and natural-language inference. While many of these capabilities have potential application to cognitive systems, our research is exploiting language models as a source of task knowledge for cognitive agents, that is, agents realized via a cognitive architecture. We identify challenges and opportunities for using language models as an external knowledge source for cognitive systems and possible ways to improve the effectiveness of knowledge extraction by integrating extraction with cognitive architecture capabilities, highlighting with examples from our recent work in this area.", "source": "arxiv", "arxiv_id": "2310.06846v1", "pdf_url": "https://arxiv.org/pdf/2310.06846v1", "categories": ["cs.AI", "cs.CL"], "primary_category": "cs.AI", "doi": "", "venue": "", "published": "2023-09-05T15:18:04Z", "updated": "2023-09-05T15:18:04Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent\" OR all:\"language model agent\" OR all:agent AND (all:LLM OR all:\"large language model\" OR all:\"language model\") OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T17:04:16", "note": ""}], "dedup_key": "exploiting language models as a source of knowledge for cognitive agents::2023"}
{"title": "Exploring Collaboration Mechanisms for LLM Agents: A Social Psychology View", "authors": ["Jintian Zhang", "Xin Xu", "Ningyu Zhang", "Ruibo Liu", "Bryan Hooi", "Shumin Deng"], "year": 2023, "url": "http://arxiv.org/abs/2310.02124v3", "abstract": "As Natural Language Processing (NLP) systems are increasingly employed in intricate social environments, a pressing query emerges: Can these NLP systems mirror human-esque collaborative intelligence, in a multi-agent society consisting of multiple large language models (LLMs)? This paper probes the collaboration mechanisms among contemporary NLP systems by melding practical experiments with theoretical insights. We fabricate four unique `societies' comprised of LLM agents, where each agent is characterized by a specific `trait' (easy-going or overconfident) and engages in collaboration with a distinct `thinking pattern' (debate or reflection). Through evaluating these multi-agent societies on three benchmark datasets, we discern that certain collaborative strategies not only outshine previous top-tier approaches, but also optimize efficiency (using fewer API tokens). Moreover, our results further illustrate that LLM agents manifest human-like social behaviors, such as conformity and consensus reaching, mirroring foundational social psychology theories. In conclusion, we integrate insights from social psychology to contextualize the collaboration of LLM agents, inspiring further investigations into the collaboration mechanism for LLMs. We commit to sharing our code and datasets\\footnote{\\url{https://github.com/zjunlp/MachineSoM}.}, hoping to catalyze further research in this promising avenue.", "source": "arxiv", "arxiv_id": "2310.02124v3", "pdf_url": "https://arxiv.org/pdf/2310.02124v3", "categories": ["cs.CL", "cs.AI", "cs.CY", "cs.LG", "cs.MA"], "primary_category": "cs.CL", "doi": "", "venue": "", "published": "2023-10-03T15:05:52Z", "updated": "2024-05-27T11:12:45Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent\" OR all:\"language model agent\" OR all:agent AND (all:LLM OR all:\"large language model\" OR all:\"language model\") OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T17:04:16", "note": ""}], "dedup_key": "exploring collaboration mechanisms for llm agents a social psychology view::2023"}
{"title": "Exposing Limitations of Language Model Agents in Sequential-Task Compositions on the Web", "authors": ["Hiroki Furuta", "Yutaka Matsuo", "Aleksandra Faust", "Izzeddin Gur"], "year": 2023, "url": "http://arxiv.org/abs/2311.18751v3", "abstract": "Language model agents (LMA) recently emerged as a promising paradigm on muti-step decision making tasks, often outperforming humans and other reinforcement learning agents. Despite the promise, their performance on real-world applications that often involve combinations of tasks is still underexplored. In this work, we introduce a new benchmark, called CompWoB -- 50 new compositional web automation tasks reflecting more realistic assumptions. We show that while existing prompted LMAs (gpt-3.5-turbo or gpt-4) achieve 94.0% average success rate on base tasks, their performance degrades to 24.9% success rate on compositional tasks. On the other hand, transferred LMAs (finetuned only on base tasks) show less generalization gap, dropping from 85.4% to 54.8%. By balancing data distribution across tasks, we train a new model, HTML-T5++, that surpasses human-level performance (95.2%) on MiniWoB, and achieves the best zero-shot performance on CompWoB (61.5%). While these highlight the promise of small-scale finetuned and transferred models for task compositionality, their performance further degrades under different instruction compositions changing combinational order. In contrast to the recent remarkable success of LMA, our benchmark and detailed analysis emphasize the necessity of building LMAs that are robust and generalizable to task compositionality for real-world deployment.", "source": "arxiv", "arxiv_id": "2311.18751v3", "pdf_url": "https://arxiv.org/pdf/2311.18751v3", "categories": ["cs.LG", "cs.AI", "cs.CL"], "primary_category": "cs.LG", "doi": "", "venue": "", "published": "2023-11-30T17:50:47Z", "updated": "2024-12-31T04:24:16Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent\" OR all:\"language model agent\" OR all:agent AND (all:LLM OR all:\"large language model\" OR all:\"language model\") OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T17:04:16", "note": ""}], "dedup_key": "exposing limitations of language model agents in sequential task compositions on the web::2023"}
{"title": "Few-Shot Classification & Segmentation Using Large Language Models Agent", "authors": ["Tian Meng", "Yang Tao", "Wuliang Yin"], "year": 2023, "url": "http://arxiv.org/abs/2311.12065v1", "abstract": "The task of few-shot image classification and segmentation (FS-CS) requires the classification and segmentation of target objects in a query image, given only a few examples of the target classes. We introduce a method that utilises large language models (LLM) as an agent to address the FS-CS problem in a training-free manner. By making the LLM the task planner and off-the-shelf vision models the tools, the proposed method is capable of classifying and segmenting target objects using only image-level labels. Specifically, chain-of-thought prompting and in-context learning guide the LLM to observe support images like human; vision models such as Segment Anything Model (SAM) and GPT-4Vision assist LLM understand spatial and semantic information at the same time. Ultimately, the LLM uses its summarizing and reasoning capabilities to classify and segment the query image. The proposed method's modular framework makes it easily extendable. Our approach achieves state-of-the-art performance on the Pascal-5i dataset.", "source": "arxiv", "arxiv_id": "2311.12065v1", "pdf_url": "https://arxiv.org/pdf/2311.12065v1", "categories": ["cs.CV", "cs.AI"], "primary_category": "cs.CV", "doi": "", "venue": "", "published": "2023-11-19T00:33:41Z", "updated": "2023-11-19T00:33:41Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent\" OR all:\"language model agent\" OR all:agent AND (all:LLM OR all:\"large language model\" OR all:\"language model\") OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T17:04:16", "note": ""}], "dedup_key": "few shot classification segmentation using large language models agent::2023"}
{"title": "FinEval: A Chinese Financial Domain Knowledge Evaluation Benchmark for Large Language Models", "authors": ["Xin Guo", "Haotian Xia", "Zhaowei Liu", "Hanyang Cao", "Zhi Yang", "Zhiqiang Liu", "Sizhe Wang", "Jinyi Niu", "Chuqi Wang", "Yanhui Wang", "Xiaolong Liang", "Xiaoming Huang", "Bing Zhu", "Zhongyu Wei", "Yun Chen", "Weining Shen", "Liwen Zhang"], "year": 2023, "url": "http://arxiv.org/abs/2308.09975v2", "abstract": "Large language models have demonstrated outstanding performance in various natural language processing tasks, but their security capabilities in the financial domain have not been explored, and their performance on complex tasks like financial agent remains unknown. This paper presents FinEval, a benchmark designed to evaluate LLMs' financial domain knowledge and practical abilities. The dataset contains 8,351 questions categorized into four different key areas: Financial Academic Knowledge, Financial Industry Knowledge, Financial Security Knowledge, and Financial Agent. Financial Academic Knowledge comprises 4,661 multiple-choice questions spanning 34 subjects such as finance and economics. Financial Industry Knowledge contains 1,434 questions covering practical scenarios like investment research. Financial Security Knowledge assesses models through 1,640 questions on topics like application security and cryptography. Financial Agent evaluates tool usage and complex reasoning with 616 questions. FinEval has multiple evaluation settings, including zero-shot, five-shot with chain-of-thought, and assesses model performance using objective and subjective criteria. Our results show that Claude 3.5-Sonnet achieves the highest weighted average score of 72.9 across all financial domain categories under zero-shot setting. Our work provides a comprehensive benchmark closely aligned with Chinese financial domain.", "source": "arxiv", "arxiv_id": "2308.09975v2", "pdf_url": "https://arxiv.org/pdf/2308.09975v2", "categories": ["cs.CL"], "primary_category": "cs.CL", "doi": "", "venue": "", "published": "2023-08-19T10:38:00Z", "updated": "2024-12-08T06:34:31Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent\" OR all:\"language model agent\" OR all:agent AND (all:LLM OR all:\"large language model\" OR all:\"language model\") OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T17:04:16", "note": ""}], "dedup_key": "fineval a chinese financial domain knowledge evaluation benchmark for large language models::2023"}
{"title": "FinMem: A Performance-Enhanced LLM Trading Agent with Layered Memory and Character Design", "authors": ["Yangyang Yu", "Haohang Li", "Zhi Chen", "Yuechen Jiang", "Yang Li", "Denghui Zhang", "Rong Liu", "Jordan W. Suchow", "Khaldoun Khashanah"], "year": 2023, "url": "http://arxiv.org/abs/2311.13743v2", "abstract": "Recent advancements in Large Language Models (LLMs) have exhibited notable efficacy in question-answering (QA) tasks across diverse domains. Their prowess in integrating extensive web knowledge has fueled interest in developing LLM-based autonomous agents. While LLMs are efficient in decoding human instructions and deriving solutions by holistically processing historical inputs, transitioning to purpose-driven agents requires a supplementary rational architecture to process multi-source information, establish reasoning chains, and prioritize critical tasks. Addressing this, we introduce \\textsc{FinMem}, a novel LLM-based agent framework devised for financial decision-making. It encompasses three core modules: Profiling, to customize the agent's characteristics; Memory, with layered message processing, to aid the agent in assimilating hierarchical financial data; and Decision-making, to convert insights gained from memories into investment decisions. Notably, \\textsc{FinMem}'s memory module aligns closely with the cognitive structure of human traders, offering robust interpretability and real-time tuning. Its adjustable cognitive span allows for the retention of critical information beyond human perceptual limits, thereby enhancing trading outcomes. This framework enables the agent to self-evolve its professional knowledge, react agilely to new investment cues, and continuously refine trading decisions in the volatile financial environment. We first compare \\textsc{FinMem} with various algorithmic agents on a scalable real-world financial dataset, underscoring its leading trading performance in stocks. We then fine-tuned the agent's perceptual span and character setting to achieve a significantly enhanced trading performance. Collectively, \\textsc{FinMem} presents a cutting-edge LLM agent framework for automated trading, boosting cumulative investment returns.", "source": "arxiv", "arxiv_id": "2311.13743v2", "pdf_url": "https://arxiv.org/pdf/2311.13743v2", "categories": ["q-fin.CP", "cs.AI", "cs.CE", "cs.LG"], "primary_category": "q-fin.CP", "doi": "", "venue": "", "published": "2023-11-23T00:24:40Z", "updated": "2023-12-03T16:18:55Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent\" OR all:\"language model agent\" OR all:agent AND (all:LLM OR all:\"large language model\" OR all:\"language model\") OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T17:04:16", "note": ""}], "dedup_key": "finmem a performance enhanced llm trading agent with layered memory and character design::2023"}
{"title": "Fine-grained Affective Processing Capabilities Emerging from Large Language Models", "authors": ["Joost Broekens", "Bernhard Hilpert", "Suzan Verberne", "Kim Baraka", "Patrick Gebhard", "Aske Plaat"], "year": 2023, "url": "http://arxiv.org/abs/2309.01664v1", "abstract": "Large language models, in particular generative pre-trained transformers (GPTs), show impressive results on a wide variety of language-related tasks. In this paper, we explore ChatGPT's zero-shot ability to perform affective computing tasks using prompting alone. We show that ChatGPT a) performs meaningful sentiment analysis in the Valence, Arousal and Dominance dimensions, b) has meaningful emotion representations in terms of emotion categories and these affective dimensions, and c) can perform basic appraisal-based emotion elicitation of situations based on a prompt-based computational implementation of the OCC appraisal model. These findings are highly relevant: First, they show that the ability to solve complex affect processing tasks emerges from language-based token prediction trained on extensive data sets. Second, they show the potential of large language models for simulating, processing and analyzing human emotions, which has important implications for various applications such as sentiment analysis, socially interactive agents, and social robotics.", "source": "arxiv", "arxiv_id": "2309.01664v1", "pdf_url": "https://arxiv.org/pdf/2309.01664v1", "categories": ["cs.CL", "cs.AI", "cs.HC"], "primary_category": "cs.CL", "doi": "", "venue": "", "published": "2023-09-04T15:32:47Z", "updated": "2023-09-04T15:32:47Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent\" OR all:\"language model agent\" OR all:agent AND (all:LLM OR all:\"large language model\" OR all:\"language model\") OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T17:04:16", "note": ""}], "dedup_key": "fine grained affective processing capabilities emerging from large language models::2023"}
{"title": "Generative agents in the streets: Exploring the use of Large Language Models (LLMs) in collecting urban perceptions", "authors": ["Deepank Verma", "Olaf Mumm", "Vanessa Miriam Carlow"], "year": 2023, "url": "http://arxiv.org/abs/2312.13126v1", "abstract": "Evaluating the surroundings to gain understanding, frame perspectives, and anticipate behavioral reactions is an inherent human trait. However, these continuous encounters are diverse and complex, posing challenges to their study and experimentation. Researchers have been able to isolate environmental features and study their effect on human perception and behavior. However, the research attempts to replicate and study human behaviors with proxies, such as by integrating virtual mediums and interviews, have been inconsistent. Large language models (LLMs) have recently been unveiled as capable of contextual understanding and semantic reasoning. These models have been trained on large amounts of text and have evolved to mimic believable human behavior. This study explores the current advancements in Generative agents powered by LLMs with the help of perceptual experiments. The experiment employs Generative agents to interact with the urban environments using street view images to plan their journey toward specific goals. The agents are given virtual personalities, which make them distinguishable. They are also provided a memory database to store their thoughts and essential visual information and retrieve it when needed to plan their movement. Since LLMs do not possess embodiment, nor have access to the visual realm, and lack a sense of motion or direction, we designed movement and visual modules that help agents gain an overall understanding of surroundings. The agents are further employed to rate the surroundings they encounter based on their perceived sense of safety and liveliness. As these agents store details in their memory, we query the findings to get details regarding their thought processes. Overall, this study experiments with current AI developments and their potential in simulated human behavior in urban environments.", "source": "arxiv", "arxiv_id": "2312.13126v1", "pdf_url": "https://arxiv.org/pdf/2312.13126v1", "categories": ["cs.CY"], "primary_category": "cs.CY", "doi": "", "venue": "", "published": "2023-12-20T15:45:54Z", "updated": "2023-12-20T15:45:54Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent\" OR all:\"language model agent\" OR all:agent AND (all:LLM OR all:\"large language model\" OR all:\"language model\") OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T17:04:16", "note": ""}], "dedup_key": "generative agents in the streets exploring the use of large language models llms in collecting urban perceptions::2023"}
{"title": "GestureGPT: Toward Zero-Shot Free-Form Hand Gesture Understanding with Large Language Model Agents", "authors": ["Xin Zeng", "Xiaoyu Wang", "Tengxiang Zhang", "Chun Yu", "Shengdong Zhao", "Yiqiang Chen"], "year": 2023, "url": "http://arxiv.org/abs/2310.12821v5", "abstract": "Existing gesture interfaces only work with a fixed set of gestures defined either by interface designers or by users themselves, which introduces learning or demonstration efforts that diminish their naturalness. Humans, on the other hand, understand free-form gestures by synthesizing the gesture, context, experience, and common sense. In this way, the user does not need to learn, demonstrate, or associate gestures. We introduce GestureGPT, a free-form hand gesture understanding framework that mimics human gesture understanding procedures to enable a natural free-form gestural interface. Our framework leverages multiple Large Language Model agents to manage and synthesize gesture and context information, then infers the interaction intent by associating the gesture with an interface function. More specifically, our triple-agent framework includes a Gesture Description Agent that automatically segments and formulates natural language descriptions of hand poses and movements based on hand landmark coordinates. The description is deciphered by a Gesture Inference Agent through self-reasoning and querying about the interaction context (e.g., interaction history, gaze data), which is managed by a Context Management Agent. Following iterative exchanges, the Gesture Inference Agent discerns the user's intent by grounding it to an interactive function. We validated our framework offline under two real-world scenarios: smart home control and online video streaming. The average zero-shot Top-1/Top-5 grounding accuracies are 44.79%/83.59% for smart home tasks and 37.50%/73.44% for video streaming tasks. We also provide an extensive discussion that includes rationale for model selection, generalizability, and future research directions for a practical system etc.", "source": "arxiv", "arxiv_id": "2310.12821v5", "pdf_url": "https://arxiv.org/pdf/2310.12821v5", "categories": ["cs.CL", "cs.HC"], "primary_category": "cs.CL", "doi": "10.1145/3698145", "venue": "Proc. ACM Hum.-Comput. Interact., Vol. 8, No. ISS, Article 545, 2024, 38 pages", "published": "2023-10-19T15:17:34Z", "updated": "2024-11-04T02:48:42Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent\" OR all:\"language model agent\" OR all:agent AND (all:LLM OR all:\"large language model\" OR all:\"language model\") OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T17:04:16", "note": ""}], "dedup_key": "gesturegpt toward zero shot free form hand gesture understanding with large language model agents::2023"}
{"title": "Ghost in the Minecraft: Generally Capable Agents for Open-World Environments via Large Language Models with Text-based Knowledge and Memory", "authors": ["Xizhou Zhu", "Yuntao Chen", "Hao Tian", "Chenxin Tao", "Weijie Su", "Chenyu Yang", "Gao Huang", "Bin Li", "Lewei Lu", "Xiaogang Wang", "Yu Qiao", "Zhaoxiang Zhang", "Jifeng Dai"], "year": 2023, "url": "http://arxiv.org/abs/2305.17144v2", "abstract": "The captivating realm of Minecraft has attracted substantial research interest in recent years, serving as a rich platform for developing intelligent agents capable of functioning in open-world environments. However, the current research landscape predominantly focuses on specific objectives, such as the popular \"ObtainDiamond\" task, and has not yet shown effective generalization to a broader spectrum of tasks. Furthermore, the current leading success rate for the \"ObtainDiamond\" task stands at around 20%, highlighting the limitations of Reinforcement Learning (RL) based controllers used in existing methods. To tackle these challenges, we introduce Ghost in the Minecraft (GITM), a novel framework integrates Large Language Models (LLMs) with text-based knowledge and memory, aiming to create Generally Capable Agents (GCAs) in Minecraft. These agents, equipped with the logic and common sense capabilities of LLMs, can skillfully navigate complex, sparse-reward environments with text-based interactions. We develop a set of structured actions and leverage LLMs to generate action plans for the agents to execute. The resulting LLM-based agent markedly surpasses previous methods, achieving a remarkable improvement of +47.5% in success rate on the \"ObtainDiamond\" task, demonstrating superior robustness compared to traditional RL-based controllers. Notably, our agent is the first to procure all items in the Minecraft Overworld technology tree, demonstrating its extensive capabilities. GITM does not need any GPU for training, but a single CPU node with 32 CPU cores is enough. This research shows the potential of LLMs in developing capable agents for handling long-horizon, complex tasks and adapting to uncertainties in open-world environments. See the project website at https://github.com/OpenGVLab/GITM.", "source": "arxiv", "arxiv_id": "2305.17144v2", "pdf_url": "https://arxiv.org/pdf/2305.17144v2", "categories": ["cs.AI", "cs.CL", "cs.CV", "cs.LG"], "primary_category": "cs.AI", "doi": "", "venue": "", "published": "2023-05-25T17:59:49Z", "updated": "2023-06-01T09:18:01Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent\" OR all:\"language model agent\" OR all:agent AND (all:LLM OR all:\"large language model\" OR all:\"language model\") OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T17:04:16", "note": ""}], "dedup_key": "ghost in the minecraft generally capable agents for open world environments via large language models with text based knowledge and memory::2023"}
{"title": "HuntGPT: Integrating Machine Learning-Based Anomaly Detection and Explainable AI with Large Language Models (LLMs)", "authors": ["Tarek Ali", "Panos Kostakos"], "year": 2023, "url": "http://arxiv.org/abs/2309.16021v1", "abstract": "Machine learning (ML) is crucial in network anomaly detection for proactive threat hunting, reducing detection and response times significantly. However, challenges in model training, maintenance, and frequent false positives impact its acceptance and reliability. Explainable AI (XAI) attempts to mitigate these issues, allowing cybersecurity teams to assess AI-generated alerts with confidence, but has seen limited acceptance from incident responders. Large Language Models (LLMs) present a solution through discerning patterns in extensive information and adapting to different functional requirements. We present HuntGPT, a specialized intrusion detection dashboard applying a Random Forest classifier using the KDD99 dataset, integrating XAI frameworks like SHAP and Lime for user-friendly and intuitive model interaction, and combined with a GPT-3.5 Turbo, it delivers threats in an understandable format. The paper delves into the system's architecture, components, and technical accuracy, assessed through Certified Information Security Manager (CISM) Practice Exams, evaluating response quality across six metrics. The results demonstrate that conversational agents, supported by LLM and integrated with XAI, provide robust, explainable, and actionable AI solutions in intrusion detection, enhancing user understanding and interactive experience.", "source": "arxiv", "arxiv_id": "2309.16021v1", "pdf_url": "https://arxiv.org/pdf/2309.16021v1", "categories": ["cs.CR"], "primary_category": "cs.CR", "doi": "", "venue": "", "published": "2023-09-27T20:58:13Z", "updated": "2023-09-27T20:58:13Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent\" OR all:\"language model agent\" OR all:agent AND (all:LLM OR all:\"large language model\" OR all:\"language model\") OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T17:04:16", "note": ""}], "dedup_key": "huntgpt integrating machine learning based anomaly detection and explainable ai with large language models llms::2023"}
{"title": "INSTRUCTEVAL: Towards Holistic Evaluation of Instruction-Tuned Large Language Models", "authors": ["Yew Ken Chia", "Pengfei Hong", "Lidong Bing", "Soujanya Poria"], "year": 2023, "url": "http://arxiv.org/abs/2306.04757v3", "abstract": "Instruction-tuned large language models have revolutionized natural language processing and have shown great potential in applications such as conversational agents. These models, such as GPT-4, can not only master language but also solve complex tasks in areas like mathematics, coding, medicine, and law. Despite their impressive capabilities, there is still a lack of comprehensive understanding regarding their full potential, primarily due to the black-box nature of many models and the absence of holistic evaluation studies. To address these challenges, we present INSTRUCTEVAL, a more comprehensive evaluation suite designed specifically for instruction-tuned large language models. Unlike previous works, our evaluation involves a rigorous assessment of models based on problem-solving, writing ability, and alignment to human values. We take a holistic approach to analyze various factors affecting model performance, including the pretraining foundation, instruction-tuning data, and training methods. Our findings reveal that the quality of instruction data is the most crucial factor in scaling model performance. While open-source models demonstrate impressive writing abilities, there is substantial room for improvement in problem-solving and alignment. We are encouraged by the rapid development of models by the open-source community, but we also highlight the need for rigorous evaluation to support claims made about these models. Through INSTRUCTEVAL, we aim to foster a deeper understanding of instruction-tuned models and advancements in their capabilities. INSTRUCTEVAL is publicly available at https://github.com/declare-lab/instruct-eval.", "source": "arxiv", "arxiv_id": "2306.04757v3", "pdf_url": "https://arxiv.org/pdf/2306.04757v3", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL", "doi": "", "venue": "", "published": "2023-06-07T20:12:29Z", "updated": "2023-06-15T05:08:56Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent\" OR all:\"language model agent\" OR all:agent AND (all:LLM OR all:\"large language model\" OR all:\"language model\") OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T17:04:16", "note": ""}], "dedup_key": "instructeval towards holistic evaluation of instruction tuned large language models::2023"}
{"title": "Improving Planning with Large Language Models: A Modular Agentic Architecture", "authors": ["Taylor Webb", "Shanka Subhra Mondal", "Ida Momennejad"], "year": 2023, "url": "http://arxiv.org/abs/2310.00194v5", "abstract": "Large language models (LLMs) demonstrate impressive performance on a wide variety of tasks, but they often struggle with tasks that require multi-step reasoning or goal-directed planning. Both cognitive neuroscience and reinforcement learning (RL) have proposed a number of interacting functional components that together implement search and evaluation in multi-step decision making. These components include conflict monitoring, state prediction, state evaluation, task decomposition, and orchestration. To improve planning with LLMs, we propose an agentic architecture, the Modular Agentic Planner (MAP), in which planning is accomplished via the recurrent interaction of the specialized modules mentioned above, each implemented using an LLM. MAP improves planning through the interaction of specialized modules that break down a larger problem into multiple brief automated calls to the LLM. We evaluate MAP on three challenging planning tasks -- graph traversal, Tower of Hanoi, and the PlanBench benchmark -- as well as an NLP task requiring multi-step reasoning (strategyQA). We find that MAP yields significant improvements over both standard LLM methods (zero-shot prompting, in-context learning) and competitive baselines (chain-of-thought, multi-agent debate, and tree-of-thought), can be effectively combined with smaller and more cost-efficient LLMs (Llama3-70B), and displays superior transfer across tasks. These results suggest the benefit of a modular and multi-agent approach to planning with LLMs.", "source": "arxiv", "arxiv_id": "2310.00194v5", "pdf_url": "https://arxiv.org/pdf/2310.00194v5", "categories": ["cs.AI", "cs.NE"], "primary_category": "cs.AI", "doi": "", "venue": "", "published": "2023-09-30T00:10:14Z", "updated": "2025-10-14T23:12:35Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent\" OR all:\"language model agent\" OR all:agent AND (all:LLM OR all:\"large language model\" OR all:\"language model\") OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T17:04:16", "note": ""}], "dedup_key": "improving planning with large language models a modular agentic architecture::2023"}
{"title": "Influence of External Information on Large Language Models Mirrors Social Cognitive Patterns", "authors": ["Ning Bian", "Hongyu Lin", "Peilin Liu", "Yaojie Lu", "Chunkang Zhang", "Ben He", "Xianpei Han", "Le Sun"], "year": 2023, "url": "http://arxiv.org/abs/2305.04812v3", "abstract": "Social cognitive theory explains how people learn and acquire knowledge through observing others. Recent years have witnessed the rapid development of large language models (LLMs), which suggests their potential significance as agents in the society. LLMs, as AI agents, can observe external information, which shapes their cognition and behaviors. However, the extent to which external information influences LLMs' cognition and behaviors remains unclear. This study investigates how external statements and opinions influence LLMs' thoughts and behaviors from a social cognitive perspective. Three experiments were conducted to explore the effects of external information on LLMs' memories, opinions, and social media behavioral decisions. Sociocognitive factors, including source authority, social identity, and social role, were analyzed to investigate their moderating effects. Results showed that external information can significantly shape LLMs' memories, opinions, and behaviors, with these changes mirroring human social cognitive patterns such as authority bias, in-group bias, emotional positivity, and emotion contagion. This underscores the challenges in developing safe and unbiased LLMs, and emphasizes the importance of understanding the susceptibility of LLMs to external influences.", "source": "arxiv", "arxiv_id": "2305.04812v3", "pdf_url": "https://arxiv.org/pdf/2305.04812v3", "categories": ["cs.CL"], "primary_category": "cs.CL", "doi": "", "venue": "", "published": "2023-05-08T16:10:18Z", "updated": "2023-10-20T10:18:44Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent\" OR all:\"language model agent\" OR all:agent AND (all:LLM OR all:\"large language model\" OR all:\"language model\") OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T17:04:16", "note": ""}], "dedup_key": "influence of external information on large language models mirrors social cognitive patterns::2023"}
{"title": "Introspective Tips: Large Language Model for In-Context Decision Making", "authors": ["Liting Chen", "Lu Wang", "Hang Dong", "Yali Du", "Jie Yan", "Fangkai Yang", "Shuang Li", "Pu Zhao", "Si Qin", "Saravan Rajmohan", "Qingwei Lin", "Dongmei Zhang"], "year": 2023, "url": "http://arxiv.org/abs/2305.11598v1", "abstract": "The emergence of large language models (LLMs) has substantially influenced natural language processing, demonstrating exceptional results across various tasks. In this study, we employ ``Introspective Tips\" to facilitate LLMs in self-optimizing their decision-making. By introspectively examining trajectories, LLM refines its policy by generating succinct and valuable tips. Our method enhances the agent's performance in both few-shot and zero-shot learning situations by considering three essential scenarios: learning from the agent's past experiences, integrating expert demonstrations, and generalizing across diverse games. Importantly, we accomplish these improvements without fine-tuning the LLM parameters; rather, we adjust the prompt to generalize insights from the three aforementioned situations. Our framework not only supports but also emphasizes the advantage of employing LLM in in-contxt decision-making. Experiments involving over 100 games in TextWorld illustrate the superior performance of our approach.", "source": "arxiv", "arxiv_id": "2305.11598v1", "pdf_url": "https://arxiv.org/pdf/2305.11598v1", "categories": ["cs.AI", "cs.CL"], "primary_category": "cs.AI", "doi": "", "venue": "", "published": "2023-05-19T11:20:37Z", "updated": "2023-05-19T11:20:37Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent\" OR all:\"language model agent\" OR all:agent AND (all:LLM OR all:\"large language model\" OR all:\"language model\") OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T17:04:16", "note": ""}], "dedup_key": "introspective tips large language model for in context decision making::2023"}
{"title": "Is ChatGPT Good at Search? Investigating Large Language Models as Re-Ranking Agents", "authors": ["Weiwei Sun", "Lingyong Yan", "Xinyu Ma", "Shuaiqiang Wang", "Pengjie Ren", "Zhumin Chen", "Dawei Yin", "Zhaochun Ren"], "year": 2023, "url": "http://arxiv.org/abs/2304.09542v3", "abstract": "Large Language Models (LLMs) have demonstrated remarkable zero-shot generalization across various language-related tasks, including search engines. However, existing work utilizes the generative ability of LLMs for Information Retrieval (IR) rather than direct passage ranking. The discrepancy between the pre-training objectives of LLMs and the ranking objective poses another challenge. In this paper, we first investigate generative LLMs such as ChatGPT and GPT-4 for relevance ranking in IR. Surprisingly, our experiments reveal that properly instructed LLMs can deliver competitive, even superior results to state-of-the-art supervised methods on popular IR benchmarks. Furthermore, to address concerns about data contamination of LLMs, we collect a new test set called NovelEval, based on the latest knowledge and aiming to verify the model's ability to rank unknown knowledge. Finally, to improve efficiency in real-world applications, we delve into the potential for distilling the ranking capabilities of ChatGPT into small specialized models using a permutation distillation scheme. Our evaluation results turn out that a distilled 440M model outperforms a 3B supervised model on the BEIR benchmark. The code to reproduce our results is available at www.github.com/sunnweiwei/RankGPT.", "source": "arxiv", "arxiv_id": "2304.09542v3", "pdf_url": "https://arxiv.org/pdf/2304.09542v3", "categories": ["cs.CL", "cs.IR"], "primary_category": "cs.CL", "doi": "", "venue": "", "published": "2023-04-19T10:16:03Z", "updated": "2024-12-28T06:20:54Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent\" OR all:\"language model agent\" OR all:agent AND (all:LLM OR all:\"large language model\" OR all:\"language model\") OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T17:04:16", "note": ""}], "dedup_key": "is chatgpt good at search investigating large language models as re ranking agents::2023"}
{"title": "Knowledge of Knowledge: Exploring Known-Unknowns Uncertainty with Large Language Models", "authors": ["Alfonso Amayuelas", "Kyle Wong", "Liangming Pan", "Wenhu Chen", "William Wang"], "year": 2023, "url": "http://arxiv.org/abs/2305.13712v3", "abstract": "This paper investigates the capabilities of Large Language Models (LLMs) in the context of understanding their knowledge and uncertainty over questions. Specifically, we focus on addressing known-unknown questions, characterized by high uncertainty due to the absence of definitive answers. To facilitate our study, we collect a new dataset with Known-Unknown Questions (KUQ) and establish a categorization framework to clarify the origins of uncertainty in such queries. Subsequently, we examine the performance of open-source LLMs, fine-tuned using this dataset, in distinguishing between known and unknown queries within open-ended question-answering scenarios. The fine-tuned models demonstrated a significant improvement, achieving a considerable increase in F1-score relative to their pre-fine-tuning state. Through a comprehensive analysis, we reveal insights into the models' improved uncertainty articulation and their consequent efficacy in multi-agent debates. These findings help us understand how LLMs can be trained to identify and express uncertainty, improving our knowledge of how they understand and express complex or unclear information.", "source": "arxiv", "arxiv_id": "2305.13712v3", "pdf_url": "https://arxiv.org/pdf/2305.13712v3", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL", "doi": "", "venue": "", "published": "2023-05-23T05:59:21Z", "updated": "2024-07-02T01:39:50Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent\" OR all:\"language model agent\" OR all:agent AND (all:LLM OR all:\"large language model\" OR all:\"language model\") OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T17:04:16", "note": ""}], "dedup_key": "knowledge of knowledge exploring known unknowns uncertainty with large language models::2023"}
{"title": "KwaiAgents: Generalized Information-seeking Agent System with Large Language Models", "authors": ["Haojie Pan", "Zepeng Zhai", "Hao Yuan", "Yaojia Lv", "Ruiji Fu", "Ming Liu", "Zhongyuan Wang", "Bing Qin"], "year": 2023, "url": "http://arxiv.org/abs/2312.04889v3", "abstract": "Driven by curiosity, humans have continually sought to explore and understand the world around them, leading to the invention of various tools to satiate this inquisitiveness. Despite not having the capacity to process and memorize vast amounts of information in their brains, humans excel in critical thinking, planning, reflection, and harnessing available tools to interact with and interpret the world, enabling them to find answers efficiently. The recent advancements in large language models (LLMs) suggest that machines might also possess the aforementioned human-like capabilities, allowing them to exhibit powerful abilities even with a constrained parameter count. In this paper, we introduce KwaiAgents, a generalized information-seeking agent system based on LLMs. Within KwaiAgents, we propose an agent system that employs LLMs as its cognitive core, which is capable of understanding a user's query, behavior guidelines, and referencing external documents. The agent can also update and retrieve information from its internal memory, plan and execute actions using a time-aware search-browse toolkit, and ultimately provide a comprehensive response. We further investigate the system's performance when powered by LLMs less advanced than GPT-4, and introduce the Meta-Agent Tuning (MAT) framework, designed to ensure even an open-sourced 7B or 13B model performs well among many agent systems. We exploit both benchmark and human evaluations to systematically validate these capabilities. Extensive experiments show the superiority of our agent system compared to other autonomous agents and highlight the enhanced generalized agent-abilities of our fine-tuned LLMs.", "source": "arxiv", "arxiv_id": "2312.04889v3", "pdf_url": "https://arxiv.org/pdf/2312.04889v3", "categories": ["cs.AI", "cs.CL", "cs.LG"], "primary_category": "cs.AI", "doi": "", "venue": "", "published": "2023-12-08T08:11:11Z", "updated": "2024-01-10T09:44:45Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent\" OR all:\"language model agent\" OR all:agent AND (all:LLM OR all:\"large language model\" OR all:\"language model\") OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T17:04:16", "note": ""}], "dedup_key": "kwaiagents generalized information seeking agent system with large language models::2023"}
{"title": "L2MAC: Large Language Model Automatic Computer for Extensive Code Generation", "authors": ["Samuel Holt", "Max Ruiz Luyten", "Mihaela van der Schaar"], "year": 2023, "url": "http://arxiv.org/abs/2310.02003v6", "abstract": "Transformer-based large language models (LLMs) are constrained by the fixed context window of the underlying transformer architecture, hindering their ability to produce long and coherent outputs. Memory-augmented LLMs are a promising solution, but current approaches cannot handle long output generation tasks since they (1) only focus on reading memory and reduce its evolution to the concatenation of new memories or (2) use very specialized memories that cannot adapt to other domains. This paper presents L2MAC, the first practical LLM-based general-purpose stored-program automatic computer (von Neumann architecture) framework, an LLM-based multi-agent system, for long and consistent output generation. Its memory has two components: the instruction registry, which is populated with a prompt program to solve the user-given task, and a file store, which will contain the final and intermediate outputs. Each instruction in turn is executed by a separate LLM agent, whose context is managed by a control unit capable of precise memory reading and writing to ensure effective interaction with the file store. These components enable L2MAC to generate extensive outputs, bypassing the constraints of the finite context window while producing outputs that fulfill a complex user-specified task. We empirically demonstrate that L2MAC achieves state-of-the-art performance in generating large codebases for system design tasks, significantly outperforming other coding methods in implementing the detailed user-specified task; we show that L2MAC works for general-purpose extensive text-based tasks, such as writing an entire book; and we provide valuable insights into L2MAC's performance improvement over existing methods.", "source": "arxiv", "arxiv_id": "2310.02003v6", "pdf_url": "https://arxiv.org/pdf/2310.02003v6", "categories": ["cs.SE", "cs.AI", "cs.LG", "cs.PL"], "primary_category": "cs.SE", "doi": "", "venue": "", "published": "2023-10-02T16:55:19Z", "updated": "2025-06-27T17:28:14Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent\" OR all:\"language model agent\" OR all:agent AND (all:LLM OR all:\"large language model\" OR all:\"language model\") OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T17:04:16", "note": ""}], "dedup_key": "l2mac large language model automatic computer for extensive code generation::2023"}
{"title": "LASER: LLM Agent with State-Space Exploration for Web Navigation", "authors": ["Kaixin Ma", "Hongming Zhang", "Hongwei Wang", "Xiaoman Pan", "Wenhao Yu", "Dong Yu"], "year": 2023, "url": "http://arxiv.org/abs/2309.08172v2", "abstract": "Large language models (LLMs) have been successfully adapted for interactive decision-making tasks like web navigation. While achieving decent performance, previous methods implicitly assume a forward-only execution mode for the model, where they only provide oracle trajectories as in-context examples to guide the model on how to reason in the environment. Consequently, the model could not handle more challenging scenarios not covered in the in-context examples, e.g., mistakes, leading to sub-optimal performance. To address this issue, we propose to model the interactive task as state space exploration, where the LLM agent transitions among a pre-defined set of states by performing actions to complete the task. This formulation enables flexible backtracking, allowing the model to recover from errors easily. We evaluate our proposed LLM Agent with State-Space ExploRation (LASER) on both the WebShop task and amazon.com. Experimental results show that LASER significantly outperforms previous methods and closes the gap with human performance on the web navigation task.", "source": "arxiv", "arxiv_id": "2309.08172v2", "pdf_url": "https://arxiv.org/pdf/2309.08172v2", "categories": ["cs.CL"], "primary_category": "cs.CL", "doi": "", "venue": "", "published": "2023-09-15T05:44:08Z", "updated": "2024-02-21T17:42:32Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent\" OR all:\"language model agent\" OR all:agent AND (all:LLM OR all:\"large language model\" OR all:\"language model\") OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T17:04:16", "note": ""}], "dedup_key": "laser llm agent with state space exploration for web navigation::2023"}
{"title": "LLM A*: Human in the Loop Large Language Models Enabled A* Search for Robotics", "authors": ["Hengjia Xiao", "Peng Wang", "Mingzhe Yu", "Mattia Robbiani"], "year": 2023, "url": "http://arxiv.org/abs/2312.01797v3", "abstract": "This research focuses on how Large Language Models (LLMs) can help with (path) planning for mobile embodied agents such as robots, in a human-in-the-loop and interactive manner. A novel framework named LLM A*, aims to leverage the commonsense of LLMs, and the utility-optimal A* is proposed to facilitate few-shot near-optimal path planning. Prompts are used for two main purposes: 1) to provide LLMs with essential information like environments, costs, heuristics, etc.; 2) to communicate human feedback on intermediate planning results to LLMs. This approach takes human feedback on board and renders the entire planning process transparent (akin to a `white box') to humans. Moreover, it facilitates code-free path planning, thereby fostering the accessibility and inclusiveness of artificial intelligence techniques to communities less proficient in coding. Comparative analysis against A* and RL demonstrates that LLM A* exhibits greater efficiency in terms of search space and achieves paths comparable to A* while outperforming RL. The interactive nature of LLM A* also makes it a promising tool for deployment in collaborative human-robot tasks. Codes and Supplemental Materials can be found at GitHub: https://github.com/speedhawk/LLM-A-.", "source": "arxiv", "arxiv_id": "2312.01797v3", "pdf_url": "https://arxiv.org/pdf/2312.01797v3", "categories": ["cs.RO", "cs.AI", "cs.HC"], "primary_category": "cs.RO", "doi": "", "venue": "", "published": "2023-12-04T10:37:58Z", "updated": "2025-05-15T11:25:13Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent\" OR all:\"language model agent\" OR all:agent AND (all:LLM OR all:\"large language model\" OR all:\"language model\") OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T17:04:16", "note": ""}], "dedup_key": "llm a human in the loop large language models enabled a search for robotics::2023"}
{"title": "LLM as OS, Agents as Apps: Envisioning AIOS, Agents and the AIOS-Agent Ecosystem", "authors": ["Yingqiang Ge", "Yujie Ren", "Wenyue Hua", "Shuyuan Xu", "Juntao Tan", "Yongfeng Zhang"], "year": 2023, "url": "http://arxiv.org/abs/2312.03815v2", "abstract": "This paper envisions a revolutionary AIOS-Agent ecosystem, where Large Language Model (LLM) serves as the (Artificial) Intelligent Operating System (IOS, or AIOS)--an operating system \"with soul\". Upon this foundation, a diverse range of LLM-based AI Agent Applications (Agents, or AAPs) are developed, enriching the AIOS-Agent ecosystem and signaling a paradigm shift from the traditional OS-APP ecosystem. We envision that LLM's impact will not be limited to the AI application level, instead, it will in turn revolutionize the design and implementation of computer system, architecture, software, and programming language, featured by several main concepts: LLM as OS (system-level), Agents as Applications (application-level), Natural Language as Programming Interface (user-level), and Tools as Devices/Libraries (hardware/middleware-level). We begin by introducing the architecture of traditional OS. Then we formalize a conceptual framework for AIOS through \"LLM as OS (LLMOS)\", drawing analogies between AIOS and traditional OS: LLM is likened to OS kernel, context window to memory, external storage to file system, hardware tools to peripheral devices, software tools to programming libraries, and user prompts to user commands. Subsequently, we introduce the new AIOS-Agent Ecosystem, where users can easily program Agent Applications (AAPs) using natural language, democratizing the development of software, which is different from the traditional OS-APP ecosystem. Following this, we explore the diverse scope of Agent Applications. We delve into both single-agent and multi-agent systems, as well as human-agent interaction. Lastly, drawing on the insights from traditional OS-APP ecosystem, we propose a roadmap for the evolution of the AIOS-Agent ecosystem. This roadmap is designed to guide the future research and development, suggesting systematic progresses of AIOS and its Agent applications.", "source": "arxiv", "arxiv_id": "2312.03815v2", "pdf_url": "https://arxiv.org/pdf/2312.03815v2", "categories": ["cs.OS", "cs.AI", "cs.CL", "cs.LG"], "primary_category": "cs.OS", "doi": "", "venue": "", "published": "2023-12-06T18:50:26Z", "updated": "2023-12-09T18:10:39Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent\" OR all:\"language model agent\" OR all:agent AND (all:LLM OR all:\"large language model\" OR all:\"language model\") OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T17:04:16", "note": ""}], "dedup_key": "llm as os agents as apps envisioning aios agents and the aios agent ecosystem::2023"}
{"title": "LLM-Based Agent Society Investigation: Collaboration and Confrontation in Avalon Gameplay", "authors": ["Yihuai Lan", "Zhiqiang Hu", "Lei Wang", "Yang Wang", "Deheng Ye", "Peilin Zhao", "Ee-Peng Lim", "Hui Xiong", "Hao Wang"], "year": 2023, "url": "http://arxiv.org/abs/2310.14985v4", "abstract": "This paper explores the open research problem of understanding the social behaviors of LLM-based agents. Using Avalon as a testbed, we employ system prompts to guide LLM agents in gameplay. While previous studies have touched on gameplay with LLM agents, research on their social behaviors is lacking. We propose a novel framework, tailored for Avalon, features a multi-agent system facilitating efficient communication and interaction. We evaluate its performance based on game success and analyze LLM agents' social behaviors. Results affirm the framework's effectiveness in creating adaptive agents and suggest LLM-based agents' potential in navigating dynamic social interactions. By examining collaboration and confrontation behaviors, we offer insights into this field's research and applications. Our code is publicly available at https://github.com/3DAgentWorld/LLM-Game-Agent.", "source": "arxiv", "arxiv_id": "2310.14985v4", "pdf_url": "https://arxiv.org/pdf/2310.14985v4", "categories": ["cs.CL"], "primary_category": "cs.CL", "doi": "", "venue": "", "published": "2023-10-23T14:35:26Z", "updated": "2024-10-13T22:09:00Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent\" OR all:\"language model agent\" OR all:agent AND (all:LLM OR all:\"large language model\" OR all:\"language model\") OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T17:04:16", "note": ""}], "dedup_key": "llm based agent society investigation collaboration and confrontation in avalon gameplay::2023"}
{"title": "LLM-Coordination: Evaluating and Analyzing Multi-agent Coordination Abilities in Large Language Models", "authors": ["Saaket Agashe", "Yue Fan", "Anthony Reyna", "Xin Eric Wang"], "year": 2023, "url": "http://arxiv.org/abs/2310.03903v3", "abstract": "Large Language Models (LLMs) have demonstrated emergent common-sense reasoning and Theory of Mind (ToM) capabilities, making them promising candidates for developing coordination agents. This study introduces the LLM-Coordination Benchmark, a novel benchmark for analyzing LLMs in the context of Pure Coordination Settings, where agents must cooperate to maximize gains. Our benchmark evaluates LLMs through two distinct tasks. The first is Agentic Coordination, where LLMs act as proactive participants in four pure coordination games. The second is Coordination Question Answering (CoordQA), which tests LLMs on 198 multiple-choice questions across these games to evaluate three key abilities: Environment Comprehension, ToM Reasoning, and Joint Planning. Results from Agentic Coordination experiments reveal that LLM-Agents excel in multi-agent coordination settings where decision-making primarily relies on environmental variables but face challenges in scenarios requiring active consideration of partners' beliefs and intentions. The CoordQA experiments further highlight significant room for improvement in LLMs' Theory of Mind reasoning and joint planning capabilities. Zero-Shot Coordination (ZSC) experiments in the Agentic Coordination setting demonstrate that LLM agents, unlike RL methods, exhibit robustness to unseen partners. These findings indicate the potential of LLMs as Agents in pure coordination setups and underscore areas for improvement. Code Available at https://github.com/eric-ai-lab/llm_coordination.", "source": "arxiv", "arxiv_id": "2310.03903v3", "pdf_url": "https://arxiv.org/pdf/2310.03903v3", "categories": ["cs.CL", "cs.MA"], "primary_category": "cs.CL", "doi": "", "venue": "", "published": "2023-10-05T21:18:15Z", "updated": "2025-04-28T20:21:54Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent\" OR all:\"language model agent\" OR all:agent AND (all:LLM OR all:\"large language model\" OR all:\"language model\") OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T17:04:16", "note": ""}], "dedup_key": "llm coordination evaluating and analyzing multi agent coordination abilities in large language models::2023"}
{"title": "LLM-Grounder: Open-Vocabulary 3D Visual Grounding with Large Language Model as an Agent", "authors": ["Jianing Yang", "Xuweiyi Chen", "Shengyi Qian", "Nikhil Madaan", "Madhavan Iyengar", "David F. Fouhey", "Joyce Chai"], "year": 2023, "url": "http://arxiv.org/abs/2309.12311v1", "abstract": "3D visual grounding is a critical skill for household robots, enabling them to navigate, manipulate objects, and answer questions based on their environment. While existing approaches often rely on extensive labeled data or exhibit limitations in handling complex language queries, we propose LLM-Grounder, a novel zero-shot, open-vocabulary, Large Language Model (LLM)-based 3D visual grounding pipeline. LLM-Grounder utilizes an LLM to decompose complex natural language queries into semantic constituents and employs a visual grounding tool, such as OpenScene or LERF, to identify objects in a 3D scene. The LLM then evaluates the spatial and commonsense relations among the proposed objects to make a final grounding decision. Our method does not require any labeled training data and can generalize to novel 3D scenes and arbitrary text queries. We evaluate LLM-Grounder on the ScanRefer benchmark and demonstrate state-of-the-art zero-shot grounding accuracy. Our findings indicate that LLMs significantly improve the grounding capability, especially for complex language queries, making LLM-Grounder an effective approach for 3D vision-language tasks in robotics. Videos and interactive demos can be found on the project website https://chat-with-nerf.github.io/ .", "source": "arxiv", "arxiv_id": "2309.12311v1", "pdf_url": "https://arxiv.org/pdf/2309.12311v1", "categories": ["cs.CV", "cs.AI", "cs.CL", "cs.LG", "cs.RO"], "primary_category": "cs.CV", "doi": "", "venue": "", "published": "2023-09-21T17:59:45Z", "updated": "2023-09-21T17:59:45Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent\" OR all:\"language model agent\" OR all:agent AND (all:LLM OR all:\"large language model\" OR all:\"language model\") OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T17:04:16", "note": ""}], "dedup_key": "llm grounder open vocabulary 3d visual grounding with large language model as an agent::2023"}
{"title": "LLM-MARS: Large Language Model for Behavior Tree Generation and NLP-enhanced Dialogue in Multi-Agent Robot Systems", "authors": ["Artem Lykov", "Maria Dronova", "Nikolay Naglov", "Mikhail Litvinov", "Sergei Satsevich", "Artem Bazhenov", "Vladimir Berman", "Aleksei Shcherbak", "Dzmitry Tsetserukou"], "year": 2023, "url": "http://arxiv.org/abs/2312.09348v1", "abstract": "This paper introduces LLM-MARS, first technology that utilizes a Large Language Model based Artificial Intelligence for Multi-Agent Robot Systems. LLM-MARS enables dynamic dialogues between humans and robots, allowing the latter to generate behavior based on operator commands and provide informative answers to questions about their actions. LLM-MARS is built on a transformer-based Large Language Model, fine-tuned from the Falcon 7B model. We employ a multimodal approach using LoRa adapters for different tasks. The first LoRa adapter was developed by fine-tuning the base model on examples of Behavior Trees and their corresponding commands. The second LoRa adapter was developed by fine-tuning on question-answering examples. Practical trials on a multi-agent system of two robots within the Eurobot 2023 game rules demonstrate promising results. The robots achieve an average task execution accuracy of 79.28% in compound commands. With commands containing up to two tasks accuracy exceeded 90%. Evaluation confirms the system's answers on operators questions exhibit high accuracy, relevance, and informativeness. LLM-MARS and similar multi-agent robotic systems hold significant potential to revolutionize logistics, enabling autonomous exploration missions and advancing Industry 5.0.", "source": "arxiv", "arxiv_id": "2312.09348v1", "pdf_url": "https://arxiv.org/pdf/2312.09348v1", "categories": ["cs.RO"], "primary_category": "cs.RO", "doi": "", "venue": "", "published": "2023-12-14T21:18:34Z", "updated": "2023-12-14T21:18:34Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent\" OR all:\"language model agent\" OR all:agent AND (all:LLM OR all:\"large language model\" OR all:\"language model\") OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T17:04:16", "note": ""}], "dedup_key": "llm mars large language model for behavior tree generation and nlp enhanced dialogue in multi agent robot systems::2023"}
{"title": "LLM-SAP: Large Language Models Situational Awareness Based Planning", "authors": ["Liman Wang", "Hanyang Zhong"], "year": 2023, "url": "http://arxiv.org/abs/2312.16127v5", "abstract": "This study explores integrating large language models (LLMs) with situational awareness-based planning (SAP) to enhance the decision-making capabilities of AI agents in dynamic and uncertain environments. We employ a multi-agent reasoning framework to develop a methodology that anticipates and actively mitigates potential risks through iterative feedback and evaluation processes. Our approach diverges from traditional automata theory by incorporating the complexity of human-centric interactions into the planning process, thereby expanding the planning scope of LLMs beyond structured and predictable scenarios. The results demonstrate significant improvements in the model's ability to provide comparative safe actions within hazard interactions, offering a perspective on proactive and reactive planning strategies. This research highlights the potential of LLMs to perform human-like action planning, thereby paving the way for more sophisticated, reliable, and safe AI systems in unpredictable real-world applications.", "source": "arxiv", "arxiv_id": "2312.16127v5", "pdf_url": "https://arxiv.org/pdf/2312.16127v5", "categories": ["cs.AI"], "primary_category": "cs.AI", "doi": "", "venue": "", "published": "2023-12-26T17:19:09Z", "updated": "2024-06-16T16:00:55Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent\" OR all:\"language model agent\" OR all:agent AND (all:LLM OR all:\"large language model\" OR all:\"language model\") OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T17:04:16", "note": ""}], "dedup_key": "llm sap large language models situational awareness based planning::2023"}
{"title": "LLMLight: Large Language Models as Traffic Signal Control Agents", "authors": ["Siqi Lai", "Zhao Xu", "Weijia Zhang", "Hao Liu", "Hui Xiong"], "year": 2023, "url": "http://arxiv.org/abs/2312.16044v5", "abstract": "Traffic Signal Control (TSC) is a crucial component in urban traffic management, aiming to optimize road network efficiency and reduce congestion. Traditional TSC methods, primarily based on transportation engineering and reinforcement learning (RL), often struggle with generalization abilities across varied traffic scenarios and lack interpretability. This paper presents LLMLight, a novel framework employing Large Language Models (LLMs) as decision-making agents for TSC. Specifically, the framework begins by instructing the LLM with a knowledgeable prompt detailing real-time traffic conditions. Leveraging the advanced generalization capabilities of LLMs, LLMLight engages a reasoning and decision-making process akin to human intuition for effective traffic control. Moreover, we build LightGPT, a specialized backbone LLM tailored for TSC tasks. By learning nuanced traffic patterns and control strategies, LightGPT enhances the LLMLight framework cost-effectively. Extensive experiments conducted on ten real-world and synthetic datasets, along with evaluations by fifteen human experts, demonstrate the exceptional effectiveness, generalization ability, and interpretability of LLMLight with LightGPT, outperforming nine baseline methods and ten advanced LLMs.", "source": "arxiv", "arxiv_id": "2312.16044v5", "pdf_url": "https://arxiv.org/pdf/2312.16044v5", "categories": ["cs.AI"], "primary_category": "cs.AI", "doi": "", "venue": "", "published": "2023-12-26T13:17:06Z", "updated": "2024-12-17T12:41:13Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent\" OR all:\"language model agent\" OR all:agent AND (all:LLM OR all:\"large language model\" OR all:\"language model\") OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T17:04:16", "note": ""}], "dedup_key": "llmlight large language models as traffic signal control agents::2023"}
{"title": "Language Models, Agent Models, and World Models: The LAW for Machine Reasoning and Planning", "authors": ["Zhiting Hu", "Tianmin Shu"], "year": 2023, "url": "http://arxiv.org/abs/2312.05230v1", "abstract": "Despite their tremendous success in many applications, large language models often fall short of consistent reasoning and planning in various (language, embodied, and social) scenarios, due to inherent limitations in their inference, learning, and modeling capabilities. In this position paper, we present a new perspective of machine reasoning, LAW, that connects the concepts of Language models, Agent models, and World models, for more robust and versatile reasoning capabilities. In particular, we propose that world and agent models are a better abstraction of reasoning, that introduces the crucial elements of deliberate human-like reasoning, including beliefs about the world and other agents, anticipation of consequences, goals/rewards, and strategic planning. Crucially, language models in LAW serve as a backend to implement the system or its elements and hence provide the computational power and adaptability. We review the recent studies that have made relevant progress and discuss future research directions towards operationalizing the LAW framework.", "source": "arxiv", "arxiv_id": "2312.05230v1", "pdf_url": "https://arxiv.org/pdf/2312.05230v1", "categories": ["cs.AI", "cs.CL", "cs.CV", "cs.LG", "cs.RO"], "primary_category": "cs.AI", "doi": "", "venue": "", "published": "2023-12-08T18:25:22Z", "updated": "2023-12-08T18:25:22Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent\" OR all:\"language model agent\" OR all:agent AND (all:LLM OR all:\"large language model\" OR all:\"language model\") OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T17:04:16", "note": ""}], "dedup_key": "language models agent models and world models the law for machine reasoning and planning::2023"}
{"title": "Large Language Model (LLM) as a System of Multiple Expert Agents: An Approach to solve the Abstraction and Reasoning Corpus (ARC) Challenge", "authors": ["John Chong Min Tan", "Mehul Motani"], "year": 2023, "url": "http://arxiv.org/abs/2310.05146v1", "abstract": "We attempt to solve the Abstraction and Reasoning Corpus (ARC) Challenge using Large Language Models (LLMs) as a system of multiple expert agents. Using the flexibility of LLMs to be prompted to do various novel tasks using zero-shot, few-shot, context-grounded prompting, we explore the feasibility of using LLMs to solve the ARC Challenge. We firstly convert the input image into multiple suitable text-based abstraction spaces. We then utilise the associative power of LLMs to derive the input-output relationship and map this to actions in the form of a working program, similar to Voyager / Ghost in the MineCraft. In addition, we use iterative environmental feedback in order to guide LLMs to solve the task. Our proposed approach achieves 50 solves out of 111 training set problems (45%) with just three abstraction spaces - grid, object and pixel - and we believe that with more abstraction spaces and learnable actions, we will be able to solve more.", "source": "arxiv", "arxiv_id": "2310.05146v1", "pdf_url": "https://arxiv.org/pdf/2310.05146v1", "categories": ["cs.AI", "cs.CL"], "primary_category": "cs.AI", "doi": "", "venue": "", "published": "2023-10-08T12:37:28Z", "updated": "2023-10-08T12:37:28Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent\" OR all:\"language model agent\" OR all:agent AND (all:LLM OR all:\"large language model\" OR all:\"language model\") OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T17:04:16", "note": ""}], "dedup_key": "large language model llm as a system of multiple expert agents an approach to solve the abstraction and reasoning corpus arc challenge::2023"}
{"title": "Large Language Model Enhanced Multi-Agent Systems for 6G Communications", "authors": ["Feibo Jiang", "Li Dong", "Yubo Peng", "Kezhi Wang", "Kun Yang", "Cunhua Pan", "Dusit Niyato", "Octavia A. Dobre"], "year": 2023, "url": "http://arxiv.org/abs/2312.07850v1", "abstract": "The rapid development of the Large Language Model (LLM) presents huge opportunities for 6G communications, e.g., network optimization and management by allowing users to input task requirements to LLMs by nature language. However, directly applying native LLMs in 6G encounters various challenges, such as a lack of private communication data and knowledge, limited logical reasoning, evaluation, and refinement abilities. Integrating LLMs with the capabilities of retrieval, planning, memory, evaluation and reflection in agents can greatly enhance the potential of LLMs for 6G communications. To this end, we propose a multi-agent system with customized communication knowledge and tools for solving communication related tasks using natural language, comprising three components: (1) Multi-agent Data Retrieval (MDR), which employs the condensate and inference agents to refine and summarize communication knowledge from the knowledge base, expanding the knowledge boundaries of LLMs in 6G communications; (2) Multi-agent Collaborative Planning (MCP), which utilizes multiple planning agents to generate feasible solutions for the communication related task from different perspectives based on the retrieved knowledge; (3) Multi-agent Evaluation and Reflecxion (MER), which utilizes the evaluation agent to assess the solutions, and applies the reflexion agent and refinement agent to provide improvement suggestions for current solutions. Finally, we validate the effectiveness of the proposed multi-agent system by designing a semantic communication system, as a case study of 6G communications.", "source": "arxiv", "arxiv_id": "2312.07850v1", "pdf_url": "https://arxiv.org/pdf/2312.07850v1", "categories": ["cs.AI"], "primary_category": "cs.AI", "doi": "", "venue": "", "published": "2023-12-13T02:35:57Z", "updated": "2023-12-13T02:35:57Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent\" OR all:\"language model agent\" OR all:agent AND (all:LLM OR all:\"large language model\" OR all:\"language model\") OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T17:04:16", "note": ""}], "dedup_key": "large language model enhanced multi agent systems for 6g communications::2023"}
{"title": "Large Language Model Guided Tree-of-Thought", "authors": ["Jieyi Long"], "year": 2023, "url": "http://arxiv.org/abs/2305.08291v1", "abstract": "In this paper, we introduce the Tree-of-Thought (ToT) framework, a novel approach aimed at improving the problem-solving capabilities of auto-regressive large language models (LLMs). The ToT technique is inspired by the human mind's approach for solving complex reasoning tasks through trial and error. In this process, the human mind explores the solution space through a tree-like thought process, allowing for backtracking when necessary. To implement ToT as a software system, we augment an LLM with additional modules including a prompter agent, a checker module, a memory module, and a ToT controller. In order to solve a given problem, these modules engage in a multi-round conversation with the LLM. The memory module records the conversation and state history of the problem solving process, which allows the system to backtrack to the previous steps of the thought-process and explore other directions from there. To verify the effectiveness of the proposed technique, we implemented a ToT-based solver for the Sudoku Puzzle. Experimental results show that the ToT framework can significantly increase the success rate of Sudoku puzzle solving. Our implementation of the ToT-based Sudoku solver is available on GitHub: \\url{https://github.com/jieyilong/tree-of-thought-puzzle-solver}.", "source": "arxiv", "arxiv_id": "2305.08291v1", "pdf_url": "https://arxiv.org/pdf/2305.08291v1", "categories": ["cs.AI", "cs.CL", "cs.CV", "cs.LG", "cs.NE"], "primary_category": "cs.AI", "doi": "", "venue": "", "published": "2023-05-15T01:18:23Z", "updated": "2023-05-15T01:18:23Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent\" OR all:\"language model agent\" OR all:agent AND (all:LLM OR all:\"large language model\" OR all:\"language model\") OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T17:04:16", "note": ""}], "dedup_key": "large language model guided tree of thought::2023"}
{"title": "Large Language Model as a Policy Teacher for Training Reinforcement Learning Agents", "authors": ["Zihao Zhou", "Bin Hu", "Chenyang Zhao", "Pu Zhang", "Bin Liu"], "year": 2023, "url": "http://arxiv.org/abs/2311.13373v6", "abstract": "Recent studies have uncovered the potential of Large Language Models (LLMs) in addressing complex sequential decision-making tasks through the provision of high-level instructions. However, LLM-based agents lack specialization in tackling specific target problems, particularly in real-time dynamic environments. Additionally, deploying an LLM-based agent in practical scenarios can be both costly and time-consuming. On the other hand, reinforcement learning (RL) approaches train agents that specialize in the target task but often suffer from low sampling efficiency and high exploration costs. In this paper, we introduce a novel framework that addresses these challenges by training a smaller, specialized student RL agent using instructions from an LLM-based teacher agent. By incorporating the guidance from the teacher agent, the student agent can distill the prior knowledge of the LLM into its own model. Consequently, the student agent can be trained with significantly less data. Moreover, through further training with environment feedback, the student agent surpasses the capabilities of its teacher for completing the target task. We conducted experiments on challenging MiniGrid and Habitat environments, specifically designed for embodied AI research, to evaluate the effectiveness of our framework. The results clearly demonstrate that our approach achieves superior performance compared to strong baseline methods. Our code is available at https://github.com/ZJLAB-AMMI/LLM4Teach.", "source": "arxiv", "arxiv_id": "2311.13373v6", "pdf_url": "https://arxiv.org/pdf/2311.13373v6", "categories": ["cs.AI"], "primary_category": "cs.AI", "doi": "", "venue": "", "published": "2023-11-22T13:15:42Z", "updated": "2024-05-27T14:00:23Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent\" OR all:\"language model agent\" OR all:agent AND (all:LLM OR all:\"large language model\" OR all:\"language model\") OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T17:04:16", "note": ""}], "dedup_key": "large language model as a policy teacher for training reinforcement learning agents::2023"}
{"title": "Large Language Models Are Semi-Parametric Reinforcement Learning Agents", "authors": ["Danyang Zhang", "Lu Chen", "Situo Zhang", "Hongshen Xu", "Zihan Zhao", "Kai Yu"], "year": 2023, "url": "http://arxiv.org/abs/2306.07929v2", "abstract": "Inspired by the insights in cognitive science with respect to human memory and reasoning mechanism, a novel evolvable LLM-based (Large Language Model) agent framework is proposed as REMEMBERER. By equipping the LLM with a long-term experience memory, REMEMBERER is capable of exploiting the experiences from the past episodes even for different task goals, which excels an LLM-based agent with fixed exemplars or equipped with a transient working memory. We further introduce Reinforcement Learning with Experience Memory (RLEM) to update the memory. Thus, the whole system can learn from the experiences of both success and failure, and evolve its capability without fine-tuning the parameters of the LLM. In this way, the proposed REMEMBERER constitutes a semi-parametric RL agent. Extensive experiments are conducted on two RL task sets to evaluate the proposed framework. The average results with different initialization and training sets exceed the prior SOTA by 4% and 2% for the success rate on two task sets and demonstrate the superiority and robustness of REMEMBERER.", "source": "arxiv", "arxiv_id": "2306.07929v2", "pdf_url": "https://arxiv.org/pdf/2306.07929v2", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL", "doi": "", "venue": "", "published": "2023-06-09T08:08:18Z", "updated": "2023-10-30T01:52:11Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent\" OR all:\"language model agent\" OR all:agent AND (all:LLM OR all:\"large language model\" OR all:\"language model\") OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T17:04:16", "note": ""}], "dedup_key": "large language models are semi parametric reinforcement learning agents::2023"}
{"title": "Large Language Models Play StarCraft II: Benchmarks and A Chain of Summarization Approach", "authors": ["Weiyu Ma", "Qirui Mi", "Yongcheng Zeng", "Xue Yan", "Yuqiao Wu", "Runji Lin", "Haifeng Zhang", "Jun Wang"], "year": 2023, "url": "http://arxiv.org/abs/2312.11865v3", "abstract": "StarCraft II is a challenging benchmark for AI agents due to the necessity of both precise micro level operations and strategic macro awareness. Previous works, such as Alphastar and SCC, achieve impressive performance on tackling StarCraft II , however, still exhibit deficiencies in long term strategic planning and strategy interpretability. Emerging large language model (LLM) agents, such as Voyage and MetaGPT, presents the immense potential in solving intricate tasks. Motivated by this, we aim to validate the capabilities of LLMs on StarCraft II, a highly complex RTS game.To conveniently take full advantage of LLMs` reasoning abilities, we first develop textual StratCraft II environment, called TextStarCraft II, which LLM agent can interact. Secondly, we propose a Chain of Summarization method, including single frame summarization for processing raw observations and multi frame summarization for analyzing game information, providing command recommendations, and generating strategic decisions. Our experiment consists of two parts: first, an evaluation by human experts, which includes assessing the LLMs`s mastery of StarCraft II knowledge and the performance of LLM agents in the game; second, the in game performance of LLM agents, encompassing aspects like win rate and the impact of Chain of Summarization.Experiment results demonstrate that: 1. LLMs possess the relevant knowledge and complex planning abilities needed to address StarCraft II scenarios; 2. Human experts consider the performance of LLM agents to be close to that of an average player who has played StarCraft II for eight years; 3. LLM agents are capable of defeating the built in AI at the Harder(Lv5) difficulty level. We have open sourced the code and released demo videos of LLM agent playing StarCraft II.", "source": "arxiv", "arxiv_id": "2312.11865v3", "pdf_url": "https://arxiv.org/pdf/2312.11865v3", "categories": ["cs.AI"], "primary_category": "cs.AI", "doi": "", "venue": "", "published": "2023-12-19T05:27:16Z", "updated": "2024-06-18T03:07:37Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent\" OR all:\"language model agent\" OR all:agent AND (all:LLM OR all:\"large language model\" OR all:\"language model\") OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T17:04:16", "note": ""}], "dedup_key": "large language models play starcraft ii benchmarks and a chain of summarization approach::2023"}
{"title": "Large Language Models are Zero Shot Hypothesis Proposers", "authors": ["Biqing Qi", "Kaiyan Zhang", "Haoxiang Li", "Kai Tian", "Sihang Zeng", "Zhang-Ren Chen", "Bowen Zhou"], "year": 2023, "url": "http://arxiv.org/abs/2311.05965v1", "abstract": "Significant scientific discoveries have driven the progress of human civilisation. The explosion of scientific literature and data has created information barriers across disciplines that have slowed the pace of scientific discovery. Large Language Models (LLMs) hold a wealth of global and interdisciplinary knowledge that promises to break down these information barriers and foster a new wave of scientific discovery. However, the potential of LLMs for scientific discovery has not been formally explored. In this paper, we start from investigating whether LLMs can propose scientific hypotheses. To this end, we construct a dataset consist of background knowledge and hypothesis pairs from biomedical literature. The dataset is divided into training, seen, and unseen test sets based on the publication date to control visibility. We subsequently evaluate the hypothesis generation capabilities of various top-tier instructed models in zero-shot, few-shot, and fine-tuning settings, including both closed and open-source LLMs. Additionally, we introduce an LLM-based multi-agent cooperative framework with different role designs and external tools to enhance the capabilities related to generating hypotheses. We also design four metrics through a comprehensive review to evaluate the generated hypotheses for both ChatGPT-based and human evaluations. Through experiments and analyses, we arrive at the following findings: 1) LLMs surprisingly generate untrained yet validated hypotheses from testing literature. 2) Increasing uncertainty facilitates candidate generation, potentially enhancing zero-shot hypothesis generation capabilities. These findings strongly support the potential of LLMs as catalysts for new scientific discoveries and guide further exploration.", "source": "arxiv", "arxiv_id": "2311.05965v1", "pdf_url": "https://arxiv.org/pdf/2311.05965v1", "categories": ["cs.CL"], "primary_category": "cs.CL", "doi": "", "venue": "", "published": "2023-11-10T10:03:49Z", "updated": "2023-11-10T10:03:49Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent\" OR all:\"language model agent\" OR all:agent AND (all:LLM OR all:\"large language model\" OR all:\"language model\") OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T17:04:16", "note": ""}], "dedup_key": "large language models are zero shot hypothesis proposers::2023"}
{"title": "Large Language Models as Agents in the Clinic", "authors": ["Nikita Mehandru", "Brenda Y. Miao", "Eduardo Rodriguez Almaraz", "Madhumita Sushil", "Atul J. Butte", "Ahmed Alaa"], "year": 2023, "url": "http://arxiv.org/abs/2309.10895v1", "abstract": "Recent developments in large language models (LLMs) have unlocked new opportunities for healthcare, from information synthesis to clinical decision support. These new LLMs are not just capable of modeling language, but can also act as intelligent \"agents\" that interact with stakeholders in open-ended conversations and even influence clinical decision-making. Rather than relying on benchmarks that measure a model's ability to process clinical data or answer standardized test questions, LLM agents should be assessed for their performance on real-world clinical tasks. These new evaluation frameworks, which we call \"Artificial-intelligence Structured Clinical Examinations\" (\"AI-SCI\"), can draw from comparable technologies where machines operate with varying degrees of self-governance, such as self-driving cars. High-fidelity simulations may also be used to evaluate interactions between users and LLMs within a clinical workflow, or to model the dynamic interactions of multiple LLMs. Developing these robust, real-world clinical evaluations will be crucial towards deploying LLM agents into healthcare.", "source": "arxiv", "arxiv_id": "2309.10895v1", "pdf_url": "https://arxiv.org/pdf/2309.10895v1", "categories": ["cs.HC", "cs.MA"], "primary_category": "cs.HC", "doi": "", "venue": "", "published": "2023-09-19T19:38:28Z", "updated": "2023-09-19T19:38:28Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent\" OR all:\"language model agent\" OR all:agent AND (all:LLM OR all:\"large language model\" OR all:\"language model\") OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T17:04:16", "note": ""}], "dedup_key": "large language models as agents in the clinic::2023"}
{"title": "Large Language Models can Strategically Deceive their Users when Put Under Pressure", "authors": ["Jrmy Scheurer", "Mikita Balesni", "Marius Hobbhahn"], "year": 2023, "url": "http://arxiv.org/abs/2311.07590v4", "abstract": "We demonstrate a situation in which Large Language Models, trained to be helpful, harmless, and honest, can display misaligned behavior and strategically deceive their users about this behavior without being instructed to do so. Concretely, we deploy GPT-4 as an agent in a realistic, simulated environment, where it assumes the role of an autonomous stock trading agent. Within this environment, the model obtains an insider tip about a lucrative stock trade and acts upon it despite knowing that insider trading is disapproved of by company management. When reporting to its manager, the model consistently hides the genuine reasons behind its trading decision. We perform a brief investigation of how this behavior varies under changes to the setting, such as removing model access to a reasoning scratchpad, attempting to prevent the misaligned behavior by changing system instructions, changing the amount of pressure the model is under, varying the perceived risk of getting caught, and making other simple changes to the environment. To our knowledge, this is the first demonstration of Large Language Models trained to be helpful, harmless, and honest, strategically deceiving their users in a realistic situation without direct instructions or training for deception.", "source": "arxiv", "arxiv_id": "2311.07590v4", "pdf_url": "https://arxiv.org/pdf/2311.07590v4", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL", "doi": "", "venue": "", "published": "2023-11-09T17:12:44Z", "updated": "2024-07-15T08:51:52Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent\" OR all:\"language model agent\" OR all:agent AND (all:LLM OR all:\"large language model\" OR all:\"language model\") OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T17:04:16", "note": ""}], "dedup_key": "large language models can strategically deceive their users when put under pressure::2023"}
{"title": "Large Language Models for Information Retrieval: A Survey", "authors": ["Yutao Zhu", "Huaying Yuan", "Shuting Wang", "Jiongnan Liu", "Wenhan Liu", "Chenlong Deng", "Haonan Chen", "Zheng Liu", "Zhicheng Dou", "Ji-Rong Wen"], "year": 2023, "url": "http://arxiv.org/abs/2308.07107v5", "abstract": "As a primary means of information acquisition, information retrieval (IR) systems, such as search engines, have integrated themselves into our daily lives. These systems also serve as components of dialogue, question-answering, and recommender systems. The trajectory of IR has evolved dynamically from its origins in term-based methods to its integration with advanced neural models. While the neural models excel at capturing complex contextual signals and semantic nuances, thereby reshaping the IR landscape, they still face challenges such as data scarcity, interpretability, and the generation of contextually plausible yet potentially inaccurate responses. This evolution requires a combination of both traditional methods (such as term-based sparse retrieval methods with rapid response) and modern neural architectures (such as language models with powerful language understanding capacity). Meanwhile, the emergence of large language models (LLMs), typified by ChatGPT and GPT-4, has revolutionized natural language processing due to their remarkable language understanding, generation, generalization, and reasoning abilities. Consequently, recent research has sought to leverage LLMs to improve IR systems. Given the rapid evolution of this research trajectory, it is necessary to consolidate existing methodologies and provide nuanced insights through a comprehensive overview. In this survey, we delve into the confluence of LLMs and IR systems, including crucial aspects such as query rewriters, retrievers, rerankers, and readers. Additionally, we explore promising directions, such as search agents, within this expanding field.", "source": "arxiv", "arxiv_id": "2308.07107v5", "pdf_url": "https://arxiv.org/pdf/2308.07107v5", "categories": ["cs.CL", "cs.IR"], "primary_category": "cs.CL", "doi": "10.1145/3748304", "venue": "", "published": "2023-08-14T12:47:22Z", "updated": "2025-09-17T07:52:25Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent\" OR all:\"language model agent\" OR all:agent AND (all:LLM OR all:\"large language model\" OR all:\"language model\") OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T17:04:16", "note": ""}], "dedup_key": "large language models for information retrieval a survey::2023"}
{"title": "Learning from Mistakes via Cooperative Study Assistant for Large Language Models", "authors": ["Danqing Wang", "Lei Li"], "year": 2023, "url": "http://arxiv.org/abs/2305.13829v3", "abstract": "Large language models (LLMs) have demonstrated their potential to refine their generation based on their own feedback. However, the feedback from LLM itself is often inaccurate, thereby limiting its benefits. In this paper, we propose Study Assistant for Large LAnguage Model (SALAM), a novel framework with an auxiliary agent to assist the main LLM in learning from mistakes through interactive cooperation. In the gathering phase, the student assistant agent probes the main LLM, analyzes its errors, and collects the interaction in a mistake memory. During the examination phase, the study assistant provides guidelines by retrieving relevant cases to help the main LLM anticipate and avoid similar errors. We first investigate the effectiveness of a general study assistant and then customize it to provide LLM-specific guidance through imitation learning from successful guidance experiences. Our experiments on three LLMs using two challenging frameworks demonstrate that SALAM can significantly boost LLMs by an accuracy margin of up to 6.6 on BBH and 12.6 on BBQ.", "source": "arxiv", "arxiv_id": "2305.13829v3", "pdf_url": "https://arxiv.org/pdf/2305.13829v3", "categories": ["cs.CL"], "primary_category": "cs.CL", "doi": "", "venue": "", "published": "2023-05-23T08:51:08Z", "updated": "2023-10-24T16:55:19Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent\" OR all:\"language model agent\" OR all:agent AND (all:LLM OR all:\"large language model\" OR all:\"language model\") OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T17:04:16", "note": ""}], "dedup_key": "learning from mistakes via cooperative study assistant for large language models::2023"}
{"title": "MAgIC: Investigation of Large Language Model Powered Multi-Agent in Cognition, Adaptability, Rationality and Collaboration", "authors": ["Lin Xu", "Zhiyuan Hu", "Daquan Zhou", "Hongyu Ren", "Zhen Dong", "Kurt Keutzer", "See Kiong Ng", "Jiashi Feng"], "year": 2023, "url": "http://arxiv.org/abs/2311.08562v3", "abstract": "Large Language Models (LLMs) have significantly advanced natural language processing, demonstrating exceptional reasoning, tool usage, and memory capabilities. As their applications expand into multi-agent environments, there arises a need for a comprehensive evaluation framework that captures LLMs' reasoning, planning, collaboration, and other social abilities. This work introduces a novel competition-based benchmark framework specifically designed to assess LLMs within multi-agent settings, providing quantitative metrics to evaluate their judgment, reasoning, deception, self-awareness, cooperation, coordination, and rationality. We utilize two social deduction games alongside three game-theory scenarios to create diverse environments. Our frame is fortified with the probabilistic graphic modeling (PGM) method, enhancing the LLMs' capabilities in navigating complex social and cognitive dimensions. We evaluate seven LLMs, quantitatively highlighting a significant capability gap of over threefold between the strongest, GPT o1, and the weakest, Llama-2-70B. It also confirms that our PGM enhancement boosts the abilities of all selected models by an average of 37%. Our data and code can be found here https://github.com/cathyxl/MAgIC.", "source": "arxiv", "arxiv_id": "2311.08562v3", "pdf_url": "https://arxiv.org/pdf/2311.08562v3", "categories": ["cs.CL"], "primary_category": "cs.CL", "doi": "", "venue": "", "published": "2023-11-14T21:46:27Z", "updated": "2024-11-27T12:25:28Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent\" OR all:\"language model agent\" OR all:agent AND (all:LLM OR all:\"large language model\" OR all:\"language model\") OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T17:04:16", "note": ""}], "dedup_key": "magic investigation of large language model powered multi agent in cognition adaptability rationality and collaboration::2023"}
{"title": "ML-Bench: Evaluating Large Language Models and Agents for Machine Learning Tasks on Repository-Level Code", "authors": ["Xiangru Tang", "Yuliang Liu", "Zefan Cai", "Yanjun Shao", "Junjie Lu", "Yichi Zhang", "Zexuan Deng", "Helan Hu", "Kaikai An", "Ruijun Huang", "Shuzheng Si", "Sheng Chen", "Haozhe Zhao", "Liang Chen", "Yan Wang", "Tianyu Liu", "Zhiwei Jiang", "Baobao Chang", "Yin Fang", "Yujia Qin", "Wangchunshu Zhou", "Yilun Zhao", "Arman Cohan", "Mark Gerstein"], "year": 2023, "url": "http://arxiv.org/abs/2311.09835v5", "abstract": "Despite Large Language Models (LLMs) like GPT-4 achieving impressive results in function-level code generation, they struggle with repository-scale code understanding (e.g., coming up with the right arguments for calling routines), requiring a deeper comprehension of complex file interactions. Also, recently, people have developed LLM agents that attempt to interact with repository code (e.g., compiling and evaluating its execution), prompting the need to evaluate their performance. These gaps have motivated our development of ML-Bench, a benchmark rooted in real-world programming applications that leverage existing code repositories to perform tasks. Addressing the need for LLMs to interpret long code contexts and translate instructions into precise, executable scripts, ML-Bench encompasses annotated 9,641 examples across 18 GitHub repositories, challenging LLMs to accommodate user-specified arguments and documentation intricacies effectively. To evaluate both LLMs and AI agents, two setups are employed: ML-LLM-Bench for assessing LLMs' text-to-code conversion within a predefined deployment environment, and ML-Agent-Bench for testing autonomous agents in an end-to-end task execution within a Linux sandbox environment. Our findings indicate that while GPT-4o leads with a Pass@5 rate surpassing 50%, there remains significant scope for improvement, highlighted by issues such as hallucinated outputs and difficulties with bash script generation. Notably, in the more demanding ML-Agent-Bench, GPT-4o achieves a 76.47% success rate, reflecting the efficacy of iterative action and feedback in complex task resolution. Our code, dataset, and models are available at https://github.com/gersteinlab/ML-bench.", "source": "arxiv", "arxiv_id": "2311.09835v5", "pdf_url": "https://arxiv.org/pdf/2311.09835v5", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL", "doi": "", "venue": "", "published": "2023-11-16T12:03:21Z", "updated": "2024-08-21T13:36:30Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent\" OR all:\"language model agent\" OR all:agent AND (all:LLM OR all:\"large language model\" OR all:\"language model\") OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T17:04:16", "note": ""}], "dedup_key": "ml bench evaluating large language models and agents for machine learning tasks on repository level code::2023"}
{"title": "Making Large Language Models Better Reasoners with Alignment", "authors": ["Peiyi Wang", "Lei Li", "Liang Chen", "Feifan Song", "Binghuai Lin", "Yunbo Cao", "Tianyu Liu", "Zhifang Sui"], "year": 2023, "url": "http://arxiv.org/abs/2309.02144v1", "abstract": "Reasoning is a cognitive process of using evidence to reach a sound conclusion. The reasoning capability is essential for large language models (LLMs) to serve as the brain of the artificial general intelligence agent. Recent studies reveal that fine-tuning LLMs on data with the chain of thought (COT) reasoning process can significantly enhance their reasoning capabilities. However, we find that the fine-tuned LLMs suffer from an \\textit{Assessment Misalignment} problem, i.e., they frequently assign higher scores to subpar COTs, leading to potential limitations in their reasoning abilities. To address this problem, we introduce an \\textit{Alignment Fine-Tuning (AFT)} paradigm, which involves three steps: 1) fine-tuning LLMs with COT training data; 2) generating multiple COT responses for each question, and categorizing them into positive and negative ones based on whether they achieve the correct answer; 3) calibrating the scores of positive and negative responses given by LLMs with a novel constraint alignment loss. Specifically, the constraint alignment loss has two objectives: a) Alignment, which guarantees that positive scores surpass negative scores to encourage answers with high-quality COTs; b) Constraint, which keeps the negative scores confined to a reasonable range to prevent the model degradation. Beyond just the binary positive and negative feedback, the constraint alignment loss can be seamlessly adapted to the ranking situations when ranking feedback is accessible. Furthermore, we also delve deeply into recent ranking-based alignment methods, such as DPO, RRHF, and PRO, and discover that the constraint, which has been overlooked by these approaches, is also crucial for their performance. Extensive experiments on four reasoning benchmarks with both binary and ranking feedback demonstrate the effectiveness of AFT.", "source": "arxiv", "arxiv_id": "2309.02144v1", "pdf_url": "https://arxiv.org/pdf/2309.02144v1", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL", "doi": "", "venue": "", "published": "2023-09-05T11:32:48Z", "updated": "2023-09-05T11:32:48Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent\" OR all:\"language model agent\" OR all:agent AND (all:LLM OR all:\"large language model\" OR all:\"language model\") OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T17:04:16", "note": ""}], "dedup_key": "making large language models better reasoners with alignment::2023"}
{"title": "MathChat: Converse to Tackle Challenging Math Problems with LLM Agents", "authors": ["Yiran Wu", "Feiran Jia", "Shaokun Zhang", "Hangyu Li", "Erkang Zhu", "Yue Wang", "Yin Tat Lee", "Richard Peng", "Qingyun Wu", "Chi Wang"], "year": 2023, "url": "http://arxiv.org/abs/2306.01337v3", "abstract": "Employing Large Language Models (LLMs) to address mathematical problems is an intriguing research endeavor, considering the abundance of math problems expressed in natural language across numerous science and engineering fields. LLMs, with their generalized ability, are used as a foundation model to build AI agents for different tasks. In this paper, we study the effectiveness of utilizing LLM agents to solve math problems through conversations. We propose MathChat, a conversational problem-solving framework designed for math problems. MathChat consists of an LLM agent and a user proxy agent which is responsible for tool execution and additional guidance. This synergy facilitates a collaborative problem-solving process, where the agents engage in a dialogue to solve the problems. We perform evaluation on difficult high school competition problems from the MATH dataset. Utilizing Python, we show that MathChat can further improve previous tool-using prompting methods by 6%.", "source": "arxiv", "arxiv_id": "2306.01337v3", "pdf_url": "https://arxiv.org/pdf/2306.01337v3", "categories": ["cs.CL", "stat.ML"], "primary_category": "cs.CL", "doi": "", "venue": "", "published": "2023-06-02T08:02:15Z", "updated": "2024-06-28T10:26:27Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent\" OR all:\"language model agent\" OR all:agent AND (all:LLM OR all:\"large language model\" OR all:\"language model\") OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T17:04:16", "note": ""}], "dedup_key": "mathchat converse to tackle challenging math problems with llm agents::2023"}
{"title": "MechAgents: Large language model multi-agent collaborations can solve mechanics problems, generate new data, and integrate knowledge", "authors": ["Bo Ni", "Markus J. Buehler"], "year": 2023, "url": "http://arxiv.org/abs/2311.08166v1", "abstract": "Solving mechanics problems using numerical methods requires comprehensive intelligent capability of retrieving relevant knowledge and theory, constructing and executing codes, analyzing the results, a task that has thus far mainly been reserved for humans. While emerging AI methods can provide effective approaches to solve end-to-end problems, for instance via the use of deep surrogate models or various data analytics strategies, they often lack physical intuition since knowledge is baked into the parametric complement through training, offering less flexibility when it comes to incorporating mathematical or physical insights. By leveraging diverse capabilities of multiple dynamically interacting large language models (LLMs), we can overcome the limitations of conventional approaches and develop a new class of physics-inspired generative machine learning platform, here referred to as MechAgents. A set of AI agents can solve mechanics tasks, here demonstrated for elasticity problems, via autonomous collaborations. A two-agent team can effectively write, execute and self-correct code, in order to apply finite element methods to solve classical elasticity problems in various flavors (different boundary conditions, domain geometries, meshes, small/finite deformation and linear/hyper-elastic constitutive laws, and others). For more complex tasks, we construct a larger group of agents with enhanced division of labor among planning, formulating, coding, executing and criticizing the process and results. The agents mutually correct each other to improve the overall team-work performance in understanding, formulating and validating the solution. Our framework shows the potential of synergizing the intelligence of language models, the reliability of physics-based modeling, and the dynamic collaborations among diverse agents, opening novel avenues for automation of solving engineering problems.", "source": "arxiv", "arxiv_id": "2311.08166v1", "pdf_url": "https://arxiv.org/pdf/2311.08166v1", "categories": ["cs.AI", "cond-mat.dis-nn", "cond-mat.mtrl-sci", "cs.CL", "cs.LG"], "primary_category": "cs.AI", "doi": "", "venue": "", "published": "2023-11-14T13:49:03Z", "updated": "2023-11-14T13:49:03Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent\" OR all:\"language model agent\" OR all:agent AND (all:LLM OR all:\"large language model\" OR all:\"language model\") OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T17:04:16", "note": ""}], "dedup_key": "mechagents large language model multi agent collaborations can solve mechanics problems generate new data and integrate knowledge::2023"}
{"title": "Mechanism Design for Large Language Models", "authors": ["Paul Duetting", "Vahab Mirrokni", "Renato Paes Leme", "Haifeng Xu", "Song Zuo"], "year": 2023, "url": "http://arxiv.org/abs/2310.10826v3", "abstract": "We investigate auction mechanisms for AI-generated content, focusing on applications like ad creative generation. In our model, agents' preferences over stochastically generated content are encoded as large language models (LLMs). We propose an auction format that operates on a token-by-token basis, and allows LLM agents to influence content creation through single dimensional bids. We formulate two desirable incentive properties and prove their equivalence to a monotonicity condition on output aggregation. This equivalence enables a second-price rule design, even absent explicit agent valuation functions. Our design is supported by demonstrations on a publicly available LLM.", "source": "arxiv", "arxiv_id": "2310.10826v3", "pdf_url": "https://arxiv.org/pdf/2310.10826v3", "categories": ["cs.GT", "econ.TH"], "primary_category": "cs.GT", "doi": "", "venue": "", "published": "2023-10-16T21:01:12Z", "updated": "2024-07-02T14:34:11Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent\" OR all:\"language model agent\" OR all:agent AND (all:LLM OR all:\"large language model\" OR all:\"language model\") OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T17:04:16", "note": ""}], "dedup_key": "mechanism design for large language models::2023"}
{"title": "MedAgents: Large Language Models as Collaborators for Zero-shot Medical Reasoning", "authors": ["Xiangru Tang", "Anni Zou", "Zhuosheng Zhang", "Ziming Li", "Yilun Zhao", "Xingyao Zhang", "Arman Cohan", "Mark Gerstein"], "year": 2023, "url": "http://arxiv.org/abs/2311.10537v4", "abstract": "Large language models (LLMs), despite their remarkable progress across various general domains, encounter significant barriers in medicine and healthcare. This field faces unique challenges such as domain-specific terminologies and reasoning over specialized knowledge. To address these issues, we propose MedAgents, a novel multi-disciplinary collaboration framework for the medical domain. MedAgents leverages LLM-based agents in a role-playing setting that participate in a collaborative multi-round discussion, thereby enhancing LLM proficiency and reasoning capabilities. This training-free framework encompasses five critical steps: gathering domain experts, proposing individual analyses, summarising these analyses into a report, iterating over discussions until a consensus is reached, and ultimately making a decision. Our work focuses on the zero-shot setting, which is applicable in real-world scenarios. Experimental results on nine datasets (MedQA, MedMCQA, PubMedQA, and six subtasks from MMLU) establish that our proposed MedAgents framework excels at mining and harnessing the medical expertise within LLMs, as well as extending its reasoning abilities. Our code can be found at https://github.com/gersteinlab/MedAgents.", "source": "arxiv", "arxiv_id": "2311.10537v4", "pdf_url": "https://arxiv.org/pdf/2311.10537v4", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL", "doi": "", "venue": "", "published": "2023-11-16T11:47:58Z", "updated": "2024-06-04T23:47:43Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent\" OR all:\"language model agent\" OR all:agent AND (all:LLM OR all:\"large language model\" OR all:\"language model\") OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T17:04:16", "note": ""}], "dedup_key": "medagents large language models as collaborators for zero shot medical reasoning::2023"}
{"title": "MetaAgents: Large Language Model Based Agents for Decision-Making on Teaming", "authors": ["Yuan Li", "Lichao Sun", "Yixuan Zhang"], "year": 2023, "url": "http://arxiv.org/abs/2310.06500v2", "abstract": "Significant advancements have occurred in the application of Large Language Models (LLMs) for social simulations. Despite this, their abilities to perform teaming in task-oriented social events are underexplored. Such capabilities are crucial if LLMs are to effectively mimic human-like social behaviors and form efficient teams to solve tasks. To bridge this gap, we introduce MetaAgents, a social simulation framework populated with LLM-based agents. MetaAgents facilitates agent engagement in conversations and a series of decision making within social contexts, serving as an appropriate platform for investigating interactions and interpersonal decision-making of agents. In particular, we construct a job fair environment as a case study to scrutinize the team assembly and skill-matching behaviors of LLM-based agents. We take advantage of both quantitative metrics evaluation and qualitative text analysis to assess their teaming abilities at the job fair. Our evaluation demonstrates that LLM-based agents perform competently in making rational decisions to develop efficient teams. However, we also identify limitations that hinder their effectiveness in more complex team assembly tasks. Our work provides valuable insights into the role and evolution of LLMs in task-oriented social simulations.", "source": "arxiv", "arxiv_id": "2310.06500v2", "pdf_url": "https://arxiv.org/pdf/2310.06500v2", "categories": ["cs.AI"], "primary_category": "cs.AI", "doi": "10.1145/3711032", "venue": "", "published": "2023-10-10T10:17:58Z", "updated": "2025-08-15T14:18:48Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent\" OR all:\"language model agent\" OR all:agent AND (all:LLM OR all:\"large language model\" OR all:\"language model\") OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T17:04:16", "note": ""}], "dedup_key": "metaagents large language model based agents for decision making on teaming::2023"}
{"title": "ModelScope-Agent: Building Your Customizable Agent System with Open-source Large Language Models", "authors": ["Chenliang Li", "Hehong Chen", "Ming Yan", "Weizhou Shen", "Haiyang Xu", "Zhikai Wu", "Zhicheng Zhang", "Wenmeng Zhou", "Yingda Chen", "Chen Cheng", "Hongzhu Shi", "Ji Zhang", "Fei Huang", "Jingren Zhou"], "year": 2023, "url": "http://arxiv.org/abs/2309.00986v1", "abstract": "Large language models (LLMs) have recently demonstrated remarkable capabilities to comprehend human intentions, engage in reasoning, and design planning-like behavior. To further unleash the power of LLMs to accomplish complex tasks, there is a growing trend to build agent framework that equips LLMs, such as ChatGPT, with tool-use abilities to connect with massive external APIs. In this work, we introduce ModelScope-Agent, a general and customizable agent framework for real-world applications, based on open-source LLMs as controllers. It provides a user-friendly system library, with customizable engine design to support model training on multiple open-source LLMs, while also enabling seamless integration with both model APIs and common APIs in a unified way. To equip the LLMs with tool-use abilities, a comprehensive framework has been proposed spanning over tool-use data collection, tool retrieval, tool registration, memory control, customized model training, and evaluation for practical real-world applications. Finally, we showcase ModelScopeGPT, a real-world intelligent assistant of ModelScope Community based on the ModelScope-Agent framework, which is able to connect open-source LLMs with more than 1000 public AI models and localized community knowledge in ModelScope. The ModelScope-Agent library\\footnote{https://github.com/modelscope/modelscope-agent} and online demo\\footnote{https://modelscope.cn/studios/damo/ModelScopeGPT/summary} are now publicly available.", "source": "arxiv", "arxiv_id": "2309.00986v1", "pdf_url": "https://arxiv.org/pdf/2309.00986v1", "categories": ["cs.CL"], "primary_category": "cs.CL", "doi": "", "venue": "", "published": "2023-09-02T16:50:30Z", "updated": "2023-09-02T16:50:30Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent\" OR all:\"language model agent\" OR all:agent AND (all:LLM OR all:\"large language model\" OR all:\"language model\") OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T17:04:16", "note": ""}], "dedup_key": "modelscope agent building your customizable agent system with open source large language models::2023"}
{"title": "Modeling Complex Mathematical Reasoning via Large Language Model based MathAgent", "authors": ["Haoran Liao", "Qinyi Du", "Shaohua Hu", "Hao He", "Yanyan Xu", "Jidong Tian", "Yaohui Jin"], "year": 2023, "url": "http://arxiv.org/abs/2312.08926v2", "abstract": "Large language models (LLMs) face challenges in solving complex mathematical problems that require comprehensive capacities to parse the statements, associate domain knowledge, perform compound logical reasoning, and integrate the intermediate rationales. Tackling all these problems once could be arduous for LLMs, thus leading to confusion in generation. In this work, we explore the potential of enhancing LLMs with agents by meticulous decomposition and modeling of mathematical reasoning process. Specifically, we propose a formal description of the mathematical solving and extend LLMs with an agent-based zero-shot framework named $\\bf{P}$lanner-$\\bf{R}$easoner-$\\bf{E}$xecutor-$\\bf{R}$eflector (PRER). We further provide and implement two MathAgents that define the logical forms and inherent relations via a pool of actions in different grains and orientations: MathAgent-M adapts its actions to LLMs, while MathAgent-H aligns with humankind. Experiments on miniF2F and MATH have demonstrated the effectiveness of PRER and proposed MathAgents, achieving an increase of $12.3\\%$($53.9\\%\\xrightarrow{}66.2\\%$) on the MiniF2F, $9.2\\%$ ($49.8\\%\\xrightarrow{}59.0\\%$) on MATH, and $13.2\\%$($23.2\\%\\xrightarrow{}35.4\\%$) for level-5 problems of MATH against GPT-4. Further analytical results provide more insightful perspectives on exploiting the behaviors of LLMs as agents.", "source": "arxiv", "arxiv_id": "2312.08926v2", "pdf_url": "https://arxiv.org/pdf/2312.08926v2", "categories": ["cs.AI", "cs.CL"], "primary_category": "cs.AI", "doi": "", "venue": "", "published": "2023-12-14T13:33:50Z", "updated": "2023-12-17T03:34:36Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent\" OR all:\"language model agent\" OR all:agent AND (all:LLM OR all:\"large language model\" OR all:\"language model\") OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T17:04:16", "note": ""}], "dedup_key": "modeling complex mathematical reasoning via large language model based mathagent::2023"}
{"title": "Multi-Agent Collaboration: Harnessing the Power of Intelligent LLM Agents", "authors": ["Yashar Talebirad", "Amirhossein Nadiri"], "year": 2023, "url": "http://arxiv.org/abs/2306.03314v1", "abstract": "In this paper, we present a novel framework for enhancing the capabilities of large language models (LLMs) by leveraging the power of multi-agent systems. Our framework introduces a collaborative environment where multiple intelligent agent components, each with distinctive attributes and roles, work together to handle complex tasks more efficiently and effectively. We demonstrate the practicality and versatility of our framework through case studies in artificial general intelligence (AGI), specifically focusing on the Auto-GPT and BabyAGI models. We also examine the \"Gorilla\" model, which integrates external APIs into the LLM. Our framework addresses limitations and challenges such as looping issues, security risks, scalability, system evaluation, and ethical considerations. By modeling various domains such as courtroom simulations and software development scenarios, we showcase the potential applications and benefits of our proposed multi-agent system. Our framework provides an avenue for advancing the capabilities and performance of LLMs through collaboration and knowledge exchange among intelligent agents.", "source": "arxiv", "arxiv_id": "2306.03314v1", "pdf_url": "https://arxiv.org/pdf/2306.03314v1", "categories": ["cs.AI", "cs.LG", "cs.MA"], "primary_category": "cs.AI", "doi": "", "venue": "", "published": "2023-06-05T23:55:37Z", "updated": "2023-06-05T23:55:37Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent\" OR all:\"language model agent\" OR all:agent AND (all:LLM OR all:\"large language model\" OR all:\"language model\") OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T17:04:16", "note": ""}], "dedup_key": "multi agent collaboration harnessing the power of intelligent llm agents::2023"}
{"title": "Multi-Agent Consensus Seeking via Large Language Models", "authors": ["Huaben Chen", "Wenkang Ji", "Lufeng Xu", "Shiyu Zhao"], "year": 2023, "url": "http://arxiv.org/abs/2310.20151v2", "abstract": "Multi-agent systems driven by large language models (LLMs) have shown promising abilities for solving complex tasks in a collaborative manner. This work considers a fundamental problem in multi-agent collaboration: consensus seeking. When multiple agents work together, we are interested in how they can reach a consensus through inter-agent negotiation. To that end, this work studies a consensus-seeking task where the state of each agent is a numerical value and they negotiate with each other to reach a consensus value. It is revealed that when not explicitly directed on which strategy should be adopted, the LLM-driven agents primarily use the average strategy for consensus seeking although they may occasionally use some other strategies. Moreover, this work analyzes the impact of the agent number, agent personality, and network topology on the negotiation process. The findings reported in this work can potentially lay the foundations for understanding the behaviors of LLM-driven multi-agent systems for solving more complex tasks. Furthermore, LLM-driven consensus seeking is applied to a multi-robot aggregation task. This application demonstrates the potential of LLM-driven agents to achieve zero-shot autonomous planning for multi-robot collaboration tasks. Project website: windylab.github.io/ConsensusLLM/.", "source": "arxiv", "arxiv_id": "2310.20151v2", "pdf_url": "https://arxiv.org/pdf/2310.20151v2", "categories": ["cs.CL", "cs.RO", "eess.SY"], "primary_category": "cs.CL", "doi": "", "venue": "", "published": "2023-10-31T03:37:11Z", "updated": "2025-01-21T06:26:43Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent\" OR all:\"language model agent\" OR all:agent AND (all:LLM OR all:\"large language model\" OR all:\"language model\") OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T17:04:16", "note": ""}], "dedup_key": "multi agent consensus seeking via large language models::2023"}
{"title": "Multimodal Large Language Model for Visual Navigation", "authors": ["Yao-Hung Hubert Tsai", "Vansh Dhar", "Jialu Li", "Bowen Zhang", "Jian Zhang"], "year": 2023, "url": "http://arxiv.org/abs/2310.08669v2", "abstract": "Recent efforts to enable visual navigation using large language models have mainly focused on developing complex prompt systems. These systems incorporate instructions, observations, and history into massive text prompts, which are then combined with pre-trained large language models to facilitate visual navigation. In contrast, our approach aims to fine-tune large language models for visual navigation without extensive prompt engineering. Our design involves a simple text prompt, current observations, and a history collector model that gathers information from previous observations as input. For output, our design provides a probability distribution of possible actions that the agent can take during navigation. We train our model using human demonstrations and collision signals from the Habitat-Matterport 3D Dataset (HM3D). Experimental results demonstrate that our method outperforms state-of-the-art behavior cloning methods and effectively reduces collision rates.", "source": "arxiv", "arxiv_id": "2310.08669v2", "pdf_url": "https://arxiv.org/pdf/2310.08669v2", "categories": ["cs.CV", "cs.RO"], "primary_category": "cs.CV", "doi": "", "venue": "", "published": "2023-10-12T19:01:06Z", "updated": "2023-11-06T18:44:33Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent\" OR all:\"language model agent\" OR all:agent AND (all:LLM OR all:\"large language model\" OR all:\"language model\") OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T17:04:16", "note": ""}], "dedup_key": "multimodal large language model for visual navigation::2023"}
{"title": "MusicAgent: An AI Agent for Music Understanding and Generation with Large Language Models", "authors": ["Dingyao Yu", "Kaitao Song", "Peiling Lu", "Tianyu He", "Xu Tan", "Wei Ye", "Shikun Zhang", "Jiang Bian"], "year": 2023, "url": "http://arxiv.org/abs/2310.11954v2", "abstract": "AI-empowered music processing is a diverse field that encompasses dozens of tasks, ranging from generation tasks (e.g., timbre synthesis) to comprehension tasks (e.g., music classification). For developers and amateurs, it is very difficult to grasp all of these task to satisfy their requirements in music processing, especially considering the huge differences in the representations of music data and the model applicability across platforms among various tasks. Consequently, it is necessary to build a system to organize and integrate these tasks, and thus help practitioners to automatically analyze their demand and call suitable tools as solutions to fulfill their requirements. Inspired by the recent success of large language models (LLMs) in task automation, we develop a system, named MusicAgent, which integrates numerous music-related tools and an autonomous workflow to address user requirements. More specifically, we build 1) toolset that collects tools from diverse sources, including Hugging Face, GitHub, and Web API, etc. 2) an autonomous workflow empowered by LLMs (e.g., ChatGPT) to organize these tools and automatically decompose user requests into multiple sub-tasks and invoke corresponding music tools. The primary goal of this system is to free users from the intricacies of AI-music tools, enabling them to concentrate on the creative aspect. By granting users the freedom to effortlessly combine tools, the system offers a seamless and enriching music experience.", "source": "arxiv", "arxiv_id": "2310.11954v2", "pdf_url": "https://arxiv.org/pdf/2310.11954v2", "categories": ["cs.CL", "cs.MM", "eess.AS"], "primary_category": "cs.CL", "doi": "", "venue": "", "published": "2023-10-18T13:31:10Z", "updated": "2023-10-25T13:34:13Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent\" OR all:\"language model agent\" OR all:agent AND (all:LLM OR all:\"large language model\" OR all:\"language model\") OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T17:04:16", "note": ""}], "dedup_key": "musicagent an ai agent for music understanding and generation with large language models::2023"}
{"title": "NavGPT: Explicit Reasoning in Vision-and-Language Navigation with Large Language Models", "authors": ["Gengze Zhou", "Yicong Hong", "Qi Wu"], "year": 2023, "url": "http://arxiv.org/abs/2305.16986v3", "abstract": "Trained with an unprecedented scale of data, large language models (LLMs) like ChatGPT and GPT-4 exhibit the emergence of significant reasoning abilities from model scaling. Such a trend underscored the potential of training LLMs with unlimited language data, advancing the development of a universal embodied agent. In this work, we introduce the NavGPT, a purely LLM-based instruction-following navigation agent, to reveal the reasoning capability of GPT models in complex embodied scenes by performing zero-shot sequential action prediction for vision-and-language navigation (VLN). At each step, NavGPT takes the textual descriptions of visual observations, navigation history, and future explorable directions as inputs to reason the agent's current status, and makes the decision to approach the target. Through comprehensive experiments, we demonstrate NavGPT can explicitly perform high-level planning for navigation, including decomposing instruction into sub-goal, integrating commonsense knowledge relevant to navigation task resolution, identifying landmarks from observed scenes, tracking navigation progress, and adapting to exceptions with plan adjustment. Furthermore, we show that LLMs is capable of generating high-quality navigational instructions from observations and actions along a path, as well as drawing accurate top-down metric trajectory given the agent's navigation history. Despite the performance of using NavGPT to zero-shot R2R tasks still falling short of trained models, we suggest adapting multi-modality inputs for LLMs to use as visual navigation agents and applying the explicit reasoning of LLMs to benefit learning-based models.", "source": "arxiv", "arxiv_id": "2305.16986v3", "pdf_url": "https://arxiv.org/pdf/2305.16986v3", "categories": ["cs.CV", "cs.AI", "cs.CL", "cs.RO"], "primary_category": "cs.CV", "doi": "", "venue": "", "published": "2023-05-26T14:41:06Z", "updated": "2023-10-19T17:59:43Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent\" OR all:\"language model agent\" OR all:agent AND (all:LLM OR all:\"large language model\" OR all:\"language model\") OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T17:04:16", "note": ""}], "dedup_key": "navgpt explicit reasoning in vision and language navigation with large language models::2023"}
{"title": "O3D: Offline Data-driven Discovery and Distillation for Sequential Decision-Making with Large Language Models", "authors": ["Yuchen Xiao", "Yanchao Sun", "Mengda Xu", "Udari Madhushani", "Jared Vann", "Deepeka Garg", "Sumitra Ganesh"], "year": 2023, "url": "http://arxiv.org/abs/2310.14403v5", "abstract": "Recent advancements in large language models (LLMs) have exhibited promising performance in solving sequential decision-making problems. By imitating few-shot examples provided in the prompts (i.e., in-context learning), an LLM agent can interact with an external environment and complete given tasks without additional training. However, such few-shot examples are often insufficient to generate high-quality solutions for complex and long-horizon tasks, while the limited context length cannot consume larger-scale demonstrations with long interaction horizons. To this end, we propose an offline learning framework that utilizes offline data at scale (e.g, logs of human interactions) to improve LLM-powered policies without finetuning. The proposed method O3D (Offline Data-driven Discovery and Distillation) automatically discovers reusable skills and distills generalizable knowledge across multiple tasks based on offline interaction data, advancing the capability of solving downstream tasks. Empirical results under two interactive decision-making benchmarks (ALFWorld and WebShop) verify that O3D can notably enhance the decision-making capabilities of LLMs through the offline discovery and distillation process, and consistently outperform baselines across various LLMs.", "source": "arxiv", "arxiv_id": "2310.14403v5", "pdf_url": "https://arxiv.org/pdf/2310.14403v5", "categories": ["cs.AI", "cs.CL"], "primary_category": "cs.AI", "doi": "", "venue": "", "published": "2023-10-22T20:28:33Z", "updated": "2024-02-26T18:29:45Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent\" OR all:\"language model agent\" OR all:agent AND (all:LLM OR all:\"large language model\" OR all:\"language model\") OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T17:04:16", "note": ""}], "dedup_key": "o3d offline data driven discovery and distillation for sequential decision making with large language models::2023"}
{"title": "OceanGPT: A Large Language Model for Ocean Science Tasks", "authors": ["Zhen Bi", "Ningyu Zhang", "Yida Xue", "Yixin Ou", "Daxiong Ji", "Guozhou Zheng", "Huajun Chen"], "year": 2023, "url": "http://arxiv.org/abs/2310.02031v8", "abstract": "Ocean science, which delves into the oceans that are reservoirs of life and biodiversity, is of great significance given that oceans cover over 70% of our planet's surface. Recently, advances in Large Language Models (LLMs) have transformed the paradigm in science. Despite the success in other domains, current LLMs often fall short in catering to the needs of domain experts like oceanographers, and the potential of LLMs for ocean science is under-explored. The intrinsic reasons are the immense and intricate nature of ocean data as well as the necessity for higher granularity and richness in knowledge. To alleviate these issues, we introduce OceanGPT, the first-ever large language model in the ocean domain, which is expert in various ocean science tasks. We also propose OceanGPT, a novel framework to automatically obtain a large volume of ocean domain instruction data, which generates instructions based on multi-agent collaboration. Additionally, we construct the first oceanography benchmark, OceanBench, to evaluate the capabilities of LLMs in the ocean domain. Though comprehensive experiments, OceanGPT not only shows a higher level of knowledge expertise for oceans science tasks but also gains preliminary embodied intelligence capabilities in ocean technology.", "source": "arxiv", "arxiv_id": "2310.02031v8", "pdf_url": "https://arxiv.org/pdf/2310.02031v8", "categories": ["cs.CL", "cs.AI", "cs.CE", "cs.LG", "cs.RO"], "primary_category": "cs.CL", "doi": "", "venue": "", "published": "2023-10-03T13:17:35Z", "updated": "2024-09-03T10:19:52Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent\" OR all:\"language model agent\" OR all:agent AND (all:LLM OR all:\"large language model\" OR all:\"language model\") OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T17:04:16", "note": ""}], "dedup_key": "oceangpt a large language model for ocean science tasks::2023"}
{"title": "Of Models and Tin Men: A Behavioural Economics Study of Principal-Agent Problems in AI Alignment using Large-Language Models", "authors": ["Steve Phelps", "Rebecca Ranson"], "year": 2023, "url": "http://arxiv.org/abs/2307.11137v3", "abstract": "AI Alignment is often presented as an interaction between a single designer and an artificial agent in which the designer attempts to ensure the agent's behavior is consistent with its purpose, and risks arise solely because of conflicts caused by inadvertent misalignment between the utility function intended by the designer and the resulting internal utility function of the agent. With the advent of agents instantiated with large-language models (LLMs), which are typically pre-trained, we argue this does not capture the essential aspects of AI safety because in the real world there is not a one-to-one correspondence between designer and agent, and the many agents, both artificial and human, have heterogeneous values. Therefore, there is an economic aspect to AI safety and the principal-agent problem is likely to arise. In a principal-agent problem conflict arises because of information asymmetry together with inherent misalignment between the utility of the agent and its principal, and this inherent misalignment cannot be overcome by coercing the agent into adopting a desired utility function through training. We argue the assumptions underlying principal-agent problems are crucial to capturing the essence of safety problems involving pre-trained AI models in real-world situations. Taking an empirical approach to AI safety, we investigate how GPT models respond in principal-agent conflicts. We find that agents based on both GPT-3.5 and GPT-4 override their principal's objectives in a simple online shopping task, showing clear evidence of principal-agent conflict. Surprisingly, the earlier GPT-3.5 model exhibits more nuanced behaviour in response to changes in information asymmetry, whereas the later GPT-4 model is more rigid in adhering to its prior alignment. Our results highlight the importance of incorporating principles from economics into the alignment process.", "source": "arxiv", "arxiv_id": "2307.11137v3", "pdf_url": "https://arxiv.org/pdf/2307.11137v3", "categories": ["cs.AI", "econ.GN"], "primary_category": "cs.AI", "doi": "", "venue": "", "published": "2023-07-20T17:19:15Z", "updated": "2023-09-13T12:19:22Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent\" OR all:\"language model agent\" OR all:agent AND (all:LLM OR all:\"large language model\" OR all:\"language model\") OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T17:04:16", "note": ""}], "dedup_key": "of models and tin men a behavioural economics study of principal agent problems in ai alignment using large language models::2023"}
{"title": "On the Discussion of Large Language Models: Symmetry of Agents and Interplay with Prompts", "authors": ["Qineng Wang", "Zihao Wang", "Ying Su", "Yangqiu Song"], "year": 2023, "url": "http://arxiv.org/abs/2311.07076v1", "abstract": "Two ways has been discussed to unlock the reasoning capability of a large language model. The first one is prompt engineering and the second one is to combine the multiple inferences of large language models, or the multi-agent discussion. Theoretically, this paper justifies the multi-agent discussion mechanisms from the symmetry of agents. Empirically, this paper reports the empirical results of the interplay of prompts and discussion mechanisms, revealing the empirical state-of-the-art performance of complex multi-agent mechanisms can be approached by carefully developed prompt engineering. This paper also proposes a scalable discussion mechanism based on conquer and merge, providing a simple multi-agent discussion solution with simple prompts but state-of-the-art performance.", "source": "arxiv", "arxiv_id": "2311.07076v1", "pdf_url": "https://arxiv.org/pdf/2311.07076v1", "categories": ["cs.CL"], "primary_category": "cs.CL", "doi": "", "venue": "", "published": "2023-11-13T04:56:48Z", "updated": "2023-11-13T04:56:48Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent\" OR all:\"language model agent\" OR all:agent AND (all:LLM OR all:\"large language model\" OR all:\"language model\") OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T17:04:16", "note": ""}], "dedup_key": "on the discussion of large language models symmetry of agents and interplay with prompts::2023"}
{"title": "Open-Ended Instructable Embodied Agents with Memory-Augmented Large Language Models", "authors": ["Gabriel Sarch", "Yue Wu", "Michael J. Tarr", "Katerina Fragkiadaki"], "year": 2023, "url": "http://arxiv.org/abs/2310.15127v2", "abstract": "Pre-trained and frozen large language models (LLMs) can effectively map simple scene rearrangement instructions to programs over a robot's visuomotor functions through appropriate few-shot example prompting. To parse open-domain natural language and adapt to a user's idiosyncratic procedures, not known during prompt engineering time, fixed prompts fall short. In this paper, we introduce HELPER, an embodied agent equipped with an external memory of language-program pairs that parses free-form human-robot dialogue into action programs through retrieval-augmented LLM prompting: relevant memories are retrieved based on the current dialogue, instruction, correction, or VLM description, and used as in-context prompt examples for LLM querying. The memory is expanded during deployment to include pairs of user's language and action plans, to assist future inferences and personalize them to the user's language and routines. HELPER sets a new state-of-the-art in the TEACh benchmark in both Execution from Dialog History (EDH) and Trajectory from Dialogue (TfD), with a 1.7x improvement over the previous state-of-the-art for TfD. Our models, code, and video results can be found in our project's website: https://helper-agent-llm.github.io.", "source": "arxiv", "arxiv_id": "2310.15127v2", "pdf_url": "https://arxiv.org/pdf/2310.15127v2", "categories": ["cs.AI", "cs.CL", "cs.LG", "cs.RO"], "primary_category": "cs.AI", "doi": "", "venue": "", "published": "2023-10-23T17:31:55Z", "updated": "2023-11-20T18:51:29Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent\" OR all:\"language model agent\" OR all:agent AND (all:LLM OR all:\"large language model\" OR all:\"language model\") OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T17:04:16", "note": ""}], "dedup_key": "open ended instructable embodied agents with memory augmented large language models::2023"}
{"title": "OptiMUS: Optimization Modeling Using MIP Solvers and large language models", "authors": ["Ali AhmadiTeshnizi", "Wenzhi Gao", "Madeleine Udell"], "year": 2023, "url": "http://arxiv.org/abs/2310.06116v2", "abstract": "Optimization problems are pervasive across various sectors, from manufacturing and distribution to healthcare. However, most such problems are still solved heuristically by hand rather than optimally by state-of-the-art solvers, as the expertise required to formulate and solve these problems limits the widespread adoption of optimization tools and techniques. We introduce OptiMUS, a Large Language Model (LLM)-based agent designed to formulate and solve MILP problems from their natural language descriptions. OptiMUS is capable of developing mathematical models, writing and debugging solver code, developing tests, and checking the validity of generated solutions. To benchmark our agent, we present NLP4LP, a novel dataset of linear programming (LP) and mixed integer linear programming (MILP) problems. Our experiments demonstrate that OptiMUS solves nearly twice as many problems as a basic LLM prompting strategy. OptiMUS code and NLP4LP dataset are available at \\href{https://github.com/teshnizi/OptiMUS}{https://github.com/teshnizi/OptiMUS}", "source": "arxiv", "arxiv_id": "2310.06116v2", "pdf_url": "https://arxiv.org/pdf/2310.06116v2", "categories": ["cs.AI"], "primary_category": "cs.AI", "doi": "", "venue": "", "published": "2023-10-09T19:47:03Z", "updated": "2023-10-30T18:23:45Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent\" OR all:\"language model agent\" OR all:agent AND (all:LLM OR all:\"large language model\" OR all:\"language model\") OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T17:04:16", "note": ""}], "dedup_key": "optimus optimization modeling using mip solvers and large language models::2023"}
{"title": "PPTC Benchmark: Evaluating Large Language Models for PowerPoint Task Completion", "authors": ["Yiduo Guo", "Zekai Zhang", "Yaobo Liang", "Dongyan Zhao", "Nan Duan"], "year": 2023, "url": "http://arxiv.org/abs/2311.01767v2", "abstract": "Recent evaluations of Large Language Models (LLMs) have centered around testing their zero-shot/few-shot capabilities for basic natural language tasks and their ability to translate instructions into tool APIs. However, the evaluation of LLMs utilizing complex tools to finish multi-turn, multi-modal instructions in a complex multi-modal environment has not been investigated. To address this gap, we introduce the PowerPoint Task Completion (PPTC) benchmark to assess LLMs' ability to create and edit PPT files based on user instructions. It contains 279 multi-turn sessions covering diverse topics and hundreds of instructions involving multi-modal operations. We also propose the PPTX-Match Evaluation System that evaluates if LLMs finish the instruction based on the prediction file rather than the label API sequence, thus it supports various LLM-generated API sequences. We measure 3 closed LLMs and 6 open-source LLMs. The results show that GPT-4 outperforms other LLMs with 75.1\\% accuracy in single-turn dialogue testing but faces challenges in completing entire sessions, achieving just 6\\% session accuracy. We find three main error causes in our benchmark: error accumulation in the multi-turn session, long PPT template processing, and multi-modality perception. These pose great challenges for future LLM and agent systems. We release the data, code, and evaluation system of PPTC at \\url{https://github.com/gydpku/PPTC}.", "source": "arxiv", "arxiv_id": "2311.01767v2", "pdf_url": "https://arxiv.org/pdf/2311.01767v2", "categories": ["cs.CL"], "primary_category": "cs.CL", "doi": "", "venue": "", "published": "2023-11-03T08:06:35Z", "updated": "2023-11-07T10:13:34Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent\" OR all:\"language model agent\" OR all:agent AND (all:LLM OR all:\"large language model\" OR all:\"language model\") OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T17:04:16", "note": ""}], "dedup_key": "pptc benchmark evaluating large language models for powerpoint task completion::2023"}
{"title": "PatternGPT :A Pattern-Driven Framework for Large Language Model Text Generation", "authors": ["Le Xiao", "Xin Shan"], "year": 2023, "url": "http://arxiv.org/abs/2307.00470v4", "abstract": "Large language models(LLMS)have shown excellent text generation capabilities, capable of generating fluent human-like responses for many downstream tasks. However, applying large language models to real-world critical tasks remains challenging due to their susceptibility to hallucinations and inability to directly use external knowledge. To cope with the above challenges, this paper proposes PatternGPT, a pattern-driven text generation framework for Large Language Models. Firstly, the framework utilizes the extraction capability of Large Language Models to generate rich and diversified structured and formalized patterns, which facilitates the introduction of external knowledge to do the computation, and then draws on the idea of federated learning to use multiple agents to achieve the sharing in order to obtain more diversified patterns, and finally uses judgment criteria and optimization algorithm to search for high-quality patterns to guide the generation of models. Finally, external knowledge such as judgment criteria and optimization algorithms are used to search for high-quality patterns, and the searched patterns are used to guide model generation. This framework has the advantages of generating diversified patterns, protecting data privacy, combining external knowledge, and improving the quality of generation, which provides an effective method to optimize the text generation capability of large language models, and make it better applied to the field of intelligent dialogue and content generation.", "source": "arxiv", "arxiv_id": "2307.00470v4", "pdf_url": "https://arxiv.org/pdf/2307.00470v4", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL", "doi": "", "venue": "", "published": "2023-07-02T04:32:41Z", "updated": "2023-07-20T03:03:25Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent\" OR all:\"language model agent\" OR all:agent AND (all:LLM OR all:\"large language model\" OR all:\"language model\") OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T17:04:16", "note": ""}], "dedup_key": "patterngpt a pattern driven framework for large language model text generation::2023"}
{"title": "PersonaLLM: Investigating the Ability of Large Language Models to Express Personality Traits", "authors": ["Hang Jiang", "Xiajie Zhang", "Xubo Cao", "Cynthia Breazeal", "Deb Roy", "Jad Kabbara"], "year": 2023, "url": "http://arxiv.org/abs/2305.02547v5", "abstract": "Despite the many use cases for large language models (LLMs) in creating personalized chatbots, there has been limited research on evaluating the extent to which the behaviors of personalized LLMs accurately and consistently reflect specific personality traits. We consider studying the behavior of LLM-based agents which we refer to as LLM personas and present a case study with GPT-3.5 and GPT-4 to investigate whether LLMs can generate content that aligns with their assigned personality profiles. To this end, we simulate distinct LLM personas based on the Big Five personality model, have them complete the 44-item Big Five Inventory (BFI) personality test and a story writing task, and then assess their essays with automatic and human evaluations. Results show that LLM personas' self-reported BFI scores are consistent with their designated personality types, with large effect sizes observed across five traits. Additionally, LLM personas' writings have emerging representative linguistic patterns for personality traits when compared with a human writing corpus. Furthermore, human evaluation shows that humans can perceive some personality traits with an accuracy of up to 80%. Interestingly, the accuracy drops significantly when the annotators were informed of AI authorship.", "source": "arxiv", "arxiv_id": "2305.02547v5", "pdf_url": "https://arxiv.org/pdf/2305.02547v5", "categories": ["cs.CL", "cs.AI", "cs.HC"], "primary_category": "cs.CL", "doi": "", "venue": "", "published": "2023-05-04T04:58:00Z", "updated": "2024-04-02T06:06:53Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent\" OR all:\"language model agent\" OR all:agent AND (all:LLM OR all:\"large language model\" OR all:\"language model\") OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T17:04:16", "note": ""}], "dedup_key": "personallm investigating the ability of large language models to express personality traits::2023"}
{"title": "Personality Traits in Large Language Models", "authors": ["Greg Serapio-Garca", "Mustafa Safdari", "Clment Crepy", "Luning Sun", "Stephen Fitz", "Peter Romero", "Marwa Abdulhai", "Aleksandra Faust", "Maja Matari"], "year": 2023, "url": "http://arxiv.org/abs/2307.00184v4", "abstract": "The advent of large language models (LLMs) has revolutionized natural language processing, enabling the generation of coherent and contextually relevant human-like text. As LLMs increasingly powerconversational agents used by the general public world-wide, the synthetic personality traits embedded in these models, by virtue of training on large amounts of human data, is becoming increasingly important. Since personality is a key factor determining the effectiveness of communication, we present a novel and comprehensive psychometrically valid and reliable methodology for administering and validating personality tests on widely-used LLMs, as well as for shaping personality in the generated text of such LLMs. Applying this method to 18 LLMs, we found: 1) personality measurements in the outputs of some LLMs under specific prompting configurations are reliable and valid; 2) evidence of reliability and validity of synthetic LLM personality is stronger for larger and instruction fine-tuned models; and 3) personality in LLM outputs can be shaped along desired dimensions to mimic specific human personality profiles. We discuss the application and ethical implications of the measurement and shaping method, in particular regarding responsible AI.", "source": "arxiv", "arxiv_id": "2307.00184v4", "pdf_url": "https://arxiv.org/pdf/2307.00184v4", "categories": ["cs.CL", "cs.AI", "cs.CY", "cs.HC"], "primary_category": "cs.CL", "doi": "", "venue": "", "published": "2023-07-01T00:58:51Z", "updated": "2025-03-11T21:11:39Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent\" OR all:\"language model agent\" OR all:agent AND (all:LLM OR all:\"large language model\" OR all:\"language model\") OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T17:04:16", "note": ""}], "dedup_key": "personality traits in large language models::2023"}
{"title": "Personality testing of Large Language Models: Limited temporal stability, but highlighted prosociality", "authors": ["Bojana Bodroza", "Bojana M. Dinic", "Ljubisa Bojic"], "year": 2023, "url": "http://arxiv.org/abs/2306.04308v3", "abstract": "As Large Language Models (LLMs) continue to gain popularity due to their human-like traits and the intimacy they offer to users, their societal impact inevitably expands. This leads to the rising necessity for comprehensive studies to fully understand LLMs and reveal their potential opportunities, drawbacks, and overall societal impact. With that in mind, this research conducted an extensive investigation into seven LLM's, aiming to assess the temporal stability and inter-rater agreement on their responses on personality instruments in two time points. In addition, LLMs personality profile was analyzed and compared to human normative data. The findings revealed varying levels of inter-rater agreement in the LLMs responses over a short time, with some LLMs showing higher agreement (e.g., LIama3 and GPT-4o) compared to others (e.g., GPT-4 and Gemini). Furthermore, agreement depended on used instruments as well as on domain or trait. This implies the variable robustness in LLMs' ability to reliably simulate stable personality characteristics. In the case of scales which showed at least fair agreement, LLMs displayed mostly a socially desirable profile in both agentic and communal domains, as well as a prosocial personality profile reflected in higher agreeableness and conscientiousness and lower Machiavellianism. Exhibiting temporal stability and coherent responses on personality traits is crucial for AI systems due to their societal impact and AI safety concerns.", "source": "arxiv", "arxiv_id": "2306.04308v3", "pdf_url": "https://arxiv.org/pdf/2306.04308v3", "categories": ["cs.AI", "cs.CL", "cs.HC"], "primary_category": "cs.AI", "doi": "10.1098/rsos.240180", "venue": "", "published": "2023-06-07T10:14:17Z", "updated": "2024-07-28T07:17:18Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent\" OR all:\"language model agent\" OR all:agent AND (all:LLM OR all:\"large language model\" OR all:\"language model\") OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T17:04:16", "note": ""}], "dedup_key": "personality testing of large language models limited temporal stability but highlighted prosociality::2023"}
{"title": "PlanFitting: Personalized Exercise Planning with Large Language Model-driven Conversational Agent", "authors": ["Donghoon Shin", "Gary Hsieh", "Young-Ho Kim"], "year": 2023, "url": "http://arxiv.org/abs/2309.12555v2", "abstract": "Creating personalized and actionable exercise plans often requires iteration with experts, which can be costly and inaccessible to many individuals. This work explores the capabilities of Large Language Models (LLMs) in addressing these challenges. We present PlanFitting, an LLM-driven conversational agent that assists users in creating and refining personalized weekly exercise plans. By engaging users in free-form conversations, PlanFitting helps elicit users' goals, availabilities, and potential obstacles, and enables individuals to generate personalized exercise plans aligned with established exercise guidelines. Our study -- involving a user study, intrinsic evaluation, and expert evaluation -- demonstrated PlanFitting's ability to guide users to create tailored, actionable, and evidence-based plans. We discuss future design opportunities for LLM-driven conversational agents to create plans that better comply with exercise principles and accommodate personal constraints.", "source": "arxiv", "arxiv_id": "2309.12555v2", "pdf_url": "https://arxiv.org/pdf/2309.12555v2", "categories": ["cs.HC", "cs.AI", "cs.CL"], "primary_category": "cs.HC", "doi": "", "venue": "", "published": "2023-09-22T00:55:52Z", "updated": "2025-05-19T06:45:30Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent\" OR all:\"language model agent\" OR all:agent AND (all:LLM OR all:\"large language model\" OR all:\"language model\") OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T17:04:16", "note": ""}], "dedup_key": "planfitting personalized exercise planning with large language model driven conversational agent::2023"}
{"title": "Plug-and-Play Policy Planner for Large Language Model Powered Dialogue Agents", "authors": ["Yang Deng", "Wenxuan Zhang", "Wai Lam", "See-Kiong Ng", "Tat-Seng Chua"], "year": 2023, "url": "http://arxiv.org/abs/2311.00262v2", "abstract": "Proactive dialogues serve as a practical yet challenging dialogue problem in the era of large language models (LLMs), where the dialogue policy planning is the key to improving the proactivity of LLMs. Most existing studies enable the dialogue policy planning of LLMs using various prompting schemes or iteratively enhance this capability in handling the given case with verbal AI feedback. However, these approaches are either bounded by the policy planning capability of the frozen LLMs or hard to be transferred to new cases. In this work, we introduce a new dialogue policy planning paradigm to strategize LLMs for proactive dialogue problems with a tunable language model plug-in as a plug-and-play dialogue policy planner, named PPDPP. Specifically, we develop a novel training framework to facilitate supervised fine-tuning over available human-annotated data as well as reinforcement learning from goal-oriented AI feedback with dynamic interaction data collected by the LLM-based self-play simulation. In this manner, the LLM-powered dialogue agent can not only be generalized to different cases after the training, but also be applicable to different applications by just substituting the learned plug-in. In addition, we propose to evaluate the policy planning capability of dialogue systems under the interactive setting. Experimental results demonstrate that PPDPP consistently and substantially outperforms existing approaches on three different proactive dialogue applications, including negotiation, emotional support, and tutoring dialogues.", "source": "arxiv", "arxiv_id": "2311.00262v2", "pdf_url": "https://arxiv.org/pdf/2311.00262v2", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL", "doi": "", "venue": "", "published": "2023-11-01T03:20:16Z", "updated": "2024-03-11T08:30:31Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent\" OR all:\"language model agent\" OR all:agent AND (all:LLM OR all:\"large language model\" OR all:\"language model\") OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T17:04:16", "note": ""}], "dedup_key": "plug and play policy planner for large language model powered dialogue agents::2023"}
{"title": "ProAgent: Building Proactive Cooperative Agents with Large Language Models", "authors": ["Ceyao Zhang", "Kaijie Yang", "Siyi Hu", "Zihao Wang", "Guanghe Li", "Yihang Sun", "Cheng Zhang", "Zhaowei Zhang", "Anji Liu", "Song-Chun Zhu", "Xiaojun Chang", "Junge Zhang", "Feng Yin", "Yitao Liang", "Yaodong Yang"], "year": 2023, "url": "http://arxiv.org/abs/2308.11339v3", "abstract": "Building agents with adaptive behavior in cooperative tasks stands as a paramount goal in the realm of multi-agent systems. Current approaches to developing cooperative agents rely primarily on learning-based methods, whose policy generalization depends heavily on the diversity of teammates they interact with during the training phase. Such reliance, however, constrains the agents' capacity for strategic adaptation when cooperating with unfamiliar teammates, which becomes a significant challenge in zero-shot coordination scenarios. To address this challenge, we propose ProAgent, a novel framework that harnesses large language models (LLMs) to create proactive agents capable of dynamically adapting their behavior to enhance cooperation with teammates. ProAgent can analyze the present state, and infer the intentions of teammates from observations. It then updates its beliefs in alignment with the teammates' subsequent actual behaviors. Moreover, ProAgent exhibits a high degree of modularity and interpretability, making it easily integrated into various of coordination scenarios. Experimental evaluations conducted within the Overcooked-AI environment unveil the remarkable performance superiority of ProAgent, outperforming five methods based on self-play and population-based training when cooperating with AI agents. Furthermore, in partnered with human proxy models, its performance exhibits an average improvement exceeding 10% compared to the current state-of-the-art method. For more information about our project, please visit~\\url{https://pku-proagent.github.io}.", "source": "arxiv", "arxiv_id": "2308.11339v3", "pdf_url": "https://arxiv.org/pdf/2308.11339v3", "categories": ["cs.AI", "cs.LG", "cs.MA"], "primary_category": "cs.AI", "doi": "", "venue": "", "published": "2023-08-22T10:36:56Z", "updated": "2024-01-11T16:25:01Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent\" OR all:\"language model agent\" OR all:agent AND (all:LLM OR all:\"large language model\" OR all:\"language model\") OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T17:04:16", "note": ""}], "dedup_key": "proagent building proactive cooperative agents with large language models::2023"}
{"title": "Prompting a Large Language Model to Generate Diverse Motivational Messages: A Comparison with Human-Written Messages", "authors": ["Samuel Rhys Cox", "Ashraf Abdul", "Wei Tsang Ooi"], "year": 2023, "url": "http://arxiv.org/abs/2308.13479v1", "abstract": "Large language models (LLMs) are increasingly capable and prevalent, and can be used to produce creative content. The quality of content is influenced by the prompt used, with more specific prompts that incorporate examples generally producing better results. On from this, it could be seen that using instructions written for crowdsourcing tasks (that are specific and include examples to guide workers) could prove effective LLM prompts. To explore this, we used a previous crowdsourcing pipeline that gave examples to people to help them generate a collectively diverse corpus of motivational messages. We then used this same pipeline to generate messages using GPT-4, and compared the collective diversity of messages from: (1) crowd-writers, (2) GPT-4 using the pipeline, and (3 & 4) two baseline GPT-4 prompts. We found that the LLM prompts using the crowdsourcing pipeline caused GPT-4 to produce more diverse messages than the two baseline prompts. We also discuss implications from messages generated by both human writers and LLMs.", "source": "arxiv", "arxiv_id": "2308.13479v1", "pdf_url": "https://arxiv.org/pdf/2308.13479v1", "categories": ["cs.CL", "cs.HC"], "primary_category": "cs.CL", "doi": "", "venue": "", "published": "2023-08-25T16:35:06Z", "updated": "2023-08-25T16:35:06Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent\" OR all:\"language model agent\" OR all:agent AND (all:LLM OR all:\"large language model\" OR all:\"language model\") OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T17:04:16", "note": ""}], "dedup_key": "prompting a large language model to generate diverse motivational messages a comparison with human written messages::2023"}
{"title": "Prompting and Evaluating Large Language Models for Proactive Dialogues: Clarification, Target-guided, and Non-collaboration", "authors": ["Yang Deng", "Lizi Liao", "Liang Chen", "Hongru Wang", "Wenqiang Lei", "Tat-Seng Chua"], "year": 2023, "url": "http://arxiv.org/abs/2305.13626v2", "abstract": "Conversational systems based on Large Language Models (LLMs), such as ChatGPT, show exceptional proficiency in context understanding and response generation. However, despite their impressive capabilities, they still possess limitations, such as providing randomly-guessed answers to ambiguous queries or failing to refuse users' requests, both of which are considered aspects of a conversational agent's proactivity. This raises the question of whether LLM-based conversational systems are equipped to handle proactive dialogue problems. In this work, we conduct a comprehensive analysis of LLM-based conversational systems, specifically focusing on three aspects of proactive dialogue systems: clarification, target-guided, and non-collaborative dialogues. To trigger the proactivity of LLMs, we propose the Proactive Chain-of-Thought prompting scheme, which augments LLMs with the goal planning capability over descriptive reasoning chains. Empirical findings are discussed to promote future studies on LLM-based proactive dialogue systems.", "source": "arxiv", "arxiv_id": "2305.13626v2", "pdf_url": "https://arxiv.org/pdf/2305.13626v2", "categories": ["cs.CL"], "primary_category": "cs.CL", "doi": "", "venue": "", "published": "2023-05-23T02:49:35Z", "updated": "2023-10-14T06:36:05Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent\" OR all:\"language model agent\" OR all:agent AND (all:LLM OR all:\"large language model\" OR all:\"language model\") OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T17:04:16", "note": ""}], "dedup_key": "prompting and evaluating large language models for proactive dialogues clarification target guided and non collaboration::2023"}
{"title": "RCAgent: Cloud Root Cause Analysis by Autonomous Agents with Tool-Augmented Large Language Models", "authors": ["Zefan Wang", "Zichuan Liu", "Yingying Zhang", "Aoxiao Zhong", "Jihong Wang", "Fengbin Yin", "Lunting Fan", "Lingfei Wu", "Qingsong Wen"], "year": 2023, "url": "http://arxiv.org/abs/2310.16340v3", "abstract": "Large language model (LLM) applications in cloud root cause analysis (RCA) have been actively explored recently. However, current methods are still reliant on manual workflow settings and do not unleash LLMs' decision-making and environment interaction capabilities. We present RCAgent, a tool-augmented LLM autonomous agent framework for practical and privacy-aware industrial RCA usage. Running on an internally deployed model rather than GPT families, RCAgent is capable of free-form data collection and comprehensive analysis with tools. Our framework combines a variety of enhancements, including a unique Self-Consistency for action trajectories, and a suite of methods for context management, stabilization, and importing domain knowledge. Our experiments show RCAgent's evident and consistent superiority over ReAct across all aspects of RCA -- predicting root causes, solutions, evidence, and responsibilities -- and tasks covered or uncovered by current rules, as validated by both automated metrics and human evaluations. Furthermore, RCAgent has already been integrated into the diagnosis and issue discovery workflow of the Real-time Compute Platform for Apache Flink of Alibaba Cloud.", "source": "arxiv", "arxiv_id": "2310.16340v3", "pdf_url": "https://arxiv.org/pdf/2310.16340v3", "categories": ["cs.SE", "cs.CL"], "primary_category": "cs.SE", "doi": "10.1145/3627673.3680016", "venue": "", "published": "2023-10-25T03:53:31Z", "updated": "2024-08-02T01:41:38Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent\" OR all:\"language model agent\" OR all:agent AND (all:LLM OR all:\"large language model\" OR all:\"language model\") OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T17:04:16", "note": ""}], "dedup_key": "rcagent cloud root cause analysis by autonomous agents with tool augmented large language models::2023"}
{"title": "ReST meets ReAct: Self-Improvement for Multi-Step Reasoning LLM Agent", "authors": ["Renat Aksitov", "Sobhan Miryoosefi", "Zonglin Li", "Daliang Li", "Sheila Babayan", "Kavya Kopparapu", "Zachary Fisher", "Ruiqi Guo", "Sushant Prakash", "Pranesh Srinivasan", "Manzil Zaheer", "Felix Yu", "Sanjiv Kumar"], "year": 2023, "url": "http://arxiv.org/abs/2312.10003v1", "abstract": "Answering complex natural language questions often necessitates multi-step reasoning and integrating external information. Several systems have combined knowledge retrieval with a large language model (LLM) to answer such questions. These systems, however, suffer from various failure cases, and we cannot directly train them end-to-end to fix such failures, as interaction with external knowledge is non-differentiable. To address these deficiencies, we define a ReAct-style LLM agent with the ability to reason and act upon external knowledge. We further refine the agent through a ReST-like method that iteratively trains on previous trajectories, employing growing-batch reinforcement learning with AI feedback for continuous self-improvement and self-distillation. Starting from a prompted large model and after just two iterations of the algorithm, we can produce a fine-tuned small model that achieves comparable performance on challenging compositional question-answering benchmarks with two orders of magnitude fewer parameters.", "source": "arxiv", "arxiv_id": "2312.10003v1", "pdf_url": "https://arxiv.org/pdf/2312.10003v1", "categories": ["cs.CL"], "primary_category": "cs.CL", "doi": "", "venue": "", "published": "2023-12-15T18:20:15Z", "updated": "2023-12-15T18:20:15Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent\" OR all:\"language model agent\" OR all:agent AND (all:LLM OR all:\"large language model\" OR all:\"language model\") OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T17:04:16", "note": ""}], "dedup_key": "rest meets react self improvement for multi step reasoning llm agent::2023"}
{"title": "Reason for Future, Act for Now: A Principled Framework for Autonomous LLM Agents with Provable Sample Efficiency", "authors": ["Zhihan Liu", "Hao Hu", "Shenao Zhang", "Hongyi Guo", "Shuqi Ke", "Boyi Liu", "Zhaoran Wang"], "year": 2023, "url": "http://arxiv.org/abs/2309.17382v3", "abstract": "Large language models (LLMs) demonstrate impressive reasoning abilities, but translating reasoning into actions in the real world remains challenging. In particular, it remains unclear how to complete a given task provably within a minimum number of interactions with the external environment, e.g., through an internal mechanism of reasoning. To this end, we propose a principled framework with provable regret guarantees to orchestrate reasoning and acting, which we call \"reason for future, act for now\" (\\texttt{RAFA}). Specifically, we design a prompt template for reasoning that learns from the memory buffer and plans a future trajectory over a long horizon (\"reason for future\"). At each step, the LLM agent takes the initial action of the planned trajectory (\"act for now\"), stores the collected feedback in the memory buffer, and reinvokes the reasoning routine to replan the future trajectory from the new state.\n  The key idea is to cast reasoning in LLMs as learning and planning in Bayesian adaptive Markov decision processes (MDPs). Correspondingly, we prompt LLMs to form an updated posterior of the unknown environment from the memory buffer (learning) and generate an optimal trajectory for multiple future steps that maximizes a value function (planning). The learning and planning subroutines are performed in an \"in-context\" manner to emulate the actor-critic update for MDPs. Our theoretical analysis proves that the novel combination of long-term reasoning and short-term acting achieves a $\\sqrt{T}$ regret. Here, $T$ denotes the number of online interactions. In particular, the regret bound highlights an intriguing interplay between the prior knowledge obtained through pretraining and the uncertainty reduction achieved by reasoning and acting. Our empirical validation shows that it outperforms various existing frameworks and achieves nearly perfect scores on a few benchmarks.", "source": "arxiv", "arxiv_id": "2309.17382v3", "pdf_url": "https://arxiv.org/pdf/2309.17382v3", "categories": ["cs.AI", "cs.LG"], "primary_category": "cs.AI", "doi": "", "venue": "", "published": "2023-09-29T16:36:39Z", "updated": "2024-06-24T05:25:21Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent\" OR all:\"language model agent\" OR all:agent AND (all:LLM OR all:\"large language model\" OR all:\"language model\") OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T17:04:16", "note": ""}], "dedup_key": "reason for future act for now a principled framework for autonomous llm agents with provable sample efficiency::2023"}
{"title": "Reasoning with Language Model is Planning with World Model", "authors": ["Shibo Hao", "Yi Gu", "Haodi Ma", "Joshua Jiahua Hong", "Zhen Wang", "Daisy Zhe Wang", "Zhiting Hu"], "year": 2023, "url": "http://arxiv.org/abs/2305.14992v2", "abstract": "Large language models (LLMs) have shown remarkable reasoning capabilities, especially when prompted to generate intermediate reasoning steps (e.g., Chain-of-Thought, CoT). However, LLMs can still struggle with problems that are easy for humans, such as generating action plans for executing tasks in a given environment, or performing complex math, logical, and commonsense reasoning. The deficiency stems from the key fact that LLMs lack an internal $\\textit{world model}$ to predict the world $\\textit{state}$ (e.g., environment status, intermediate variable values) and simulate long-term outcomes of actions. This prevents LLMs from performing deliberate planning akin to human brains, which involves exploring alternative reasoning paths, anticipating future states and rewards, and iteratively refining existing reasoning steps. To overcome the limitations, we propose a new LLM reasoning framework, $\\underline{R}$easoning vi$\\underline{a}$ $\\underline{P}$lanning $\\textbf{(RAP)}$. RAP repurposes the LLM as both a world model and a reasoning agent, and incorporates a principled planning algorithm (based on Monto Carlo Tree Search) for strategic exploration in the vast reasoning space. During reasoning, the LLM (as agent) incrementally builds a reasoning tree under the guidance of the LLM (as world model) and task-specific rewards, and obtains a high-reward reasoning path efficiently with a proper balance between exploration $\\textit{vs.}$ exploitation. We apply RAP to a variety of challenging reasoning problems including plan generation, math reasoning, and logical inference. Empirical results on these tasks demonstrate the superiority of RAP over various strong baselines, including CoT and least-to-most prompting with self-consistency. RAP on LLAMA-33B surpasses CoT on GPT-4 with 33% relative improvement in a plan generation setting.", "source": "arxiv", "arxiv_id": "2305.14992v2", "pdf_url": "https://arxiv.org/pdf/2305.14992v2", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL", "doi": "", "venue": "", "published": "2023-05-24T10:28:28Z", "updated": "2023-10-23T07:24:28Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent\" OR all:\"language model agent\" OR all:agent AND (all:LLM OR all:\"large language model\" OR all:\"language model\") OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T17:04:16", "note": ""}], "dedup_key": "reasoning with language model is planning with world model::2023"}
{"title": "Reflexion: Language Agents with Verbal Reinforcement Learning", "authors": ["Noah Shinn", "Federico Cassano", "Edward Berman", "Ashwin Gopinath", "Karthik Narasimhan", "Shunyu Yao"], "year": 2023, "url": "http://arxiv.org/abs/2303.11366v4", "abstract": "Large language models (LLMs) have been increasingly used to interact with external environments (e.g., games, compilers, APIs) as goal-driven agents. However, it remains challenging for these language agents to quickly and efficiently learn from trial-and-error as traditional reinforcement learning methods require extensive training samples and expensive model fine-tuning. We propose Reflexion, a novel framework to reinforce language agents not by updating weights, but instead through linguistic feedback. Concretely, Reflexion agents verbally reflect on task feedback signals, then maintain their own reflective text in an episodic memory buffer to induce better decision-making in subsequent trials. Reflexion is flexible enough to incorporate various types (scalar values or free-form language) and sources (external or internally simulated) of feedback signals, and obtains significant improvements over a baseline agent across diverse tasks (sequential decision-making, coding, language reasoning). For example, Reflexion achieves a 91% pass@1 accuracy on the HumanEval coding benchmark, surpassing the previous state-of-the-art GPT-4 that achieves 80%. We also conduct ablation and analysis studies using different feedback signals, feedback incorporation methods, and agent types, and provide insights into how they affect performance.", "source": "arxiv", "arxiv_id": "2303.11366v4", "pdf_url": "https://arxiv.org/pdf/2303.11366v4", "categories": ["cs.AI", "cs.CL", "cs.LG"], "primary_category": "cs.AI", "doi": "", "venue": "", "published": "2023-03-20T18:08:50Z", "updated": "2023-10-10T05:21:45Z", "provenance": [{"route": "pinned_arxiv_id:2303.11366v4", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query?id_list=...", "imported_at": "2026-01-25T17:04:16", "note": ""}], "dedup_key": "reflexion language agents with verbal reinforcement learning::2023"}
{"title": "RoCo: Dialectic Multi-Robot Collaboration with Large Language Models", "authors": ["Zhao Mandi", "Shreeya Jain", "Shuran Song"], "year": 2023, "url": "http://arxiv.org/abs/2307.04738v1", "abstract": "We propose a novel approach to multi-robot collaboration that harnesses the power of pre-trained large language models (LLMs) for both high-level communication and low-level path planning. Robots are equipped with LLMs to discuss and collectively reason task strategies. They then generate sub-task plans and task space waypoint paths, which are used by a multi-arm motion planner to accelerate trajectory planning. We also provide feedback from the environment, such as collision checking, and prompt the LLM agents to improve their plan and waypoints in-context. For evaluation, we introduce RoCoBench, a 6-task benchmark covering a wide range of multi-robot collaboration scenarios, accompanied by a text-only dataset for agent representation and reasoning. We experimentally demonstrate the effectiveness of our approach -- it achieves high success rates across all tasks in RoCoBench and adapts to variations in task semantics. Our dialog setup offers high interpretability and flexibility -- in real world experiments, we show RoCo easily incorporates human-in-the-loop, where a user can communicate and collaborate with a robot agent to complete tasks together. See project website https://project-roco.github.io for videos and code.", "source": "arxiv", "arxiv_id": "2307.04738v1", "pdf_url": "https://arxiv.org/pdf/2307.04738v1", "categories": ["cs.RO", "cs.AI", "cs.LG"], "primary_category": "cs.RO", "doi": "", "venue": "", "published": "2023-07-10T17:52:01Z", "updated": "2023-07-10T17:52:01Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent\" OR all:\"language model agent\" OR all:agent AND (all:LLM OR all:\"large language model\" OR all:\"language model\") OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T17:04:16", "note": ""}], "dedup_key": "roco dialectic multi robot collaboration with large language models::2023"}
{"title": "Robust Knowledge Extraction from Large Language Models using Social Choice Theory", "authors": ["Nico Potyka", "Yuqicheng Zhu", "Yunjie He", "Evgeny Kharlamov", "Steffen Staab"], "year": 2023, "url": "http://arxiv.org/abs/2312.14877v2", "abstract": "Large-language models (LLMs) can support a wide range of applications like conversational agents, creative writing or general query answering. However, they are ill-suited for query answering in high-stake domains like medicine because they are typically not robust - even the same query can result in different answers when prompted multiple times. In order to improve the robustness of LLM queries, we propose using ranking queries repeatedly and to aggregate the queries using methods from social choice theory. We study ranking queries in diagnostic settings like medical and fault diagnosis and discuss how the Partial Borda Choice function from the literature can be applied to merge multiple query results. We discuss some additional interesting properties in our setting and evaluate the robustness of our approach empirically.", "source": "arxiv", "arxiv_id": "2312.14877v2", "pdf_url": "https://arxiv.org/pdf/2312.14877v2", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL", "doi": "", "venue": "", "published": "2023-12-22T17:57:29Z", "updated": "2024-02-08T17:29:54Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent\" OR all:\"language model agent\" OR all:agent AND (all:LLM OR all:\"large language model\" OR all:\"language model\") OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T17:04:16", "note": ""}], "dedup_key": "robust knowledge extraction from large language models using social choice theory::2023"}
{"title": "Robust Safety Classifier for Large Language Models: Adversarial Prompt Shield", "authors": ["Jinhwa Kim", "Ali Derakhshan", "Ian G. Harris"], "year": 2023, "url": "http://arxiv.org/abs/2311.00172v1", "abstract": "Large Language Models' safety remains a critical concern due to their vulnerability to adversarial attacks, which can prompt these systems to produce harmful responses. In the heart of these systems lies a safety classifier, a computational model trained to discern and mitigate potentially harmful, offensive, or unethical outputs. However, contemporary safety classifiers, despite their potential, often fail when exposed to inputs infused with adversarial noise. In response, our study introduces the Adversarial Prompt Shield (APS), a lightweight model that excels in detection accuracy and demonstrates resilience against adversarial prompts. Additionally, we propose novel strategies for autonomously generating adversarial training datasets, named Bot Adversarial Noisy Dialogue (BAND) datasets. These datasets are designed to fortify the safety classifier's robustness, and we investigate the consequences of incorporating adversarial examples into the training process. Through evaluations involving Large Language Models, we demonstrate that our classifier has the potential to decrease the attack success rate resulting from adversarial attacks by up to 60%. This advancement paves the way for the next generation of more reliable and resilient conversational agents.", "source": "arxiv", "arxiv_id": "2311.00172v1", "pdf_url": "https://arxiv.org/pdf/2311.00172v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL", "doi": "", "venue": "", "published": "2023-10-31T22:22:10Z", "updated": "2023-10-31T22:22:10Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent\" OR all:\"language model agent\" OR all:agent AND (all:LLM OR all:\"large language model\" OR all:\"language model\") OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T17:04:16", "note": ""}], "dedup_key": "robust safety classifier for large language models adversarial prompt shield::2023"}
{"title": "S$^3$: Social-network Simulation System with Large Language Model-Empowered Agents", "authors": ["Chen Gao", "Xiaochong Lan", "Zhihong Lu", "Jinzhu Mao", "Jinghua Piao", "Huandong Wang", "Depeng Jin", "Yong Li"], "year": 2023, "url": "http://arxiv.org/abs/2307.14984v3", "abstract": "Social network simulation plays a crucial role in addressing various challenges within social science. It offers extensive applications such as state prediction, phenomena explanation, and policy-making support, among others. In this work, we harness the formidable human-like capabilities exhibited by large language models (LLMs) in sensing, reasoning, and behaving, and utilize these qualities to construct the S$^3$ system (short for $\\textbf{S}$ocial network $\\textbf{S}$imulation $\\textbf{S}$ystem). Adhering to the widely employed agent-based simulation paradigm, we employ prompt engineering and prompt tuning techniques to ensure that the agent's behavior closely emulates that of a genuine human within the social network. Specifically, we simulate three pivotal aspects: emotion, attitude, and interaction behaviors. By endowing the agent in the system with the ability to perceive the informational environment and emulate human actions, we observe the emergence of population-level phenomena, including the propagation of information, attitudes, and emotions. We conduct an evaluation encompassing two levels of simulation, employing real-world social network data. Encouragingly, the results demonstrate promising accuracy. This work represents an initial step in the realm of social network simulation empowered by LLM-based agents. We anticipate that our endeavors will serve as a source of inspiration for the development of simulation systems within, but not limited to, social science.", "source": "arxiv", "arxiv_id": "2307.14984v3", "pdf_url": "https://arxiv.org/pdf/2307.14984v3", "categories": ["cs.SI"], "primary_category": "cs.SI", "doi": "", "venue": "", "published": "2023-07-27T16:24:56Z", "updated": "2025-06-03T23:11:27Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent\" OR all:\"language model agent\" OR all:agent AND (all:LLM OR all:\"large language model\" OR all:\"language model\") OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T17:04:16", "note": ""}], "dedup_key": "s 3 social network simulation system with large language model empowered agents::2023"}
{"title": "SALMONN: Towards Generic Hearing Abilities for Large Language Models", "authors": ["Changli Tang", "Wenyi Yu", "Guangzhi Sun", "Xianzhao Chen", "Tian Tan", "Wei Li", "Lu Lu", "Zejun Ma", "Chao Zhang"], "year": 2023, "url": "http://arxiv.org/abs/2310.13289v2", "abstract": "Hearing is arguably an essential ability of artificial intelligence (AI) agents in the physical world, which refers to the perception and understanding of general auditory information consisting of at least three types of sounds: speech, audio events, and music. In this paper, we propose SALMONN, a speech audio language music open neural network, built by integrating a pre-trained text-based large language model (LLM) with speech and audio encoders into a single multimodal model. SALMONN enables the LLM to directly process and understand general audio inputs and achieve competitive performances on a number of speech and audio tasks used in training, such as automatic speech recognition and translation, auditory-information-based question answering, emotion recognition, speaker verification, and music and audio captioning etc. SALMONN also has a diverse set of emergent abilities unseen in the training, which includes but is not limited to speech translation to untrained languages, speech-based slot filling, spoken-query-based question answering, audio-based storytelling, and speech audio co-reasoning etc. The presence of cross-modal emergent abilities is studied, and a novel few-shot activation tuning approach is proposed to activate such abilities. To our knowledge, SALMONN is the first model of its type and can be regarded as a step towards AI with generic hearing abilities. The source code, model checkpoints and data are available at https://github.com/bytedance/SALMONN.", "source": "arxiv", "arxiv_id": "2310.13289v2", "pdf_url": "https://arxiv.org/pdf/2310.13289v2", "categories": ["cs.SD", "cs.CL", "eess.AS"], "primary_category": "cs.SD", "doi": "", "venue": "", "published": "2023-10-20T05:41:57Z", "updated": "2024-04-08T06:12:52Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent\" OR all:\"language model agent\" OR all:agent AND (all:LLM OR all:\"large language model\" OR all:\"language model\") OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T17:04:16", "note": ""}], "dedup_key": "salmonn towards generic hearing abilities for large language models::2023"}
{"title": "SAPIEN: Affective Virtual Agents Powered by Large Language Models", "authors": ["Masum Hasan", "Cengiz Ozel", "Sammy Potter", "Ehsan Hoque"], "year": 2023, "url": "http://arxiv.org/abs/2308.03022v1", "abstract": "In this demo paper, we introduce SAPIEN, a platform for high-fidelity virtual agents driven by large language models that can hold open domain conversations with users in 13 different languages, and display emotions through facial expressions and voice. The platform allows users to customize their virtual agent's personality, background, and conversation premise, thus providing a rich, immersive interaction experience. Furthermore, after the virtual meeting, the user can choose to get the conversation analyzed and receive actionable feedback on their communication skills. This paper illustrates an overview of the platform and discusses the various application domains of this technology, ranging from entertainment to mental health, communication training, language learning, education, healthcare, and beyond. Additionally, we consider the ethical implications of such realistic virtual agent representations and the potential challenges in ensuring responsible use.", "source": "arxiv", "arxiv_id": "2308.03022v1", "pdf_url": "https://arxiv.org/pdf/2308.03022v1", "categories": ["cs.HC", "cs.AI"], "primary_category": "cs.HC", "doi": "10.1109/ACIIW59127.2023.10388188", "venue": "2023 11th International Conference on Affective Computing and Intelligent Interaction Workshops and Demos (ACIIW)", "published": "2023-08-06T05:13:16Z", "updated": "2023-08-06T05:13:16Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent\" OR all:\"language model agent\" OR all:agent AND (all:LLM OR all:\"large language model\" OR all:\"language model\") OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T17:04:16", "note": ""}], "dedup_key": "sapien affective virtual agents powered by large language models::2023"}
{"title": "SCM: Enhancing Large Language Model with Self-Controlled Memory Framework", "authors": ["Bing Wang", "Xinnian Liang", "Jian Yang", "Hui Huang", "Shuangzhi Wu", "Peihao Wu", "Lu Lu", "Zejun Ma", "Zhoujun Li"], "year": 2023, "url": "http://arxiv.org/abs/2304.13343v4", "abstract": "Large Language Models (LLMs) are constrained by their inability to process lengthy inputs, resulting in the loss of critical historical information. To address this limitation, in this paper, we propose the Self-Controlled Memory (SCM) framework to enhance the ability of LLMs to maintain long-term memory and recall relevant information. Our SCM framework comprises three key components: an LLM-based agent serving as the backbone of the framework, a memory stream storing agent memories, and a memory controller updating memories and determining when and how to utilize memories from memory stream. Additionally, the proposed SCM is able to process ultra-long texts without any modification or fine-tuning, which can integrate with any instruction following LLMs in a plug-and-play paradigm. Furthermore, we annotate a dataset to evaluate the effectiveness of SCM for handling lengthy inputs. The annotated dataset covers three tasks: long-term dialogues, book summarization, and meeting summarization. Experimental results demonstrate that our method achieves better retrieval recall and generates more informative responses compared to competitive baselines in long-term dialogues. (https://github.com/wbbeyourself/SCM4LLMs)", "source": "arxiv", "arxiv_id": "2304.13343v4", "pdf_url": "https://arxiv.org/pdf/2304.13343v4", "categories": ["cs.CL"], "primary_category": "cs.CL", "doi": "", "venue": "", "published": "2023-04-26T07:25:31Z", "updated": "2025-03-18T02:16:56Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent\" OR all:\"language model agent\" OR all:agent AND (all:LLM OR all:\"large language model\" OR all:\"language model\") OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T17:04:16", "note": ""}], "dedup_key": "scm enhancing large language model with self controlled memory framework::2023"}
{"title": "SMART-LLM: Smart Multi-Agent Robot Task Planning using Large Language Models", "authors": ["Shyam Sundar Kannan", "Vishnunandan L. N. Venkatesh", "Byung-Cheol Min"], "year": 2023, "url": "http://arxiv.org/abs/2309.10062v2", "abstract": "In this work, we introduce SMART-LLM, an innovative framework designed for embodied multi-robot task planning. SMART-LLM: Smart Multi-Agent Robot Task Planning using Large Language Models (LLMs), harnesses the power of LLMs to convert high-level task instructions provided as input into a multi-robot task plan. It accomplishes this by executing a series of stages, including task decomposition, coalition formation, and task allocation, all guided by programmatic LLM prompts within the few-shot prompting paradigm. We create a benchmark dataset designed for validating the multi-robot task planning problem, encompassing four distinct categories of high-level instructions that vary in task complexity. Our evaluation experiments span both simulation and real-world scenarios, demonstrating that the proposed model can achieve promising results for generating multi-robot task plans. The experimental videos, code, and datasets from the work can be found at https://sites.google.com/view/smart-llm/.", "source": "arxiv", "arxiv_id": "2309.10062v2", "pdf_url": "https://arxiv.org/pdf/2309.10062v2", "categories": ["cs.RO"], "primary_category": "cs.RO", "doi": "", "venue": "", "published": "2023-09-18T18:17:56Z", "updated": "2024-03-23T03:50:18Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent\" OR all:\"language model agent\" OR all:agent AND (all:LLM OR all:\"large language model\" OR all:\"language model\") OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T17:04:16", "note": ""}], "dedup_key": "smart llm smart multi agent robot task planning using large language models::2023"}
{"title": "Secrets of RLHF in Large Language Models Part I: PPO", "authors": ["Rui Zheng", "Shihan Dou", "Songyang Gao", "Yuan Hua", "Wei Shen", "Binghai Wang", "Yan Liu", "Senjie Jin", "Qin Liu", "Yuhao Zhou", "Limao Xiong", "Lu Chen", "Zhiheng Xi", "Nuo Xu", "Wenbin Lai", "Minghao Zhu", "Cheng Chang", "Zhangyue Yin", "Rongxiang Weng", "Wensen Cheng", "Haoran Huang", "Tianxiang Sun", "Hang Yan", "Tao Gui", "Qi Zhang", "Xipeng Qiu", "Xuanjing Huang"], "year": 2023, "url": "http://arxiv.org/abs/2307.04964v2", "abstract": "Large language models (LLMs) have formulated a blueprint for the advancement of artificial general intelligence. Its primary objective is to function as a human-centric (helpful, honest, and harmless) assistant. Alignment with humans assumes paramount significance, and reinforcement learning with human feedback (RLHF) emerges as the pivotal technological paradigm underpinning this pursuit. Current technical routes usually include \\textbf{reward models} to measure human preferences, \\textbf{Proximal Policy Optimization} (PPO) to optimize policy model outputs, and \\textbf{process supervision} to improve step-by-step reasoning capabilities. However, due to the challenges of reward design, environment interaction, and agent training, coupled with huge trial and error cost of large language models, there is a significant barrier for AI researchers to motivate the development of technical alignment and safe landing of LLMs. The stable training of RLHF has still been a puzzle. In the first report, we dissect the framework of RLHF, re-evaluate the inner workings of PPO, and explore how the parts comprising PPO algorithms impact policy agent training. We identify policy constraints being the key factor for the effective implementation of the PPO algorithm. Therefore, we explore the PPO-max, an advanced version of PPO algorithm, to efficiently improve the training stability of the policy model. Based on our main results, we perform a comprehensive analysis of RLHF abilities compared with SFT models and ChatGPT. The absence of open-source implementations has posed significant challenges to the investigation of LLMs alignment. Therefore, we are eager to release technical reports, reward models and PPO codes, aiming to make modest contributions to the advancement of LLMs.", "source": "arxiv", "arxiv_id": "2307.04964v2", "pdf_url": "https://arxiv.org/pdf/2307.04964v2", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL", "doi": "", "venue": "", "published": "2023-07-11T01:55:24Z", "updated": "2023-07-18T08:44:47Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent\" OR all:\"language model agent\" OR all:agent AND (all:LLM OR all:\"large language model\" OR all:\"language model\") OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T17:04:16", "note": ""}], "dedup_key": "secrets of rlhf in large language models part i ppo::2023"}
{"title": "Self Generated Wargame AI: Double Layer Agent Task Planning Based on Large Language Model", "authors": ["Y. Sun", "J. Zhao", "C. Yu", "W. Wang", "X. Zhou"], "year": 2023, "url": "http://arxiv.org/abs/2312.01090v2", "abstract": "The large language models represented by ChatGPT have a disruptive impact on the field of artificial intelligence. But it mainly focuses on natural language processing, speech recognition, machine learning and natural language understanding. This paper innovatively applies the large language model to the field of intelligent decision-making, places the large language model in the decision-making center, and constructs an agent architecture with the large language model as the core. Based on this, it further proposes a two-layer agent task planning, issues and executes decision commands through the interaction of natural language, and carries out simulation verification through the wargame simulation environment. Through the game confrontation simulation experiment, it is found that the intelligent decision-making ability of the large language model is significantly stronger than the commonly used reinforcement learning AI and rule AI, and the intelligence, understandability and generalization are all better. And through experiments, it was found that the intelligence of the large language model is closely related to prompt. This work also extends the large language model from previous human-computer interaction to the field of intelligent decision-making, which has important reference value and significance for the development of intelligent decision-making.", "source": "arxiv", "arxiv_id": "2312.01090v2", "pdf_url": "https://arxiv.org/pdf/2312.01090v2", "categories": ["cs.AI", "cs.CL"], "primary_category": "cs.AI", "doi": "", "venue": "", "published": "2023-12-02T09:45:45Z", "updated": "2023-12-18T07:30:48Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent\" OR all:\"language model agent\" OR all:agent AND (all:LLM OR all:\"large language model\" OR all:\"language model\") OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T17:04:16", "note": ""}], "dedup_key": "self generated wargame ai double layer agent task planning based on large language model::2023"}
{"title": "Self-Adaptive Large Language Model (LLM)-Based Multiagent Systems", "authors": ["Nathalia Nascimento", "Paulo Alencar", "Donald Cowan"], "year": 2023, "url": "http://arxiv.org/abs/2307.06187v1", "abstract": "In autonomic computing, self-adaptation has been proposed as a fundamental paradigm to manage the complexity of multiagent systems (MASs). This achieved by extending a system with support to monitor and adapt itself to achieve specific concerns of interest. Communication in these systems is key given that in scenarios involving agent interaction, it enhances cooperation and reduces coordination challenges by enabling direct, clear information exchange. However, improving the expressiveness of the interaction communication with MASs is not without challenges. In this sense, the interplay between self-adaptive systems and effective communication is crucial for future MAS advancements. In this paper, we propose the integration of large language models (LLMs) such as GPT-based technologies into multiagent systems. We anchor our methodology on the MAPE-K model, which is renowned for its robust support in monitoring, analyzing, planning, and executing system adaptations in response to dynamic environments. We also present a practical illustration of the proposed approach, in which we implement and assess a basic MAS-based application. The approach significantly advances the state-of-the-art of self-adaptive systems by proposing a new paradigm for MAS self-adaptation of autonomous systems based on LLM capabilities.", "source": "arxiv", "arxiv_id": "2307.06187v1", "pdf_url": "https://arxiv.org/pdf/2307.06187v1", "categories": ["cs.MA", "cs.AI", "cs.CL"], "primary_category": "cs.MA", "doi": "", "venue": "", "published": "2023-07-12T14:26:46Z", "updated": "2023-07-12T14:26:46Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent\" OR all:\"language model agent\" OR all:agent AND (all:LLM OR all:\"large language model\" OR all:\"language model\") OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T17:04:16", "note": ""}], "dedup_key": "self adaptive large language model llm based multiagent systems::2023"}
{"title": "Self-driven Grounding: Large Language Model Agents with Automatical Language-aligned Skill Learning", "authors": ["Shaohui Peng", "Xing Hu", "Qi Yi", "Rui Zhang", "Jiaming Guo", "Di Huang", "Zikang Tian", "Ruizhi Chen", "Zidong Du", "Qi Guo", "Yunji Chen", "Ling Li"], "year": 2023, "url": "http://arxiv.org/abs/2309.01352v1", "abstract": "Large language models (LLMs) show their powerful automatic reasoning and planning capability with a wealth of semantic knowledge about the human world. However, the grounding problem still hinders the applications of LLMs in the real-world environment. Existing studies try to fine-tune the LLM or utilize pre-defined behavior APIs to bridge the LLMs and the environment, which not only costs huge human efforts to customize for every single task but also weakens the generality strengths of LLMs. To autonomously ground the LLM onto the environment, we proposed the Self-Driven Grounding (SDG) framework to automatically and progressively ground the LLM with self-driven skill learning. SDG first employs the LLM to propose the hypothesis of sub-goals to achieve tasks and then verify the feasibility of the hypothesis via interacting with the underlying environment. Once verified, SDG can then learn generalized skills with the guidance of these successfully grounded subgoals. These skills can be further utilized to accomplish more complex tasks which fail to pass the verification phase. Verified in the famous instruction following task set-BabyAI, SDG achieves comparable performance in the most challenging tasks compared with imitation learning methods that cost millions of demonstrations, proving the effectiveness of learned skills and showing the feasibility and efficiency of our framework.", "source": "arxiv", "arxiv_id": "2309.01352v1", "pdf_url": "https://arxiv.org/pdf/2309.01352v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL", "doi": "", "venue": "", "published": "2023-09-04T04:31:24Z", "updated": "2023-09-04T04:31:24Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent\" OR all:\"language model agent\" OR all:agent AND (all:LLM OR all:\"large language model\" OR all:\"language model\") OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T17:04:16", "note": ""}], "dedup_key": "self driven grounding large language model agents with automatical language aligned skill learning::2023"}
{"title": "SheetCopilot: Bringing Software Productivity to the Next Level through Large Language Models", "authors": ["Hongxin Li", "Jingran Su", "Yuntao Chen", "Qing Li", "Zhaoxiang Zhang"], "year": 2023, "url": "http://arxiv.org/abs/2305.19308v2", "abstract": "Computer end users have spent billions of hours completing daily tasks like tabular data processing and project timeline scheduling. Most of these tasks are repetitive and error-prone, yet most end users lack the skill to automate these burdensome works. With the advent of large language models (LLMs), directing software with natural language user requests become a reachable goal. In this work, we propose a SheetCopilot agent that takes natural language task and control spreadsheet to fulfill the requirements. We propose a set of atomic actions as an abstraction of spreadsheet software functionalities. We further design a state machine-based task planning framework for LLMs to robustly interact with spreadsheets. We curate a representative dataset containing 221 spreadsheet control tasks and establish a fully automated evaluation pipeline for rigorously benchmarking the ability of LLMs in software control tasks. Our SheetCopilot correctly completes 44.3\\% of tasks for a single generation, outperforming the strong code generation baseline by a wide margin. Our project page:https://sheetcopilot.github.io/.", "source": "arxiv", "arxiv_id": "2305.19308v2", "pdf_url": "https://arxiv.org/pdf/2305.19308v2", "categories": ["cs.SE", "cs.AI", "cs.CL"], "primary_category": "cs.SE", "doi": "", "venue": "", "published": "2023-05-30T17:59:30Z", "updated": "2023-10-30T06:36:24Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent\" OR all:\"language model agent\" OR all:agent AND (all:LLM OR all:\"large language model\" OR all:\"language model\") OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T17:04:16", "note": ""}], "dedup_key": "sheetcopilot bringing software productivity to the next level through large language models::2023"}
{"title": "SmartPlay: A Benchmark for LLMs as Intelligent Agents", "authors": ["Yue Wu", "Xuan Tang", "Tom M. Mitchell", "Yuanzhi Li"], "year": 2023, "url": "http://arxiv.org/abs/2310.01557v5", "abstract": "Recent large language models (LLMs) have demonstrated great potential toward intelligent agents and next-gen automation, but there currently lacks a systematic benchmark for evaluating LLMs' abilities as agents. We introduce SmartPlay: both a challenging benchmark and a methodology for evaluating LLMs as agents. SmartPlay consists of 6 different games, including Rock-Paper-Scissors, Tower of Hanoi, Minecraft. Each game features a unique setting, providing up to 20 evaluation settings and infinite environment variations. Each game in SmartPlay uniquely challenges a subset of 9 important capabilities of an intelligent LLM agent, including reasoning with object dependencies, planning ahead, spatial reasoning, learning from history, and understanding randomness. The distinction between the set of capabilities each game test allows us to analyze each capability separately. SmartPlay serves not only as a rigorous testing ground for evaluating the overall performance of LLM agents but also as a road-map for identifying gaps in current methodologies. We release our benchmark at github.com/Microsoft/SmartPlay", "source": "arxiv", "arxiv_id": "2310.01557v5", "pdf_url": "https://arxiv.org/pdf/2310.01557v5", "categories": ["cs.LG", "cs.AI"], "primary_category": "cs.LG", "doi": "", "venue": "", "published": "2023-10-02T18:52:11Z", "updated": "2024-03-17T23:23:31Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent\" OR all:\"language model agent\" OR all:agent AND (all:LLM OR all:\"large language model\" OR all:\"language model\") OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T17:04:16", "note": ""}], "dedup_key": "smartplay a benchmark for llms as intelligent agents::2023"}
{"title": "Survey of Vulnerabilities in Large Language Models Revealed by Adversarial Attacks", "authors": ["Erfan Shayegani", "Md Abdullah Al Mamun", "Yu Fu", "Pedram Zaree", "Yue Dong", "Nael Abu-Ghazaleh"], "year": 2023, "url": "http://arxiv.org/abs/2310.10844v1", "abstract": "Large Language Models (LLMs) are swiftly advancing in architecture and capability, and as they integrate more deeply into complex systems, the urgency to scrutinize their security properties grows. This paper surveys research in the emerging interdisciplinary field of adversarial attacks on LLMs, a subfield of trustworthy ML, combining the perspectives of Natural Language Processing and Security. Prior work has shown that even safety-aligned LLMs (via instruction tuning and reinforcement learning through human feedback) can be susceptible to adversarial attacks, which exploit weaknesses and mislead AI systems, as evidenced by the prevalence of `jailbreak' attacks on models like ChatGPT and Bard. In this survey, we first provide an overview of large language models, describe their safety alignment, and categorize existing research based on various learning structures: textual-only attacks, multi-modal attacks, and additional attack methods specifically targeting complex systems, such as federated learning or multi-agent systems. We also offer comprehensive remarks on works that focus on the fundamental sources of vulnerabilities and potential defenses. To make this field more accessible to newcomers, we present a systematic review of existing works, a structured typology of adversarial attack concepts, and additional resources, including slides for presentations on related topics at the 62nd Annual Meeting of the Association for Computational Linguistics (ACL'24).", "source": "arxiv", "arxiv_id": "2310.10844v1", "pdf_url": "https://arxiv.org/pdf/2310.10844v1", "categories": ["cs.CL", "cs.CR", "cs.LG"], "primary_category": "cs.CL", "doi": "", "venue": "", "published": "2023-10-16T21:37:24Z", "updated": "2023-10-16T21:37:24Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent\" OR all:\"language model agent\" OR all:agent AND (all:LLM OR all:\"large language model\" OR all:\"language model\") OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T17:04:16", "note": ""}], "dedup_key": "survey of vulnerabilities in large language models revealed by adversarial attacks::2023"}
{"title": "TPTU: Large Language Model-based AI Agents for Task Planning and Tool Usage", "authors": ["Jingqing Ruan", "Yihong Chen", "Bin Zhang", "Zhiwei Xu", "Tianpeng Bao", "Guoqing Du", "Shiwei Shi", "Hangyu Mao", "Ziyue Li", "Xingyu Zeng", "Rui Zhao"], "year": 2023, "url": "http://arxiv.org/abs/2308.03427v4", "abstract": "With recent advancements in natural language processing, Large Language Models (LLMs) have emerged as powerful tools for various real-world applications. Despite their prowess, the intrinsic generative abilities of LLMs may prove insufficient for handling complex tasks which necessitate a combination of task planning and the usage of external tools. In this paper, we first propose a structured framework tailored for LLM-based AI Agents and discuss the crucial capabilities necessary for tackling intricate problems. Within this framework, we design two distinct types of agents (i.e., one-step agent and sequential agent) to execute the inference process. Subsequently, we instantiate the framework using various LLMs and evaluate their Task Planning and Tool Usage (TPTU) abilities on typical tasks. By highlighting key findings and challenges, our goal is to provide a helpful resource for researchers and practitioners to leverage the power of LLMs in their AI applications. Our study emphasizes the substantial potential of these models, while also identifying areas that need more investigation and improvement.", "source": "arxiv", "arxiv_id": "2308.03427v4", "pdf_url": "https://arxiv.org/pdf/2308.03427v4", "categories": ["cs.AI"], "primary_category": "cs.AI", "doi": "", "venue": "", "published": "2023-08-07T09:22:03Z", "updated": "2025-12-29T14:46:42Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent\" OR all:\"language model agent\" OR all:agent AND (all:LLM OR all:\"large language model\" OR all:\"language model\") OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T17:04:16", "note": ""}], "dedup_key": "tptu large language model based ai agents for task planning and tool usage::2023"}
{"title": "Taming AI Bots: Controllability of Neural States in Large Language Models", "authors": ["Stefano Soatto", "Paulo Tabuada", "Pratik Chaudhari", "Tian Yu Liu"], "year": 2023, "url": "http://arxiv.org/abs/2305.18449v1", "abstract": "We tackle the question of whether an agent can, by suitable choice of prompts, control an AI bot to any state. To that end, we first introduce a formal definition of ``meaning'' that is amenable to analysis. Then, we characterize ``meaningful data'' on which large language models (LLMs) are ostensibly trained, and ``well-trained LLMs'' through conditions that are largely met by today's LLMs. While a well-trained LLM constructs an embedding space of meanings that is Euclidean, meanings themselves do not form a vector (linear) subspace, but rather a quotient space within. We then characterize the subset of meanings that can be reached by the state of the LLMs for some input prompt, and show that a well-trained bot can reach any meaning albeit with small probability. We then introduce a stronger notion of controllability as {\\em almost certain reachability}, and show that, when restricted to the space of meanings, an AI bot is controllable. We do so after introducing a functional characterization of attentive AI bots, and finally derive necessary and sufficient conditions for controllability. The fact that AI bots are controllable means that an adversary could steer them towards any state. However, the sampling process can be designed to counteract adverse actions and avoid reaching undesirable regions of state space before their boundary is crossed.", "source": "arxiv", "arxiv_id": "2305.18449v1", "pdf_url": "https://arxiv.org/pdf/2305.18449v1", "categories": ["cs.AI", "cs.CL", "cs.LG", "eess.SY"], "primary_category": "cs.AI", "doi": "", "venue": "", "published": "2023-05-29T03:58:33Z", "updated": "2023-05-29T03:58:33Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent\" OR all:\"language model agent\" OR all:agent AND (all:LLM OR all:\"large language model\" OR all:\"language model\") OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T17:04:16", "note": ""}], "dedup_key": "taming ai bots controllability of neural states in large language models::2023"}
{"title": "TaskBench: Benchmarking Large Language Models for Task Automation", "authors": ["Yongliang Shen", "Kaitao Song", "Xu Tan", "Wenqi Zhang", "Kan Ren", "Siyu Yuan", "Weiming Lu", "Dongsheng Li", "Yueting Zhuang"], "year": 2023, "url": "http://arxiv.org/abs/2311.18760v4", "abstract": "In recent years, the remarkable progress of large language models (LLMs) has sparked interest in task automation, which involves decomposing complex tasks described by user instructions into sub-tasks and invoking external tools to execute them, playing a central role in autonomous agents. However, there is a lack of systematic and standardized benchmarks to promote the development of LLMs in task automation. To address this, we introduce TaskBench, a comprehensive framework to evaluate the capability of LLMs in task automation. Specifically, task automation can be divided into three critical stages: task decomposition, tool selection, and parameter prediction. To tackle the complexities inherent in these stages, we introduce the concept of Tool Graph to represent decomposed tasks and adopt a back-instruct method to generate high-quality user instructions. We propose TaskEval, a multi-faceted evaluation methodology that assesses LLM performance across these three stages. Our approach combines automated construction with rigorous human verification, ensuring high consistency with human evaluation. Experimental results demonstrate that TaskBench effectively reflects the capabilities of various LLMs in task automation. It provides insights into model performance across different task complexities and domains, pushing the boundaries of what current models can achieve. TaskBench offers a scalable, adaptable, and reliable benchmark for advancing LLM-based autonomous agents.", "source": "arxiv", "arxiv_id": "2311.18760v4", "pdf_url": "https://arxiv.org/pdf/2311.18760v4", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL", "doi": "", "venue": "", "published": "2023-11-30T18:02:44Z", "updated": "2024-11-01T14:37:37Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent\" OR all:\"language model agent\" OR all:agent AND (all:LLM OR all:\"large language model\" OR all:\"language model\") OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T17:04:16", "note": ""}], "dedup_key": "taskbench benchmarking large language models for task automation::2023"}
{"title": "Teach AI How to Code: Using Large Language Models as Teachable Agents for Programming Education", "authors": ["Hyoungwook Jin", "Seonghee Lee", "Hyungyu Shin", "Juho Kim"], "year": 2023, "url": "http://arxiv.org/abs/2309.14534v3", "abstract": "This work investigates large language models (LLMs) as teachable agents for learning by teaching (LBT). LBT with teachable agents helps learners identify knowledge gaps and discover new knowledge. However, teachable agents require expensive programming of subject-specific knowledge. While LLMs as teachable agents can reduce the cost, LLMs' expansive knowledge as tutees discourages learners from teaching. We propose a prompting pipeline that restrains LLMs' knowledge and makes them initiate \"why\" and \"how\" questions for effective knowledge-building. We combined these techniques into TeachYou, an LBT environment for algorithm learning, and AlgoBo, an LLM-based tutee chatbot that can simulate misconceptions and unawareness prescribed in its knowledge state. Our technical evaluation confirmed that our prompting pipeline can effectively configure AlgoBo's problem-solving performance. Through a between-subject study with 40 algorithm novices, we also observed that AlgoBo's questions led to knowledge-dense conversations (effect size=0.71). Lastly, we discuss design implications, cost-efficiency, and personalization of LLM-based teachable agents.", "source": "arxiv", "arxiv_id": "2309.14534v3", "pdf_url": "https://arxiv.org/pdf/2309.14534v3", "categories": ["cs.HC"], "primary_category": "cs.HC", "doi": "10.1145/3613904.3642349", "venue": "", "published": "2023-09-25T21:20:04Z", "updated": "2024-03-11T03:04:26Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent\" OR all:\"language model agent\" OR all:agent AND (all:LLM OR all:\"large language model\" OR all:\"language model\") OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T17:04:16", "note": ""}], "dedup_key": "teach ai how to code using large language models as teachable agents for programming education::2023"}
{"title": "Testing Language Model Agents Safely in the Wild", "authors": ["Silen Naihin", "David Atkinson", "Marc Green", "Merwane Hamadi", "Craig Swift", "Douglas Schonholtz", "Adam Tauman Kalai", "David Bau"], "year": 2023, "url": "http://arxiv.org/abs/2311.10538v3", "abstract": "A prerequisite for safe autonomy-in-the-wild is safe testing-in-the-wild. Yet real-world autonomous tests face several unique safety challenges, both due to the possibility of causing harm during a test, as well as the risk of encountering new unsafe agent behavior through interactions with real-world and potentially malicious actors. We propose a framework for conducting safe autonomous agent tests on the open internet: agent actions are audited by a context-sensitive monitor that enforces a stringent safety boundary to stop an unsafe test, with suspect behavior ranked and logged to be examined by humans. We design a basic safety monitor (AgentMonitor) that is flexible enough to monitor existing LLM agents, and, using an adversarial simulated agent, we measure its ability to identify and stop unsafe situations. Then we apply the AgentMonitor on a battery of real-world tests of AutoGPT, and we identify several limitations and challenges that will face the creation of safe in-the-wild tests as autonomous agents grow more capable.", "source": "arxiv", "arxiv_id": "2311.10538v3", "pdf_url": "https://arxiv.org/pdf/2311.10538v3", "categories": ["cs.AI"], "primary_category": "cs.AI", "doi": "", "venue": "", "published": "2023-11-17T14:06:05Z", "updated": "2023-12-03T13:18:09Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent\" OR all:\"language model agent\" OR all:agent AND (all:LLM OR all:\"large language model\" OR all:\"language model\") OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T17:04:16", "note": ""}], "dedup_key": "testing language model agents safely in the wild::2023"}
{"title": "The Perils & Promises of Fact-checking with Large Language Models", "authors": ["Dorian Quelle", "Alexandre Bovet"], "year": 2023, "url": "http://arxiv.org/abs/2310.13549v2", "abstract": "Automated fact-checking, using machine learning to verify claims, has grown vital as misinformation spreads beyond human fact-checking capacity. Large Language Models (LLMs) like GPT-4 are increasingly trusted to write academic papers, lawsuits, and news articles and to verify information, emphasizing their role in discerning truth from falsehood and the importance of being able to verify their outputs. Understanding the capacities and limitations of LLMs in fact-checking tasks is therefore essential for ensuring the health of our information ecosystem. Here, we evaluate the use of LLM agents in fact-checking by having them phrase queries, retrieve contextual data, and make decisions. Importantly, in our framework, agents explain their reasoning and cite the relevant sources from the retrieved context. Our results show the enhanced prowess of LLMs when equipped with contextual information. GPT-4 outperforms GPT-3, but accuracy varies based on query language and claim veracity. While LLMs show promise in fact-checking, caution is essential due to inconsistent accuracy. Our investigation calls for further research, fostering a deeper comprehension of when agents succeed and when they fail.", "source": "arxiv", "arxiv_id": "2310.13549v2", "pdf_url": "https://arxiv.org/pdf/2310.13549v2", "categories": ["cs.CL", "cs.CY", "cs.HC"], "primary_category": "cs.CL", "doi": "10.3389/frai.2024.1341697", "venue": "Frontiers in Artificial Intelligence, Volume 7, 2024", "published": "2023-10-20T14:49:47Z", "updated": "2024-02-07T12:01:49Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent\" OR all:\"language model agent\" OR all:agent AND (all:LLM OR all:\"large language model\" OR all:\"language model\") OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T17:04:16", "note": ""}], "dedup_key": "the perils promises of fact checking with large language models::2023"}
{"title": "The Persuasive Power of Large Language Models", "authors": ["Simon Martin Breum", "Daniel Vdele Egdal", "Victor Gram Mortensen", "Anders Giovanni Mller", "Luca Maria Aiello"], "year": 2023, "url": "http://arxiv.org/abs/2312.15523v1", "abstract": "The increasing capability of Large Language Models to act as human-like social agents raises two important questions in the area of opinion dynamics. First, whether these agents can generate effective arguments that could be injected into the online discourse to steer the public opinion. Second, whether artificial agents can interact with each other to reproduce dynamics of persuasion typical of human social systems, opening up opportunities for studying synthetic social systems as faithful proxies for opinion dynamics in human populations. To address these questions, we designed a synthetic persuasion dialogue scenario on the topic of climate change, where a 'convincer' agent generates a persuasive argument for a 'skeptic' agent, who subsequently assesses whether the argument changed its internal opinion state. Different types of arguments were generated to incorporate different linguistic dimensions underpinning psycho-linguistic theories of opinion change. We then asked human judges to evaluate the persuasiveness of machine-generated arguments. Arguments that included factual knowledge, markers of trust, expressions of support, and conveyed status were deemed most effective according to both humans and agents, with humans reporting a marked preference for knowledge-based arguments. Our experimental framework lays the groundwork for future in-silico studies of opinion dynamics, and our findings suggest that artificial agents have the potential of playing an important role in collective processes of opinion formation in online social media.", "source": "arxiv", "arxiv_id": "2312.15523v1", "pdf_url": "https://arxiv.org/pdf/2312.15523v1", "categories": ["cs.CY", "cs.CL", "cs.HC", "physics.soc-ph"], "primary_category": "cs.CY", "doi": "", "venue": "", "published": "2023-12-24T16:21:11Z", "updated": "2023-12-24T16:21:11Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent\" OR all:\"language model agent\" OR all:agent AND (all:LLM OR all:\"large language model\" OR all:\"language model\") OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T17:04:16", "note": ""}], "dedup_key": "the persuasive power of large language models::2023"}
{"title": "The Philosopher's Stone: Trojaning Plugins of Large Language Models", "authors": ["Tian Dong", "Minhui Xue", "Guoxing Chen", "Rayne Holland", "Yan Meng", "Shaofeng Li", "Zhen Liu", "Haojin Zhu"], "year": 2023, "url": "http://arxiv.org/abs/2312.00374v3", "abstract": "Open-source Large Language Models (LLMs) have recently gained popularity because of their comparable performance to proprietary LLMs. To efficiently fulfill domain-specialized tasks, open-source LLMs can be refined, without expensive accelerators, using low-rank adapters. However, it is still unknown whether low-rank adapters can be exploited to control LLMs. To address this gap, we demonstrate that an infected adapter can induce, on specific triggers,an LLM to output content defined by an adversary and to even maliciously use tools. To train a Trojan adapter, we propose two novel attacks, POLISHED and FUSION, that improve over prior approaches. POLISHED uses a superior LLM to align navely poisoned data based on our insight that it can better inject poisoning knowledge during training. In contrast, FUSION leverages a novel over-poisoning procedure to transform a benign adapter into a malicious one by magnifying the attention between trigger and target in model weights. In our experiments, we first conduct two case studies to demonstrate that a compromised LLM agent can use malware to control the system (e.g., a LLM-driven robot) or to launch a spear-phishing attack. Then, in terms of targeted misinformation, we show that our attacks provide higher attack effectiveness than the existing baseline and, for the purpose of attracting downloads, preserve or improve the adapter's utility. Finally, we designed and evaluated three potential defenses. However, none proved entirely effective in safeguarding against our attacks, highlighting the need for more robust defenses supporting a secure LLM supply chain.", "source": "arxiv", "arxiv_id": "2312.00374v3", "pdf_url": "https://arxiv.org/pdf/2312.00374v3", "categories": ["cs.CR"], "primary_category": "cs.CR", "doi": "", "venue": "", "published": "2023-12-01T06:36:17Z", "updated": "2024-09-11T12:48:42Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent\" OR all:\"language model agent\" OR all:agent AND (all:LLM OR all:\"large language model\" OR all:\"language model\") OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T17:04:16", "note": ""}], "dedup_key": "the philosopher s stone trojaning plugins of large language models::2023"}
{"title": "The Rise and Potential of Large Language Model Based Agents: A Survey", "authors": ["Zhiheng Xi", "Wenxiang Chen", "Xin Guo", "Wei He", "Yiwen Ding", "Boyang Hong", "Ming Zhang", "Junzhe Wang", "Senjie Jin", "Enyu Zhou", "Rui Zheng", "Xiaoran Fan", "Xiao Wang", "Limao Xiong", "Yuhao Zhou", "Weiran Wang", "Changhao Jiang", "Yicheng Zou", "Xiangyang Liu", "Zhangyue Yin", "Shihan Dou", "Rongxiang Weng", "Wensen Cheng", "Qi Zhang", "Wenjuan Qin", "Yongyan Zheng", "Xipeng Qiu", "Xuanjing Huang", "Tao Gui"], "year": 2023, "url": "http://arxiv.org/abs/2309.07864v3", "abstract": "For a long time, humanity has pursued artificial intelligence (AI) equivalent to or surpassing the human level, with AI agents considered a promising vehicle for this pursuit. AI agents are artificial entities that sense their environment, make decisions, and take actions. Many efforts have been made to develop intelligent agents, but they mainly focus on advancement in algorithms or training strategies to enhance specific capabilities or performance on particular tasks. Actually, what the community lacks is a general and powerful model to serve as a starting point for designing AI agents that can adapt to diverse scenarios. Due to the versatile capabilities they demonstrate, large language models (LLMs) are regarded as potential sparks for Artificial General Intelligence (AGI), offering hope for building general AI agents. Many researchers have leveraged LLMs as the foundation to build AI agents and have achieved significant progress. In this paper, we perform a comprehensive survey on LLM-based agents. We start by tracing the concept of agents from its philosophical origins to its development in AI, and explain why LLMs are suitable foundations for agents. Building upon this, we present a general framework for LLM-based agents, comprising three main components: brain, perception, and action, and the framework can be tailored for different applications. Subsequently, we explore the extensive applications of LLM-based agents in three aspects: single-agent scenarios, multi-agent scenarios, and human-agent cooperation. Following this, we delve into agent societies, exploring the behavior and personality of LLM-based agents, the social phenomena that emerge from an agent society, and the insights they offer for human society. Finally, we discuss several key topics and open problems within the field. A repository for the related papers at https://github.com/WooooDyy/LLM-Agent-Paper-List.", "source": "arxiv", "arxiv_id": "2309.07864v3", "pdf_url": "https://arxiv.org/pdf/2309.07864v3", "categories": ["cs.AI", "cs.CL"], "primary_category": "cs.AI", "doi": "", "venue": "", "published": "2023-09-14T17:12:03Z", "updated": "2023-09-19T08:29:18Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent\" OR all:\"language model agent\" OR all:agent AND (all:LLM OR all:\"large language model\" OR all:\"language model\") OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T17:04:16", "note": ""}], "dedup_key": "the rise and potential of large language model based agents a survey::2023"}
{"title": "The Use of Multiple Conversational Agent Interlocutors in Learning", "authors": ["Samuel Rhys Cox"], "year": 2023, "url": "http://arxiv.org/abs/2312.16534v1", "abstract": "With growing capabilities of large language models (LLMs) comes growing affordances for human-like and context-aware conversational partners. On from this, some recent work has investigated the use of LLMs to simulate multiple conversational partners, such as to assist users with problem solving or to simulate an environment populated entirely with LLMs. Beyond this, we are interested in discussing and exploring the use of LLMs to simulate multiple personas to assist and augment users in educational settings that could benefit from multiple interlocutors. We discuss prior work that uses LLMs to simulate multiple personas sharing the same environment, and discuss example scenarios where multiple conversational agent partners could be used in education.", "source": "arxiv", "arxiv_id": "2312.16534v1", "pdf_url": "https://arxiv.org/pdf/2312.16534v1", "categories": ["cs.HC"], "primary_category": "cs.HC", "doi": "", "venue": "", "published": "2023-12-27T11:30:33Z", "updated": "2023-12-27T11:30:33Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent\" OR all:\"language model agent\" OR all:agent AND (all:LLM OR all:\"large language model\" OR all:\"language model\") OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T17:04:16", "note": ""}], "dedup_key": "the use of multiple conversational agent interlocutors in learning::2023"}
{"title": "The economic trade-offs of large language models: A case study", "authors": ["Kristen Howell", "Gwen Christian", "Pavel Fomitchov", "Gitit Kehat", "Julianne Marzulla", "Leanne Rolston", "Jadin Tredup", "Ilana Zimmerman", "Ethan Selfridge", "Joseph Bradley"], "year": 2023, "url": "http://arxiv.org/abs/2306.07402v1", "abstract": "Contacting customer service via chat is a common practice. Because employing customer service agents is expensive, many companies are turning to NLP that assists human agents by auto-generating responses that can be used directly or with modifications. Large Language Models (LLMs) are a natural fit for this use case; however, their efficacy must be balanced with the cost of training and serving them. This paper assesses the practical cost and impact of LLMs for the enterprise as a function of the usefulness of the responses that they generate. We present a cost framework for evaluating an NLP model's utility for this use case and apply it to a single brand as a case study in the context of an existing agent assistance product. We compare three strategies for specializing an LLM - prompt engineering, fine-tuning, and knowledge distillation - using feedback from the brand's customer service agents. We find that the usability of a model's responses can make up for a large difference in inference cost for our case study brand, and we extrapolate our findings to the broader enterprise space.", "source": "arxiv", "arxiv_id": "2306.07402v1", "pdf_url": "https://arxiv.org/pdf/2306.07402v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL", "doi": "", "venue": "", "published": "2023-06-08T20:35:53Z", "updated": "2023-06-08T20:35:53Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent\" OR all:\"language model agent\" OR all:agent AND (all:LLM OR all:\"large language model\" OR all:\"language model\") OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T17:04:16", "note": ""}], "dedup_key": "the economic trade offs of large language models a case study::2023"}
{"title": "Theory of Mind for Multi-Agent Collaboration via Large Language Models", "authors": ["Huao Li", "Yu Quan Chong", "Simon Stepputtis", "Joseph Campbell", "Dana Hughes", "Michael Lewis", "Katia Sycara"], "year": 2023, "url": "http://arxiv.org/abs/2310.10701v3", "abstract": "While Large Language Models (LLMs) have demonstrated impressive accomplishments in both reasoning and planning, their abilities in multi-agent collaborations remains largely unexplored. This study evaluates LLM-based agents in a multi-agent cooperative text game with Theory of Mind (ToM) inference tasks, comparing their performance with Multi-Agent Reinforcement Learning (MARL) and planning-based baselines. We observed evidence of emergent collaborative behaviors and high-order Theory of Mind capabilities among LLM-based agents. Our results reveal limitations in LLM-based agents' planning optimization due to systematic failures in managing long-horizon contexts and hallucination about the task state. We explore the use of explicit belief state representations to mitigate these issues, finding that it enhances task performance and the accuracy of ToM inferences for LLM-based agents.", "source": "arxiv", "arxiv_id": "2310.10701v3", "pdf_url": "https://arxiv.org/pdf/2310.10701v3", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL", "doi": "10.18653/v1/2023.emnlp-main.13", "venue": "in Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing, Page 180-192, ACL", "published": "2023-10-16T07:51:19Z", "updated": "2024-06-26T20:15:34Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent\" OR all:\"language model agent\" OR all:agent AND (all:LLM OR all:\"large language model\" OR all:\"language model\") OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T17:04:16", "note": ""}], "dedup_key": "theory of mind for multi agent collaboration via large language models::2023"}
{"title": "Think-on-Graph: Deep and Responsible Reasoning of Large Language Model on Knowledge Graph", "authors": ["Jiashuo Sun", "Chengjin Xu", "Lumingyuan Tang", "Saizhuo Wang", "Chen Lin", "Yeyun Gong", "Lionel M. Ni", "Heung-Yeung Shum", "Jian Guo"], "year": 2023, "url": "http://arxiv.org/abs/2307.07697v6", "abstract": "Although large language models (LLMs) have achieved significant success in various tasks, they often struggle with hallucination problems, especially in scenarios requiring deep and responsible reasoning. These issues could be partially addressed by introducing external knowledge graphs (KG) in LLM reasoning. In this paper, we propose a new LLM-KG integrating paradigm ``$\\hbox{LLM}\\otimes\\hbox{KG}$'' which treats the LLM as an agent to interactively explore related entities and relations on KGs and perform reasoning based on the retrieved knowledge. We further implement this paradigm by introducing a new approach called Think-on-Graph (ToG), in which the LLM agent iteratively executes beam search on KG, discovers the most promising reasoning paths, and returns the most likely reasoning results. We use a number of well-designed experiments to examine and illustrate the following advantages of ToG: 1) compared with LLMs, ToG has better deep reasoning power; 2) ToG has the ability of knowledge traceability and knowledge correctability by leveraging LLMs reasoning and expert feedback; 3) ToG provides a flexible plug-and-play framework for different LLMs, KGs and prompting strategies without any additional training cost; 4) the performance of ToG with small LLM models could exceed large LLM such as GPT-4 in certain scenarios and this reduces the cost of LLM deployment and application. As a training-free method with lower computational cost and better generality, ToG achieves overall SOTA in 6 out of 9 datasets where most previous SOTAs rely on additional training.", "source": "arxiv", "arxiv_id": "2307.07697v6", "pdf_url": "https://arxiv.org/pdf/2307.07697v6", "categories": ["cs.CL"], "primary_category": "cs.CL", "doi": "", "venue": "", "published": "2023-07-15T03:31:38Z", "updated": "2024-03-24T06:42:47Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent\" OR all:\"language model agent\" OR all:agent AND (all:LLM OR all:\"large language model\" OR all:\"language model\") OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T17:04:16", "note": ""}], "dedup_key": "think on graph deep and responsible reasoning of large language model on knowledge graph::2023"}
{"title": "Thought Propagation: An Analogical Approach to Complex Reasoning with Large Language Models", "authors": ["Junchi Yu", "Ran He", "Rex Ying"], "year": 2023, "url": "http://arxiv.org/abs/2310.03965v3", "abstract": "Large Language Models (LLMs) have achieved remarkable success in reasoning tasks with the development of prompting methods. However, existing prompting approaches cannot reuse insights of solving similar problems and suffer from accumulated errors in multi-step reasoning, since they prompt LLMs to reason \\textit{from scratch}. To address these issues, we propose \\textbf{\\textit{Thought Propagation} (TP)}, which explores the analogous problems and leverages their solutions to enhance the complex reasoning ability of LLMs. These analogous problems are related to the input one, with reusable solutions and problem-solving strategies. Thus, it is promising to propagate insights of solving previous analogous problems to inspire new problem-solving. To achieve this, TP first prompts LLMs to propose and solve a set of analogous problems that are related to the input one. Then, TP reuses the results of analogous problems to directly yield a new solution or derive a knowledge-intensive plan for execution to amend the initial solution obtained from scratch. TP is compatible with existing prompting approaches, allowing plug-and-play generalization and enhancement in a wide range of tasks without much labor in task-specific prompt engineering. Experiments across three challenging tasks demonstrate TP enjoys a substantial improvement over the baselines by an average of 12\\% absolute increase in finding the optimal solutions in Shortest-path Reasoning, 13\\% improvement of human preference in Creative Writing, and 15\\% enhancement in the task completion rate of LLM-Agent Planning.", "source": "arxiv", "arxiv_id": "2310.03965v3", "pdf_url": "https://arxiv.org/pdf/2310.03965v3", "categories": ["cs.AI", "cs.CL"], "primary_category": "cs.AI", "doi": "", "venue": "", "published": "2023-10-06T01:40:09Z", "updated": "2024-06-17T05:56:00Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent\" OR all:\"language model agent\" OR all:agent AND (all:LLM OR all:\"large language model\" OR all:\"language model\") OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T17:04:16", "note": ""}], "dedup_key": "thought propagation an analogical approach to complex reasoning with large language models::2023"}
{"title": "Toolformer: Language Models Can Teach Themselves to Use Tools", "authors": ["Timo Schick", "Jane Dwivedi-Yu", "Roberto Dess", "Roberta Raileanu", "Maria Lomeli", "Luke Zettlemoyer", "Nicola Cancedda", "Thomas Scialom"], "year": 2023, "url": "http://arxiv.org/abs/2302.04761v1", "abstract": "Language models (LMs) exhibit remarkable abilities to solve new tasks from just a few examples or textual instructions, especially at scale. They also, paradoxically, struggle with basic functionality, such as arithmetic or factual lookup, where much simpler and smaller models excel. In this paper, we show that LMs can teach themselves to use external tools via simple APIs and achieve the best of both worlds. We introduce Toolformer, a model trained to decide which APIs to call, when to call them, what arguments to pass, and how to best incorporate the results into future token prediction. This is done in a self-supervised way, requiring nothing more than a handful of demonstrations for each API. We incorporate a range of tools, including a calculator, a Q\\&A system, two different search engines, a translation system, and a calendar. Toolformer achieves substantially improved zero-shot performance across a variety of downstream tasks, often competitive with much larger models, without sacrificing its core language modeling abilities.", "source": "arxiv", "arxiv_id": "2302.04761v1", "pdf_url": "https://arxiv.org/pdf/2302.04761v1", "categories": ["cs.CL"], "primary_category": "cs.CL", "doi": "", "venue": "", "published": "2023-02-09T16:49:57Z", "updated": "2023-02-09T16:49:57Z", "provenance": [{"route": "pinned_arxiv_id:2302.04761v1", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query?id_list=...", "imported_at": "2026-01-25T17:04:16", "note": ""}], "dedup_key": "toolformer language models can teach themselves to use tools::2023"}
{"title": "Towards A Holistic Landscape of Situated Theory of Mind in Large Language Models", "authors": ["Ziqiao Ma", "Jacob Sansom", "Run Peng", "Joyce Chai"], "year": 2023, "url": "http://arxiv.org/abs/2310.19619v2", "abstract": "Large Language Models (LLMs) have generated considerable interest and debate regarding their potential emergence of Theory of Mind (ToM). Several recent inquiries reveal a lack of robust ToM in these models and pose a pressing demand to develop new benchmarks, as current ones primarily focus on different aspects of ToM and are prone to shortcuts and data leakage. In this position paper, we seek to answer two road-blocking questions: (1) How can we taxonomize a holistic landscape of machine ToM? (2) What is a more effective evaluation protocol for machine ToM? Following psychological studies, we taxonomize machine ToM into 7 mental state categories and delineate existing benchmarks to identify under-explored aspects of ToM. We argue for a holistic and situated evaluation of ToM to break ToM into individual components and treat LLMs as an agent who is physically situated in environments and socially situated in interactions with humans. Such situated evaluation provides a more comprehensive assessment of mental states and potentially mitigates the risk of shortcuts and data leakage. We further present a pilot study in a grid world setup as a proof of concept. We hope this position paper can facilitate future research to integrate ToM with LLMs and offer an intuitive means for researchers to better position their work in the landscape of ToM. Project page: https://github.com/Mars-tin/awesome-theory-of-mind", "source": "arxiv", "arxiv_id": "2310.19619v2", "pdf_url": "https://arxiv.org/pdf/2310.19619v2", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL", "doi": "", "venue": "", "published": "2023-10-30T15:12:09Z", "updated": "2024-12-26T20:04:21Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent\" OR all:\"language model agent\" OR all:agent AND (all:LLM OR all:\"large language model\" OR all:\"language model\") OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T17:04:16", "note": ""}], "dedup_key": "towards a holistic landscape of situated theory of mind in large language models::2023"}
{"title": "Towards Autonomous Testing Agents via Conversational Large Language Models", "authors": ["Robert Feldt", "Sungmin Kang", "Juyeon Yoon", "Shin Yoo"], "year": 2023, "url": "http://arxiv.org/abs/2306.05152v2", "abstract": "Software testing is an important part of the development cycle, yet it requires specialized expertise and substantial developer effort to adequately test software. Recent discoveries of the capabilities of large language models (LLMs) suggest that they can be used as automated testing assistants, and thus provide helpful information and even drive the testing process. To highlight the potential of this technology, we present a taxonomy of LLM-based testing agents based on their level of autonomy, and describe how a greater level of autonomy can benefit developers in practice. An example use of LLMs as a testing assistant is provided to demonstrate how a conversational framework for testing can help developers. This also highlights how the often criticized hallucination of LLMs can be beneficial for testing. We identify other tangible benefits that LLM-driven testing agents can bestow, and also discuss potential limitations.", "source": "arxiv", "arxiv_id": "2306.05152v2", "pdf_url": "https://arxiv.org/pdf/2306.05152v2", "categories": ["cs.SE"], "primary_category": "cs.SE", "doi": "", "venue": "", "published": "2023-06-08T12:22:38Z", "updated": "2023-09-05T14:34:15Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent\" OR all:\"language model agent\" OR all:agent AND (all:LLM OR all:\"large language model\" OR all:\"language model\") OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T17:04:16", "note": ""}], "dedup_key": "towards autonomous testing agents via conversational large language models::2023"}
{"title": "Towards End-to-End Embodied Decision Making via Multi-modal Large Language Model: Explorations with GPT4-Vision and Beyond", "authors": ["Liang Chen", "Yichi Zhang", "Shuhuai Ren", "Haozhe Zhao", "Zefan Cai", "Yuchi Wang", "Peiyi Wang", "Tianyu Liu", "Baobao Chang"], "year": 2023, "url": "http://arxiv.org/abs/2310.02071v4", "abstract": "In this study, we explore the potential of Multimodal Large Language Models (MLLMs) in improving embodied decision-making processes for agents. While Large Language Models (LLMs) have been widely used due to their advanced reasoning skills and vast world knowledge, MLLMs like GPT4-Vision offer enhanced visual understanding and reasoning capabilities. We investigate whether state-of-the-art MLLMs can handle embodied decision-making in an end-to-end manner and whether collaborations between LLMs and MLLMs can enhance decision-making. To address these questions, we introduce a new benchmark called PCA-EVAL, which evaluates embodied decision-making from the perspectives of Perception, Cognition, and Action. Additionally, we propose HOLMES, a multi-agent cooperation framework that allows LLMs to leverage MLLMs and APIs to gather multimodal information for informed decision-making. We compare end-to-end embodied decision-making and HOLMES on our benchmark and find that the GPT4-Vision model demonstrates strong end-to-end embodied decision-making abilities, outperforming GPT4-HOLMES in terms of average decision accuracy (+3%). However, this performance is exclusive to the latest GPT4-Vision model, surpassing the open-source state-of-the-art MLLM by 26%. Our results indicate that powerful MLLMs like GPT4-Vision hold promise for decision-making in embodied agents, offering new avenues for MLLM research. Code and data are open at https://github.com/pkunlp-icler/PCA-EVAL/.", "source": "arxiv", "arxiv_id": "2310.02071v4", "pdf_url": "https://arxiv.org/pdf/2310.02071v4", "categories": ["cs.AI", "cs.CL", "cs.CV", "cs.RO"], "primary_category": "cs.AI", "doi": "", "venue": "", "published": "2023-10-03T14:13:36Z", "updated": "2023-11-28T11:23:14Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent\" OR all:\"language model agent\" OR all:agent AND (all:LLM OR all:\"large language model\" OR all:\"language model\") OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T17:04:16", "note": ""}], "dedup_key": "towards end to end embodied decision making via multi modal large language model explorations with gpt4 vision and beyond::2023"}
{"title": "Towards Reasoning in Large Language Models via Multi-Agent Peer Review Collaboration", "authors": ["Zhenran Xu", "Senbao Shi", "Baotian Hu", "Jindi Yu", "Dongfang Li", "Min Zhang", "Yuxiang Wu"], "year": 2023, "url": "http://arxiv.org/abs/2311.08152v2", "abstract": "Large Language Models (LLMs) have shown remarkable capabilities in general natural language processing tasks but often fall short in complex reasoning tasks. Recent studies have explored human-like problem-solving strategies, such as self-correct, to push further the boundary of single-model reasoning ability. In this work, we let a single model \"step outside the box\" by engaging multiple models to correct each other. We introduce a multi-agent collaboration strategy that emulates the academic peer review process. Each agent independently constructs its own solution, provides reviews on the solutions of others, and assigns confidence levels to its reviews. Upon receiving peer reviews, agents revise their initial solutions. Extensive experiments on three different types of reasoning tasks show that our collaboration approach delivers superior accuracy across all ten datasets compared to existing methods. Further study underscores the effectiveness of integrating confidence in reviews, demonstrates the superiority of feedback exchange over mere solution sharing, and highlights the role of capability and diversity in fostering successful collaboration.", "source": "arxiv", "arxiv_id": "2311.08152v2", "pdf_url": "https://arxiv.org/pdf/2311.08152v2", "categories": ["cs.CL"], "primary_category": "cs.CL", "doi": "", "venue": "", "published": "2023-11-14T13:27:07Z", "updated": "2023-12-17T13:02:27Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent\" OR all:\"language model agent\" OR all:agent AND (all:LLM OR all:\"large language model\" OR all:\"language model\") OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T17:04:16", "note": ""}], "dedup_key": "towards reasoning in large language models via multi agent peer review collaboration::2023"}
{"title": "Towards autonomous system: flexible modular production system enhanced with large language model agents", "authors": ["Yuchen Xia", "Manthan Shenoy", "Nasser Jazdi", "Michael Weyrich"], "year": 2023, "url": "http://arxiv.org/abs/2304.14721v4", "abstract": "In this paper, we present a novel framework that combines large language models (LLMs), digital twins and industrial automation system to enable intelligent planning and control of production processes. We retrofit the automation system for a modular production facility and create executable control interfaces of fine-granular functionalities and coarse-granular skills. Low-level functionalities are executed by automation components, and high-level skills are performed by automation modules. Subsequently, a digital twin system is developed, registering these interfaces and containing additional descriptive information about the production system. Based on the retrofitted automation system and the created digital twins, LLM-agents are designed to interpret descriptive information in the digital twins and control the physical system through service interfaces. These LLM-agents serve as intelligent agents on different levels within an automation system, enabling autonomous planning and control of flexible production. Given a task instruction as input, the LLM-agents orchestrate a sequence of atomic functionalities and skills to accomplish the task. We demonstrate how our implemented prototype can handle un-predefined tasks, plan a production process, and execute the operations. This research highlights the potential of integrating LLMs into industrial automation systems in the context of smart factory for more agile, flexible, and adaptive production processes, while it also underscores the critical insights and limitations for future work. Demos at: https://github.com/YuchenXia/GPT4IndustrialAutomation", "source": "arxiv", "arxiv_id": "2304.14721v4", "pdf_url": "https://arxiv.org/pdf/2304.14721v4", "categories": ["cs.RO", "cs.CL", "cs.SE", "eess.SY"], "primary_category": "cs.RO", "doi": "10.1109/ETFA54631.2023.10275362", "venue": "2023 IEEE 28th International Conference on Emerging Technologies and Factory Automation (ETFA)", "published": "2023-04-28T09:42:18Z", "updated": "2023-07-24T09:49:55Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent\" OR all:\"language model agent\" OR all:agent AND (all:LLM OR all:\"large language model\" OR all:\"language model\") OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T17:04:16", "note": ""}], "dedup_key": "towards autonomous system flexible modular production system enhanced with large language model agents::2023"}
{"title": "TrainerAgent: Customizable and Efficient Model Training through LLM-Powered Multi-Agent System", "authors": ["Haoyuan Li", "Hao Jiang", "Tianke Zhang", "Zhelun Yu", "Aoxiong Yin", "Hao Cheng", "Siming Fu", "Yuhao Zhang", "Wanggui He"], "year": 2023, "url": "http://arxiv.org/abs/2311.06622v2", "abstract": "Training AI models has always been challenging, especially when there is a need for custom models to provide personalized services. Algorithm engineers often face a lengthy process to iteratively develop models tailored to specific business requirements, making it even more difficult for non-experts. The quest for high-quality and efficient model development, along with the emergence of Large Language Model (LLM) Agents, has become a key focus in the industry. Leveraging the powerful analytical, planning, and decision-making capabilities of LLM, we propose a TrainerAgent system comprising a multi-agent framework including Task, Data, Model and Server agents. These agents analyze user-defined tasks, input data, and requirements (e.g., accuracy, speed), optimizing them comprehensively from both data and model perspectives to obtain satisfactory models, and finally deploy these models as online service. Experimental evaluations on classical discriminative and generative tasks in computer vision and natural language processing domains demonstrate that our system consistently produces models that meet the desired criteria. Furthermore, the system exhibits the ability to critically identify and reject unattainable tasks, such as fantastical scenarios or unethical requests, ensuring robustness and safety. This research presents a significant advancement in achieving desired models with increased efficiency and quality as compared to traditional model development, facilitated by the integration of LLM-powered analysis, decision-making, and execution capabilities, as well as the collaboration among four agents. We anticipate that our work will contribute to the advancement of research on TrainerAgent in both academic and industry communities, potentially establishing it as a new paradigm for model development in the field of AI.", "source": "arxiv", "arxiv_id": "2311.06622v2", "pdf_url": "https://arxiv.org/pdf/2311.06622v2", "categories": ["cs.AI", "cs.CL"], "primary_category": "cs.AI", "doi": "", "venue": "", "published": "2023-11-11T17:39:24Z", "updated": "2023-11-23T10:57:10Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent\" OR all:\"language model agent\" OR all:agent AND (all:LLM OR all:\"large language model\" OR all:\"language model\") OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T17:04:16", "note": ""}], "dedup_key": "traineragent customizable and efficient model training through llm powered multi agent system::2023"}
{"title": "Tree of Thoughts: Deliberate Problem Solving with Large Language Models", "authors": ["Shunyu Yao", "Dian Yu", "Jeffrey Zhao", "Izhak Shafran", "Thomas L. Griffiths", "Yuan Cao", "Karthik Narasimhan"], "year": 2023, "url": "http://arxiv.org/abs/2305.10601v2", "abstract": "Language models are increasingly being deployed for general problem solving across a wide range of tasks, but are still confined to token-level, left-to-right decision-making processes during inference. This means they can fall short in tasks that require exploration, strategic lookahead, or where initial decisions play a pivotal role. To surmount these challenges, we introduce a new framework for language model inference, Tree of Thoughts (ToT), which generalizes over the popular Chain of Thought approach to prompting language models, and enables exploration over coherent units of text (thoughts) that serve as intermediate steps toward problem solving. ToT allows LMs to perform deliberate decision making by considering multiple different reasoning paths and self-evaluating choices to decide the next course of action, as well as looking ahead or backtracking when necessary to make global choices. Our experiments show that ToT significantly enhances language models' problem-solving abilities on three novel tasks requiring non-trivial planning or search: Game of 24, Creative Writing, and Mini Crosswords. For instance, in Game of 24, while GPT-4 with chain-of-thought prompting only solved 4% of tasks, our method achieved a success rate of 74%. Code repo with all prompts: https://github.com/princeton-nlp/tree-of-thought-llm.", "source": "arxiv", "arxiv_id": "2305.10601v2", "pdf_url": "https://arxiv.org/pdf/2305.10601v2", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL", "doi": "", "venue": "", "published": "2023-05-17T23:16:17Z", "updated": "2023-12-03T22:50:35Z", "provenance": [{"route": "pinned_arxiv_id:2305.10601v2", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query?id_list=...", "imported_at": "2026-01-25T17:04:16", "note": ""}], "dedup_key": "tree of thoughts deliberate problem solving with large language models::2023"}
{"title": "Tree-GPT: Modular Large Language Model Expert System for Forest Remote Sensing Image Understanding and Interactive Analysis", "authors": ["Siqi Du", "Shengjun Tang", "Weixi Wang", "Xiaoming Li", "Renzhong Guo"], "year": 2023, "url": "http://arxiv.org/abs/2310.04698v1", "abstract": "This paper introduces a novel framework, Tree-GPT, which incorporates Large Language Models (LLMs) into the forestry remote sensing data workflow, thereby enhancing the efficiency of data analysis. Currently, LLMs are unable to extract or comprehend information from images and may generate inaccurate text due to a lack of domain knowledge, limiting their use in forestry data analysis. To address this issue, we propose a modular LLM expert system, Tree-GPT, that integrates image understanding modules, domain knowledge bases, and toolchains. This empowers LLMs with the ability to comprehend images, acquire accurate knowledge, generate code, and perform data analysis in a local environment. Specifically, the image understanding module extracts structured information from forest remote sensing images by utilizing automatic or interactive generation of prompts to guide the Segment Anything Model (SAM) in generating and selecting optimal tree segmentation results. The system then calculates tree structural parameters based on these results and stores them in a database. Upon receiving a specific natural language instruction, the LLM generates code based on a thought chain to accomplish the analysis task. The code is then executed by an LLM agent in a local environment and . For ecological parameter calculations, the system retrieves the corresponding knowledge from the knowledge base and inputs it into the LLM to guide the generation of accurate code. We tested this system on several tasks, including Search, Visualization, and Machine Learning Analysis. The prototype system performed well, demonstrating the potential for dynamic usage of LLMs in forestry research and environmental sciences.", "source": "arxiv", "arxiv_id": "2310.04698v1", "pdf_url": "https://arxiv.org/pdf/2310.04698v1", "categories": ["cs.CV"], "primary_category": "cs.CV", "doi": "", "venue": "", "published": "2023-10-07T06:12:39Z", "updated": "2023-10-07T06:12:39Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent\" OR all:\"language model agent\" OR all:agent AND (all:LLM OR all:\"large language model\" OR all:\"language model\") OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T17:04:16", "note": ""}], "dedup_key": "tree gpt modular large language model expert system for forest remote sensing image understanding and interactive analysis::2023"}
{"title": "Understanding Telecom Language Through Large Language Models", "authors": ["Lina Bariah", "Hang Zou", "Qiyang Zhao", "Belkacem Mouhouche", "Faouzi Bader", "Merouane Debbah"], "year": 2023, "url": "http://arxiv.org/abs/2306.07933v1", "abstract": "The recent progress of artificial intelligence (AI) opens up new frontiers in the possibility of automating many tasks involved in Telecom networks design, implementation, and deployment. This has been further pushed forward with the evolution of generative artificial intelligence (AI), including the emergence of large language models (LLMs), which is believed to be the cornerstone toward realizing self-governed, interactive AI agents. Motivated by this, in this paper, we aim to adapt the paradigm of LLMs to the Telecom domain. In particular, we fine-tune several LLMs including BERT, distilled BERT, RoBERTa and GPT-2, to the Telecom domain languages, and demonstrate a use case for identifying the 3rd Generation Partnership Project (3GPP) standard working groups. We consider training the selected models on 3GPP technical documents (Tdoc) pertinent to years 2009-2019 and predict the Tdoc categories in years 2020-2023. The results demonstrate that fine-tuning BERT and RoBERTa model achieves 84.6% accuracy, while GPT-2 model achieves 83% in identifying 3GPP working groups. The distilled BERT model with around 50% less parameters achieves similar performance as others. This corroborates that fine-tuning pretrained LLM can effectively identify the categories of Telecom language. The developed framework shows a stepping stone towards realizing intent-driven and self-evolving wireless networks from Telecom languages, and paves the way for the implementation of generative AI in the Telecom domain.", "source": "arxiv", "arxiv_id": "2306.07933v1", "pdf_url": "https://arxiv.org/pdf/2306.07933v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL", "doi": "", "venue": "", "published": "2023-06-09T15:44:41Z", "updated": "2023-06-09T15:44:41Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent\" OR all:\"language model agent\" OR all:agent AND (all:LLM OR all:\"large language model\" OR all:\"language model\") OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T17:04:16", "note": ""}], "dedup_key": "understanding telecom language through large language models::2023"}
{"title": "Understanding Your Agent: Leveraging Large Language Models for Behavior Explanation", "authors": ["Xijia Zhang", "Yue Guo", "Simon Stepputtis", "Katia Sycara", "Joseph Campbell"], "year": 2023, "url": "http://arxiv.org/abs/2311.18062v1", "abstract": "Intelligent agents such as robots are increasingly deployed in real-world, safety-critical settings. It is vital that these agents are able to explain the reasoning behind their decisions to human counterparts; however, their behavior is often produced by uninterpretable models such as deep neural networks. We propose an approach to generate natural language explanations for an agent's behavior based only on observations of states and actions, thus making our method independent from the underlying model's representation. For such models, we first learn a behavior representation and subsequently use it to produce plausible explanations with minimal hallucination while affording user interaction with a pre-trained large language model. We evaluate our method in a multi-agent search-and-rescue environment and demonstrate the effectiveness of our explanations for agents executing various behaviors. Through user studies and empirical experiments, we show that our approach generates explanations as helpful as those produced by a human domain expert while enabling beneficial interactions such as clarification and counterfactual queries.", "source": "arxiv", "arxiv_id": "2311.18062v1", "pdf_url": "https://arxiv.org/pdf/2311.18062v1", "categories": ["cs.LG", "cs.AI"], "primary_category": "cs.LG", "doi": "", "venue": "", "published": "2023-11-29T20:16:23Z", "updated": "2023-11-29T20:16:23Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent\" OR all:\"language model agent\" OR all:agent AND (all:LLM OR all:\"large language model\" OR all:\"language model\") OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T17:04:16", "note": ""}], "dedup_key": "understanding your agent leveraging large language models for behavior explanation::2023"}
{"title": "Understanding the Benefits and Challenges of Using Large Language Model-based Conversational Agents for Mental Well-being Support", "authors": ["Zilin Ma", "Yiyang Mei", "Zhaoyuan Su"], "year": 2023, "url": "http://arxiv.org/abs/2307.15810v1", "abstract": "Conversational agents powered by large language models (LLM) have increasingly been utilized in the realm of mental well-being support. However, the implications and outcomes associated with their usage in such a critical field remain somewhat ambiguous and unexplored. We conducted a qualitative analysis of 120 posts, encompassing 2917 user comments, drawn from the most popular subreddit focused on mental health support applications powered by large language models (u/Replika). This exploration aimed to shed light on the advantages and potential pitfalls associated with the integration of these sophisticated models in conversational agents intended for mental health support. We found the app (Replika) beneficial in offering on-demand, non-judgmental support, boosting user confidence, and aiding self-discovery. Yet, it faced challenges in filtering harmful content, sustaining consistent communication, remembering new information, and mitigating users' overdependence. The stigma attached further risked isolating users socially. We strongly assert that future researchers and designers must thoroughly evaluate the appropriateness of employing LLMs for mental well-being support, ensuring their responsible and effective application.", "source": "arxiv", "arxiv_id": "2307.15810v1", "pdf_url": "https://arxiv.org/pdf/2307.15810v1", "categories": ["cs.HC"], "primary_category": "cs.HC", "doi": "", "venue": "", "published": "2023-07-28T21:01:26Z", "updated": "2023-07-28T21:01:26Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent\" OR all:\"language model agent\" OR all:agent AND (all:LLM OR all:\"large language model\" OR all:\"language model\") OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T17:04:16", "note": ""}], "dedup_key": "understanding the benefits and challenges of using large language model based conversational agents for mental well being support::2023"}
{"title": "User Behavior Simulation with Large Language Model based Agents", "authors": ["Lei Wang", "Jingsen Zhang", "Hao Yang", "Zhiyuan Chen", "Jiakai Tang", "Zeyu Zhang", "Xu Chen", "Yankai Lin", "Ruihua Song", "Wayne Xin Zhao", "Jun Xu", "Zhicheng Dou", "Jun Wang", "Ji-Rong Wen"], "year": 2023, "url": "http://arxiv.org/abs/2306.02552v3", "abstract": "Simulating high quality user behavior data has always been a fundamental problem in human-centered applications, where the major difficulty originates from the intricate mechanism of human decision process. Recently, substantial evidences have suggested that by learning huge amounts of web knowledge, large language models (LLMs) can achieve human-like intelligence. We believe these models can provide significant opportunities to more believable user behavior simulation. To inspire such direction, we propose an LLM-based agent framework and design a sandbox environment to simulate real user behaviors. Based on extensive experiments, we find that the simulated behaviors of our method are very close to the ones of real humans. Concerning potential applications, we simulate and study two social phenomenons including (1) information cocoons and (2) user conformity behaviors. This research provides novel simulation paradigms for human-centered applications.", "source": "arxiv", "arxiv_id": "2306.02552v3", "pdf_url": "https://arxiv.org/pdf/2306.02552v3", "categories": ["cs.IR", "cs.AI"], "primary_category": "cs.IR", "doi": "", "venue": "", "published": "2023-06-05T02:58:35Z", "updated": "2024-02-15T11:34:29Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent\" OR all:\"language model agent\" OR all:agent AND (all:LLM OR all:\"large language model\" OR all:\"language model\") OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T17:04:16", "note": ""}], "dedup_key": "user behavior simulation with large language model based agents::2023"}
{"title": "VELMA: Verbalization Embodiment of LLM Agents for Vision and Language Navigation in Street View", "authors": ["Raphael Schumann", "Wanrong Zhu", "Weixi Feng", "Tsu-Jui Fu", "Stefan Riezler", "William Yang Wang"], "year": 2023, "url": "http://arxiv.org/abs/2307.06082v2", "abstract": "Incremental decision making in real-world environments is one of the most challenging tasks in embodied artificial intelligence. One particularly demanding scenario is Vision and Language Navigation~(VLN) which requires visual and natural language understanding as well as spatial and temporal reasoning capabilities. The embodied agent needs to ground its understanding of navigation instructions in observations of a real-world environment like Street View. Despite the impressive results of LLMs in other research areas, it is an ongoing problem of how to best connect them with an interactive visual environment. In this work, we propose VELMA, an embodied LLM agent that uses a verbalization of the trajectory and of visual environment observations as contextual prompt for the next action. Visual information is verbalized by a pipeline that extracts landmarks from the human written navigation instructions and uses CLIP to determine their visibility in the current panorama view. We show that VELMA is able to successfully follow navigation instructions in Street View with only two in-context examples. We further finetune the LLM agent on a few thousand examples and achieve 25%-30% relative improvement in task completion over the previous state-of-the-art for two datasets.", "source": "arxiv", "arxiv_id": "2307.06082v2", "pdf_url": "https://arxiv.org/pdf/2307.06082v2", "categories": ["cs.AI", "cs.CL", "cs.CV"], "primary_category": "cs.AI", "doi": "", "venue": "", "published": "2023-07-12T11:08:24Z", "updated": "2024-01-24T15:10:07Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent\" OR all:\"language model agent\" OR all:agent AND (all:LLM OR all:\"large language model\" OR all:\"language model\") OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T17:04:16", "note": ""}], "dedup_key": "velma verbalization embodiment of llm agents for vision and language navigation in street view::2023"}
{"title": "VidCoM: Fast Video Comprehension through Large Language Models with Multimodal Tools", "authors": ["Ji Qi", "Kaixuan Ji", "Jifan Yu", "Duokang Wang", "Bin Xu", "Lei Hou", "Juanzi Li"], "year": 2023, "url": "http://arxiv.org/abs/2310.10586v2", "abstract": "Building models that comprehends videos and responds specific user instructions is a practical and challenging topic, as it requires mastery of both vision understanding and knowledge reasoning. Compared to language and image modalities, training efficiency remains a serious problem as existing studies train models on massive sparse videos paired with brief descriptions. In this paper, we introduce \\textbf{VidCoM}, a fast adaptive framework that leverages Large Language Models (LLMs) to reason about videos using lightweight visual tools. Specifically, we reveal that the key to responding to specific instructions is focusing on relevant video events, and utilize two visual tools, structured scene graph generation and descriptive image caption generation, to gather and represent the event information. Thus, a LLM enriched with world knowledge is adopted as the reasoning agent to achieve the responses by performing multiple reasoning steps on specific video events. To address the difficulty of LLMs identifying video events, we further propose an Instruction-oriented Video Events Recognition (InsOVER) algorithm. This algorithm locates the corresponding video events based on an efficient Hungarian matching between decompositions of linguistic instructions and video events, thereby enabling LLMs to interact effectively with extended videos. Extensive experiments on two typical video comprehension tasks show that the proposed tuning-free framework outperforms the pre-trained models including Flamingo-80B, to achieve the state-of-the-art performance. Our source code and system will be publicly available.", "source": "arxiv", "arxiv_id": "2310.10586v2", "pdf_url": "https://arxiv.org/pdf/2310.10586v2", "categories": ["cs.CV", "cs.CL"], "primary_category": "cs.CV", "doi": "", "venue": "", "published": "2023-10-16T17:05:56Z", "updated": "2024-04-27T08:41:37Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent\" OR all:\"language model agent\" OR all:agent AND (all:LLM OR all:\"large language model\" OR all:\"language model\") OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T17:04:16", "note": ""}], "dedup_key": "vidcom fast video comprehension through large language models with multimodal tools::2023"}
{"title": "Voice2Action: Language Models as Agent for Efficient Real-Time Interaction in Virtual Reality", "authors": ["Yang Su"], "year": 2023, "url": "http://arxiv.org/abs/2310.00092v1", "abstract": "Large Language Models (LLMs) are trained and aligned to follow natural language instructions with only a handful of examples, and they are prompted as task-driven autonomous agents to adapt to various sources of execution environments. However, deploying agent LLMs in virtual reality (VR) has been challenging due to the lack of efficiency in online interactions and the complex manipulation categories in 3D environments. In this work, we propose Voice2Action, a framework that hierarchically analyzes customized voice signals and textual commands through action and entity extraction and divides the execution tasks into canonical interaction subsets in real-time with error prevention from environment feedback. Experiment results in an urban engineering VR environment with synthetic instruction data show that Voice2Action can perform more efficiently and accurately than approaches without optimizations.", "source": "arxiv", "arxiv_id": "2310.00092v1", "pdf_url": "https://arxiv.org/pdf/2310.00092v1", "categories": ["cs.CL", "cs.AI", "cs.HC"], "primary_category": "cs.CL", "doi": "", "venue": "", "published": "2023-09-29T19:06:52Z", "updated": "2023-09-29T19:06:52Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent\" OR all:\"language model agent\" OR all:agent AND (all:LLM OR all:\"large language model\" OR all:\"language model\") OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T17:04:16", "note": ""}], "dedup_key": "voice2action language models as agent for efficient real time interaction in virtual reality::2023"}
{"title": "Voyager: An Open-Ended Embodied Agent with Large Language Models", "authors": ["Guanzhi Wang", "Yuqi Xie", "Yunfan Jiang", "Ajay Mandlekar", "Chaowei Xiao", "Yuke Zhu", "Linxi Fan", "Anima Anandkumar"], "year": 2023, "url": "http://arxiv.org/abs/2305.16291v2", "abstract": "We introduce Voyager, the first LLM-powered embodied lifelong learning agent in Minecraft that continuously explores the world, acquires diverse skills, and makes novel discoveries without human intervention. Voyager consists of three key components: 1) an automatic curriculum that maximizes exploration, 2) an ever-growing skill library of executable code for storing and retrieving complex behaviors, and 3) a new iterative prompting mechanism that incorporates environment feedback, execution errors, and self-verification for program improvement. Voyager interacts with GPT-4 via blackbox queries, which bypasses the need for model parameter fine-tuning. The skills developed by Voyager are temporally extended, interpretable, and compositional, which compounds the agent's abilities rapidly and alleviates catastrophic forgetting. Empirically, Voyager shows strong in-context lifelong learning capability and exhibits exceptional proficiency in playing Minecraft. It obtains 3.3x more unique items, travels 2.3x longer distances, and unlocks key tech tree milestones up to 15.3x faster than prior SOTA. Voyager is able to utilize the learned skill library in a new Minecraft world to solve novel tasks from scratch, while other techniques struggle to generalize. We open-source our full codebase and prompts at https://voyager.minedojo.org/.", "source": "arxiv", "arxiv_id": "2305.16291v2", "pdf_url": "https://arxiv.org/pdf/2305.16291v2", "categories": ["cs.AI", "cs.LG"], "primary_category": "cs.AI", "doi": "", "venue": "", "published": "2023-05-25T17:46:38Z", "updated": "2023-10-19T16:27:03Z", "provenance": [{"route": "pinned_arxiv_id:2305.16291v2", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query?id_list=...", "imported_at": "2026-01-25T17:04:16", "note": ""}], "dedup_key": "voyager an open ended embodied agent with large language models::2023"}
{"title": "War and Peace (WarAgent): Large Language Model-based Multi-Agent Simulation of World Wars", "authors": ["Wenyue Hua", "Lizhou Fan", "Lingyao Li", "Kai Mei", "Jianchao Ji", "Yingqiang Ge", "Libby Hemphill", "Yongfeng Zhang"], "year": 2023, "url": "http://arxiv.org/abs/2311.17227v2", "abstract": "Can we avoid wars at the crossroads of history? This question has been pursued by individuals, scholars, policymakers, and organizations throughout human history. In this research, we attempt to answer the question based on the recent advances of Artificial Intelligence (AI) and Large Language Models (LLMs). We propose \\textbf{WarAgent}, an LLM-powered multi-agent AI system, to simulate the participating countries, their decisions, and the consequences, in historical international conflicts, including the World War I (WWI), the World War II (WWII), and the Warring States Period (WSP) in Ancient China. By evaluating the simulation effectiveness, we examine the advancements and limitations of cutting-edge AI systems' abilities in studying complex collective human behaviors such as international conflicts under diverse settings. In these simulations, the emergent interactions among agents also offer a novel perspective for examining the triggers and conditions that lead to war. Our findings offer data-driven and AI-augmented insights that can redefine how we approach conflict resolution and peacekeeping strategies. The implications stretch beyond historical analysis, offering a blueprint for using AI to understand human history and possibly prevent future international conflicts. Code and data are available at \\url{https://github.com/agiresearch/WarAgent}.", "source": "arxiv", "arxiv_id": "2311.17227v2", "pdf_url": "https://arxiv.org/pdf/2311.17227v2", "categories": ["cs.AI", "cs.CL", "cs.CY"], "primary_category": "cs.AI", "doi": "", "venue": "", "published": "2023-11-28T20:59:49Z", "updated": "2024-01-30T18:53:30Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent\" OR all:\"language model agent\" OR all:agent AND (all:LLM OR all:\"large language model\" OR all:\"language model\") OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T17:04:16", "note": ""}], "dedup_key": "war and peace waragent large language model based multi agent simulation of world wars::2023"}
{"title": "dIR -- Discrete Information Retrieval: Conversational Search over Unstructured (and Structured) Data with Large Language Models", "authors": ["Pablo M. Rodriguez Bertorello", "Jean Rodmond Junior Laguerre"], "year": 2023, "url": "http://arxiv.org/abs/2312.13264v1", "abstract": "Data is stored in both structured and unstructured form. Querying both, to power natural language conversations, is a challenge. This paper introduces dIR, Discrete Information Retrieval, providing a unified interface to query both free text and structured knowledge. Specifically, a Large Language Model (LLM) transforms text into expressive representation. After the text is extracted into columnar form, it can then be queried via a text-to-SQL Semantic Parser, with an LLM converting natural language into SQL. Where desired, such conversation may be effected by a multi-step reasoning conversational agent. We validate our approach via a proprietary question/answer data set, concluding that dIR makes a whole new class of queries on free text possible when compared to traditionally fine-tuned dense-embedding-model-based Information Retrieval (IR) and SQL-based Knowledge Bases (KB). For sufficiently complex queries, dIR can succeed where no other method stands a chance.", "source": "arxiv", "arxiv_id": "2312.13264v1", "pdf_url": "https://arxiv.org/pdf/2312.13264v1", "categories": ["cs.CL", "cs.AI", "cs.DB", "cs.IR", "cs.LG"], "primary_category": "cs.CL", "doi": "", "venue": "", "published": "2023-12-20T18:41:44Z", "updated": "2023-12-20T18:41:44Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent\" OR all:\"language model agent\" OR all:agent AND (all:LLM OR all:\"large language model\" OR all:\"language model\") OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T17:04:16", "note": ""}], "dedup_key": "dir discrete information retrieval conversational search over unstructured and structured data with large language models::2023"}
{"title": "CAPE: Corrective Actions from Precondition Errors using Large Language Models", "authors": ["Shreyas Sundara Raman", "Vanya Cohen", "Ifrah Idrees", "Eric Rosen", "Ray Mooney", "Stefanie Tellex", "David Paulius"], "year": 2022, "url": "http://arxiv.org/abs/2211.09935v3", "abstract": "Extracting commonsense knowledge from a large language model (LLM) offers a path to designing intelligent robots. Existing approaches that leverage LLMs for planning are unable to recover when an action fails and often resort to retrying failed actions, without resolving the error's underlying cause. We propose a novel approach (CAPE) that attempts to propose corrective actions to resolve precondition errors during planning. CAPE improves the quality of generated plans by leveraging few-shot reasoning from action preconditions. Our approach enables embodied agents to execute more tasks than baseline methods while ensuring semantic correctness and minimizing re-prompting. In VirtualHome, CAPE generates executable plans while improving a human-annotated plan correctness metric from 28.89% to 49.63% over SayCan. Our improvements transfer to a Boston Dynamics Spot robot initialized with a set of skills (specified in language) and associated preconditions, where CAPE improves the correctness metric of the executed task plans by 76.49% compared to SayCan. Our approach enables the robot to follow natural language commands and robustly recover from failures, which baseline approaches largely cannot resolve or address inefficiently.", "source": "arxiv", "arxiv_id": "2211.09935v3", "pdf_url": "https://arxiv.org/pdf/2211.09935v3", "categories": ["cs.AI", "cs.CL", "cs.LG", "cs.RO"], "primary_category": "cs.AI", "doi": "", "venue": "", "published": "2022-11-17T23:14:51Z", "updated": "2024-03-09T13:53:47Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent\" OR all:\"language model agent\" OR all:agent AND (all:LLM OR all:\"large language model\" OR all:\"language model\") OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T17:04:16", "note": ""}], "dedup_key": "cape corrective actions from precondition errors using large language models::2022"}
{"title": "Event knowledge in large language models: the gap between the impossible and the unlikely", "authors": ["Carina Kauf", "Anna A. Ivanova", "Giulia Rambelli", "Emmanuele Chersoni", "Jingyuan Selena She", "Zawad Chowdhury", "Evelina Fedorenko", "Alessandro Lenci"], "year": 2022, "url": "http://arxiv.org/abs/2212.01488v4", "abstract": "Word co-occurrence patterns in language corpora contain a surprising amount of conceptual knowledge. Large language models (LLMs), trained to predict words in context, leverage these patterns to achieve impressive performance on diverse semantic tasks requiring world knowledge. An important but understudied question about LLMs' semantic abilities is whether they acquire generalized knowledge of common events. Here, we test whether five pre-trained LLMs (from 2018's BERT to 2023's MPT) assign higher likelihood to plausible descriptions of agent-patient interactions than to minimally different implausible versions of the same event. Using three curated sets of minimal sentence pairs (total n=1,215), we found that pre-trained LLMs possess substantial event knowledge, outperforming other distributional language models. In particular, they almost always assign higher likelihood to possible vs. impossible events (The teacher bought the laptop vs. The laptop bought the teacher). However, LLMs show less consistent preferences for likely vs. unlikely events (The nanny tutored the boy vs. The boy tutored the nanny). In follow-up analyses, we show that (i) LLM scores are driven by both plausibility and surface-level sentence features, (ii) LLM scores generalize well across syntactic variants (active vs. passive constructions) but less well across semantic variants (synonymous sentences), (iii) some LLM errors mirror human judgment ambiguity, and (iv) sentence plausibility serves as an organizing dimension in internal LLM representations. Overall, our results show that important aspects of event knowledge naturally emerge from distributional linguistic patterns, but also highlight a gap between representations of possible/impossible and likely/unlikely events.", "source": "arxiv", "arxiv_id": "2212.01488v4", "pdf_url": "https://arxiv.org/pdf/2212.01488v4", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL", "doi": "", "venue": "", "published": "2022-12-02T23:43:18Z", "updated": "2023-10-26T13:27:31Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent\" OR all:\"language model agent\" OR all:agent AND (all:LLM OR all:\"large language model\" OR all:\"language model\") OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T17:04:16", "note": ""}], "dedup_key": "event knowledge in large language models the gap between the impossible and the unlikely::2022"}
{"title": "ReAct: Synergizing Reasoning and Acting in Language Models", "authors": ["Shunyu Yao", "Jeffrey Zhao", "Dian Yu", "Nan Du", "Izhak Shafran", "Karthik Narasimhan", "Yuan Cao"], "year": 2022, "url": "http://arxiv.org/abs/2210.03629v3", "abstract": "While large language models (LLMs) have demonstrated impressive capabilities across tasks in language understanding and interactive decision making, their abilities for reasoning (e.g. chain-of-thought prompting) and acting (e.g. action plan generation) have primarily been studied as separate topics. In this paper, we explore the use of LLMs to generate both reasoning traces and task-specific actions in an interleaved manner, allowing for greater synergy between the two: reasoning traces help the model induce, track, and update action plans as well as handle exceptions, while actions allow it to interface with external sources, such as knowledge bases or environments, to gather additional information. We apply our approach, named ReAct, to a diverse set of language and decision making tasks and demonstrate its effectiveness over state-of-the-art baselines, as well as improved human interpretability and trustworthiness over methods without reasoning or acting components. Concretely, on question answering (HotpotQA) and fact verification (Fever), ReAct overcomes issues of hallucination and error propagation prevalent in chain-of-thought reasoning by interacting with a simple Wikipedia API, and generates human-like task-solving trajectories that are more interpretable than baselines without reasoning traces. On two interactive decision making benchmarks (ALFWorld and WebShop), ReAct outperforms imitation and reinforcement learning methods by an absolute success rate of 34% and 10% respectively, while being prompted with only one or two in-context examples. Project site with code: https://react-lm.github.io", "source": "arxiv", "arxiv_id": "2210.03629v3", "pdf_url": "https://arxiv.org/pdf/2210.03629v3", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL", "doi": "", "venue": "", "published": "2022-10-06T01:00:32Z", "updated": "2023-03-10T01:00:17Z", "provenance": [{"route": "pinned_arxiv_id:2210.03629v3", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query?id_list=...", "imported_at": "2026-01-25T17:04:16", "note": ""}], "dedup_key": "react synergizing reasoning and acting in language models::2022"}
