# Evidence draft: 3.1 Agent loop and action spaces

## Evidence snippets (with provenance)
- (E-P0047-cb9370ac71) Comprehensive evaluations on multiple benchmarks show that A-MemGuard effectively cuts attack success rates by over 95% while incurring a minimal utility cost. Wei2025Memguard (provenance: paper_notes | papers/paper_notes.jsonl:paper_id=P0047#key_results[0])
- (E-P0013-9d9d60644a) We introduce Structured Cognitive Loop (SCL), a modular architecture that explicitly separates agent cognition into five phases: Retrieval, Cognition, Control, Action, and Memory (R-CCAM). Kim2025Bridging (provenance: paper_notes | papers/paper_notes.jsonl:paper_id=P0013#method)
- (E-P0071-9a8a480e9c) To overcome these limitations, LLM-based human-agent systems (LLM-HAS) incorporate human-provided information, feedback, or control into the agent system to enhance system performance, reliability and safety. Zou2025Based (provenance: paper_notes | papers/paper_notes.jsonl:paper_id=P0071#limitations[1])
- (E-P0037-38a26e4777) Extensive experiments across six benchmarks, covering the diverse scenarios of web, embodied, tool use and game applications, show that AgentSquare substantially outperforms hand-crafted agents, achieving an average performance gain of 17.2% against best-known human designs. Shang2024Agentsquare (provenance: paper_notes | papers/paper_notes.jsonl:paper_id=P0037#key_results[0])
- (E-P0049-904ba35500) Evaluated across a comprehensive set of seven benchmarks spanning embodied, math, web, tool, and game domains, AgentSwift discovers agents that achieve an average performance gain of 8.34\% over both existing automated agent search methods and manually designed agents. Li2025Agentswift (provenance: paper_notes | papers/paper_notes.jsonl:paper_id=P0049#key_results[0])
- (E-P0029-4b027dfb27) We evaluate GiGPO on challenging agent benchmarks, including ALFWorld and WebShop, as well as tool-integrated reasoning on search-augmented QA tasks, using Qwen2.5-1.5B/3B/7B-Instruct. Feng2025Group (provenance: paper_notes | papers/paper_notes.jsonl:paper_id=P0029#key_results[0])
- (E-P0058-8e34a29629) Function Calling showed higher overall attack success rates (73.5% vs 62.59% for MCP), with greater system-centric vulnerability while MCP exhibited increased LLM-centric exposure. Gasmi2025Bridging (provenance: paper_notes | papers/paper_notes.jsonl:paper_id=P0058#key_results[0])
- (E-P0177-acf4a27e30) The system employs tool-augmented reasoning, hierarchical retrieval-augmented generation, forensic-tuned LLMs, and human-in-the-loop feedback to ensure legal and medical validity. Shen2025Feat (provenance: paper_notes | papers/paper_notes.jsonl:paper_id=P0177#key_results[0])
- (E-P0045-7219ca15f4) On evaluation, we examine over 190 benchmark datasets and trace a shift from static exams toward process- and discovery-oriented assessments with advanced evaluation protocols. Hu2025Survey (provenance: paper_notes | papers/paper_notes.jsonl:paper_id=P0045#key_results[1])
- (E-P0124-60cc0d458f) Experiments on challenging agentic benchmarks such as GAIA and BrowseComp+ demonstrate that EvoRoute, when integrated into off-the-shelf agentic systems, not only sustains or enhances system performance but also reduces execution cost by up to $80\%$ and latency by over $70\%$. Zhang2026Evoroute (provenance: paper_notes | papers/paper_notes.jsonl:paper_id=P0124#key_results[0])
- (E-P0203-e294aeefb5) To evaluate MuaLLM, we introduce two custom datasets: RAG-250, targeting retrieval and citation performance, and Reasoning-100 (Reas-100), focused on multistep reasoning in circuit design. Abbineni2025Muallm (provenance: paper_notes | papers/paper_notes.jsonl:paper_id=P0203#key_results[1])
- (E-P0001-ca4a00b5cf) On two interactive decision making benchmarks (ALFWorld and WebShop), ReAct outperforms imitation and reinforcement learning methods by an absolute success rate of 34% and 10% respectively, while being prompted with only one or two in-context examples. Yao2022React (provenance: paper_notes | papers/paper_notes.jsonl:paper_id=P0001#key_results[0])
- (E-P0255-5e37132da5) We conducted comprehensive experiments using a kinase inhibitor dataset, where our multi-agent LLM method outperformed the non-reasoning multi-agent model (GPT-4o mini) by 45% in F1 score (0.514 vs 0.355). Inoue2024Drugagent (provenance: paper_notes | papers/paper_notes.jsonl:paper_id=P0255#key_results[0])
- (E-P0270-764758958d) However, existing benchmarks for evaluating LLM Agents either use static datasets, potentially leading to data leakage or focus only on single-agent scenarios, overlooking the complexities of multi-agent interactions. Chen2024Llmarena (provenance: paper_notes | papers/paper_notes.jsonl:paper_id=P0270#key_results[1])
- (E-P0102-0cc318c2c4) Extensive experiments demonstrate that only using 10K samples for tuning LLaMA-7B can outperform state-of-the-art methods using larger LLMs or more data, on both in-domain and out-domain datasets. Jiang2024Agent (provenance: paper_notes | papers/paper_notes.jsonl:paper_id=P0102#key_results[0])
- (E-P0138-da6dd163cf) We use the recently released AircraftVerse dataset, which is especially suited for developing and evaluating large language models for designs. Samplawski2025Agent (provenance: paper_notes | papers/paper_notes.jsonl:paper_id=P0138#key_results[0])
- (E-P0175-be70dd09e9) Extensive experiments on two real-world KGQA datasets, WebQSP and CWQ, demonstrate that PoG-EGP significantly improves over the baseline PoG system and other compared methods. Xu2025Exemplar (provenance: paper_notes | papers/paper_notes.jsonl:paper_id=P0175#key_results[1])
- (E-P0209-aa504d163c) Results show that ProAgent achieves up to 33.4% higher proactive prediction accuracy, 16.8% higher tool-calling F1 score, and notable improvements in user satisfaction over state-of-the-art baselines, marking a significant step toward proactive assistants. Yang2025Proagent (provenance: paper_notes | papers/paper_notes.jsonl:paper_id=P0209#key_results[0])

## Definitions / setup

- Setup: Which design choices in Agent loop and action spaces drive the major trade-offs, and how are those trade-offs measured? Scope: in-scope: Core topics directly relevant to 'Agent loop and action spaces'.. Axes: evaluation protocol (datasets, metrics, human evaluation); compute and latency constraints; tool interface contract (schemas / protocols); tool selection / routing policy; sandboxing / permissions / observability. Zhang2026Evoroute Kim2025Bridging Feng2025Group

## Claim candidates

- Comprehensive evaluations on multiple benchmarks show that A-MemGuard effectively cuts attack success rates by over 95% while incurring a minimal utility cost. Wei2025Memguard
- We introduce Structured Cognitive Loop (SCL), a modular architecture that explicitly separates agent cognition into five phases: Retrieval, Cognition, Control, Action, and Memory (R-CCAM). Kim2025Bridging
- To overcome these limitations, LLM-based human-agent systems (LLM-HAS) incorporate human-provided information, feedback, or control into the agent system to enhance system performance, reliability and safety. Zou2025Based
- Extensive experiments across six benchmarks, covering the diverse scenarios of web, embodied, tool use and game applications, show that AgentSquare substantially outperforms hand-crafted agents, achieving an average performance gain of 17.2% against best-known human designs. Shang2024Agentsquare
- Evaluated across a comprehensive set of seven benchmarks spanning embodied, math, web, tool, and game domains, AgentSwift discovers agents that achieve an average performance gain of 8.34\% over both existing automated agent search methods and manually designed agents. Li2025Agentswift

## Concrete comparisons

- Axis: evaluation protocol (datasets, metrics, human evaluation); A: Agent frameworks / architectures: `P0124`, `P0013`, `P0029`; B: Planning / reasoning loops: `P0013`, `P0049`, `P0055`. Hu2025Survey Li2025Agentswift Shang2024Agentsquare
  - A highlight: (E-P0045-7219ca15f4) On evaluation, we examine over 190 benchmark datasets and trace a shift from static exams toward process- and discovery-oriented assessments with advanced evaluation protocols. Hu2025Survey (pointer: papers/paper_notes.jsonl:paper_id=P0045#key_results[1])
  - A highlight: (E-P0049-904ba35500) Evaluated across a comprehensive set of seven benchmarks spanning embodied, math, web, tool, and game domains, AgentSwift discovers agents that achieve an average performance gain of 8.34\% over both existing automated agent search methods and manually designed agents. Li2025Agentswift (pointer: papers/paper_notes.jsonl:paper_id=P0049#key_results[0])
  - B highlight: (E-P0037-38a26e4777) Extensive experiments across six benchmarks, covering the diverse scenarios of web, embodied, tool use and game applications, show that AgentSquare substantially outperforms hand-crafted agents, achieving an average performance gain of 17.2% against best-known human designs. Shang2024Agentsquare (pointer: papers/paper_notes.jsonl:paper_id=P0037#key_results[0])
  - B highlight: (E-P0049-904ba35500) Evaluated across a comprehensive set of seven benchmarks spanning embodied, math, web, tool, and game domains, AgentSwift discovers agents that achieve an average performance gain of 8.34\% over both existing automated agent search methods and manually designed agents. Li2025Agentswift (pointer: papers/paper_notes.jsonl:paper_id=P0049#key_results[0])
- Axis: evaluation protocol (datasets, metrics, human evaluation); A: Agent frameworks / architectures: `P0124`, `P0013`, `P0029`; B: Memory / retrieval augmentation: `P0047`, `P0201`, `P0203`. Hu2025Survey Li2025Agentswift Abbineni2025Muallm Wei2025Memguard
  - A highlight: (E-P0045-7219ca15f4) On evaluation, we examine over 190 benchmark datasets and trace a shift from static exams toward process- and discovery-oriented assessments with advanced evaluation protocols. Hu2025Survey (pointer: papers/paper_notes.jsonl:paper_id=P0045#key_results[1])
  - A highlight: (E-P0049-904ba35500) Evaluated across a comprehensive set of seven benchmarks spanning embodied, math, web, tool, and game domains, AgentSwift discovers agents that achieve an average performance gain of 8.34\% over both existing automated agent search methods and manually designed agents. Li2025Agentswift (pointer: papers/paper_notes.jsonl:paper_id=P0049#key_results[0])
  - B highlight: (E-P0203-e294aeefb5) To evaluate MuaLLM, we introduce two custom datasets: RAG-250, targeting retrieval and citation performance, and Reasoning-100 (Reas-100), focused on multistep reasoning in circuit design. Abbineni2025Muallm (pointer: papers/paper_notes.jsonl:paper_id=P0203#key_results[1])
  - B highlight: (E-P0047-cb9370ac71) Comprehensive evaluations on multiple benchmarks show that A-MemGuard effectively cuts attack success rates by over 95% while incurring a minimal utility cost. Wei2025Memguard (pointer: papers/paper_notes.jsonl:paper_id=P0047#key_results[0])
- Axis: evaluation protocol (datasets, metrics, human evaluation); A: Planning / reasoning loops: `P0013`, `P0049`, `P0055`; B: Memory / retrieval augmentation: `P0047`, `P0201`, `P0203`. Shang2024Agentsquare Li2025Agentswift Abbineni2025Muallm Wei2025Memguard
  - A highlight: (E-P0037-38a26e4777) Extensive experiments across six benchmarks, covering the diverse scenarios of web, embodied, tool use and game applications, show that AgentSquare substantially outperforms hand-crafted agents, achieving an average performance gain of 17.2% against best-known human designs. Shang2024Agentsquare (pointer: papers/paper_notes.jsonl:paper_id=P0037#key_results[0])
  - A highlight: (E-P0049-904ba35500) Evaluated across a comprehensive set of seven benchmarks spanning embodied, math, web, tool, and game domains, AgentSwift discovers agents that achieve an average performance gain of 8.34\% over both existing automated agent search methods and manually designed agents. Li2025Agentswift (pointer: papers/paper_notes.jsonl:paper_id=P0049#key_results[0])
  - B highlight: (E-P0203-e294aeefb5) To evaluate MuaLLM, we introduce two custom datasets: RAG-250, targeting retrieval and citation performance, and Reasoning-100 (Reas-100), focused on multistep reasoning in circuit design. Abbineni2025Muallm (pointer: papers/paper_notes.jsonl:paper_id=P0203#key_results[1])
  - B highlight: (E-P0047-cb9370ac71) Comprehensive evaluations on multiple benchmarks show that A-MemGuard effectively cuts attack success rates by over 95% while incurring a minimal utility cost. Wei2025Memguard (pointer: papers/paper_notes.jsonl:paper_id=P0047#key_results[0])
- Axis: compute and latency constraints; A: Agent frameworks / architectures: `P0124`, `P0013`, `P0029`; B: Planning / reasoning loops: `P0013`, `P0049`, `P0055`. Zhang2026Evoroute Wei2025Memguard Shang2024Agentsquare Li2025Agentswift
  - A highlight: (E-P0124-60cc0d458f) Experiments on challenging agentic benchmarks such as GAIA and BrowseComp+ demonstrate that EvoRoute, when integrated into off-the-shelf agentic systems, not only sustains or enhances system performance but also reduces execution cost by up to $80\%$ and latency by over $70\%$. Zhang2026Evoroute (pointer: papers/paper_notes.jsonl:paper_id=P0124#key_results[0])
  - A highlight: (E-P0047-cb9370ac71) Comprehensive evaluations on multiple benchmarks show that A-MemGuard effectively cuts attack success rates by over 95% while incurring a minimal utility cost. Wei2025Memguard (pointer: papers/paper_notes.jsonl:paper_id=P0047#key_results[0])
  - B highlight: (E-P0037-38a26e4777) Extensive experiments across six benchmarks, covering the diverse scenarios of web, embodied, tool use and game applications, show that AgentSquare substantially outperforms hand-crafted agents, achieving an average performance gain of 17.2% against best-known human designs. Shang2024Agentsquare (pointer: papers/paper_notes.jsonl:paper_id=P0037#key_results[0])
  - B highlight: (E-P0049-904ba35500) Evaluated across a comprehensive set of seven benchmarks spanning embodied, math, web, tool, and game domains, AgentSwift discovers agents that achieve an average performance gain of 8.34\% over both existing automated agent search methods and manually designed agents. Li2025Agentswift (pointer: papers/paper_notes.jsonl:paper_id=P0049#key_results[0])
- Axis: compute and latency constraints; A: Agent frameworks / architectures: `P0124`, `P0013`, `P0029`; B: Memory / retrieval augmentation: `P0047`, `P0201`, `P0203`. Zhang2026Evoroute Wei2025Memguard Abbineni2025Muallm
  - A highlight: (E-P0124-60cc0d458f) Experiments on challenging agentic benchmarks such as GAIA and BrowseComp+ demonstrate that EvoRoute, when integrated into off-the-shelf agentic systems, not only sustains or enhances system performance but also reduces execution cost by up to $80\%$ and latency by over $70\%$. Zhang2026Evoroute (pointer: papers/paper_notes.jsonl:paper_id=P0124#key_results[0])
  - A highlight: (E-P0047-cb9370ac71) Comprehensive evaluations on multiple benchmarks show that A-MemGuard effectively cuts attack success rates by over 95% while incurring a minimal utility cost. Wei2025Memguard (pointer: papers/paper_notes.jsonl:paper_id=P0047#key_results[0])
  - B highlight: (E-P0047-cb9370ac71) Comprehensive evaluations on multiple benchmarks show that A-MemGuard effectively cuts attack success rates by over 95% while incurring a minimal utility cost. Wei2025Memguard (pointer: papers/paper_notes.jsonl:paper_id=P0047#key_results[0])
  - B highlight: (E-P0203-e294aeefb5) To evaluate MuaLLM, we introduce two custom datasets: RAG-250, targeting retrieval and citation performance, and Reasoning-100 (Reas-100), focused on multistep reasoning in circuit design. Abbineni2025Muallm (pointer: papers/paper_notes.jsonl:paper_id=P0203#key_results[1])
- Axis: compute and latency constraints; A: Planning / reasoning loops: `P0013`, `P0049`, `P0055`; B: Memory / retrieval augmentation: `P0047`, `P0201`, `P0203`. Shang2024Agentsquare Li2025Agentswift Wei2025Memguard Abbineni2025Muallm
  - A highlight: (E-P0037-38a26e4777) Extensive experiments across six benchmarks, covering the diverse scenarios of web, embodied, tool use and game applications, show that AgentSquare substantially outperforms hand-crafted agents, achieving an average performance gain of 17.2% against best-known human designs. Shang2024Agentsquare (pointer: papers/paper_notes.jsonl:paper_id=P0037#key_results[0])
  - A highlight: (E-P0049-904ba35500) Evaluated across a comprehensive set of seven benchmarks spanning embodied, math, web, tool, and game domains, AgentSwift discovers agents that achieve an average performance gain of 8.34\% over both existing automated agent search methods and manually designed agents. Li2025Agentswift (pointer: papers/paper_notes.jsonl:paper_id=P0049#key_results[0])
  - B highlight: (E-P0047-cb9370ac71) Comprehensive evaluations on multiple benchmarks show that A-MemGuard effectively cuts attack success rates by over 95% while incurring a minimal utility cost. Wei2025Memguard (pointer: papers/paper_notes.jsonl:paper_id=P0047#key_results[0])
  - B highlight: (E-P0203-e294aeefb5) To evaluate MuaLLM, we introduce two custom datasets: RAG-250, targeting retrieval and citation performance, and Reasoning-100 (Reas-100), focused on multistep reasoning in circuit design. Abbineni2025Muallm (pointer: papers/paper_notes.jsonl:paper_id=P0203#key_results[1])
- Axis: tool interface contract (schemas / protocols); A: Agent frameworks / architectures: `P0124`, `P0013`, `P0029`; B: Planning / reasoning loops: `P0013`, `P0049`, `P0055`. Li2025Agentswift Feng2025Group Shang2024Agentsquare
  - A highlight: (E-P0049-904ba35500) Evaluated across a comprehensive set of seven benchmarks spanning embodied, math, web, tool, and game domains, AgentSwift discovers agents that achieve an average performance gain of 8.34\% over both existing automated agent search methods and manually designed agents. Li2025Agentswift (pointer: papers/paper_notes.jsonl:paper_id=P0049#key_results[0])
  - A highlight: (E-P0029-4b027dfb27) We evaluate GiGPO on challenging agent benchmarks, including ALFWorld and WebShop, as well as tool-integrated reasoning on search-augmented QA tasks, using Qwen2.5-1.5B/3B/7B-Instruct. Feng2025Group (pointer: papers/paper_notes.jsonl:paper_id=P0029#key_results[0])
  - B highlight: (E-P0037-38a26e4777) Extensive experiments across six benchmarks, covering the diverse scenarios of web, embodied, tool use and game applications, show that AgentSquare substantially outperforms hand-crafted agents, achieving an average performance gain of 17.2% against best-known human designs. Shang2024Agentsquare (pointer: papers/paper_notes.jsonl:paper_id=P0037#key_results[0])
  - B highlight: (E-P0049-904ba35500) Evaluated across a comprehensive set of seven benchmarks spanning embodied, math, web, tool, and game domains, AgentSwift discovers agents that achieve an average performance gain of 8.34\% over both existing automated agent search methods and manually designed agents. Li2025Agentswift (pointer: papers/paper_notes.jsonl:paper_id=P0049#key_results[0])
- Axis: tool interface contract (schemas / protocols); A: Agent frameworks / architectures: `P0124`, `P0013`, `P0029`; B: Memory / retrieval augmentation: `P0047`, `P0201`, `P0203`. Li2025Agentswift Feng2025Group Abbineni2025Muallm Wei2025Memguard
  - A highlight: (E-P0049-904ba35500) Evaluated across a comprehensive set of seven benchmarks spanning embodied, math, web, tool, and game domains, AgentSwift discovers agents that achieve an average performance gain of 8.34\% over both existing automated agent search methods and manually designed agents. Li2025Agentswift (pointer: papers/paper_notes.jsonl:paper_id=P0049#key_results[0])
  - A highlight: (E-P0029-4b027dfb27) We evaluate GiGPO on challenging agent benchmarks, including ALFWorld and WebShop, as well as tool-integrated reasoning on search-augmented QA tasks, using Qwen2.5-1.5B/3B/7B-Instruct. Feng2025Group (pointer: papers/paper_notes.jsonl:paper_id=P0029#key_results[0])
  - B highlight: (E-P0203-e294aeefb5) To evaluate MuaLLM, we introduce two custom datasets: RAG-250, targeting retrieval and citation performance, and Reasoning-100 (Reas-100), focused on multistep reasoning in circuit design. Abbineni2025Muallm (pointer: papers/paper_notes.jsonl:paper_id=P0203#key_results[1])
  - B highlight: (E-P0047-cb9370ac71) Comprehensive evaluations on multiple benchmarks show that A-MemGuard effectively cuts attack success rates by over 95% while incurring a minimal utility cost. Wei2025Memguard (pointer: papers/paper_notes.jsonl:paper_id=P0047#key_results[0])
- Axis: tool interface contract (schemas / protocols); A: Planning / reasoning loops: `P0013`, `P0049`, `P0055`; B: Memory / retrieval augmentation: `P0047`, `P0201`, `P0203`. Shang2024Agentsquare Li2025Agentswift Abbineni2025Muallm Wei2025Memguard
  - A highlight: (E-P0037-38a26e4777) Extensive experiments across six benchmarks, covering the diverse scenarios of web, embodied, tool use and game applications, show that AgentSquare substantially outperforms hand-crafted agents, achieving an average performance gain of 17.2% against best-known human designs. Shang2024Agentsquare (pointer: papers/paper_notes.jsonl:paper_id=P0037#key_results[0])
  - A highlight: (E-P0049-904ba35500) Evaluated across a comprehensive set of seven benchmarks spanning embodied, math, web, tool, and game domains, AgentSwift discovers agents that achieve an average performance gain of 8.34\% over both existing automated agent search methods and manually designed agents. Li2025Agentswift (pointer: papers/paper_notes.jsonl:paper_id=P0049#key_results[0])
  - B highlight: (E-P0203-e294aeefb5) To evaluate MuaLLM, we introduce two custom datasets: RAG-250, targeting retrieval and citation performance, and Reasoning-100 (Reas-100), focused on multistep reasoning in circuit design. Abbineni2025Muallm (pointer: papers/paper_notes.jsonl:paper_id=P0203#key_results[1])
  - B highlight: (E-P0047-cb9370ac71) Comprehensive evaluations on multiple benchmarks show that A-MemGuard effectively cuts attack success rates by over 95% while incurring a minimal utility cost. Wei2025Memguard (pointer: papers/paper_notes.jsonl:paper_id=P0047#key_results[0])
- Axis: tool selection / routing policy; A: Agent frameworks / architectures: `P0124`, `P0013`, `P0029`; B: Planning / reasoning loops: `P0013`, `P0049`, `P0055`. Li2025Agentswift Feng2025Group Shang2024Agentsquare
  - A highlight: (E-P0049-904ba35500) Evaluated across a comprehensive set of seven benchmarks spanning embodied, math, web, tool, and game domains, AgentSwift discovers agents that achieve an average performance gain of 8.34\% over both existing automated agent search methods and manually designed agents. Li2025Agentswift (pointer: papers/paper_notes.jsonl:paper_id=P0049#key_results[0])
  - A highlight: (E-P0029-4b027dfb27) We evaluate GiGPO on challenging agent benchmarks, including ALFWorld and WebShop, as well as tool-integrated reasoning on search-augmented QA tasks, using Qwen2.5-1.5B/3B/7B-Instruct. Feng2025Group (pointer: papers/paper_notes.jsonl:paper_id=P0029#key_results[0])
  - B highlight: (E-P0037-38a26e4777) Extensive experiments across six benchmarks, covering the diverse scenarios of web, embodied, tool use and game applications, show that AgentSquare substantially outperforms hand-crafted agents, achieving an average performance gain of 17.2% against best-known human designs. Shang2024Agentsquare (pointer: papers/paper_notes.jsonl:paper_id=P0037#key_results[0])
  - B highlight: (E-P0049-904ba35500) Evaluated across a comprehensive set of seven benchmarks spanning embodied, math, web, tool, and game domains, AgentSwift discovers agents that achieve an average performance gain of 8.34\% over both existing automated agent search methods and manually designed agents. Li2025Agentswift (pointer: papers/paper_notes.jsonl:paper_id=P0049#key_results[0])

## Evaluation protocol

- Evaluation mentions include: LLMs, GAIA, EvoRoute, BrowseComp, SCL, CCAM, GPT-4o-powered, ReAct, AutoGPT, ALFWorld. Zhang2026Evoroute Kim2025Bridging Feng2025Group Hu2025Survey
- When comparing results, anchor the paragraph with: task type + metric + constraint (budget, tool access, horizon, or threat model) when stated. Zhang2026Evoroute Kim2025Bridging
- Prefer head-to-head comparisons only when benchmark/metric are shared; otherwise frame differences as protocol-driven rather than method superiority. Zhang2026Evoroute Kim2025Bridging
- Avoid underspecified model/baseline naming; if abstracts omit details, state that the baseline is reported but underspecified instead of guessing. Zhang2026Evoroute Kim2025Bridging
- If a claim relies on a single reported number, pair it with a limitation/caveat from the same evidence so the draft remains conservative. Zhang2026Evoroute Kim2025Bridging
- If budgets or environments differ across papers, treat cross-paper numeric comparison as fragile and prefer qualitative contrasts aligned to the subsection axes. Zhang2026Evoroute Kim2025Bridging

## Failures / limitations

- We formalize this challenge as the \textbf{Agent System Trilemma}: the inherent tension among achieving state-of-the-art performance, minimizing monetary cost, and ensuring rapid task completion. Zhang2026Evoroute
- However, this reliance on memory introduces a critical security risk: an adversary can inject seemingly harmless records into an agent's memory to manipulate its future behavior. Wei2025Memguard
- Comprehensive evaluations on multiple benchmarks show that A-MemGuard effectively cuts attack success rates by over 95% while incurring a minimal utility cost. Wei2025Memguard
- This work shifts LLM memory security from static filtering to a proactive, experience-driven model where defenses strengthen over time. Wei2025Memguard
- Large language model (LLM) agents have demonstrated strong capabilities across diverse domains, yet automated agent design remains a significant challenge. Li2025Agentswift
- To address these limitations, we propose a novel framework, Exemplar-Guided Planning (EGP), which enhances the planning capabilities of LLM agents for KGQA. Xu2025Exemplar
- Still, limitations in their training prevent them from answering accurately in scenarios that could benefit from multiple perspectives. Inoue2024Drugagent
- MPR (i) externalizes reusable corrective knowledge without model weight updates, (ii) enforces domain constraints to reduce unsafe or invalid actions, and (iii) retains the adaptability of language-based reflection. Wu2025Meta

## Verify fields (non-blocking)

- named benchmarks/datasets used
- metrics/human-eval protocol
- compute/training/inference cost
- training data and supervision signal
- baseline choices and ablation evidence
