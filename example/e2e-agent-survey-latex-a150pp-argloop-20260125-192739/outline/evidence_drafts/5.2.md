# Evidence draft: 5.2 Multi-agent coordination

## Evidence snippets (with provenance)
- (E-P0040-96b056b80b) To this end, we propose a multi-agent system with customized communication knowledge and tools for solving communication related tasks using natural language, comprising three components: (1) Multi-agent Data Retrieval (MDR), which employs the condensate and inference agents to refine and summarize communication knowledge from the knowledge base, expanding the knowledge boundaries of LLMs in 6G communications; (2) Multi-agent Collaborative Planning (MCP), which utilizes multiple planning agents to generate feasible solutions for the communication related task from different perspectives based on the retrieved knowledge; (3) Multi-agent Evaluation and Reflecxion (MER), which utilizes the evaluation agent to assess the solutions, and applies the reflexion agent and refinement agent to provide improvement suggestions for current solutions. Jiang2023Large (provenance: paper_notes | papers/paper_notes.jsonl:paper_id=P0040#key_results[0])
- (E-P0012-7cac5db9a9) By analyzing recent advancements and their limitations - such as scalability, real-time response challenges, and agent coordination constraints, we provide a detailed view of the technological landscape. Aratchige2025Llms (provenance: paper_notes | papers/paper_notes.jsonl:paper_id=P0012#limitations[1])
- (E-P0139-8eb9ff221b) Using a corpus of 219 labeled rubrics derived from the GDPVal benchmark, we evaluate ARCANE on challenging tasks requiring multi-step reasoning and tool use. Masters2025Arcane (provenance: paper_notes | papers/paper_notes.jsonl:paper_id=P0139#key_results[0])
- (E-P0194-f2e78c673e) Current frameworks for multi-agent debate are often designed towards tool use, lack integrated evaluation, or provide limited configurability of agent personas, response generators, discussion paradigms, and decision protocols. Becker2025Mallm (provenance: paper_notes | papers/paper_notes.jsonl:paper_id=P0194#key_results[1])
- (E-P0255-5e37132da5) We conducted comprehensive experiments using a kinase inhibitor dataset, where our multi-agent LLM method outperformed the non-reasoning multi-agent model (GPT-4o mini) by 45% in F1 score (0.514 vs 0.355). Inoue2024Drugagent (provenance: paper_notes | papers/paper_notes.jsonl:paper_id=P0255#key_results[0])
- (E-P0149-42512d3cac) We evaluate AutoSCORE on four benchmark datasets from the ASAP benchmark, using both proprietary and open-source LLMs (GPT-4o, LLaMA-3.1-8B, and LLaMA-3.1-70B). Wang2025Autoscore (provenance: paper_notes | papers/paper_notes.jsonl:paper_id=P0149#key_results[0])
- (E-P0080-0e82f75090) We find that 20 of the 48 issues reported by MARVEL pose security vulnerabilities. Collini2025Marvel (provenance: paper_notes | papers/paper_notes.jsonl:paper_id=P0080#key_results[0])
- (E-P0183-6770e6174b) The increasing integration of Industrial IoT (IIoT) exposes critical cyber-physical systems to sophisticated, multi-stage attacks that elude traditional defenses lacking contextual awareness. Xu2025Autonomous (provenance: paper_notes | papers/paper_notes.jsonl:paper_id=P0183#summary_bullets[0])
- (E-P0108-9640816b42) Evaluation across various tool-use benchmarks illustrates that our proposed multi-LLM framework surpasses the traditional single-LLM approach, highlighting its efficacy and advantages in tool learning. Shen2024Small (provenance: paper_notes | papers/paper_notes.jsonl:paper_id=P0108#key_results[1])
- (E-P0159-1ea69fbec3) Validation using 551 GNWT-Agents and Columbia University Speed Dating dataset demonstrates 72% correlation with human attraction patterns, 77.8% match prediction accuracy, and 74% agreement in human validation studies. Ye2025Cognipair (provenance: paper_notes | papers/paper_notes.jsonl:paper_id=P0159#key_results[0])
- (E-P0115-af857798be) We evaluate seven LLMs, quantitatively highlighting a significant capability gap of over threefold between the strongest, GPT o1, and the weakest, Llama-2-70B. Xu2023Magic (provenance: paper_notes | papers/paper_notes.jsonl:paper_id=P0115#key_results[0])
- (E-P0270-764758958d) However, existing benchmarks for evaluating LLM Agents either use static datasets, potentially leading to data leakage or focus only on single-agent scenarios, overlooking the complexities of multi-agent interactions. Chen2024Llmarena (provenance: paper_notes | papers/paper_notes.jsonl:paper_id=P0270#key_results[1])
- (E-P0078-fa4336d046) Systematic literature reviews and meta-analyses are essential for synthesizing research insights, but they remain time-intensive and labor-intensive due to the iterative processes of screening, evaluation, and data extraction. Rouzrokh2025Lattereview (provenance: paper_notes | papers/paper_notes.jsonl:paper_id=P0078#key_results[0])
- (E-P0079-1dd544863c) Our experiments on Communicative Watch-And-Help and ThreeD-World Multi-Agent Transport benchmarks demonstrate that LIET, instantiated with both LLaMA and GPT-4o, outperforms existing baselines and exhibits strong cooperative planning abilities. Li2025Learn (provenance: paper_notes | papers/paper_notes.jsonl:paper_id=P0079#key_results[1])
- (E-P0230-15e523063d) Evaluation metrics include standard classification metrics, quality assessment of reasoning processes, and robustness testing against rewritten content. Cui2025Toward (provenance: paper_notes | papers/paper_notes.jsonl:paper_id=P0230#key_results[1])
- (E-P0212-1167f52f16) In this work, we propose SG^2, an iterative Schema-Guided Scene-Graph reasoning framework based on multi-agent LLMs. Chen2025Schema (provenance: paper_notes | papers/paper_notes.jsonl:paper_id=P0212#key_results[0])
- (E-P0070-0f34c44fa9) To evaluate the system, we propose new performance metrics assessing both individual agents and the system as a whole. Zahedifar2025Agent (provenance: paper_notes | papers/paper_notes.jsonl:paper_id=P0070#key_results[1])
- (E-P0119-02f16b54ff) Extensive experiments on three different types of reasoning tasks show that our collaboration approach delivers superior accuracy across all ten datasets compared to existing methods. Xu2023Towards (provenance: paper_notes | papers/paper_notes.jsonl:paper_id=P0119#key_results[1])

## Definitions / setup

- Setup: Which design choices in Multi-agent coordination drive the major trade-offs, and how are those trade-offs measured? Scope: in-scope: Core topics directly relevant to 'Multi-agent coordination'.. Axes: evaluation protocol (datasets, metrics, human evaluation); compute and latency constraints; tool interface contract (schemas / protocols); tool selection / routing policy; sandboxing / permissions / observability. Aratchige2025Llms Zahedifar2025Agent Rouzrokh2025Lattereview

## Claim candidates

- To this end, we propose a multi-agent system with customized communication knowledge and tools for solving communication related tasks using natural language, comprising three components: (1) Multi-agent Data Retrieval (MDR), which employs the condensate and inference agents to refine and summarize communication knowledge from the knowledge base, expanding the knowledge boundaries of LLMs in 6G Jiang2023Large
- By analyzing recent advancements and their limitations - such as scalability, real-time response challenges, and agent coordination constraints, we provide a detailed view of the technological landscape. Aratchige2025Llms
- Using a corpus of 219 labeled rubrics derived from the GDPVal benchmark, we evaluate ARCANE on challenging tasks requiring multi-step reasoning and tool use. Masters2025Arcane
- Current frameworks for multi-agent debate are often designed towards tool use, lack integrated evaluation, or provide limited configurability of agent personas, response generators, discussion paradigms, and decision protocols. Becker2025Mallm
- We conducted comprehensive experiments using a kinase inhibitor dataset, where our multi-agent LLM method outperformed the non-reasoning multi-agent model (GPT-4o mini) by 45% in F1 score (0.514 vs 0.355). Inoue2024Drugagent

## Concrete comparisons

- Axis: evaluation protocol (datasets, metrics, human evaluation); A: Agent frameworks / architectures: `P0012`, `P0070`, `P0078`; B: Multi-agent coordination: `P0070`, `P0078`, `P0079`. Masters2025Arcane Li2025Learn Wang2025Autoscore
  - A highlight: (E-P0139-8eb9ff221b) Using a corpus of 219 labeled rubrics derived from the GDPVal benchmark, we evaluate ARCANE on challenging tasks requiring multi-step reasoning and tool use. Masters2025Arcane (pointer: papers/paper_notes.jsonl:paper_id=P0139#key_results[0])
  - A highlight: (E-P0079-1dd544863c) Our experiments on Communicative Watch-And-Help and ThreeD-World Multi-Agent Transport benchmarks demonstrate that LIET, instantiated with both LLaMA and GPT-4o, outperforms existing baselines and exhibits strong cooperative planning abilities. Li2025Learn (pointer: papers/paper_notes.jsonl:paper_id=P0079#key_results[1])
  - B highlight: (E-P0149-42512d3cac) We evaluate AutoSCORE on four benchmark datasets from the ASAP benchmark, using both proprietary and open-source LLMs (GPT-4o, LLaMA-3.1-8B, and LLaMA-3.1-70B). Wang2025Autoscore (pointer: papers/paper_notes.jsonl:paper_id=P0149#key_results[0])
  - B highlight: (E-P0139-8eb9ff221b) Using a corpus of 219 labeled rubrics derived from the GDPVal benchmark, we evaluate ARCANE on challenging tasks requiring multi-step reasoning and tool use. Masters2025Arcane (pointer: papers/paper_notes.jsonl:paper_id=P0139#key_results[0])
- Axis: evaluation protocol (datasets, metrics, human evaluation); A: Agent frameworks / architectures: `P0012`, `P0070`, `P0078`; B: Planning / reasoning loops: `P0183`, `P0212`, `P0255`. Masters2025Arcane Li2025Learn Inoue2024Drugagent Xu2023Towards
  - A highlight: (E-P0139-8eb9ff221b) Using a corpus of 219 labeled rubrics derived from the GDPVal benchmark, we evaluate ARCANE on challenging tasks requiring multi-step reasoning and tool use. Masters2025Arcane (pointer: papers/paper_notes.jsonl:paper_id=P0139#key_results[0])
  - A highlight: (E-P0079-1dd544863c) Our experiments on Communicative Watch-And-Help and ThreeD-World Multi-Agent Transport benchmarks demonstrate that LIET, instantiated with both LLaMA and GPT-4o, outperforms existing baselines and exhibits strong cooperative planning abilities. Li2025Learn (pointer: papers/paper_notes.jsonl:paper_id=P0079#key_results[1])
  - B highlight: (E-P0255-5e37132da5) We conducted comprehensive experiments using a kinase inhibitor dataset, where our multi-agent LLM method outperformed the non-reasoning multi-agent model (GPT-4o mini) by 45% in F1 score (0.514 vs 0.355). Inoue2024Drugagent (pointer: papers/paper_notes.jsonl:paper_id=P0255#key_results[0])
  - B highlight: (E-P0119-02f16b54ff) Extensive experiments on three different types of reasoning tasks show that our collaboration approach delivers superior accuracy across all ten datasets compared to existing methods. Xu2023Towards (pointer: papers/paper_notes.jsonl:paper_id=P0119#key_results[1])
- Axis: evaluation protocol (datasets, metrics, human evaluation); A: Multi-agent coordination: `P0070`, `P0078`, `P0079`; B: Planning / reasoning loops: `P0183`, `P0212`, `P0255`. Wang2025Autoscore Masters2025Arcane Inoue2024Drugagent Xu2023Towards
  - A highlight: (E-P0149-42512d3cac) We evaluate AutoSCORE on four benchmark datasets from the ASAP benchmark, using both proprietary and open-source LLMs (GPT-4o, LLaMA-3.1-8B, and LLaMA-3.1-70B). Wang2025Autoscore (pointer: papers/paper_notes.jsonl:paper_id=P0149#key_results[0])
  - A highlight: (E-P0139-8eb9ff221b) Using a corpus of 219 labeled rubrics derived from the GDPVal benchmark, we evaluate ARCANE on challenging tasks requiring multi-step reasoning and tool use. Masters2025Arcane (pointer: papers/paper_notes.jsonl:paper_id=P0139#key_results[0])
  - B highlight: (E-P0255-5e37132da5) We conducted comprehensive experiments using a kinase inhibitor dataset, where our multi-agent LLM method outperformed the non-reasoning multi-agent model (GPT-4o mini) by 45% in F1 score (0.514 vs 0.355). Inoue2024Drugagent (pointer: papers/paper_notes.jsonl:paper_id=P0255#key_results[0])
  - B highlight: (E-P0119-02f16b54ff) Extensive experiments on three different types of reasoning tasks show that our collaboration approach delivers superior accuracy across all ten datasets compared to existing methods. Xu2023Towards (pointer: papers/paper_notes.jsonl:paper_id=P0119#key_results[1])
- Axis: compute and latency constraints; A: Agent frameworks / architectures: `P0012`, `P0070`, `P0078`; B: Multi-agent coordination: `P0070`, `P0078`, `P0079`. Aratchige2025Llms Li2025Learn Rouzrokh2025Lattereview
  - A highlight: (E-P0012-7cac5db9a9) By analyzing recent advancements and their limitations - such as scalability, real-time response challenges, and agent coordination constraints, we provide a detailed view of the technological landscape. Aratchige2025Llms (pointer: papers/paper_notes.jsonl:paper_id=P0012#limitations[1])
  - A highlight: (E-P0079-1dd544863c) Our experiments on Communicative Watch-And-Help and ThreeD-World Multi-Agent Transport benchmarks demonstrate that LIET, instantiated with both LLaMA and GPT-4o, outperforms existing baselines and exhibits strong cooperative planning abilities. Li2025Learn (pointer: papers/paper_notes.jsonl:paper_id=P0079#key_results[1])
  - B highlight: (E-P0079-1dd544863c) Our experiments on Communicative Watch-And-Help and ThreeD-World Multi-Agent Transport benchmarks demonstrate that LIET, instantiated with both LLaMA and GPT-4o, outperforms existing baselines and exhibits strong cooperative planning abilities. Li2025Learn (pointer: papers/paper_notes.jsonl:paper_id=P0079#key_results[1])
  - B highlight: (E-P0078-fa4336d046) Systematic literature reviews and meta-analyses are essential for synthesizing research insights, but they remain time-intensive and labor-intensive due to the iterative processes of screening, evaluation, and data extraction. Rouzrokh2025Lattereview (pointer: papers/paper_notes.jsonl:paper_id=P0078#key_results[0])
- Axis: compute and latency constraints; A: Agent frameworks / architectures: `P0012`, `P0070`, `P0078`; B: Planning / reasoning loops: `P0183`, `P0212`, `P0255`. Aratchige2025Llms Li2025Learn Inoue2024Drugagent Xu2025Autonomous
  - A highlight: (E-P0012-7cac5db9a9) By analyzing recent advancements and their limitations - such as scalability, real-time response challenges, and agent coordination constraints, we provide a detailed view of the technological landscape. Aratchige2025Llms (pointer: papers/paper_notes.jsonl:paper_id=P0012#limitations[1])
  - A highlight: (E-P0079-1dd544863c) Our experiments on Communicative Watch-And-Help and ThreeD-World Multi-Agent Transport benchmarks demonstrate that LIET, instantiated with both LLaMA and GPT-4o, outperforms existing baselines and exhibits strong cooperative planning abilities. Li2025Learn (pointer: papers/paper_notes.jsonl:paper_id=P0079#key_results[1])
  - B highlight: (E-P0255-5e37132da5) We conducted comprehensive experiments using a kinase inhibitor dataset, where our multi-agent LLM method outperformed the non-reasoning multi-agent model (GPT-4o mini) by 45% in F1 score (0.514 vs 0.355). Inoue2024Drugagent (pointer: papers/paper_notes.jsonl:paper_id=P0255#key_results[0])
  - B highlight: (E-P0183-6770e6174b) The increasing integration of Industrial IoT (IIoT) exposes critical cyber-physical systems to sophisticated, multi-stage attacks that elude traditional defenses lacking contextual awareness. Xu2025Autonomous (pointer: papers/paper_notes.jsonl:paper_id=P0183#summary_bullets[0])
- Axis: compute and latency constraints; A: Multi-agent coordination: `P0070`, `P0078`, `P0079`; B: Planning / reasoning loops: `P0183`, `P0212`, `P0255`. Li2025Learn Rouzrokh2025Lattereview Inoue2024Drugagent Xu2025Autonomous
  - A highlight: (E-P0079-1dd544863c) Our experiments on Communicative Watch-And-Help and ThreeD-World Multi-Agent Transport benchmarks demonstrate that LIET, instantiated with both LLaMA and GPT-4o, outperforms existing baselines and exhibits strong cooperative planning abilities. Li2025Learn (pointer: papers/paper_notes.jsonl:paper_id=P0079#key_results[1])
  - A highlight: (E-P0078-fa4336d046) Systematic literature reviews and meta-analyses are essential for synthesizing research insights, but they remain time-intensive and labor-intensive due to the iterative processes of screening, evaluation, and data extraction. Rouzrokh2025Lattereview (pointer: papers/paper_notes.jsonl:paper_id=P0078#key_results[0])
  - B highlight: (E-P0255-5e37132da5) We conducted comprehensive experiments using a kinase inhibitor dataset, where our multi-agent LLM method outperformed the non-reasoning multi-agent model (GPT-4o mini) by 45% in F1 score (0.514 vs 0.355). Inoue2024Drugagent (pointer: papers/paper_notes.jsonl:paper_id=P0255#key_results[0])
  - B highlight: (E-P0183-6770e6174b) The increasing integration of Industrial IoT (IIoT) exposes critical cyber-physical systems to sophisticated, multi-stage attacks that elude traditional defenses lacking contextual awareness. Xu2025Autonomous (pointer: papers/paper_notes.jsonl:paper_id=P0183#summary_bullets[0])
- Axis: tool interface contract (schemas / protocols); A: Agent frameworks / architectures: `P0012`, `P0070`, `P0078`; B: Multi-agent coordination: `P0070`, `P0078`, `P0079`. Masters2025Arcane Li2025Learn
  - A highlight: (E-P0139-8eb9ff221b) Using a corpus of 219 labeled rubrics derived from the GDPVal benchmark, we evaluate ARCANE on challenging tasks requiring multi-step reasoning and tool use. Masters2025Arcane (pointer: papers/paper_notes.jsonl:paper_id=P0139#key_results[0])
  - A highlight: (E-P0079-1dd544863c) Our experiments on Communicative Watch-And-Help and ThreeD-World Multi-Agent Transport benchmarks demonstrate that LIET, instantiated with both LLaMA and GPT-4o, outperforms existing baselines and exhibits strong cooperative planning abilities. Li2025Learn (pointer: papers/paper_notes.jsonl:paper_id=P0079#key_results[1])
  - B highlight: (E-P0139-8eb9ff221b) Using a corpus of 219 labeled rubrics derived from the GDPVal benchmark, we evaluate ARCANE on challenging tasks requiring multi-step reasoning and tool use. Masters2025Arcane (pointer: papers/paper_notes.jsonl:paper_id=P0139#key_results[0])
  - B highlight: (E-P0079-1dd544863c) Our experiments on Communicative Watch-And-Help and ThreeD-World Multi-Agent Transport benchmarks demonstrate that LIET, instantiated with both LLaMA and GPT-4o, outperforms existing baselines and exhibits strong cooperative planning abilities. Li2025Learn (pointer: papers/paper_notes.jsonl:paper_id=P0079#key_results[1])
- Axis: tool interface contract (schemas / protocols); A: Agent frameworks / architectures: `P0012`, `P0070`, `P0078`; B: Planning / reasoning loops: `P0183`, `P0212`, `P0255`. Masters2025Arcane Li2025Learn Chen2025Schema Inoue2024Drugagent
  - A highlight: (E-P0139-8eb9ff221b) Using a corpus of 219 labeled rubrics derived from the GDPVal benchmark, we evaluate ARCANE on challenging tasks requiring multi-step reasoning and tool use. Masters2025Arcane (pointer: papers/paper_notes.jsonl:paper_id=P0139#key_results[0])
  - A highlight: (E-P0079-1dd544863c) Our experiments on Communicative Watch-And-Help and ThreeD-World Multi-Agent Transport benchmarks demonstrate that LIET, instantiated with both LLaMA and GPT-4o, outperforms existing baselines and exhibits strong cooperative planning abilities. Li2025Learn (pointer: papers/paper_notes.jsonl:paper_id=P0079#key_results[1])
  - B highlight: (E-P0212-1167f52f16) In this work, we propose SG^2, an iterative Schema-Guided Scene-Graph reasoning framework based on multi-agent LLMs. Chen2025Schema (pointer: papers/paper_notes.jsonl:paper_id=P0212#key_results[0])
  - B highlight: (E-P0255-5e37132da5) We conducted comprehensive experiments using a kinase inhibitor dataset, where our multi-agent LLM method outperformed the non-reasoning multi-agent model (GPT-4o mini) by 45% in F1 score (0.514 vs 0.355). Inoue2024Drugagent (pointer: papers/paper_notes.jsonl:paper_id=P0255#key_results[0])
- Axis: tool interface contract (schemas / protocols); A: Multi-agent coordination: `P0070`, `P0078`, `P0079`; B: Planning / reasoning loops: `P0183`, `P0212`, `P0255`. Masters2025Arcane Li2025Learn Chen2025Schema Inoue2024Drugagent
  - A highlight: (E-P0139-8eb9ff221b) Using a corpus of 219 labeled rubrics derived from the GDPVal benchmark, we evaluate ARCANE on challenging tasks requiring multi-step reasoning and tool use. Masters2025Arcane (pointer: papers/paper_notes.jsonl:paper_id=P0139#key_results[0])
  - A highlight: (E-P0079-1dd544863c) Our experiments on Communicative Watch-And-Help and ThreeD-World Multi-Agent Transport benchmarks demonstrate that LIET, instantiated with both LLaMA and GPT-4o, outperforms existing baselines and exhibits strong cooperative planning abilities. Li2025Learn (pointer: papers/paper_notes.jsonl:paper_id=P0079#key_results[1])
  - B highlight: (E-P0212-1167f52f16) In this work, we propose SG^2, an iterative Schema-Guided Scene-Graph reasoning framework based on multi-agent LLMs. Chen2025Schema (pointer: papers/paper_notes.jsonl:paper_id=P0212#key_results[0])
  - B highlight: (E-P0255-5e37132da5) We conducted comprehensive experiments using a kinase inhibitor dataset, where our multi-agent LLM method outperformed the non-reasoning multi-agent model (GPT-4o mini) by 45% in F1 score (0.514 vs 0.355). Inoue2024Drugagent (pointer: papers/paper_notes.jsonl:paper_id=P0255#key_results[0])
- Axis: tool selection / routing policy; A: Agent frameworks / architectures: `P0012`, `P0070`, `P0078`; B: Multi-agent coordination: `P0070`, `P0078`, `P0079`. Masters2025Arcane Li2025Learn
  - A highlight: (E-P0139-8eb9ff221b) Using a corpus of 219 labeled rubrics derived from the GDPVal benchmark, we evaluate ARCANE on challenging tasks requiring multi-step reasoning and tool use. Masters2025Arcane (pointer: papers/paper_notes.jsonl:paper_id=P0139#key_results[0])
  - A highlight: (E-P0079-1dd544863c) Our experiments on Communicative Watch-And-Help and ThreeD-World Multi-Agent Transport benchmarks demonstrate that LIET, instantiated with both LLaMA and GPT-4o, outperforms existing baselines and exhibits strong cooperative planning abilities. Li2025Learn (pointer: papers/paper_notes.jsonl:paper_id=P0079#key_results[1])
  - B highlight: (E-P0139-8eb9ff221b) Using a corpus of 219 labeled rubrics derived from the GDPVal benchmark, we evaluate ARCANE on challenging tasks requiring multi-step reasoning and tool use. Masters2025Arcane (pointer: papers/paper_notes.jsonl:paper_id=P0139#key_results[0])
  - B highlight: (E-P0079-1dd544863c) Our experiments on Communicative Watch-And-Help and ThreeD-World Multi-Agent Transport benchmarks demonstrate that LIET, instantiated with both LLaMA and GPT-4o, outperforms existing baselines and exhibits strong cooperative planning abilities. Li2025Learn (pointer: papers/paper_notes.jsonl:paper_id=P0079#key_results[1])

## Evaluation protocol

- Evaluation mentions include: ReAct, LLM-Agent-Controller, RAG, LLMs, LatteReview, GitHub, LLM-based, LIET, LLaMA, GPT-4o. Aratchige2025Llms Zahedifar2025Agent Rouzrokh2025Lattereview Li2025Learn
- When comparing results, anchor the paragraph with: task type + metric + constraint (budget, tool access, horizon, or threat model) when stated. Aratchige2025Llms Zahedifar2025Agent
- Prefer head-to-head comparisons only when benchmark/metric are shared; otherwise frame differences as protocol-driven rather than method superiority. Aratchige2025Llms Zahedifar2025Agent
- Avoid underspecified model/baseline naming; if abstracts omit details, state that the baseline is reported but underspecified instead of guessing. Aratchige2025Llms Zahedifar2025Agent
- If a claim relies on a single reported number, pair it with a limitation/caveat from the same evidence so the draft remains conservative. Aratchige2025Llms Zahedifar2025Agent
- If budgets or environments differ across papers, treat cross-paper numeric comparison as fragile and prefer qualitative contrasts aligned to the subsection axes. Aratchige2025Llms Zahedifar2025Agent

## Failures / limitations

- By analyzing recent advancements and their limitations - such as scalability, real-time response challenges, and agent coordination constraints, we provide a detailed view of the technological landscape. Aratchige2025Llms
- We address this limitation by introducing a framework that enables LLM agents to learn and evolve both before and during test time, equipping them with environment-relevant knowledge for better planning and enhanced communication for improved cooperation. Li2025Learn
- Hardware security verification is a challenging and time-consuming task. Collini2025Marvel
- MARVEL mimics the cognitive process of a designer looking for security vulnerabilities in RTL code. Collini2025Marvel
- It consists of a supervisor agent that devises the security policy of the system-on-chips (SoCs) using its security documentation. Collini2025Marvel
- It delegates tasks to validate the security policy to individual executor agents. Collini2025Marvel
- Each executor agent may use one or more tools to identify potential security bugs in the design and send the results back to the supervisor agent for further analysis and confirmation. Collini2025Marvel
- We find that 20 of the 48 issues reported by MARVEL pose security vulnerabilities. Collini2025Marvel

## Verify fields (non-blocking)

- named benchmarks/datasets used
- metrics/human-eval protocol
- compute/training/inference cost
- training data and supervision signal
- baseline choices and ablation evidence
