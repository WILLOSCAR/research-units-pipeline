**Index Table 1. Subsection map (axes + representative works).**

| Subsection | Axes | Representative works |
|---|---|---|
| 3.1 Agent loop and action spaces | evaluation protocol (datasets, metrics, human evaluation)<br>compute and latency constraints<br>tool interface contract (schemas / protocols)<br>tool selection / routing policy<br>sandboxing / permissions / observability | [@Wei2025Memguard; @Kim2025Bridging; @Zou2025Based; @Shang2024Agentsquare; @Li2025Agentswift] |
| 3.2 Tool interfaces and orchestration | evaluation protocol (datasets, metrics, human evaluation)<br>compute and latency constraints<br>tool interface contract (schemas / protocols)<br>tool selection / routing policy<br>sandboxing / permissions / observability | [@Li2025Dissonances; @Zhang2025Tool; @Luo2025Universe; @Zhou2026Beyond; @Zhou2025Self] |
| 4.1 Planning and reasoning loops | evaluation protocol (datasets, metrics, human evaluation)<br>compute and latency constraints<br>tool interface contract (schemas / protocols)<br>tool selection / routing policy<br>sandboxing / permissions / observability | [@Hu2025Training; @Lai2025Ustbench; @Wang2025Automated; @Chen2024Steering; @Yao2022React] |
| 4.2 Memory and retrieval (RAG) | evaluation protocol (datasets, metrics, human evaluation)<br>compute and latency constraints<br>tool interface contract (schemas / protocols)<br>tool selection / routing policy<br>sandboxing / permissions / observability | [@Wei2025Memguard; @Kang2025Distilling; @Ge2025Surveya; @Zhang2025Largea; @Du2025Memr] |
| 5.1 Self-improvement and adaptation | evaluation protocol (datasets, metrics, human evaluation)<br>compute and latency constraints<br>communication protocol / roles<br>aggregation (vote / debate / referee)<br>stability / robustness | [@Zhang2025Agentic; @Zhang2025Security; @Li2025Learn; @Zhou2025Self; @Chen2025Largea] |
| 5.2 Multi-agent coordination | evaluation protocol (datasets, metrics, human evaluation)<br>compute and latency constraints<br>tool interface contract (schemas / protocols)<br>tool selection / routing policy<br>sandboxing / permissions / observability | [@Jiang2023Large; @Aratchige2025Llms; @Masters2025Arcane; @Becker2025Mallm; @Inoue2024Drugagent] |
| 6.1 Benchmarks and evaluation protocols | evaluation protocol (datasets, metrics, human evaluation)<br>compute and latency constraints<br>tool interface contract (schemas / protocols)<br>tool selection / routing policy<br>sandboxing / permissions / observability | [@Lin2023Agentsims; @Kim2026Beyond; @Seo2025Simuhome; @Shi2025Progent; @Shang2024Agentsquare] |
| 6.2 Safety, security, and governance | evaluation protocol (datasets, metrics, human evaluation)<br>compute and latency constraints<br>tool interface contract (schemas / protocols)<br>tool selection / routing policy<br>sandboxing / permissions / observability | [@Zhang2025Security; @Luo2025Universe; @Shi2025Progent; @Wang2025Agentvigil; @Weng2025Bridgescope] |

**Index Table 2. Concrete anchors (benchmarks / numbers / caveats).**

| Subsection | Anchor facts | Representative works |
|---|---|---|
| 3.1 Agent loop and action spaces | On evaluation, we examine over 190 benchmark datasets and trace a shift from static exams toward process- and discovery-oriented a<br>Evaluated across a comprehensive set of seven benchmarks spanning embodied, math, web, tool, and game domains, AgentSwift discover<br>Extensive experiments across six benchmarks, covering the diverse scenarios of web, embodied, tool use and game applications, show | [@Hu2025Survey; @Li2025Agentswift; @Shang2024Agentsquare] |
| 3.2 Tool interfaces and orchestration | Evaluating each MemTool mode across 13+ LLMs on the ScaleMCP benchmark, we conducted experiments over 100 consecutive user interac<br>Furthermore, we evaluated current benchmarks and assessment protocols and have provided an analysis of 68 publicly available datas<br>We find AvaTaR consistently outperforms state-of-the-art approaches across all seven tasks, exhibiting strong generalization abili | [@Lumer2025Memtool; @Chowa2025From; @Wu2024Avatar] |
| 4.1 Planning and reasoning loops | Experimental evaluation on the complex task planning benchmark demonstrates that our 1.5B parameter model trained with single-turn<br>From this observation, we build memory retrieval as an autonomous, accurate, and compatible agent system, named MemR$^3$, which ha<br>Experimental evaluation on the complex task planning benchmark demonstrates that our 1.5B parameter model trained with single-turn | [@Hu2025Training; @Du2025Memr] |
| 4.2 Memory and retrieval (RAG) | From this observation, we build memory retrieval as an autonomous, accurate, and compatible agent system, named MemR$^3$, which ha<br>On two interactive decision making benchmarks (ALFWorld and WebShop), ReAct outperforms imitation and reinforcement learning metho<br>Comprehensive evaluations on multiple benchmarks show that A-MemGuard effectively cuts attack success rates by over 95% while incu | [@Du2025Memr; @Yao2022React; @Wei2025Memguard] |
| 5.1 Self-improvement and adaptation | Across agent and domain-specific benchmarks, ACE optimizes contexts both offline (e.g., system prompts) and online (e.g., agent me<br>Across agent and domain-specific benchmarks, ACE optimizes contexts both offline (e.g., system prompts) and online (e.g., agent me<br>We present MSB (MCP Security Benchmark), the first end-to-end evaluation suite that systematically measures how well LLM agents re | [@Zhang2025Agentic; @Zhang2025Security] |
| 5.2 Multi-agent coordination | Using a corpus of 219 labeled rubrics derived from the GDPVal benchmark, we evaluate ARCANE on challenging tasks requiring multi-s<br>Our experiments on Communicative Watch-And-Help and ThreeD-World Multi-Agent Transport benchmarks demonstrate that LIET, instantia<br>We evaluate AutoSCORE on four benchmark datasets from the ASAP benchmark, using both proprietary and open-source LLMs (GPT-4o, LLa | [@Masters2025Arcane; @Li2025Learn; @Wang2025Autoscore] |
| 6.1 Benchmarks and evaluation protocols | Experimental results demonstrate that API-based models outperform open-sourced models on all metrics and Deepseek-Coder-33B-Instru<br>We evaluate AutoSCORE on four benchmark datasets from the ASAP benchmark, using both proprietary and open-source LLMs (GPT-4o, LLa<br>Experiments on challenging agentic benchmarks such as GAIA and BrowseComp+ demonstrate that EvoRoute, when integrated into off-the | [@Zhang2025Datascibench; @Wang2025Autoscore; @Zhang2026Evoroute] |
| 6.2 Safety, security, and governance | Function Calling showed higher overall attack success rates (73.5% vs 62.59% for MCP), with greater system-centric vulnerability w<br>MSB contributes: (1) a taxonomy of 12 attacks including name-collision, preference manipulation, prompt injections embedded in too<br>Evaluations on two novel benchmarks demonstrate that BridgeScope enables LLM agents to operate databases more effectively, reduces | [@Gasmi2025Bridging; @Zhang2025Security; @Weng2025Bridgescope] |
