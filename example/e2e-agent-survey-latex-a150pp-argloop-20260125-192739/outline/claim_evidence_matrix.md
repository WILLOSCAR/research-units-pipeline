# Claim–Evidence matrix

This artifact is bullets-only and is meant to make evidence explicit before writing.

Generated as a projection of `outline/evidence_drafts.jsonl` (evidence packs).

## 3.1 Agent loop and action spaces

- RQ: Which design choices in Agent loop and action spaces drive the major trade-offs, and how are those trade-offs measured?
- Claim: Comprehensive evaluations on multiple benchmarks show that A-MemGuard effectively cuts attack success rates by over 95% while incurring a minimal utility cost.
  - Axes: evaluation protocol (datasets, metrics, human evaluation); compute and latency constraints; tool interface contract (schemas / protocols); tool selection / routing policy; sandboxing / permissions / observability
  - Evidence levels: fulltext=0, abstract=28, title=0.
  - Evidence: `P0047` [@Wei2025Memguard] — Comprehensive evaluations on multiple benchmarks show that A-MemGuard effectively cuts attack success rates by over 95% while incurring a minimal utility cost. (provenance: paper_notes | papers/paper_notes.jsonl:paper_id=P0047#key_results[0])
  - Evidence: `P0013` [@Kim2025Bridging] — We introduce Structured Cognitive Loop (SCL), a modular architecture that explicitly separates agent cognition into five phases: Retrieval, Cognition, Control, Action, and Memory (R-CCAM). (provenance: paper_notes | papers/paper_notes.jsonl:paper_id=P0013#method)
  - Evidence: `P0071` [@Zou2025Based] — To overcome these limitations, LLM-based human-agent systems (LLM-HAS) incorporate human-provided information, feedback, or control into the agent system to enhance system performance, reliability and safety. (provenance: paper_notes | papers/paper_notes.jsonl:paper_id=P0071#limitations[1])
  - Evidence: `P0037` [@Shang2024Agentsquare] — Extensive experiments across six benchmarks, covering the diverse scenarios of web, embodied, tool use and game applications, show that AgentSquare substantially outperforms hand-crafted agents, achieving an average performance gain of 17.2% against best-known human designs. (provenance: paper_notes | papers/paper_notes.jsonl:paper_id=P0037#key_results[0])
  - Evidence: `P0049` [@Li2025Agentswift] — Evaluated across a comprehensive set of seven benchmarks spanning embodied, math, web, tool, and game domains, AgentSwift discovers agents that achieve an average performance gain of 8.34\% over both existing automated agent search methods and manually designed agents. (provenance: paper_notes | papers/paper_notes.jsonl:paper_id=P0049#key_results[0])
  - Evidence: `P0029` [@Feng2025Group] — We evaluate GiGPO on challenging agent benchmarks, including ALFWorld and WebShop, as well as tool-integrated reasoning on search-augmented QA tasks, using Qwen2.5-1.5B/3B/7B-Instruct. (provenance: paper_notes | papers/paper_notes.jsonl:paper_id=P0029#key_results[0])
  - Caveat: Evidence is not full-text grounded for this subsection; treat claims as provisional and avoid strong generalizations.

## 3.2 Tool interfaces and orchestration

- RQ: Which design choices in Tool interfaces and orchestration drive the major trade-offs, and how are those trade-offs measured?
- Claim: Our evaluation of 66 real-world tools from the repositories of two major LLM agent development frameworks, LangChain and LlamaIndex, revealed a significant security concern: 75% are vulnerable to XTHP attacks, highlighti
  - Axes: evaluation protocol (datasets, metrics, human evaluation); compute and latency constraints; tool interface contract (schemas / protocols); tool selection / routing policy; sandboxing / permissions / observability
  - Evidence levels: fulltext=0, abstract=28, title=0.
  - Evidence: `P0192` [@Li2025Dissonances] — Our evaluation of 66 real-world tools from the repositories of two major LLM agent development frameworks, LangChain and LlamaIndex, revealed a significant security concern: 75% are vulnerable to XTHP attacks, highlighting the prevalence of this threat. (provenance: paper_notes | papers/paper_notes.jsonl:paper_id=P0192#key_results[0])
  - Evidence: `P0229` [@Zhang2025Tool] — To evaluate different autonomy levels, we propose four LLM paradigms: (1) centralized cooperation, where a single LLM allocates tools to all agents; (2) centralized self-organization, where a central LLM autonomously activates agents while keeping others inactive; (3) decentralized cooperation, where each agent has its own LLM and calls tools based on local information; and (4) self-organization, where a randomly chosen initial agent can request collaboration, activating additional agents via tool calls. (provenance: paper_notes | papers/paper_notes.jsonl:paper_id=P0229#method)
  - Evidence: `P0196` [@Luo2025Universe] — Through extensive evaluation of leading LLMs, we find that even SOTA models such as GPT-5 (43.72%), Grok-4 (33.33%) and Claude-4.0-Sonnet (29.44%) exhibit significant performance limitations. (provenance: paper_notes | papers/paper_notes.jsonl:paper_id=P0196#limitations[1])
  - Evidence: `P0022` [@Zhou2026Beyond] — Across six LLMs on the ToolBench and BFCL benchmarks, our attack expands tasks into trajectories exceeding 60,000 tokens, inflates costs by up to 658x, and raises energy by 100-560x. (provenance: paper_notes | papers/paper_notes.jsonl:paper_id=P0022#key_results[0])
  - Evidence: `P0214` [@Zhou2025Self] — Evaluation on two existing multi-turn tool-use agent benchmarks, M3ToolEval and TauBench, shows the Self-Challenging framework achieves over a two-fold improvement in Llama-3.1-8B-Instruct, despite using only self-generated training data. (provenance: paper_notes | papers/paper_notes.jsonl:paper_id=P0214#key_results[0])
  - Evidence: `P0083` [@Lumer2025Memtool] — Evaluating each MemTool mode across 13+ LLMs on the ScaleMCP benchmark, we conducted experiments over 100 consecutive user interactions, measuring tool removal ratios (short-term memory efficiency) and task completion accuracy. (provenance: paper_notes | papers/paper_notes.jsonl:paper_id=P0083#key_results[0])
  - Caveat: Evidence is not full-text grounded for this subsection; treat claims as provisional and avoid strong generalizations.

## 4.1 Planning and reasoning loops

- RQ: Which design choices in Planning and reasoning loops drive the major trade-offs, and how are those trade-offs measured?
- Claim: Experimental evaluation on the complex task planning benchmark demonstrates that our 1.5B parameter model trained with single-turn GRPO achieves superior performance compared to larger baseline models up to 14B parameter
  - Axes: evaluation protocol (datasets, metrics, human evaluation); compute and latency constraints; tool interface contract (schemas / protocols); tool selection / routing policy; sandboxing / permissions / observability
  - Evidence levels: fulltext=0, abstract=28, title=0.
  - Evidence: `P0089` [@Hu2025Training] — Experimental evaluation on the complex task planning benchmark demonstrates that our 1.5B parameter model trained with single-turn GRPO achieves superior performance compared to larger baseline models up to 14B parameters, with success rates of 70% for long-horizon planning tasks. (provenance: paper_notes | papers/paper_notes.jsonl:paper_id=P0089#key_results[0])
  - Evidence: `P0233` [@Lai2025Ustbench] — To this end, we introduce USTBench, the first benchmark to evaluate LLMs' spatiotemporal reasoning abilities as urban agents across four decomposed dimensions: spatiotemporal understanding, forecasting, planning, and reflection with feedback. (provenance: paper_notes | papers/paper_notes.jsonl:paper_id=P0233#method)
  - Evidence: `P0150` [@Wang2025Automated] — However, a detailed analysis of their testing processes reveals specific strengths and limitations; notably, LLM agents struggle with maintaining coherent long-horizon plans, performing complex reasoning, and effectively utilizing specialized tools. (provenance: paper_notes | papers/paper_notes.jsonl:paper_id=P0150#limitations[1])
  - Evidence: `P0109` [@Chen2024Steering] — While a lot of recent research focuses on enhancing the textual reasoning capabilities of Large Language Models (LLMs) by optimizing the multi-agent framework or reasoning chains, several benchmark tasks can be solved with 100\% success through direct coding, which is more scalable and avoids the computational overhead associated with textual iterating and searching. (provenance: paper_notes | papers/paper_notes.jsonl:paper_id=P0109#key_results[0])
  - Evidence: `P0001` [@Yao2022React] — On two interactive decision making benchmarks (ALFWorld and WebShop), ReAct outperforms imitation and reinforcement learning methods by an absolute success rate of 34% and 10% respectively, while being prompted with only one or two in-context examples. (provenance: paper_notes | papers/paper_notes.jsonl:paper_id=P0001#key_results[0])
  - Evidence: `P0038` [@Shi2024Ehragent] — Experiments on three real-world multi-tabular EHR datasets show that EHRAgent outperforms the strongest baseline by up to 29.6% in success rate. (provenance: paper_notes | papers/paper_notes.jsonl:paper_id=P0038#key_results[0])
  - Caveat: Evidence is not full-text grounded for this subsection; treat claims as provisional and avoid strong generalizations.

## 4.2 Memory and retrieval (RAG)

- RQ: Which design choices in Memory and retrieval (RAG) drive the major trade-offs, and how are those trade-offs measured?
- Claim: Comprehensive evaluations on multiple benchmarks show that A-MemGuard effectively cuts attack success rates by over 95% while incurring a minimal utility cost.
  - Axes: evaluation protocol (datasets, metrics, human evaluation); compute and latency constraints; tool interface contract (schemas / protocols); tool selection / routing policy; sandboxing / permissions / observability
  - Evidence levels: fulltext=0, abstract=28, title=0.
  - Evidence: `P0047` [@Wei2025Memguard] — Comprehensive evaluations on multiple benchmarks show that A-MemGuard effectively cuts attack success rates by over 95% while incurring a minimal utility cost. (provenance: paper_notes | papers/paper_notes.jsonl:paper_id=P0047#key_results[0])
  - Evidence: `P0166` [@Kang2025Distilling] — In this work, we propose Agent Distillation, a framework for transferring not only reasoning capability but full task-solving behavior from LLM-based agents into sLMs with retrieval and code tools. (provenance: paper_notes | papers/paper_notes.jsonl:paper_id=P0166#method)
  - Evidence: `P0220` [@Ge2025Surveya] — Despite the growing prominence of LLMs in this field, there is a scarcity of scholarly works that systematically synthesize how advanced enhancement techniques, specifically Retrieval-Augmented Generation (RAG) and Agentic systems, can be utilized to address these reliability and reasoning limitations. (provenance: paper_notes | papers/paper_notes.jsonl:paper_id=P0220#limitations[1])
  - Evidence: `P0189` [@Zhang2025Largea] — Structural drawings are widely used in many fields, e.g., mechanical engineering, civil engineering, etc. In civil engineering, structural drawings serve as the main communication tool between architects, engineers, and builders to avoid conflicts, act as legal documentation, and provide a reference for future maintenance or evaluation needs. (provenance: paper_notes | papers/paper_notes.jsonl:paper_id=P0189#key_results[0])
  - Evidence: `P0200` [@Du2025Memr] — From this observation, we build memory retrieval as an autonomous, accurate, and compatible agent system, named MemR$^3$, which has two core mechanisms: 1) a router that selects among retrieve, reflect, and answer actions to optimize answer quality; 2) a global evidence-gap tracker that explicitly renders the answering process transparent and tracks the evidence collection process. (provenance: paper_notes | papers/paper_notes.jsonl:paper_id=P0200#key_results[1])
  - Evidence: `P0173` [@Wei2025Memory] — We unify and implement over ten representative memory modules and evaluate them across 10 diverse multi-turn goal-oriented and single-turn reasoning and QA datasets. (provenance: paper_notes | papers/paper_notes.jsonl:paper_id=P0173#key_results[0])
  - Caveat: Evidence is not full-text grounded for this subsection; treat claims as provisional and avoid strong generalizations.

## 5.1 Self-improvement and adaptation

- RQ: Which design choices in Self-improvement and adaptation drive the major trade-offs, and how are those trade-offs measured?
- Claim: Across agent and domain-specific benchmarks, ACE optimizes contexts both offline (e.g., system prompts) and online (e.g., agent memory), consistently outperforming strong baselines: +10.6% on agents and +8.6% on finance,
  - Axes: evaluation protocol (datasets, metrics, human evaluation); compute and latency constraints; communication protocol / roles; aggregation (vote / debate / referee); stability / robustness
  - Evidence levels: fulltext=0, abstract=28, title=0.
  - Evidence: `P0144` [@Zhang2025Agentic] — Across agent and domain-specific benchmarks, ACE optimizes contexts both offline (e.g., system prompts) and online (e.g., agent memory), consistently outperforming strong baselines: +10.6% on agents and +8.6% on finance, while significantly reducing adaptation latency and rollout cost. (provenance: paper_notes | papers/paper_notes.jsonl:paper_id=P0144#key_results[0])
  - Evidence: `P0195` [@Zhang2025Security] — We present MSB (MCP Security Benchmark), the first end-to-end evaluation suite that systematically measures how well LLM agents resist MCP-specific attacks throughout the full tool-use pipeline: task planning, tool invocation, and response handling. (provenance: paper_notes | papers/paper_notes.jsonl:paper_id=P0195#method)
  - Evidence: `P0079` [@Li2025Learn] — We address this limitation by introducing a framework that enables LLM agents to learn and evolve both before and during test time, equipping them with environment-relevant knowledge for better planning and enhanced communication for improved cooperation. (provenance: paper_notes | papers/paper_notes.jsonl:paper_id=P0079#limitations[1])
  - Evidence: `P0214` [@Zhou2025Self] — Evaluation on two existing multi-turn tool-use agent benchmarks, M3ToolEval and TauBench, shows the Self-Challenging framework achieves over a two-fold improvement in Llama-3.1-8B-Instruct, despite using only self-generated training data. (provenance: paper_notes | papers/paper_notes.jsonl:paper_id=P0214#key_results[0])
  - Evidence: `P0190` [@Chen2025Largea] — From the data science perspective, we identify key processes for LLM-based agents, including data preprocessing, model development, evaluation, visualization, etc. Our work offers two key contributions: (1) a comprehensive review of recent developments in applying LLMbased agents to data science tasks; (2) a dual-perspective framework that connects general agent design principles with the practical workflows in data science. (provenance: paper_notes | papers/paper_notes.jsonl:paper_id=P0190#key_results[0])
  - Evidence: `P0087` [@Xia2025Sand] — Evaluating on two representative interactive agent tasks, SAND achieves an average 20% improvement over initial supervised finetuning and also outperforms state-of-the-art agent tuning approaches. (provenance: paper_notes | papers/paper_notes.jsonl:paper_id=P0087#key_results[0])
  - Caveat: Evidence is not full-text grounded for this subsection; treat claims as provisional and avoid strong generalizations.

## 5.2 Multi-agent coordination

- RQ: Which design choices in Multi-agent coordination drive the major trade-offs, and how are those trade-offs measured?
- Claim: To this end, we propose a multi-agent system with customized communication knowledge and tools for solving communication related tasks using natural language, comprising three components: (1) Multi-agent Data Retrieval (
  - Axes: evaluation protocol (datasets, metrics, human evaluation); compute and latency constraints; tool interface contract (schemas / protocols); tool selection / routing policy; sandboxing / permissions / observability
  - Evidence levels: fulltext=0, abstract=28, title=0.
  - Evidence: `P0040` [@Jiang2023Large] — To this end, we propose a multi-agent system with customized communication knowledge and tools for solving communication related tasks using natural language, comprising three components: (1) Multi-agent Data Retrieval (MDR), which employs the condensate and inference agents to refine and summarize communication knowledge from the knowledge base, expanding the knowledge boundaries of LLMs in 6G communications; (2) Multi-agent Collaborative Planning (MCP), which utilizes multiple planning agents to generate feasible solutions for the communication related task from different perspectives based on the retrieved knowledge; (3) Multi-agent Evaluation and Reflecxion (MER), which utilizes the evaluation agent to assess the solutions, and applies the reflexion agent and refinement agent to provide improvement suggestions for current solutions. (provenance: paper_notes | papers/paper_notes.jsonl:paper_id=P0040#key_results[0])
  - Evidence: `P0012` [@Aratchige2025Llms] — By analyzing recent advancements and their limitations - such as scalability, real-time response challenges, and agent coordination constraints, we provide a detailed view of the technological landscape. (provenance: paper_notes | papers/paper_notes.jsonl:paper_id=P0012#limitations[1])
  - Evidence: `P0139` [@Masters2025Arcane] — Using a corpus of 219 labeled rubrics derived from the GDPVal benchmark, we evaluate ARCANE on challenging tasks requiring multi-step reasoning and tool use. (provenance: paper_notes | papers/paper_notes.jsonl:paper_id=P0139#key_results[0])
  - Evidence: `P0194` [@Becker2025Mallm] — Current frameworks for multi-agent debate are often designed towards tool use, lack integrated evaluation, or provide limited configurability of agent personas, response generators, discussion paradigms, and decision protocols. (provenance: paper_notes | papers/paper_notes.jsonl:paper_id=P0194#key_results[1])
  - Evidence: `P0255` [@Inoue2024Drugagent] — We conducted comprehensive experiments using a kinase inhibitor dataset, where our multi-agent LLM method outperformed the non-reasoning multi-agent model (GPT-4o mini) by 45% in F1 score (0.514 vs 0.355). (provenance: paper_notes | papers/paper_notes.jsonl:paper_id=P0255#key_results[0])
  - Evidence: `P0149` [@Wang2025Autoscore] — We evaluate AutoSCORE on four benchmark datasets from the ASAP benchmark, using both proprietary and open-source LLMs (GPT-4o, LLaMA-3.1-8B, and LLaMA-3.1-70B). (provenance: paper_notes | papers/paper_notes.jsonl:paper_id=P0149#key_results[0])
  - Caveat: Evidence is not full-text grounded for this subsection; treat claims as provisional and avoid strong generalizations.

## 6.1 Benchmarks and evaluation protocols

- RQ: Which design choices in Benchmarks and evaluation protocols drive the major trade-offs, and how are those trade-offs measured?
- Claim: Existing evaluation methods suffer from following shortcomings: (1) constrained evaluation abilities, (2) vulnerable benchmarks, (3) unobjective metrics.
  - Axes: evaluation protocol (datasets, metrics, human evaluation); compute and latency constraints; tool interface contract (schemas / protocols); tool selection / routing policy; sandboxing / permissions / observability
  - Evidence levels: fulltext=0, abstract=28, title=0.
  - Evidence: `P0113` [@Lin2023Agentsims] — Existing evaluation methods suffer from following shortcomings: (1) constrained evaluation abilities, (2) vulnerable benchmarks, (3) unobjective metrics. (provenance: paper_notes | papers/paper_notes.jsonl:paper_id=P0113#key_results[0])
  - Evidence: `P0122` [@Kim2026Beyond] — We introduce WildAGTEval, a benchmark designed to evaluate large language model (LLM) agents' function-calling capabilities under realistic API complexity. (provenance: paper_notes | papers/paper_notes.jsonl:paper_id=P0122#method)
  - Evidence: `P0216` [@Seo2025Simuhome] — Our evaluation of 16 agents under a unified ReAct framework reveals distinct capabilities and limitations across models. (provenance: paper_notes | papers/paper_notes.jsonl:paper_id=P0216#limitations[1])
  - Evidence: `P0086` [@Shi2025Progent] — Our extensive evaluation across various agent use cases, using benchmarks like AgentDojo, ASB, and AgentPoison, demonstrates that Progent reduces attack success rates to 0%, while preserving agent utility and speed. (provenance: paper_notes | papers/paper_notes.jsonl:paper_id=P0086#key_results[0])
  - Evidence: `P0037` [@Shang2024Agentsquare] — Extensive experiments across six benchmarks, covering the diverse scenarios of web, embodied, tool use and game applications, show that AgentSquare substantially outperforms hand-crafted agents, achieving an average performance gain of 17.2% against best-known human designs. (provenance: paper_notes | papers/paper_notes.jsonl:paper_id=P0037#key_results[0])
  - Evidence: `P0244` [@Huang2024Survey] — This paper presents a systematic and comprehensive review of MLLM evaluation methods, covering the following key aspects: (1) the background of MLLMs and their evaluation; (2) "what to evaluate" that reviews and categorizes existing MLLM evaluation tasks based on the capabilities assessed, including general multimodal recognition, perception, reasoning and trustworthiness, and domain-specific applications such as socioeconomic, natural sciences and engineering, medical usage, AI agent, remote sensing, video and audio processing, 3D point cloud analysis, and others; (3) "where to evaluate" that summarizes MLLM evaluation benchmarks into general and specific benchmarks; (4) "how to evaluate" that reviews and illustrates MLLM evaluation steps and metrics; Our overarching goal is to provide valuable insights for researchers in the field of MLLM evaluation, thereby facilitating the development of more capable and reliable MLLMs. (provenance: paper_notes | papers/paper_notes.jsonl:paper_id=P0244#key_results[0])
  - Caveat: Evidence is not full-text grounded for this subsection; treat claims as provisional and avoid strong generalizations.

## 6.2 Safety, security, and governance

- RQ: Which design choices in Safety, security, and governance drive the major trade-offs, and how are those trade-offs measured?
- Claim: MSB contributes: (1) a taxonomy of 12 attacks including name-collision, preference manipulation, prompt injections embedded in tool descriptions, out-of-scope parameter requests, user-impersonating responses, false-error
  - Axes: evaluation protocol (datasets, metrics, human evaluation); compute and latency constraints; tool interface contract (schemas / protocols); tool selection / routing policy; sandboxing / permissions / observability
  - Evidence levels: fulltext=0, abstract=28, title=0.
  - Evidence: `P0195` [@Zhang2025Security] — MSB contributes: (1) a taxonomy of 12 attacks including name-collision, preference manipulation, prompt injections embedded in tool descriptions, out-of-scope parameter requests, user-impersonating responses, false-error escalation, tool-transfer, retrieval injection, and mixed attacks; (2) an evaluation harness that executes attacks by running real tools (both benign and malicious) via MCP rather than simulation; and (3) a robustness metric that quantifies the trade-off between security and performance: Net Resilient Performance (NRP). (provenance: paper_notes | papers/paper_notes.jsonl:paper_id=P0195#key_results[0])
  - Evidence: `P0196` [@Luo2025Universe] — Through extensive evaluation of leading LLMs, we find that even SOTA models such as GPT-5 (43.72%), Grok-4 (33.33%) and Claude-4.0-Sonnet (29.44%) exhibit significant performance limitations. (provenance: paper_notes | papers/paper_notes.jsonl:paper_id=P0196#limitations[1])
  - Evidence: `P0086` [@Shi2025Progent] — Our extensive evaluation across various agent use cases, using benchmarks like AgentDojo, ASB, and AgentPoison, demonstrates that Progent reduces attack success rates to 0%, while preserving agent utility and speed. (provenance: paper_notes | papers/paper_notes.jsonl:paper_id=P0086#key_results[0])
  - Evidence: `P0142` [@Wang2025Agentvigil] — We evaluate AgentVigil on two public benchmarks, AgentDojo and VWA-adv, where it achieves 71% and 70% success rates against agents based on o3-mini and GPT-4o, respectively, nearly doubling the performance of baseline attacks. (provenance: paper_notes | papers/paper_notes.jsonl:paper_id=P0142#key_results[0])
  - Evidence: `P0155` [@Weng2025Bridgescope] — Evaluations on two novel benchmarks demonstrate that BridgeScope enables LLM agents to operate databases more effectively, reduces token usage by up to 80% through improved security awareness, and uniquely supports data-intensive workflows beyond existing toolkits, establishing BridgeScope as a robust foundation for next-generation intelligent data automation. (provenance: paper_notes | papers/paper_notes.jsonl:paper_id=P0155#key_results[0])
  - Evidence: `P0158` [@Bonagiri2025Check] — Leveraging the ToolEmu framework, we conduct a systematic evaluation of quitting behavior across 12 state-of-the-art LLMs. (provenance: paper_notes | papers/paper_notes.jsonl:paper_id=P0158#key_results[0])
  - Caveat: Evidence is not full-text grounded for this subsection; treat claims as provisional and avoid strong generalizations.
