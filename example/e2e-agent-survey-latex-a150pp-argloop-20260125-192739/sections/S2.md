Several recent surveys and reviews chart the rapid growth of LLM-based agents, tool use, and evaluation practices, providing useful snapshots of emerging capabilities and application areas [@Hu2025Survey; @Du2025Survey; @Li2024Review; @Ge2025Surveya; @Zhang2025Largea]. Their organizing principles vary (task categories, component taxonomies, or application verticals), and many emphasize breadth over protocol-aligned comparisons [@Zhang2025Generalizability].

Work on tool interfaces and orchestration studies how action spaces are exposed to language models (e.g., function calling and protocolized tool APIs) and how agents select, sequence, and verify tool calls [@Zhang2025Tool; @Jia2025Autotool; @Li2024Stride; @Chowa2025From; @Liu2025Mcpagentbench; @Lumer2025Memtool]. These lines highlight that interface contracts (schema design, grounding, permissions) are often the dominant determinant of reliability, yet are inconsistently reported across benchmarks [@Gao2025Radar; @Cui2025Toward].

System papers that focus on the agent loop often frame agents as iterative decision-makers that interleave reasoning traces with actions and observations, producing behavior that cannot be evaluated with static QA-style metrics alone [@Yao2022React; @Luo2025Universe; @Zhou2026Beyond; @Wu2024Avatar; @Zhou2025Self]. Architectural choices such as where memory and planning live, how tool failures are handled, and whether intermediate states are exposed shape both performance and failure modes [@Chen2024Architectural; @Shen2024Small].

Planning and reasoning loops span approaches from training specialized planners to steering inference-time deliberation, often trading off compute cost, controllability, and robustness under environment noise [@Hu2025Training; @Lai2025Ustbench; @Wang2025Automated; @Chen2024Steering; @Zhou2025Reasoning; @Webb2023Improving]. A recurring challenge is that planning gains can be protocol-dependent: tool budgets, verification access, and horizon length can flip which method appears superior [@Hu2025Evaluating].

Memory and retrieval-augmented agents extend the loop with mechanisms for state persistence and evidence access, including retrieval policies and representations that mediate what context is available at decision time [@Du2025Memr; @Sun2025Search; @Xu2025Exemplar; @Shen2025Feat; @Wei2025Memguard; @Kang2025Distilling]. This body of work underscores that more context is not universally better: contamination, prompt injection, and stale memories can degrade reliability unless protocols define what information is trusted [@Wei2025Memguard].

Self-improvement and adaptation mechanisms modify policies, prompts, or tools based on feedback, ranging from offline prompt and context optimization to online reflection and revision loops [@Zhang2025Agentic; @Samplawski2025Agent; @Yang2025Proagent; @Wu2025Agentic; @Cao2025Large; @Zhang2025Generalizability]. These approaches raise stability questions (reward hacking, overfitting to benchmarks) that are easy to miss when evaluation protocols are underspecified [@Hu2025Evaluating; @Ji2024Testing].

Multi-agent systems introduce additional degrees of freedom such as role protocols, communication channels, and aggregation rules. These mechanisms can improve coverage and verification but also create new failure modes (collusion, feedback loops, cost blow-ups) [@Jiang2023Large; @Abbineni2025Muallm; @Masters2025Arcane; @Inoue2024Drugagent; @Silva2025Agents; @Wei2026Agentic]. Many evaluations in this area rely on rubric-based or task-specific protocols, making it important to track interaction budgets and aggregation assumptions when comparing results [@Masters2025Arcane].

Benchmark suites and evaluation protocols attempt to make agent behavior comparable across environments, but they vary in what constitutes success, how tool access is granted, and how long-horizon credit is assigned [@Hu2025Survey; @Shang2024Agentsquare; @Zhang2026Evoroute; @Hu2025Evaluating; @Ji2024Testing; @Zhang2025Datascibench]. Recent work also emphasizes reproducibility issues such as leakage, hidden environment changes, and dependence on proprietary tool backends, which can invalidate cross-paper rankings if not disclosed [@Zhang2025Generalizability; @Cui2025Toward].

Safety and security work argues that deploying tool-using agents requires explicit threat models and interface-aware mitigations, since attacks and defenses depend on the system boundary between model, tools, and external resources [@Zhang2025Security; @Gasmi2025Bridging; @Kim2025Bridging; @Wei2025Memguard; @Weng2025Bridgescope; @Zou2025Based]. This line connects evaluation to governance: metrics that ignore misuse and exfiltration can overstate capability while understating risk [@Zhang2025Security].

Across this literature, a persistent obstacle is that comparisons are frequently made without making protocol context and interface assumptions explicit. The organizing choice in this paper is to treat interfaces and protocols as first-class comparison objects, so that contrasts are anchored in task, metric, budget, and tool-access statements rather than in narrative summaries of individual papers [@Hu2025Survey; @Du2025Survey; @Lumer2025Memtool; @Zhang2025Security; @Zhang2026Evoroute].
