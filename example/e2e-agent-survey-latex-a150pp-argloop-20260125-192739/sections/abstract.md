## Abstract

Large language models increasingly act as closed-loop agents that iteratively perceive, decide, and act through tools and environments, rather than producing single-turn answers [@Yao2022React; @Luo2025Universe; @Zhou2026Beyond]. Yet reported performance often conflates model capability with interface contracts, tool access assumptions, and evaluation protocols, making cross-paper comparisons brittle [@Hu2025Survey; @Shang2024Agentsquare; @Zhang2026Evoroute]. This paper organizes the design space around foundations and interfaces, core components for planning and memory, adaptation and multi-agent coordination, and evaluation and risks, emphasizing protocol-aware contrasts over per-paper summaries [@Du2025Survey; @Li2024Review]. Across these axes, we highlight recurring failure modes in tool orchestration and long-horizon loops, and we summarize mitigation patterns for reliability and security [@Wei2025Memguard; @Zhang2025Security; @Gasmi2025Bridging]. Appendix tables compress representative approaches and protocol anchors to support fast comparison [@Lumer2025Memtool; @Masters2025Arcane; @Wang2025Autoscore].

