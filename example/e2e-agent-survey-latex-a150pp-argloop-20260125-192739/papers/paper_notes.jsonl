{"paper_id": "P0001", "title": "ReAct: Synergizing Reasoning and Acting in Language Models", "year": 2022, "url": "http://arxiv.org/abs/2210.03629v3", "arxiv_id": "2210.03629v3", "primary_category": "cs.CL", "categories": ["cs.CL", "cs.AI", "cs.LG"], "pdf_url": "https://arxiv.org/pdf/2210.03629v3", "priority": "high", "mapped_sections": ["3.1", "3.2", "4.1", "4.2", "6.1"], "evidence_level": "abstract", "fulltext_path": "", "authors": ["Shunyu Yao", "Jeffrey Zhao", "Dian Yu", "Nan Du", "Izhak Shafran", "Karthik Narasimhan", "Yuan Cao"], "abstract": "While large language models (LLMs) have demonstrated impressive capabilities across tasks in language understanding and interactive decision making, their abilities for reasoning (e.g. chain-of-thought prompting) and acting (e.g. action plan generation) have primarily been studied as separate topics. In this paper, we explore the use of LLMs to generate both reasoning traces and task-specific actions in an interleaved manner, allowing for greater synergy between the two: reasoning traces help the model induce, track, and update action plans as well as handle exceptions, while actions allow it to interface with external sources, such as knowledge bases or environments, to gather additional information. We apply our approach, named ReAct, to a diverse set of language and decision making tasks and demonstrate its effectiveness over state-of-the-art baselines, as well as improved human interpretability and trustworthiness over methods without reasoning or acting components. Concretely, on question answering (HotpotQA) and fact verification (Fever), ReAct overcomes issues of hallucination and error propagation prevalent in chain-of-thought reasoning by interacting with a simple Wikipedia API, and generates human-like task-solving trajectories that are more interpretable than baselines without reasoning traces. On two interactive decision making benchmarks (ALFWorld and WebShop), ReAct outperforms imitation and reinforcement learning methods by an absolute success rate of 34% and 10% respectively, while being prompted with only one or two in-context examples. Project site with code: https://react-lm.github.io", "summary_bullets": ["While large language models (LLMs) have demonstrated impressive capabilities across tasks in language understanding and interactive decision making, their abilities for reasoning (e.g. chain-of-thought prompting) and acting (e.g. action plan generation) have primarily been studied as separate topics.", "In this paper, we explore the use of LLMs to generate both reasoning traces and task-specific actions in an interleaved manner, allowing for greater synergy between the two: reasoning traces help the model induce, track, and update action plans as well as handle exceptions, while actions allow it to interface with external sources, such as knowledge bases or environments, to gather additional information.", "We apply our approach, named ReAct, to a diverse set of language and decision making tasks and demonstrate its effectiveness over state-of-the-art baselines, as well as improved human interpretability and trustworthiness over methods without reasoning or acting components.", "Concretely, on question answering (HotpotQA) and fact verification (Fever), ReAct overcomes issues of hallucination and error propagation prevalent in chain-of-thought reasoning by interacting with a simple Wikipedia API, and generates human-like task-solving trajectories that are more interpretable than baselines without reasoning traces.", "On two interactive decision making benchmarks (ALFWorld and WebShop), ReAct outperforms imitation and reinforcement learning methods by an absolute success rate of 34% and 10% respectively, while being prompted with only one or two in-context examples."], "method": "We apply our approach, named ReAct, to a diverse set of language and decision making tasks and demonstrate its effectiveness over state-of-the-art baselines, as well as improved human interpretability and trustworthiness over methods without reasoning or acting components.", "key_results": ["On two interactive decision making benchmarks (ALFWorld and WebShop), ReAct outperforms imitation and reinforcement learning methods by an absolute success rate of 34% and 10% respectively, while being prompted with only one or two in-context examples.", "We apply our approach, named ReAct, to a diverse set of language and decision making tasks and demonstrate its effectiveness over state-of-the-art baselines, as well as improved human interpretability and trustworthiness over methods without reasoning or acting components."], "limitations": ["Abstract-level evidence only: validate assumptions, evaluation protocol, and failure cases in the full paper before relying on this as key evidence."], "bibkey": "Yao2022React"}
{"paper_id": "P0002", "title": "Toolformer: Language Models Can Teach Themselves to Use Tools", "year": 2023, "url": "http://arxiv.org/abs/2302.04761v1", "arxiv_id": "2302.04761v1", "primary_category": "cs.CL", "categories": ["cs.CL"], "pdf_url": "https://arxiv.org/pdf/2302.04761v1", "priority": "high", "mapped_sections": [], "evidence_level": "abstract", "fulltext_path": "", "authors": ["Timo Schick", "Jane Dwivedi-Yu", "Roberto Dess√¨", "Roberta Raileanu", "Maria Lomeli", "Luke Zettlemoyer", "Nicola Cancedda", "Thomas Scialom"], "abstract": "Language models (LMs) exhibit remarkable abilities to solve new tasks from just a few examples or textual instructions, especially at scale. They also, paradoxically, struggle with basic functionality, such as arithmetic or factual lookup, where much simpler and smaller models excel. In this paper, we show that LMs can teach themselves to use external tools via simple APIs and achieve the best of both worlds. We introduce Toolformer, a model trained to decide which APIs to call, when to call them, what arguments to pass, and how to best incorporate the results into future token prediction. This is done in a self-supervised way, requiring nothing more than a handful of demonstrations for each API. We incorporate a range of tools, including a calculator, a Q\\&A system, two different search engines, a translation system, and a calendar. Toolformer achieves substantially improved zero-shot performance across a variety of downstream tasks, often competitive with much larger models, without sacrificing its core language modeling abilities.", "summary_bullets": ["Language models (LMs) exhibit remarkable abilities to solve new tasks from just a few examples or textual instructions, especially at scale.", "They also, paradoxically, struggle with basic functionality, such as arithmetic or factual lookup, where much simpler and smaller models excel.", "In this paper, we show that LMs can teach themselves to use external tools via simple APIs and achieve the best of both worlds.", "We introduce Toolformer, a model trained to decide which APIs to call, when to call them, what arguments to pass, and how to best incorporate the results into future token prediction.", "This is done in a self-supervised way, requiring nothing more than a handful of demonstrations for each API."], "method": "We introduce Toolformer, a model trained to decide which APIs to call, when to call them, what arguments to pass, and how to best incorporate the results into future token prediction.", "key_results": ["Toolformer achieves substantially improved zero-shot performance across a variety of downstream tasks, often competitive with much larger models, without sacrificing its core language modeling abilities."], "limitations": ["Abstract-level evidence only: validate assumptions, evaluation protocol, and failure cases in the full paper before relying on this as key evidence."], "bibkey": "Schick2023Toolformer"}
{"paper_id": "P0003", "title": "Reflexion: Language Agents with Verbal Reinforcement Learning", "year": 2023, "url": "http://arxiv.org/abs/2303.11366v4", "arxiv_id": "2303.11366v4", "primary_category": "cs.AI", "categories": ["cs.AI", "cs.CL", "cs.LG"], "pdf_url": "https://arxiv.org/pdf/2303.11366v4", "priority": "high", "mapped_sections": [], "evidence_level": "abstract", "fulltext_path": "", "authors": ["Noah Shinn", "Federico Cassano", "Edward Berman", "Ashwin Gopinath", "Karthik Narasimhan", "Shunyu Yao"], "abstract": "Large language models (LLMs) have been increasingly used to interact with external environments (e.g., games, compilers, APIs) as goal-driven agents. However, it remains challenging for these language agents to quickly and efficiently learn from trial-and-error as traditional reinforcement learning methods require extensive training samples and expensive model fine-tuning. We propose Reflexion, a novel framework to reinforce language agents not by updating weights, but instead through linguistic feedback. Concretely, Reflexion agents verbally reflect on task feedback signals, then maintain their own reflective text in an episodic memory buffer to induce better decision-making in subsequent trials. Reflexion is flexible enough to incorporate various types (scalar values or free-form language) and sources (external or internally simulated) of feedback signals, and obtains significant improvements over a baseline agent across diverse tasks (sequential decision-making, coding, language reasoning). For example, Reflexion achieves a 91% pass@1 accuracy on the HumanEval coding benchmark, surpassing the previous state-of-the-art GPT-4 that achieves 80%. We also conduct ablation and analysis studies using different feedback signals, feedback incorporation methods, and agent types, and provide insights into how they affect performance.", "summary_bullets": ["Large language models (LLMs) have been increasingly used to interact with external environments (e.g., games, compilers, APIs) as goal-driven agents.", "However, it remains challenging for these language agents to quickly and efficiently learn from trial-and-error as traditional reinforcement learning methods require extensive training samples and expensive model fine-tuning.", "We propose Reflexion, a novel framework to reinforce language agents not by updating weights, but instead through linguistic feedback.", "Concretely, Reflexion agents verbally reflect on task feedback signals, then maintain their own reflective text in an episodic memory buffer to induce better decision-making in subsequent trials.", "Reflexion is flexible enough to incorporate various types (scalar values or free-form language) and sources (external or internally simulated) of feedback signals, and obtains significant improvements over a baseline agent across diverse tasks (sequential decision-making, coding, language reasoning)."], "method": "We propose Reflexion, a novel framework to reinforce language agents not by updating weights, but instead through linguistic feedback.", "key_results": ["For example, Reflexion achieves a 91% pass@1 accuracy on the HumanEval coding benchmark, surpassing the previous state-of-the-art GPT-4 that achieves 80%."], "limitations": ["Abstract-level evidence only: validate assumptions, evaluation protocol, and failure cases in the full paper before relying on this as key evidence."], "bibkey": "Shinn2023Reflexion"}
{"paper_id": "P0004", "title": "Tree of Thoughts: Deliberate Problem Solving with Large Language Models", "year": 2023, "url": "http://arxiv.org/abs/2305.10601v2", "arxiv_id": "2305.10601v2", "primary_category": "cs.CL", "categories": ["cs.CL", "cs.AI", "cs.LG"], "pdf_url": "https://arxiv.org/pdf/2305.10601v2", "priority": "high", "mapped_sections": [], "evidence_level": "abstract", "fulltext_path": "", "authors": ["Shunyu Yao", "Dian Yu", "Jeffrey Zhao", "Izhak Shafran", "Thomas L. Griffiths", "Yuan Cao", "Karthik Narasimhan"], "abstract": "Language models are increasingly being deployed for general problem solving across a wide range of tasks, but are still confined to token-level, left-to-right decision-making processes during inference. This means they can fall short in tasks that require exploration, strategic lookahead, or where initial decisions play a pivotal role. To surmount these challenges, we introduce a new framework for language model inference, Tree of Thoughts (ToT), which generalizes over the popular Chain of Thought approach to prompting language models, and enables exploration over coherent units of text (thoughts) that serve as intermediate steps toward problem solving. ToT allows LMs to perform deliberate decision making by considering multiple different reasoning paths and self-evaluating choices to decide the next course of action, as well as looking ahead or backtracking when necessary to make global choices. Our experiments show that ToT significantly enhances language models' problem-solving abilities on three novel tasks requiring non-trivial planning or search: Game of 24, Creative Writing, and Mini Crosswords. For instance, in Game of 24, while GPT-4 with chain-of-thought prompting only solved 4% of tasks, our method achieved a success rate of 74%. Code repo with all prompts: https://github.com/princeton-nlp/tree-of-thought-llm.", "summary_bullets": ["Language models are increasingly being deployed for general problem solving across a wide range of tasks, but are still confined to token-level, left-to-right decision-making processes during inference.", "This means they can fall short in tasks that require exploration, strategic lookahead, or where initial decisions play a pivotal role.", "To surmount these challenges, we introduce a new framework for language model inference, Tree of Thoughts (ToT), which generalizes over the popular Chain of Thought approach to prompting language models, and enables exploration over coherent units of text (thoughts) that serve as intermediate steps toward problem solving.", "ToT allows LMs to perform deliberate decision making by considering multiple different reasoning paths and self-evaluating choices to decide the next course of action, as well as looking ahead or backtracking when necessary to make global choices.", "Our experiments show that ToT significantly enhances language models' problem-solving abilities on three novel tasks requiring non-trivial planning or search: Game of 24, Creative Writing, and Mini Crosswords."], "method": "To surmount these challenges, we introduce a new framework for language model inference, Tree of Thoughts (ToT), which generalizes over the popular Chain of Thought approach to prompting language models, and enables exploration over coherent units of text (thoughts) that serve as intermediate steps toward problem solving.", "key_results": ["For instance, in Game of 24, while GPT-4 with chain-of-thought prompting only solved 4% of tasks, our method achieved a success rate of 74%.", "Our experiments show that ToT significantly enhances language models' problem-solving abilities on three novel tasks requiring non-trivial planning or search: Game of 24, Creative Writing, and Mini Crosswords."], "limitations": ["Abstract-level evidence only: validate assumptions, evaluation protocol, and failure cases in the full paper before relying on this as key evidence."], "bibkey": "Yao2023Tree"}
{"paper_id": "P0005", "title": "Voyager: An Open-Ended Embodied Agent with Large Language Models", "year": 2023, "url": "http://arxiv.org/abs/2305.16291v2", "arxiv_id": "2305.16291v2", "primary_category": "cs.AI", "categories": ["cs.AI", "cs.LG"], "pdf_url": "https://arxiv.org/pdf/2305.16291v2", "priority": "high", "mapped_sections": [], "evidence_level": "abstract", "fulltext_path": "", "authors": ["Guanzhi Wang", "Yuqi Xie", "Yunfan Jiang", "Ajay Mandlekar", "Chaowei Xiao", "Yuke Zhu", "Linxi Fan", "Anima Anandkumar"], "abstract": "We introduce Voyager, the first LLM-powered embodied lifelong learning agent in Minecraft that continuously explores the world, acquires diverse skills, and makes novel discoveries without human intervention. Voyager consists of three key components: 1) an automatic curriculum that maximizes exploration, 2) an ever-growing skill library of executable code for storing and retrieving complex behaviors, and 3) a new iterative prompting mechanism that incorporates environment feedback, execution errors, and self-verification for program improvement. Voyager interacts with GPT-4 via blackbox queries, which bypasses the need for model parameter fine-tuning. The skills developed by Voyager are temporally extended, interpretable, and compositional, which compounds the agent's abilities rapidly and alleviates catastrophic forgetting. Empirically, Voyager shows strong in-context lifelong learning capability and exhibits exceptional proficiency in playing Minecraft. It obtains 3.3x more unique items, travels 2.3x longer distances, and unlocks key tech tree milestones up to 15.3x faster than prior SOTA. Voyager is able to utilize the learned skill library in a new Minecraft world to solve novel tasks from scratch, while other techniques struggle to generalize. We open-source our full codebase and prompts at https://voyager.minedojo.org/.", "summary_bullets": ["We introduce Voyager, the first LLM-powered embodied lifelong learning agent in Minecraft that continuously explores the world, acquires diverse skills, and makes novel discoveries without human intervention.", "Voyager consists of three key components: 1) an automatic curriculum that maximizes exploration, 2) an ever-growing skill library of executable code for storing and retrieving complex behaviors, and 3) a new iterative prompting mechanism that incorporates environment feedback, execution errors, and self-verification for program improvement.", "Voyager interacts with GPT-4 via blackbox queries, which bypasses the need for model parameter fine-tuning.", "The skills developed by Voyager are temporally extended, interpretable, and compositional, which compounds the agent's abilities rapidly and alleviates catastrophic forgetting.", "Empirically, Voyager shows strong in-context lifelong learning capability and exhibits exceptional proficiency in playing Minecraft."], "method": "We introduce Voyager, the first LLM-powered embodied lifelong learning agent in Minecraft that continuously explores the world, acquires diverse skills, and makes novel discoveries without human intervention.", "key_results": ["It obtains 3.3x more unique items, travels 2.3x longer distances, and unlocks key tech tree milestones up to 15.3x faster than prior SOTA.", "Voyager consists of three key components: 1) an automatic curriculum that maximizes exploration, 2) an ever-growing skill library of executable code for storing and retrieving complex behaviors, and 3) a new iterative prompting mechanism that incorporates environment feedback, execution errors, and self-verification for program improvement."], "limitations": ["Abstract-level evidence only: validate assumptions, evaluation protocol, and failure cases in the full paper before relying on this as key evidence."], "bibkey": "Wang2023Voyager"}
{"paper_id": "P0006", "title": "Empowering Real-World: A Survey on the Technology, Practice, and Evaluation of LLM-driven Industry Agents", "year": 2025, "url": "http://arxiv.org/abs/2510.17491v1", "arxiv_id": "2510.17491v1", "primary_category": "cs.CL", "categories": ["cs.CL"], "pdf_url": "https://arxiv.org/pdf/2510.17491v1", "priority": "normal", "mapped_sections": ["6.1"], "evidence_level": "abstract", "fulltext_path": "", "authors": ["Yihong Tang", "Kehai Chen", "Liang Yue", "Jinxin Fan", "Caishen Zhou", "Xiaoguang Li", "Yuyang Zhang", "Mingming Zhao", "Shixiong Kai", "Kaiyang Guo", "Xingshan Zeng", "Wenjing Cun", "Lifeng Shang", "Min Zhang"], "abstract": "With the rise of large language models (LLMs), LLM agents capable of autonomous reasoning, planning, and executing complex tasks have become a frontier in artificial intelligence. However, how to translate the research on general agents into productivity that drives industry transformations remains a significant challenge. To address this, this paper systematically reviews the technologies, applications, and evaluation methods of industry agents based on LLMs. Using an industry agent capability maturity framework, it outlines the evolution of agents in industry applications, from \"process execution systems\" to \"adaptive social systems.\" First, we examine the three key technological pillars that support the advancement of agent capabilities: Memory, Planning, and Tool Use. We discuss how these technologies evolve from supporting simple tasks in their early forms to enabling complex autonomous systems and collective intelligence in more advanced forms. Then, we provide an overview of the application of industry agents in real-world domains such as digital engineering, scientific discovery, embodied intelligence, collaborative business execution, and complex system simulation. Additionally, this paper reviews the evaluation benchmarks and methods for both fundamental and specialized capabilities, identifying the challenges existing evaluation systems face regarding authenticity, safety, and industry specificity. Finally, we focus on the practical challenges faced by industry agents, exploring their capability boundaries, developmental potential, and governance issues in various scenarios, while providing insights into future directions. By combining technological evolution with industry practices, this review aims to clarify the current state and offer a clear roadmap and theoretical foundation for understanding and building the next generation of industry agents.", "summary_bullets": ["With the rise of large language models (LLMs), LLM agents capable of autonomous reasoning, planning, and executing complex tasks have become a frontier in artificial intelligence.", "However, how to translate the research on general agents into productivity that drives industry transformations remains a significant challenge.", "To address this, this paper systematically reviews the technologies, applications, and evaluation methods of industry agents based on LLMs."], "method": "With the rise of large language models (LLMs), LLM agents capable of autonomous reasoning, planning, and executing complex tasks have become a frontier in artificial intelligence.", "key_results": ["To address this, this paper systematically reviews the technologies, applications, and evaluation methods of industry agents based on LLMs.", "Additionally, this paper reviews the evaluation benchmarks and methods for both fundamental and specialized capabilities, identifying the challenges existing evaluation systems face regarding authenticity, safety, and industry specificity."], "limitations": ["Abstract-level evidence only: validate assumptions, evaluation protocol, and failure cases in the full paper before relying on this as key evidence."], "bibkey": "Tang2025Empowering"}
{"paper_id": "P0007", "title": "An In-depth Survey of Large Language Model-based Artificial Intelligence Agents", "year": 2023, "url": "http://arxiv.org/abs/2309.14365v1", "arxiv_id": "2309.14365v1", "primary_category": "cs.CL", "categories": ["cs.CL", "cs.AI"], "pdf_url": "https://arxiv.org/pdf/2309.14365v1", "priority": "normal", "mapped_sections": [], "evidence_level": "abstract", "fulltext_path": "", "authors": ["Pengyu Zhao", "Zijian Jin", "Ning Cheng"], "abstract": "Due to the powerful capabilities demonstrated by large language model (LLM), there has been a recent surge in efforts to integrate them with AI agents to enhance their performance. In this paper, we have explored the core differences and characteristics between LLM-based AI agents and traditional AI agents. Specifically, we first compare the fundamental characteristics of these two types of agents, clarifying the significant advantages of LLM-based agents in handling natural language, knowledge storage, and reasoning capabilities. Subsequently, we conducted an in-depth analysis of the key components of AI agents, including planning, memory, and tool use. Particularly, for the crucial component of memory, this paper introduced an innovative classification scheme, not only departing from traditional classification methods but also providing a fresh perspective on the design of an AI agent's memory system. We firmly believe that in-depth research and understanding of these core components will lay a solid foundation for the future advancement of AI agent technology. At the end of the paper, we provide directional suggestions for further research in this field, with the hope of offering valuable insights to scholars and researchers in the field.", "summary_bullets": ["Due to the powerful capabilities demonstrated by large language model (LLM), there has been a recent surge in efforts to integrate them with AI agents to enhance their performance.", "In this paper, we have explored the core differences and characteristics between LLM-based AI agents and traditional AI agents.", "Specifically, we first compare the fundamental characteristics of these two types of agents, clarifying the significant advantages of LLM-based agents in handling natural language, knowledge storage, and reasoning capabilities."], "method": "Due to the powerful capabilities demonstrated by large language model (LLM), there has been a recent surge in efforts to integrate them with AI agents to enhance their performance.", "key_results": ["At the end of the paper, we provide directional suggestions for further research in this field, with the hope of offering valuable insights to scholars and researchers in the field."], "limitations": ["Abstract-level evidence only: validate assumptions, evaluation protocol, and failure cases in the full paper before relying on this as key evidence."], "bibkey": "Zhao2023Depth"}
{"paper_id": "P0008", "title": "Agentic Large Language Models, a survey", "year": 2025, "url": "http://arxiv.org/abs/2503.23037v3", "arxiv_id": "2503.23037v3", "primary_category": "cs.AI", "categories": ["cs.AI", "cs.CL", "cs.LG"], "pdf_url": "https://arxiv.org/pdf/2503.23037v3", "priority": "normal", "mapped_sections": ["6.2"], "evidence_level": "abstract", "fulltext_path": "", "authors": ["Aske Plaat", "Max van Duijn", "Niki van Stein", "Mike Preuss", "Peter van der Putten", "Kees Joost Batenburg"], "abstract": "Background: There is great interest in agentic LLMs, large language models that act as agents.\n  Objectives: We review the growing body of work in this area and provide a research agenda.\n  Methods: Agentic LLMs are LLMs that (1) reason, (2) act, and (3) interact. We organize the literature according to these three categories.\n  Results: The research in the first category focuses on reasoning, reflection, and retrieval, aiming to improve decision making; the second category focuses on action models, robots, and tools, aiming for agents that act as useful assistants; the third category focuses on multi-agent systems, aiming for collaborative task solving and simulating interaction to study emergent social behavior. We find that works mutually benefit from results in other categories: retrieval enables tool use, reflection improves multi-agent collaboration, and reasoning benefits all categories.\n  Conclusions: We discuss applications of agentic LLMs and provide an agenda for further research. Important applications are in medical diagnosis, logistics and financial market analysis. Meanwhile, self-reflective agents playing roles and interacting with one another augment the process of scientific research itself. Further, agentic LLMs provide a solution for the problem of LLMs running out of training data: inference-time behavior generates new training states, such that LLMs can keep learning without needing ever larger datasets. We note that there is risk associated with LLM assistants taking action in the real world-safety, liability and security are open problems-while agentic LLMs are also likely to benefit society.", "summary_bullets": ["Background: There is great interest in agentic LLMs, large language models that act as agents.", "Objectives: We review the growing body of work in this area and provide a research agenda.", "Methods: Agentic LLMs are LLMs that (1) reason, (2) act, and (3) interact."], "method": "Background: There is great interest in agentic LLMs, large language models that act as agents.", "key_results": ["Methods: Agentic LLMs are LLMs that (1) reason, (2) act, and (3) interact.", "Further, agentic LLMs provide a solution for the problem of LLMs running out of training data: inference-time behavior generates new training states, such that LLMs can keep learning without needing ever larger datasets."], "limitations": ["Abstract-level evidence only: validate assumptions, evaluation protocol, and failure cases in the full paper before relying on this as key evidence.", "We note that there is risk associated with LLM assistants taking action in the real world-safety, liability and security are open problems-while agentic LLMs are also likely to benefit society."], "bibkey": "Plaat2025Agentic"}
{"paper_id": "P0009", "title": "From Language to Action: A Review of Large Language Models as Autonomous Agents and Tool Users", "year": 2025, "url": "http://arxiv.org/abs/2508.17281v2", "arxiv_id": "2508.17281v2", "primary_category": "cs.CL", "categories": ["cs.CL"], "pdf_url": "https://arxiv.org/pdf/2508.17281v2", "priority": "normal", "mapped_sections": ["3.2"], "evidence_level": "abstract", "fulltext_path": "", "authors": ["Sadia Sultana Chowa", "Riasad Alvi", "Subhey Sadi Rahman", "Md Abdur Rahman", "Mohaimenul Azam Khan Raiaan", "Md Rafiqul Islam", "Mukhtar Hussain", "Sami Azam"], "abstract": "The pursuit of human-level artificial intelligence (AI) has significantly advanced the development of autonomous agents and Large Language Models (LLMs). LLMs are now widely utilized as decision-making agents for their ability to interpret instructions, manage sequential tasks, and adapt through feedback. This review examines recent developments in employing LLMs as autonomous agents and tool users and comprises seven research questions. We only used the papers published between 2023 and 2025 in conferences of the A* and A rank and Q1 journals. A structured analysis of the LLM agents' architectural design principles, dividing their applications into single-agent and multi-agent systems, and strategies for integrating external tools is presented. In addition, the cognitive mechanisms of LLM, including reasoning, planning, and memory, and the impact of prompting methods and fine-tuning procedures on agent performance are also investigated. Furthermore, we evaluated current benchmarks and assessment protocols and have provided an analysis of 68 publicly available datasets to assess the performance of LLM-based agents in various tasks. In conducting this review, we have identified critical findings on verifiable reasoning of LLMs, the capacity for self-improvement, and the personalization of LLM-based agents. Finally, we have discussed ten future research directions to overcome these gaps.", "summary_bullets": ["The pursuit of human-level artificial intelligence (AI) has significantly advanced the development of autonomous agents and Large Language Models (LLMs).", "LLMs are now widely utilized as decision-making agents for their ability to interpret instructions, manage sequential tasks, and adapt through feedback.", "This review examines recent developments in employing LLMs as autonomous agents and tool users and comprises seven research questions."], "method": "The pursuit of human-level artificial intelligence (AI) has significantly advanced the development of autonomous agents and Large Language Models (LLMs).", "key_results": ["Furthermore, we evaluated current benchmarks and assessment protocols and have provided an analysis of 68 publicly available datasets to assess the performance of LLM-based agents in various tasks.", "We only used the papers published between 2023 and 2025 in conferences of the A* and A rank and Q1 journals."], "limitations": ["Abstract-level evidence only: validate assumptions, evaluation protocol, and failure cases in the full paper before relying on this as key evidence."], "bibkey": "Chowa2025From"}
{"paper_id": "P0010", "title": "Generalizability of Large Language Model-Based Agents: A Comprehensive Survey", "year": 2025, "url": "http://arxiv.org/abs/2509.16330v1", "arxiv_id": "2509.16330v1", "primary_category": "cs.AI", "categories": ["cs.AI"], "pdf_url": "https://arxiv.org/pdf/2509.16330v1", "priority": "high", "mapped_sections": ["3.2", "6.1"], "evidence_level": "abstract", "fulltext_path": "", "authors": ["Minxing Zhang", "Yi Yang", "Roy Xie", "Bhuwan Dhingra", "Shuyan Zhou", "Jian Pei"], "abstract": "Large Language Model (LLM)-based agents have emerged as a new paradigm that extends LLMs' capabilities beyond text generation to dynamic interaction with external environments. By integrating reasoning with perception, memory, and tool use, agents are increasingly deployed in diverse domains like web navigation and household robotics. A critical challenge, however, lies in ensuring agent generalizability - the ability to maintain consistent performance across varied instructions, tasks, environments, and domains, especially those beyond agents' fine-tuning data. Despite growing interest, the concept of generalizability in LLM-based agents remains underdefined, and systematic approaches to measure and improve it are lacking. In this survey, we provide the first comprehensive review of generalizability in LLM-based agents. We begin by emphasizing agent generalizability's importance by appealing to stakeholders and clarifying the boundaries of agent generalizability by situating it within a hierarchical domain-task ontology. We then review datasets, evaluation dimensions, and metrics, highlighting their limitations. Next, we categorize methods for improving generalizability into three groups: methods for the backbone LLM, for agent components, and for their interactions. Moreover, we introduce the distinction between generalizable frameworks and generalizable agents and outline how generalizable frameworks can be translated into agent-level generalizability. Finally, we identify critical challenges and future directions, including developing standardized frameworks, variance- and cost-based metrics, and approaches that integrate methodological innovations with architecture-level designs. By synthesizing progress and highlighting opportunities, this survey aims to establish a foundation for principled research on building LLM-based agents that generalize reliably across diverse applications.", "summary_bullets": ["Large Language Model (LLM)-based agents have emerged as a new paradigm that extends LLMs' capabilities beyond text generation to dynamic interaction with external environments.", "By integrating reasoning with perception, memory, and tool use, agents are increasingly deployed in diverse domains like web navigation and household robotics.", "A critical challenge, however, lies in ensuring agent generalizability - the ability to maintain consistent performance across varied instructions, tasks, environments, and domains, especially those beyond agents' fine-tuning data.", "Despite growing interest, the concept of generalizability in LLM-based agents remains underdefined, and systematic approaches to measure and improve it are lacking.", "In this survey, we provide the first comprehensive review of generalizability in LLM-based agents."], "method": "Moreover, we introduce the distinction between generalizable frameworks and generalizable agents and outline how generalizable frameworks can be translated into agent-level generalizability.", "key_results": ["We then review datasets, evaluation dimensions, and metrics, highlighting their limitations.", "Finally, we identify critical challenges and future directions, including developing standardized frameworks, variance- and cost-based metrics, and approaches that integrate methodological innovations with architecture-level designs."], "limitations": ["Abstract-level evidence only: validate assumptions, evaluation protocol, and failure cases in the full paper before relying on this as key evidence.", "We then review datasets, evaluation dimensions, and metrics, highlighting their limitations."], "bibkey": "Zhang2025Generalizability"}
{"paper_id": "P0011", "title": "A Survey on Agentic Multimodal Large Language Models", "year": 2025, "url": "http://arxiv.org/abs/2510.10991v1", "arxiv_id": "2510.10991v1", "primary_category": "cs.CV", "categories": ["cs.CV", "cs.AI", "cs.CL"], "pdf_url": "https://arxiv.org/pdf/2510.10991v1", "priority": "normal", "mapped_sections": ["4.2"], "evidence_level": "abstract", "fulltext_path": "", "authors": ["Huanjin Yao", "Ruifei Zhang", "Jiaxing Huang", "Jingyi Zhang", "Yibo Wang", "Bo Fang", "Ruolin Zhu", "Yongcheng Jing", "Shunyu Liu", "Guanbin Li", "Dacheng Tao"], "abstract": "With the recent emergence of revolutionary autonomous agentic systems, research community is witnessing a significant shift from traditional static, passive, and domain-specific AI agents toward more dynamic, proactive, and generalizable agentic AI. Motivated by the growing interest in agentic AI and its potential trajectory toward AGI, we present a comprehensive survey on Agentic Multimodal Large Language Models (Agentic MLLMs). In this survey, we explore the emerging paradigm of agentic MLLMs, delineating their conceptual foundations and distinguishing characteristics from conventional MLLM-based agents. We establish a conceptual framework that organizes agentic MLLMs along three fundamental dimensions: (i) Agentic internal intelligence functions as the system's commander, enabling accurate long-horizon planning through reasoning, reflection, and memory; (ii) Agentic external tool invocation, whereby models proactively use various external tools to extend their problem-solving capabilities beyond their intrinsic knowledge; and (iii) Agentic environment interaction further situates models within virtual or physical environments, allowing them to take actions, adapt strategies, and sustain goal-directed behavior in dynamic real-world scenarios. To further accelerate research in this area for the community, we compile open-source training frameworks, training and evaluation datasets for developing agentic MLLMs. Finally, we review the downstream applications of agentic MLLMs and outline future research directions for this rapidly evolving field. To continuously track developments in this rapidly evolving field, we will also actively update a public repository at https://github.com/HJYao00/Awesome-Agentic-MLLMs.", "summary_bullets": ["With the recent emergence of revolutionary autonomous agentic systems, research community is witnessing a significant shift from traditional static, passive, and domain-specific AI agents toward more dynamic, proactive, and generalizable agentic AI.", "Motivated by the growing interest in agentic AI and its potential trajectory toward AGI, we present a comprehensive survey on Agentic Multimodal Large Language Models (Agentic MLLMs).", "In this survey, we explore the emerging paradigm of agentic MLLMs, delineating their conceptual foundations and distinguishing characteristics from conventional MLLM-based agents."], "method": "Motivated by the growing interest in agentic AI and its potential trajectory toward AGI, we present a comprehensive survey on Agentic Multimodal Large Language Models (Agentic MLLMs).", "key_results": ["To further accelerate research in this area for the community, we compile open-source training frameworks, training and evaluation datasets for developing agentic MLLMs."], "limitations": ["Abstract-level evidence only: validate assumptions, evaluation protocol, and failure cases in the full paper before relying on this as key evidence."], "bibkey": "Yao2025Survey"}
{"paper_id": "P0012", "title": "LLMs Working in Harmony: A Survey on the Technological Aspects of Building Effective LLM-Based Multi Agent Systems", "year": 2025, "url": "http://arxiv.org/abs/2504.01963v1", "arxiv_id": "2504.01963v1", "primary_category": "cs.MA", "categories": ["cs.MA", "cs.AI", "cs.CL"], "pdf_url": "https://arxiv.org/pdf/2504.01963v1", "priority": "normal", "mapped_sections": ["5.2"], "evidence_level": "abstract", "fulltext_path": "", "authors": ["R. M. Aratchige", "W. M. K. S. Ilmini"], "abstract": "This survey investigates foundational technologies essential for developing effective Large Language Model (LLM)-based multi-agent systems. Aiming to answer how best to optimize these systems for collaborative, dynamic environments, we focus on four critical areas: Architecture, Memory, Planning, and Technologies/Frameworks. By analyzing recent advancements and their limitations - such as scalability, real-time response challenges, and agent coordination constraints, we provide a detailed view of the technological landscape. Frameworks like the Mixture of Agents architecture and the ReAct planning model exemplify current innovations, showcasing improvements in role assignment and decision-making. This review synthesizes key strengths and persistent challenges, offering practical recommendations to enhance system scalability, agent collaboration, and adaptability. Our findings provide a roadmap for future research, supporting the creation of robust, efficient multi-agent systems that advance both individual agent performance and collective system resilience.", "summary_bullets": ["This survey investigates foundational technologies essential for developing effective Large Language Model (LLM)-based multi-agent systems.", "Aiming to answer how best to optimize these systems for collaborative, dynamic environments, we focus on four critical areas: Architecture, Memory, Planning, and Technologies/Frameworks.", "By analyzing recent advancements and their limitations - such as scalability, real-time response challenges, and agent coordination constraints, we provide a detailed view of the technological landscape."], "method": "This survey investigates foundational technologies essential for developing effective Large Language Model (LLM)-based multi-agent systems.", "key_results": ["Our findings provide a roadmap for future research, supporting the creation of robust, efficient multi-agent systems that advance both individual agent performance and collective system resilience."], "limitations": ["Abstract-level evidence only: validate assumptions, evaluation protocol, and failure cases in the full paper before relying on this as key evidence.", "By analyzing recent advancements and their limitations - such as scalability, real-time response challenges, and agent coordination constraints, we provide a detailed view of the technological landscape."], "bibkey": "Aratchige2025Llms"}
{"paper_id": "P0013", "title": "Bridging Symbolic Control and Neural Reasoning in LLM Agents: The Structured Cognitive Loop", "year": 2025, "url": "http://arxiv.org/abs/2511.17673v3", "arxiv_id": "2511.17673v3", "primary_category": "cs.AI", "categories": ["cs.AI", "cs.CL"], "pdf_url": "https://arxiv.org/pdf/2511.17673v3", "priority": "high", "mapped_sections": ["3.1", "3.2"], "evidence_level": "abstract", "fulltext_path": "", "authors": ["Myung Ho Kim"], "abstract": "Large language model agents suffer from fundamental architectural problems: entangled reasoning and execution, memory volatility, and uncontrolled action sequences. We introduce Structured Cognitive Loop (SCL), a modular architecture that explicitly separates agent cognition into five phases: Retrieval, Cognition, Control, Action, and Memory (R-CCAM). At the core of SCL is Soft Symbolic Control, an adaptive governance mechanism that applies symbolic constraints to probabilistic inference, preserving neural flexibility while restoring the explainability and controllability of classical symbolic systems. Through empirical validation on multi-step conditional reasoning tasks, we demonstrate that SCL achieves zero policy violations, eliminates redundant tool calls, and maintains complete decision traceability. These results address critical gaps in existing frameworks such as ReAct, AutoGPT, and memory-augmented approaches. Our contributions are threefold: (1) we situate SCL within the taxonomy of hybrid intelligence, differentiating it from prompt-centric and memory-only approaches; (2) we formally define Soft Symbolic Control and contrast it with neuro-symbolic AI; and (3) we derive three design principles for trustworthy agents: modular decomposition, adaptive symbolic governance, and transparent state management. We provide a complete open-source implementation demonstrating the R-CCAM loop architecture, alongside a live GPT-4o-powered travel planning agent. By connecting expert system principles with modern LLM capabilities, this work offers a practical and theoretically grounded path toward reliable, explainable, and governable AI agents.", "summary_bullets": ["Large language model agents suffer from fundamental architectural problems: entangled reasoning and execution, memory volatility, and uncontrolled action sequences.", "We introduce Structured Cognitive Loop (SCL), a modular architecture that explicitly separates agent cognition into five phases: Retrieval, Cognition, Control, Action, and Memory (R-CCAM).", "At the core of SCL is Soft Symbolic Control, an adaptive governance mechanism that applies symbolic constraints to probabilistic inference, preserving neural flexibility while restoring the explainability and controllability of classical symbolic systems.", "Through empirical validation on multi-step conditional reasoning tasks, we demonstrate that SCL achieves zero policy violations, eliminates redundant tool calls, and maintains complete decision traceability.", "These results address critical gaps in existing frameworks such as ReAct, AutoGPT, and memory-augmented approaches."], "method": "We introduce Structured Cognitive Loop (SCL), a modular architecture that explicitly separates agent cognition into five phases: Retrieval, Cognition, Control, Action, and Memory (R-CCAM).", "key_results": ["Our contributions are threefold: (1) we situate SCL within the taxonomy of hybrid intelligence, differentiating it from prompt-centric and memory-only approaches; (2) we formally define Soft Symbolic Control and contrast it with neuro-symbolic AI; and (3) we derive three design principles for trustworthy agents: modular decomposition, adaptive symbolic governance, and transparent state management."], "limitations": ["Abstract-level evidence only: validate assumptions, evaluation protocol, and failure cases in the full paper before relying on this as key evidence."], "bibkey": "Kim2025Bridging"}
{"paper_id": "P0014", "title": "ReAcTree: Hierarchical LLM Agent Trees with Control Flow for Long-Horizon Task Planning", "year": 2025, "url": "http://arxiv.org/abs/2511.02424v1", "arxiv_id": "2511.02424v1", "primary_category": "cs.AI", "categories": ["cs.AI"], "pdf_url": "https://arxiv.org/pdf/2511.02424v1", "priority": "normal", "mapped_sections": [], "evidence_level": "abstract", "fulltext_path": "", "authors": ["Jae-Woo Choi", "Hyungmin Kim", "Hyobin Ong", "Minsu Jang", "Dohyung Kim", "Jaehong Kim", "Youngwoo Yoon"], "abstract": "Recent advancements in large language models (LLMs) have enabled significant progress in decision-making and task planning for embodied autonomous agents. However, most existing methods still struggle with complex, long-horizon tasks because they rely on a monolithic trajectory that entangles all past decisions and observations, attempting to solve the entire task in a single unified process. To address this limitation, we propose ReAcTree, a hierarchical task-planning method that decomposes a complex goal into more manageable subgoals within a dynamically constructed agent tree. Each subgoal is handled by an LLM agent node capable of reasoning, acting, and further expanding the tree, while control flow nodes coordinate the execution strategies of agent nodes. In addition, we integrate two complementary memory systems: each agent node retrieves goal-specific, subgoal-level examples from episodic memory and shares environment-specific observations through working memory. Experiments on the WAH-NL and ALFRED datasets demonstrate that ReAcTree consistently outperforms strong task-planning baselines such as ReAct across diverse LLMs. Notably, on WAH-NL, ReAcTree achieves a 61% goal success rate with Qwen 2.5 72B, nearly doubling ReAct's 31%.", "summary_bullets": ["Recent advancements in large language models (LLMs) have enabled significant progress in decision-making and task planning for embodied autonomous agents.", "However, most existing methods still struggle with complex, long-horizon tasks because they rely on a monolithic trajectory that entangles all past decisions and observations, attempting to solve the entire task in a single unified process.", "To address this limitation, we propose ReAcTree, a hierarchical task-planning method that decomposes a complex goal into more manageable subgoals within a dynamically constructed agent tree."], "method": "To address this limitation, we propose ReAcTree, a hierarchical task-planning method that decomposes a complex goal into more manageable subgoals within a dynamically constructed agent tree.", "key_results": ["Notably, on WAH-NL, ReAcTree achieves a 61% goal success rate with Qwen 2.5 72B, nearly doubling ReAct's 31%.", "Experiments on the WAH-NL and ALFRED datasets demonstrate that ReAcTree consistently outperforms strong task-planning baselines such as ReAct across diverse LLMs."], "limitations": ["Abstract-level evidence only: validate assumptions, evaluation protocol, and failure cases in the full paper before relying on this as key evidence.", "To address this limitation, we propose ReAcTree, a hierarchical task-planning method that decomposes a complex goal into more manageable subgoals within a dynamically constructed agent tree."], "bibkey": "Choi2025Reactree"}
{"paper_id": "P0015", "title": "Review of Case-Based Reasoning for LLM Agents: Theoretical Foundations, Architectural Components, and Cognitive Integration", "year": 2025, "url": "http://arxiv.org/abs/2504.06943v2", "arxiv_id": "2504.06943v2", "primary_category": "cs.AI", "categories": ["cs.AI", "cs.MA"], "pdf_url": "https://arxiv.org/pdf/2504.06943v2", "priority": "normal", "mapped_sections": ["4.1"], "evidence_level": "abstract", "fulltext_path": "", "authors": ["Kostas Hatalis", "Despina Christou", "Vyshnavi Kondapalli"], "abstract": "Agents powered by Large Language Models (LLMs) have recently demonstrated impressive capabilities in various tasks. Still, they face limitations in tasks requiring specific, structured knowledge, flexibility, or accountable decision-making. While agents are capable of perceiving their environments, forming inferences, planning, and executing actions towards goals, they often face issues such as hallucinations and lack of contextual memory across interactions. This paper explores how Case-Based Reasoning (CBR), a strategy that solves new problems by referencing past experiences, can be integrated into LLM agent frameworks. This integration allows LLMs to leverage explicit knowledge, enhancing their effectiveness. We systematically review the theoretical foundations of these enhanced agents, identify critical framework components, and formulate a mathematical model for the CBR processes of case retrieval, adaptation, and learning. We also evaluate CBR-enhanced agents against other methods like Chain-of-Thought reasoning and standard Retrieval-Augmented Generation, analyzing their relative strengths. Moreover, we explore how leveraging CBR's cognitive dimensions (including self-reflection, introspection, and curiosity) via goal-driven autonomy mechanisms can further enhance the LLM agent capabilities. Contributing to the ongoing research on neuro-symbolic hybrid systems, this work posits CBR as a viable technique for enhancing the reasoning skills and cognitive aspects of autonomous LLM agents.", "summary_bullets": ["Agents powered by Large Language Models (LLMs) have recently demonstrated impressive capabilities in various tasks.", "Still, they face limitations in tasks requiring specific, structured knowledge, flexibility, or accountable decision-making.", "While agents are capable of perceiving their environments, forming inferences, planning, and executing actions towards goals, they often face issues such as hallucinations and lack of contextual memory across interactions."], "method": "Agents powered by Large Language Models (LLMs) have recently demonstrated impressive capabilities in various tasks.", "key_results": ["Contributing to the ongoing research on neuro-symbolic hybrid systems, this work posits CBR as a viable technique for enhancing the reasoning skills and cognitive aspects of autonomous LLM agents."], "limitations": ["Abstract-level evidence only: validate assumptions, evaluation protocol, and failure cases in the full paper before relying on this as key evidence.", "Still, they face limitations in tasks requiring specific, structured knowledge, flexibility, or accountable decision-making."], "bibkey": "Hatalis2025Review"}
{"paper_id": "P0016", "title": "A Review of Prominent Paradigms for LLM-Based Agents: Tool Use (Including RAG), Planning, and Feedback Learning", "year": 2024, "url": "http://arxiv.org/abs/2406.05804v6", "arxiv_id": "2406.05804v6", "primary_category": "cs.AI", "categories": ["cs.AI", "cs.CL", "cs.SE"], "pdf_url": "https://arxiv.org/pdf/2406.05804v6", "priority": "high", "mapped_sections": ["3.2", "4.2"], "evidence_level": "abstract", "fulltext_path": "", "authors": ["Xinzhe Li"], "abstract": "Tool use, planning, and feedback learning are currently three prominent paradigms for developing Large Language Model (LLM)-based agents across various tasks. Although numerous frameworks have been devised for each paradigm, their intricate workflows and inconsistent taxonomy create challenges in understanding and reviewing the frameworks across different paradigms. This survey introduces a unified taxonomy to systematically review and discuss these frameworks. Specifically, 1) the taxonomy defines environments/tasks, common LLM-profiled roles or LMPRs (policy models, evaluators, and dynamic models), and universally applicable workflows found in prior work, and 2) it enables a comparison of key perspectives on the implementations of LMPRs and workflow designs across different agent paradigms and frameworks. 3) Finally, we identify three limitations in existing workflow designs and systematically discuss the future work. Resources have been made publicly available at in our GitHub repository https://github.com/xinzhel/LLM-Agent-Survey.", "summary_bullets": ["Tool use, planning, and feedback learning are currently three prominent paradigms for developing Large Language Model (LLM)-based agents across various tasks.", "Although numerous frameworks have been devised for each paradigm, their intricate workflows and inconsistent taxonomy create challenges in understanding and reviewing the frameworks across different paradigms.", "This survey introduces a unified taxonomy to systematically review and discuss these frameworks.", "Specifically, 1) the taxonomy defines environments/tasks, common LLM-profiled roles or LMPRs (policy models, evaluators, and dynamic models), and universally applicable workflows found in prior work, and 2) it enables a comparison of key perspectives on the implementations of LMPRs and workflow designs across different agent paradigms and frameworks.", "3) Finally, we identify three limitations in existing workflow designs and systematically discuss the future work."], "method": "Tool use, planning, and feedback learning are currently three prominent paradigms for developing Large Language Model (LLM)-based agents across various tasks.", "key_results": ["Specifically, 1) the taxonomy defines environments/tasks, common LLM-profiled roles or LMPRs (policy models, evaluators, and dynamic models), and universally applicable workflows found in prior work, and 2) it enables a comparison of key perspectives on the implementations of LMPRs and workflow designs across different agent paradigms and frameworks.", "3) Finally, we identify three limitations in existing workflow designs and systematically discuss the future work."], "limitations": ["Abstract-level evidence only: validate assumptions, evaluation protocol, and failure cases in the full paper before relying on this as key evidence.", "3) Finally, we identify three limitations in existing workflow designs and systematically discuss the future work."], "bibkey": "Li2024Review"}
{"paper_id": "P0017", "title": "Causal Agent based on Large Language Model", "year": 2024, "url": "http://arxiv.org/abs/2408.06849v2", "arxiv_id": "2408.06849v2", "primary_category": "cs.AI", "categories": ["cs.AI", "cs.CL"], "pdf_url": "https://arxiv.org/pdf/2408.06849v2", "priority": "normal", "mapped_sections": [], "evidence_level": "abstract", "fulltext_path": "", "authors": ["Kairong Han", "Kun Kuang", "Ziyu Zhao", "Junjian Ye", "Fei Wu"], "abstract": "The large language model (LLM) has achieved significant success across various domains. However, the inherent complexity of causal problems and causal theory poses challenges in accurately describing them in natural language, making it difficult for LLM to comprehend and use them effectively. Causal methods are not easily conveyed through natural language, which hinders LLM's ability to apply them accurately. Additionally, causal datasets are typically tabular, while LLM excels in handling natural language data, creating a structural mismatch that impedes effective reasoning with tabular data. To address these challenges, we have equipped the LLM with causal tools within an agent framework, named the Causal Agent, enabling it to tackle causal problems. The causal agent comprises tools, memory, and reasoning modules. In the tool module, the causal agent calls Python code and uses the encapsulated causal function module to align tabular data with natural language. In the reasoning module, the causal agent performs reasoning through multiple iterations with the tools. In the memory module, the causal agent maintains a dictionary instance where the keys are unique names and the values are causal graphs. To verify the causal ability of the causal agent, we established a Causal Tabular Question Answer (CausalTQA) benchmark consisting of four levels of causal problems: variable level, edge level, causal graph level, and causal effect level. CausalTQA consists of about 1.4K for these four levels questions. Causal agent demonstrates remarkable efficacy on the four-level causal problems, with accuracy rates all above 80\\%. Through verification on the real-world dataset QRData, the causal agent is 6\\% higher than the original SOTA. For further insights and implementation details, our code is accessible via the GitHub repository https://github.com/kairong-han/causal_agent.", "summary_bullets": ["The large language model (LLM) has achieved significant success across various domains.", "However, the inherent complexity of causal problems and causal theory poses challenges in accurately describing them in natural language, making it difficult for LLM to comprehend and use them effectively.", "Causal methods are not easily conveyed through natural language, which hinders LLM's ability to apply them accurately."], "method": "The large language model (LLM) has achieved significant success across various domains.", "key_results": ["Through verification on the real-world dataset QRData, the causal agent is 6\\% higher than the original SOTA.", "Causal agent demonstrates remarkable efficacy on the four-level causal problems, with accuracy rates all above 80\\%."], "limitations": ["Abstract-level evidence only: validate assumptions, evaluation protocol, and failure cases in the full paper before relying on this as key evidence."], "bibkey": "Han2024Causal"}
{"paper_id": "P0018", "title": "Large Language Model based Multi-Agents: A Survey of Progress and Challenges", "year": 2024, "url": "http://arxiv.org/abs/2402.01680v2", "arxiv_id": "2402.01680v2", "primary_category": "cs.CL", "categories": ["cs.CL", "cs.AI", "cs.MA"], "pdf_url": "https://arxiv.org/pdf/2402.01680v2", "priority": "normal", "mapped_sections": [], "evidence_level": "abstract", "fulltext_path": "", "authors": ["Taicheng Guo", "Xiuying Chen", "Yaqi Wang", "Ruidi Chang", "Shichao Pei", "Nitesh V. Chawla", "Olaf Wiest", "Xiangliang Zhang"], "abstract": "Large Language Models (LLMs) have achieved remarkable success across a wide array of tasks. Due to the impressive planning and reasoning abilities of LLMs, they have been used as autonomous agents to do many tasks automatically. Recently, based on the development of using one LLM as a single planning or decision-making agent, LLM-based multi-agent systems have achieved considerable progress in complex problem-solving and world simulation. To provide the community with an overview of this dynamic field, we present this survey to offer an in-depth discussion on the essential aspects of multi-agent systems based on LLMs, as well as the challenges. Our goal is for readers to gain substantial insights on the following questions: What domains and environments do LLM-based multi-agents simulate? How are these agents profiled and how do they communicate? What mechanisms contribute to the growth of agents' capacities? For those interested in delving into this field of study, we also summarize the commonly used datasets or benchmarks for them to have convenient access. To keep researchers updated on the latest studies, we maintain an open-source GitHub repository, dedicated to outlining the research on LLM-based multi-agent systems.", "summary_bullets": ["Large Language Models (LLMs) have achieved remarkable success across a wide array of tasks.", "Due to the impressive planning and reasoning abilities of LLMs, they have been used as autonomous agents to do many tasks automatically.", "Recently, based on the development of using one LLM as a single planning or decision-making agent, LLM-based multi-agent systems have achieved considerable progress in complex problem-solving and world simulation."], "method": "To provide the community with an overview of this dynamic field, we present this survey to offer an in-depth discussion on the essential aspects of multi-agent systems based on LLMs, as well as the challenges.", "key_results": ["Large Language Models (LLMs) have achieved remarkable success across a wide array of tasks.", "For those interested in delving into this field of study, we also summarize the commonly used datasets or benchmarks for them to have convenient access."], "limitations": ["Abstract-level evidence only: validate assumptions, evaluation protocol, and failure cases in the full paper before relying on this as key evidence."], "bibkey": "Guo2024Large"}
{"paper_id": "P0019", "title": "Personal LLM Agents: Insights and Survey about the Capability, Efficiency and Security", "year": 2024, "url": "http://arxiv.org/abs/2401.05459v2", "arxiv_id": "2401.05459v2", "primary_category": "cs.HC", "categories": ["cs.HC", "cs.AI", "cs.SE"], "pdf_url": "https://arxiv.org/pdf/2401.05459v2", "priority": "high", "mapped_sections": ["3.2", "6.2"], "evidence_level": "abstract", "fulltext_path": "", "authors": ["Yuanchun Li", "Hao Wen", "Weijun Wang", "Xiangyu Li", "Yizhen Yuan", "Guohong Liu", "Jiacheng Liu", "Wenxing Xu", "Xiang Wang", "Yi Sun", "Rui Kong", "Yile Wang", "Hanfei Geng", "Jian Luan", "Xuefeng Jin", "Zilong Ye", "Guanjing Xiong", "Fan Zhang", "Xiang Li", "Mengwei Xu", "Zhijun Li", "Peng Li", "Yang Liu", "Ya-Qin Zhang", "Yunxin Liu"], "abstract": "Since the advent of personal computing devices, intelligent personal assistants (IPAs) have been one of the key technologies that researchers and engineers have focused on, aiming to help users efficiently obtain information and execute tasks, and provide users with more intelligent, convenient, and rich interaction experiences. With the development of smartphones and IoT, computing and sensing devices have become ubiquitous, greatly expanding the boundaries of IPAs. However, due to the lack of capabilities such as user intent understanding, task planning, tool using, and personal data management etc., existing IPAs still have limited practicality and scalability. Recently, the emergence of foundation models, represented by large language models (LLMs), brings new opportunities for the development of IPAs. With the powerful semantic understanding and reasoning capabilities, LLM can enable intelligent agents to solve complex problems autonomously. In this paper, we focus on Personal LLM Agents, which are LLM-based agents that are deeply integrated with personal data and personal devices and used for personal assistance. We envision that Personal LLM Agents will become a major software paradigm for end-users in the upcoming era. To realize this vision, we take the first step to discuss several important questions about Personal LLM Agents, including their architecture, capability, efficiency and security. We start by summarizing the key components and design choices in the architecture of Personal LLM Agents, followed by an in-depth analysis of the opinions collected from domain experts. Next, we discuss several key challenges to achieve intelligent, efficient and secure Personal LLM Agents, followed by a comprehensive survey of representative solutions to address these challenges.", "summary_bullets": ["Since the advent of personal computing devices, intelligent personal assistants (IPAs) have been one of the key technologies that researchers and engineers have focused on, aiming to help users efficiently obtain information and execute tasks, and provide users with more intelligent, convenient, and rich interaction experiences.", "With the development of smartphones and IoT, computing and sensing devices have become ubiquitous, greatly expanding the boundaries of IPAs.", "However, due to the lack of capabilities such as user intent understanding, task planning, tool using, and personal data management etc., existing IPAs still have limited practicality and scalability.", "Recently, the emergence of foundation models, represented by large language models (LLMs), brings new opportunities for the development of IPAs.", "With the powerful semantic understanding and reasoning capabilities, LLM can enable intelligent agents to solve complex problems autonomously."], "method": "Since the advent of personal computing devices, intelligent personal assistants (IPAs) have been one of the key technologies that researchers and engineers have focused on, aiming to help users efficiently obtain information and execute tasks, and provide users with more intelligent, convenient, and rich interaction experiences.", "key_results": ["Next, we discuss several key challenges to achieve intelligent, efficient and secure Personal LLM Agents, followed by a comprehensive survey of representative solutions to address these challenges."], "limitations": ["Abstract-level evidence only: validate assumptions, evaluation protocol, and failure cases in the full paper before relying on this as key evidence."], "bibkey": "Li2024Personal"}
{"paper_id": "P0020", "title": "The RL/LLM Taxonomy Tree: Reviewing Synergies Between Reinforcement Learning and Large Language Models", "year": 2024, "url": "http://arxiv.org/abs/2402.01874v1", "arxiv_id": "2402.01874v1", "primary_category": "cs.CL", "categories": ["cs.CL", "cs.AI", "cs.LG", "cs.RO"], "pdf_url": "https://arxiv.org/pdf/2402.01874v1", "priority": "normal", "mapped_sections": [], "evidence_level": "abstract", "fulltext_path": "", "authors": ["Moschoula Pternea", "Prerna Singh", "Abir Chakraborty", "Yagna Oruganti", "Mirco Milletari", "Sayli Bapat", "Kebei Jiang"], "abstract": "In this work, we review research studies that combine Reinforcement Learning (RL) and Large Language Models (LLMs), two areas that owe their momentum to the development of deep neural networks. We propose a novel taxonomy of three main classes based on the way that the two model types interact with each other. The first class, RL4LLM, includes studies where RL is leveraged to improve the performance of LLMs on tasks related to Natural Language Processing. L4LLM is divided into two sub-categories depending on whether RL is used to directly fine-tune an existing LLM or to improve the prompt of the LLM. In the second class, LLM4RL, an LLM assists the training of an RL model that performs a task that is not inherently related to natural language. We further break down LLM4RL based on the component of the RL training framework that the LLM assists or replaces, namely reward shaping, goal generation, and policy function. Finally, in the third class, RL+LLM, an LLM and an RL agent are embedded in a common planning framework without either of them contributing to training or fine-tuning of the other. We further branch this class to distinguish between studies with and without natural language feedback. We use this taxonomy to explore the motivations behind the synergy of LLMs and RL and explain the reasons for its success, while pinpointing potential shortcomings and areas where further research is needed, as well as alternative methodologies that serve the same goal.", "summary_bullets": ["In this work, we review research studies that combine Reinforcement Learning (RL) and Large Language Models (LLMs), two areas that owe their momentum to the development of deep neural networks.", "We propose a novel taxonomy of three main classes based on the way that the two model types interact with each other.", "The first class, RL4LLM, includes studies where RL is leveraged to improve the performance of LLMs on tasks related to Natural Language Processing."], "method": "We propose a novel taxonomy of three main classes based on the way that the two model types interact with each other.", "key_results": ["We use this taxonomy to explore the motivations behind the synergy of LLMs and RL and explain the reasons for its success, while pinpointing potential shortcomings and areas where further research is needed, as well as alternative methodologies that serve the same goal."], "limitations": ["Abstract-level evidence only: validate assumptions, evaluation protocol, and failure cases in the full paper before relying on this as key evidence."], "bibkey": "Pternea2024Taxonomy"}
{"paper_id": "P0021", "title": "BackdoorAgent: A Unified Framework for Backdoor Attacks on LLM-based Agents", "year": 2026, "url": "http://arxiv.org/abs/2601.04566v2", "arxiv_id": "2601.04566v2", "primary_category": "cs.AI", "categories": ["cs.AI", "cs.CL"], "pdf_url": "https://arxiv.org/pdf/2601.04566v2", "priority": "normal", "mapped_sections": [], "evidence_level": "abstract", "fulltext_path": "", "authors": ["Yunhao Feng", "Yige Li", "Yutao Wu", "Yingshui Tan", "Yanming Guo", "Yifan Ding", "Kun Zhai", "Xingjun Ma", "Yu-Gang Jiang"], "abstract": "Large language model (LLM) agents execute tasks through multi-step workflows that combine planning, memory, and tool use. While this design enables autonomy, it also expands the attack surface for backdoor threats. Backdoor triggers injected into specific stages of an agent workflow can persist through multiple intermediate states and adversely influence downstream outputs. However, existing studies remain fragmented and typically analyze individual attack vectors in isolation, leaving the cross-stage interaction and propagation of backdoor triggers poorly understood from an agent-centric perspective. To fill this gap, we propose \\textbf{BackdoorAgent}, a modular and stage-aware framework that provides a unified, agent-centric view of backdoor threats in LLM agents. BackdoorAgent structures the attack surface into three functional stages of agentic workflows, including \\textbf{planning attacks}, \\textbf{memory attacks}, and \\textbf{tool-use attacks}, and instruments agent execution to enable systematic analysis of trigger activation and propagation across different stages. Building on this framework, we construct a standardized benchmark spanning four representative agent applications: \\textbf{Agent QA}, \\textbf{Agent Code}, \\textbf{Agent Web}, and \\textbf{Agent Drive}, covering both language-only and multimodal settings. Our empirical analysis shows that \\textit{triggers implanted at a single stage can persist across multiple steps and propagate through intermediate states.} For instance, when using a GPT-based backbone, we observe trigger persistence in 43.58\\% of planning attacks, 77.97\\% of memory attacks, and 60.28\\% of tool-stage attacks, highlighting the vulnerabilities of the agentic workflow itself to backdoor threats. To facilitate reproducibility and future research, our code and benchmark are publicly available at GitHub.", "summary_bullets": ["Large language model (LLM) agents execute tasks through multi-step workflows that combine planning, memory, and tool use.", "While this design enables autonomy, it also expands the attack surface for backdoor threats.", "Backdoor triggers injected into specific stages of an agent workflow can persist through multiple intermediate states and adversely influence downstream outputs."], "method": "To fill this gap, we propose \\textbf{BackdoorAgent}, a modular and stage-aware framework that provides a unified, agent-centric view of backdoor threats in LLM agents.", "key_results": ["Our empirical analysis shows that \\textit{triggers implanted at a single stage can persist across multiple steps and propagate through intermediate states.} For instance, when using a GPT-based backbone, we observe trigger persistence in 43.58\\% of planning attacks, 77.97\\% of memory attacks, and 60.28\\% of tool-stage attacks, highlighting the vulnerabilities of the agentic workflow itself to backdoor threats.", "Building on this framework, we construct a standardized benchmark spanning four representative agent applications: \\textbf{Agent QA}, \\textbf{Agent Code}, \\textbf{Agent Web}, and \\textbf{Agent Drive}, covering both language-only and multimodal settings."], "limitations": ["Abstract-level evidence only: validate assumptions, evaluation protocol, and failure cases in the full paper before relying on this as key evidence."], "bibkey": "Feng2026Backdooragent"}
{"paper_id": "P0022", "title": "Beyond Max Tokens: Stealthy Resource Amplification via Tool Calling Chains in LLM Agents", "year": 2026, "url": "http://arxiv.org/abs/2601.10955v1", "arxiv_id": "2601.10955v1", "primary_category": "cs.CR", "categories": ["cs.CR", "cs.AI"], "pdf_url": "https://arxiv.org/pdf/2601.10955v1", "priority": "normal", "mapped_sections": ["3.2"], "evidence_level": "abstract", "fulltext_path": "", "authors": ["Kaiyu Zhou", "Yongsen Zheng", "Yicheng He", "Meng Xue", "Xueluan Gong", "Yuji Wang", "Kwok-Yan Lam"], "abstract": "The agent-tool communication loop is a critical attack surface in modern Large Language Model (LLM) agents. Existing Denial-of-Service (DoS) attacks, primarily triggered via user prompts or injected retrieval-augmented generation (RAG) context, are ineffective for this new paradigm. They are fundamentally single-turn and often lack a task-oriented approach, making them conspicuous in goal-oriented workflows and unable to exploit the compounding costs of multi-turn agent-tool interactions. We introduce a stealthy, multi-turn economic DoS attack that operates at the tool layer under the guise of a correctly completed task. Our method adjusts text-visible fields and a template-governed return policy in a benign, Model Context Protocol (MCP)-compatible tool server, optimizing these edits with a Monte Carlo Tree Search (MCTS) optimizer. These adjustments leave function signatures unchanged and preserve the final payload, steering the agent into prolonged, verbose tool-calling sequences using text-only notices. This compounds costs across turns, escaping single-turn caps while keeping the final answer correct to evade validation. Across six LLMs on the ToolBench and BFCL benchmarks, our attack expands tasks into trajectories exceeding 60,000 tokens, inflates costs by up to 658x, and raises energy by 100-560x. It drives GPU KV cache occupancy from <1% to 35-74% and cuts co-running throughput by approximately 50%. Because the server remains protocol-compatible and task outcomes are correct, conventional checks fail. These results elevate the agent-tool interface to a first-class security frontier, demanding a paradigm shift from validating final answers to monitoring the economic and computational cost of the entire agentic process.", "summary_bullets": ["The agent-tool communication loop is a critical attack surface in modern Large Language Model (LLM) agents.", "Existing Denial-of-Service (DoS) attacks, primarily triggered via user prompts or injected retrieval-augmented generation (RAG) context, are ineffective for this new paradigm.", "They are fundamentally single-turn and often lack a task-oriented approach, making them conspicuous in goal-oriented workflows and unable to exploit the compounding costs of multi-turn agent-tool interactions."], "method": "We introduce a stealthy, multi-turn economic DoS attack that operates at the tool layer under the guise of a correctly completed task.", "key_results": ["Across six LLMs on the ToolBench and BFCL benchmarks, our attack expands tasks into trajectories exceeding 60,000 tokens, inflates costs by up to 658x, and raises energy by 100-560x.", "It drives GPU KV cache occupancy from <1% to 35-74% and cuts co-running throughput by approximately 50%."], "limitations": ["Abstract-level evidence only: validate assumptions, evaluation protocol, and failure cases in the full paper before relying on this as key evidence."], "bibkey": "Zhou2026Beyond"}
{"paper_id": "P0023", "title": "Modeling LLM Agent Reviewer Dynamics in Elo-Ranked Review System", "year": 2026, "url": "http://arxiv.org/abs/2601.08829v1", "arxiv_id": "2601.08829v1", "primary_category": "cs.CL", "categories": ["cs.CL", "cs.AI"], "pdf_url": "https://arxiv.org/pdf/2601.08829v1", "priority": "normal", "mapped_sections": [], "evidence_level": "abstract", "fulltext_path": "", "authors": ["Hsiang-Wei Huang", "Junbin Lu", "Kuang-Ming Chen", "Jenq-Neng Hwang"], "abstract": "In this work, we explore the Large Language Model (LLM) agent reviewer dynamics in an Elo-ranked review system using real-world conference paper submissions. Multiple LLM agent reviewers with different personas are engage in multi round review interactions moderated by an Area Chair. We compare a baseline setting with conditions that incorporate Elo ratings and reviewer memory. Our simulation results showcase several interesting findings, including how incorporating Elo improves Area Chair decision accuracy, as well as reviewers' adaptive review strategy that exploits our Elo system without improving review effort. Our code is available at https://github.com/hsiangwei0903/EloReview.", "summary_bullets": ["In this work, we explore the Large Language Model (LLM) agent reviewer dynamics in an Elo-ranked review system using real-world conference paper submissions.", "Multiple LLM agent reviewers with different personas are engage in multi round review interactions moderated by an Area Chair.", "We compare a baseline setting with conditions that incorporate Elo ratings and reviewer memory."], "method": "In this work, we explore the Large Language Model (LLM) agent reviewer dynamics in an Elo-ranked review system using real-world conference paper submissions.", "key_results": ["Our simulation results showcase several interesting findings, including how incorporating Elo improves Area Chair decision accuracy, as well as reviewers' adaptive review strategy that exploits our Elo system without improving review effort."], "limitations": ["Abstract-level evidence only: validate assumptions, evaluation protocol, and failure cases in the full paper before relying on this as key evidence."], "bibkey": "Huang2026Modeling"}
{"paper_id": "P0024", "title": "A Survey of Large Language Model Agents for Question Answering", "year": 2025, "url": "http://arxiv.org/abs/2503.19213v1", "arxiv_id": "2503.19213v1", "primary_category": "cs.CL", "categories": ["cs.CL", "cs.AI", "cs.HC"], "pdf_url": "https://arxiv.org/pdf/2503.19213v1", "priority": "normal", "mapped_sections": [], "evidence_level": "abstract", "fulltext_path": "", "authors": ["Murong Yue"], "abstract": "This paper surveys the development of large language model (LLM)-based agents for question answering (QA). Traditional agents face significant limitations, including substantial data requirements and difficulty in generalizing to new environments. LLM-based agents address these challenges by leveraging LLMs as their core reasoning engine. These agents achieve superior QA results compared to traditional QA pipelines and naive LLM QA systems by enabling interaction with external environments. We systematically review the design of LLM agents in the context of QA tasks, organizing our discussion across key stages: planning, question understanding, information retrieval, and answer generation. Additionally, this paper identifies ongoing challenges and explores future research directions to enhance the performance of LLM agent QA systems.", "summary_bullets": ["This paper surveys the development of large language model (LLM)-based agents for question answering (QA).", "Traditional agents face significant limitations, including substantial data requirements and difficulty in generalizing to new environments.", "LLM-based agents address these challenges by leveraging LLMs as their core reasoning engine."], "method": "This paper surveys the development of large language model (LLM)-based agents for question answering (QA).", "key_results": ["Additionally, this paper identifies ongoing challenges and explores future research directions to enhance the performance of LLM agent QA systems."], "limitations": ["Abstract-level evidence only: validate assumptions, evaluation protocol, and failure cases in the full paper before relying on this as key evidence.", "Traditional agents face significant limitations, including substantial data requirements and difficulty in generalizing to new environments."], "bibkey": "Yue2025Survey"}
{"paper_id": "P0025", "title": "A Survey on Trustworthy LLM Agents: Threats and Countermeasures", "year": 2025, "url": "http://arxiv.org/abs/2503.09648v1", "arxiv_id": "2503.09648v1", "primary_category": "cs.MA", "categories": ["cs.MA", "cs.CY"], "pdf_url": "https://arxiv.org/pdf/2503.09648v1", "priority": "normal", "mapped_sections": [], "evidence_level": "abstract", "fulltext_path": "", "authors": ["Miao Yu", "Fanci Meng", "Xinyun Zhou", "Shilong Wang", "Junyuan Mao", "Linsey Pang", "Tianlong Chen", "Kun Wang", "Xinfeng Li", "Yongfeng Zhang", "Bo An", "Qingsong Wen"], "abstract": "With the rapid evolution of Large Language Models (LLMs), LLM-based agents and Multi-agent Systems (MAS) have significantly expanded the capabilities of LLM ecosystems. This evolution stems from empowering LLMs with additional modules such as memory, tools, environment, and even other agents. However, this advancement has also introduced more complex issues of trustworthiness, which previous research focused solely on LLMs could not cover. In this survey, we propose the TrustAgent framework, a comprehensive study on the trustworthiness of agents, characterized by modular taxonomy, multi-dimensional connotations, and technical implementation. By thoroughly investigating and summarizing newly emerged attacks, defenses, and evaluation methods for agents and MAS, we extend the concept of Trustworthy LLM to the emerging paradigm of Trustworthy Agent. In TrustAgent, we begin by deconstructing and introducing various components of the Agent and MAS. Then, we categorize their trustworthiness into intrinsic (brain, memory, and tool) and extrinsic (user, agent, and environment) aspects. Subsequently, we delineate the multifaceted meanings of trustworthiness and elaborate on the implementation techniques of existing research related to these internal and external modules. Finally, we present our insights and outlook on this domain, aiming to provide guidance for future endeavors.", "summary_bullets": ["With the rapid evolution of Large Language Models (LLMs), LLM-based agents and Multi-agent Systems (MAS) have significantly expanded the capabilities of LLM ecosystems.", "This evolution stems from empowering LLMs with additional modules such as memory, tools, environment, and even other agents.", "However, this advancement has also introduced more complex issues of trustworthiness, which previous research focused solely on LLMs could not cover."], "method": "In this survey, we propose the TrustAgent framework, a comprehensive study on the trustworthiness of agents, characterized by modular taxonomy, multi-dimensional connotations, and technical implementation.", "key_results": ["By thoroughly investigating and summarizing newly emerged attacks, defenses, and evaluation methods for agents and MAS, we extend the concept of Trustworthy LLM to the emerging paradigm of Trustworthy Agent."], "limitations": ["Abstract-level evidence only: validate assumptions, evaluation protocol, and failure cases in the full paper before relying on this as key evidence."], "bibkey": "Yu2025Survey"}
{"paper_id": "P0026", "title": "A Survey on the Optimization of Large Language Model-based Agents", "year": 2025, "url": "http://arxiv.org/abs/2503.12434v1", "arxiv_id": "2503.12434v1", "primary_category": "cs.AI", "categories": ["cs.AI"], "pdf_url": "https://arxiv.org/pdf/2503.12434v1", "priority": "high", "mapped_sections": ["3.2", "4.2"], "evidence_level": "abstract", "fulltext_path": "", "authors": ["Shangheng Du", "Jiabao Zhao", "Jinxin Shi", "Zhentao Xie", "Xin Jiang", "Yanhong Bai", "Liang He"], "abstract": "With the rapid development of Large Language Models (LLMs), LLM-based agents have been widely adopted in various fields, becoming essential for autonomous decision-making and interactive tasks. However, current work typically relies on prompt design or fine-tuning strategies applied to vanilla LLMs, which often leads to limited effectiveness or suboptimal performance in complex agent-related environments. Although LLM optimization techniques can improve model performance across many general tasks, they lack specialized optimization towards critical agent functionalities such as long-term planning, dynamic environmental interaction, and complex decision-making. Although numerous recent studies have explored various strategies to optimize LLM-based agents for complex agent tasks, a systematic review summarizing and comparing these methods from a holistic perspective is still lacking. In this survey, we provide a comprehensive review of LLM-based agent optimization approaches, categorizing them into parameter-driven and parameter-free methods. We first focus on parameter-driven optimization, covering fine-tuning-based optimization, reinforcement learning-based optimization, and hybrid strategies, analyzing key aspects such as trajectory data construction, fine-tuning techniques, reward function design, and optimization algorithms. Additionally, we briefly discuss parameter-free strategies that optimize agent behavior through prompt engineering and external knowledge retrieval. Finally, we summarize the datasets and benchmarks used for evaluation and tuning, review key applications of LLM-based agents, and discuss major challenges and promising future directions. Our repository for related references is available at https://github.com/YoungDubbyDu/LLM-Agent-Optimization.", "summary_bullets": ["With the rapid development of Large Language Models (LLMs), LLM-based agents have been widely adopted in various fields, becoming essential for autonomous decision-making and interactive tasks.", "However, current work typically relies on prompt design or fine-tuning strategies applied to vanilla LLMs, which often leads to limited effectiveness or suboptimal performance in complex agent-related environments.", "Although LLM optimization techniques can improve model performance across many general tasks, they lack specialized optimization towards critical agent functionalities such as long-term planning, dynamic environmental interaction, and complex decision-making.", "Although numerous recent studies have explored various strategies to optimize LLM-based agents for complex agent tasks, a systematic review summarizing and comparing these methods from a holistic perspective is still lacking.", "In this survey, we provide a comprehensive review of LLM-based agent optimization approaches, categorizing them into parameter-driven and parameter-free methods."], "method": "With the rapid development of Large Language Models (LLMs), LLM-based agents have been widely adopted in various fields, becoming essential for autonomous decision-making and interactive tasks.", "key_results": ["Finally, we summarize the datasets and benchmarks used for evaluation and tuning, review key applications of LLM-based agents, and discuss major challenges and promising future directions."], "limitations": ["Abstract-level evidence only: validate assumptions, evaluation protocol, and failure cases in the full paper before relying on this as key evidence."], "bibkey": "Du2025Survey"}
{"paper_id": "P0027", "title": "Agentic Reasoning: A Streamlined Framework for Enhancing LLM Reasoning with Agentic Tools", "year": 2025, "url": "http://arxiv.org/abs/2502.04644v2", "arxiv_id": "2502.04644v2", "primary_category": "cs.AI", "categories": ["cs.AI", "cs.CL"], "pdf_url": "https://arxiv.org/pdf/2502.04644v2", "priority": "normal", "mapped_sections": ["4.1"], "evidence_level": "abstract", "fulltext_path": "", "authors": ["Junde Wu", "Jiayuan Zhu", "Yuyuan Liu", "Min Xu", "Yueming Jin"], "abstract": "We introduce Agentic Reasoning, a framework that enhances large language model (LLM) reasoning by integrating external tool-using agents. Agentic Reasoning dynamically leverages web search, code execution, and structured memory to address complex problems requiring deep research. A key innovation in our framework is the Mind-Map agent, which constructs a structured knowledge graph to store reasoning context and track logical relationships, ensuring coherence in long reasoning chains with extensive tool usage. Additionally, we conduct a comprehensive exploration of the Web-Search agent, leading to a highly effective search mechanism that surpasses all prior approaches. When deployed on DeepSeek-R1, our method achieves a new state-of-the-art (SOTA) among public models and delivers performance comparable to OpenAI Deep Research, the leading proprietary model in this domain. Extensive ablation studies validate the optimal selection of agentic tools and confirm the effectiveness of our Mind-Map and Web-Search agents in enhancing LLM reasoning. The code is at: https://github.com/theworldofagents/Agentic-Reasoning", "summary_bullets": ["We introduce Agentic Reasoning, a framework that enhances large language model (LLM) reasoning by integrating external tool-using agents.", "Agentic Reasoning dynamically leverages web search, code execution, and structured memory to address complex problems requiring deep research.", "A key innovation in our framework is the Mind-Map agent, which constructs a structured knowledge graph to store reasoning context and track logical relationships, ensuring coherence in long reasoning chains with extensive tool usage."], "method": "We introduce Agentic Reasoning, a framework that enhances large language model (LLM) reasoning by integrating external tool-using agents.", "key_results": ["When deployed on DeepSeek-R1, our method achieves a new state-of-the-art (SOTA) among public models and delivers performance comparable to OpenAI Deep Research, the leading proprietary model in this domain."], "limitations": ["Abstract-level evidence only: validate assumptions, evaluation protocol, and failure cases in the full paper before relying on this as key evidence."], "bibkey": "Wu2025Agentic"}
{"paper_id": "P0028", "title": "Can Compressed LLMs Truly Act? An Empirical Evaluation of Agentic Capabilities in LLM Compression", "year": 2025, "url": "http://arxiv.org/abs/2505.19433v2", "arxiv_id": "2505.19433v2", "primary_category": "cs.LG", "categories": ["cs.LG"], "pdf_url": "https://arxiv.org/pdf/2505.19433v2", "priority": "normal", "mapped_sections": ["6.1"], "evidence_level": "abstract", "fulltext_path": "", "authors": ["Peijie Dong", "Zhenheng Tang", "Xiang Liu", "Lujun Li", "Xiaowen Chu", "Bo Li"], "abstract": "Post-training compression reduces the computational and memory costs of large language models (LLMs), enabling resource-efficient deployment. However, existing compression benchmarks only focus on language modeling (e.g., perplexity) and natural language understanding tasks (e.g., GLUE accuracy), ignoring the agentic capabilities - workflow, tool use/function call, long-context understanding and real-world application. We introduce the Agent Compression Benchmark (ACBench), the first comprehensive benchmark for evaluating how compression impacts LLMs' agentic abilities. ACBench spans (1) 12 tasks across 4 capabilities (e.g., WorfBench for workflow generation, Needle-in-Haystack for long-context retrieval), (2) quantization (GPTQ, AWQ) and pruning (Wanda, SparseGPT), and (3) 15 models, including small (Gemma-2B), standard (Qwen2.5 7B-32B), and distilled reasoning LLMs (DeepSeek-R1-Distill). Our experiments reveal compression tradeoffs: 4-bit quantization preserves workflow generation and tool use (1%-3% drop) but degrades real-world application accuracy by 10%-15%. We introduce ERank, Top-k Ranking Correlation and Energy to systematize analysis. ACBench provides actionable insights for optimizing LLM compression in agentic scenarios. The code can be found in https://github.com/pprp/ACBench.", "summary_bullets": ["Post-training compression reduces the computational and memory costs of large language models (LLMs), enabling resource-efficient deployment.", "However, existing compression benchmarks only focus on language modeling (e.g., perplexity) and natural language understanding tasks (e.g., GLUE accuracy), ignoring the agentic capabilities - workflow, tool use/function call, long-context understanding and real-world application.", "We introduce the Agent Compression Benchmark (ACBench), the first comprehensive benchmark for evaluating how compression impacts LLMs' agentic abilities."], "method": "We introduce the Agent Compression Benchmark (ACBench), the first comprehensive benchmark for evaluating how compression impacts LLMs' agentic abilities.", "key_results": ["Our experiments reveal compression tradeoffs: 4-bit quantization preserves workflow generation and tool use (1%-3% drop) but degrades real-world application accuracy by 10%-15%.", "ACBench spans (1) 12 tasks across 4 capabilities (e.g., WorfBench for workflow generation, Needle-in-Haystack for long-context retrieval), (2) quantization (GPTQ, AWQ) and pruning (Wanda, SparseGPT), and (3) 15 models, including small (Gemma-2B), standard (Qwen2.5 7B-32B), and distilled reasoning LLMs (DeepSeek-R1-Distill)."], "limitations": ["Abstract-level evidence only: validate assumptions, evaluation protocol, and failure cases in the full paper before relying on this as key evidence."], "bibkey": "Dong2025Compressed"}
{"paper_id": "P0029", "title": "Group-in-Group Policy Optimization for LLM Agent Training", "year": 2025, "url": "http://arxiv.org/abs/2505.10978v3", "arxiv_id": "2505.10978v3", "primary_category": "cs.LG", "categories": ["cs.LG", "cs.AI"], "pdf_url": "https://arxiv.org/pdf/2505.10978v3", "priority": "normal", "mapped_sections": ["3.1"], "evidence_level": "abstract", "fulltext_path": "", "authors": ["Lang Feng", "Zhenghai Xue", "Tingcong Liu", "Bo An"], "abstract": "Recent advances in group-based reinforcement learning (RL) have driven frontier large language models (LLMs) in single-turn tasks like mathematical reasoning. However, their scalability to multi-turn LLM agent training remains limited. Unlike static tasks, agent-environment interactions unfold over many steps and often yield sparse or delayed rewards, making credit assignment across individual steps significantly more challenging. In this work, we propose Group-in-Group Policy Optimization (GiGPO), a novel RL algorithm that achieves fine-grained credit assignment for LLM agents while preserving the appealing properties of group-based RL: critic-free, low memory, and stable convergence. GiGPO introduces a two-level structure for estimating relative advantage: (i) At the episode-level, GiGPO computes macro relative advantages based on groups of complete trajectories; (ii) At the step-level, GiGPO introduces an anchor state grouping mechanism that retroactively constructs step-level groups by identifying repeated environment states across trajectories. Actions stemming from the same state are grouped together, enabling micro relative advantage estimation. This hierarchical structure effectively captures both global trajectory quality and local step effectiveness without relying on auxiliary models or additional rollouts. We evaluate GiGPO on challenging agent benchmarks, including ALFWorld and WebShop, as well as tool-integrated reasoning on search-augmented QA tasks, using Qwen2.5-1.5B/3B/7B-Instruct. Crucially, GiGPO delivers fine-grained per-step credit signals, achieves performance gains of > 12% on ALFWorld and > 9% on WebShop over GRPO, and obtains superior performance on QA tasks (42.1% on 3B and 47.2% on 7B): all while maintaining the same GPU memory overhead, identical LLM rollout, and incurring little to no additional time cost.", "summary_bullets": ["Recent advances in group-based reinforcement learning (RL) have driven frontier large language models (LLMs) in single-turn tasks like mathematical reasoning.", "However, their scalability to multi-turn LLM agent training remains limited.", "Unlike static tasks, agent-environment interactions unfold over many steps and often yield sparse or delayed rewards, making credit assignment across individual steps significantly more challenging."], "method": "In this work, we propose Group-in-Group Policy Optimization (GiGPO), a novel RL algorithm that achieves fine-grained credit assignment for LLM agents while preserving the appealing properties of group-based RL: critic-free, low memory, and stable convergence.", "key_results": ["We evaluate GiGPO on challenging agent benchmarks, including ALFWorld and WebShop, as well as tool-integrated reasoning on search-augmented QA tasks, using Qwen2.5-1.5B/3B/7B-Instruct.", "Crucially, GiGPO delivers fine-grained per-step credit signals, achieves performance gains of > 12% on ALFWorld and > 9% on WebShop over GRPO, and obtains superior performance on QA tasks (42.1% on 3B and 47.2% on 7B): all while maintaining the same GPU memory overhead, identical LLM rollout, and incurring little to no additional time cost."], "limitations": ["Abstract-level evidence only: validate assumptions, evaluation protocol, and failure cases in the full paper before relying on this as key evidence."], "bibkey": "Feng2025Group"}
{"paper_id": "P0030", "title": "Reasoning-Style Poisoning of LLM Agents via Stealthy Style Transfer: Process-Level Attacks and Runtime Monitoring in RSV Space", "year": 2025, "url": "http://arxiv.org/abs/2512.14448v1", "arxiv_id": "2512.14448v1", "primary_category": "cs.CR", "categories": ["cs.CR", "cs.AI"], "pdf_url": "https://arxiv.org/pdf/2512.14448v1", "priority": "high", "mapped_sections": ["4.1", "6.2"], "evidence_level": "abstract", "fulltext_path": "", "authors": ["Xingfu Zhou", "Pengfei Wang"], "abstract": "Large Language Model (LLM) agents relying on external retrieval are increasingly deployed in high-stakes environments. While existing adversarial attacks primarily focus on content falsification or instruction injection, we identify a novel, process-oriented attack surface: the agent's reasoning style. We propose Reasoning-Style Poisoning (RSP), a paradigm that manipulates how agents process information rather than what they process. We introduce Generative Style Injection (GSI), an attack method that rewrites retrieved documents into pathological tones--specifically \"analysis paralysis\" or \"cognitive haste\"--without altering underlying facts or using explicit triggers. To quantify these shifts, we develop the Reasoning Style Vector (RSV), a metric tracking Verification depth, Self-confidence, and Attention focus. Experiments on HotpotQA and FEVER using ReAct, Reflection, and Tree of Thoughts (ToT) architectures reveal that GSI significantly degrades performance. It increases reasoning steps by up to 4.4 times or induces premature errors, successfully bypassing state-of-the-art content filters. Finally, we propose RSP-M, a lightweight runtime monitor that calculates RSV metrics in real-time and triggers alerts when values exceed safety thresholds. Our work demonstrates that reasoning style is a distinct, exploitable vulnerability, necessitating process-level defenses beyond static content analysis.", "summary_bullets": ["Large Language Model (LLM) agents relying on external retrieval are increasingly deployed in high-stakes environments.", "While existing adversarial attacks primarily focus on content falsification or instruction injection, we identify a novel, process-oriented attack surface: the agent's reasoning style.", "We propose Reasoning-Style Poisoning (RSP), a paradigm that manipulates how agents process information rather than what they process.", "We introduce Generative Style Injection (GSI), an attack method that rewrites retrieved documents into pathological tones--specifically \"analysis paralysis\" or \"cognitive haste\"--without altering underlying facts or using explicit triggers.", "To quantify these shifts, we develop the Reasoning Style Vector (RSV), a metric tracking Verification depth, Self-confidence, and Attention focus."], "method": "We propose Reasoning-Style Poisoning (RSP), a paradigm that manipulates how agents process information rather than what they process.", "key_results": ["It increases reasoning steps by up to 4.4 times or induces premature errors, successfully bypassing state-of-the-art content filters.", "To quantify these shifts, we develop the Reasoning Style Vector (RSV), a metric tracking Verification depth, Self-confidence, and Attention focus."], "limitations": ["Abstract-level evidence only: validate assumptions, evaluation protocol, and failure cases in the full paper before relying on this as key evidence."], "bibkey": "Zhou2025Reasoning"}
{"paper_id": "P0031", "title": "Talk Less, Call Right: Enhancing Role-Play LLM Agents with Automatic Prompt Optimization and Role Prompting", "year": 2025, "url": "http://arxiv.org/abs/2509.00482v2", "arxiv_id": "2509.00482v2", "primary_category": "cs.CL", "categories": ["cs.CL", "cs.AI", "cs.HC"], "pdf_url": "https://arxiv.org/pdf/2509.00482v2", "priority": "normal", "mapped_sections": [], "evidence_level": "abstract", "fulltext_path": "", "authors": ["Saksorn Ruangtanusak", "Pittawat Taveekitworachai", "Kunat Pipatanakul"], "abstract": "This report investigates approaches for prompting a tool-augmented large language model (LLM) to act as a role-playing dialogue agent in the API track of the Commonsense Persona-grounded Dialogue Challenge (CPDC) 2025. In this setting, dialogue agents often produce overly long in-character responses (over-speaking) while failing to use tools effectively according to the persona (under-acting), such as generating function calls that do not exist or making unnecessary tool calls before answering. We explore four prompting approaches to address these issues: 1) basic role prompting, 2) improved role prompting, 3) automatic prompt optimization (APO), and 4) rule-based role prompting. The rule-based role prompting (RRP) approach achieved the best performance through two novel techniques-character-card/scene-contract design and strict enforcement of function calling-which led to an overall score of 0.571, improving on the zero-shot baseline score of 0.519. These findings demonstrate that RRP design can substantially improve the effectiveness and reliability of role-playing dialogue agents compared with more elaborate methods such as APO. To support future efforts in developing persona prompts, we are open-sourcing all of our best-performing prompts and the APO tool Source code is available at https://github.com/scb-10x/apo", "summary_bullets": ["This report investigates approaches for prompting a tool-augmented large language model (LLM) to act as a role-playing dialogue agent in the API track of the Commonsense Persona-grounded Dialogue Challenge (CPDC) 2025.", "In this setting, dialogue agents often produce overly long in-character responses (over-speaking) while failing to use tools effectively according to the persona (under-acting), such as generating function calls that do not exist or making unnecessary tool calls before answering.", "We explore four prompting approaches to address these issues: 1) basic role prompting, 2) improved role prompting, 3) automatic prompt optimization (APO), and 4) rule-based role prompting."], "method": "This report investigates approaches for prompting a tool-augmented large language model (LLM) to act as a role-playing dialogue agent in the API track of the Commonsense Persona-grounded Dialogue Challenge (CPDC) 2025.", "key_results": ["This report investigates approaches for prompting a tool-augmented large language model (LLM) to act as a role-playing dialogue agent in the API track of the Commonsense Persona-grounded Dialogue Challenge (CPDC) 2025.", "We explore four prompting approaches to address these issues: 1) basic role prompting, 2) improved role prompting, 3) automatic prompt optimization (APO), and 4) rule-based role prompting."], "limitations": ["Abstract-level evidence only: validate assumptions, evaluation protocol, and failure cases in the full paper before relying on this as key evidence."], "bibkey": "Ruangtanusak2025Talk"}
{"paper_id": "P0032", "title": "Task Memory Engine: Spatial Memory for Robust Multi-Step LLM Agents", "year": 2025, "url": "http://arxiv.org/abs/2505.19436v1", "arxiv_id": "2505.19436v1", "primary_category": "cs.AI", "categories": ["cs.AI", "cs.CL"], "pdf_url": "https://arxiv.org/pdf/2505.19436v1", "priority": "normal", "mapped_sections": ["4.2"], "evidence_level": "abstract", "fulltext_path": "", "authors": ["Ye Ye"], "abstract": "Large Language Models (LLMs) falter in multi-step interactions -- often hallucinating, repeating actions, or misinterpreting user corrections -- due to reliance on linear, unstructured context. This fragility stems from the lack of persistent memory to track evolving goals and task dependencies, undermining trust in autonomous agents. We introduce the Task Memory Engine (TME), a modular memory controller that transforms existing LLMs into robust, revision-aware agents without fine-tuning. TME implements a spatial memory framework that replaces flat context with graph-based structures to support consistent, multi-turn reasoning. Departing from linear concatenation and ReAct-style prompting, TME builds a dynamic task graph -- either a tree or directed acyclic graph (DAG) -- to map user inputs to subtasks, align them with prior context, and enable dependency-tracked revisions. Its Task Representation and Intent Management (TRIM) component models task semantics and user intent to ensure accurate interpretation. Across four multi-turn scenarios-trip planning, cooking, meeting scheduling, and shopping cart editing -- TME eliminates 100% of hallucinations and misinterpretations in three tasks, and reduces hallucinations by 66.7% and misinterpretations by 83.3% across 27 user turns, outperforming ReAct. TME's modular design supports plug-and-play deployment and domain-specific customization, adaptable to both personal assistants and enterprise automation. We release TME's codebase, benchmarks, and components as open-source resources, enabling researchers to develop reliable LLM agents. TME's scalable architecture addresses a critical gap in agent performance across complex, interactive settings.", "summary_bullets": ["Large Language Models (LLMs) falter in multi-step interactions -- often hallucinating, repeating actions, or misinterpreting user corrections -- due to reliance on linear, unstructured context.", "This fragility stems from the lack of persistent memory to track evolving goals and task dependencies, undermining trust in autonomous agents.", "We introduce the Task Memory Engine (TME), a modular memory controller that transforms existing LLMs into robust, revision-aware agents without fine-tuning."], "method": "We introduce the Task Memory Engine (TME), a modular memory controller that transforms existing LLMs into robust, revision-aware agents without fine-tuning.", "key_results": ["Across four multi-turn scenarios-trip planning, cooking, meeting scheduling, and shopping cart editing -- TME eliminates 100% of hallucinations and misinterpretations in three tasks, and reduces hallucinations by 66.7% and misinterpretations by 83.3% across 27 user turns, outperforming ReAct.", "We release TME's codebase, benchmarks, and components as open-source resources, enabling researchers to develop reliable LLM agents."], "limitations": ["Abstract-level evidence only: validate assumptions, evaluation protocol, and failure cases in the full paper before relying on this as key evidence."], "bibkey": "Ye2025Task"}
{"paper_id": "P0033", "title": "Where LLM Agents Fail and How They can Learn From Failures", "year": 2025, "url": "http://arxiv.org/abs/2509.25370v1", "arxiv_id": "2509.25370v1", "primary_category": "cs.AI", "categories": ["cs.AI"], "pdf_url": "https://arxiv.org/pdf/2509.25370v1", "priority": "high", "mapped_sections": ["4.2", "6.1"], "evidence_level": "abstract", "fulltext_path": "", "authors": ["Kunlun Zhu", "Zijia Liu", "Bingxuan Li", "Muxin Tian", "Yingxuan Yang", "Jiaxun Zhang", "Pengrui Han", "Qipeng Xie", "Fuyang Cui", "Weijia Zhang", "Xiaoteng Ma", "Xiaodong Yu", "Gowtham Ramesh", "Jialian Wu", "Zicheng Liu", "Pan Lu", "James Zou", "Jiaxuan You"], "abstract": "Large Language Model (LLM) agents, which integrate planning, memory, reflection, and tool-use modules, have shown promise in solving complex, multi-step tasks. Yet their sophisticated architectures amplify vulnerability to cascading failures, where a single root-cause error propagates through subsequent decisions, leading to task failure. Current systems lack a framework that can comprehensively understand agent error in a modular and systemic way, and therefore fail to detect these errors accordingly. We address this gap with three contributions. First, we introduce the AgentErrorTaxonomy, a modular classification of failure modes spanning memory, reflection, planning, action, and system-level operations. Second, we construct AgentErrorBench, the first dataset of systematically annotated failure trajectories from ALFWorld, GAIA, and WebShop, grounding error analysis in real-world agent rollouts. Third, we propose AgentDebug, a debugging framework that isolates root-cause failures and provides corrective feedback, enabling agents to recover and iteratively improve. Experiments on AgentErrorBench show that AgentDebug achieves 24% higher all-correct accuracy and 17% higher step accuracy compared to the strongest baseline. Beyond detection, the targeted feedback generated by AgentDebug enables LLM agents to iteratively recover from failures, yielding up to 26% relative improvements in task success across ALFWorld, GAIA, and WebShop. These results establish principled debugging as a pathway to more reliable and adaptive LLM agents. The code and data will be available at https://github.com/ulab-uiuc/AgentDebug", "summary_bullets": ["Large Language Model (LLM) agents, which integrate planning, memory, reflection, and tool-use modules, have shown promise in solving complex, multi-step tasks.", "Yet their sophisticated architectures amplify vulnerability to cascading failures, where a single root-cause error propagates through subsequent decisions, leading to task failure.", "Current systems lack a framework that can comprehensively understand agent error in a modular and systemic way, and therefore fail to detect these errors accordingly.", "We address this gap with three contributions.", "First, we introduce the AgentErrorTaxonomy, a modular classification of failure modes spanning memory, reflection, planning, action, and system-level operations."], "method": "First, we introduce the AgentErrorTaxonomy, a modular classification of failure modes spanning memory, reflection, planning, action, and system-level operations.", "key_results": ["Experiments on AgentErrorBench show that AgentDebug achieves 24% higher all-correct accuracy and 17% higher step accuracy compared to the strongest baseline.", "Beyond detection, the targeted feedback generated by AgentDebug enables LLM agents to iteratively recover from failures, yielding up to 26% relative improvements in task success across ALFWorld, GAIA, and WebShop."], "limitations": ["Abstract-level evidence only: validate assumptions, evaluation protocol, and failure cases in the full paper before relying on this as key evidence."], "bibkey": "Zhu2025Where"}
{"paper_id": "P0034", "title": "Youtu-LLM: Unlocking the Native Agentic Potential for Lightweight Large Language Models", "year": 2025, "url": "http://arxiv.org/abs/2512.24618v2", "arxiv_id": "2512.24618v2", "primary_category": "cs.CL", "categories": ["cs.CL"], "pdf_url": "https://arxiv.org/pdf/2512.24618v2", "priority": "normal", "mapped_sections": ["4.2"], "evidence_level": "abstract", "fulltext_path": "", "authors": ["Junru Lu", "Jiarui Qin", "Lingfeng Qiao", "Yinghui Li", "Xinyi Dai", "Bo Ke", "Jianfeng He", "Ruizhi Qiao", "Di Yin", "Xing Sun", "Yunsheng Wu", "Yinsong Liu", "Shuangyin Liu", "Mingkong Tang", "Haodong Lin", "Jiayi Kuang", "Fanxu Meng", "Xiaojuan Tang", "Yunjia Xi", "Junjie Huang", "Haotong Yang", "Zhenyi Shen", "Yangning Li", "Qianwen Zhang", "Yifei Yu", "Siyu An", "Junnan Dong", "Qiufeng Wang", "Jie Wang", "Keyu Chen", "Wei Wen", "Taian Guo", "Zhifeng Shen", "Daohai Yu", "Jiahao Li", "Ke Li", "Zongyi Li", "Xiaoyu Tan"], "abstract": "We introduce Youtu-LLM, a lightweight yet powerful language model that harmonizes high computational efficiency with native agentic intelligence. Unlike typical small models that rely on distillation, Youtu-LLM (1.96B) is pre-trained from scratch to systematically cultivate reasoning and planning capabilities. The key technical advancements are as follows: (1) Compact Architecture with Long-Context Support: Built on a dense Multi-Latent Attention (MLA) architecture with a novel STEM-oriented vocabulary, Youtu-LLM supports a 128k context window. This design enables robust long-context reasoning and state tracking within a minimal memory footprint, making it ideal for long-horizon agent and reasoning tasks. (2) Principled \"Commonsense-STEM-Agent\" Curriculum: We curated a massive corpus of approximately 11T tokens and implemented a multi-stage training strategy. By progressively shifting the pre-training data distribution from general commonsense to complex STEM and agentic tasks, we ensure the model acquires deep cognitive abilities rather than superficial alignment. (3) Scalable Agentic Mid-training: Specifically for the agentic mid-training, we employ diverse data construction schemes to synthesize rich and varied trajectories across math, coding, and tool-use domains. This high-quality data enables the model to internalize planning and reflection behaviors effectively. Extensive evaluations show that Youtu-LLM sets a new state-of-the-art for sub-2B LLMs. On general benchmarks, it achieves competitive performance against larger models, while on agent-specific tasks, it significantly surpasses existing SOTA baselines, demonstrating that lightweight models can possess strong intrinsic agentic capabilities.", "summary_bullets": ["We introduce Youtu-LLM, a lightweight yet powerful language model that harmonizes high computational efficiency with native agentic intelligence.", "Unlike typical small models that rely on distillation, Youtu-LLM (1.96B) is pre-trained from scratch to systematically cultivate reasoning and planning capabilities.", "The key technical advancements are as follows: (1) Compact Architecture with Long-Context Support: Built on a dense Multi-Latent Attention (MLA) architecture with a novel STEM-oriented vocabulary, Youtu-LLM supports a 128k context window."], "method": "We introduce Youtu-LLM, a lightweight yet powerful language model that harmonizes high computational efficiency with native agentic intelligence.", "key_results": ["Unlike typical small models that rely on distillation, Youtu-LLM (1.96B) is pre-trained from scratch to systematically cultivate reasoning and planning capabilities.", "The key technical advancements are as follows: (1) Compact Architecture with Long-Context Support: Built on a dense Multi-Latent Attention (MLA) architecture with a novel STEM-oriented vocabulary, Youtu-LLM supports a 128k context window."], "limitations": ["Abstract-level evidence only: validate assumptions, evaluation protocol, and failure cases in the full paper before relying on this as key evidence."], "bibkey": "Lu2025Youtu"}
{"paper_id": "P0035", "title": "A Survey on Large Language Model-Based Game Agents", "year": 2024, "url": "http://arxiv.org/abs/2404.02039v4", "arxiv_id": "2404.02039v4", "primary_category": "cs.AI", "categories": ["cs.AI"], "pdf_url": "https://arxiv.org/pdf/2404.02039v4", "priority": "normal", "mapped_sections": ["5.1"], "evidence_level": "abstract", "fulltext_path": "", "authors": ["Sihao Hu", "Tiansheng Huang", "Gaowen Liu", "Ramana Rao Kompella", "Fatih Ilhan", "Selim Furkan Tekin", "Yichang Xu", "Zachary Yahn", "Ling Liu"], "abstract": "Game environments provide rich, controllable settings that stimulate many aspects of real-world complexity. As such, game agents offer a valuable testbed for exploring capabilities relevant to Artificial General Intelligence. Recently, the emergence of Large Language Models (LLMs) provides new opportunities to endow these agents with generalizable reasoning, memory, and adaptability in complex game environments. This survey offers an up-to-date review of LLM-based game agents (LLMGAs) through a unified reference architecture. At the single-agent level, we synthesize existing studies around three core components: memory, reasoning, and perception-action interfaces, which jointly characterize how language enables agents to perceive, think, and act. At the multi-agent level, we outline how communication protocols and organizational models support coordination, role differentiation, and large-scale social behaviors. To contextualize these designs, we introduce a challenge-centered taxonomy linking six major game genres to their dominant agent requirements, from low-latency control in action games to open-ended goal formation in sandbox worlds. A curated list of related papers is available at https://github.com/git-disl/awesome-LLM-game-agent-papers", "summary_bullets": ["Game environments provide rich, controllable settings that stimulate many aspects of real-world complexity.", "As such, game agents offer a valuable testbed for exploring capabilities relevant to Artificial General Intelligence.", "Recently, the emergence of Large Language Models (LLMs) provides new opportunities to endow these agents with generalizable reasoning, memory, and adaptability in complex game environments."], "method": "To contextualize these designs, we introduce a challenge-centered taxonomy linking six major game genres to their dominant agent requirements, from low-latency control in action games to open-ended goal formation in sandbox worlds.", "key_results": ["A curated list of related papers is available at https://github.com/git-disl/awesome-LLM-game-agent-papers"], "limitations": ["Abstract-level evidence only: validate assumptions, evaluation protocol, and failure cases in the full paper before relying on this as key evidence."], "bibkey": "Hu2024Survey"}
{"paper_id": "P0036", "title": "A Survey on Large Language Model-based Agents for Statistics and Data Science", "year": 2024, "url": "http://arxiv.org/abs/2412.14222v2", "arxiv_id": "2412.14222v2", "primary_category": "cs.AI", "categories": ["cs.AI", "cs.CL", "cs.LG", "stat.OT"], "pdf_url": "https://arxiv.org/pdf/2412.14222v2", "priority": "normal", "mapped_sections": ["6.2"], "evidence_level": "abstract", "fulltext_path": "", "authors": ["Maojun Sun", "Ruijian Han", "Binyan Jiang", "Houduo Qi", "Defeng Sun", "Yancheng Yuan", "Jian Huang"], "abstract": "In recent years, data science agents powered by Large Language Models (LLMs), known as \"data agents,\" have shown significant potential to transform the traditional data analysis paradigm. This survey provides an overview of the evolution, capabilities, and applications of LLM-based data agents, highlighting their role in simplifying complex data tasks and lowering the entry barrier for users without related expertise. We explore current trends in the design of LLM-based frameworks, detailing essential features such as planning, reasoning, reflection, multi-agent collaboration, user interface, knowledge integration, and system design, which enable agents to address data-centric problems with minimal human intervention. Furthermore, we analyze several case studies to demonstrate the practical applications of various data agents in real-world scenarios. Finally, we identify key challenges and propose future research directions to advance the development of data agents into intelligent statistical analysis software.", "summary_bullets": ["In recent years, data science agents powered by Large Language Models (LLMs), known as \"data agents,\" have shown significant potential to transform the traditional data analysis paradigm.", "This survey provides an overview of the evolution, capabilities, and applications of LLM-based data agents, highlighting their role in simplifying complex data tasks and lowering the entry barrier for users without related expertise.", "We explore current trends in the design of LLM-based frameworks, detailing essential features such as planning, reasoning, reflection, multi-agent collaboration, user interface, knowledge integration, and system design, which enable agents to address data-centric problems with minimal human intervention."], "method": "Furthermore, we analyze several case studies to demonstrate the practical applications of various data agents in real-world scenarios.", "key_results": ["We explore current trends in the design of LLM-based frameworks, detailing essential features such as planning, reasoning, reflection, multi-agent collaboration, user interface, knowledge integration, and system design, which enable agents to address data-centric problems with minimal human intervention."], "limitations": ["Abstract-level evidence only: validate assumptions, evaluation protocol, and failure cases in the full paper before relying on this as key evidence."], "bibkey": "Sun2024Survey"}
{"paper_id": "P0037", "title": "AgentSquare: Automatic LLM Agent Search in Modular Design Space", "year": 2024, "url": "http://arxiv.org/abs/2410.06153v3", "arxiv_id": "2410.06153v3", "primary_category": "cs.CL", "categories": ["cs.CL"], "pdf_url": "https://arxiv.org/pdf/2410.06153v3", "priority": "high", "mapped_sections": ["3.1", "6.1"], "evidence_level": "abstract", "fulltext_path": "", "authors": ["Yu Shang", "Yu Li", "Keyu Zhao", "Likai Ma", "Jiahe Liu", "Fengli Xu", "Yong Li"], "abstract": "Recent advancements in Large Language Models (LLMs) have led to a rapid growth of agentic systems capable of handling a wide range of complex tasks. However, current research largely relies on manual, task-specific design, limiting their adaptability to novel tasks. In this paper, we introduce a new research problem: Modularized LLM Agent Search (MoLAS). We propose a modular design space that abstracts existing LLM agent designs into four fundamental modules with uniform IO interface: Planning, Reasoning, Tool Use, and Memory. Building on this design space, we present a novel LLM agent search framework called AgentSquare, which introduces two core mechanisms, i.e., module evolution and recombination, to efficiently search for optimized LLM agents. To further accelerate the process, we design a performance predictor that uses in-context surrogate models to skip unpromising agent designs. Extensive experiments across six benchmarks, covering the diverse scenarios of web, embodied, tool use and game applications, show that AgentSquare substantially outperforms hand-crafted agents, achieving an average performance gain of 17.2% against best-known human designs. Moreover, AgentSquare can generate interpretable design insights, enabling a deeper understanding of agentic architecture and its impact on task performance. We believe that the modular design space and AgentSquare search framework offer a platform for fully exploiting the potential of prior successful designs and consolidating the collective efforts of research community. Code repo is available at https://github.com/tsinghua-fib-lab/AgentSquare.", "summary_bullets": ["Recent advancements in Large Language Models (LLMs) have led to a rapid growth of agentic systems capable of handling a wide range of complex tasks.", "However, current research largely relies on manual, task-specific design, limiting their adaptability to novel tasks.", "In this paper, we introduce a new research problem: Modularized LLM Agent Search (MoLAS).", "We propose a modular design space that abstracts existing LLM agent designs into four fundamental modules with uniform IO interface: Planning, Reasoning, Tool Use, and Memory.", "Building on this design space, we present a novel LLM agent search framework called AgentSquare, which introduces two core mechanisms, i.e., module evolution and recombination, to efficiently search for optimized LLM agents."], "method": "In this paper, we introduce a new research problem: Modularized LLM Agent Search (MoLAS).", "key_results": ["Extensive experiments across six benchmarks, covering the diverse scenarios of web, embodied, tool use and game applications, show that AgentSquare substantially outperforms hand-crafted agents, achieving an average performance gain of 17.2% against best-known human designs."], "limitations": ["Abstract-level evidence only: validate assumptions, evaluation protocol, and failure cases in the full paper before relying on this as key evidence."], "bibkey": "Shang2024Agentsquare"}
{"paper_id": "P0038", "title": "EHRAgent: Code Empowers Large Language Models for Few-shot Complex Tabular Reasoning on Electronic Health Records", "year": 2024, "url": "http://arxiv.org/abs/2401.07128v3", "arxiv_id": "2401.07128v3", "primary_category": "cs.CL", "categories": ["cs.CL", "cs.AI"], "pdf_url": "https://arxiv.org/pdf/2401.07128v3", "priority": "normal", "mapped_sections": ["4.1"], "evidence_level": "abstract", "fulltext_path": "", "authors": ["Wenqi Shi", "Ran Xu", "Yuchen Zhuang", "Yue Yu", "Jieyu Zhang", "Hang Wu", "Yuanda Zhu", "Joyce Ho", "Carl Yang", "May D. Wang"], "abstract": "Large language models (LLMs) have demonstrated exceptional capabilities in planning and tool utilization as autonomous agents, but few have been developed for medical problem-solving. We propose EHRAgent, an LLM agent empowered with a code interface, to autonomously generate and execute code for multi-tabular reasoning within electronic health records (EHRs). First, we formulate an EHR question-answering task into a tool-use planning process, efficiently decomposing a complicated task into a sequence of manageable actions. By integrating interactive coding and execution feedback, EHRAgent learns from error messages and improves the originally generated code through iterations. Furthermore, we enhance the LLM agent by incorporating long-term memory, which allows EHRAgent to effectively select and build upon the most relevant successful cases from past experiences. Experiments on three real-world multi-tabular EHR datasets show that EHRAgent outperforms the strongest baseline by up to 29.6% in success rate. EHRAgent leverages the emerging few-shot learning capabilities of LLMs, enabling autonomous code generation and execution to tackle complex clinical tasks with minimal demonstrations.", "summary_bullets": ["Large language models (LLMs) have demonstrated exceptional capabilities in planning and tool utilization as autonomous agents, but few have been developed for medical problem-solving.", "We propose EHRAgent, an LLM agent empowered with a code interface, to autonomously generate and execute code for multi-tabular reasoning within electronic health records (EHRs).", "First, we formulate an EHR question-answering task into a tool-use planning process, efficiently decomposing a complicated task into a sequence of manageable actions."], "method": "We propose EHRAgent, an LLM agent empowered with a code interface, to autonomously generate and execute code for multi-tabular reasoning within electronic health records (EHRs).", "key_results": ["Experiments on three real-world multi-tabular EHR datasets show that EHRAgent outperforms the strongest baseline by up to 29.6% in success rate."], "limitations": ["Abstract-level evidence only: validate assumptions, evaluation protocol, and failure cases in the full paper before relying on this as key evidence."], "bibkey": "Shi2024Ehragent"}
{"paper_id": "P0039", "title": "Understanding the planning of LLM agents: A survey", "year": 2024, "url": "http://arxiv.org/abs/2402.02716v1", "arxiv_id": "2402.02716v1", "primary_category": "cs.AI", "categories": ["cs.AI", "cs.CL", "cs.LG"], "pdf_url": "https://arxiv.org/pdf/2402.02716v1", "priority": "normal", "mapped_sections": [], "evidence_level": "abstract", "fulltext_path": "", "authors": ["Xu Huang", "Weiwen Liu", "Xiaolong Chen", "Xingmei Wang", "Hao Wang", "Defu Lian", "Yasheng Wang", "Ruiming Tang", "Enhong Chen"], "abstract": "As Large Language Models (LLMs) have shown significant intelligence, the progress to leverage LLMs as planning modules of autonomous agents has attracted more attention. This survey provides the first systematic view of LLM-based agents planning, covering recent works aiming to improve planning ability. We provide a taxonomy of existing works on LLM-Agent planning, which can be categorized into Task Decomposition, Plan Selection, External Module, Reflection and Memory. Comprehensive analyses are conducted for each direction, and further challenges for the field of research are discussed.", "summary_bullets": ["As Large Language Models (LLMs) have shown significant intelligence, the progress to leverage LLMs as planning modules of autonomous agents has attracted more attention.", "This survey provides the first systematic view of LLM-based agents planning, covering recent works aiming to improve planning ability.", "We provide a taxonomy of existing works on LLM-Agent planning, which can be categorized into Task Decomposition, Plan Selection, External Module, Reflection and Memory."], "method": "As Large Language Models (LLMs) have shown significant intelligence, the progress to leverage LLMs as planning modules of autonomous agents has attracted more attention.", "key_results": ["Comprehensive analyses are conducted for each direction, and further challenges for the field of research are discussed."], "limitations": ["Abstract-level evidence only: validate assumptions, evaluation protocol, and failure cases in the full paper before relying on this as key evidence."], "bibkey": "Huang2024Understanding"}
{"paper_id": "P0040", "title": "Large Language Model Enhanced Multi-Agent Systems for 6G Communications", "year": 2023, "url": "http://arxiv.org/abs/2312.07850v1", "arxiv_id": "2312.07850v1", "primary_category": "cs.AI", "categories": ["cs.AI"], "pdf_url": "https://arxiv.org/pdf/2312.07850v1", "priority": "normal", "mapped_sections": ["5.2"], "evidence_level": "abstract", "fulltext_path": "", "authors": ["Feibo Jiang", "Li Dong", "Yubo Peng", "Kezhi Wang", "Kun Yang", "Cunhua Pan", "Dusit Niyato", "Octavia A. Dobre"], "abstract": "The rapid development of the Large Language Model (LLM) presents huge opportunities for 6G communications, e.g., network optimization and management by allowing users to input task requirements to LLMs by nature language. However, directly applying native LLMs in 6G encounters various challenges, such as a lack of private communication data and knowledge, limited logical reasoning, evaluation, and refinement abilities. Integrating LLMs with the capabilities of retrieval, planning, memory, evaluation and reflection in agents can greatly enhance the potential of LLMs for 6G communications. To this end, we propose a multi-agent system with customized communication knowledge and tools for solving communication related tasks using natural language, comprising three components: (1) Multi-agent Data Retrieval (MDR), which employs the condensate and inference agents to refine and summarize communication knowledge from the knowledge base, expanding the knowledge boundaries of LLMs in 6G communications; (2) Multi-agent Collaborative Planning (MCP), which utilizes multiple planning agents to generate feasible solutions for the communication related task from different perspectives based on the retrieved knowledge; (3) Multi-agent Evaluation and Reflecxion (MER), which utilizes the evaluation agent to assess the solutions, and applies the reflexion agent and refinement agent to provide improvement suggestions for current solutions. Finally, we validate the effectiveness of the proposed multi-agent system by designing a semantic communication system, as a case study of 6G communications.", "summary_bullets": ["The rapid development of the Large Language Model (LLM) presents huge opportunities for 6G communications, e.g., network optimization and management by allowing users to input task requirements to LLMs by nature language.", "However, directly applying native LLMs in 6G encounters various challenges, such as a lack of private communication data and knowledge, limited logical reasoning, evaluation, and refinement abilities.", "Integrating LLMs with the capabilities of retrieval, planning, memory, evaluation and reflection in agents can greatly enhance the potential of LLMs for 6G communications."], "method": "To this end, we propose a multi-agent system with customized communication knowledge and tools for solving communication related tasks using natural language, comprising three components: (1) Multi-agent Data Retrieval (MDR), which employs the condensate and inference agents to refine and summarize communication knowledge from the knowledge base, expanding the knowledge boundaries of LLMs in 6G communications; (2) Multi-agent Collaborative Planning (MCP), which utilizes multiple planning agents to generate feasible solutions for the communication related task from different perspectives based on the retrieved knowledge; (3) Multi-agent Evaluation and Reflecxion (MER), which utilizes the evaluation agent to assess the solutions, and applies the reflexion agent and refinement agent to provide improvement suggestions for current solutions.", "key_results": ["To this end, we propose a multi-agent system with customized communication knowledge and tools for solving communication related tasks using natural language, comprising three components: (1) Multi-agent Data Retrieval (MDR), which employs the condensate and inference agents to refine and summarize communication knowledge from the knowledge base, expanding the knowledge boundaries of LLMs in 6G communications; (2) Multi-agent Collaborative Planning (MCP), which utilizes multiple planning agents to generate feasible solutions for the communication related task from different perspectives based on the retrieved knowledge; (3) Multi-agent Evaluation and Reflecxion (MER), which utilizes the evaluation agent to assess the solutions, and applies the reflexion agent and refinement agent to provide improvement suggestions for current solutions.", "However, directly applying native LLMs in 6G encounters various challenges, such as a lack of private communication data and knowledge, limited logical reasoning, evaluation, and refinement abilities."], "limitations": ["Abstract-level evidence only: validate assumptions, evaluation protocol, and failure cases in the full paper before relying on this as key evidence."], "bibkey": "Jiang2023Large"}
{"paper_id": "P0041", "title": "Agentic Artificial Intelligence (AI): Architectures, Taxonomies, and Evaluation of Large Language Model Agents", "year": 2026, "url": "http://arxiv.org/abs/2601.12560v1", "arxiv_id": "2601.12560v1", "primary_category": "cs.AI", "categories": ["cs.AI", "cs.MA"], "pdf_url": "https://arxiv.org/pdf/2601.12560v1", "priority": "normal", "mapped_sections": [], "evidence_level": "abstract", "fulltext_path": "", "authors": ["Arunkumar V", "Gangadharan G. R.", "Rajkumar Buyya"], "abstract": "Artificial Intelligence is moving from models that only generate text to Agentic AI, where systems behave as autonomous entities that can perceive, reason, plan, and act. Large Language Models (LLMs) are no longer used only as passive knowledge engines but as cognitive controllers that combine memory, tool use, and feedback from their environment to pursue extended goals. This shift already supports the automation of complex workflows in software engineering, scientific discovery, and web navigation, yet the variety of emerging designs, from simple single loop agents to hierarchical multi agent systems, makes the landscape hard to navigate. In this paper, we investigate architectures and propose a unified taxonomy that breaks agents into Perception, Brain, Planning, Action, Tool Use, and Collaboration. We use this lens to describe the move from linear reasoning procedures to native inference time reasoning models, and the transition from fixed API calls to open standards like the Model Context Protocol (MCP) and Native Computer Use. We also group the environments in which these agents operate, including digital operating systems, embodied robotics, and other specialized domains, and we review current evaluation practices. Finally, we highlight open challenges, such as hallucination in action, infinite loops, and prompt injection, and outline future research directions toward more robust and reliable autonomous systems.", "summary_bullets": ["Artificial Intelligence is moving from models that only generate text to Agentic AI, where systems behave as autonomous entities that can perceive, reason, plan, and act.", "Large Language Models (LLMs) are no longer used only as passive knowledge engines but as cognitive controllers that combine memory, tool use, and feedback from their environment to pursue extended goals.", "This shift already supports the automation of complex workflows in software engineering, scientific discovery, and web navigation, yet the variety of emerging designs, from simple single loop agents to hierarchical multi agent systems, makes the landscape hard to navigate."], "method": "Artificial Intelligence is moving from models that only generate text to Agentic AI, where systems behave as autonomous entities that can perceive, reason, plan, and act.", "key_results": ["We also group the environments in which these agents operate, including digital operating systems, embodied robotics, and other specialized domains, and we review current evaluation practices."], "limitations": ["Abstract-level evidence only: validate assumptions, evaluation protocol, and failure cases in the full paper before relying on this as key evidence."], "bibkey": "V2026Agentic"}
{"paper_id": "P0042", "title": "Agentic Reasoning for Large Language Models", "year": 2026, "url": "http://arxiv.org/abs/2601.12538v1", "arxiv_id": "2601.12538v1", "primary_category": "cs.AI", "categories": ["cs.AI", "cs.CL"], "pdf_url": "https://arxiv.org/pdf/2601.12538v1", "priority": "high", "mapped_sections": ["4.1", "5.1"], "evidence_level": "abstract", "fulltext_path": "", "authors": ["Tianxin Wei", "Ting-Wei Li", "Zhining Liu", "Xuying Ning", "Ze Yang", "Jiaru Zou", "Zhichen Zeng", "Ruizhong Qiu", "Xiao Lin", "Dongqi Fu", "Zihao Li", "Mengting Ai", "Duo Zhou", "Wenxuan Bao", "Yunzhe Li", "Gaotang Li", "Cheng Qian", "Yu Wang", "Xiangru Tang", "Yin Xiao", "Liri Fang", "Hui Liu", "Xianfeng Tang", "Yuji Zhang", "Chi Wang", "Jiaxuan You", "Heng Ji", "Hanghang Tong", "Jingrui He"], "abstract": "Reasoning is a fundamental cognitive process underlying inference, problem-solving, and decision-making. While large language models (LLMs) demonstrate strong reasoning capabilities in closed-world settings, they struggle in open-ended and dynamic environments. Agentic reasoning marks a paradigm shift by reframing LLMs as autonomous agents that plan, act, and learn through continual interaction. In this survey, we organize agentic reasoning along three complementary dimensions. First, we characterize environmental dynamics through three layers: foundational agentic reasoning, which establishes core single-agent capabilities including planning, tool use, and search in stable environments; self-evolving agentic reasoning, which studies how agents refine these capabilities through feedback, memory, and adaptation; and collective multi-agent reasoning, which extends intelligence to collaborative settings involving coordination, knowledge sharing, and shared goals. Across these layers, we distinguish in-context reasoning, which scales test-time interaction through structured orchestration, from post-training reasoning, which optimizes behaviors via reinforcement learning and supervised fine-tuning. We further review representative agentic reasoning frameworks across real-world applications and benchmarks, including science, robotics, healthcare, autonomous research, and mathematics. This survey synthesizes agentic reasoning methods into a unified roadmap bridging thought and action, and outlines open challenges and future directions, including personalization, long-horizon interaction, world modeling, scalable multi-agent training, and governance for real-world deployment.", "summary_bullets": ["Reasoning is a fundamental cognitive process underlying inference, problem-solving, and decision-making.", "While large language models (LLMs) demonstrate strong reasoning capabilities in closed-world settings, they struggle in open-ended and dynamic environments.", "Agentic reasoning marks a paradigm shift by reframing LLMs as autonomous agents that plan, act, and learn through continual interaction.", "In this survey, we organize agentic reasoning along three complementary dimensions.", "First, we characterize environmental dynamics through three layers: foundational agentic reasoning, which establishes core single-agent capabilities including planning, tool use, and search in stable environments; self-evolving agentic reasoning, which studies how agents refine these capabilities through feedback, memory, and adaptation; and collective multi-agent reasoning, which extends intelligence to collaborative settings involving coordination, knowledge sharing, and shared goals."], "method": "Reasoning is a fundamental cognitive process underlying inference, problem-solving, and decision-making.", "key_results": ["We further review representative agentic reasoning frameworks across real-world applications and benchmarks, including science, robotics, healthcare, autonomous research, and mathematics."], "limitations": ["Abstract-level evidence only: validate assumptions, evaluation protocol, and failure cases in the full paper before relying on this as key evidence."], "bibkey": "Wei2026Agentic"}
{"paper_id": "P0043", "title": "A Survey of Context Engineering for Large Language Models", "year": 2025, "url": "http://arxiv.org/abs/2507.13334v2", "arxiv_id": "2507.13334v2", "primary_category": "cs.CL", "categories": ["cs.CL"], "pdf_url": "https://arxiv.org/pdf/2507.13334v2", "priority": "normal", "mapped_sections": [], "evidence_level": "abstract", "fulltext_path": "", "authors": ["Lingrui Mei", "Jiayu Yao", "Yuyao Ge", "Yiwei Wang", "Baolong Bi", "Yujun Cai", "Jiazhi Liu", "Mingyu Li", "Zhong-Zhi Li", "Duzhen Zhang", "Chenlin Zhou", "Jiayi Mao", "Tianze Xia", "Jiafeng Guo", "Shenghua Liu"], "abstract": "The performance of Large Language Models (LLMs) is fundamentally determined by the contextual information provided during inference. This survey introduces Context Engineering, a formal discipline that transcends simple prompt design to encompass the systematic optimization of information payloads for LLMs. We present a comprehensive taxonomy decomposing Context Engineering into its foundational components and the sophisticated implementations that integrate them into intelligent systems. We first examine the foundational components: context retrieval and generation, context processing and context management. We then explore how these components are architecturally integrated to create sophisticated system implementations: retrieval-augmented generation (RAG), memory systems and tool-integrated reasoning, and multi-agent systems. Through this systematic analysis of over 1400 research papers, our survey not only establishes a technical roadmap for the field but also reveals a critical research gap: a fundamental asymmetry exists between model capabilities. While current models, augmented by advanced context engineering, demonstrate remarkable proficiency in understanding complex contexts, they exhibit pronounced limitations in generating equally sophisticated, long-form outputs. Addressing this gap is a defining priority for future research. Ultimately, this survey provides a unified framework for both researchers and engineers advancing context-aware AI.", "summary_bullets": ["The performance of Large Language Models (LLMs) is fundamentally determined by the contextual information provided during inference.", "This survey introduces Context Engineering, a formal discipline that transcends simple prompt design to encompass the systematic optimization of information payloads for LLMs.", "We present a comprehensive taxonomy decomposing Context Engineering into its foundational components and the sophisticated implementations that integrate them into intelligent systems."], "method": "We present a comprehensive taxonomy decomposing Context Engineering into its foundational components and the sophisticated implementations that integrate them into intelligent systems.", "key_results": ["Through this systematic analysis of over 1400 research papers, our survey not only establishes a technical roadmap for the field but also reveals a critical research gap: a fundamental asymmetry exists between model capabilities."], "limitations": ["Abstract-level evidence only: validate assumptions, evaluation protocol, and failure cases in the full paper before relying on this as key evidence.", "While current models, augmented by advanced context engineering, demonstrate remarkable proficiency in understanding complex contexts, they exhibit pronounced limitations in generating equally sophisticated, long-form outputs."], "bibkey": "Mei2025Survey"}
{"paper_id": "P0044", "title": "A Survey of Large Language Model Empowered Agents for Recommendation and Search: Towards Next-Generation Information Retrieval", "year": 2025, "url": "http://arxiv.org/abs/2503.05659v2", "arxiv_id": "2503.05659v2", "primary_category": "cs.IR", "categories": ["cs.IR"], "pdf_url": "https://arxiv.org/pdf/2503.05659v2", "priority": "normal", "mapped_sections": [], "evidence_level": "abstract", "fulltext_path": "", "authors": ["Yu Zhang", "Shutong Qiao", "Jiaqi Zhang", "Tzu-Heng Lin", "Chen Gao", "Yong Li"], "abstract": "Information technology has profoundly altered the way humans interact with information. The vast amount of content created, shared, and disseminated online has made it increasingly difficult to access relevant information. Over the past two decades, recommender systems and search (collectively referred to as information retrieval systems) have evolved significantly to address these challenges. Recent advances in large language models (LLMs) have demonstrated capabilities that surpass human performance in various language-related tasks and exhibit general understanding, reasoning, and decision-making abilities. This paper explores the transformative potential of LLM agents in enhancing recommender and search systems. We discuss the motivations and roles of LLM agents, and establish a classification framework to elaborate on the existing research. We highlight the immense potential of LLM agents in addressing current challenges in recommendation and search, providing insights into future research directions. This paper is the first to systematically review and classify the research on LLM agents in these domains, offering a novel perspective on leveraging this advanced AI technology for information retrieval. To help understand the existing works, we list the existing papers on LLM agent based recommendation and search at this link: https://github.com/tsinghua-fib-lab/LLM-Agent-for-Recommendation-and-Search.", "summary_bullets": ["Information technology has profoundly altered the way humans interact with information.", "The vast amount of content created, shared, and disseminated online has made it increasingly difficult to access relevant information.", "Over the past two decades, recommender systems and search (collectively referred to as information retrieval systems) have evolved significantly to address these challenges."], "method": "Information technology has profoundly altered the way humans interact with information.", "key_results": ["Recent advances in large language models (LLMs) have demonstrated capabilities that surpass human performance in various language-related tasks and exhibit general understanding, reasoning, and decision-making abilities."], "limitations": ["Abstract-level evidence only: validate assumptions, evaluation protocol, and failure cases in the full paper before relying on this as key evidence."], "bibkey": "Zhang2025Survey"}
{"paper_id": "P0045", "title": "A Survey of Scientific Large Language Models: From Data Foundations to Agent Frontiers", "year": 2025, "url": "http://arxiv.org/abs/2508.21148v2", "arxiv_id": "2508.21148v2", "primary_category": "cs.CL", "categories": ["cs.CL", "cs.AI"], "pdf_url": "https://arxiv.org/pdf/2508.21148v2", "priority": "normal", "mapped_sections": ["3.1"], "evidence_level": "abstract", "fulltext_path": "", "authors": ["Ming Hu", "Chenglong Ma", "Wei Li", "Wanghan Xu", "Jiamin Wu", "Jucheng Hu", "Tianbin Li", "Guohang Zhuang", "Jiaqi Liu", "Yingzhou Lu", "Ying Chen", "Chaoyang Zhang", "Cheng Tan", "Jie Ying", "Guocheng Wu", "Shujian Gao", "Pengcheng Chen", "Jiashi Lin", "Haitao Wu", "Lulu Chen", "Fengxiang Wang", "Yuanyuan Zhang", "Xiangyu Zhao", "Feilong Tang", "Encheng Su", "Junzhi Ning", "Xinyao Liu", "Ye Du", "Changkai Ji", "Pengfei Jiang", "Cheng Tang", "Ziyan Huang", "Jiyao Liu", "Jiaqi Wei", "Yuejin Yang", "Xiang Zhang", "Guangshuai Wang", "Yue Yang", "Huihui Xu", "Ziyang Chen", "Yizhou Wang", "Chen Tang", "Jianyu Wu", "Yuchen Ren", "Siyuan Yan", "Zhonghua Wang", "Zhongxing Xu", "Shiyan Su", "Shangquan Sun", "Runkai Zhao", "Zhisheng Zhang", "Dingkang Yang", "Jinjie Wei", "Jiaqi Wang", "Jiahao Xu", "Jiangtao Yan", "Wenhao Tang", "Hongze Zhu", "Yu Liu", "Fudi Wang", "Yiqing Shen", "Yuanfeng Ji", "Yanzhou Su", "Tong Xie", "Hongming Shan", "Chun-Mei Feng", "Zhi Hou", "Diping Song", "Lihao Liu", "Yanyan Huang", "Lequan Yu", "Bin Fu", "Shujun Wang", "Xiaomeng Li", "Xiaowei Hu", "Yun Gu", "Ben Fei", "Benyou Wang", "Yuewen Cao", "Minjie Shen", "Jie Xu", "Haodong Duan", "Fang Yan", "Hongxia Hao", "Jielan Li", "Jiajun Du", "Yanbo Wang", "Imran Razzak", "Zhongying Deng", "Chi Zhang", "Lijun Wu", "Conghui He", "Zhaohui Lu", "Jinhai Huang", "Wenqi Shao", "Yihao Liu", "Siqi Luo", "Yi Xin", "Xiaohong Liu", "Fenghua Ling", "Yuqiang Li", "Aoran Wang", "Siqi Sun", "Qihao Zheng", "Nanqing Dong", "Tianfan Fu", "Dongzhan Zhou", "Yan Lu", "Wenlong Zhang", "Jin Ye", "Jianfei Cai", "Yirong Chen", "Wanli Ouyang", "Yu Qiao", "Zongyuan Ge", "Shixiang Tang", "Junjun He", "Chunfeng Song", "Lei Bai", "Bowen Zhou"], "abstract": "Scientific Large Language Models (Sci-LLMs) are transforming how knowledge is represented, integrated, and applied in scientific research, yet their progress is shaped by the complex nature of scientific data. This survey presents a comprehensive, data-centric synthesis that reframes the development of Sci-LLMs as a co-evolution between models and their underlying data substrate. We formulate a unified taxonomy of scientific data and a hierarchical model of scientific knowledge, emphasizing the multimodal, cross-scale, and domain-specific challenges that differentiate scientific corpora from general natural language processing datasets. We systematically review recent Sci-LLMs, from general-purpose foundations to specialized models across diverse scientific disciplines, alongside an extensive analysis of over 270 pre-/post-training datasets, showing why Sci-LLMs pose distinct demands -- heterogeneous, multi-scale, uncertainty-laden corpora that require representations preserving domain invariance and enabling cross-modal reasoning. On evaluation, we examine over 190 benchmark datasets and trace a shift from static exams toward process- and discovery-oriented assessments with advanced evaluation protocols. These data-centric analyses highlight persistent issues in scientific data development and discuss emerging solutions involving semi-automated annotation pipelines and expert validation. Finally, we outline a paradigm shift toward closed-loop systems where autonomous agents based on Sci-LLMs actively experiment, validate, and contribute to a living, evolving knowledge base. Collectively, this work provides a roadmap for building trustworthy, continually evolving artificial intelligence (AI) systems that function as a true partner in accelerating scientific discovery.", "summary_bullets": ["Scientific Large Language Models (Sci-LLMs) are transforming how knowledge is represented, integrated, and applied in scientific research, yet their progress is shaped by the complex nature of scientific data.", "This survey presents a comprehensive, data-centric synthesis that reframes the development of Sci-LLMs as a co-evolution between models and their underlying data substrate.", "We formulate a unified taxonomy of scientific data and a hierarchical model of scientific knowledge, emphasizing the multimodal, cross-scale, and domain-specific challenges that differentiate scientific corpora from general natural language processing datasets."], "method": "Scientific Large Language Models (Sci-LLMs) are transforming how knowledge is represented, integrated, and applied in scientific research, yet their progress is shaped by the complex nature of scientific data.", "key_results": ["We systematically review recent Sci-LLMs, from general-purpose foundations to specialized models across diverse scientific disciplines, alongside an extensive analysis of over 270 pre-/post-training datasets, showing why Sci-LLMs pose distinct demands -- heterogeneous, multi-scale, uncertainty-laden corpora that require representations preserving domain invariance and enabling cross-modal reasoning.", "On evaluation, we examine over 190 benchmark datasets and trace a shift from static exams toward process- and discovery-oriented assessments with advanced evaluation protocols."], "limitations": ["Abstract-level evidence only: validate assumptions, evaluation protocol, and failure cases in the full paper before relying on this as key evidence."], "bibkey": "Hu2025Survey"}
{"paper_id": "P0046", "title": "A Survey on Inference Engines for Large Language Models: Perspectives on Optimization and Efficiency", "year": 2025, "url": "http://arxiv.org/abs/2505.01658v3", "arxiv_id": "2505.01658v3", "primary_category": "cs.CL", "categories": ["cs.CL"], "pdf_url": "https://arxiv.org/pdf/2505.01658v3", "priority": "normal", "mapped_sections": [], "evidence_level": "abstract", "fulltext_path": "", "authors": ["Sihyeong Park", "Sungryeol Jeon", "Chaelyn Lee", "Seokhun Jeon", "Byung-Soo Kim", "Jemin Lee"], "abstract": "Large language models (LLMs) are widely applied in chatbots, code generators, and search engines. Workload such as chain-of-throught, complex reasoning, agent services significantly increase the inference cost by invoke the model repeatedly. Optimization methods such as parallelism, compression, and caching have been adopted to reduce costs, but the diverse service requirements make it hard to select the right method. Recently, specialized LLM inference engines have emerged as a key component for integrating the optimization methods into service-oriented infrastructures. However, a systematic study on inference engines is still lacking.This paper provides a comprehensive evaluation of 25 open-source and commercial inference engines. We examine each inference engine in terms of ease-of-use, ease-of-deployment, general-purpose support, scalability, and suitability for throughput- and latency-aware computation. Furthermore, we explore the design goals of each inference engine by investigating the optimization techniques it supports. In addition, we assess the ecosystem maturity of open source inference engines and handle the performance and cost policy of commercial solutions.We outline future research directions that include support for complex LLM-based services, support of various hardware, and enhanced security, offering practical guidance to researchers and developers in selecting and designing optimized LLM inference engines. We also provide a public repository to continually track developments in this fast-evolving field: \\href{https://github.com/sihyeong/Awesome-LLM-Inference-Engine}{https://github.com/sihyeong/Awesome-LLM-Inference-Engine}.", "summary_bullets": ["Large language models (LLMs) are widely applied in chatbots, code generators, and search engines.", "Workload such as chain-of-throught, complex reasoning, agent services significantly increase the inference cost by invoke the model repeatedly.", "Optimization methods such as parallelism, compression, and caching have been adopted to reduce costs, but the diverse service requirements make it hard to select the right method."], "method": "Large language models (LLMs) are widely applied in chatbots, code generators, and search engines.", "key_results": ["However, a systematic study on inference engines is still lacking.This paper provides a comprehensive evaluation of 25 open-source and commercial inference engines.", "Workload such as chain-of-throught, complex reasoning, agent services significantly increase the inference cost by invoke the model repeatedly."], "limitations": ["Abstract-level evidence only: validate assumptions, evaluation protocol, and failure cases in the full paper before relying on this as key evidence."], "bibkey": "Park2025Survey"}
{"paper_id": "P0047", "title": "A-MemGuard: A Proactive Defense Framework for LLM-Based Agent Memory", "year": 2025, "url": "http://arxiv.org/abs/2510.02373v1", "arxiv_id": "2510.02373v1", "primary_category": "cs.CR", "categories": ["cs.CR", "cs.AI"], "pdf_url": "https://arxiv.org/pdf/2510.02373v1", "priority": "high", "mapped_sections": ["3.1", "4.2"], "evidence_level": "abstract", "fulltext_path": "", "authors": ["Qianshan Wei", "Tengchao Yang", "Yaochen Wang", "Xinfeng Li", "Lijun Li", "Zhenfei Yin", "Yi Zhan", "Thorsten Holz", "Zhiqiang Lin", "XiaoFeng Wang"], "abstract": "Large Language Model (LLM) agents use memory to learn from past interactions, enabling autonomous planning and decision-making in complex environments. However, this reliance on memory introduces a critical security risk: an adversary can inject seemingly harmless records into an agent's memory to manipulate its future behavior. This vulnerability is characterized by two core aspects: First, the malicious effect of injected records is only activated within a specific context, making them hard to detect when individual memory entries are audited in isolation. Second, once triggered, the manipulation can initiate a self-reinforcing error cycle: the corrupted outcome is stored as precedent, which not only amplifies the initial error but also progressively lowers the threshold for similar attacks in the future. To address these challenges, we introduce A-MemGuard (Agent-Memory Guard), the first proactive defense framework for LLM agent memory. The core idea of our work is the insight that memory itself must become both self-checking and self-correcting. Without modifying the agent's core architecture, A-MemGuard combines two mechanisms: (1) consensus-based validation, which detects anomalies by comparing reasoning paths derived from multiple related memories and (2) a dual-memory structure, where detected failures are distilled into ``lessons'' stored separately and consulted before future actions, breaking error cycles and enabling adaptation. Comprehensive evaluations on multiple benchmarks show that A-MemGuard effectively cuts attack success rates by over 95% while incurring a minimal utility cost. This work shifts LLM memory security from static filtering to a proactive, experience-driven model where defenses strengthen over time. Our code is available in https://github.com/TangciuYueng/AMemGuard", "summary_bullets": ["Large Language Model (LLM) agents use memory to learn from past interactions, enabling autonomous planning and decision-making in complex environments.", "However, this reliance on memory introduces a critical security risk: an adversary can inject seemingly harmless records into an agent's memory to manipulate its future behavior.", "This vulnerability is characterized by two core aspects: First, the malicious effect of injected records is only activated within a specific context, making them hard to detect when individual memory entries are audited in isolation.", "Second, once triggered, the manipulation can initiate a self-reinforcing error cycle: the corrupted outcome is stored as precedent, which not only amplifies the initial error but also progressively lowers the threshold for similar attacks in the future.", "To address these challenges, we introduce A-MemGuard (Agent-Memory Guard), the first proactive defense framework for LLM agent memory."], "method": "To address these challenges, we introduce A-MemGuard (Agent-Memory Guard), the first proactive defense framework for LLM agent memory.", "key_results": ["Comprehensive evaluations on multiple benchmarks show that A-MemGuard effectively cuts attack success rates by over 95% while incurring a minimal utility cost.", "Without modifying the agent's core architecture, A-MemGuard combines two mechanisms: (1) consensus-based validation, which detects anomalies by comparing reasoning paths derived from multiple related memories and (2) a dual-memory structure, where detected failures are distilled into ``lessons'' stored separately and consulted before future actions, breaking error cycles and enabling adaptation."], "limitations": ["Abstract-level evidence only: validate assumptions, evaluation protocol, and failure cases in the full paper before relying on this as key evidence."], "bibkey": "Wei2025Memguard"}
{"paper_id": "P0048", "title": "ACON: Optimizing Context Compression for Long-horizon LLM Agents", "year": 2025, "url": "http://arxiv.org/abs/2510.00615v2", "arxiv_id": "2510.00615v2", "primary_category": "cs.AI", "categories": ["cs.AI", "cs.CL"], "pdf_url": "https://arxiv.org/pdf/2510.00615v2", "priority": "normal", "mapped_sections": ["6.2"], "evidence_level": "abstract", "fulltext_path": "", "authors": ["Minki Kang", "Wei-Ning Chen", "Dongge Han", "Huseyin A. Inan", "Lukas Wutschitz", "Yanzhi Chen", "Robert Sim", "Saravan Rajmohan"], "abstract": "Large language models (LLMs) are increasingly deployed as agents in dynamic, real-world environments, where success requires both reasoning and effective tool use. A central challenge for agentic tasks is the growing context length, as agents must accumulate long histories of actions and observations. This expansion raises costs and reduces efficiency in long-horizon tasks, yet prior work on context compression has mostly focused on single-step tasks or narrow applications. We introduce Agent Context Optimization (ACON), a unified framework that optimally compresses both environment observations and interaction histories into concise yet informative condensations. ACON leverages compression guideline optimization in natural language space: given paired trajectories where full context succeeds but compressed context fails, capable LLMs analyze the causes of failure, and the compression guideline is updated accordingly. Furthermore, we propose distilling the optimized LLM compressor into smaller models to reduce the overhead of the additional module. Experiments on AppWorld, OfficeBench, and Multi-objective QA show that ACON reduces memory usage by 26-54% (peak tokens) while largely preserving task performance, preserves over 95% of accuracy when distilled into smaller compressors, and enhances smaller LMs as long-horizon agents with up to 46% performance improvement. Our code is available at https://github.com/microsoft/acon.", "summary_bullets": ["Large language models (LLMs) are increasingly deployed as agents in dynamic, real-world environments, where success requires both reasoning and effective tool use.", "A central challenge for agentic tasks is the growing context length, as agents must accumulate long histories of actions and observations.", "This expansion raises costs and reduces efficiency in long-horizon tasks, yet prior work on context compression has mostly focused on single-step tasks or narrow applications."], "method": "We introduce Agent Context Optimization (ACON), a unified framework that optimally compresses both environment observations and interaction histories into concise yet informative condensations.", "key_results": ["Experiments on AppWorld, OfficeBench, and Multi-objective QA show that ACON reduces memory usage by 26-54% (peak tokens) while largely preserving task performance, preserves over 95% of accuracy when distilled into smaller compressors, and enhances smaller LMs as long-horizon agents with up to 46% performance improvement.", "Large language models (LLMs) are increasingly deployed as agents in dynamic, real-world environments, where success requires both reasoning and effective tool use."], "limitations": ["Abstract-level evidence only: validate assumptions, evaluation protocol, and failure cases in the full paper before relying on this as key evidence."], "bibkey": "Kang2025Acon"}
{"paper_id": "P0049", "title": "AgentSwift: Efficient LLM Agent Design via Value-guided Hierarchical Search", "year": 2025, "url": "http://arxiv.org/abs/2506.06017v2", "arxiv_id": "2506.06017v2", "primary_category": "cs.CL", "categories": ["cs.CL"], "pdf_url": "https://arxiv.org/pdf/2506.06017v2", "priority": "normal", "mapped_sections": ["3.1"], "evidence_level": "abstract", "fulltext_path": "", "authors": ["Yu Li", "Lehui Li", "Zhihao Wu", "Qingmin Liao", "Jianye Hao", "Kun Shao", "Fengli Xu", "Yong Li"], "abstract": "Large language model (LLM) agents have demonstrated strong capabilities across diverse domains, yet automated agent design remains a significant challenge. Current automated agent design approaches are often constrained by limited search spaces that primarily optimize workflows but fail to integrate crucial human-designed components like memory, planning, and tool use. Furthermore, these methods are hampered by high evaluation costs, as evaluating even a single new agent on a benchmark can require tens of dollars. The difficulty of this exploration is further exacerbated by inefficient search strategies that struggle to navigate the large design space effectively, making the discovery of novel agents a slow and resource-intensive process. To address these challenges, we propose AgentSwift, a novel framework for automated agent design. We formalize a hierarchical search space that jointly models agentic workflow and composable functional components. This structure moves beyond optimizing workflows alone by co-optimizing functional components, which enables the discovery of more complex and effective agent architectures. To make exploration within this expansive space feasible, we mitigate high evaluation costs by training a value model on a high-quality dataset, generated via a novel strategy combining combinatorial coverage and balanced Bayesian sampling for low-cost evaluation. Guiding the entire process is a hierarchical MCTS strategy, which is informed by uncertainty to efficiently navigate the search space. Evaluated across a comprehensive set of seven benchmarks spanning embodied, math, web, tool, and game domains, AgentSwift discovers agents that achieve an average performance gain of 8.34\\% over both existing automated agent search methods and manually designed agents. Our framework serves as a launchpad for researchers to rapidly discover powerful agent architectures.", "summary_bullets": ["Large language model (LLM) agents have demonstrated strong capabilities across diverse domains, yet automated agent design remains a significant challenge.", "Current automated agent design approaches are often constrained by limited search spaces that primarily optimize workflows but fail to integrate crucial human-designed components like memory, planning, and tool use.", "Furthermore, these methods are hampered by high evaluation costs, as evaluating even a single new agent on a benchmark can require tens of dollars."], "method": "To address these challenges, we propose AgentSwift, a novel framework for automated agent design.", "key_results": ["Evaluated across a comprehensive set of seven benchmarks spanning embodied, math, web, tool, and game domains, AgentSwift discovers agents that achieve an average performance gain of 8.34\\% over both existing automated agent search methods and manually designed agents.", "Current automated agent design approaches are often constrained by limited search spaces that primarily optimize workflows but fail to integrate crucial human-designed components like memory, planning, and tool use."], "limitations": ["Abstract-level evidence only: validate assumptions, evaluation protocol, and failure cases in the full paper before relying on this as key evidence."], "bibkey": "Li2025Agentswift"}
{"paper_id": "P0050", "title": "Agentic Software Issue Resolution with Large Language Models: A Survey", "year": 2025, "url": "http://arxiv.org/abs/2512.22256v1", "arxiv_id": "2512.22256v1", "primary_category": "cs.SE", "categories": ["cs.SE", "cs.AI"], "pdf_url": "https://arxiv.org/pdf/2512.22256v1", "priority": "normal", "mapped_sections": ["5.1"], "evidence_level": "abstract", "fulltext_path": "", "authors": ["Zhonghao Jiang", "David Lo", "Zhongxin Liu"], "abstract": "Software issue resolution aims to address real-world issues in software repositories (e.g., bug fixing and efficiency optimization) based on natural language descriptions provided by users, representing a key aspect of software maintenance. With the rapid development of large language models (LLMs) in reasoning and generative capabilities, LLM-based approaches have made significant progress in automated software issue resolution. However, real-world software issue resolution is inherently complex and requires long-horizon reasoning, iterative exploration, and feedback-driven decision making, which demand agentic capabilities beyond conventional single-step approaches. Recently, LLM-based agentic systems have become mainstream for software issue resolution. Advancements in agentic software issue resolution not only greatly enhance software maintenance efficiency and quality but also provide a realistic environment for validating agentic systems' reasoning, planning, and execution capabilities, bridging artificial intelligence and software engineering.\n  This work presents a systematic survey of 126 recent studies at the forefront of LLM-based agentic software issue resolution research. It outlines the general workflow of the task and establishes a taxonomy across three dimensions: benchmarks, techniques, and empirical studies. Furthermore, it highlights how the emergence of agentic reinforcement learning has brought a paradigm shift in the design and training of agentic systems for software engineering. Finally, it summarizes key challenges and outlines promising directions for future research.", "summary_bullets": ["Software issue resolution aims to address real-world issues in software repositories (e.g., bug fixing and efficiency optimization) based on natural language descriptions provided by users, representing a key aspect of software maintenance.", "With the rapid development of large language models (LLMs) in reasoning and generative capabilities, LLM-based approaches have made significant progress in automated software issue resolution.", "However, real-world software issue resolution is inherently complex and requires long-horizon reasoning, iterative exploration, and feedback-driven decision making, which demand agentic capabilities beyond conventional single-step approaches."], "method": "Software issue resolution aims to address real-world issues in software repositories (e.g., bug fixing and efficiency optimization) based on natural language descriptions provided by users, representing a key aspect of software maintenance.", "key_results": ["This work presents a systematic survey of 126 recent studies at the forefront of LLM-based agentic software issue resolution research.", "It outlines the general workflow of the task and establishes a taxonomy across three dimensions: benchmarks, techniques, and empirical studies."], "limitations": ["Abstract-level evidence only: validate assumptions, evaluation protocol, and failure cases in the full paper before relying on this as key evidence."], "bibkey": "Jiang2025Agentic"}
{"paper_id": "P0051", "title": "Agents of Change: Self-Evolving LLM Agents for Strategic Planning", "year": 2025, "url": "http://arxiv.org/abs/2506.04651v2", "arxiv_id": "2506.04651v2", "primary_category": "cs.AI", "categories": ["cs.AI"], "pdf_url": "https://arxiv.org/pdf/2506.04651v2", "priority": "normal", "mapped_sections": ["5.1"], "evidence_level": "abstract", "fulltext_path": "", "authors": ["Nikolas Belle", "Dakota Barnes", "Alfonso Amayuelas", "Ivan Bercovich", "Xin Eric Wang", "William Wang"], "abstract": "We address the long-horizon gap in large language model (LLM) agents by enabling them to sustain coherent strategies in adversarial, stochastic environments. Settlers of Catan provides a challenging benchmark: success depends on balancing short- and long-term goals amid randomness, trading, expansion, and blocking. Prompt-centric LLM agents (e.g., ReAct, Reflexion) must re-interpret large, evolving game states each turn, quickly saturating context windows and losing strategic consistency. We propose HexMachina, a continual learning multi-agent system that separates environment discovery (inducing an adapter layer without documentation) from strategy improvement (evolving a compiled player through code refinement and simulation). This design preserves executable artifacts, allowing the LLM to focus on high-level strategy rather than per-turn reasoning. In controlled Catanatron experiments, HexMachina learns from scratch and evolves players that outperform the strongest human-crafted baseline (AlphaBeta), achieving a 54% win rate and surpassing prompt-driven and no-discovery baselines. Ablations confirm that isolating pure strategy learning improves performance. Overall, artifact-centric continual learning transforms LLMs from brittle stepwise deciders into stable strategy designers, advancing long-horizon autonomy.", "summary_bullets": ["We address the long-horizon gap in large language model (LLM) agents by enabling them to sustain coherent strategies in adversarial, stochastic environments.", "Settlers of Catan provides a challenging benchmark: success depends on balancing short- and long-term goals amid randomness, trading, expansion, and blocking.", "Prompt-centric LLM agents (e.g., ReAct, Reflexion) must re-interpret large, evolving game states each turn, quickly saturating context windows and losing strategic consistency."], "method": "We propose HexMachina, a continual learning multi-agent system that separates environment discovery (inducing an adapter layer without documentation) from strategy improvement (evolving a compiled player through code refinement and simulation).", "key_results": ["In controlled Catanatron experiments, HexMachina learns from scratch and evolves players that outperform the strongest human-crafted baseline (AlphaBeta), achieving a 54% win rate and surpassing prompt-driven and no-discovery baselines.", "Settlers of Catan provides a challenging benchmark: success depends on balancing short- and long-term goals amid randomness, trading, expansion, and blocking."], "limitations": ["Abstract-level evidence only: validate assumptions, evaluation protocol, and failure cases in the full paper before relying on this as key evidence."], "bibkey": "Belle2025Agents"}
{"paper_id": "P0052", "title": "Aligning LLM agents with human learning and adjustment behavior: a dual agent approach", "year": 2025, "url": "http://arxiv.org/abs/2511.00993v1", "arxiv_id": "2511.00993v1", "primary_category": "cs.AI", "categories": ["cs.AI", "cs.LG"], "pdf_url": "https://arxiv.org/pdf/2511.00993v1", "priority": "normal", "mapped_sections": ["3.1"], "evidence_level": "abstract", "fulltext_path": "", "authors": ["Tianming Liu", "Jirong Yang", "Yafeng Yin", "Manzi Li", "Linghao Wang", "Zheng Zhu"], "abstract": "Effective modeling of how human travelers learn and adjust their travel behavior from interacting with transportation systems is critical for system assessment and planning. However, this task is also difficult due to the complex cognition and decision-making involved in such behavior. Recent research has begun to leverage Large Language Model (LLM) agents for this task. Building on this, we introduce a novel dual-agent framework that enables continuous learning and alignment between LLM agents and human travelers on learning and adaptation behavior from online data streams. Our approach involves a set of LLM traveler agents, equipped with a memory system and a learnable persona, which serve as simulators for human travelers. To ensure behavioral alignment, we introduce an LLM calibration agent that leverages the reasoning and analytical capabilities of LLMs to train the personas of these traveler agents. Working together, this dual-agent system is designed to track and align the underlying decision-making mechanisms of travelers and produce realistic, adaptive simulations. Using a real-world dataset from a day-to-day route choice experiment, we show our approach significantly outperforms existing LLM-based methods in both individual behavioral alignment and aggregate simulation accuracy. Furthermore, we demonstrate that our method moves beyond simple behavioral mimicry to capture the evolution of underlying learning processes, a deeper alignment that fosters robust generalization. Overall, our framework provides a new approach for creating adaptive and behaviorally realistic agents to simulate travelers' learning and adaptation that can benefit transportation simulation and policy analysis.", "summary_bullets": ["Effective modeling of how human travelers learn and adjust their travel behavior from interacting with transportation systems is critical for system assessment and planning.", "However, this task is also difficult due to the complex cognition and decision-making involved in such behavior.", "Recent research has begun to leverage Large Language Model (LLM) agents for this task."], "method": "Building on this, we introduce a novel dual-agent framework that enables continuous learning and alignment between LLM agents and human travelers on learning and adaptation behavior from online data streams.", "key_results": ["Effective modeling of how human travelers learn and adjust their travel behavior from interacting with transportation systems is critical for system assessment and planning.", "Building on this, we introduce a novel dual-agent framework that enables continuous learning and alignment between LLM agents and human travelers on learning and adaptation behavior from online data streams."], "limitations": ["Abstract-level evidence only: validate assumptions, evaluation protocol, and failure cases in the full paper before relying on this as key evidence."], "bibkey": "Liu2025Aligning"}
{"paper_id": "P0053", "title": "AudioToolAgent: An Agentic Framework for Audio-Language Models", "year": 2025, "url": "http://arxiv.org/abs/2510.02995v1", "arxiv_id": "2510.02995v1", "primary_category": "cs.SD", "categories": ["cs.SD"], "pdf_url": "https://arxiv.org/pdf/2510.02995v1", "priority": "normal", "mapped_sections": [], "evidence_level": "abstract", "fulltext_path": "", "authors": ["Gijs Wijngaard", "Elia Formisano", "Michel Dumontier"], "abstract": "Large Audio-Language Models (LALMs) perform well on audio understanding tasks but lack multi-step reasoning and tool-calling found in recent Large Language Models (LLMs). This paper presents AudioToolAgent, a framework that coordinates audio-language models as tools via a central LLM agent that accesses tool adapters for audio question answering and speech-to-text. The agent selects tools, asks follow-up questions, and compares outputs for verification. Experiments with MMAU, MMAR, and MMAU-Pro show state-of-the-art accuracy: up to 74.10% on MMAU, 68.80% on MMAR, and 57.96% on MMAU-Pro. Monte Carlo sampling for shapley values across 374 configurations identifies effective agent-tool combinations. The modular design allows integration of new tools and eliminates the use of data and training costs. Code and reproduction materials are available at: github.com/GLJS/AudioToolAgent", "summary_bullets": ["Large Audio-Language Models (LALMs) perform well on audio understanding tasks but lack multi-step reasoning and tool-calling found in recent Large Language Models (LLMs).", "This paper presents AudioToolAgent, a framework that coordinates audio-language models as tools via a central LLM agent that accesses tool adapters for audio question answering and speech-to-text.", "The agent selects tools, asks follow-up questions, and compares outputs for verification."], "method": "Large Audio-Language Models (LALMs) perform well on audio understanding tasks but lack multi-step reasoning and tool-calling found in recent Large Language Models (LLMs).", "key_results": ["Experiments with MMAU, MMAR, and MMAU-Pro show state-of-the-art accuracy: up to 74.10% on MMAU, 68.80% on MMAR, and 57.96% on MMAU-Pro.", "Monte Carlo sampling for shapley values across 374 configurations identifies effective agent-tool combinations."], "limitations": ["Abstract-level evidence only: validate assumptions, evaluation protocol, and failure cases in the full paper before relying on this as key evidence."], "bibkey": "Wijngaard2025Audiotoolagent"}
{"paper_id": "P0054", "title": "AutoTool: Efficient Tool Selection for Large Language Model Agents", "year": 2025, "url": "http://arxiv.org/abs/2511.14650v1", "arxiv_id": "2511.14650v1", "primary_category": "cs.AI", "categories": ["cs.AI"], "pdf_url": "https://arxiv.org/pdf/2511.14650v1", "priority": "normal", "mapped_sections": ["3.2"], "evidence_level": "abstract", "fulltext_path": "", "authors": ["Jingyi Jia", "Qinbin Li"], "abstract": "Large Language Model (LLM) agents have emerged as powerful tools for automating complex tasks by leveraging the reasoning and decision-making abilities of LLMs. However, a major bottleneck in current agent frameworks lies in the high inference cost of tool selection, especially in approaches like ReAct that repeatedly invoke the LLM to determine which tool to use at each step. In this work, we propose AutoTool, a novel graph-based framework that bypasses repeated LLM inference by exploiting a key empirical observation: tool usage inertia - the tendency of tool invocations to follow predictable sequential patterns. AutoTool constructs a directed graph from historical agent trajectories, where nodes represent tools and edges capture transition probabilities, effectively modeling the inertia in tool selection. It further integrates parameter-level information to refine tool input generation. By traversing this structured representation, AutoTool efficiently selects tools and their parameters with minimal reliance on LLM inference. Extensive experiments across diverse agent tasks demonstrate that AutoTool reduces inference costs by up to 30% while maintaining competitive task completion rates, offering a practical and scalable enhancement for inference-heavy frameworks. Our work highlights the promise of integrating statistical structure into LLM agent design for greater efficiency without sacrificing performance.", "summary_bullets": ["Large Language Model (LLM) agents have emerged as powerful tools for automating complex tasks by leveraging the reasoning and decision-making abilities of LLMs.", "However, a major bottleneck in current agent frameworks lies in the high inference cost of tool selection, especially in approaches like ReAct that repeatedly invoke the LLM to determine which tool to use at each step.", "In this work, we propose AutoTool, a novel graph-based framework that bypasses repeated LLM inference by exploiting a key empirical observation: tool usage inertia - the tendency of tool invocations to follow predictable sequential patterns."], "method": "In this work, we propose AutoTool, a novel graph-based framework that bypasses repeated LLM inference by exploiting a key empirical observation: tool usage inertia - the tendency of tool invocations to follow predictable sequential patterns.", "key_results": ["Extensive experiments across diverse agent tasks demonstrate that AutoTool reduces inference costs by up to 30% while maintaining competitive task completion rates, offering a practical and scalable enhancement for inference-heavy frameworks."], "limitations": ["Abstract-level evidence only: validate assumptions, evaluation protocol, and failure cases in the full paper before relying on this as key evidence."], "bibkey": "Jia2025Autotool"}
{"paper_id": "P0055", "title": "Automated stereotactic radiosurgery planning using a human-in-the-loop reasoning large language model agent", "year": 2025, "url": "http://arxiv.org/abs/2512.20586v1", "arxiv_id": "2512.20586v1", "primary_category": "cs.AI", "categories": ["cs.AI", "cs.CL", "cs.HC"], "pdf_url": "https://arxiv.org/pdf/2512.20586v1", "priority": "high", "mapped_sections": ["3.1", "4.1"], "evidence_level": "abstract", "fulltext_path": "", "authors": ["Humza Nusrat", "Luke Francisco", "Bing Luo", "Hassan Bagher-Ebadian", "Joshua Kim", "Karen Chin-Snyder", "Salim Siddiqui", "Mira Shah", "Eric Mellon", "Mohammad Ghassemi", "Anthony Doemer", "Benjamin Movsas", "Kundan Thind"], "abstract": "Stereotactic radiosurgery (SRS) demands precise dose shaping around critical structures, yet black-box AI systems have limited clinical adoption due to opacity concerns. We tested whether chain-of-thought reasoning improves agentic planning in a retrospective cohort of 41 patients with brain metastases treated with 18 Gy single-fraction SRS. We developed SAGE (Secure Agent for Generative Dose Expertise), an LLM-based planning agent for automated SRS treatment planning. Two variants generated plans for each case: one using a non-reasoning model, one using a reasoning model. The reasoning variant showed comparable plan dosimetry relative to human planners on primary endpoints (PTV coverage, maximum dose, conformity index, gradient index; all p > 0.21) while reducing cochlear dose below human baselines (p = 0.022). When prompted to improve conformity, the reasoning model demonstrated systematic planning behaviors including prospective constraint verification (457 instances) and trade-off deliberation (609 instances), while the standard model exhibited none of these deliberative processes (0 and 7 instances, respectively). Content analysis revealed that constraint verification and causal explanation concentrated in the reasoning agent. The optimization traces serve as auditable logs, offering a path toward transparent automated planning.", "summary_bullets": ["Stereotactic radiosurgery (SRS) demands precise dose shaping around critical structures, yet black-box AI systems have limited clinical adoption due to opacity concerns.", "We tested whether chain-of-thought reasoning improves agentic planning in a retrospective cohort of 41 patients with brain metastases treated with 18 Gy single-fraction SRS.", "We developed SAGE (Secure Agent for Generative Dose Expertise), an LLM-based planning agent for automated SRS treatment planning.", "Two variants generated plans for each case: one using a non-reasoning model, one using a reasoning model.", "The reasoning variant showed comparable plan dosimetry relative to human planners on primary endpoints (PTV coverage, maximum dose, conformity index, gradient index; all p > 0.21) while reducing cochlear dose below human baselines (p = 0.022)."], "method": "Stereotactic radiosurgery (SRS) demands precise dose shaping around critical structures, yet black-box AI systems have limited clinical adoption due to opacity concerns.", "key_results": ["The reasoning variant showed comparable plan dosimetry relative to human planners on primary endpoints (PTV coverage, maximum dose, conformity index, gradient index; all p > 0.21) while reducing cochlear dose below human baselines (p = 0.022).", "We tested whether chain-of-thought reasoning improves agentic planning in a retrospective cohort of 41 patients with brain metastases treated with 18 Gy single-fraction SRS."], "limitations": ["Abstract-level evidence only: validate assumptions, evaluation protocol, and failure cases in the full paper before relying on this as key evidence."], "bibkey": "Nusrat2025Automated"}
{"paper_id": "P0056", "title": "Automating Android Build Repair: Bridging the Reasoning-Execution Gap in LLM Agents with Domain-Specific Tools", "year": 2025, "url": "http://arxiv.org/abs/2510.08640v2", "arxiv_id": "2510.08640v2", "primary_category": "cs.SE", "categories": ["cs.SE", "cs.AI"], "pdf_url": "https://arxiv.org/pdf/2510.08640v2", "priority": "normal", "mapped_sections": [], "evidence_level": "abstract", "fulltext_path": "", "authors": ["Ha Min Son", "Huan Ren", "Xin Liu", "Zhe Zhao"], "abstract": "Android is the largest mobile platform, yet automatically building applications remains a practical challenge. While Large Language Models (LLMs) show promise for code repair, their use for fixing Android build errors remains underexplored. To address this gap, we first introduce AndroidBuildBench, a benchmark of 1,019 build failures curated from the commit histories of 43 open-source Android projects. Each problem is paired with a verified solution from a subsequent commit, ensuring that fixes are feasible. Second, we propose GradleFixer, an LLM agent with domain-specific tools for inspecting and manipulating the Gradle build environment. GradleFixer achieves a resolve rate of 81.4% (pass@1), significantly outperforming a state-of-the-art coding agent that relies on a general-purpose shell. GradleFixer's success suggests that while LLMs possess the high-level knowledge to solve these failures, they struggle to translate this knowledge into effective low-level actions using a general-purpose shell. We demonstrate the effectiveness of a strategy we term Tool Bridging, which replaces general-purpose shell commands with domain-aware abstractions. We hypothesize this approach works through two mechanisms: 1) it provides tools in an API-like format that LLMs use more reliably, and 2) it constrains the action space to relevant operations. This approach bridges the gap between the model's high-level reasoning and effective low-level execution.", "summary_bullets": ["Android is the largest mobile platform, yet automatically building applications remains a practical challenge.", "While Large Language Models (LLMs) show promise for code repair, their use for fixing Android build errors remains underexplored.", "To address this gap, we first introduce AndroidBuildBench, a benchmark of 1,019 build failures curated from the commit histories of 43 open-source Android projects."], "method": "Second, we propose GradleFixer, an LLM agent with domain-specific tools for inspecting and manipulating the Gradle build environment.", "key_results": ["To address this gap, we first introduce AndroidBuildBench, a benchmark of 1,019 build failures curated from the commit histories of 43 open-source Android projects.", "GradleFixer achieves a resolve rate of 81.4% (pass@1), significantly outperforming a state-of-the-art coding agent that relies on a general-purpose shell."], "limitations": ["Abstract-level evidence only: validate assumptions, evaluation protocol, and failure cases in the full paper before relying on this as key evidence."], "bibkey": "Son2025Automating"}
{"paper_id": "P0057", "title": "Autonomous Radiotherapy Treatment Planning Using DOLA: A Privacy-Preserving, LLM-Based Optimization Agent", "year": 2025, "url": "http://arxiv.org/abs/2503.17553v1", "arxiv_id": "2503.17553v1", "primary_category": "physics.med-ph", "categories": ["physics.med-ph", "cs.AI", "cs.CL", "cs.ET", "cs.HC"], "pdf_url": "https://arxiv.org/pdf/2503.17553v1", "priority": "normal", "mapped_sections": [], "evidence_level": "abstract", "fulltext_path": "", "authors": ["Humza Nusrat", "Bing Luo", "Ryan Hall", "Joshua Kim", "Hassan Bagher-Ebadian", "Anthony Doemer", "Benjamin Movsas", "Kundan Thind"], "abstract": "Radiotherapy treatment planning is a complex and time-intensive process, often impacted by inter-planner variability and subjective decision-making. To address these challenges, we introduce Dose Optimization Language Agent (DOLA), an autonomous large language model (LLM)-based agent designed for optimizing radiotherapy treatment plans while rigorously protecting patient privacy. DOLA integrates the LLaMa3.1 LLM directly with a commercial treatment planning system, utilizing chain-of-thought prompting, retrieval-augmented generation (RAG), and reinforcement learning (RL). Operating entirely within secure local infrastructure, this agent eliminates external data sharing. We evaluated DOLA using a retrospective cohort of 18 prostate cancer patients prescribed 60 Gy in 20 fractions, comparing model sizes (8 billion vs. 70 billion parameters) and optimization strategies (No-RAG, RAG, and RAG+RL) over 10 planning iterations. The 70B model demonstrated significantly improved performance, achieving approximately 16.4% higher final scores than the 8B model. The RAG approach outperformed the No-RAG baseline by 19.8%, and incorporating RL accelerated convergence, highlighting the synergy of retrieval-based memory and reinforcement learning. Optimal temperature hyperparameter analysis identified 0.4 as providing the best balance between exploration and exploitation. This proof of concept study represents the first successful deployment of locally hosted LLM agents for autonomous optimization of treatment plans within a commercial radiotherapy planning system. By extending human-machine interaction through interpretable natural language reasoning, DOLA offers a scalable and privacy-conscious framework, with significant potential for clinical implementation and workflow improvement.", "summary_bullets": ["Radiotherapy treatment planning is a complex and time-intensive process, often impacted by inter-planner variability and subjective decision-making.", "To address these challenges, we introduce Dose Optimization Language Agent (DOLA), an autonomous large language model (LLM)-based agent designed for optimizing radiotherapy treatment plans while rigorously protecting patient privacy.", "DOLA integrates the LLaMa3.1 LLM directly with a commercial treatment planning system, utilizing chain-of-thought prompting, retrieval-augmented generation (RAG), and reinforcement learning (RL)."], "method": "To address these challenges, we introduce Dose Optimization Language Agent (DOLA), an autonomous large language model (LLM)-based agent designed for optimizing radiotherapy treatment plans while rigorously protecting patient privacy.", "key_results": ["DOLA integrates the LLaMa3.1 LLM directly with a commercial treatment planning system, utilizing chain-of-thought prompting, retrieval-augmented generation (RAG), and reinforcement learning (RL).", "We evaluated DOLA using a retrospective cohort of 18 prostate cancer patients prescribed 60 Gy in 20 fractions, comparing model sizes (8 billion vs. 70 billion parameters) and optimization strategies (No-RAG, RAG, and RAG+RL) over 10 planning iterations."], "limitations": ["Abstract-level evidence only: validate assumptions, evaluation protocol, and failure cases in the full paper before relying on this as key evidence."], "bibkey": "Nusrat2025Autonomous"}
{"paper_id": "P0058", "title": "Bridging AI and Software Security: A Comparative Vulnerability Assessment of LLM Agent Deployment Paradigms", "year": 2025, "url": "http://arxiv.org/abs/2507.06323v1", "arxiv_id": "2507.06323v1", "primary_category": "cs.CR", "categories": ["cs.CR", "cs.AI"], "pdf_url": "https://arxiv.org/pdf/2507.06323v1", "priority": "high", "mapped_sections": ["3.1", "6.2"], "evidence_level": "abstract", "fulltext_path": "", "authors": ["Tarek Gasmi", "Ramzi Guesmi", "Ines Belhadj", "Jihene Bennaceur"], "abstract": "Large Language Model (LLM) agents face security vulnerabilities spanning AI-specific and traditional software domains, yet current research addresses these separately. This study bridges this gap through comparative evaluation of Function Calling architecture and Model Context Protocol (MCP) deployment paradigms using a unified threat classification framework. We tested 3,250 attack scenarios across seven language models, evaluating simple, composed, and chained attacks targeting both AI-specific threats (prompt injection) and software vulnerabilities (JSON injection, denial-of-service). Function Calling showed higher overall attack success rates (73.5% vs 62.59% for MCP), with greater system-centric vulnerability while MCP exhibited increased LLM-centric exposure. Attack complexity dramatically amplified effectiveness, with chained attacks achieving 91-96% success rates. Counterintuitively, advanced reasoning models demonstrated higher exploitability despite better threat detection. Results demonstrate that architectural choices fundamentally reshape threat landscapes. This work establishes methodological foundations for cross-domain LLM agent security assessment and provides evidence-based guidance for secure deployment. Code and experimental materials are available at https: // github. com/ theconsciouslab-ai/llm-agent-security.", "summary_bullets": ["Large Language Model (LLM) agents face security vulnerabilities spanning AI-specific and traditional software domains, yet current research addresses these separately.", "This study bridges this gap through comparative evaluation of Function Calling architecture and Model Context Protocol (MCP) deployment paradigms using a unified threat classification framework.", "We tested 3,250 attack scenarios across seven language models, evaluating simple, composed, and chained attacks targeting both AI-specific threats (prompt injection) and software vulnerabilities (JSON injection, denial-of-service).", "Function Calling showed higher overall attack success rates (73.5% vs 62.59% for MCP), with greater system-centric vulnerability while MCP exhibited increased LLM-centric exposure.", "Attack complexity dramatically amplified effectiveness, with chained attacks achieving 91-96% success rates."], "method": "Large Language Model (LLM) agents face security vulnerabilities spanning AI-specific and traditional software domains, yet current research addresses these separately.", "key_results": ["Function Calling showed higher overall attack success rates (73.5% vs 62.59% for MCP), with greater system-centric vulnerability while MCP exhibited increased LLM-centric exposure.", "Attack complexity dramatically amplified effectiveness, with chained attacks achieving 91-96% success rates."], "limitations": ["Abstract-level evidence only: validate assumptions, evaluation protocol, and failure cases in the full paper before relying on this as key evidence."], "bibkey": "Gasmi2025Bridging"}
{"paper_id": "P0059", "title": "Can LLM Agents Solve Collaborative Tasks? A Study on Urgency-Aware Planning and Coordination", "year": 2025, "url": "http://arxiv.org/abs/2508.14635v1", "arxiv_id": "2508.14635v1", "primary_category": "cs.RO", "categories": ["cs.RO", "cs.AI"], "pdf_url": "https://arxiv.org/pdf/2508.14635v1", "priority": "normal", "mapped_sections": ["4.1"], "evidence_level": "abstract", "fulltext_path": "", "authors": ["Jo√£o Vitor de Carvalho Silva", "Douglas G. Macharet"], "abstract": "The ability to coordinate actions across multiple agents is critical for solving complex, real-world problems. Large Language Models (LLMs) have shown strong capabilities in communication, planning, and reasoning, raising the question of whether they can also support effective collaboration in multi-agent settings. In this work, we investigate the use of LLM agents to solve a structured victim rescue task that requires division of labor, prioritization, and cooperative planning. Agents operate in a fully known graph-based environment and must allocate resources to victims with varying needs and urgency levels. We systematically evaluate their performance using a suite of coordination-sensitive metrics, including task success rate, redundant actions, room conflicts, and urgency-weighted efficiency. This study offers new insights into the strengths and failure modes of LLMs in physically grounded multi-agent collaboration tasks, contributing to future benchmarks and architectural improvements.", "summary_bullets": ["The ability to coordinate actions across multiple agents is critical for solving complex, real-world problems.", "Large Language Models (LLMs) have shown strong capabilities in communication, planning, and reasoning, raising the question of whether they can also support effective collaboration in multi-agent settings.", "In this work, we investigate the use of LLM agents to solve a structured victim rescue task that requires division of labor, prioritization, and cooperative planning."], "method": "The ability to coordinate actions across multiple agents is critical for solving complex, real-world problems.", "key_results": ["We systematically evaluate their performance using a suite of coordination-sensitive metrics, including task success rate, redundant actions, room conflicts, and urgency-weighted efficiency.", "This study offers new insights into the strengths and failure modes of LLMs in physically grounded multi-agent collaboration tasks, contributing to future benchmarks and architectural improvements."], "limitations": ["Abstract-level evidence only: validate assumptions, evaluation protocol, and failure cases in the full paper before relying on this as key evidence."], "bibkey": "Silva2025Agents"}
{"paper_id": "P0060", "title": "Complex System Diagnostics Using a Knowledge Graph-Informed and Large Language Model-Enhanced Framework", "year": 2025, "url": "http://arxiv.org/abs/2505.21291v1", "arxiv_id": "2505.21291v1", "primary_category": "cs.AI", "categories": ["cs.AI"], "pdf_url": "https://arxiv.org/pdf/2505.21291v1", "priority": "normal", "mapped_sections": ["6.2"], "evidence_level": "abstract", "fulltext_path": "", "authors": ["Saman Marandi", "Yu-Shu Hu", "Mohammad Modarres"], "abstract": "In this paper, we present a novel diagnostic framework that integrates Knowledge Graphs (KGs) and Large Language Models (LLMs) to support system diagnostics in high-reliability systems such as nuclear power plants. Traditional diagnostic modeling struggles when systems become too complex, making functional modeling a more attractive approach. Our approach introduces a diagnostic framework grounded in the functional modeling principles of the Dynamic Master Logic (DML) model. It incorporates two coordinated LLM components, including an LLM-based workflow for automated construction of DML logic from system documentation and an LLM agent that facilitates interactive diagnostics. The generated logic is encoded into a structured KG, referred to as KG-DML, which supports hierarchical fault reasoning. Expert knowledge or operational data can also be incorporated to refine the model's precision and diagnostic depth. In the interaction phase, users submit natural language queries, which are interpreted by the LLM agent. The agent selects appropriate tools for structured reasoning, including upward and downward propagation across the KG-DML. Rather than embedding KG content into every prompt, the LLM agent distinguishes between diagnostic and interpretive tasks. For diagnostics, the agent selects and executes external tools that perform structured KG reasoning. For general queries, a Graph-based Retrieval-Augmented Generation (Graph-RAG) approach is used, retrieving relevant KG segments and embedding them into the prompt to generate natural explanations. A case study on an auxiliary feedwater system demonstrated the framework's effectiveness, with over 90% accuracy in key elements and consistent tool and argument extraction, supporting its use in safety-critical diagnostics.", "summary_bullets": ["In this paper, we present a novel diagnostic framework that integrates Knowledge Graphs (KGs) and Large Language Models (LLMs) to support system diagnostics in high-reliability systems such as nuclear power plants.", "Traditional diagnostic modeling struggles when systems become too complex, making functional modeling a more attractive approach.", "Our approach introduces a diagnostic framework grounded in the functional modeling principles of the Dynamic Master Logic (DML) model."], "method": "In this paper, we present a novel diagnostic framework that integrates Knowledge Graphs (KGs) and Large Language Models (LLMs) to support system diagnostics in high-reliability systems such as nuclear power plants.", "key_results": ["A case study on an auxiliary feedwater system demonstrated the framework's effectiveness, with over 90% accuracy in key elements and consistent tool and argument extraction, supporting its use in safety-critical diagnostics."], "limitations": ["Abstract-level evidence only: validate assumptions, evaluation protocol, and failure cases in the full paper before relying on this as key evidence."], "bibkey": "Marandi2025Complex"}
{"paper_id": "P0061", "title": "Connecting the Dots: A Chain-of-Collaboration Prompting Framework for LLM Agents", "year": 2025, "url": "http://arxiv.org/abs/2505.10936v1", "arxiv_id": "2505.10936v1", "primary_category": "cs.CL", "categories": ["cs.CL"], "pdf_url": "https://arxiv.org/pdf/2505.10936v1", "priority": "normal", "mapped_sections": [], "evidence_level": "abstract", "fulltext_path": "", "authors": ["Jiaxing Zhao", "Hongbin Xie", "Yuzhen Lei", "Xuan Song", "Zhuoran Shi", "Lianxin Li", "Shuangxue Liu", "Haoran Zhang"], "abstract": "Large Language Models (LLMs) have demonstrated impressive performance in executing complex reasoning tasks. Chain-of-thought effectively enhances reasoning capabilities by unlocking the potential of large models, while multi-agent systems provide more comprehensive solutions by integrating collective intelligence of multiple agents. However, both approaches face significant limitations. Single-agent with chain-of-thought, due to the inherent complexity of designing cross-domain prompts, faces collaboration challenges. Meanwhile, multi-agent systems consume substantial tokens and inevitably dilute the primary problem, which is particularly problematic in business workflow tasks. To address these challenges, we propose Cochain, a collaboration prompting framework that effectively solves business workflow collaboration problem by combining knowledge and prompts at a reduced cost. Specifically, we construct an integrated knowledge graph that incorporates knowledge from multiple stages. Furthermore, by maintaining and retrieving a prompts tree, we can obtain prompt information relevant to other stages of the business workflow. We perform extensive evaluations of Cochain across multiple datasets, demonstrating that Cochain outperforms all baselines in both prompt engineering and multi-agent LLMs. Additionally, expert evaluation results indicate that the use of a small model in combination with Cochain outperforms GPT-4.", "summary_bullets": ["Large Language Models (LLMs) have demonstrated impressive performance in executing complex reasoning tasks.", "Chain-of-thought effectively enhances reasoning capabilities by unlocking the potential of large models, while multi-agent systems provide more comprehensive solutions by integrating collective intelligence of multiple agents.", "However, both approaches face significant limitations."], "method": "To address these challenges, we propose Cochain, a collaboration prompting framework that effectively solves business workflow collaboration problem by combining knowledge and prompts at a reduced cost.", "key_results": ["Additionally, expert evaluation results indicate that the use of a small model in combination with Cochain outperforms GPT-4.", "We perform extensive evaluations of Cochain across multiple datasets, demonstrating that Cochain outperforms all baselines in both prompt engineering and multi-agent LLMs."], "limitations": ["Abstract-level evidence only: validate assumptions, evaluation protocol, and failure cases in the full paper before relying on this as key evidence.", "However, both approaches face significant limitations."], "bibkey": "Zhao2025Connecting"}
{"paper_id": "P0062", "title": "Evaluating Memory in LLM Agents via Incremental Multi-Turn Interactions", "year": 2025, "url": "http://arxiv.org/abs/2507.05257v2", "arxiv_id": "2507.05257v2", "primary_category": "cs.CL", "categories": ["cs.CL", "cs.AI"], "pdf_url": "https://arxiv.org/pdf/2507.05257v2", "priority": "normal", "mapped_sections": ["4.1", "4.2"], "evidence_level": "abstract", "fulltext_path": "", "authors": ["Yuanzhe Hu", "Yu Wang", "Julian McAuley"], "abstract": "Recent benchmarks for Large Language Model (LLM) agents primarily focus on evaluating reasoning, planning, and execution capabilities, while another critical component-memory, encompassing how agents memorize, update, and retrieve long-term information-is under-evaluated due to the lack of benchmarks. We term agents with memory mechanisms as memory agents. In this paper, based on classic theories from memory science and cognitive science, we identify four core competencies essential for memory agents: accurate retrieval, test-time learning, long-range understanding, and selective forgetting. Existing benchmarks either rely on limited context lengths or are tailored for static, long-context settings like book-based QA, which do not reflect the interactive, multi-turn nature of memory agents that incrementally accumulate information. Moreover, no existing benchmarks cover all four competencies. We introduce MemoryAgentBench, a new benchmark specifically designed for memory agents. Our benchmark transforms existing long-context datasets and incorporates newly constructed datasets into a multi-turn format, effectively simulating the incremental information processing characteristic of memory agents. By carefully selecting and curating datasets, our benchmark provides comprehensive coverage of the four core memory competencies outlined above, thereby offering a systematic and challenging testbed for assessing memory quality. We evaluate a diverse set of memory agents, ranging from simple context-based and retrieval-augmented generation (RAG) systems to advanced agents with external memory modules and tool integration. Empirical results reveal that current methods fall short of mastering all four competencies, underscoring the need for further research into comprehensive memory mechanisms for LLM agents.", "summary_bullets": ["Recent benchmarks for Large Language Model (LLM) agents primarily focus on evaluating reasoning, planning, and execution capabilities, while another critical component-memory, encompassing how agents memorize, update, and retrieve long-term information-is under-evaluated due to the lack of benchmarks.", "We term agents with memory mechanisms as memory agents.", "In this paper, based on classic theories from memory science and cognitive science, we identify four core competencies essential for memory agents: accurate retrieval, test-time learning, long-range understanding, and selective forgetting."], "method": "We introduce MemoryAgentBench, a new benchmark specifically designed for memory agents.", "key_results": ["Recent benchmarks for Large Language Model (LLM) agents primarily focus on evaluating reasoning, planning, and execution capabilities, while another critical component-memory, encompassing how agents memorize, update, and retrieve long-term information-is under-evaluated due to the lack of benchmarks.", "Existing benchmarks either rely on limited context lengths or are tailored for static, long-context settings like book-based QA, which do not reflect the interactive, multi-turn nature of memory agents that incrementally accumulate information."], "limitations": ["Abstract-level evidence only: validate assumptions, evaluation protocol, and failure cases in the full paper before relying on this as key evidence."], "bibkey": "Hu2025Evaluating"}
{"paper_id": "P0063", "title": "Forewarned is Forearmed: A Survey on Large Language Model-based Agents in Autonomous Cyberattacks", "year": 2025, "url": "http://arxiv.org/abs/2505.12786v2", "arxiv_id": "2505.12786v2", "primary_category": "cs.NI", "categories": ["cs.NI"], "pdf_url": "https://arxiv.org/pdf/2505.12786v2", "priority": "normal", "mapped_sections": [], "evidence_level": "abstract", "fulltext_path": "", "authors": ["Minrui Xu", "Jiani Fan", "Xinyu Huang", "Conghao Zhou", "Jiawen Kang", "Dusit Niyato", "Shiwen Mao", "Zhu Han", "Xuemin", "Shen", "Kwok-Yan Lam"], "abstract": "With the continuous evolution of Large Language Models (LLMs), LLM-based agents have advanced beyond passive chatbots to become autonomous cyber entities capable of performing complex tasks, including web browsing, malicious code and deceptive content generation, and decision-making. By significantly reducing the time, expertise, and resources, AI-assisted cyberattacks orchestrated by LLM-based agents have led to a phenomenon termed Cyber Threat Inflation, characterized by a significant reduction in attack costs and a tremendous increase in attack scale. To provide actionable defensive insights, in this survey, we focus on the potential cyber threats posed by LLM-based agents across diverse network systems. Firstly, we present the capabilities of LLM-based cyberattack agents, which include executing autonomous attack strategies, comprising scouting, memory, reasoning, and action, and facilitating collaborative operations with other agents or human operators. Building on these capabilities, we examine common cyberattacks initiated by LLM-based agents and compare their effectiveness across different types of networks, including static, mobile, and infrastructure-free paradigms. Moreover, we analyze threat bottlenecks of LLM-based agents across different network infrastructures and review their defense methods. Due to operational imbalances, existing defense methods are inadequate against autonomous cyberattacks. Finally, we outline future research directions and potential defensive strategies for legacy network systems.", "summary_bullets": ["With the continuous evolution of Large Language Models (LLMs), LLM-based agents have advanced beyond passive chatbots to become autonomous cyber entities capable of performing complex tasks, including web browsing, malicious code and deceptive content generation, and decision-making.", "By significantly reducing the time, expertise, and resources, AI-assisted cyberattacks orchestrated by LLM-based agents have led to a phenomenon termed Cyber Threat Inflation, characterized by a significant reduction in attack costs and a tremendous increase in attack scale.", "To provide actionable defensive insights, in this survey, we focus on the potential cyber threats posed by LLM-based agents across diverse network systems."], "method": "Firstly, we present the capabilities of LLM-based cyberattack agents, which include executing autonomous attack strategies, comprising scouting, memory, reasoning, and action, and facilitating collaborative operations with other agents or human operators.", "key_results": ["Firstly, we present the capabilities of LLM-based cyberattack agents, which include executing autonomous attack strategies, comprising scouting, memory, reasoning, and action, and facilitating collaborative operations with other agents or human operators.", "By significantly reducing the time, expertise, and resources, AI-assisted cyberattacks orchestrated by LLM-based agents have led to a phenomenon termed Cyber Threat Inflation, characterized by a significant reduction in attack costs and a tremendous increase in attack scale."], "limitations": ["Abstract-level evidence only: validate assumptions, evaluation protocol, and failure cases in the full paper before relying on this as key evidence."], "bibkey": "Xu2025Forewarned"}
{"paper_id": "P0064", "title": "GraphCodeAgent: Dual Graph-Guided LLM Agent for Retrieval-Augmented Repo-Level Code Generation", "year": 2025, "url": "http://arxiv.org/abs/2504.10046v2", "arxiv_id": "2504.10046v2", "primary_category": "cs.SE", "categories": ["cs.SE"], "pdf_url": "https://arxiv.org/pdf/2504.10046v2", "priority": "normal", "mapped_sections": ["4.2"], "evidence_level": "abstract", "fulltext_path": "", "authors": ["Jia Li", "Xianjie Shi", "Kechi Zhang", "Ge Li", "Zhi Jin", "Lei Li", "Huangzhao Zhang", "Jia Li", "Fang Liu", "Yuwei Zhang", "Zhengwei Tao", "Yihong Dong", "Yuqi Zhu", "Chongyang Tao"], "abstract": "Writing code requires significant time and effort in software development. To automate this process, researchers have made substantial progress for code generation. Recently, large language models (LLMs) have demonstrated remarkable proficiency in function-level code generation, yet their performance significantly degrades in the real-world software development process, where coding tasks are deeply embedded within specific repository contexts. Existing studies attempt to use retrieval-augmented code generation (RACG) approaches to mitigate this demand. However, there is a gap between natural language (NL) requirements and programming implementations. This results in the failure to retrieve the relevant code of these fine-grained subtasks. To address this challenge, we propose GraphCodeAgent, a dual graph-guided LLM agent for retrieval-augmented repo-level code generation, bridging the gap between NL requirements and programming implementations. Our approach constructs two interconnected graphs: a Requirement Graph (RG) to model requirement relations of code snippets within the repository, as well as the relations between the target requirement and the requirements of these code snippets, and a Structural-Semantic Code Graph (SSCG) to capture the repository's intricate code dependencies. Guided by this, an LLM-powered agent performs multi-hop reasoning to systematically retrieve all context code snippets, including implicit and explicit code snippets, even if they are not explicitly expressed in requirements. We evaluated GraphCodeAgent on three advanced LLMs with the two widely-used repo-level code generation benchmarks DevEval and CoderEval. Extensive experiment results show that GraphCodeAgent significantly outperforms state-of-the-art baselines.", "summary_bullets": ["Writing code requires significant time and effort in software development.", "To automate this process, researchers have made substantial progress for code generation.", "Recently, large language models (LLMs) have demonstrated remarkable proficiency in function-level code generation, yet their performance significantly degrades in the real-world software development process, where coding tasks are deeply embedded within specific repository contexts."], "method": "To address this challenge, we propose GraphCodeAgent, a dual graph-guided LLM agent for retrieval-augmented repo-level code generation, bridging the gap between NL requirements and programming implementations.", "key_results": ["We evaluated GraphCodeAgent on three advanced LLMs with the two widely-used repo-level code generation benchmarks DevEval and CoderEval.", "Extensive experiment results show that GraphCodeAgent significantly outperforms state-of-the-art baselines."], "limitations": ["Abstract-level evidence only: validate assumptions, evaluation protocol, and failure cases in the full paper before relying on this as key evidence."], "bibkey": "Li2025Graphcodeagent"}
{"paper_id": "P0065", "title": "HierRouter: Coordinated Routing of Specialized Large Language Models via Reinforcement Learning", "year": 2025, "url": "http://arxiv.org/abs/2511.09873v1", "arxiv_id": "2511.09873v1", "primary_category": "cs.CL", "categories": ["cs.CL", "cs.LG"], "pdf_url": "https://arxiv.org/pdf/2511.09873v1", "priority": "normal", "mapped_sections": [], "evidence_level": "abstract", "fulltext_path": "", "authors": ["Nikunj Gupta", "Bill Guo", "Rajgopal Kannan", "Viktor K. Prasanna"], "abstract": "Large Language Models (LLMs) deliver state-of-the-art performance across many tasks but impose high computational and memory costs, limiting their deployment in resource-constrained or real-time settings. To address this, we propose HierRouter, a hierarchical routing approach that dynamically assembles inference pipelines from a pool of specialized, lightweight language models. Formulated as a finite-horizon Markov Decision Process (MDP), our approach trains a Proximal Policy Optimization (PPO)-based reinforcement learning agent to iteratively select which models to invoke at each stage of multi-hop inference. The agent conditions on the evolving context and accumulated cost to make context-aware routing decisions. Experiments with three open-source candidate LLMs across six benchmarks, including QA, code generation, and mathematical reasoning, show that HierRouter improves response quality by up to 2.4x compared to using individual models independently, while incurring only a minimal additional inference cost on average. These results highlight the promise of hierarchical routing for cost-efficient, high-performance LLM inference. All codes can be found here https://github.com/ Nikunj-Gupta/hierouter.", "summary_bullets": ["Large Language Models (LLMs) deliver state-of-the-art performance across many tasks but impose high computational and memory costs, limiting their deployment in resource-constrained or real-time settings.", "To address this, we propose HierRouter, a hierarchical routing approach that dynamically assembles inference pipelines from a pool of specialized, lightweight language models.", "Formulated as a finite-horizon Markov Decision Process (MDP), our approach trains a Proximal Policy Optimization (PPO)-based reinforcement learning agent to iteratively select which models to invoke at each stage of multi-hop inference."], "method": "To address this, we propose HierRouter, a hierarchical routing approach that dynamically assembles inference pipelines from a pool of specialized, lightweight language models.", "key_results": ["Experiments with three open-source candidate LLMs across six benchmarks, including QA, code generation, and mathematical reasoning, show that HierRouter improves response quality by up to 2.4x compared to using individual models independently, while incurring only a minimal additional inference cost on average.", "Large Language Models (LLMs) deliver state-of-the-art performance across many tasks but impose high computational and memory costs, limiting their deployment in resource-constrained or real-time settings."], "limitations": ["Abstract-level evidence only: validate assumptions, evaluation protocol, and failure cases in the full paper before relying on this as key evidence."], "bibkey": "Gupta2025Hierrouter"}
{"paper_id": "P0066", "title": "InfoBid: A Simulation Framework for Studying Information Disclosure in Auctions with Large Language Model-based Agents", "year": 2025, "url": "http://arxiv.org/abs/2503.22726v1", "arxiv_id": "2503.22726v1", "primary_category": "cs.GT", "categories": ["cs.GT", "cs.CL", "cs.HC", "cs.MA", "econ.GN"], "pdf_url": "https://arxiv.org/pdf/2503.22726v1", "priority": "normal", "mapped_sections": ["3.2", "4.2"], "evidence_level": "abstract", "fulltext_path": "", "authors": ["Yue Yin"], "abstract": "In online advertising systems, publishers often face a trade-off in information disclosure strategies: while disclosing more information can enhance efficiency by enabling optimal allocation of ad impressions, it may lose revenue potential by decreasing uncertainty among competing advertisers. Similar to other challenges in market design, understanding this trade-off is constrained by limited access to real-world data, leading researchers and practitioners to turn to simulation frameworks. The recent emergence of large language models (LLMs) offers a novel approach to simulations, providing human-like reasoning and adaptability without necessarily relying on explicit assumptions about agent behavior modeling. Despite their potential, existing frameworks have yet to integrate LLM-based agents for studying information asymmetry and signaling strategies, particularly in the context of auctions. To address this gap, we introduce InfoBid, a flexible simulation framework that leverages LLM agents to examine the effects of information disclosure strategies in multi-agent auction settings. Using GPT-4o, we implemented simulations of second-price auctions with diverse information schemas. The results reveal key insights into how signaling influences strategic behavior and auction outcomes, which align with both economic and social learning theories. Through InfoBid, we hope to foster the use of LLMs as proxies for human economic and social agents in empirical studies, enhancing our understanding of their capabilities and limitations. This work bridges the gap between theoretical market designs and practical applications, advancing research in market simulations, information design, and agent-based reasoning while offering a valuable tool for exploring the dynamics of digital economies.", "summary_bullets": ["In online advertising systems, publishers often face a trade-off in information disclosure strategies: while disclosing more information can enhance efficiency by enabling optimal allocation of ad impressions, it may lose revenue potential by decreasing uncertainty among competing advertisers.", "Similar to other challenges in market design, understanding this trade-off is constrained by limited access to real-world data, leading researchers and practitioners to turn to simulation frameworks.", "The recent emergence of large language models (LLMs) offers a novel approach to simulations, providing human-like reasoning and adaptability without necessarily relying on explicit assumptions about agent behavior modeling."], "method": "To address this gap, we introduce InfoBid, a flexible simulation framework that leverages LLM agents to examine the effects of information disclosure strategies in multi-agent auction settings.", "key_results": ["The recent emergence of large language models (LLMs) offers a novel approach to simulations, providing human-like reasoning and adaptability without necessarily relying on explicit assumptions about agent behavior modeling.", "Through InfoBid, we hope to foster the use of LLMs as proxies for human economic and social agents in empirical studies, enhancing our understanding of their capabilities and limitations."], "limitations": ["Abstract-level evidence only: validate assumptions, evaluation protocol, and failure cases in the full paper before relying on this as key evidence.", "Through InfoBid, we hope to foster the use of LLMs as proxies for human economic and social agents in empirical studies, enhancing our understanding of their capabilities and limitations."], "bibkey": "Yin2025Infobid"}
{"paper_id": "P0067", "title": "InsurAgent: A Large Language Model-Empowered Agent for Simulating Individual Behavior in Purchasing Flood Insurance", "year": 2025, "url": "http://arxiv.org/abs/2511.02119v1", "arxiv_id": "2511.02119v1", "primary_category": "cs.AI", "categories": ["cs.AI", "cs.CL"], "pdf_url": "https://arxiv.org/pdf/2511.02119v1", "priority": "normal", "mapped_sections": [], "evidence_level": "abstract", "fulltext_path": "", "authors": ["Ziheng Geng", "Jiachen Liu", "Ran Cao", "Lu Cheng", "Dan M. Frangopol", "Minghui Cheng"], "abstract": "Flood insurance is an effective strategy for individuals to mitigate disaster-related losses. However, participation rates among at-risk populations in the United States remain strikingly low. This gap underscores the need to understand and model the behavioral mechanisms underlying insurance decisions. Large language models (LLMs) have recently exhibited human-like intelligence across wide-ranging tasks, offering promising tools for simulating human decision-making. This study constructs a benchmark dataset to capture insurance purchase probabilities across factors. Using this dataset, the capacity of LLMs is evaluated: while LLMs exhibit a qualitative understanding of factors, they fall short in estimating quantitative probabilities. To address this limitation, InsurAgent, an LLM-empowered agent comprising five modules including perception, retrieval, reasoning, action, and memory, is proposed. The retrieval module leverages retrieval-augmented generation (RAG) to ground decisions in empirical survey data, achieving accurate estimation of marginal and bivariate probabilities. The reasoning module leverages LLM common sense to extrapolate beyond survey data, capturing contextual information that is intractable for traditional models. The memory module supports the simulation of temporal decision evolutions, illustrated through a roller coaster life trajectory. Overall, InsurAgent provides a valuable tool for behavioral modeling and policy analysis.", "summary_bullets": ["Flood insurance is an effective strategy for individuals to mitigate disaster-related losses.", "However, participation rates among at-risk populations in the United States remain strikingly low.", "This gap underscores the need to understand and model the behavioral mechanisms underlying insurance decisions."], "method": "Flood insurance is an effective strategy for individuals to mitigate disaster-related losses.", "key_results": ["Large language models (LLMs) have recently exhibited human-like intelligence across wide-ranging tasks, offering promising tools for simulating human decision-making.", "This study constructs a benchmark dataset to capture insurance purchase probabilities across factors."], "limitations": ["Abstract-level evidence only: validate assumptions, evaluation protocol, and failure cases in the full paper before relying on this as key evidence.", "To address this limitation, InsurAgent, an LLM-empowered agent comprising five modules including perception, retrieval, reasoning, action, and memory, is proposed."], "bibkey": "Geng2025Insuragent"}
{"paper_id": "P0068", "title": "LLM Agents Beyond Utility: An Open-Ended Perspective", "year": 2025, "url": "http://arxiv.org/abs/2510.14548v1", "arxiv_id": "2510.14548v1", "primary_category": "cs.AI", "categories": ["cs.AI"], "pdf_url": "https://arxiv.org/pdf/2510.14548v1", "priority": "normal", "mapped_sections": [], "evidence_level": "abstract", "fulltext_path": "", "authors": ["Asen Nachkov", "Xi Wang", "Luc Van Gool"], "abstract": "Recent LLM agents have made great use of chain of thought reasoning and function calling. As their capabilities grow, an important question arises: can this software represent not only a smart problem-solving tool, but an entity in its own right, that can plan, design immediate tasks, and reason toward broader, more ambiguous goals? To study this question, we adopt an open-ended experimental setting where we augment a pretrained LLM agent with the ability to generate its own tasks, accumulate knowledge, and interact extensively with its environment. We study the resulting open-ended agent qualitatively. It can reliably follow complex multi-step instructions, store and reuse information across runs, and propose and solve its own tasks, though it remains sensitive to prompt design, prone to repetitive task generation, and unable to form self-representations. These findings illustrate both the promise and current limits of adapting pretrained LLMs toward open-endedness, and point to future directions for training agents to manage memory, explore productively, and pursue abstract long-term goals.", "summary_bullets": ["Recent LLM agents have made great use of chain of thought reasoning and function calling.", "As their capabilities grow, an important question arises: can this software represent not only a smart problem-solving tool, but an entity in its own right, that can plan, design immediate tasks, and reason toward broader, more ambiguous goals?", "To study this question, we adopt an open-ended experimental setting where we augment a pretrained LLM agent with the ability to generate its own tasks, accumulate knowledge, and interact extensively with its environment."], "method": "We study the resulting open-ended agent qualitatively.", "key_results": ["These findings illustrate both the promise and current limits of adapting pretrained LLMs toward open-endedness, and point to future directions for training agents to manage memory, explore productively, and pursue abstract long-term goals."], "limitations": ["Abstract-level evidence only: validate assumptions, evaluation protocol, and failure cases in the full paper before relying on this as key evidence."], "bibkey": "Nachkov2025Agents"}
{"paper_id": "P0069", "title": "LLM Agents for Knowledge Discovery in Atomic Layer Processing", "year": 2025, "url": "http://arxiv.org/abs/2509.26201v1", "arxiv_id": "2509.26201v1", "primary_category": "cs.AI", "categories": ["cs.AI", "cond-mat.mes-hall", "cond-mat.mtrl-sci"], "pdf_url": "https://arxiv.org/pdf/2509.26201v1", "priority": "normal", "mapped_sections": [], "evidence_level": "abstract", "fulltext_path": "", "authors": ["Andreas Werbrouck", "Marshall B. Lindsay", "Matthew Maschmann", "Matthias J. Young"], "abstract": "Large Language Models (LLMs) have garnered significant attention for several years now. Recently, their use as independently reasoning agents has been proposed. In this work, we test the potential of such agents for knowledge discovery in materials science. We repurpose LangGraph's tool functionality to supply agents with a black box function to interrogate. In contrast to process optimization or performing specific, user-defined tasks, knowledge discovery consists of freely exploring the system, posing and verifying statements about the behavior of this black box, with the sole objective of generating and verifying generalizable statements. We provide proof of concept for this approach through a children's parlor game, demonstrating the role of trial-and-error and persistence in knowledge discovery, and the strong path-dependence of results. We then apply the same strategy to show that LLM agents can explore, discover, and exploit diverse chemical interactions in an advanced Atomic Layer Processing reactor simulation using intentionally limited probe capabilities without explicit instructions.", "summary_bullets": ["Large Language Models (LLMs) have garnered significant attention for several years now.", "Recently, their use as independently reasoning agents has been proposed.", "In this work, we test the potential of such agents for knowledge discovery in materials science."], "method": "Large Language Models (LLMs) have garnered significant attention for several years now.", "key_results": ["We then apply the same strategy to show that LLM agents can explore, discover, and exploit diverse chemical interactions in an advanced Atomic Layer Processing reactor simulation using intentionally limited probe capabilities without explicit instructions."], "limitations": ["Abstract-level evidence only: validate assumptions, evaluation protocol, and failure cases in the full paper before relying on this as key evidence."], "bibkey": "Werbrouck2025Agents"}
{"paper_id": "P0070", "title": "LLM-Agent-Controller: A Universal Multi-Agent Large Language Model System as a Control Engineer", "year": 2025, "url": "http://arxiv.org/abs/2505.19567v1", "arxiv_id": "2505.19567v1", "primary_category": "cs.AI", "categories": ["cs.AI", "cs.MA"], "pdf_url": "https://arxiv.org/pdf/2505.19567v1", "priority": "normal", "mapped_sections": ["5.2"], "evidence_level": "abstract", "fulltext_path": "", "authors": ["Rasoul Zahedifar", "Sayyed Ali Mirghasemi", "Mahdieh Soleymani Baghshah", "Alireza Taheri"], "abstract": "This study presents the LLM-Agent-Controller, a multi-agent large language model (LLM) system developed to address a wide range of problems in control engineering (Control Theory). The system integrates a central controller agent with multiple specialized auxiliary agents, responsible for tasks such as controller design, model representation, control analysis, time-domain response, and simulation. A supervisor oversees high-level decision-making and workflow coordination, enhancing the system's reliability and efficiency. The LLM-Agent-Controller incorporates advanced capabilities, including Retrieval-Augmented Generation (RAG), Chain-of-Thought reasoning, self-criticism and correction, efficient memory handling, and user-friendly natural language communication. It is designed to function without requiring users to have prior knowledge of Control Theory, enabling them to input problems in plain language and receive complete, real-time solutions. To evaluate the system, we propose new performance metrics assessing both individual agents and the system as a whole. We test five categories of Control Theory problems and benchmark performance across three advanced LLMs. Additionally, we conduct a comprehensive qualitative conversational analysis covering all key services. Results show that the LLM-Agent-Controller successfully solved 83% of general tasks, with individual agents achieving an average success rate of 87%. Performance improved with more advanced LLMs. This research demonstrates the potential of multi-agent LLM architectures to solve complex, domain-specific problems. By integrating specialized agents, supervisory control, and advanced reasoning, the LLM-Agent-Controller offers a scalable, robust, and accessible solution framework that can be extended to various technical domains.", "summary_bullets": ["This study presents the LLM-Agent-Controller, a multi-agent large language model (LLM) system developed to address a wide range of problems in control engineering (Control Theory).", "The system integrates a central controller agent with multiple specialized auxiliary agents, responsible for tasks such as controller design, model representation, control analysis, time-domain response, and simulation.", "A supervisor oversees high-level decision-making and workflow coordination, enhancing the system's reliability and efficiency."], "method": "To evaluate the system, we propose new performance metrics assessing both individual agents and the system as a whole.", "key_results": ["Results show that the LLM-Agent-Controller successfully solved 83% of general tasks, with individual agents achieving an average success rate of 87%.", "To evaluate the system, we propose new performance metrics assessing both individual agents and the system as a whole."], "limitations": ["Abstract-level evidence only: validate assumptions, evaluation protocol, and failure cases in the full paper before relying on this as key evidence."], "bibkey": "Zahedifar2025Agent"}
{"paper_id": "P0071", "title": "LLM-Based Human-Agent Collaboration and Interaction Systems: A Survey", "year": 2025, "url": "http://arxiv.org/abs/2505.00753v4", "arxiv_id": "2505.00753v4", "primary_category": "cs.CL", "categories": ["cs.CL", "cs.LG"], "pdf_url": "https://arxiv.org/pdf/2505.00753v4", "priority": "normal", "mapped_sections": ["3.1"], "evidence_level": "abstract", "fulltext_path": "", "authors": ["Henry Peng Zou", "Wei-Chieh Huang", "Yaozu Wu", "Yankai Chen", "Chunyu Miao", "Hoang Nguyen", "Yue Zhou", "Weizhi Zhang", "Liancheng Fang", "Langzhou He", "Yangning Li", "Dongyuan Li", "Renhe Jiang", "Xue Liu", "Philip S. Yu"], "abstract": "Recent advances in large language models (LLMs) have sparked growing interest in building fully autonomous agents. However, fully autonomous LLM-based agents still face significant challenges, including limited reliability due to hallucinations, difficulty in handling complex tasks, and substantial safety and ethical risks, all of which limit their feasibility and trustworthiness in real-world applications. To overcome these limitations, LLM-based human-agent systems (LLM-HAS) incorporate human-provided information, feedback, or control into the agent system to enhance system performance, reliability and safety. These human-agent collaboration systems enable humans and LLM-based agents to collaborate effectively by leveraging their complementary strengths. This paper provides the first comprehensive and structured survey of LLM-HAS. It clarifies fundamental concepts, systematically presents core components shaping these systems, including environment & profiling, human feedback, interaction types, orchestration and communication, explores emerging applications, and discusses unique challenges and opportunities arising from human-AI collaboration. By consolidating current knowledge and offering a structured overview, we aim to foster further research and innovation in this rapidly evolving interdisciplinary field. Paper lists and resources are available at https://github.com/HenryPengZou/Awesome-Human-Agent-Collaboration-Interaction-Systems.", "summary_bullets": ["Recent advances in large language models (LLMs) have sparked growing interest in building fully autonomous agents.", "However, fully autonomous LLM-based agents still face significant challenges, including limited reliability due to hallucinations, difficulty in handling complex tasks, and substantial safety and ethical risks, all of which limit their feasibility and trustworthiness in real-world applications.", "To overcome these limitations, LLM-based human-agent systems (LLM-HAS) incorporate human-provided information, feedback, or control into the agent system to enhance system performance, reliability and safety."], "method": "Recent advances in large language models (LLMs) have sparked growing interest in building fully autonomous agents.", "key_results": ["To overcome these limitations, LLM-based human-agent systems (LLM-HAS) incorporate human-provided information, feedback, or control into the agent system to enhance system performance, reliability and safety.", "These human-agent collaboration systems enable humans and LLM-based agents to collaborate effectively by leveraging their complementary strengths."], "limitations": ["Abstract-level evidence only: validate assumptions, evaluation protocol, and failure cases in the full paper before relying on this as key evidence.", "To overcome these limitations, LLM-based human-agent systems (LLM-HAS) incorporate human-provided information, feedback, or control into the agent system to enhance system performance, reliability and safety."], "bibkey": "Zou2025Based"}
{"paper_id": "P0072", "title": "LLM-Guided Reinforcement Learning with Representative Agents for Traffic Modeling", "year": 2025, "url": "http://arxiv.org/abs/2511.06260v1", "arxiv_id": "2511.06260v1", "primary_category": "cs.GT", "categories": ["cs.GT", "cs.AI", "eess.SY"], "pdf_url": "https://arxiv.org/pdf/2511.06260v1", "priority": "normal", "mapped_sections": [], "evidence_level": "abstract", "fulltext_path": "", "authors": ["Hanlin Sun", "Jiayang Li"], "abstract": "Large language models (LLMs) are increasingly used as behavioral proxies for self-interested travelers in agent-based traffic models. Although more flexible and generalizable than conventional models, the practical use of these approaches remains limited by scalability due to the cost of calling one LLM for every traveler. Moreover, it has been found that LLM agents often make opaque choices and produce unstable day-to-day dynamics. To address these challenges, we propose to model each homogeneous traveler group facing the same decision context with a single representative LLM agent who behaves like the population's average, maintaining and updating a mixed strategy over routes that coincides with the group's aggregate flow proportions. Each day, the LLM reviews the travel experience and flags routes with positive reinforcement that they hope to use more often, and an interpretable update rule then converts this judgment into strategy adjustments using a tunable (progressively decaying) step size. The representative-agent design improves scalability, while the separation of reasoning from updating clarifies the decision logic while stabilizing learning. In classic traffic assignment settings, we find that the proposed approach converges rapidly to the user equilibrium. In richer settings with income heterogeneity, multi-criteria costs, and multi-modal choices, the generated dynamics remain stable and interpretable, reproducing plausible behavioral patterns well-documented in psychology and economics, for example, the decoy effect in toll versus non-toll road selection, and higher willingness-to-pay for convenience among higher-income travelers when choosing between driving, transit, and park-and-ride options.", "summary_bullets": ["Large language models (LLMs) are increasingly used as behavioral proxies for self-interested travelers in agent-based traffic models.", "Although more flexible and generalizable than conventional models, the practical use of these approaches remains limited by scalability due to the cost of calling one LLM for every traveler.", "Moreover, it has been found that LLM agents often make opaque choices and produce unstable day-to-day dynamics."], "method": "To address these challenges, we propose to model each homogeneous traveler group facing the same decision context with a single representative LLM agent who behaves like the population's average, maintaining and updating a mixed strategy over routes that coincides with the group's aggregate flow proportions.", "key_results": ["In richer settings with income heterogeneity, multi-criteria costs, and multi-modal choices, the generated dynamics remain stable and interpretable, reproducing plausible behavioral patterns well-documented in psychology and economics, for example, the decoy effect in toll versus non-toll road selection, and higher willingness-to-pay for convenience among higher-income travelers when choosing between driving, transit, and park-and-ride options."], "limitations": ["Abstract-level evidence only: validate assumptions, evaluation protocol, and failure cases in the full paper before relying on this as key evidence."], "bibkey": "Sun2025Guided"}
{"paper_id": "P0073", "title": "LLM/Agent-as-Data-Analyst: A Survey", "year": 2025, "url": "http://arxiv.org/abs/2509.23988v3", "arxiv_id": "2509.23988v3", "primary_category": "cs.AI", "categories": ["cs.AI", "cs.DB"], "pdf_url": "https://arxiv.org/pdf/2509.23988v3", "priority": "normal", "mapped_sections": [], "evidence_level": "abstract", "fulltext_path": "", "authors": ["Zirui Tang", "Weizheng Wang", "Zihang Zhou", "Yang Jiao", "Bangrui Xu", "Boyu Niu", "Dayou Zhou", "Xuanhe Zhou", "Guoliang Li", "Yeye He", "Wei Zhou", "Yitong Song", "Cheng Tan", "Xue Yang", "Chunwei Liu", "Bin Wang", "Conghui He", "Xiaoyang Wang", "Fan Wu"], "abstract": "Large language models (LLMs) and agent techniques have brought a fundamental shift in the functionality and development paradigm of data analysis tasks (a.k.a LLM/Agent-as-Data-Analyst), demonstrating substantial impact across both academia and industry. In comparison with traditional rule or small-model based approaches, (agentic) LLMs enable complex data understanding, natural language interfaces, semantic analysis functions, and autonomous pipeline orchestration. From a modality perspective, we review LLM-based techniques for (i) structured data (e.g., NL2SQL, NL2GQL, ModelQA), (ii) semi-structured data (e.g., markup languages understanding, semi-structured table question answering), (iii) unstructured data (e.g., chart understanding, text/image document understanding), and (iv) heterogeneous data (e.g., data retrieval and modality alignment in data lakes). The technical evolution further distills four key design goals for intelligent data analysis agents, namely semantic-aware design, autonomous pipelines, tool-augmented workflows, and support for open-world tasks. Finally, we outline the remaining challenges and propose several insights and practical directions for advancing LLM/Agent-powered data analysis.", "summary_bullets": ["Large language models (LLMs) and agent techniques have brought a fundamental shift in the functionality and development paradigm of data analysis tasks (a.k.a LLM/Agent-as-Data-Analyst), demonstrating substantial impact across both academia and industry.", "In comparison with traditional rule or small-model based approaches, (agentic) LLMs enable complex data understanding, natural language interfaces, semantic analysis functions, and autonomous pipeline orchestration.", "From a modality perspective, we review LLM-based techniques for (i) structured data (e.g., NL2SQL, NL2GQL, ModelQA), (ii) semi-structured data (e.g., markup languages understanding, semi-structured table question answering), (iii) unstructured data (e.g., chart understanding, text/image document understanding), and (iv) heterogeneous data (e.g., data retrieval and modality alignment in data lakes)."], "method": "Large language models (LLMs) and agent techniques have brought a fundamental shift in the functionality and development paradigm of data analysis tasks (a.k.a LLM/Agent-as-Data-Analyst), demonstrating substantial impact across both academia and industry.", "key_results": ["Finally, we outline the remaining challenges and propose several insights and practical directions for advancing LLM/Agent-powered data analysis."], "limitations": ["Abstract-level evidence only: validate assumptions, evaluation protocol, and failure cases in the full paper before relying on this as key evidence."], "bibkey": "Tang2025Agent"}
{"paper_id": "P0074", "title": "Large Language Model Agent: A Survey on Methodology, Applications and Challenges", "year": 2025, "url": "http://arxiv.org/abs/2503.21460v1", "arxiv_id": "2503.21460v1", "primary_category": "cs.CL", "categories": ["cs.CL"], "pdf_url": "https://arxiv.org/pdf/2503.21460v1", "priority": "normal", "mapped_sections": [], "evidence_level": "abstract", "fulltext_path": "", "authors": ["Junyu Luo", "Weizhi Zhang", "Ye Yuan", "Yusheng Zhao", "Junwei Yang", "Yiyang Gu", "Bohan Wu", "Binqi Chen", "Ziyue Qiao", "Qingqing Long", "Rongcheng Tu", "Xiao Luo", "Wei Ju", "Zhiping Xiao", "Yifan Wang", "Meng Xiao", "Chenwu Liu", "Jingyang Yuan", "Shichang Zhang", "Yiqiao Jin", "Fan Zhang", "Xian Wu", "Hanqing Zhao", "Dacheng Tao", "Philip S. Yu", "Ming Zhang"], "abstract": "The era of intelligent agents is upon us, driven by revolutionary advancements in large language models. Large Language Model (LLM) agents, with goal-driven behaviors and dynamic adaptation capabilities, potentially represent a critical pathway toward artificial general intelligence. This survey systematically deconstructs LLM agent systems through a methodology-centered taxonomy, linking architectural foundations, collaboration mechanisms, and evolutionary pathways. We unify fragmented research threads by revealing fundamental connections between agent design principles and their emergent behaviors in complex environments. Our work provides a unified architectural perspective, examining how agents are constructed, how they collaborate, and how they evolve over time, while also addressing evaluation methodologies, tool applications, practical challenges, and diverse application domains. By surveying the latest developments in this rapidly evolving field, we offer researchers a structured taxonomy for understanding LLM agents and identify promising directions for future research. The collection is available at https://github.com/luo-junyu/Awesome-Agent-Papers.", "summary_bullets": ["The era of intelligent agents is upon us, driven by revolutionary advancements in large language models.", "Large Language Model (LLM) agents, with goal-driven behaviors and dynamic adaptation capabilities, potentially represent a critical pathway toward artificial general intelligence.", "This survey systematically deconstructs LLM agent systems through a methodology-centered taxonomy, linking architectural foundations, collaboration mechanisms, and evolutionary pathways."], "method": "The era of intelligent agents is upon us, driven by revolutionary advancements in large language models.", "key_results": ["Our work provides a unified architectural perspective, examining how agents are constructed, how they collaborate, and how they evolve over time, while also addressing evaluation methodologies, tool applications, practical challenges, and diverse application domains."], "limitations": ["Abstract-level evidence only: validate assumptions, evaluation protocol, and failure cases in the full paper before relying on this as key evidence."], "bibkey": "Luo2025Large"}
{"paper_id": "P0075", "title": "Large Language Model Powered Intelligent Urban Agents: Concepts, Capabilities, and Applications", "year": 2025, "url": "http://arxiv.org/abs/2507.00914v1", "arxiv_id": "2507.00914v1", "primary_category": "cs.MA", "categories": ["cs.MA", "cs.AI"], "pdf_url": "https://arxiv.org/pdf/2507.00914v1", "priority": "normal", "mapped_sections": ["6.2"], "evidence_level": "abstract", "fulltext_path": "", "authors": ["Jindong Han", "Yansong Ning", "Zirui Yuan", "Hang Ni", "Fan Liu", "Tengfei Lyu", "Hao Liu"], "abstract": "The long-standing vision of intelligent cities is to create efficient, livable, and sustainable urban environments using big data and artificial intelligence technologies. Recently, the advent of Large Language Models (LLMs) has opened new ways toward realizing this vision. With powerful semantic understanding and reasoning capabilities, LLMs can be deployed as intelligent agents capable of autonomously solving complex problems across domains. In this article, we focus on Urban LLM Agents, which are LLM-powered agents that are semi-embodied within the hybrid cyber-physical-social space of cities and used for system-level urban decision-making. First, we introduce the concept of urban LLM agents, discussing their unique capabilities and features. Second, we survey the current research landscape from the perspective of agent workflows, encompassing urban sensing, memory management, reasoning, execution, and learning. Third, we categorize the application domains of urban LLM agents into five groups: urban planning, transportation, environment, public safety, and urban society, presenting representative works in each group. Finally, we discuss trustworthiness and evaluation issues that are critical for real-world deployment, and identify several open problems for future research. This survey aims to establish a foundation for the emerging field of urban LLM agents and to provide a roadmap for advancing the intersection of LLMs and urban intelligence. A curated list of relevant papers and open-source resources is maintained and continuously updated at https://github.com/usail-hkust/Awesome-Urban-LLM-Agents.", "summary_bullets": ["The long-standing vision of intelligent cities is to create efficient, livable, and sustainable urban environments using big data and artificial intelligence technologies.", "Recently, the advent of Large Language Models (LLMs) has opened new ways toward realizing this vision.", "With powerful semantic understanding and reasoning capabilities, LLMs can be deployed as intelligent agents capable of autonomously solving complex problems across domains."], "method": "First, we introduce the concept of urban LLM agents, discussing their unique capabilities and features.", "key_results": ["Finally, we discuss trustworthiness and evaluation issues that are critical for real-world deployment, and identify several open problems for future research."], "limitations": ["Abstract-level evidence only: validate assumptions, evaluation protocol, and failure cases in the full paper before relying on this as key evidence.", "Finally, we discuss trustworthiness and evaluation issues that are critical for real-world deployment, and identify several open problems for future research."], "bibkey": "Han2025Large"}
{"paper_id": "P0076", "title": "Large Language Model enabled Mathematical Modeling", "year": 2025, "url": "http://arxiv.org/abs/2510.19895v1", "arxiv_id": "2510.19895v1", "primary_category": "cs.CL", "categories": ["cs.CL", "cs.AI"], "pdf_url": "https://arxiv.org/pdf/2510.19895v1", "priority": "normal", "mapped_sections": [], "evidence_level": "abstract", "fulltext_path": "", "authors": ["Guoyun Zhang"], "abstract": "The integration of Large Language Models (LLMs) with optimization modeling offers a promising avenue for advancing decision-making in operations research (OR). Traditional optimization methods,such as linear programming, mixed integer programming, and simulation depend heavily on domain expertise to translate real-world problems into solvable mathematical models. While solvers like Gurobi and COPT are powerful, expert input remains essential for defining objectives, constraints, and variables. This research investigates the potential of LLMs, specifically the DeepSeek-R1 model, to bridge this formulation gap using natural language understanding and code generation. Although prior models like GPT-4, Claude, and Bard have shown strong performance in NLP and reasoning tasks, their high token costs and tendency toward hallucinations limit real-world applicability in supply chain contexts. In contrast, DeepSeek-R1, a cost-efficient and high-performing model trained with reinforcement learning, presents a viable alternative. Despite its success in benchmarks such as LiveCodeBench and Math-500, its effectiveness in applied OR scenarios remains under explored. This study systematically evaluates DeepSeek-R1 across four key OR benchmarks: NL4OPT, IndustryOR, EasyLP, and ComplexOR. Our methodology includes baseline assessments, the development of a hallucination taxonomy, and the application of mitigation strategies like LLM-as-a-Judge, Few-shot Learning (FSL), Tool Calling, and a Multi-agent Framework. These techniques aim to reduce hallucinations, enhance formulation accuracy, and better align model outputs with user intent.", "summary_bullets": ["The integration of Large Language Models (LLMs) with optimization modeling offers a promising avenue for advancing decision-making in operations research (OR).", "Traditional optimization methods,such as linear programming, mixed integer programming, and simulation depend heavily on domain expertise to translate real-world problems into solvable mathematical models.", "While solvers like Gurobi and COPT are powerful, expert input remains essential for defining objectives, constraints, and variables."], "method": "The integration of Large Language Models (LLMs) with optimization modeling offers a promising avenue for advancing decision-making in operations research (OR).", "key_results": ["Despite its success in benchmarks such as LiveCodeBench and Math-500, its effectiveness in applied OR scenarios remains under explored.", "Although prior models like GPT-4, Claude, and Bard have shown strong performance in NLP and reasoning tasks, their high token costs and tendency toward hallucinations limit real-world applicability in supply chain contexts."], "limitations": ["Abstract-level evidence only: validate assumptions, evaluation protocol, and failure cases in the full paper before relying on this as key evidence."], "bibkey": "Zhang2025Large"}
{"paper_id": "P0077", "title": "Large Language Models for Planning: A Comprehensive and Systematic Survey", "year": 2025, "url": "http://arxiv.org/abs/2505.19683v1", "arxiv_id": "2505.19683v1", "primary_category": "cs.AI", "categories": ["cs.AI", "cs.CL"], "pdf_url": "https://arxiv.org/pdf/2505.19683v1", "priority": "normal", "mapped_sections": ["4.1"], "evidence_level": "abstract", "fulltext_path": "", "authors": ["Pengfei Cao", "Tianyi Men", "Wencan Liu", "Jingwen Zhang", "Xuzhao Li", "Xixun Lin", "Dianbo Sui", "Yanan Cao", "Kang Liu", "Jun Zhao"], "abstract": "Planning represents a fundamental capability of intelligent agents, requiring comprehensive environmental understanding, rigorous logical reasoning, and effective sequential decision-making. While Large Language Models (LLMs) have demonstrated remarkable performance on certain planning tasks, their broader application in this domain warrants systematic investigation. This paper presents a comprehensive review of LLM-based planning. Specifically, this survey is structured as follows: First, we establish the theoretical foundations by introducing essential definitions and categories about automated planning. Next, we provide a detailed taxonomy and analysis of contemporary LLM-based planning methodologies, categorizing them into three principal approaches: 1) External Module Augmented Methods that combine LLMs with additional components for planning, 2) Finetuning-based Methods that involve using trajectory data and feedback signals to adjust LLMs in order to improve their planning abilities, and 3) Searching-based Methods that break down complex tasks into simpler components, navigate the planning space, or enhance decoding strategies to find the best solutions. Subsequently, we systematically summarize existing evaluation frameworks, including benchmark datasets, evaluation metrics and performance comparisons between representative planning methods. Finally, we discuss the underlying mechanisms enabling LLM-based planning and outline promising research directions for this rapidly evolving field. We hope this survey will serve as a valuable resource to inspire innovation and drive progress in this field.", "summary_bullets": ["Planning represents a fundamental capability of intelligent agents, requiring comprehensive environmental understanding, rigorous logical reasoning, and effective sequential decision-making.", "While Large Language Models (LLMs) have demonstrated remarkable performance on certain planning tasks, their broader application in this domain warrants systematic investigation.", "This paper presents a comprehensive review of LLM-based planning."], "method": "Planning represents a fundamental capability of intelligent agents, requiring comprehensive environmental understanding, rigorous logical reasoning, and effective sequential decision-making.", "key_results": ["Next, we provide a detailed taxonomy and analysis of contemporary LLM-based planning methodologies, categorizing them into three principal approaches: 1) External Module Augmented Methods that combine LLMs with additional components for planning, 2) Finetuning-based Methods that involve using trajectory data and feedback signals to adjust LLMs in order to improve their planning abilities, and 3) Searching-based Methods that break down complex tasks into simpler components, navigate the planning space, or enhance decoding strategies to find the best solutions.", "Subsequently, we systematically summarize existing evaluation frameworks, including benchmark datasets, evaluation metrics and performance comparisons between representative planning methods."], "limitations": ["Abstract-level evidence only: validate assumptions, evaluation protocol, and failure cases in the full paper before relying on this as key evidence."], "bibkey": "Cao2025Large"}
{"paper_id": "P0078", "title": "LatteReview: A Multi-Agent Framework for Systematic Review Automation Using Large Language Models", "year": 2025, "url": "http://arxiv.org/abs/2501.05468v2", "arxiv_id": "2501.05468v2", "primary_category": "cs.CL", "categories": ["cs.CL"], "pdf_url": "https://arxiv.org/pdf/2501.05468v2", "priority": "normal", "mapped_sections": ["5.2"], "evidence_level": "abstract", "fulltext_path": "", "authors": ["Pouria Rouzrokh", "Bardia Khosravi", "Parsa Rouzrokh", "Moein Shariatnia"], "abstract": "Systematic literature reviews and meta-analyses are essential for synthesizing research insights, but they remain time-intensive and labor-intensive due to the iterative processes of screening, evaluation, and data extraction. This paper introduces and evaluates LatteReview, a Python-based framework that leverages large language models (LLMs) and multi-agent systems to automate key elements of the systematic review process. Designed to streamline workflows while maintaining rigor, LatteReview utilizes modular agents for tasks such as title and abstract screening, relevance scoring, and structured data extraction. These agents operate within orchestrated workflows, supporting sequential and parallel review rounds, dynamic decision-making, and iterative refinement based on user feedback. LatteReview's architecture integrates LLM providers, enabling compatibility with both cloud-based and locally hosted models. The framework supports features such as Retrieval-Augmented Generation (RAG) for incorporating external context, multimodal reviews, Pydantic-based validation for structured inputs and outputs, and asynchronous programming for handling large-scale datasets. The framework is available on the GitHub repository, with detailed documentation and an installable package.", "summary_bullets": ["Systematic literature reviews and meta-analyses are essential for synthesizing research insights, but they remain time-intensive and labor-intensive due to the iterative processes of screening, evaluation, and data extraction.", "This paper introduces and evaluates LatteReview, a Python-based framework that leverages large language models (LLMs) and multi-agent systems to automate key elements of the systematic review process.", "Designed to streamline workflows while maintaining rigor, LatteReview utilizes modular agents for tasks such as title and abstract screening, relevance scoring, and structured data extraction."], "method": "Systematic literature reviews and meta-analyses are essential for synthesizing research insights, but they remain time-intensive and labor-intensive due to the iterative processes of screening, evaluation, and data extraction.", "key_results": ["Systematic literature reviews and meta-analyses are essential for synthesizing research insights, but they remain time-intensive and labor-intensive due to the iterative processes of screening, evaluation, and data extraction.", "The framework supports features such as Retrieval-Augmented Generation (RAG) for incorporating external context, multimodal reviews, Pydantic-based validation for structured inputs and outputs, and asynchronous programming for handling large-scale datasets."], "limitations": ["Abstract-level evidence only: validate assumptions, evaluation protocol, and failure cases in the full paper before relying on this as key evidence."], "bibkey": "Rouzrokh2025Lattereview"}
{"paper_id": "P0079", "title": "Learn as Individuals, Evolve as a Team: Multi-agent LLMs Adaptation in Embodied Environments", "year": 2025, "url": "http://arxiv.org/abs/2506.07232v1", "arxiv_id": "2506.07232v1", "primary_category": "cs.MA", "categories": ["cs.MA", "cs.AI", "cs.LG"], "pdf_url": "https://arxiv.org/pdf/2506.07232v1", "priority": "normal", "mapped_sections": ["5.1", "5.2"], "evidence_level": "abstract", "fulltext_path": "", "authors": ["Xinran Li", "Chenjia Bai", "Zijian Li", "Jiakun Zheng", "Ting Xiao", "Jun Zhang"], "abstract": "Large language models (LLMs) possess extensive knowledge bases and strong reasoning capabilities, making them promising tools for complex, multi-agent planning in embodied environments. However, despite LLMs' advanced abilities and the sophisticated modular design of agentic methods, existing LLM-based planning algorithms remain limited by weak adaptation capabilities to multi-agent embodied scenarios. We address this limitation by introducing a framework that enables LLM agents to learn and evolve both before and during test time, equipping them with environment-relevant knowledge for better planning and enhanced communication for improved cooperation. Inspired by centralized training with decentralized execution in multi-agent reinforcement learning, we propose a \\textit{Learn as Individuals, Evolve as a Team (LIET)} paradigm for multi-agent LLMs adaptation. At the individual level, LLM agents learn a local utility function from exploratory datasets to better comprehend the embodied environment, which is then queried during test time to support informed decision-making. At the team level, LLM agents collaboratively and iteratively maintain and update a shared cooperation knowledge list based on new experiences, using it to guide more effective communication. By combining individual learning with team evolution, LIET enables comprehensive and flexible adaptation for LLM agents. Our experiments on Communicative Watch-And-Help and ThreeD-World Multi-Agent Transport benchmarks demonstrate that LIET, instantiated with both LLaMA and GPT-4o, outperforms existing baselines and exhibits strong cooperative planning abilities.", "summary_bullets": ["Large language models (LLMs) possess extensive knowledge bases and strong reasoning capabilities, making them promising tools for complex, multi-agent planning in embodied environments.", "However, despite LLMs' advanced abilities and the sophisticated modular design of agentic methods, existing LLM-based planning algorithms remain limited by weak adaptation capabilities to multi-agent embodied scenarios.", "We address this limitation by introducing a framework that enables LLM agents to learn and evolve both before and during test time, equipping them with environment-relevant knowledge for better planning and enhanced communication for improved cooperation."], "method": "Inspired by centralized training with decentralized execution in multi-agent reinforcement learning, we propose a \\textit{Learn as Individuals, Evolve as a Team (LIET)} paradigm for multi-agent LLMs adaptation.", "key_results": ["At the individual level, LLM agents learn a local utility function from exploratory datasets to better comprehend the embodied environment, which is then queried during test time to support informed decision-making.", "Our experiments on Communicative Watch-And-Help and ThreeD-World Multi-Agent Transport benchmarks demonstrate that LIET, instantiated with both LLaMA and GPT-4o, outperforms existing baselines and exhibits strong cooperative planning abilities."], "limitations": ["Abstract-level evidence only: validate assumptions, evaluation protocol, and failure cases in the full paper before relying on this as key evidence.", "We address this limitation by introducing a framework that enables LLM agents to learn and evolve both before and during test time, equipping them with environment-relevant knowledge for better planning and enhanced communication for improved cooperation."], "bibkey": "Li2025Learn"}
{"paper_id": "P0080", "title": "MARVEL: Multi-Agent RTL Vulnerability Extraction using Large Language Models", "year": 2025, "url": "http://arxiv.org/abs/2505.11963v2", "arxiv_id": "2505.11963v2", "primary_category": "cs.CR", "categories": ["cs.CR", "cs.AI"], "pdf_url": "https://arxiv.org/pdf/2505.11963v2", "priority": "normal", "mapped_sections": ["5.2"], "evidence_level": "abstract", "fulltext_path": "", "authors": ["Luca Collini", "Baleegh Ahmad", "Joey Ah-kiow", "Ramesh Karri"], "abstract": "Hardware security verification is a challenging and time-consuming task. For this purpose, design engineers may utilize tools such as formal verification, linters, and functional simulation tests, coupled with analysis and a deep understanding of the hardware design being inspected. Large Language Models (LLMs) have been used to assist during this task, either directly or in conjunction with existing tools. We improve the state of the art by proposing MARVEL, a multi-agent LLM framework for a unified approach to decision-making, tool use, and reasoning. MARVEL mimics the cognitive process of a designer looking for security vulnerabilities in RTL code. It consists of a supervisor agent that devises the security policy of the system-on-chips (SoCs) using its security documentation. It delegates tasks to validate the security policy to individual executor agents. Each executor agent carries out its assigned task using a particular strategy. Each executor agent may use one or more tools to identify potential security bugs in the design and send the results back to the supervisor agent for further analysis and confirmation. MARVEL includes executor agents that leverage formal tools, linters, simulation tests, LLM-based detection schemes, and static analysis-based checks. We test our approach on a known buggy SoC based on OpenTitan from the Hack@DATE competition. We find that 20 of the 48 issues reported by MARVEL pose security vulnerabilities.", "summary_bullets": ["Hardware security verification is a challenging and time-consuming task.", "For this purpose, design engineers may utilize tools such as formal verification, linters, and functional simulation tests, coupled with analysis and a deep understanding of the hardware design being inspected.", "Large Language Models (LLMs) have been used to assist during this task, either directly or in conjunction with existing tools."], "method": "We test our approach on a known buggy SoC based on OpenTitan from the Hack@DATE competition.", "key_results": ["We find that 20 of the 48 issues reported by MARVEL pose security vulnerabilities.", "We improve the state of the art by proposing MARVEL, a multi-agent LLM framework for a unified approach to decision-making, tool use, and reasoning."], "limitations": ["Abstract-level evidence only: validate assumptions, evaluation protocol, and failure cases in the full paper before relying on this as key evidence."], "bibkey": "Collini2025Marvel"}
{"paper_id": "P0081", "title": "MCP-RADAR: A Multi-Dimensional Benchmark for Evaluating Tool Use Capabilities in Large Language Models", "year": 2025, "url": "http://arxiv.org/abs/2505.16700v2", "arxiv_id": "2505.16700v2", "primary_category": "cs.AI", "categories": ["cs.AI"], "pdf_url": "https://arxiv.org/pdf/2505.16700v2", "priority": "normal", "mapped_sections": ["3.2", "6.2"], "evidence_level": "abstract", "fulltext_path": "", "authors": ["Xuanqi Gao", "Siyi Xie", "Juan Zhai", "Shiqing Ma", "Chao Shen"], "abstract": "As Large Language Models (LLMs) evolve from passive text generators to active reasoning agents capable of interacting with external tools, the Model Context Protocol (MCP) has emerged as a key standardized framework for dynamic tool discovery and orchestration. Despite its widespread industry adoption, existing evaluation methods do not adequately assess tool utilization capabilities under this new paradigm. To address this gap, this paper introduces MCP-RADAR, the first comprehensive benchmark specifically designed to evaluate LLM performance within the MCP framework. MCP-RADAR features a challenging dataset of 507 tasks spanning six domains: mathematical reasoning, web search, email, calendar, file management, and terminal operations. It quantifies performance based on two primary criteria: answer correctness and operational accuracy. To closely emulate real-world usage, our evaluation employs both authentic MCP tools and high-fidelity simulations of official tools. Unlike traditional benchmarks that rely on subjective human evaluation or binary success metrics, MCP-RADAR adopts objective, quantifiable measurements across multiple task domains, including computational resource efficiency and the number of successful tool-invocation rounds. Our evaluation of leading closed-source and open-source LLMs reveals distinct capability profiles and highlights a significant trade-off between accuracy and efficiency. Our findings provide actionable insights for both LLM developers and tool creators, establishing a standardized methodology applicable to the broader LLM agent ecosystem. All implementations, configurations, and datasets are publicly available at https://anonymous.4open.science/r/MCPRadar-B143.", "summary_bullets": ["As Large Language Models (LLMs) evolve from passive text generators to active reasoning agents capable of interacting with external tools, the Model Context Protocol (MCP) has emerged as a key standardized framework for dynamic tool discovery and orchestration.", "Despite its widespread industry adoption, existing evaluation methods do not adequately assess tool utilization capabilities under this new paradigm.", "To address this gap, this paper introduces MCP-RADAR, the first comprehensive benchmark specifically designed to evaluate LLM performance within the MCP framework."], "method": "As Large Language Models (LLMs) evolve from passive text generators to active reasoning agents capable of interacting with external tools, the Model Context Protocol (MCP) has emerged as a key standardized framework for dynamic tool discovery and orchestration.", "key_results": ["MCP-RADAR features a challenging dataset of 507 tasks spanning six domains: mathematical reasoning, web search, email, calendar, file management, and terminal operations.", "Despite its widespread industry adoption, existing evaluation methods do not adequately assess tool utilization capabilities under this new paradigm."], "limitations": ["Abstract-level evidence only: validate assumptions, evaluation protocol, and failure cases in the full paper before relying on this as key evidence."], "bibkey": "Gao2025Radar"}
{"paper_id": "P0082", "title": "Measuring temporal effects of agent knowledge by date-controlled tool use", "year": 2025, "url": "http://arxiv.org/abs/2503.04188v2", "arxiv_id": "2503.04188v2", "primary_category": "cs.CL", "categories": ["cs.CL", "cs.IR"], "pdf_url": "https://arxiv.org/pdf/2503.04188v2", "priority": "normal", "mapped_sections": [], "evidence_level": "abstract", "fulltext_path": "", "authors": ["R. Patrick Xian", "Qiming Cui", "Stefan Bauer", "Reza Abbasi-Asl"], "abstract": "Temporal progression is an integral part of knowledge accumulation and update. Web search is frequently adopted as grounding for agent knowledge, yet an improper configuration affects the quality of the agent's responses. Here, we assess the agent behavior using distinct date-controlled tools (DCTs) as stress test to measure the knowledge variability of large language model (LLM) agents. We demonstrate the temporal effects of an LLM agent as a writing assistant, which uses web search to complete scientific publication abstracts. We show that the temporality of search engine translates into tool-dependent agent performance but can be alleviated with base model choice and explicit reasoning instructions such as chain-of-thought prompting. Our results indicate that agent design and evaluations should take a dynamical view and implement measures to account for the temporal influence of external resources to ensure reliability.", "summary_bullets": ["Temporal progression is an integral part of knowledge accumulation and update.", "Web search is frequently adopted as grounding for agent knowledge, yet an improper configuration affects the quality of the agent's responses.", "Here, we assess the agent behavior using distinct date-controlled tools (DCTs) as stress test to measure the knowledge variability of large language model (LLM) agents."], "method": "Temporal progression is an integral part of knowledge accumulation and update.", "key_results": ["Our results indicate that agent design and evaluations should take a dynamical view and implement measures to account for the temporal influence of external resources to ensure reliability."], "limitations": ["Abstract-level evidence only: validate assumptions, evaluation protocol, and failure cases in the full paper before relying on this as key evidence."], "bibkey": "Xian2025Measuring"}
{"paper_id": "P0083", "title": "MemTool: Optimizing Short-Term Memory Management for Dynamic Tool Calling in LLM Agent Multi-Turn Conversations", "year": 2025, "url": "http://arxiv.org/abs/2507.21428v1", "arxiv_id": "2507.21428v1", "primary_category": "cs.CL", "categories": ["cs.CL"], "pdf_url": "https://arxiv.org/pdf/2507.21428v1", "priority": "normal", "mapped_sections": ["3.2"], "evidence_level": "abstract", "fulltext_path": "", "authors": ["Elias Lumer", "Anmol Gulati", "Vamse Kumar Subbiah", "Pradeep Honaganahalli Basavaraju", "James A. Burke"], "abstract": "Large Language Model (LLM) agents have shown significant autonomous capabilities in dynamically searching and incorporating relevant tools or Model Context Protocol (MCP) servers for individual queries. However, fixed context windows limit effectiveness in multi-turn interactions requiring repeated, independent tool usage. We introduce MemTool, a short-term memory framework enabling LLM agents to dynamically manage tools or MCP server contexts across multi-turn conversations. MemTool offers three agentic architectures: 1) Autonomous Agent Mode, granting full tool management autonomy, 2) Workflow Mode, providing deterministic control without autonomy, and 3) Hybrid Mode, combining autonomous and deterministic control. Evaluating each MemTool mode across 13+ LLMs on the ScaleMCP benchmark, we conducted experiments over 100 consecutive user interactions, measuring tool removal ratios (short-term memory efficiency) and task completion accuracy. In Autonomous Agent Mode, reasoning LLMs achieve high tool-removal efficiency (90-94% over a 3-window average), while medium-sized models exhibit significantly lower efficiency (0-60%). Workflow and Hybrid modes consistently manage tool removal effectively, whereas Autonomous and Hybrid modes excel at task completion. We present trade-offs and recommendations for each MemTool mode based on task accuracy, agency, and model capabilities.", "summary_bullets": ["Large Language Model (LLM) agents have shown significant autonomous capabilities in dynamically searching and incorporating relevant tools or Model Context Protocol (MCP) servers for individual queries.", "However, fixed context windows limit effectiveness in multi-turn interactions requiring repeated, independent tool usage.", "We introduce MemTool, a short-term memory framework enabling LLM agents to dynamically manage tools or MCP server contexts across multi-turn conversations."], "method": "We introduce MemTool, a short-term memory framework enabling LLM agents to dynamically manage tools or MCP server contexts across multi-turn conversations.", "key_results": ["Evaluating each MemTool mode across 13+ LLMs on the ScaleMCP benchmark, we conducted experiments over 100 consecutive user interactions, measuring tool removal ratios (short-term memory efficiency) and task completion accuracy.", "MemTool offers three agentic architectures: 1) Autonomous Agent Mode, granting full tool management autonomy, 2) Workflow Mode, providing deterministic control without autonomy, and 3) Hybrid Mode, combining autonomous and deterministic control."], "limitations": ["Abstract-level evidence only: validate assumptions, evaluation protocol, and failure cases in the full paper before relying on this as key evidence."], "bibkey": "Lumer2025Memtool"}
{"paper_id": "P0084", "title": "Meta-Thinking in LLMs via Multi-Agent Reinforcement Learning: A Survey", "year": 2025, "url": "http://arxiv.org/abs/2504.14520v1", "arxiv_id": "2504.14520v1", "primary_category": "cs.AI", "categories": ["cs.AI", "cs.CL"], "pdf_url": "https://arxiv.org/pdf/2504.14520v1", "priority": "normal", "mapped_sections": ["5.1", "5.2"], "evidence_level": "abstract", "fulltext_path": "", "authors": ["Ahsan Bilal", "Muhammad Ahmed Mohsin", "Muhammad Umer", "Muhammad Awais Khan Bangash", "Muhammad Ali Jamshed"], "abstract": "This survey explores the development of meta-thinking capabilities in Large Language Models (LLMs) from a Multi-Agent Reinforcement Learning (MARL) perspective. Meta-thinking self-reflection, assessment, and control of thinking processes is an important next step in enhancing LLM reliability, flexibility, and performance, particularly for complex or high-stakes tasks. The survey begins by analyzing current LLM limitations, such as hallucinations and the lack of internal self-assessment mechanisms. It then talks about newer methods, including RL from human feedback (RLHF), self-distillation, and chain-of-thought prompting, and each of their limitations. The crux of the survey is to talk about how multi-agent architectures, namely supervisor-agent hierarchies, agent debates, and theory of mind frameworks, can emulate human-like introspective behavior and enhance LLM robustness. By exploring reward mechanisms, self-play, and continuous learning methods in MARL, this survey gives a comprehensive roadmap to building introspective, adaptive, and trustworthy LLMs. Evaluation metrics, datasets, and future research avenues, including neuroscience-inspired architectures and hybrid symbolic reasoning, are also discussed.", "summary_bullets": ["This survey explores the development of meta-thinking capabilities in Large Language Models (LLMs) from a Multi-Agent Reinforcement Learning (MARL) perspective.", "Meta-thinking self-reflection, assessment, and control of thinking processes is an important next step in enhancing LLM reliability, flexibility, and performance, particularly for complex or high-stakes tasks.", "The survey begins by analyzing current LLM limitations, such as hallucinations and the lack of internal self-assessment mechanisms."], "method": "This survey explores the development of meta-thinking capabilities in Large Language Models (LLMs) from a Multi-Agent Reinforcement Learning (MARL) perspective.", "key_results": ["It then talks about newer methods, including RL from human feedback (RLHF), self-distillation, and chain-of-thought prompting, and each of their limitations.", "The crux of the survey is to talk about how multi-agent architectures, namely supervisor-agent hierarchies, agent debates, and theory of mind frameworks, can emulate human-like introspective behavior and enhance LLM robustness."], "limitations": ["Abstract-level evidence only: validate assumptions, evaluation protocol, and failure cases in the full paper before relying on this as key evidence.", "The survey begins by analyzing current LLM limitations, such as hallucinations and the lack of internal self-assessment mechanisms."], "bibkey": "Bilal2025Meta"}
{"paper_id": "P0085", "title": "Multi-Agent Autonomous Driving Systems with Large Language Models: A Survey of Recent Advances", "year": 2025, "url": "http://arxiv.org/abs/2502.16804v2", "arxiv_id": "2502.16804v2", "primary_category": "cs.MA", "categories": ["cs.MA", "cs.AI"], "pdf_url": "https://arxiv.org/pdf/2502.16804v2", "priority": "normal", "mapped_sections": ["5.2"], "evidence_level": "abstract", "fulltext_path": "", "authors": ["Yaozu Wu", "Dongyuan Li", "Yankai Chen", "Renhe Jiang", "Henry Peng Zou", "Wei-Chieh Huang", "Yangning Li", "Liancheng Fang", "Zhen Wang", "Philip S. Yu"], "abstract": "Autonomous Driving Systems (ADSs) are revolutionizing transportation by reducing human intervention, improving operational efficiency, and enhancing safety. Large Language Models (LLMs) have been integrated into ADSs to support high-level decision-making through their powerful reasoning, instruction-following, and communication abilities. However, LLM-based single-agent ADSs face three major challenges: limited perception, insufficient collaboration, and high computational demands. To address these issues, recent advances in LLM-based multi-agent ADSs leverage language-driven communication and coordination to enhance inter-agent collaboration. This paper provides a frontier survey of this emerging intersection between NLP and multi-agent ADSs. We begin with a background introduction to related concepts, followed by a categorization of existing LLM-based methods based on different agent interaction modes. We then discuss agent-human interactions in scenarios where LLM-based agents engage with humans. Finally, we summarize key applications, datasets, and challenges to support future research.", "summary_bullets": ["Autonomous Driving Systems (ADSs) are revolutionizing transportation by reducing human intervention, improving operational efficiency, and enhancing safety.", "Large Language Models (LLMs) have been integrated into ADSs to support high-level decision-making through their powerful reasoning, instruction-following, and communication abilities.", "However, LLM-based single-agent ADSs face three major challenges: limited perception, insufficient collaboration, and high computational demands."], "method": "Autonomous Driving Systems (ADSs) are revolutionizing transportation by reducing human intervention, improving operational efficiency, and enhancing safety.", "key_results": ["Autonomous Driving Systems (ADSs) are revolutionizing transportation by reducing human intervention, improving operational efficiency, and enhancing safety.", "We then discuss agent-human interactions in scenarios where LLM-based agents engage with humans."], "limitations": ["Abstract-level evidence only: validate assumptions, evaluation protocol, and failure cases in the full paper before relying on this as key evidence."], "bibkey": "Wu2025Multi"}
{"paper_id": "P0086", "title": "Progent: Programmable Privilege Control for LLM Agents", "year": 2025, "url": "http://arxiv.org/abs/2504.11703v2", "arxiv_id": "2504.11703v2", "primary_category": "cs.CR", "categories": ["cs.CR", "cs.AI"], "pdf_url": "https://arxiv.org/pdf/2504.11703v2", "priority": "normal", "mapped_sections": ["6.1", "6.2"], "evidence_level": "abstract", "fulltext_path": "", "authors": ["Tianneng Shi", "Jingxuan He", "Zhun Wang", "Hongwei Li", "Linyu Wu", "Wenbo Guo", "Dawn Song"], "abstract": "LLM agents utilize Large Language Models as central components with diverse tools to complete various user tasks, but face significant security risks when interacting with external environments. Attackers can exploit these agents through various vectors, including indirect prompt injection, memory/knowledge base poisoning, and malicious tools, tricking agents into performing dangerous actions such as unauthorized financial transactions or data leakage. The core problem that enables attacks to succeed lies in over-privileged tool access. We introduce Progent, the first privilege control framework to secure LLM agents. Progent enforces security at the tool level by restricting agents to performing tool calls necessary for user tasks while blocking potentially malicious ones. Progent features a domain-specific language that allows for expressing fine-grained policies for controlling tool privileges, flexible fallback actions when calls are blocked, and dynamic policy updates to adapt to changing agent states. The framework operates deterministically at runtime, providing provable security guarantees. Thanks to our modular design, integrating Progent does not alter agent internals and only requires minimal changes to the existing agent implementation, enhancing its practicality and potential for widespread adoption. Our extensive evaluation across various agent use cases, using benchmarks like AgentDojo, ASB, and AgentPoison, demonstrates that Progent reduces attack success rates to 0%, while preserving agent utility and speed. Additionally, we show that LLMs can automatically generate effective policies, highlighting their potential for automating the process of writing Progent's security policies.", "summary_bullets": ["LLM agents utilize Large Language Models as central components with diverse tools to complete various user tasks, but face significant security risks when interacting with external environments.", "Attackers can exploit these agents through various vectors, including indirect prompt injection, memory/knowledge base poisoning, and malicious tools, tricking agents into performing dangerous actions such as unauthorized financial transactions or data leakage.", "The core problem that enables attacks to succeed lies in over-privileged tool access."], "method": "We introduce Progent, the first privilege control framework to secure LLM agents.", "key_results": ["Our extensive evaluation across various agent use cases, using benchmarks like AgentDojo, ASB, and AgentPoison, demonstrates that Progent reduces attack success rates to 0%, while preserving agent utility and speed."], "limitations": ["Abstract-level evidence only: validate assumptions, evaluation protocol, and failure cases in the full paper before relying on this as key evidence.", "Thanks to our modular design, integrating Progent does not alter agent internals and only requires minimal changes to the existing agent implementation, enhancing its practicality and potential for widespread adoption."], "bibkey": "Shi2025Progent"}
{"paper_id": "P0087", "title": "SAND: Boosting LLM Agents with Self-Taught Action Deliberation", "year": 2025, "url": "http://arxiv.org/abs/2507.07441v2", "arxiv_id": "2507.07441v2", "primary_category": "cs.CL", "categories": ["cs.CL"], "pdf_url": "https://arxiv.org/pdf/2507.07441v2", "priority": "normal", "mapped_sections": ["5.1"], "evidence_level": "abstract", "fulltext_path": "", "authors": ["Yu Xia", "Yiran Shen", "Junda Wu", "Tong Yu", "Sungchul Kim", "Ryan A. Rossi", "Lina Yao", "Julian McAuley"], "abstract": "Large Language Model (LLM) agents are commonly tuned with supervised finetuning on ReAct-style expert trajectories or preference optimization over pairwise rollouts. Most of these methods focus on imitating specific expert behaviors or promoting chosen reasoning thoughts and actions over rejected ones. However, without reasoning and comparing over alternatives actions, LLM agents finetuned with these methods may over-commit towards seemingly plausible but suboptimal actions due to limited action space exploration. To address this, in this paper we propose Self-taught ActioN Deliberation (SAND) framework, enabling LLM agents to explicitly deliberate over candidate actions before committing to one. To tackle the challenges of when and what to deliberate given large action space and step-level action evaluation, we incorporate self-consistency action sampling and execution-guided action critique to help synthesize step-wise action deliberation thoughts using the base model of the LLM agent. In an iterative manner, the deliberation trajectories are then used to finetune the LLM agent itself. Evaluating on two representative interactive agent tasks, SAND achieves an average 20% improvement over initial supervised finetuning and also outperforms state-of-the-art agent tuning approaches.", "summary_bullets": ["Large Language Model (LLM) agents are commonly tuned with supervised finetuning on ReAct-style expert trajectories or preference optimization over pairwise rollouts.", "Most of these methods focus on imitating specific expert behaviors or promoting chosen reasoning thoughts and actions over rejected ones.", "However, without reasoning and comparing over alternatives actions, LLM agents finetuned with these methods may over-commit towards seemingly plausible but suboptimal actions due to limited action space exploration."], "method": "To address this, in this paper we propose Self-taught ActioN Deliberation (SAND) framework, enabling LLM agents to explicitly deliberate over candidate actions before committing to one.", "key_results": ["Evaluating on two representative interactive agent tasks, SAND achieves an average 20% improvement over initial supervised finetuning and also outperforms state-of-the-art agent tuning approaches.", "To tackle the challenges of when and what to deliberate given large action space and step-level action evaluation, we incorporate self-consistency action sampling and execution-guided action critique to help synthesize step-wise action deliberation thoughts using the base model of the LLM agent."], "limitations": ["Abstract-level evidence only: validate assumptions, evaluation protocol, and failure cases in the full paper before relying on this as key evidence."], "bibkey": "Xia2025Sand"}
{"paper_id": "P0088", "title": "Search-on-Graph: Iterative Informed Navigation for Large Language Model Reasoning on Knowledge Graphs", "year": 2025, "url": "http://arxiv.org/abs/2510.08825v1", "arxiv_id": "2510.08825v1", "primary_category": "cs.CL", "categories": ["cs.CL"], "pdf_url": "https://arxiv.org/pdf/2510.08825v1", "priority": "normal", "mapped_sections": ["4.1"], "evidence_level": "abstract", "fulltext_path": "", "authors": ["Jia Ao Sun", "Hao Yu", "Fabrizio Gotti", "Fengran Mo", "Yihong Wu", "Yuchen Hui", "Jian-Yun Nie"], "abstract": "Large language models (LLMs) have demonstrated impressive reasoning abilities yet remain unreliable on knowledge-intensive, multi-hop questions -- they miss long-tail facts, hallucinate when uncertain, and their internal knowledge lags behind real-world change. Knowledge graphs (KGs) offer a structured source of relational evidence, but existing KGQA methods face fundamental trade-offs: compiling complete SPARQL queries without knowing available relations proves brittle, retrieving large subgraphs introduces noise, and complex agent frameworks with parallel exploration exponentially expand search spaces. To address these limitations, we propose Search-on-Graph (SoG), a simple yet effective framework that enables LLMs to perform iterative informed graph navigation using a single, carefully designed \\textsc{Search} function. Rather than pre-planning paths or retrieving large subgraphs, SoG follows an ``observe-then-navigate'' principle: at each step, the LLM examines actual available relations from the current entity before deciding on the next hop. This approach further adapts seamlessly to different KG schemas and handles high-degree nodes through adaptive filtering. Across six KGQA benchmarks spanning Freebase and Wikidata, SoG achieves state-of-the-art performance without fine-tuning. We demonstrate particularly strong gains on Wikidata benchmarks (+16\\% improvement over previous best methods) alongside consistent improvements on Freebase benchmarks.", "summary_bullets": ["Large language models (LLMs) have demonstrated impressive reasoning abilities yet remain unreliable on knowledge-intensive, multi-hop questions -- they miss long-tail facts, hallucinate when uncertain, and their internal knowledge lags behind real-world change.", "Knowledge graphs (KGs) offer a structured source of relational evidence, but existing KGQA methods face fundamental trade-offs: compiling complete SPARQL queries without knowing available relations proves brittle, retrieving large subgraphs introduces noise, and complex agent frameworks with parallel exploration exponentially expand search spaces.", "To address these limitations, we propose Search-on-Graph (SoG), a simple yet effective framework that enables LLMs to perform iterative informed graph navigation using a single, carefully designed \\textsc{Search} function."], "method": "To address these limitations, we propose Search-on-Graph (SoG), a simple yet effective framework that enables LLMs to perform iterative informed graph navigation using a single, carefully designed \\textsc{Search} function.", "key_results": ["We demonstrate particularly strong gains on Wikidata benchmarks (+16\\% improvement over previous best methods) alongside consistent improvements on Freebase benchmarks.", "Across six KGQA benchmarks spanning Freebase and Wikidata, SoG achieves state-of-the-art performance without fine-tuning."], "limitations": ["Abstract-level evidence only: validate assumptions, evaluation protocol, and failure cases in the full paper before relying on this as key evidence.", "To address these limitations, we propose Search-on-Graph (SoG), a simple yet effective framework that enables LLMs to perform iterative informed graph navigation using a single, carefully designed \\textsc{Search} function."], "bibkey": "Sun2025Search"}
{"paper_id": "P0089", "title": "Training Task Reasoning LLM Agents for Multi-turn Task Planning via Single-turn Reinforcement Learning", "year": 2025, "url": "http://arxiv.org/abs/2509.20616v2", "arxiv_id": "2509.20616v2", "primary_category": "cs.LG", "categories": ["cs.LG", "eess.SY"], "pdf_url": "https://arxiv.org/pdf/2509.20616v2", "priority": "normal", "mapped_sections": ["4.1"], "evidence_level": "abstract", "fulltext_path": "", "authors": ["Hanjiang Hu", "Changliu Liu", "Na Li", "Yebin Wang"], "abstract": "Large Language Models (LLMs) have demonstrated remarkable capabilities in knowledge acquisition, reasoning, and tool use, making them promising candidates for autonomous agent applications. However, training LLM agents for complex multi-turn task planning faces significant challenges, including sparse episode-wise rewards, credit assignment across long horizons, and the computational overhead of reinforcement learning in multi-turn interaction settings. To this end, this paper introduces a novel approach that transforms multi-turn task planning into single-turn task reasoning problems, enabling efficient policy optimization through Group Relative Policy Optimization (GRPO) with dense and verifiable reward from expert trajectories. Our theoretical analysis shows that GRPO improvement on single-turn task reasoning results in a lower bound of the multi-turn success probability under the minimal turns, as well as the generalization to subtasks with shorter horizons. Experimental evaluation on the complex task planning benchmark demonstrates that our 1.5B parameter model trained with single-turn GRPO achieves superior performance compared to larger baseline models up to 14B parameters, with success rates of 70% for long-horizon planning tasks.", "summary_bullets": ["Large Language Models (LLMs) have demonstrated remarkable capabilities in knowledge acquisition, reasoning, and tool use, making them promising candidates for autonomous agent applications.", "However, training LLM agents for complex multi-turn task planning faces significant challenges, including sparse episode-wise rewards, credit assignment across long horizons, and the computational overhead of reinforcement learning in multi-turn interaction settings.", "To this end, this paper introduces a novel approach that transforms multi-turn task planning into single-turn task reasoning problems, enabling efficient policy optimization through Group Relative Policy Optimization (GRPO) with dense and verifiable reward from expert trajectories."], "method": "Large Language Models (LLMs) have demonstrated remarkable capabilities in knowledge acquisition, reasoning, and tool use, making them promising candidates for autonomous agent applications.", "key_results": ["Experimental evaluation on the complex task planning benchmark demonstrates that our 1.5B parameter model trained with single-turn GRPO achieves superior performance compared to larger baseline models up to 14B parameters, with success rates of 70% for long-horizon planning tasks.", "Our theoretical analysis shows that GRPO improvement on single-turn task reasoning results in a lower bound of the multi-turn success probability under the minimal turns, as well as the generalization to subtasks with shorter horizons."], "limitations": ["Abstract-level evidence only: validate assumptions, evaluation protocol, and failure cases in the full paper before relying on this as key evidence."], "bibkey": "Hu2025Training"}
{"paper_id": "P0090", "title": "A Survey on Large Language Model-Based Social Agents in Game-Theoretic Scenarios", "year": 2024, "url": "http://arxiv.org/abs/2412.03920v2", "arxiv_id": "2412.03920v2", "primary_category": "cs.CL", "categories": ["cs.CL", "cs.AI"], "pdf_url": "https://arxiv.org/pdf/2412.03920v2", "priority": "normal", "mapped_sections": [], "evidence_level": "abstract", "fulltext_path": "", "authors": ["Xiachong Feng", "Longxu Dou", "Ella Li", "Qinghao Wang", "Haochuan Wang", "Yu Guo", "Chang Ma", "Lingpeng Kong"], "abstract": "Game-theoretic scenarios have become pivotal in evaluating the social intelligence of Large Language Model (LLM)-based social agents. While numerous studies have explored these agents in such settings, there is a lack of a comprehensive survey summarizing the current progress. To address this gap, we systematically review existing research on LLM-based social agents within game-theoretic scenarios. Our survey organizes the findings into three core components: Game Framework, Social Agent, and Evaluation Protocol. The game framework encompasses diverse game scenarios, ranging from choice-focusing to communication-focusing games. The social agent part explores agents' preferences, beliefs, and reasoning abilities, as well as their interactions and synergistic effects on decision-making. The evaluation protocol covers both game-agnostic and game-specific metrics for assessing agent performance. Additionally, we analyze the performance of current social agents across various game scenarios. By reflecting on the current research and identifying future research directions, this survey provides insights to advance the development and evaluation of social agents in game-theoretic scenarios.", "summary_bullets": ["Game-theoretic scenarios have become pivotal in evaluating the social intelligence of Large Language Model (LLM)-based social agents.", "While numerous studies have explored these agents in such settings, there is a lack of a comprehensive survey summarizing the current progress.", "To address this gap, we systematically review existing research on LLM-based social agents within game-theoretic scenarios."], "method": "Additionally, we analyze the performance of current social agents across various game scenarios.", "key_results": ["Our survey organizes the findings into three core components: Game Framework, Social Agent, and Evaluation Protocol.", "The evaluation protocol covers both game-agnostic and game-specific metrics for assessing agent performance."], "limitations": ["Abstract-level evidence only: validate assumptions, evaluation protocol, and failure cases in the full paper before relying on this as key evidence."], "bibkey": "Feng2024Survey"}
{"paper_id": "P0091", "title": "A Survey on the Memory Mechanism of Large Language Model based Agents", "year": 2024, "url": "http://arxiv.org/abs/2404.13501v1", "arxiv_id": "2404.13501v1", "primary_category": "cs.AI", "categories": ["cs.AI"], "pdf_url": "https://arxiv.org/pdf/2404.13501v1", "priority": "normal", "mapped_sections": ["4.2"], "evidence_level": "abstract", "fulltext_path": "", "authors": ["Zeyu Zhang", "Xiaohe Bo", "Chen Ma", "Rui Li", "Xu Chen", "Quanyu Dai", "Jieming Zhu", "Zhenhua Dong", "Ji-Rong Wen"], "abstract": "Large language model (LLM) based agents have recently attracted much attention from the research and industry communities. Compared with original LLMs, LLM-based agents are featured in their self-evolving capability, which is the basis for solving real-world problems that need long-term and complex agent-environment interactions. The key component to support agent-environment interactions is the memory of the agents. While previous studies have proposed many promising memory mechanisms, they are scattered in different papers, and there lacks a systematical review to summarize and compare these works from a holistic perspective, failing to abstract common and effective designing patterns for inspiring future studies. To bridge this gap, in this paper, we propose a comprehensive survey on the memory mechanism of LLM-based agents. In specific, we first discuss ''what is'' and ''why do we need'' the memory in LLM-based agents. Then, we systematically review previous studies on how to design and evaluate the memory module. In addition, we also present many agent applications, where the memory module plays an important role. At last, we analyze the limitations of existing work and show important future directions. To keep up with the latest advances in this field, we create a repository at \\url{https://github.com/nuster1128/LLM_Agent_Memory_Survey}.", "summary_bullets": ["Large language model (LLM) based agents have recently attracted much attention from the research and industry communities.", "Compared with original LLMs, LLM-based agents are featured in their self-evolving capability, which is the basis for solving real-world problems that need long-term and complex agent-environment interactions.", "The key component to support agent-environment interactions is the memory of the agents."], "method": "To bridge this gap, in this paper, we propose a comprehensive survey on the memory mechanism of LLM-based agents.", "key_results": ["To keep up with the latest advances in this field, we create a repository at \\url{https://github.com/nuster1128/LLM_Agent_Memory_Survey}."], "limitations": ["Abstract-level evidence only: validate assumptions, evaluation protocol, and failure cases in the full paper before relying on this as key evidence.", "At last, we analyze the limitations of existing work and show important future directions."], "bibkey": "Zhang2024Survey"}
{"paper_id": "P0092", "title": "APT: Architectural Planning and Text-to-Blueprint Construction Using Large Language Models for Open-World Agents", "year": 2024, "url": "http://arxiv.org/abs/2411.17255v2", "arxiv_id": "2411.17255v2", "primary_category": "cs.LG", "categories": ["cs.LG", "cs.AI"], "pdf_url": "https://arxiv.org/pdf/2411.17255v2", "priority": "normal", "mapped_sections": ["4.1"], "evidence_level": "abstract", "fulltext_path": "", "authors": ["Jun Yu Chen", "Tao Gao"], "abstract": "We present APT, an advanced Large Language Model (LLM)-driven framework that enables autonomous agents to construct complex and creative structures within the Minecraft environment. Unlike previous approaches that primarily concentrate on skill-based open-world tasks or rely on image-based diffusion models for generating voxel-based structures, our method leverages the intrinsic spatial reasoning capabilities of LLMs. By employing chain-of-thought decomposition along with multimodal inputs, the framework generates detailed architectural layouts and blueprints that the agent can execute under zero-shot or few-shot learning scenarios. Our agent incorporates both memory and reflection modules to facilitate lifelong learning, adaptive refinement, and error correction throughout the building process. To rigorously evaluate the agent's performance in this emerging research area, we introduce a comprehensive benchmark consisting of diverse construction tasks designed to test creativity, spatial reasoning, adherence to in-game rules, and the effective integration of multimodal instructions. Experimental results using various GPT-based LLM backends and agent configurations demonstrate the agent's capacity to accurately interpret extensive instructions involving numerous items, their positions, and orientations. The agent successfully produces complex structures complete with internal functionalities such as Redstone-powered systems. A/B testing indicates that the inclusion of a memory module leads to a significant increase in performance, emphasizing its role in enabling continuous learning and the reuse of accumulated experience. Additionally, the agent's unexpected emergence of scaffolding behavior highlights the potential of future LLM-driven agents to utilize subroutine planning and leverage the emergence ability of LLMs to autonomously develop human-like problem-solving techniques.", "summary_bullets": ["We present APT, an advanced Large Language Model (LLM)-driven framework that enables autonomous agents to construct complex and creative structures within the Minecraft environment.", "Unlike previous approaches that primarily concentrate on skill-based open-world tasks or rely on image-based diffusion models for generating voxel-based structures, our method leverages the intrinsic spatial reasoning capabilities of LLMs.", "By employing chain-of-thought decomposition along with multimodal inputs, the framework generates detailed architectural layouts and blueprints that the agent can execute under zero-shot or few-shot learning scenarios."], "method": "We present APT, an advanced Large Language Model (LLM)-driven framework that enables autonomous agents to construct complex and creative structures within the Minecraft environment.", "key_results": ["To rigorously evaluate the agent's performance in this emerging research area, we introduce a comprehensive benchmark consisting of diverse construction tasks designed to test creativity, spatial reasoning, adherence to in-game rules, and the effective integration of multimodal instructions.", "Additionally, the agent's unexpected emergence of scaffolding behavior highlights the potential of future LLM-driven agents to utilize subroutine planning and leverage the emergence ability of LLMs to autonomously develop human-like problem-solving techniques."], "limitations": ["Abstract-level evidence only: validate assumptions, evaluation protocol, and failure cases in the full paper before relying on this as key evidence."], "bibkey": "Chen2024Architectural"}
{"paper_id": "P0093", "title": "ArCHer: Training Language Model Agents via Hierarchical Multi-Turn RL", "year": 2024, "url": "http://arxiv.org/abs/2402.19446v1", "arxiv_id": "2402.19446v1", "primary_category": "cs.LG", "categories": ["cs.LG", "cs.AI", "cs.CL"], "pdf_url": "https://arxiv.org/pdf/2402.19446v1", "priority": "normal", "mapped_sections": ["3.2", "5.1"], "evidence_level": "abstract", "fulltext_path": "", "authors": ["Yifei Zhou", "Andrea Zanette", "Jiayi Pan", "Sergey Levine", "Aviral Kumar"], "abstract": "A broad use case of large language models (LLMs) is in goal-directed decision-making tasks (or \"agent\" tasks), where an LLM needs to not just generate completions for a given prompt, but rather make intelligent decisions over a multi-turn interaction to accomplish a task (e.g., when interacting with the web, using tools, or providing customer support). Reinforcement learning (RL) provides a general paradigm to address such agent tasks, but current RL methods for LLMs largely focus on optimizing single-turn rewards. By construction, most single-turn RL methods cannot endow LLMs with the ability to intelligently seek information over multiple turns, perform credit assignment, or reason about their past actions -- all of which are critical in agent tasks. This raises the question: how can we design effective and efficient multi-turn RL algorithms for LLMs? In this paper, we develop a framework for building multi-turn RL algorithms for fine-tuning LLMs, that preserves the flexibility of existing single-turn RL methods for LLMs (e.g., proximal policy optimization), while accommodating multiple turns, long horizons, and delayed rewards effectively. To do this, our framework adopts a hierarchical RL approach and runs two RL algorithms in parallel: a high-level off-policy value-based RL algorithm to aggregate reward over utterances, and a low-level RL algorithm that utilizes this high-level value function to train a token policy within each utterance or turn. Our hierarchical framework, Actor-Critic Framework with a Hierarchical Structure (ArCHer), can also give rise to other RL methods. Empirically, we find that ArCHer significantly improves efficiency and performance on agent tasks, attaining a sample efficiency of about 100x over existing methods, while also improving with larger model capacity (upto the 7 billion scale that we tested on).", "summary_bullets": ["A broad use case of large language models (LLMs) is in goal-directed decision-making tasks (or \"agent\" tasks), where an LLM needs to not just generate completions for a given prompt, but rather make intelligent decisions over a multi-turn interaction to accomplish a task (e.g., when interacting with the web, using tools, or providing customer support).", "Reinforcement learning (RL) provides a general paradigm to address such agent tasks, but current RL methods for LLMs largely focus on optimizing single-turn rewards.", "By construction, most single-turn RL methods cannot endow LLMs with the ability to intelligently seek information over multiple turns, perform credit assignment, or reason about their past actions -- all of which are critical in agent tasks."], "method": "In this paper, we develop a framework for building multi-turn RL algorithms for fine-tuning LLMs, that preserves the flexibility of existing single-turn RL methods for LLMs (e.g., proximal policy optimization), while accommodating multiple turns, long horizons, and delayed rewards effectively.", "key_results": ["Empirically, we find that ArCHer significantly improves efficiency and performance on agent tasks, attaining a sample efficiency of about 100x over existing methods, while also improving with larger model capacity (upto the 7 billion scale that we tested on)."], "limitations": ["Abstract-level evidence only: validate assumptions, evaluation protocol, and failure cases in the full paper before relying on this as key evidence."], "bibkey": "Zhou2024Archer"}
{"paper_id": "P0094", "title": "AriGraph: Learning Knowledge Graph World Models with Episodic Memory for LLM Agents", "year": 2024, "url": "http://arxiv.org/abs/2407.04363v3", "arxiv_id": "2407.04363v3", "primary_category": "cs.AI", "categories": ["cs.AI"], "pdf_url": "https://arxiv.org/pdf/2407.04363v3", "priority": "normal", "mapped_sections": ["4.2"], "evidence_level": "abstract", "fulltext_path": "", "authors": ["Petr Anokhin", "Nikita Semenov", "Artyom Sorokin", "Dmitry Evseev", "Andrey Kravchenko", "Mikhail Burtsev", "Evgeny Burnaev"], "abstract": "Advancements in the capabilities of Large Language Models (LLMs) have created a promising foundation for developing autonomous agents. With the right tools, these agents could learn to solve tasks in new environments by accumulating and updating their knowledge. Current LLM-based agents process past experiences using a full history of observations, summarization, retrieval augmentation. However, these unstructured memory representations do not facilitate the reasoning and planning essential for complex decision-making. In our study, we introduce AriGraph, a novel method wherein the agent constructs and updates a memory graph that integrates semantic and episodic memories while exploring the environment. We demonstrate that our Ariadne LLM agent, consisting of the proposed memory architecture augmented with planning and decision-making, effectively handles complex tasks within interactive text game environments difficult even for human players. Results show that our approach markedly outperforms other established memory methods and strong RL baselines in a range of problems of varying complexity. Additionally, AriGraph demonstrates competitive performance compared to dedicated knowledge graph-based methods in static multi-hop question-answering.", "summary_bullets": ["Advancements in the capabilities of Large Language Models (LLMs) have created a promising foundation for developing autonomous agents.", "With the right tools, these agents could learn to solve tasks in new environments by accumulating and updating their knowledge.", "Current LLM-based agents process past experiences using a full history of observations, summarization, retrieval augmentation."], "method": "In our study, we introduce AriGraph, a novel method wherein the agent constructs and updates a memory graph that integrates semantic and episodic memories while exploring the environment.", "key_results": ["We demonstrate that our Ariadne LLM agent, consisting of the proposed memory architecture augmented with planning and decision-making, effectively handles complex tasks within interactive text game environments difficult even for human players."], "limitations": ["Abstract-level evidence only: validate assumptions, evaluation protocol, and failure cases in the full paper before relying on this as key evidence."], "bibkey": "Anokhin2024Arigraph"}
{"paper_id": "P0095", "title": "Automatic Control With Human-Like Reasoning: Exploring Language Model Embodied Air Traffic Agents", "year": 2024, "url": "http://arxiv.org/abs/2409.09717v1", "arxiv_id": "2409.09717v1", "primary_category": "cs.AI", "categories": ["cs.AI", "cs.CL"], "pdf_url": "https://arxiv.org/pdf/2409.09717v1", "priority": "normal", "mapped_sections": [], "evidence_level": "abstract", "fulltext_path": "", "authors": ["Justas Andriu≈°keviƒçius", "Junzi Sun"], "abstract": "Recent developments in language models have created new opportunities in air traffic control studies. The current focus is primarily on text and language-based use cases. However, these language models may offer a higher potential impact in the air traffic control domain, thanks to their ability to interact with air traffic environments in an embodied agent form. They also provide a language-like reasoning capability to explain their decisions, which has been a significant roadblock for the implementation of automatic air traffic control.\n  This paper investigates the application of a language model-based agent with function-calling and learning capabilities to resolve air traffic conflicts without human intervention. The main components of this research are foundational large language models, tools that allow the agent to interact with the simulator, and a new concept, the experience library. An innovative part of this research, the experience library, is a vector database that stores synthesized knowledge that agents have learned from interactions with the simulations and language models.\n  To evaluate the performance of our language model-based agent, both open-source and closed-source models were tested. The results of our study reveal significant differences in performance across various configurations of the language model-based agents. The best-performing configuration was able to solve almost all 120 but one imminent conflict scenarios, including up to four aircraft at the same time. Most importantly, the agents are able to provide human-level text explanations on traffic situations and conflict resolution strategies.", "summary_bullets": ["Recent developments in language models have created new opportunities in air traffic control studies.", "The current focus is primarily on text and language-based use cases.", "However, these language models may offer a higher potential impact in the air traffic control domain, thanks to their ability to interact with air traffic environments in an embodied agent form."], "method": "Recent developments in language models have created new opportunities in air traffic control studies.", "key_results": ["The best-performing configuration was able to solve almost all 120 but one imminent conflict scenarios, including up to four aircraft at the same time.", "This paper investigates the application of a language model-based agent with function-calling and learning capabilities to resolve air traffic conflicts without human intervention."], "limitations": ["Abstract-level evidence only: validate assumptions, evaluation protocol, and failure cases in the full paper before relying on this as key evidence."], "bibkey": "Andriukeviius2024Automatic"}
{"paper_id": "P0096", "title": "AvaTaR: Optimizing LLM Agents for Tool Usage via Contrastive Reasoning", "year": 2024, "url": "http://arxiv.org/abs/2406.11200v3", "arxiv_id": "2406.11200v3", "primary_category": "cs.LG", "categories": ["cs.LG", "cs.CL"], "pdf_url": "https://arxiv.org/pdf/2406.11200v3", "priority": "normal", "mapped_sections": ["3.2"], "evidence_level": "abstract", "fulltext_path": "", "authors": ["Shirley Wu", "Shiyu Zhao", "Qian Huang", "Kexin Huang", "Michihiro Yasunaga", "Kaidi Cao", "Vassilis N. Ioannidis", "Karthik Subbian", "Jure Leskovec", "James Zou"], "abstract": "Large language model (LLM) agents have demonstrated impressive capabilities in utilizing external tools and knowledge to boost accuracy and reduce hallucinations. However, developing prompting techniques that enable LLM agents to effectively use these tools and knowledge remains a heuristic and labor-intensive task. Here, we introduce AvaTaR, a novel and automated framework that optimizes an LLM agent to effectively leverage provided tools, improving performance on a given task. During optimization, we design a comparator module to iteratively deliver insightful and comprehensive prompts to the LLM agent by contrastively reasoning between positive and negative examples sampled from training data. We demonstrate AvaTaR on four complex multimodal retrieval datasets featuring textual, visual, and relational information, and three general question-answering (QA) datasets. We find AvaTaR consistently outperforms state-of-the-art approaches across all seven tasks, exhibiting strong generalization ability when applied to novel cases and achieving an average relative improvement of 14% on the Hit@1 metric for the retrieval datasets and 13% for the QA datasets. Code and dataset are available at https://github.com/zou-group/avatar.", "summary_bullets": ["Large language model (LLM) agents have demonstrated impressive capabilities in utilizing external tools and knowledge to boost accuracy and reduce hallucinations.", "However, developing prompting techniques that enable LLM agents to effectively use these tools and knowledge remains a heuristic and labor-intensive task.", "Here, we introduce AvaTaR, a novel and automated framework that optimizes an LLM agent to effectively leverage provided tools, improving performance on a given task."], "method": "Here, we introduce AvaTaR, a novel and automated framework that optimizes an LLM agent to effectively leverage provided tools, improving performance on a given task.", "key_results": ["We find AvaTaR consistently outperforms state-of-the-art approaches across all seven tasks, exhibiting strong generalization ability when applied to novel cases and achieving an average relative improvement of 14% on the Hit@1 metric for the retrieval datasets and 13% for the QA datasets.", "Large language model (LLM) agents have demonstrated impressive capabilities in utilizing external tools and knowledge to boost accuracy and reduce hallucinations."], "limitations": ["Abstract-level evidence only: validate assumptions, evaluation protocol, and failure cases in the full paper before relying on this as key evidence."], "bibkey": "Wu2024Avatar"}
{"paper_id": "P0097", "title": "Cooperate or Collapse: Emergence of Sustainable Cooperation in a Society of LLM Agents", "year": 2024, "url": "http://arxiv.org/abs/2404.16698v4", "arxiv_id": "2404.16698v4", "primary_category": "cs.CL", "categories": ["cs.CL"], "pdf_url": "https://arxiv.org/pdf/2404.16698v4", "priority": "normal", "mapped_sections": [], "evidence_level": "abstract", "fulltext_path": "", "authors": ["Giorgio Piatti", "Zhijing Jin", "Max Kleiman-Weiner", "Bernhard Sch√∂lkopf", "Mrinmaya Sachan", "Rada Mihalcea"], "abstract": "As AI systems pervade human life, ensuring that large language models (LLMs) make safe decisions remains a significant challenge. We introduce the Governance of the Commons Simulation (GovSim), a generative simulation platform designed to study strategic interactions and cooperative decision-making in LLMs. In GovSim, a society of AI agents must collectively balance exploiting a common resource with sustaining it for future use. This environment enables the study of how ethical considerations, strategic planning, and negotiation skills impact cooperative outcomes. We develop an LLM-based agent architecture and test it with the leading open and closed LLMs. We find that all but the most powerful LLM agents fail to achieve a sustainable equilibrium in GovSim, with the highest survival rate below 54%. Ablations reveal that successful multi-agent communication between agents is critical for achieving cooperation in these cases. Furthermore, our analyses show that the failure to achieve sustainable cooperation in most LLMs stems from their inability to formulate and analyze hypotheses about the long-term effects of their actions on the equilibrium of the group. Finally, we show that agents that leverage \"Universalization\"-based reasoning, a theory of moral thinking, are able to achieve significantly better sustainability. Taken together, GovSim enables us to study the mechanisms that underlie sustainable self-government with specificity and scale. We open source the full suite of our research results, including the simulation environment, agent prompts, and a comprehensive web interface.", "summary_bullets": ["As AI systems pervade human life, ensuring that large language models (LLMs) make safe decisions remains a significant challenge.", "We introduce the Governance of the Commons Simulation (GovSim), a generative simulation platform designed to study strategic interactions and cooperative decision-making in LLMs.", "In GovSim, a society of AI agents must collectively balance exploiting a common resource with sustaining it for future use."], "method": "We introduce the Governance of the Commons Simulation (GovSim), a generative simulation platform designed to study strategic interactions and cooperative decision-making in LLMs.", "key_results": ["We find that all but the most powerful LLM agents fail to achieve a sustainable equilibrium in GovSim, with the highest survival rate below 54%.", "As AI systems pervade human life, ensuring that large language models (LLMs) make safe decisions remains a significant challenge."], "limitations": ["Abstract-level evidence only: validate assumptions, evaluation protocol, and failure cases in the full paper before relying on this as key evidence."], "bibkey": "Piatti2024Cooperate"}
{"paper_id": "P0098", "title": "EnvGen: Generating and Adapting Environments via LLMs for Training Embodied Agents", "year": 2024, "url": "http://arxiv.org/abs/2403.12014v2", "arxiv_id": "2403.12014v2", "primary_category": "cs.CL", "categories": ["cs.CL", "cs.AI", "cs.LG"], "pdf_url": "https://arxiv.org/pdf/2403.12014v2", "priority": "normal", "mapped_sections": [], "evidence_level": "abstract", "fulltext_path": "", "authors": ["Abhay Zala", "Jaemin Cho", "Han Lin", "Jaehong Yoon", "Mohit Bansal"], "abstract": "Recent SOTA approaches for embodied learning via interaction directly employ large language models (LLMs) as agents to determine the next steps in an environment. Due to their world knowledge and reasoning capabilities, LLM agents achieve stronger performance than previous smaller agents based on reinforcement learning (RL); however, frequently calling LLMs is slow and expensive. Instead of directly employing LLMs as agents, can we use LLMs' reasoning capabilities to adaptively create training environments to help smaller RL agents learn useful skills that they are weak at? We propose EnvGen, a novel framework to address this question. We first prompt an LLM to generate training environments by giving it the task description and simulator objectives that the agents should learn and then asking it to generate a set of environment configurations (e.g., different terrains, items initially given to agents, etc.). Next, we train a small RL agent in a mixture of the original and LLM-generated environments. Then, we enable the LLM to continuously adapt the generated environments to progressively improve the skills that the agent is weak at, by providing feedback to the LLM in the form of the agent's performance. We demonstrate the usefulness of EnvGen with comprehensive experiments in Crafter and Heist environments. We find that a small RL agent trained with EnvGen can outperform SOTA methods, including a GPT-4 agent, and learns long-horizon tasks significantly faster. We also show that using an LLM to adapt environments dynamically outperforms curriculum learning approaches and how the environments are adapted to help improve RL agents' weaker skills over time. Additionally, EnvGen is substantially more efficient as it only uses a small number of LLM calls (e.g., 4 in total), whereas LLM agents require thousands of calls. Lastly, we present detailed ablation studies for EnvGen design choices.", "summary_bullets": ["Recent SOTA approaches for embodied learning via interaction directly employ large language models (LLMs) as agents to determine the next steps in an environment.", "Due to their world knowledge and reasoning capabilities, LLM agents achieve stronger performance than previous smaller agents based on reinforcement learning (RL); however, frequently calling LLMs is slow and expensive.", "Instead of directly employing LLMs as agents, can we use LLMs' reasoning capabilities to adaptively create training environments to help smaller RL agents learn useful skills that they are weak at?"], "method": "We propose EnvGen, a novel framework to address this question.", "key_results": ["We find that a small RL agent trained with EnvGen can outperform SOTA methods, including a GPT-4 agent, and learns long-horizon tasks significantly faster.", "Additionally, EnvGen is substantially more efficient as it only uses a small number of LLM calls (e.g., 4 in total), whereas LLM agents require thousands of calls."], "limitations": ["Abstract-level evidence only: validate assumptions, evaluation protocol, and failure cases in the full paper before relying on this as key evidence."], "bibkey": "Zala2024Envgen"}
{"paper_id": "P0099", "title": "From LLMs to LLM-based Agents for Software Engineering: A Survey of Current, Challenges and Future", "year": 2024, "url": "http://arxiv.org/abs/2408.02479v2", "arxiv_id": "2408.02479v2", "primary_category": "cs.SE", "categories": ["cs.SE", "cs.AI", "cs.CL"], "pdf_url": "https://arxiv.org/pdf/2408.02479v2", "priority": "normal", "mapped_sections": ["5.1"], "evidence_level": "abstract", "fulltext_path": "", "authors": ["Haolin Jin", "Linghan Huang", "Haipeng Cai", "Jun Yan", "Bo Li", "Huaming Chen"], "abstract": "With the rise of large language models (LLMs), researchers are increasingly exploring their applications in var ious vertical domains, such as software engineering. LLMs have achieved remarkable success in areas including code generation and vulnerability detection. However, they also exhibit numerous limitations and shortcomings. LLM-based agents, a novel tech nology with the potential for Artificial General Intelligence (AGI), combine LLMs as the core for decision-making and action-taking, addressing some of the inherent limitations of LLMs such as lack of autonomy and self-improvement. Despite numerous studies and surveys exploring the possibility of using LLMs in software engineering, it lacks a clear distinction between LLMs and LLM based agents. It is still in its early stage for a unified standard and benchmarking to qualify an LLM solution as an LLM-based agent in its domain. In this survey, we broadly investigate the current practice and solutions for LLMs and LLM-based agents for software engineering. In particular we summarise six key topics: requirement engineering, code generation, autonomous decision-making, software design, test generation, and software maintenance. We review and differentiate the work of LLMs and LLM-based agents from these six topics, examining their differences and similarities in tasks, benchmarks, and evaluation metrics. Finally, we discuss the models and benchmarks used, providing a comprehensive analysis of their applications and effectiveness in software engineering. We anticipate this work will shed some lights on pushing the boundaries of LLM-based agents in software engineering for future research.", "summary_bullets": ["With the rise of large language models (LLMs), researchers are increasingly exploring their applications in var ious vertical domains, such as software engineering.", "LLMs have achieved remarkable success in areas including code generation and vulnerability detection.", "However, they also exhibit numerous limitations and shortcomings."], "method": "With the rise of large language models (LLMs), researchers are increasingly exploring their applications in var ious vertical domains, such as software engineering.", "key_results": ["LLMs have achieved remarkable success in areas including code generation and vulnerability detection.", "We review and differentiate the work of LLMs and LLM-based agents from these six topics, examining their differences and similarities in tasks, benchmarks, and evaluation metrics."], "limitations": ["Abstract-level evidence only: validate assumptions, evaluation protocol, and failure cases in the full paper before relying on this as key evidence.", "However, they also exhibit numerous limitations and shortcomings."], "bibkey": "Jin2024From"}
{"paper_id": "P0100", "title": "If LLM Is the Wizard, Then Code Is the Wand: A Survey on How Code Empowers Large Language Models to Serve as Intelligent Agents", "year": 2024, "url": "http://arxiv.org/abs/2401.00812v2", "arxiv_id": "2401.00812v2", "primary_category": "cs.CL", "categories": ["cs.CL"], "pdf_url": "https://arxiv.org/pdf/2401.00812v2", "priority": "normal", "mapped_sections": [], "evidence_level": "abstract", "fulltext_path": "", "authors": ["Ke Yang", "Jiateng Liu", "John Wu", "Chaoqi Yang", "Yi R. Fung", "Sha Li", "Zixuan Huang", "Xu Cao", "Xingyao Wang", "Yiquan Wang", "Heng Ji", "Chengxiang Zhai"], "abstract": "The prominent large language models (LLMs) of today differ from past language models not only in size, but also in the fact that they are trained on a combination of natural language and formal language (code). As a medium between humans and computers, code translates high-level goals into executable steps, featuring standard syntax, logical consistency, abstraction, and modularity. In this survey, we present an overview of the various benefits of integrating code into LLMs' training data. Specifically, beyond enhancing LLMs in code generation, we observe that these unique properties of code help (i) unlock the reasoning ability of LLMs, enabling their applications to a range of more complex natural language tasks; (ii) steer LLMs to produce structured and precise intermediate steps, which can then be connected to external execution ends through function calls; and (iii) take advantage of code compilation and execution environment, which also provides diverse feedback for model improvement. In addition, we trace how these profound capabilities of LLMs, brought by code, have led to their emergence as intelligent agents (IAs) in situations where the ability to understand instructions, decompose goals, plan and execute actions, and refine from feedback are crucial to their success on downstream tasks. Finally, we present several key challenges and future directions of empowering LLMs with code.", "summary_bullets": ["The prominent large language models (LLMs) of today differ from past language models not only in size, but also in the fact that they are trained on a combination of natural language and formal language (code).", "As a medium between humans and computers, code translates high-level goals into executable steps, featuring standard syntax, logical consistency, abstraction, and modularity.", "In this survey, we present an overview of the various benefits of integrating code into LLMs' training data."], "method": "In this survey, we present an overview of the various benefits of integrating code into LLMs' training data.", "key_results": ["In addition, we trace how these profound capabilities of LLMs, brought by code, have led to their emergence as intelligent agents (IAs) in situations where the ability to understand instructions, decompose goals, plan and execute actions, and refine from feedback are crucial to their success on downstream tasks."], "limitations": ["Abstract-level evidence only: validate assumptions, evaluation protocol, and failure cases in the full paper before relying on this as key evidence."], "bibkey": "Yang2024Wizard"}
{"paper_id": "P0101", "title": "Iteration of Thought: Leveraging Inner Dialogue for Autonomous Large Language Model Reasoning", "year": 2024, "url": "http://arxiv.org/abs/2409.12618v2", "arxiv_id": "2409.12618v2", "primary_category": "cs.CL", "categories": ["cs.CL", "cs.AI", "cs.LG", "cs.MA"], "pdf_url": "https://arxiv.org/pdf/2409.12618v2", "priority": "normal", "mapped_sections": [], "evidence_level": "abstract", "fulltext_path": "", "authors": ["Santosh Kumar Radha", "Yasamin Nouri Jelyani", "Ara Ghukasyan", "Oktay Goktas"], "abstract": "Iterative human engagement is a common and effective means of leveraging the advanced language processing power of large language models (LLMs). Using well-structured prompts in a conversational manner, human users can effectively influence an LLM to develop more thoughtful and accurate responses. Motivated by this insight, we propose the Iteration of Thought (IoT) framework for enhancing LLM responses by generating \"thought\"-provoking prompts vis a vis an input query and the current iteration of an LLM's response. Unlike static or semi-static approaches, e.g. Chain of Thought (CoT) or Tree of Thoughts (ToT), IoT adapts its reasoning path dynamically, based on evolving context, and without generating alternate explorative thoughts which are ultimately discarded. The three components of the IoT framework are (1) an Inner Dialogue Agent (IDA) responsible for generating instructive, context-specific prompts; (2) an LLM Agent (LLMA) that processes these prompts to refine its responses; and (3) an iterative prompting loop that implements a conversation between the former two components. We introduce two variants of our framework: Autonomous Iteration of Thought (AIoT), where an LLM decides when to stop iterating, and Guided Iteration of Thought (GIoT), which always forces a fixed number iterations. We investigate the performance of IoT across various datasets, spanning complex reasoning tasks from the GPQA dataset, explorative problem-solving in Game of 24, puzzle solving in Mini Crosswords, and multi-hop question answering from the HotpotQA dataset. Our results show that IoT represents a viable paradigm for autonomous response refinement in LLMs, showcasing significant improvements over CoT and thereby enabling more adaptive and efficient reasoning systems that minimize human intervention.", "summary_bullets": ["Iterative human engagement is a common and effective means of leveraging the advanced language processing power of large language models (LLMs).", "Using well-structured prompts in a conversational manner, human users can effectively influence an LLM to develop more thoughtful and accurate responses.", "Motivated by this insight, we propose the Iteration of Thought (IoT) framework for enhancing LLM responses by generating \"thought\"-provoking prompts vis a vis an input query and the current iteration of an LLM's response."], "method": "Motivated by this insight, we propose the Iteration of Thought (IoT) framework for enhancing LLM responses by generating \"thought\"-provoking prompts vis a vis an input query and the current iteration of an LLM's response.", "key_results": ["We investigate the performance of IoT across various datasets, spanning complex reasoning tasks from the GPQA dataset, explorative problem-solving in Game of 24, puzzle solving in Mini Crosswords, and multi-hop question answering from the HotpotQA dataset.", "The three components of the IoT framework are (1) an Inner Dialogue Agent (IDA) responsible for generating instructive, context-specific prompts; (2) an LLM Agent (LLMA) that processes these prompts to refine its responses; and (3) an iterative prompting loop that implements a conversation between the former two components."], "limitations": ["Abstract-level evidence only: validate assumptions, evaluation protocol, and failure cases in the full paper before relying on this as key evidence."], "bibkey": "Radha2024Iteration"}
{"paper_id": "P0102", "title": "KG-Agent: An Efficient Autonomous Agent Framework for Complex Reasoning over Knowledge Graph", "year": 2024, "url": "http://arxiv.org/abs/2402.11163v1", "arxiv_id": "2402.11163v1", "primary_category": "cs.CL", "categories": ["cs.CL"], "pdf_url": "https://arxiv.org/pdf/2402.11163v1", "priority": "normal", "mapped_sections": ["3.1"], "evidence_level": "abstract", "fulltext_path": "", "authors": ["Jinhao Jiang", "Kun Zhou", "Wayne Xin Zhao", "Yang Song", "Chen Zhu", "Hengshu Zhu", "Ji-Rong Wen"], "abstract": "In this paper, we aim to improve the reasoning ability of large language models (LLMs) over knowledge graphs (KGs) to answer complex questions. Inspired by existing methods that design the interaction strategy between LLMs and KG, we propose an autonomous LLM-based agent framework, called KG-Agent, which enables a small LLM to actively make decisions until finishing the reasoning process over KGs. In KG-Agent, we integrate the LLM, multifunctional toolbox, KG-based executor, and knowledge memory, and develop an iteration mechanism that autonomously selects the tool then updates the memory for reasoning over KG. To guarantee the effectiveness, we leverage program language to formulate the multi-hop reasoning process over the KG, and synthesize a code-based instruction dataset to fine-tune the base LLM. Extensive experiments demonstrate that only using 10K samples for tuning LLaMA-7B can outperform state-of-the-art methods using larger LLMs or more data, on both in-domain and out-domain datasets. Our code and data will be publicly released.", "summary_bullets": ["In this paper, we aim to improve the reasoning ability of large language models (LLMs) over knowledge graphs (KGs) to answer complex questions.", "Inspired by existing methods that design the interaction strategy between LLMs and KG, we propose an autonomous LLM-based agent framework, called KG-Agent, which enables a small LLM to actively make decisions until finishing the reasoning process over KGs.", "In KG-Agent, we integrate the LLM, multifunctional toolbox, KG-based executor, and knowledge memory, and develop an iteration mechanism that autonomously selects the tool then updates the memory for reasoning over KG."], "method": "Inspired by existing methods that design the interaction strategy between LLMs and KG, we propose an autonomous LLM-based agent framework, called KG-Agent, which enables a small LLM to actively make decisions until finishing the reasoning process over KGs.", "key_results": ["Extensive experiments demonstrate that only using 10K samples for tuning LLaMA-7B can outperform state-of-the-art methods using larger LLMs or more data, on both in-domain and out-domain datasets.", "To guarantee the effectiveness, we leverage program language to formulate the multi-hop reasoning process over the KG, and synthesize a code-based instruction dataset to fine-tune the base LLM."], "limitations": ["Abstract-level evidence only: validate assumptions, evaluation protocol, and failure cases in the full paper before relying on this as key evidence."], "bibkey": "Jiang2024Agent"}
{"paper_id": "P0103", "title": "LLMSat: A Large Language Model-Based Goal-Oriented Agent for Autonomous Space Exploration", "year": 2024, "url": "http://arxiv.org/abs/2405.01392v1", "arxiv_id": "2405.01392v1", "primary_category": "cs.RO", "categories": ["cs.RO", "cs.AI", "cs.LG", "cs.MA", "physics.space-ph"], "pdf_url": "https://arxiv.org/pdf/2405.01392v1", "priority": "normal", "mapped_sections": ["3.1"], "evidence_level": "abstract", "fulltext_path": "", "authors": ["David Maranto"], "abstract": "As spacecraft journey further from Earth with more complex missions, systems of greater autonomy and onboard intelligence are called for. Reducing reliance on human-based mission control becomes increasingly critical if we are to increase our rate of solar-system-wide exploration. Recent work has explored AI-based goal-oriented systems to increase the level of autonomy in mission execution. These systems make use of symbolic reasoning managers to make inferences from the state of a spacecraft and a handcrafted knowledge base, enabling autonomous generation of tasks and re-planning. Such systems have proven to be successful in controlled cases, but they are difficult to implement as they require human-crafted ontological models to allow the spacecraft to understand the world. Reinforcement learning has been applied to train robotic agents to pursue a goal. A new architecture for autonomy is called for. This work explores the application of Large Language Models (LLMs) as the high-level control system of a spacecraft. Using a systems engineering approach, this work presents the design and development of an agentic spacecraft controller by leveraging an LLM as a reasoning engine, to evaluate the utility of such an architecture in achieving higher levels of spacecraft autonomy. A series of deep space mission scenarios simulated within the popular game engine Kerbal Space Program (KSP) are used as case studies to evaluate the implementation against the requirements. It is shown the reasoning and planning abilities of present-day LLMs do not scale well as the complexity of a mission increases, but this can be alleviated with adequate prompting frameworks and strategic selection of the agent's level of authority over the host spacecraft. This research evaluates the potential of LLMs in augmenting autonomous decision-making systems for future robotic space applications.", "summary_bullets": ["As spacecraft journey further from Earth with more complex missions, systems of greater autonomy and onboard intelligence are called for.", "Reducing reliance on human-based mission control becomes increasingly critical if we are to increase our rate of solar-system-wide exploration.", "Recent work has explored AI-based goal-oriented systems to increase the level of autonomy in mission execution."], "method": "As spacecraft journey further from Earth with more complex missions, systems of greater autonomy and onboard intelligence are called for.", "key_results": ["Reducing reliance on human-based mission control becomes increasingly critical if we are to increase our rate of solar-system-wide exploration.", "Such systems have proven to be successful in controlled cases, but they are difficult to implement as they require human-crafted ontological models to allow the spacecraft to understand the world."], "limitations": ["Abstract-level evidence only: validate assumptions, evaluation protocol, and failure cases in the full paper before relying on this as key evidence."], "bibkey": "Maranto2024Llmsat"}
{"paper_id": "P0104", "title": "LLaMP: Large Language Model Made Powerful for High-fidelity Materials Knowledge Retrieval and Distillation", "year": 2024, "url": "http://arxiv.org/abs/2401.17244v3", "arxiv_id": "2401.17244v3", "primary_category": "cs.CL", "categories": ["cs.CL", "cond-mat.mtrl-sci", "cs.AI"], "pdf_url": "https://arxiv.org/pdf/2401.17244v3", "priority": "normal", "mapped_sections": ["4.2"], "evidence_level": "abstract", "fulltext_path": "", "authors": ["Yuan Chiang", "Elvis Hsieh", "Chia-Hong Chou", "Janosh Riebesell"], "abstract": "Reducing hallucination of Large Language Models (LLMs) is imperative for use in the sciences, where reliability and reproducibility are crucial. However, LLMs inherently lack long-term memory, making it a nontrivial, ad hoc, and inevitably biased task to fine-tune them on domain-specific literature and data. Here we introduce LLaMP, a multimodal retrieval-augmented generation (RAG) framework of hierarchical reasoning-and-acting (ReAct) agents that can dynamically and recursively interact with computational and experimental data on Materials Project (MP) and run atomistic simulations via high-throughput workflow interface. Without fine-tuning, LLaMP demonstrates strong tool usage ability to comprehend and integrate various modalities of materials science concepts, fetch relevant data stores on the fly, process higher-order data (such as crystal structure and elastic tensor), and streamline complex tasks in computational materials and chemistry. We propose a simple metric combining uncertainty and confidence estimates to evaluate the self-consistency of responses by LLaMP and vanilla LLMs. Our benchmark shows that LLaMP effectively mitigates the intrinsic bias in LLMs, counteracting the errors on bulk moduli, electronic bandgaps, and formation energies that seem to derive from mixed data sources. We also demonstrate LLaMP's capability to edit crystal structures and run annealing molecular dynamics simulations using pre-trained machine-learning force fields. The framework offers an intuitive and nearly hallucination-free approach to exploring and scaling materials informatics, and establishes a pathway for knowledge distillation and fine-tuning other language models. Code and live demo are available at https://github.com/chiang-yuan/llamp", "summary_bullets": ["Reducing hallucination of Large Language Models (LLMs) is imperative for use in the sciences, where reliability and reproducibility are crucial.", "However, LLMs inherently lack long-term memory, making it a nontrivial, ad hoc, and inevitably biased task to fine-tune them on domain-specific literature and data.", "Here we introduce LLaMP, a multimodal retrieval-augmented generation (RAG) framework of hierarchical reasoning-and-acting (ReAct) agents that can dynamically and recursively interact with computational and experimental data on Materials Project (MP) and run atomistic simulations via high-throughput workflow interface."], "method": "Here we introduce LLaMP, a multimodal retrieval-augmented generation (RAG) framework of hierarchical reasoning-and-acting (ReAct) agents that can dynamically and recursively interact with computational and experimental data on Materials Project (MP) and run atomistic simulations via high-throughput workflow interface.", "key_results": ["We propose a simple metric combining uncertainty and confidence estimates to evaluate the self-consistency of responses by LLaMP and vanilla LLMs.", "Our benchmark shows that LLaMP effectively mitigates the intrinsic bias in LLMs, counteracting the errors on bulk moduli, electronic bandgaps, and formation energies that seem to derive from mixed data sources."], "limitations": ["Abstract-level evidence only: validate assumptions, evaluation protocol, and failure cases in the full paper before relying on this as key evidence."], "bibkey": "Chiang2024Llamp"}
{"paper_id": "P0105", "title": "Large Language Model Agent in Financial Trading: A Survey", "year": 2024, "url": "http://arxiv.org/abs/2408.06361v1", "arxiv_id": "2408.06361v1", "primary_category": "q-fin.TR", "categories": ["q-fin.TR", "cs.CL"], "pdf_url": "https://arxiv.org/pdf/2408.06361v1", "priority": "normal", "mapped_sections": [], "evidence_level": "abstract", "fulltext_path": "", "authors": ["Han Ding", "Yinheng Li", "Junhao Wang", "Hang Chen"], "abstract": "Trading is a highly competitive task that requires a combination of strategy, knowledge, and psychological fortitude. With the recent success of large language models(LLMs), it is appealing to apply the emerging intelligence of LLM agents in this competitive arena and understanding if they can outperform professional traders. In this survey, we provide a comprehensive review of the current research on using LLMs as agents in financial trading. We summarize the common architecture used in the agent, the data inputs, and the performance of LLM trading agents in backtesting as well as the challenges presented in these research. This survey aims to provide insights into the current state of LLM-based financial trading agents and outline future research directions in this field.", "summary_bullets": ["Trading is a highly competitive task that requires a combination of strategy, knowledge, and psychological fortitude.", "With the recent success of large language models(LLMs), it is appealing to apply the emerging intelligence of LLM agents in this competitive arena and understanding if they can outperform professional traders.", "In this survey, we provide a comprehensive review of the current research on using LLMs as agents in financial trading."], "method": "Trading is a highly competitive task that requires a combination of strategy, knowledge, and psychological fortitude.", "key_results": ["With the recent success of large language models(LLMs), it is appealing to apply the emerging intelligence of LLM agents in this competitive arena and understanding if they can outperform professional traders."], "limitations": ["Abstract-level evidence only: validate assumptions, evaluation protocol, and failure cases in the full paper before relying on this as key evidence."], "bibkey": "Ding2024Large"}
{"paper_id": "P0106", "title": "Perceive, Reflect, and Plan: Designing LLM Agent for Goal-Directed City Navigation without Instructions", "year": 2024, "url": "http://arxiv.org/abs/2408.04168v3", "arxiv_id": "2408.04168v3", "primary_category": "cs.AI", "categories": ["cs.AI"], "pdf_url": "https://arxiv.org/pdf/2408.04168v3", "priority": "normal", "mapped_sections": [], "evidence_level": "abstract", "fulltext_path": "", "authors": ["Qingbin Zeng", "Qinglong Yang", "Shunan Dong", "Heming Du", "Liang Zheng", "Fengli Xu", "Yong Li"], "abstract": "This paper considers a scenario in city navigation: an AI agent is provided with language descriptions of the goal location with respect to some well-known landmarks; By only observing the scene around, including recognizing landmarks and road network connections, the agent has to make decisions to navigate to the goal location without instructions. This problem is very challenging, because it requires agent to establish self-position and acquire spatial representation of complex urban environment, where landmarks are often invisible. In the absence of navigation instructions, such abilities are vital for the agent to make high-quality decisions in long-range city navigation. With the emergent reasoning ability of large language models (LLMs), a tempting baseline is to prompt LLMs to \"react\" on each observation and make decisions accordingly. However, this baseline has very poor performance that the agent often repeatedly visits same locations and make short-sighted, inconsistent decisions. To address these issues, this paper introduces a novel agentic workflow featured by its abilities to perceive, reflect and plan. Specifically, we find LLaVA-7B can be fine-tuned to perceive the direction and distance of landmarks with sufficient accuracy for city navigation. Moreover, reflection is achieved through a memory mechanism, where past experiences are stored and can be retrieved with current perception for effective decision argumentation. Planning uses reflection results to produce long-term plans, which can avoid short-sighted decisions in long-range navigation. We show the designed workflow significantly improves navigation ability of the LLM agent compared with the state-of-the-art baselines.", "summary_bullets": ["This paper considers a scenario in city navigation: an AI agent is provided with language descriptions of the goal location with respect to some well-known landmarks; By only observing the scene around, including recognizing landmarks and road network connections, the agent has to make decisions to navigate to the goal location without instructions.", "This problem is very challenging, because it requires agent to establish self-position and acquire spatial representation of complex urban environment, where landmarks are often invisible.", "In the absence of navigation instructions, such abilities are vital for the agent to make high-quality decisions in long-range city navigation."], "method": "This paper considers a scenario in city navigation: an AI agent is provided with language descriptions of the goal location with respect to some well-known landmarks; By only observing the scene around, including recognizing landmarks and road network connections, the agent has to make decisions to navigate to the goal location without instructions.", "key_results": ["Specifically, we find LLaVA-7B can be fine-tuned to perceive the direction and distance of landmarks with sufficient accuracy for city navigation.", "We show the designed workflow significantly improves navigation ability of the LLM agent compared with the state-of-the-art baselines."], "limitations": ["Abstract-level evidence only: validate assumptions, evaluation protocol, and failure cases in the full paper before relying on this as key evidence."], "bibkey": "Zeng2024Perceive"}
{"paper_id": "P0107", "title": "STRIDE: A Tool-Assisted LLM Agent Framework for Strategic and Interactive Decision-Making", "year": 2024, "url": "http://arxiv.org/abs/2405.16376v2", "arxiv_id": "2405.16376v2", "primary_category": "cs.CL", "categories": ["cs.CL", "cs.GT"], "pdf_url": "https://arxiv.org/pdf/2405.16376v2", "priority": "normal", "mapped_sections": ["3.2"], "evidence_level": "abstract", "fulltext_path": "", "authors": ["Chuanhao Li", "Runhan Yang", "Tiankai Li", "Milad Bafarassat", "Kourosh Sharifi", "Dirk Bergemann", "Zhuoran Yang"], "abstract": "Large Language Models (LLMs) like GPT-4 have revolutionized natural language processing, showing remarkable linguistic proficiency and reasoning capabilities. However, their application in strategic multi-agent decision-making environments is hampered by significant limitations including poor mathematical reasoning, difficulty in following instructions, and a tendency to generate incorrect information. These deficiencies hinder their performance in strategic and interactive tasks that demand adherence to nuanced game rules, long-term planning, exploration in unknown environments, and anticipation of opponents' moves. To overcome these obstacles, this paper presents a novel LLM agent framework equipped with memory and specialized tools to enhance their strategic decision-making capabilities. We deploy the tools in a number of economically important environments, in particular bilateral bargaining and multi-agent and dynamic mechanism design. We employ quantitative metrics to assess the framework's performance in various strategic decision-making problems. Our findings establish that our enhanced framework significantly improves the strategic decision-making capability of LLMs. While we highlight the inherent limitations of current LLM models, we demonstrate the improvements through targeted enhancements, suggesting a promising direction for future developments in LLM applications for interactive environments.", "summary_bullets": ["Large Language Models (LLMs) like GPT-4 have revolutionized natural language processing, showing remarkable linguistic proficiency and reasoning capabilities.", "However, their application in strategic multi-agent decision-making environments is hampered by significant limitations including poor mathematical reasoning, difficulty in following instructions, and a tendency to generate incorrect information.", "These deficiencies hinder their performance in strategic and interactive tasks that demand adherence to nuanced game rules, long-term planning, exploration in unknown environments, and anticipation of opponents' moves."], "method": "Large Language Models (LLMs) like GPT-4 have revolutionized natural language processing, showing remarkable linguistic proficiency and reasoning capabilities.", "key_results": ["Large Language Models (LLMs) like GPT-4 have revolutionized natural language processing, showing remarkable linguistic proficiency and reasoning capabilities.", "We employ quantitative metrics to assess the framework's performance in various strategic decision-making problems."], "limitations": ["Abstract-level evidence only: validate assumptions, evaluation protocol, and failure cases in the full paper before relying on this as key evidence.", "However, their application in strategic multi-agent decision-making environments is hampered by significant limitations including poor mathematical reasoning, difficulty in following instructions, and a tendency to generate incorrect information."], "bibkey": "Li2024Stride"}
{"paper_id": "P0108", "title": "Small LLMs Are Weak Tool Learners: A Multi-LLM Agent", "year": 2024, "url": "http://arxiv.org/abs/2401.07324v3", "arxiv_id": "2401.07324v3", "primary_category": "cs.AI", "categories": ["cs.AI", "cs.CL"], "pdf_url": "https://arxiv.org/pdf/2401.07324v3", "priority": "normal", "mapped_sections": ["3.2", "5.2"], "evidence_level": "abstract", "fulltext_path": "", "authors": ["Weizhou Shen", "Chenliang Li", "Hongzhan Chen", "Ming Yan", "Xiaojun Quan", "Hehong Chen", "Ji Zhang", "Fei Huang"], "abstract": "Large Language Model (LLM) agents significantly extend the capabilities of standalone LLMs, empowering them to interact with external tools (e.g., APIs, functions) and complete various tasks in a self-directed fashion. The challenge of tool use demands that LLMs not only understand user queries and generate answers accurately but also excel in task planning, tool invocation, and result summarization. While traditional works focus on training a single LLM with all these capabilities, performance limitations become apparent, particularly with smaller models. To overcome these challenges, we propose a novel approach that decomposes the aforementioned capabilities into a planner, caller, and summarizer. Each component is implemented by a single LLM that focuses on a specific capability and collaborates with others to accomplish the task. This modular framework facilitates individual updates and the potential use of smaller LLMs for building each capability. To effectively train this framework, we introduce a two-stage training paradigm. First, we fine-tune a backbone LLM on the entire dataset without discriminating sub-tasks, providing the model with a comprehensive understanding of the task. Second, the fine-tuned LLM is used to instantiate the planner, caller, and summarizer respectively, which are continually fine-tuned on respective sub-tasks. Evaluation across various tool-use benchmarks illustrates that our proposed multi-LLM framework surpasses the traditional single-LLM approach, highlighting its efficacy and advantages in tool learning.", "summary_bullets": ["Large Language Model (LLM) agents significantly extend the capabilities of standalone LLMs, empowering them to interact with external tools (e.g., APIs, functions) and complete various tasks in a self-directed fashion.", "The challenge of tool use demands that LLMs not only understand user queries and generate answers accurately but also excel in task planning, tool invocation, and result summarization.", "While traditional works focus on training a single LLM with all these capabilities, performance limitations become apparent, particularly with smaller models."], "method": "To overcome these challenges, we propose a novel approach that decomposes the aforementioned capabilities into a planner, caller, and summarizer.", "key_results": ["First, we fine-tune a backbone LLM on the entire dataset without discriminating sub-tasks, providing the model with a comprehensive understanding of the task.", "Evaluation across various tool-use benchmarks illustrates that our proposed multi-LLM framework surpasses the traditional single-LLM approach, highlighting its efficacy and advantages in tool learning."], "limitations": ["Abstract-level evidence only: validate assumptions, evaluation protocol, and failure cases in the full paper before relying on this as key evidence.", "While traditional works focus on training a single LLM with all these capabilities, performance limitations become apparent, particularly with smaller models."], "bibkey": "Shen2024Small"}
{"paper_id": "P0109", "title": "Steering Large Language Models between Code Execution and Textual Reasoning", "year": 2024, "url": "http://arxiv.org/abs/2410.03524v2", "arxiv_id": "2410.03524v2", "primary_category": "cs.CL", "categories": ["cs.CL"], "pdf_url": "https://arxiv.org/pdf/2410.03524v2", "priority": "normal", "mapped_sections": ["4.1"], "evidence_level": "abstract", "fulltext_path": "", "authors": ["Yongchao Chen", "Harsh Jhamtani", "Srinagesh Sharma", "Chuchu Fan", "Chi Wang"], "abstract": "While a lot of recent research focuses on enhancing the textual reasoning capabilities of Large Language Models (LLMs) by optimizing the multi-agent framework or reasoning chains, several benchmark tasks can be solved with 100\\% success through direct coding, which is more scalable and avoids the computational overhead associated with textual iterating and searching. Textual reasoning has inherent limitations in solving tasks with challenges in math, logics, optimization, and searching, which is unlikely to be solved by simply scaling up the model and data size. The recently released OpenAI GPT Code Interpreter and multi-agent frameworks such as AutoGen have demonstrated remarkable proficiency of integrating code generation and execution to solve complex tasks using LLMs. However, based on our experiments on 7 existing popular methods for steering code/text generation in both single- and multi-turn settings with 14 tasks and 6 types of LLMs (including the new O1-preview), currently there is no optimal method to correctly steer LLMs to write code when needed. We discover some interesting patterns on when models use code vs. textual reasoning with the evolution to task complexity and model sizes, which even result in an astonishingly inverse scaling behavior. We also discover that results from LLM written code are not always better than using textual reasoning, even if the task could be solved through code. To mitigate the above issues, we propose three methods to better steer LLM code/text generation and achieve a notable improvement. The costs of token lengths and runtime are thoroughly discussed for all the methods. We believe the problem of steering LLM code/text generation is critical for future research and has much space for further improvement. Project Page, Datasets, and Codes are available at https://yongchao98.github.io/CodeSteer/.", "summary_bullets": ["While a lot of recent research focuses on enhancing the textual reasoning capabilities of Large Language Models (LLMs) by optimizing the multi-agent framework or reasoning chains, several benchmark tasks can be solved with 100\\% success through direct coding, which is more scalable and avoids the computational overhead associated with textual iterating and searching.", "Textual reasoning has inherent limitations in solving tasks with challenges in math, logics, optimization, and searching, which is unlikely to be solved by simply scaling up the model and data size.", "The recently released OpenAI GPT Code Interpreter and multi-agent frameworks such as AutoGen have demonstrated remarkable proficiency of integrating code generation and execution to solve complex tasks using LLMs."], "method": "To mitigate the above issues, we propose three methods to better steer LLM code/text generation and achieve a notable improvement.", "key_results": ["While a lot of recent research focuses on enhancing the textual reasoning capabilities of Large Language Models (LLMs) by optimizing the multi-agent framework or reasoning chains, several benchmark tasks can be solved with 100\\% success through direct coding, which is more scalable and avoids the computational overhead associated with textual iterating and searching.", "However, based on our experiments on 7 existing popular methods for steering code/text generation in both single- and multi-turn settings with 14 tasks and 6 types of LLMs (including the new O1-preview), currently there is no optimal method to correctly steer LLMs to write code when needed."], "limitations": ["Abstract-level evidence only: validate assumptions, evaluation protocol, and failure cases in the full paper before relying on this as key evidence.", "Textual reasoning has inherent limitations in solving tasks with challenges in math, logics, optimization, and searching, which is unlikely to be solved by simply scaling up the model and data size."], "bibkey": "Chen2024Steering"}
{"paper_id": "P0110", "title": "Testing and Understanding Erroneous Planning in LLM Agents through Synthesized User Inputs", "year": 2024, "url": "http://arxiv.org/abs/2404.17833v1", "arxiv_id": "2404.17833v1", "primary_category": "cs.AI", "categories": ["cs.AI", "cs.PL"], "pdf_url": "https://arxiv.org/pdf/2404.17833v1", "priority": "normal", "mapped_sections": ["4.1"], "evidence_level": "abstract", "fulltext_path": "", "authors": ["Zhenlan Ji", "Daoyuan Wu", "Pingchuan Ma", "Zongjie Li", "Shuai Wang"], "abstract": "Agents based on large language models (LLMs) have demonstrated effectiveness in solving a wide range of tasks by integrating LLMs with key modules such as planning, memory, and tool usage. Increasingly, customers are adopting LLM agents across a variety of commercial applications critical to reliability, including support for mental well-being, chemical synthesis, and software development. Nevertheless, our observations and daily use of LLM agents indicate that they are prone to making erroneous plans, especially when the tasks are complex and require long-term planning.\n  In this paper, we propose PDoctor, a novel and automated approach to testing LLM agents and understanding their erroneous planning. As the first work in this direction, we formulate the detection of erroneous planning as a constraint satisfiability problem: an LLM agent's plan is considered erroneous if its execution violates the constraints derived from the user inputs. To this end, PDoctor first defines a domain-specific language (DSL) for user queries and synthesizes varying inputs with the assistance of the Z3 constraint solver. These synthesized inputs are natural language paragraphs that specify the requirements for completing a series of tasks. Then, PDoctor derives constraints from these requirements to form a testing oracle. We evaluate PDoctor with three mainstream agent frameworks and two powerful LLMs (GPT-3.5 and GPT-4). The results show that PDoctor can effectively detect diverse errors in agent planning and provide insights and error characteristics that are valuable to both agent developers and users. We conclude by discussing potential alternative designs and directions to extend PDoctor.", "summary_bullets": ["Agents based on large language models (LLMs) have demonstrated effectiveness in solving a wide range of tasks by integrating LLMs with key modules such as planning, memory, and tool usage.", "Increasingly, customers are adopting LLM agents across a variety of commercial applications critical to reliability, including support for mental well-being, chemical synthesis, and software development.", "Nevertheless, our observations and daily use of LLM agents indicate that they are prone to making erroneous plans, especially when the tasks are complex and require long-term planning."], "method": "In this paper, we propose PDoctor, a novel and automated approach to testing LLM agents and understanding their erroneous planning.", "key_results": ["We evaluate PDoctor with three mainstream agent frameworks and two powerful LLMs (GPT-3.5 and GPT-4)."], "limitations": ["Abstract-level evidence only: validate assumptions, evaluation protocol, and failure cases in the full paper before relying on this as key evidence."], "bibkey": "Ji2024Testing"}
{"paper_id": "P0111", "title": "A Language Agent for Autonomous Driving", "year": 2023, "url": "http://arxiv.org/abs/2311.10813v4", "arxiv_id": "2311.10813v4", "primary_category": "cs.CV", "categories": ["cs.CV", "cs.AI", "cs.CL", "cs.RO"], "pdf_url": "https://arxiv.org/pdf/2311.10813v4", "priority": "normal", "mapped_sections": [], "evidence_level": "abstract", "fulltext_path": "", "authors": ["Jiageng Mao", "Junjie Ye", "Yuxi Qian", "Marco Pavone", "Yue Wang"], "abstract": "Human-level driving is an ultimate goal of autonomous driving. Conventional approaches formulate autonomous driving as a perception-prediction-planning framework, yet their systems do not capitalize on the inherent reasoning ability and experiential knowledge of humans. In this paper, we propose a fundamental paradigm shift from current pipelines, exploiting Large Language Models (LLMs) as a cognitive agent to integrate human-like intelligence into autonomous driving systems. Our approach, termed Agent-Driver, transforms the traditional autonomous driving pipeline by introducing a versatile tool library accessible via function calls, a cognitive memory of common sense and experiential knowledge for decision-making, and a reasoning engine capable of chain-of-thought reasoning, task planning, motion planning, and self-reflection. Powered by LLMs, our Agent-Driver is endowed with intuitive common sense and robust reasoning capabilities, thus enabling a more nuanced, human-like approach to autonomous driving. We evaluate our approach on the large-scale nuScenes benchmark, and extensive experiments substantiate that our Agent-Driver significantly outperforms the state-of-the-art driving methods by a large margin. Our approach also demonstrates superior interpretability and few-shot learning ability to these methods.", "summary_bullets": ["Human-level driving is an ultimate goal of autonomous driving.", "Conventional approaches formulate autonomous driving as a perception-prediction-planning framework, yet their systems do not capitalize on the inherent reasoning ability and experiential knowledge of humans.", "In this paper, we propose a fundamental paradigm shift from current pipelines, exploiting Large Language Models (LLMs) as a cognitive agent to integrate human-like intelligence into autonomous driving systems."], "method": "In this paper, we propose a fundamental paradigm shift from current pipelines, exploiting Large Language Models (LLMs) as a cognitive agent to integrate human-like intelligence into autonomous driving systems.", "key_results": ["We evaluate our approach on the large-scale nuScenes benchmark, and extensive experiments substantiate that our Agent-Driver significantly outperforms the state-of-the-art driving methods by a large margin.", "Human-level driving is an ultimate goal of autonomous driving."], "limitations": ["Abstract-level evidence only: validate assumptions, evaluation protocol, and failure cases in the full paper before relying on this as key evidence."], "bibkey": "Mao2023Language"}
{"paper_id": "P0112", "title": "A Survey on Evaluation of Large Language Models", "year": 2023, "url": "http://arxiv.org/abs/2307.03109v9", "arxiv_id": "2307.03109v9", "primary_category": "cs.CL", "categories": ["cs.CL", "cs.AI"], "pdf_url": "https://arxiv.org/pdf/2307.03109v9", "priority": "normal", "mapped_sections": ["6.1"], "evidence_level": "abstract", "fulltext_path": "", "authors": ["Yupeng Chang", "Xu Wang", "Jindong Wang", "Yuan Wu", "Linyi Yang", "Kaijie Zhu", "Hao Chen", "Xiaoyuan Yi", "Cunxiang Wang", "Yidong Wang", "Wei Ye", "Yue Zhang", "Yi Chang", "Philip S. Yu", "Qiang Yang", "Xing Xie"], "abstract": "Large language models (LLMs) are gaining increasing popularity in both academia and industry, owing to their unprecedented performance in various applications. As LLMs continue to play a vital role in both research and daily use, their evaluation becomes increasingly critical, not only at the task level, but also at the society level for better understanding of their potential risks. Over the past years, significant efforts have been made to examine LLMs from various perspectives. This paper presents a comprehensive review of these evaluation methods for LLMs, focusing on three key dimensions: what to evaluate, where to evaluate, and how to evaluate. Firstly, we provide an overview from the perspective of evaluation tasks, encompassing general natural language processing tasks, reasoning, medical usage, ethics, educations, natural and social sciences, agent applications, and other areas. Secondly, we answer the `where' and `how' questions by diving into the evaluation methods and benchmarks, which serve as crucial components in assessing performance of LLMs. Then, we summarize the success and failure cases of LLMs in different tasks. Finally, we shed light on several future challenges that lie ahead in LLMs evaluation. Our aim is to offer invaluable insights to researchers in the realm of LLMs evaluation, thereby aiding the development of more proficient LLMs. Our key point is that evaluation should be treated as an essential discipline to better assist the development of LLMs. We consistently maintain the related open-source materials at: https://github.com/MLGroupJLU/LLM-eval-survey.", "summary_bullets": ["Large language models (LLMs) are gaining increasing popularity in both academia and industry, owing to their unprecedented performance in various applications.", "As LLMs continue to play a vital role in both research and daily use, their evaluation becomes increasingly critical, not only at the task level, but also at the society level for better understanding of their potential risks.", "Over the past years, significant efforts have been made to examine LLMs from various perspectives."], "method": "Large language models (LLMs) are gaining increasing popularity in both academia and industry, owing to their unprecedented performance in various applications.", "key_results": ["As LLMs continue to play a vital role in both research and daily use, their evaluation becomes increasingly critical, not only at the task level, but also at the society level for better understanding of their potential risks.", "This paper presents a comprehensive review of these evaluation methods for LLMs, focusing on three key dimensions: what to evaluate, where to evaluate, and how to evaluate."], "limitations": ["Abstract-level evidence only: validate assumptions, evaluation protocol, and failure cases in the full paper before relying on this as key evidence."], "bibkey": "Chang2023Survey"}
{"paper_id": "P0113", "title": "AgentSims: An Open-Source Sandbox for Large Language Model Evaluation", "year": 2023, "url": "http://arxiv.org/abs/2308.04026v1", "arxiv_id": "2308.04026v1", "primary_category": "cs.AI", "categories": ["cs.AI"], "pdf_url": "https://arxiv.org/pdf/2308.04026v1", "priority": "normal", "mapped_sections": ["6.1"], "evidence_level": "abstract", "fulltext_path": "", "authors": ["Jiaju Lin", "Haoran Zhao", "Aochi Zhang", "Yiting Wu", "Huqiuyue Ping", "Qin Chen"], "abstract": "With ChatGPT-like large language models (LLM) prevailing in the community, how to evaluate the ability of LLMs is an open question. Existing evaluation methods suffer from following shortcomings: (1) constrained evaluation abilities, (2) vulnerable benchmarks, (3) unobjective metrics. We suggest that task-based evaluation, where LLM agents complete tasks in a simulated environment, is a one-for-all solution to solve above problems. We present AgentSims, an easy-to-use infrastructure for researchers from all disciplines to test the specific capacities they are interested in. Researchers can build their evaluation tasks by adding agents and buildings on an interactive GUI or deploy and test new support mechanisms, i.e. memory, planning and tool-use systems, by a few lines of codes. Our demo is available at https://agentsims.com .", "summary_bullets": ["With ChatGPT-like large language models (LLM) prevailing in the community, how to evaluate the ability of LLMs is an open question.", "Existing evaluation methods suffer from following shortcomings: (1) constrained evaluation abilities, (2) vulnerable benchmarks, (3) unobjective metrics.", "We suggest that task-based evaluation, where LLM agents complete tasks in a simulated environment, is a one-for-all solution to solve above problems."], "method": "We present AgentSims, an easy-to-use infrastructure for researchers from all disciplines to test the specific capacities they are interested in.", "key_results": ["Existing evaluation methods suffer from following shortcomings: (1) constrained evaluation abilities, (2) vulnerable benchmarks, (3) unobjective metrics.", "We suggest that task-based evaluation, where LLM agents complete tasks in a simulated environment, is a one-for-all solution to solve above problems."], "limitations": ["Abstract-level evidence only: validate assumptions, evaluation protocol, and failure cases in the full paper before relying on this as key evidence."], "bibkey": "Lin2023Agentsims"}
{"paper_id": "P0114", "title": "Improving Planning with Large Language Models: A Modular Agentic Architecture", "year": 2023, "url": "http://arxiv.org/abs/2310.00194v5", "arxiv_id": "2310.00194v5", "primary_category": "cs.AI", "categories": ["cs.AI", "cs.NE"], "pdf_url": "https://arxiv.org/pdf/2310.00194v5", "priority": "normal", "mapped_sections": ["4.1"], "evidence_level": "abstract", "fulltext_path": "", "authors": ["Taylor Webb", "Shanka Subhra Mondal", "Ida Momennejad"], "abstract": "Large language models (LLMs) demonstrate impressive performance on a wide variety of tasks, but they often struggle with tasks that require multi-step reasoning or goal-directed planning. Both cognitive neuroscience and reinforcement learning (RL) have proposed a number of interacting functional components that together implement search and evaluation in multi-step decision making. These components include conflict monitoring, state prediction, state evaluation, task decomposition, and orchestration. To improve planning with LLMs, we propose an agentic architecture, the Modular Agentic Planner (MAP), in which planning is accomplished via the recurrent interaction of the specialized modules mentioned above, each implemented using an LLM. MAP improves planning through the interaction of specialized modules that break down a larger problem into multiple brief automated calls to the LLM. We evaluate MAP on three challenging planning tasks -- graph traversal, Tower of Hanoi, and the PlanBench benchmark -- as well as an NLP task requiring multi-step reasoning (strategyQA). We find that MAP yields significant improvements over both standard LLM methods (zero-shot prompting, in-context learning) and competitive baselines (chain-of-thought, multi-agent debate, and tree-of-thought), can be effectively combined with smaller and more cost-efficient LLMs (Llama3-70B), and displays superior transfer across tasks. These results suggest the benefit of a modular and multi-agent approach to planning with LLMs.", "summary_bullets": ["Large language models (LLMs) demonstrate impressive performance on a wide variety of tasks, but they often struggle with tasks that require multi-step reasoning or goal-directed planning.", "Both cognitive neuroscience and reinforcement learning (RL) have proposed a number of interacting functional components that together implement search and evaluation in multi-step decision making.", "These components include conflict monitoring, state prediction, state evaluation, task decomposition, and orchestration."], "method": "To improve planning with LLMs, we propose an agentic architecture, the Modular Agentic Planner (MAP), in which planning is accomplished via the recurrent interaction of the specialized modules mentioned above, each implemented using an LLM.", "key_results": ["Both cognitive neuroscience and reinforcement learning (RL) have proposed a number of interacting functional components that together implement search and evaluation in multi-step decision making.", "These components include conflict monitoring, state prediction, state evaluation, task decomposition, and orchestration."], "limitations": ["Abstract-level evidence only: validate assumptions, evaluation protocol, and failure cases in the full paper before relying on this as key evidence."], "bibkey": "Webb2023Improving"}
{"paper_id": "P0115", "title": "MAgIC: Investigation of Large Language Model Powered Multi-Agent in Cognition, Adaptability, Rationality and Collaboration", "year": 2023, "url": "http://arxiv.org/abs/2311.08562v3", "arxiv_id": "2311.08562v3", "primary_category": "cs.CL", "categories": ["cs.CL"], "pdf_url": "https://arxiv.org/pdf/2311.08562v3", "priority": "normal", "mapped_sections": ["5.2"], "evidence_level": "abstract", "fulltext_path": "", "authors": ["Lin Xu", "Zhiyuan Hu", "Daquan Zhou", "Hongyu Ren", "Zhen Dong", "Kurt Keutzer", "See Kiong Ng", "Jiashi Feng"], "abstract": "Large Language Models (LLMs) have significantly advanced natural language processing, demonstrating exceptional reasoning, tool usage, and memory capabilities. As their applications expand into multi-agent environments, there arises a need for a comprehensive evaluation framework that captures LLMs' reasoning, planning, collaboration, and other social abilities. This work introduces a novel competition-based benchmark framework specifically designed to assess LLMs within multi-agent settings, providing quantitative metrics to evaluate their judgment, reasoning, deception, self-awareness, cooperation, coordination, and rationality. We utilize two social deduction games alongside three game-theory scenarios to create diverse environments. Our frame is fortified with the probabilistic graphic modeling (PGM) method, enhancing the LLMs' capabilities in navigating complex social and cognitive dimensions. We evaluate seven LLMs, quantitatively highlighting a significant capability gap of over threefold between the strongest, GPT o1, and the weakest, Llama-2-70B. It also confirms that our PGM enhancement boosts the abilities of all selected models by an average of 37%. Our data and code can be found here https://github.com/cathyxl/MAgIC.", "summary_bullets": ["Large Language Models (LLMs) have significantly advanced natural language processing, demonstrating exceptional reasoning, tool usage, and memory capabilities.", "As their applications expand into multi-agent environments, there arises a need for a comprehensive evaluation framework that captures LLMs' reasoning, planning, collaboration, and other social abilities.", "This work introduces a novel competition-based benchmark framework specifically designed to assess LLMs within multi-agent settings, providing quantitative metrics to evaluate their judgment, reasoning, deception, self-awareness, cooperation, coordination, and rationality."], "method": "Large Language Models (LLMs) have significantly advanced natural language processing, demonstrating exceptional reasoning, tool usage, and memory capabilities.", "key_results": ["We evaluate seven LLMs, quantitatively highlighting a significant capability gap of over threefold between the strongest, GPT o1, and the weakest, Llama-2-70B.", "It also confirms that our PGM enhancement boosts the abilities of all selected models by an average of 37%."], "limitations": ["Abstract-level evidence only: validate assumptions, evaluation protocol, and failure cases in the full paper before relying on this as key evidence."], "bibkey": "Xu2023Magic"}
{"paper_id": "P0116", "title": "ModelScope-Agent: Building Your Customizable Agent System with Open-source Large Language Models", "year": 2023, "url": "http://arxiv.org/abs/2309.00986v1", "arxiv_id": "2309.00986v1", "primary_category": "cs.CL", "categories": ["cs.CL"], "pdf_url": "https://arxiv.org/pdf/2309.00986v1", "priority": "high", "mapped_sections": ["3.1", "5.2", "6.1", "6.2"], "evidence_level": "abstract", "fulltext_path": "", "authors": ["Chenliang Li", "Hehong Chen", "Ming Yan", "Weizhou Shen", "Haiyang Xu", "Zhikai Wu", "Zhicheng Zhang", "Wenmeng Zhou", "Yingda Chen", "Chen Cheng", "Hongzhu Shi", "Ji Zhang", "Fei Huang", "Jingren Zhou"], "abstract": "Large language models (LLMs) have recently demonstrated remarkable capabilities to comprehend human intentions, engage in reasoning, and design planning-like behavior. To further unleash the power of LLMs to accomplish complex tasks, there is a growing trend to build agent framework that equips LLMs, such as ChatGPT, with tool-use abilities to connect with massive external APIs. In this work, we introduce ModelScope-Agent, a general and customizable agent framework for real-world applications, based on open-source LLMs as controllers. It provides a user-friendly system library, with customizable engine design to support model training on multiple open-source LLMs, while also enabling seamless integration with both model APIs and common APIs in a unified way. To equip the LLMs with tool-use abilities, a comprehensive framework has been proposed spanning over tool-use data collection, tool retrieval, tool registration, memory control, customized model training, and evaluation for practical real-world applications. Finally, we showcase ModelScopeGPT, a real-world intelligent assistant of ModelScope Community based on the ModelScope-Agent framework, which is able to connect open-source LLMs with more than 1000 public AI models and localized community knowledge in ModelScope. The ModelScope-Agent library\\footnote{https://github.com/modelscope/modelscope-agent} and online demo\\footnote{https://modelscope.cn/studios/damo/ModelScopeGPT/summary} are now publicly available.", "summary_bullets": ["Large language models (LLMs) have recently demonstrated remarkable capabilities to comprehend human intentions, engage in reasoning, and design planning-like behavior.", "To further unleash the power of LLMs to accomplish complex tasks, there is a growing trend to build agent framework that equips LLMs, such as ChatGPT, with tool-use abilities to connect with massive external APIs.", "In this work, we introduce ModelScope-Agent, a general and customizable agent framework for real-world applications, based on open-source LLMs as controllers.", "It provides a user-friendly system library, with customizable engine design to support model training on multiple open-source LLMs, while also enabling seamless integration with both model APIs and common APIs in a unified way.", "To equip the LLMs with tool-use abilities, a comprehensive framework has been proposed spanning over tool-use data collection, tool retrieval, tool registration, memory control, customized model training, and evaluation for practical real-world applications."], "method": "In this work, we introduce ModelScope-Agent, a general and customizable agent framework for real-world applications, based on open-source LLMs as controllers.", "key_results": ["Finally, we showcase ModelScopeGPT, a real-world intelligent assistant of ModelScope Community based on the ModelScope-Agent framework, which is able to connect open-source LLMs with more than 1000 public AI models and localized community knowledge in ModelScope.", "Large language models (LLMs) have recently demonstrated remarkable capabilities to comprehend human intentions, engage in reasoning, and design planning-like behavior."], "limitations": ["Abstract-level evidence only: validate assumptions, evaluation protocol, and failure cases in the full paper before relying on this as key evidence."], "bibkey": "Li2023Modelscope"}
{"paper_id": "P0117", "title": "NavGPT: Explicit Reasoning in Vision-and-Language Navigation with Large Language Models", "year": 2023, "url": "http://arxiv.org/abs/2305.16986v3", "arxiv_id": "2305.16986v3", "primary_category": "cs.CV", "categories": ["cs.CV", "cs.AI", "cs.CL", "cs.RO"], "pdf_url": "https://arxiv.org/pdf/2305.16986v3", "priority": "normal", "mapped_sections": ["4.1"], "evidence_level": "abstract", "fulltext_path": "", "authors": ["Gengze Zhou", "Yicong Hong", "Qi Wu"], "abstract": "Trained with an unprecedented scale of data, large language models (LLMs) like ChatGPT and GPT-4 exhibit the emergence of significant reasoning abilities from model scaling. Such a trend underscored the potential of training LLMs with unlimited language data, advancing the development of a universal embodied agent. In this work, we introduce the NavGPT, a purely LLM-based instruction-following navigation agent, to reveal the reasoning capability of GPT models in complex embodied scenes by performing zero-shot sequential action prediction for vision-and-language navigation (VLN). At each step, NavGPT takes the textual descriptions of visual observations, navigation history, and future explorable directions as inputs to reason the agent's current status, and makes the decision to approach the target. Through comprehensive experiments, we demonstrate NavGPT can explicitly perform high-level planning for navigation, including decomposing instruction into sub-goal, integrating commonsense knowledge relevant to navigation task resolution, identifying landmarks from observed scenes, tracking navigation progress, and adapting to exceptions with plan adjustment. Furthermore, we show that LLMs is capable of generating high-quality navigational instructions from observations and actions along a path, as well as drawing accurate top-down metric trajectory given the agent's navigation history. Despite the performance of using NavGPT to zero-shot R2R tasks still falling short of trained models, we suggest adapting multi-modality inputs for LLMs to use as visual navigation agents and applying the explicit reasoning of LLMs to benefit learning-based models.", "summary_bullets": ["Trained with an unprecedented scale of data, large language models (LLMs) like ChatGPT and GPT-4 exhibit the emergence of significant reasoning abilities from model scaling.", "Such a trend underscored the potential of training LLMs with unlimited language data, advancing the development of a universal embodied agent.", "In this work, we introduce the NavGPT, a purely LLM-based instruction-following navigation agent, to reveal the reasoning capability of GPT models in complex embodied scenes by performing zero-shot sequential action prediction for vision-and-language navigation (VLN)."], "method": "In this work, we introduce the NavGPT, a purely LLM-based instruction-following navigation agent, to reveal the reasoning capability of GPT models in complex embodied scenes by performing zero-shot sequential action prediction for vision-and-language navigation (VLN).", "key_results": ["Trained with an unprecedented scale of data, large language models (LLMs) like ChatGPT and GPT-4 exhibit the emergence of significant reasoning abilities from model scaling.", "Furthermore, we show that LLMs is capable of generating high-quality navigational instructions from observations and actions along a path, as well as drawing accurate top-down metric trajectory given the agent's navigation history."], "limitations": ["Abstract-level evidence only: validate assumptions, evaluation protocol, and failure cases in the full paper before relying on this as key evidence."], "bibkey": "Zhou2023Navgpt"}
{"paper_id": "P0118", "title": "Reason for Future, Act for Now: A Principled Framework for Autonomous LLM Agents with Provable Sample Efficiency", "year": 2023, "url": "http://arxiv.org/abs/2309.17382v3", "arxiv_id": "2309.17382v3", "primary_category": "cs.AI", "categories": ["cs.AI", "cs.LG"], "pdf_url": "https://arxiv.org/pdf/2309.17382v3", "priority": "normal", "mapped_sections": ["4.2"], "evidence_level": "abstract", "fulltext_path": "", "authors": ["Zhihan Liu", "Hao Hu", "Shenao Zhang", "Hongyi Guo", "Shuqi Ke", "Boyi Liu", "Zhaoran Wang"], "abstract": "Large language models (LLMs) demonstrate impressive reasoning abilities, but translating reasoning into actions in the real world remains challenging. In particular, it remains unclear how to complete a given task provably within a minimum number of interactions with the external environment, e.g., through an internal mechanism of reasoning. To this end, we propose a principled framework with provable regret guarantees to orchestrate reasoning and acting, which we call \"reason for future, act for now\" (\\texttt{RAFA}). Specifically, we design a prompt template for reasoning that learns from the memory buffer and plans a future trajectory over a long horizon (\"reason for future\"). At each step, the LLM agent takes the initial action of the planned trajectory (\"act for now\"), stores the collected feedback in the memory buffer, and reinvokes the reasoning routine to replan the future trajectory from the new state.\n  The key idea is to cast reasoning in LLMs as learning and planning in Bayesian adaptive Markov decision processes (MDPs). Correspondingly, we prompt LLMs to form an updated posterior of the unknown environment from the memory buffer (learning) and generate an optimal trajectory for multiple future steps that maximizes a value function (planning). The learning and planning subroutines are performed in an \"in-context\" manner to emulate the actor-critic update for MDPs. Our theoretical analysis proves that the novel combination of long-term reasoning and short-term acting achieves a $\\sqrt{T}$ regret. Here, $T$ denotes the number of online interactions. In particular, the regret bound highlights an intriguing interplay between the prior knowledge obtained through pretraining and the uncertainty reduction achieved by reasoning and acting. Our empirical validation shows that it outperforms various existing frameworks and achieves nearly perfect scores on a few benchmarks.", "summary_bullets": ["Large language models (LLMs) demonstrate impressive reasoning abilities, but translating reasoning into actions in the real world remains challenging.", "In particular, it remains unclear how to complete a given task provably within a minimum number of interactions with the external environment, e.g., through an internal mechanism of reasoning.", "To this end, we propose a principled framework with provable regret guarantees to orchestrate reasoning and acting, which we call \"reason for future, act for now\" (\\texttt{RAFA})."], "method": "To this end, we propose a principled framework with provable regret guarantees to orchestrate reasoning and acting, which we call \"reason for future, act for now\" (\\texttt{RAFA}).", "key_results": ["Our empirical validation shows that it outperforms various existing frameworks and achieves nearly perfect scores on a few benchmarks."], "limitations": ["Abstract-level evidence only: validate assumptions, evaluation protocol, and failure cases in the full paper before relying on this as key evidence."], "bibkey": "Liu2023Reason"}
{"paper_id": "P0119", "title": "Towards Reasoning in Large Language Models via Multi-Agent Peer Review Collaboration", "year": 2023, "url": "http://arxiv.org/abs/2311.08152v2", "arxiv_id": "2311.08152v2", "primary_category": "cs.CL", "categories": ["cs.CL"], "pdf_url": "https://arxiv.org/pdf/2311.08152v2", "priority": "normal", "mapped_sections": ["5.2"], "evidence_level": "abstract", "fulltext_path": "", "authors": ["Zhenran Xu", "Senbao Shi", "Baotian Hu", "Jindi Yu", "Dongfang Li", "Min Zhang", "Yuxiang Wu"], "abstract": "Large Language Models (LLMs) have shown remarkable capabilities in general natural language processing tasks but often fall short in complex reasoning tasks. Recent studies have explored human-like problem-solving strategies, such as self-correct, to push further the boundary of single-model reasoning ability. In this work, we let a single model \"step outside the box\" by engaging multiple models to correct each other. We introduce a multi-agent collaboration strategy that emulates the academic peer review process. Each agent independently constructs its own solution, provides reviews on the solutions of others, and assigns confidence levels to its reviews. Upon receiving peer reviews, agents revise their initial solutions. Extensive experiments on three different types of reasoning tasks show that our collaboration approach delivers superior accuracy across all ten datasets compared to existing methods. Further study underscores the effectiveness of integrating confidence in reviews, demonstrates the superiority of feedback exchange over mere solution sharing, and highlights the role of capability and diversity in fostering successful collaboration.", "summary_bullets": ["Large Language Models (LLMs) have shown remarkable capabilities in general natural language processing tasks but often fall short in complex reasoning tasks.", "Recent studies have explored human-like problem-solving strategies, such as self-correct, to push further the boundary of single-model reasoning ability.", "In this work, we let a single model \"step outside the box\" by engaging multiple models to correct each other."], "method": "We introduce a multi-agent collaboration strategy that emulates the academic peer review process.", "key_results": ["Recent studies have explored human-like problem-solving strategies, such as self-correct, to push further the boundary of single-model reasoning ability.", "Extensive experiments on three different types of reasoning tasks show that our collaboration approach delivers superior accuracy across all ten datasets compared to existing methods."], "limitations": ["Abstract-level evidence only: validate assumptions, evaluation protocol, and failure cases in the full paper before relying on this as key evidence."], "bibkey": "Xu2023Towards"}
{"paper_id": "P0120", "title": "Active Context Compression: Autonomous Memory Management in LLM Agents", "year": 2026, "url": "http://arxiv.org/abs/2601.07190v1", "arxiv_id": "2601.07190v1", "primary_category": "cs.AI", "categories": ["cs.AI"], "pdf_url": "https://arxiv.org/pdf/2601.07190v1", "priority": "normal", "mapped_sections": ["4.2"], "evidence_level": "abstract", "fulltext_path": "", "authors": ["Nikhil Verma"], "abstract": "Large Language Model (LLM) agents struggle with long-horizon software engineering tasks due to \"Context Bloat.\" As interaction history grows, computational costs explode, latency increases, and reasoning capabilities degrade due to distraction by irrelevant past errors. Existing solutions often rely on passive, external summarization mechanisms that the agent cannot control. This paper proposes Focus, an agent-centric architecture inspired by the biological exploration strategies of Physarum polycephalum (slime mold). The Focus Agent autonomously decides when to consolidate key learnings into a persistent \"Knowledge\" block and actively withdraws (prunes) the raw interaction history. Using an optimized scaffold matching industry best practices (persistent bash + string-replacement editor), we evaluated Focus on N=5 context-intensive instances from SWE-bench Lite using Claude Haiku 4.5. With aggressive prompting that encourages frequent compression, Focus achieves 22.7% token reduction (14.9M -> 11.5M tokens) while maintaining identical accuracy (3/5 = 60% for both agents). Focus performed 6.0 autonomous compressions per task on average, with token savings up to 57% on individual instances. We demonstrate that capable models can autonomously self-regulate their context when given appropriate tools and prompting, opening pathways for cost-aware agentic systems without sacrificing task performance.", "summary_bullets": ["Large Language Model (LLM) agents struggle with long-horizon software engineering tasks due to \"Context Bloat.\" As interaction history grows, computational costs explode, latency increases, and reasoning capabilities degrade due to distraction by irrelevant past errors.", "Existing solutions often rely on passive, external summarization mechanisms that the agent cannot control.", "This paper proposes Focus, an agent-centric architecture inspired by the biological exploration strategies of Physarum polycephalum (slime mold)."], "method": "Large Language Model (LLM) agents struggle with long-horizon software engineering tasks due to \"Context Bloat.\" As interaction history grows, computational costs explode, latency increases, and reasoning capabilities degrade due to distraction by irrelevant past errors.", "key_results": ["With aggressive prompting that encourages frequent compression, Focus achieves 22.7% token reduction (14.9M -> 11.5M tokens) while maintaining identical accuracy (3/5 = 60% for both agents).", "Using an optimized scaffold matching industry best practices (persistent bash + string-replacement editor), we evaluated Focus on N=5 context-intensive instances from SWE-bench Lite using Claude Haiku 4.5."], "limitations": ["Abstract-level evidence only: validate assumptions, evaluation protocol, and failure cases in the full paper before relying on this as key evidence."], "bibkey": "Verma2026Active"}
{"paper_id": "P0121", "title": "Agentic Memory: Learning Unified Long-Term and Short-Term Memory Management for Large Language Model Agents", "year": 2026, "url": "http://arxiv.org/abs/2601.01885v1", "arxiv_id": "2601.01885v1", "primary_category": "cs.CL", "categories": ["cs.CL"], "pdf_url": "https://arxiv.org/pdf/2601.01885v1", "priority": "normal", "mapped_sections": ["4.2"], "evidence_level": "abstract", "fulltext_path": "", "authors": ["Yi Yu", "Liuyi Yao", "Yuexiang Xie", "Qingquan Tan", "Jiaqi Feng", "Yaliang Li", "Libing Wu"], "abstract": "Large language model (LLM) agents face fundamental limitations in long-horizon reasoning due to finite context windows, making effective memory management critical. Existing methods typically handle long-term memory (LTM) and short-term memory (STM) as separate components, relying on heuristics or auxiliary controllers, which limits adaptability and end-to-end optimization. In this paper, we propose Agentic Memory (AgeMem), a unified framework that integrates LTM and STM management directly into the agent's policy. AgeMem exposes memory operations as tool-based actions, enabling the LLM agent to autonomously decide what and when to store, retrieve, update, summarize, or discard information. To train such unified behaviors, we propose a three-stage progressive reinforcement learning strategy and design a step-wise GRPO to address sparse and discontinuous rewards induced by memory operations. Experiments on five long-horizon benchmarks demonstrate that AgeMem consistently outperforms strong memory-augmented baselines across multiple LLM backbones, achieving improved task performance, higher-quality long-term memory, and more efficient context usage.", "summary_bullets": ["Large language model (LLM) agents face fundamental limitations in long-horizon reasoning due to finite context windows, making effective memory management critical.", "Existing methods typically handle long-term memory (LTM) and short-term memory (STM) as separate components, relying on heuristics or auxiliary controllers, which limits adaptability and end-to-end optimization.", "In this paper, we propose Agentic Memory (AgeMem), a unified framework that integrates LTM and STM management directly into the agent's policy."], "method": "In this paper, we propose Agentic Memory (AgeMem), a unified framework that integrates LTM and STM management directly into the agent's policy.", "key_results": ["Experiments on five long-horizon benchmarks demonstrate that AgeMem consistently outperforms strong memory-augmented baselines across multiple LLM backbones, achieving improved task performance, higher-quality long-term memory, and more efficient context usage."], "limitations": ["Abstract-level evidence only: validate assumptions, evaluation protocol, and failure cases in the full paper before relying on this as key evidence.", "Large language model (LLM) agents face fundamental limitations in long-horizon reasoning due to finite context windows, making effective memory management critical."], "bibkey": "Yu2026Agentic"}
{"paper_id": "P0122", "title": "Beyond Perfect APIs: A Comprehensive Evaluation of LLM Agents Under Real-World API Complexity", "year": 2026, "url": "http://arxiv.org/abs/2601.00268v1", "arxiv_id": "2601.00268v1", "primary_category": "cs.CL", "categories": ["cs.CL", "cs.AI"], "pdf_url": "https://arxiv.org/pdf/2601.00268v1", "priority": "normal", "mapped_sections": ["6.1"], "evidence_level": "abstract", "fulltext_path": "", "authors": ["Doyoung Kim", "Zhiwei Ren", "Jie Hao", "Zhongkai Sun", "Lichao Wang", "Xiyao Ma", "Zack Ye", "Xu Han", "Jun Yin", "Heng Ji", "Wei Shen", "Xing Fan", "Benjamin Yao", "Chenlei Guo"], "abstract": "We introduce WildAGTEval, a benchmark designed to evaluate large language model (LLM) agents' function-calling capabilities under realistic API complexity. Unlike prior work that assumes an idealized API system and disregards real-world factors such as noisy API outputs, WildAGTEval accounts for two dimensions of real-world complexity: 1. API specification, which includes detailed documentation and usage constraints, and 2. API execution, which captures runtime challenges. Consequently, WildAGTEval offers (i) an API system encompassing 60 distinct complexity scenarios that can be composed into approximately 32K test configurations, and (ii) user-agent interactions for evaluating LLM agents on these scenarios. Using WildAGTEval, we systematically assess several advanced LLMs and observe that most scenarios are challenging, with irrelevant information complexity posing the greatest difficulty and reducing the performance of strong LLMs by 27.3%. Furthermore, our qualitative analysis reveals that LLMs occasionally distort user intent merely to claim task completion, critically affecting user satisfaction.", "summary_bullets": ["We introduce WildAGTEval, a benchmark designed to evaluate large language model (LLM) agents' function-calling capabilities under realistic API complexity.", "Unlike prior work that assumes an idealized API system and disregards real-world factors such as noisy API outputs, WildAGTEval accounts for two dimensions of real-world complexity: 1.", "API specification, which includes detailed documentation and usage constraints, and 2."], "method": "We introduce WildAGTEval, a benchmark designed to evaluate large language model (LLM) agents' function-calling capabilities under realistic API complexity.", "key_results": ["Unlike prior work that assumes an idealized API system and disregards real-world factors such as noisy API outputs, WildAGTEval accounts for two dimensions of real-world complexity: 1.", "API specification, which includes detailed documentation and usage constraints, and 2."], "limitations": ["Abstract-level evidence only: validate assumptions, evaluation protocol, and failure cases in the full paper before relying on this as key evidence."], "bibkey": "Kim2026Beyond"}
{"paper_id": "P0123", "title": "EComStage: Stage-wise and Orientation-specific Benchmarking for Large Language Models in E-commerce", "year": 2026, "url": "http://arxiv.org/abs/2601.02752v1", "arxiv_id": "2601.02752v1", "primary_category": "cs.CL", "categories": ["cs.CL"], "pdf_url": "https://arxiv.org/pdf/2601.02752v1", "priority": "normal", "mapped_sections": [], "evidence_level": "abstract", "fulltext_path": "", "authors": ["Kaiyan Zhao", "Zijie Meng", "Zheyong Xie", "Jin Duan", "Yao Hu", "Zuozhu Liu", "Shaosheng Cao"], "abstract": "Large Language Model (LLM)-based agents are increasingly deployed in e-commerce applications to assist customer services in tasks such as product inquiries, recommendations, and order management. Existing benchmarks primarily evaluate whether these agents successfully complete the final task, overlooking the intermediate reasoning stages that are crucial for effective decision-making. To address this gap, we propose EComStage, a unified benchmark for evaluating agent-capable LLMs across the comprehensive stage-wise reasoning process: Perception (understanding user intent), Planning (formulating an action plan), and Action (executing the decision). EComStage evaluates LLMs through seven separate representative tasks spanning diverse e-commerce scenarios, with all samples human-annotated and quality-checked. Unlike prior benchmarks that focus only on customer-oriented interactions, EComStage also evaluates merchant-oriented scenarios, including promotion management, content review, and operational support relevant to real-world applications. We evaluate a wide range of over 30 LLMs, spanning from 1B to over 200B parameters, including open-source models and closed-source APIs, revealing stage/orientation-specific strengths and weaknesses. Our results provide fine-grained, actionable insights for designing and optimizing LLM-based agents in real-world e-commerce settings.", "summary_bullets": ["Large Language Model (LLM)-based agents are increasingly deployed in e-commerce applications to assist customer services in tasks such as product inquiries, recommendations, and order management.", "Existing benchmarks primarily evaluate whether these agents successfully complete the final task, overlooking the intermediate reasoning stages that are crucial for effective decision-making.", "To address this gap, we propose EComStage, a unified benchmark for evaluating agent-capable LLMs across the comprehensive stage-wise reasoning process: Perception (understanding user intent), Planning (formulating an action plan), and Action (executing the decision)."], "method": "To address this gap, we propose EComStage, a unified benchmark for evaluating agent-capable LLMs across the comprehensive stage-wise reasoning process: Perception (understanding user intent), Planning (formulating an action plan), and Action (executing the decision).", "key_results": ["We evaluate a wide range of over 30 LLMs, spanning from 1B to over 200B parameters, including open-source models and closed-source APIs, revealing stage/orientation-specific strengths and weaknesses.", "Existing benchmarks primarily evaluate whether these agents successfully complete the final task, overlooking the intermediate reasoning stages that are crucial for effective decision-making."], "limitations": ["Abstract-level evidence only: validate assumptions, evaluation protocol, and failure cases in the full paper before relying on this as key evidence."], "bibkey": "Zhao2026Ecomstage"}
{"paper_id": "P0124", "title": "EvoRoute: Experience-Driven Self-Routing LLM Agent Systems", "year": 2026, "url": "http://arxiv.org/abs/2601.02695v1", "arxiv_id": "2601.02695v1", "primary_category": "cs.CL", "categories": ["cs.CL", "cs.MA"], "pdf_url": "https://arxiv.org/pdf/2601.02695v1", "priority": "normal", "mapped_sections": ["3.1", "6.1"], "evidence_level": "abstract", "fulltext_path": "", "authors": ["Guibin Zhang", "Haiyang Yu", "Kaiming Yang", "Bingli Wu", "Fei Huang", "Yongbin Li", "Shuicheng Yan"], "abstract": "Complex agentic AI systems, powered by a coordinated ensemble of Large Language Models (LLMs), tool and memory modules, have demonstrated remarkable capabilities on intricate, multi-turn tasks. However, this success is shadowed by prohibitive economic costs and severe latency, exposing a critical, yet underexplored, trade-off. We formalize this challenge as the \\textbf{Agent System Trilemma}: the inherent tension among achieving state-of-the-art performance, minimizing monetary cost, and ensuring rapid task completion. To dismantle this trilemma, we introduce EvoRoute, a self-evolving model routing paradigm that transcends static, pre-defined model assignments. Leveraging an ever-expanding knowledge base of prior experience, EvoRoute dynamically selects Pareto-optimal LLM backbones at each step, balancing accuracy, efficiency, and resource use, while continually refining its own selection policy through environment feedback. Experiments on challenging agentic benchmarks such as GAIA and BrowseComp+ demonstrate that EvoRoute, when integrated into off-the-shelf agentic systems, not only sustains or enhances system performance but also reduces execution cost by up to $80\\%$ and latency by over $70\\%$.", "summary_bullets": ["Complex agentic AI systems, powered by a coordinated ensemble of Large Language Models (LLMs), tool and memory modules, have demonstrated remarkable capabilities on intricate, multi-turn tasks.", "However, this success is shadowed by prohibitive economic costs and severe latency, exposing a critical, yet underexplored, trade-off.", "We formalize this challenge as the \\textbf{Agent System Trilemma}: the inherent tension among achieving state-of-the-art performance, minimizing monetary cost, and ensuring rapid task completion."], "method": "To dismantle this trilemma, we introduce EvoRoute, a self-evolving model routing paradigm that transcends static, pre-defined model assignments.", "key_results": ["Experiments on challenging agentic benchmarks such as GAIA and BrowseComp+ demonstrate that EvoRoute, when integrated into off-the-shelf agentic systems, not only sustains or enhances system performance but also reduces execution cost by up to $80\\%$ and latency by over $70\\%$.", "However, this success is shadowed by prohibitive economic costs and severe latency, exposing a critical, yet underexplored, trade-off."], "limitations": ["Abstract-level evidence only: validate assumptions, evaluation protocol, and failure cases in the full paper before relying on this as key evidence."], "bibkey": "Zhang2026Evoroute"}
{"paper_id": "P0125", "title": "LLM Agent Framework for Intelligent Change Analysis in Urban Environment using Remote Sensing Imagery", "year": 2026, "url": "http://arxiv.org/abs/2601.02757v1", "arxiv_id": "2601.02757v1", "primary_category": "cs.AI", "categories": ["cs.AI"], "pdf_url": "https://arxiv.org/pdf/2601.02757v1", "priority": "normal", "mapped_sections": [], "evidence_level": "abstract", "fulltext_path": "", "authors": ["Zixuan Xiao", "Jun Ma"], "abstract": "Existing change detection methods often lack the versatility to handle diverse real-world queries and the intelligence for comprehensive analysis. This paper presents a general agent framework, integrating Large Language Models (LLM) with vision foundation models to form ChangeGPT. A hierarchical structure is employed to mitigate hallucination. The agent was evaluated on a curated dataset of 140 questions categorized by real-world scenarios, encompassing various question types (e.g., Size, Class, Number) and complexities. The evaluation assessed the agent's tool selection ability (Precision/Recall) and overall query accuracy (Match). ChangeGPT, especially with a GPT-4-turbo backend, demonstrated superior performance, achieving a 90.71 % Match rate. Its strength lies particularly in handling change-related queries requiring multi-step reasoning and robust tool selection. Practical effectiveness was further validated through a real-world urban change monitoring case study in Qianhai Bay, Shenzhen. By providing intelligence, adaptability, and multi-type change analysis, ChangeGPT offers a powerful solution for decision-making in remote sensing applications.", "summary_bullets": ["Existing change detection methods often lack the versatility to handle diverse real-world queries and the intelligence for comprehensive analysis.", "This paper presents a general agent framework, integrating Large Language Models (LLM) with vision foundation models to form ChangeGPT.", "A hierarchical structure is employed to mitigate hallucination."], "method": "Existing change detection methods often lack the versatility to handle diverse real-world queries and the intelligence for comprehensive analysis.", "key_results": ["The agent was evaluated on a curated dataset of 140 questions categorized by real-world scenarios, encompassing various question types (e.g., Size, Class, Number) and complexities.", "ChangeGPT, especially with a GPT-4-turbo backend, demonstrated superior performance, achieving a 90.71 % Match rate."], "limitations": ["Abstract-level evidence only: validate assumptions, evaluation protocol, and failure cases in the full paper before relying on this as key evidence."], "bibkey": "Xiao2026Agent"}
{"paper_id": "P0126", "title": "LLM Agents in Law: Taxonomy, Applications, and Challenges", "year": 2026, "url": "http://arxiv.org/abs/2601.06216v1", "arxiv_id": "2601.06216v1", "primary_category": "cs.CY", "categories": ["cs.CY", "cs.AI"], "pdf_url": "https://arxiv.org/pdf/2601.06216v1", "priority": "normal", "mapped_sections": ["6.2"], "evidence_level": "abstract", "fulltext_path": "", "authors": ["Shuang Liu", "Ruijia Zhang", "Ruoyun Ma", "Yujia Deng", "Lanyi Zhu", "Jiayu Li", "Zelong Li", "Zhibin Shen", "Mengnan Du"], "abstract": "Large language models (LLMs) have precipitated a dramatic improvement in the legal domain, yet the deployment of standalone models faces significant limitations regarding hallucination, outdated information, and verifiability. Recently, LLM agents have attracted significant attention as a solution to these challenges, utilizing advanced capabilities such as planning, memory, and tool usage to meet the rigorous standards of legal practice. In this paper, we present a comprehensive survey of LLM agents for legal tasks, analyzing how these architectures bridge the gap between technical capabilities and domain-specific needs. Our major contributions include: (1) systematically analyzing the technical transition from standard legal LLMs to legal agents; (2) presenting a structured taxonomy of current agent applications across distinct legal practice areas; (3) discussing evaluation methodologies specifically for agentic performance in law; and (4) identifying open challenges and outlining future directions for developing robust and autonomous legal assistants.", "summary_bullets": ["Large language models (LLMs) have precipitated a dramatic improvement in the legal domain, yet the deployment of standalone models faces significant limitations regarding hallucination, outdated information, and verifiability.", "Recently, LLM agents have attracted significant attention as a solution to these challenges, utilizing advanced capabilities such as planning, memory, and tool usage to meet the rigorous standards of legal practice.", "In this paper, we present a comprehensive survey of LLM agents for legal tasks, analyzing how these architectures bridge the gap between technical capabilities and domain-specific needs."], "method": "In this paper, we present a comprehensive survey of LLM agents for legal tasks, analyzing how these architectures bridge the gap between technical capabilities and domain-specific needs.", "key_results": ["Our major contributions include: (1) systematically analyzing the technical transition from standard legal LLMs to legal agents; (2) presenting a structured taxonomy of current agent applications across distinct legal practice areas; (3) discussing evaluation methodologies specifically for agentic performance in law; and (4) identifying open challenges and outlining future directions for developing robust and autonomous legal assistants."], "limitations": ["Abstract-level evidence only: validate assumptions, evaluation protocol, and failure cases in the full paper before relying on this as key evidence.", "Large language models (LLMs) have precipitated a dramatic improvement in the legal domain, yet the deployment of standalone models faces significant limitations regarding hallucination, outdated information, and verifiability."], "bibkey": "Liu2026Agents"}
{"paper_id": "P0127", "title": "MAXS: Meta-Adaptive Exploration with LLM Agents", "year": 2026, "url": "http://arxiv.org/abs/2601.09259v1", "arxiv_id": "2601.09259v1", "primary_category": "cs.AI", "categories": ["cs.AI"], "pdf_url": "https://arxiv.org/pdf/2601.09259v1", "priority": "normal", "mapped_sections": [], "evidence_level": "abstract", "fulltext_path": "", "authors": ["Jian Zhang", "Zhiyuan Wang", "Zhangqi Wang", "Yu He", "Haoran Luo", "li yuan", "Lingling Zhang", "Rui Mao", "Qika Lin", "Jun Liu"], "abstract": "Large Language Model (LLM) Agents exhibit inherent reasoning abilities through the collaboration of multiple tools. However, during agent inference, existing methods often suffer from (i) locally myopic generation, due to the absence of lookahead, and (ii) trajectory instability, where minor early errors can escalate into divergent reasoning paths. These issues make it difficult to balance global effectiveness and computational efficiency. To address these two issues, we propose meta-adaptive exploration with LLM agents https://github.com/exoskeletonzj/MAXS, a meta-adaptive reasoning framework based on LLM Agents that flexibly integrates tool execution and reasoning planning. MAXS employs a lookahead strategy to extend reasoning paths a few steps ahead, estimating the advantage value of tool usage, and combines step consistency variance and inter-step trend slopes to jointly select stable, consistent, and high-value reasoning steps. Additionally, we introduce a trajectory convergence mechanism that controls computational cost by halting further rollouts once path consistency is achieved, enabling a balance between resource efficiency and global effectiveness in multi-tool reasoning. We conduct extensive empirical studies across three base models (MiMo-VL-7B, Qwen2.5-VL-7B, Qwen2.5-VL-32B) and five datasets, demonstrating that MAXS consistently outperforms existing methods in both performance and inference efficiency. Further analysis confirms the effectiveness of our lookahead strategy and tool usage.", "summary_bullets": ["Large Language Model (LLM) Agents exhibit inherent reasoning abilities through the collaboration of multiple tools.", "However, during agent inference, existing methods often suffer from (i) locally myopic generation, due to the absence of lookahead, and (ii) trajectory instability, where minor early errors can escalate into divergent reasoning paths.", "These issues make it difficult to balance global effectiveness and computational efficiency."], "method": "To address these two issues, we propose meta-adaptive exploration with LLM agents https://github.com/exoskeletonzj/MAXS, a meta-adaptive reasoning framework based on LLM Agents that flexibly integrates tool execution and reasoning planning.", "key_results": ["We conduct extensive empirical studies across three base models (MiMo-VL-7B, Qwen2.5-VL-7B, Qwen2.5-VL-32B) and five datasets, demonstrating that MAXS consistently outperforms existing methods in both performance and inference efficiency."], "limitations": ["Abstract-level evidence only: validate assumptions, evaluation protocol, and failure cases in the full paper before relying on this as key evidence."], "bibkey": "Zhang2026Maxs"}
{"paper_id": "P0128", "title": "Membox: Weaving Topic Continuity into Long-Range Memory for LLM Agents", "year": 2026, "url": "http://arxiv.org/abs/2601.03785v2", "arxiv_id": "2601.03785v2", "primary_category": "cs.CL", "categories": ["cs.CL", "cs.AI"], "pdf_url": "https://arxiv.org/pdf/2601.03785v2", "priority": "normal", "mapped_sections": ["4.2"], "evidence_level": "abstract", "fulltext_path": "", "authors": ["Dehao Tao", "Guoliang Ma", "Yongfeng Huang", "Minghu Jiang"], "abstract": "Human-agent dialogues often exhibit topic continuity-a stable thematic frame that evolves through temporally adjacent exchanges-yet most large language model (LLM) agent memory systems fail to preserve it. Existing designs follow a fragmentation-compensation paradigm: they first break dialogue streams into isolated utterances for storage, then attempt to restore coherence via embedding-based retrieval. This process irreversibly damages narrative and causal flow, while biasing retrieval towards lexical similarity. We introduce membox, a hierarchical memory architecture centered on a Topic Loom that continuously monitors dialogue in a sliding-window fashion, grouping consecutive same-topic turns into coherent \"memory boxes\" at storage time. Sealed boxes are then linked by a Trace Weaver into long-range event-timeline traces, recovering macro-topic recurrences across discontinuities. Experiments on LoCoMo demonstrate that Membox achieves up to 68% F1 improvement on temporal reasoning tasks, outperforming competitive baselines (e.g., Mem0, A-MEM). Notably, Membox attains these gains while using only a fraction of the context tokens required by existing methods, highlighting a superior balance between efficiency and effectiveness. By explicitly modeling topic continuity, Membox offers a cognitively motivated mechanism for enhancing both coherence and efficiency in LLM agents.", "summary_bullets": ["Human-agent dialogues often exhibit topic continuity-a stable thematic frame that evolves through temporally adjacent exchanges-yet most large language model (LLM) agent memory systems fail to preserve it.", "Existing designs follow a fragmentation-compensation paradigm: they first break dialogue streams into isolated utterances for storage, then attempt to restore coherence via embedding-based retrieval.", "This process irreversibly damages narrative and causal flow, while biasing retrieval towards lexical similarity."], "method": "We introduce membox, a hierarchical memory architecture centered on a Topic Loom that continuously monitors dialogue in a sliding-window fashion, grouping consecutive same-topic turns into coherent \"memory boxes\" at storage time.", "key_results": ["Experiments on LoCoMo demonstrate that Membox achieves up to 68% F1 improvement on temporal reasoning tasks, outperforming competitive baselines (e.g., Mem0, A-MEM).", "Human-agent dialogues often exhibit topic continuity-a stable thematic frame that evolves through temporally adjacent exchanges-yet most large language model (LLM) agent memory systems fail to preserve it."], "limitations": ["Abstract-level evidence only: validate assumptions, evaluation protocol, and failure cases in the full paper before relying on this as key evidence."], "bibkey": "Tao2026Membox"}
{"paper_id": "P0129", "title": "A Comprehensive Survey in LLM(-Agent) Full Stack Safety: Data, Training and Deployment", "year": 2025, "url": "http://arxiv.org/abs/2504.15585v4", "arxiv_id": "2504.15585v4", "primary_category": "cs.CR", "categories": ["cs.CR", "cs.AI", "cs.CL", "cs.LG"], "pdf_url": "https://arxiv.org/pdf/2504.15585v4", "priority": "normal", "mapped_sections": ["6.2"], "evidence_level": "abstract", "fulltext_path": "", "authors": ["Kun Wang", "Guibin Zhang", "Zhenhong Zhou", "Jiahao Wu", "Miao Yu", "Shiqian Zhao", "Chenlong Yin", "Jinhu Fu", "Yibo Yan", "Hanjun Luo", "Liang Lin", "Zhihao Xu", "Haolang Lu", "Xinye Cao", "Xinyun Zhou", "Weifei Jin", "Fanci Meng", "Shicheng Xu", "Junyuan Mao", "Yu Wang", "Hao Wu", "Minghe Wang", "Fan Zhang", "Junfeng Fang", "Wenjie Qu", "Yue Liu", "Chengwei Liu", "Yifan Zhang", "Qiankun Li", "Chongye Guo", "Yalan Qin", "Zhaoxin Fan", "Kai Wang", "Yi Ding", "Donghai Hong", "Jiaming Ji", "Yingxin Lai", "Zitong Yu", "Xinfeng Li", "Yifan Jiang", "Yanhui Li", "Xinyu Deng", "Junlin Wu", "Dongxia Wang", "Yihao Huang", "Yufei Guo", "Jen-tse Huang", "Qiufeng Wang", "Xiaolong Jin", "Wenxuan Wang", "Dongrui Liu", "Yanwei Yue", "Wenke Huang", "Guancheng Wan", "Heng Chang", "Tianlin Li", "Yi Yu", "Chenghao Li", "Jiawei Li", "Lei Bai", "Jie Zhang", "Qing Guo", "Jingyi Wang", "Tianlong Chen", "Joey Tianyi Zhou", "Xiaojun Jia", "Weisong Sun", "Cong Wu", "Jing Chen", "Xuming Hu", "Yiming Li", "Xiao Wang", "Ningyu Zhang", "Luu Anh Tuan", "Guowen Xu", "Jiaheng Zhang", "Tianwei Zhang", "Xingjun Ma", "Jindong Gu", "Liang Pang", "Xiang Wang", "Bo An", "Jun Sun", "Mohit Bansal", "Shirui Pan", "Lingjuan Lyu", "Yuval Elovici", "Bhavya Kailkhura", "Yaodong Yang", "Hongwei Li", "Wenyuan Xu", "Yizhou Sun", "Wei Wang", "Qing Li", "Ke Tang", "Yu-Gang Jiang", "Felix Juefei-Xu", "Hui Xiong", "Xiaofeng Wang", "Dacheng Tao", "Philip S. Yu", "Qingsong Wen", "Yang Liu"], "abstract": "The remarkable success of Large Language Models (LLMs) has illuminated a promising pathway toward achieving Artificial General Intelligence for both academic and industrial communities, owing to their unprecedented performance across various applications. As LLMs continue to gain prominence in both research and commercial domains, their security and safety implications have become a growing concern, not only for researchers and corporations but also for every nation. Currently, existing surveys on LLM safety primarily focus on specific stages of the LLM lifecycle, e.g., deployment phase or fine-tuning phase, lacking a comprehensive understanding of the entire \"lifechain\" of LLMs. To address this gap, this paper introduces, for the first time, the concept of \"full-stack\" safety to systematically consider safety issues throughout the entire process of LLM training, deployment, and eventual commercialization. Compared to the off-the-shelf LLM safety surveys, our work demonstrates several distinctive advantages: (I) Comprehensive Perspective. We define the complete LLM lifecycle as encompassing data preparation, pre-training, post-training, deployment and final commercialization. To our knowledge, this represents the first safety survey to encompass the entire lifecycle of LLMs. (II) Extensive Literature Support. Our research is grounded in an exhaustive review of over 800+ papers, ensuring comprehensive coverage and systematic organization of security issues within a more holistic understanding. (III) Unique Insights. Through systematic literature analysis, we have developed reliable roadmaps and perspectives for each chapter. Our work identifies promising research directions, including safety in data generation, alignment techniques, model editing, and LLM-based agent systems. These insights provide valuable guidance for researchers pursuing future work in this field.", "summary_bullets": ["The remarkable success of Large Language Models (LLMs) has illuminated a promising pathway toward achieving Artificial General Intelligence for both academic and industrial communities, owing to their unprecedented performance across various applications.", "As LLMs continue to gain prominence in both research and commercial domains, their security and safety implications have become a growing concern, not only for researchers and corporations but also for every nation.", "Currently, existing surveys on LLM safety primarily focus on specific stages of the LLM lifecycle, e.g., deployment phase or fine-tuning phase, lacking a comprehensive understanding of the entire \"lifechain\" of LLMs."], "method": "The remarkable success of Large Language Models (LLMs) has illuminated a promising pathway toward achieving Artificial General Intelligence for both academic and industrial communities, owing to their unprecedented performance across various applications.", "key_results": ["Our research is grounded in an exhaustive review of over 800+ papers, ensuring comprehensive coverage and systematic organization of security issues within a more holistic understanding.", "The remarkable success of Large Language Models (LLMs) has illuminated a promising pathway toward achieving Artificial General Intelligence for both academic and industrial communities, owing to their unprecedented performance across various applications."], "limitations": ["Abstract-level evidence only: validate assumptions, evaluation protocol, and failure cases in the full paper before relying on this as key evidence.", "These insights provide valuable guidance for researchers pursuing future work in this field."], "bibkey": "Wang2025Comprehensive"}
{"paper_id": "P0130", "title": "A Comprehensive Survey on the Trustworthiness of Large Language Models in Healthcare", "year": 2025, "url": "http://arxiv.org/abs/2502.15871v2", "arxiv_id": "2502.15871v2", "primary_category": "cs.CY", "categories": ["cs.CY", "cs.AI", "cs.CL"], "pdf_url": "https://arxiv.org/pdf/2502.15871v2", "priority": "normal", "mapped_sections": [], "evidence_level": "abstract", "fulltext_path": "", "authors": ["Manar Aljohani", "Jun Hou", "Sindhura Kommu", "Xuan Wang"], "abstract": "The application of large language models (LLMs) in healthcare holds significant promise for enhancing clinical decision-making, medical research, and patient care. However, their integration into real-world clinical settings raises critical concerns around trustworthiness, particularly around dimensions of truthfulness, privacy, safety, robustness, fairness, and explainability. These dimensions are essential for ensuring that LLMs generate reliable, unbiased, and ethically sound outputs. While researchers have recently begun developing benchmarks and evaluation frameworks to assess LLM trustworthiness, the trustworthiness of LLMs in healthcare remains underexplored, lacking a systematic review that provides a comprehensive understanding and future insights. This survey addresses that gap by providing a comprehensive review of current methodologies and solutions aimed at mitigating risks across key trust dimensions. We analyze how each dimension affects the reliability and ethical deployment of healthcare LLMs, synthesize ongoing research efforts, and identify critical gaps in existing approaches. We also identify emerging challenges posed by evolving paradigms, such as multi-agent collaboration, multi-modal reasoning, and the development of small open-source medical models. Our goal is to guide future research toward more trustworthy, transparent, and clinically viable LLMs.", "summary_bullets": ["The application of large language models (LLMs) in healthcare holds significant promise for enhancing clinical decision-making, medical research, and patient care.", "However, their integration into real-world clinical settings raises critical concerns around trustworthiness, particularly around dimensions of truthfulness, privacy, safety, robustness, fairness, and explainability.", "These dimensions are essential for ensuring that LLMs generate reliable, unbiased, and ethically sound outputs."], "method": "We analyze how each dimension affects the reliability and ethical deployment of healthcare LLMs, synthesize ongoing research efforts, and identify critical gaps in existing approaches.", "key_results": ["While researchers have recently begun developing benchmarks and evaluation frameworks to assess LLM trustworthiness, the trustworthiness of LLMs in healthcare remains underexplored, lacking a systematic review that provides a comprehensive understanding and future insights."], "limitations": ["Abstract-level evidence only: validate assumptions, evaluation protocol, and failure cases in the full paper before relying on this as key evidence."], "bibkey": "Aljohani2025Comprehensive"}
{"paper_id": "P0131", "title": "A Large-Language-Model Assisted Automated Scale Bar Detection and Extraction Framework for Scanning Electron Microscopic Images", "year": 2025, "url": "http://arxiv.org/abs/2510.11260v1", "arxiv_id": "2510.11260v1", "primary_category": "cs.CV", "categories": ["cs.CV", "cond-mat.mtrl-sci", "cs.AI", "physics.data-an"], "pdf_url": "https://arxiv.org/pdf/2510.11260v1", "priority": "normal", "mapped_sections": [], "evidence_level": "abstract", "fulltext_path": "", "authors": ["Yuxuan Chen", "Ruotong Yang", "Zhengyang Zhang", "Mehreen Ahmed", "Yanming Wang"], "abstract": "Microscopic characterizations, such as Scanning Electron Microscopy (SEM), are widely used in scientific research for visualizing and analyzing microstructures. Determining the scale bars is an important first step of accurate SEM analysis; however, currently, it mainly relies on manual operations, which is both time-consuming and prone to errors. To address this issue, we propose a multi-modal and automated scale bar detection and extraction framework that provides concurrent object detection, text detection and text recognition with a Large Language Model (LLM) agent. The proposed framework operates in four phases; i) Automatic Dataset Generation (Auto-DG) model to synthesize a diverse dataset of SEM images ensuring robust training and high generalizability of the model, ii) scale bar object detection, iii) information extraction using a hybrid Optical Character Recognition (OCR) system with DenseNet and Convolutional Recurrent Neural Network (CRNN) based algorithms, iv) an LLM agent to analyze and verify accuracy of the results. The proposed model demonstrates a strong performance in object detection and accurate localization with a precision of 100%, recall of 95.8%, and a mean Average Precision (mAP) of 99.2% at IoU=0.5 and 69.1% at IoU=0.5:0.95. The hybrid OCR system achieved 89% precision, 65% recall, and a 75% F1 score on the Auto-DG dataset, significantly outperforming several mainstream standalone engines, highlighting its reliability for scientific image analysis. The LLM is introduced as a reasoning engine as well as an intelligent assistant that suggests follow-up steps and verifies the results. This automated method powered by an LLM agent significantly enhances the efficiency and accuracy of scale bar detection and extraction in SEM images, providing a valuable tool for microscopic analysis and advancing the field of scientific imaging.", "summary_bullets": ["Microscopic characterizations, such as Scanning Electron Microscopy (SEM), are widely used in scientific research for visualizing and analyzing microstructures.", "Determining the scale bars is an important first step of accurate SEM analysis; however, currently, it mainly relies on manual operations, which is both time-consuming and prone to errors.", "To address this issue, we propose a multi-modal and automated scale bar detection and extraction framework that provides concurrent object detection, text detection and text recognition with a Large Language Model (LLM) agent."], "method": "To address this issue, we propose a multi-modal and automated scale bar detection and extraction framework that provides concurrent object detection, text detection and text recognition with a Large Language Model (LLM) agent.", "key_results": ["The hybrid OCR system achieved 89% precision, 65% recall, and a 75% F1 score on the Auto-DG dataset, significantly outperforming several mainstream standalone engines, highlighting its reliability for scientific image analysis.", "The proposed model demonstrates a strong performance in object detection and accurate localization with a precision of 100%, recall of 95.8%, and a mean Average Precision (mAP) of 99.2% at IoU=0.5 and 69.1% at IoU=0.5:0.95."], "limitations": ["Abstract-level evidence only: validate assumptions, evaluation protocol, and failure cases in the full paper before relying on this as key evidence."], "bibkey": "Chen2025Large"}
{"paper_id": "P0132", "title": "A Novel Architecture for Symbolic Reasoning with Decision Trees and LLM Agents", "year": 2025, "url": "http://arxiv.org/abs/2508.05311v1", "arxiv_id": "2508.05311v1", "primary_category": "cs.AI", "categories": ["cs.AI", "cs.CL"], "pdf_url": "https://arxiv.org/pdf/2508.05311v1", "priority": "normal", "mapped_sections": ["4.1"], "evidence_level": "abstract", "fulltext_path": "", "authors": ["Andrew Kiruluta"], "abstract": "We propose a hybrid architecture that integrates decision tree-based symbolic reasoning with the generative capabilities of large language models (LLMs) within a coordinated multi-agent framework. Unlike prior approaches that loosely couple symbolic and neural modules, our design embeds decision trees and random forests as callable oracles within a unified reasoning system. Tree-based modules enable interpretable rule inference and causal logic, while LLM agents handle abductive reasoning, generalization, and interactive planning. A central orchestrator maintains belief state consistency and mediates communication across agents and external tools, enabling reasoning over both structured and unstructured inputs.\n  The system achieves strong performance on reasoning benchmarks. On \\textit{ProofWriter}, it improves entailment consistency by +7.2\\% through logic-grounded tree validation. On GSM8k, it achieves +5.3\\% accuracy gains in multistep mathematical problems via symbolic augmentation. On \\textit{ARC}, it boosts abstraction accuracy by +6.0\\% through integration of symbolic oracles. Applications in clinical decision support and scientific discovery show how the system encodes domain rules symbolically while leveraging LLMs for contextual inference and hypothesis generation. This architecture offers a robust, interpretable, and extensible solution for general-purpose neuro-symbolic reasoning.", "summary_bullets": ["We propose a hybrid architecture that integrates decision tree-based symbolic reasoning with the generative capabilities of large language models (LLMs) within a coordinated multi-agent framework.", "Unlike prior approaches that loosely couple symbolic and neural modules, our design embeds decision trees and random forests as callable oracles within a unified reasoning system.", "Tree-based modules enable interpretable rule inference and causal logic, while LLM agents handle abductive reasoning, generalization, and interactive planning."], "method": "We propose a hybrid architecture that integrates decision tree-based symbolic reasoning with the generative capabilities of large language models (LLMs) within a coordinated multi-agent framework.", "key_results": ["On GSM8k, it achieves +5.3\\% accuracy gains in multistep mathematical problems via symbolic augmentation.", "On \\textit{ARC}, it boosts abstraction accuracy by +6.0\\% through integration of symbolic oracles."], "limitations": ["Abstract-level evidence only: validate assumptions, evaluation protocol, and failure cases in the full paper before relying on this as key evidence."], "bibkey": "Kiruluta2025Novel"}
{"paper_id": "P0133", "title": "A Survey of AI for Materials Science: Foundation Models, LLM Agents, Datasets, and Tools", "year": 2025, "url": "http://arxiv.org/abs/2506.20743v1", "arxiv_id": "2506.20743v1", "primary_category": "cs.LG", "categories": ["cs.LG", "cs.CE"], "pdf_url": "https://arxiv.org/pdf/2506.20743v1", "priority": "normal", "mapped_sections": ["5.1", "6.1"], "evidence_level": "abstract", "fulltext_path": "", "authors": ["Minh-Hao Van", "Prateek Verma", "Chen Zhao", "Xintao Wu"], "abstract": "Foundation models (FMs) are catalyzing a transformative shift in materials science (MatSci) by enabling scalable, general-purpose, and multimodal AI systems for scientific discovery. Unlike traditional machine learning models, which are typically narrow in scope and require task-specific engineering, FMs offer cross-domain generalization and exhibit emergent capabilities. Their versatility is especially well-suited to materials science, where research challenges span diverse data types and scales. This survey provides a comprehensive overview of foundation models, agentic systems, datasets, and computational tools supporting this growing field. We introduce a task-driven taxonomy encompassing six broad application areas: data extraction, interpretation and Q\\&A; atomistic simulation; property prediction; materials structure, design and discovery; process planning, discovery, and optimization; and multiscale modeling. We discuss recent advances in both unimodal and multimodal FMs, as well as emerging large language model (LLM) agents. Furthermore, we review standardized datasets, open-source tools, and autonomous experimental platforms that collectively fuel the development and integration of FMs into research workflows. We assess the early successes of foundation models and identify persistent limitations, including challenges in generalizability, interpretability, data imbalance, safety concerns, and limited multimodal fusion. Finally, we articulate future research directions centered on scalable pretraining, continual learning, data governance, and trustworthiness.", "summary_bullets": ["Foundation models (FMs) are catalyzing a transformative shift in materials science (MatSci) by enabling scalable, general-purpose, and multimodal AI systems for scientific discovery.", "Unlike traditional machine learning models, which are typically narrow in scope and require task-specific engineering, FMs offer cross-domain generalization and exhibit emergent capabilities.", "Their versatility is especially well-suited to materials science, where research challenges span diverse data types and scales."], "method": "We introduce a task-driven taxonomy encompassing six broad application areas: data extraction, interpretation and Q\\&A; atomistic simulation; property prediction; materials structure, design and discovery; process planning, discovery, and optimization; and multiscale modeling.", "key_results": ["This survey provides a comprehensive overview of foundation models, agentic systems, datasets, and computational tools supporting this growing field.", "Furthermore, we review standardized datasets, open-source tools, and autonomous experimental platforms that collectively fuel the development and integration of FMs into research workflows."], "limitations": ["Abstract-level evidence only: validate assumptions, evaluation protocol, and failure cases in the full paper before relying on this as key evidence.", "We assess the early successes of foundation models and identify persistent limitations, including challenges in generalizability, interpretability, data imbalance, safety concerns, and limited multimodal fusion."], "bibkey": "Van2025Survey"}
{"paper_id": "P0134", "title": "A Survey of LLM $\\times$ DATA", "year": 2025, "url": "http://arxiv.org/abs/2505.18458v3", "arxiv_id": "2505.18458v3", "primary_category": "cs.DB", "categories": ["cs.DB", "cs.AI", "cs.CL", "cs.IR", "cs.LG"], "pdf_url": "https://arxiv.org/pdf/2505.18458v3", "priority": "normal", "mapped_sections": [], "evidence_level": "abstract", "fulltext_path": "", "authors": ["Xuanhe Zhou", "Junxuan He", "Wei Zhou", "Haodong Chen", "Zirui Tang", "Haoyu Zhao", "Xin Tong", "Guoliang Li", "Youmin Chen", "Jun Zhou", "Zhaojun Sun", "Binyuan Hui", "Shuo Wang", "Conghui He", "Zhiyuan Liu", "Jingren Zhou", "Fan Wu"], "abstract": "The integration of large language model (LLM) and data management (DATA) is rapidly redefining both domains. In this survey, we comprehensively review the bidirectional relationships. On the one hand, DATA4LLM, spanning large-scale data processing, storage, and serving, feeds LLMs with high quality, diversity, and timeliness of data required for stages like pre-training, post-training, retrieval-augmented generation, and agentic workflows: (i) Data processing for LLMs includes scalable acquisition, deduplication, filtering, selection, domain mixing, and synthetic augmentation; (ii) Data Storage for LLMs focuses on efficient data and model formats, distributed and heterogeneous storage hierarchies, KV-cache management, and fault-tolerant checkpointing; (iii) Data serving for LLMs tackles challenges in RAG (e.g., knowledge post-processing), LLM inference (e.g., prompt compression, data provenance), and training strategies (e.g., data packing and shuffling). On the other hand, in LLM4DATA, LLMs are emerging as general-purpose engines for data management. We review recent advances in (i) data manipulation, including automatic data cleaning, integration, discovery; (ii) data analysis, covering reasoning over structured, semi-structured, and unstructured data, and (iii) system optimization (e.g., configuration tuning, query rewriting, anomaly diagnosis), powered by LLM techniques like retrieval-augmented prompting, task-specialized fine-tuning, and multi-agent collaboration.", "summary_bullets": ["The integration of large language model (LLM) and data management (DATA) is rapidly redefining both domains.", "In this survey, we comprehensively review the bidirectional relationships.", "On the one hand, DATA4LLM, spanning large-scale data processing, storage, and serving, feeds LLMs with high quality, diversity, and timeliness of data required for stages like pre-training, post-training, retrieval-augmented generation, and agentic workflows: (i) Data processing for LLMs includes scalable acquisition, deduplication, filtering, selection, domain mixing, and synthetic augmentation; (ii) Data Storage for LLMs focuses on efficient data and model formats, distributed and heterogeneous storage hierarchies, KV-cache management, and fault-tolerant checkpointing; (iii) Data serving for LLMs tackles challenges in RAG (e.g., knowledge post-processing), LLM inference (e.g., prompt compression, data provenance), and training strategies (e.g., data packing and shuffling)."], "method": "The integration of large language model (LLM) and data management (DATA) is rapidly redefining both domains.", "key_results": ["We review recent advances in (i) data manipulation, including automatic data cleaning, integration, discovery; (ii) data analysis, covering reasoning over structured, semi-structured, and unstructured data, and (iii) system optimization (e.g., configuration tuning, query rewriting, anomaly diagnosis), powered by LLM techniques like retrieval-augmented prompting, task-specialized fine-tuning, and multi-agent collaboration."], "limitations": ["Abstract-level evidence only: validate assumptions, evaluation protocol, and failure cases in the full paper before relying on this as key evidence."], "bibkey": "Zhou2025Survey"}
{"paper_id": "P0135", "title": "A Survey of Large Language Models in Discipline-specific Research: Challenges, Methods and Opportunities", "year": 2025, "url": "http://arxiv.org/abs/2507.08425v1", "arxiv_id": "2507.08425v1", "primary_category": "cs.CL", "categories": ["cs.CL"], "pdf_url": "https://arxiv.org/pdf/2507.08425v1", "priority": "normal", "mapped_sections": [], "evidence_level": "abstract", "fulltext_path": "", "authors": ["Lu Xiang", "Yang Zhao", "Yaping Zhang", "Chengqing Zong"], "abstract": "Large Language Models (LLMs) have demonstrated their transformative potential across numerous disciplinary studies, reshaping the existing research methodologies and fostering interdisciplinary collaboration. However, a systematic understanding of their integration into diverse disciplines remains underexplored. This survey paper provides a comprehensive overview of the application of LLMs in interdisciplinary studies, categorising research efforts from both a technical perspective and with regard to their applicability. From a technical standpoint, key methodologies such as supervised fine-tuning, retrieval-augmented generation, agent-based approaches, and tool-use integration are examined, which enhance the adaptability and effectiveness of LLMs in discipline-specific contexts. From the perspective of their applicability, this paper explores how LLMs are contributing to various disciplines including mathematics, physics, chemistry, biology, and the humanities and social sciences, demonstrating their role in discipline-specific tasks. The prevailing challenges are critically examined and the promising research directions are highlighted alongside the recent advances in LLMs. By providing a comprehensive overview of the technical developments and applications in this field, this survey aims to serve as an invaluable resource for the researchers who are navigating the complex landscape of LLMs in the context of interdisciplinary studies.", "summary_bullets": ["Large Language Models (LLMs) have demonstrated their transformative potential across numerous disciplinary studies, reshaping the existing research methodologies and fostering interdisciplinary collaboration.", "However, a systematic understanding of their integration into diverse disciplines remains underexplored.", "This survey paper provides a comprehensive overview of the application of LLMs in interdisciplinary studies, categorising research efforts from both a technical perspective and with regard to their applicability."], "method": "Large Language Models (LLMs) have demonstrated their transformative potential across numerous disciplinary studies, reshaping the existing research methodologies and fostering interdisciplinary collaboration.", "key_results": ["By providing a comprehensive overview of the technical developments and applications in this field, this survey aims to serve as an invaluable resource for the researchers who are navigating the complex landscape of LLMs in the context of interdisciplinary studies."], "limitations": ["Abstract-level evidence only: validate assumptions, evaluation protocol, and failure cases in the full paper before relying on this as key evidence."], "bibkey": "Xiang2025Survey"}
{"paper_id": "P0136", "title": "A Survey of Scaling in Large Language Model Reasoning", "year": 2025, "url": "http://arxiv.org/abs/2504.02181v1", "arxiv_id": "2504.02181v1", "primary_category": "cs.AI", "categories": ["cs.AI"], "pdf_url": "https://arxiv.org/pdf/2504.02181v1", "priority": "normal", "mapped_sections": ["5.1"], "evidence_level": "abstract", "fulltext_path": "", "authors": ["Zihan Chen", "Song Wang", "Zhen Tan", "Xingbo Fu", "Zhenyu Lei", "Peng Wang", "Huan Liu", "Cong Shen", "Jundong Li"], "abstract": "The rapid advancements in large Language models (LLMs) have significantly enhanced their reasoning capabilities, driven by various strategies such as multi-agent collaboration. However, unlike the well-established performance improvements achieved through scaling data and model size, the scaling of reasoning in LLMs is more complex and can even negatively impact reasoning performance, introducing new challenges in model alignment and robustness. In this survey, we provide a comprehensive examination of scaling in LLM reasoning, categorizing it into multiple dimensions and analyzing how and to what extent different scaling strategies contribute to improving reasoning capabilities. We begin by exploring scaling in input size, which enables LLMs to process and utilize more extensive context for improved reasoning. Next, we analyze scaling in reasoning steps that improves multi-step inference and logical consistency. We then examine scaling in reasoning rounds, where iterative interactions refine reasoning outcomes. Furthermore, we discuss scaling in training-enabled reasoning, focusing on optimization through iterative model improvement. Finally, we review applications of scaling across domains and outline future directions for further advancing LLM reasoning. By synthesizing these diverse perspectives, this survey aims to provide insights into how scaling strategies fundamentally enhance the reasoning capabilities of LLMs and further guide the development of next-generation AI systems.", "summary_bullets": ["The rapid advancements in large Language models (LLMs) have significantly enhanced their reasoning capabilities, driven by various strategies such as multi-agent collaboration.", "However, unlike the well-established performance improvements achieved through scaling data and model size, the scaling of reasoning in LLMs is more complex and can even negatively impact reasoning performance, introducing new challenges in model alignment and robustness.", "In this survey, we provide a comprehensive examination of scaling in LLM reasoning, categorizing it into multiple dimensions and analyzing how and to what extent different scaling strategies contribute to improving reasoning capabilities."], "method": "Next, we analyze scaling in reasoning steps that improves multi-step inference and logical consistency.", "key_results": ["By synthesizing these diverse perspectives, this survey aims to provide insights into how scaling strategies fundamentally enhance the reasoning capabilities of LLMs and further guide the development of next-generation AI systems."], "limitations": ["Abstract-level evidence only: validate assumptions, evaluation protocol, and failure cases in the full paper before relying on this as key evidence."], "bibkey": "Chen2025Survey"}
{"paper_id": "P0137", "title": "A Survey of Vibe Coding with Large Language Models", "year": 2025, "url": "http://arxiv.org/abs/2510.12399v2", "arxiv_id": "2510.12399v2", "primary_category": "cs.AI", "categories": ["cs.AI"], "pdf_url": "https://arxiv.org/pdf/2510.12399v2", "priority": "normal", "mapped_sections": [], "evidence_level": "abstract", "fulltext_path": "", "authors": ["Yuyao Ge", "Lingrui Mei", "Zenghao Duan", "Tianhao Li", "Yujia Zheng", "Yiwei Wang", "Lexin Wang", "Jiayu Yao", "Tianyu Liu", "Yujun Cai", "Baolong Bi", "Fangda Guo", "Jiafeng Guo", "Shenghua Liu", "Xueqi Cheng"], "abstract": "The advancement of large language models (LLMs) has catalyzed a paradigm shift from code generation assistance to autonomous coding agents, enabling a novel development methodology termed \"Vibe Coding\" where developers validate AI-generated implementations through outcome observation rather than line-by-line code comprehension. Despite its transformative potential, the effectiveness of this emergent paradigm remains under-explored, with empirical evidence revealing unexpected productivity losses and fundamental challenges in human-AI collaboration. To address this gap, this survey provides the first comprehensive and systematic review of Vibe Coding with large language models, establishing both theoretical foundations and practical frameworks for this transformative development approach. Drawing from systematic analysis of over 1000 research papers, we survey the entire vibe coding ecosystem, examining critical infrastructure components including LLMs for coding, LLM-based coding agent, development environment of coding agent, and feedback mechanisms. We first introduce Vibe Coding as a formal discipline by formalizing it through a Constrained Markov Decision Process that captures the dynamic triadic relationship among human developers, software projects, and coding agents. Building upon this theoretical foundation, we then synthesize existing practices into five distinct development models: Unconstrained Automation, Iterative Conversational Collaboration, Planning-Driven, Test-Driven, and Context-Enhanced Models, thus providing the first comprehensive taxonomy in this domain. Critically, our analysis reveals that successful Vibe Coding depends not merely on agent capabilities but on systematic context engineering, well-established development environments, and human-agent collaborative development models.", "summary_bullets": ["The advancement of large language models (LLMs) has catalyzed a paradigm shift from code generation assistance to autonomous coding agents, enabling a novel development methodology termed \"Vibe Coding\" where developers validate AI-generated implementations through outcome observation rather than line-by-line code comprehension.", "Despite its transformative potential, the effectiveness of this emergent paradigm remains under-explored, with empirical evidence revealing unexpected productivity losses and fundamental challenges in human-AI collaboration.", "To address this gap, this survey provides the first comprehensive and systematic review of Vibe Coding with large language models, establishing both theoretical foundations and practical frameworks for this transformative development approach."], "method": "The advancement of large language models (LLMs) has catalyzed a paradigm shift from code generation assistance to autonomous coding agents, enabling a novel development methodology termed \"Vibe Coding\" where developers validate AI-generated implementations through outcome observation rather than line-by-line code comprehension.", "key_results": ["Drawing from systematic analysis of over 1000 research papers, we survey the entire vibe coding ecosystem, examining critical infrastructure components including LLMs for coding, LLM-based coding agent, development environment of coding agent, and feedback mechanisms.", "Despite its transformative potential, the effectiveness of this emergent paradigm remains under-explored, with empirical evidence revealing unexpected productivity losses and fundamental challenges in human-AI collaboration."], "limitations": ["Abstract-level evidence only: validate assumptions, evaluation protocol, and failure cases in the full paper before relying on this as key evidence."], "bibkey": "Ge2025Survey"}
{"paper_id": "P0138", "title": "AGENT: An Aerial Vehicle Generation and Design Tool Using Large Language Models", "year": 2025, "url": "http://arxiv.org/abs/2504.08981v1", "arxiv_id": "2504.08981v1", "primary_category": "cs.LG", "categories": ["cs.LG", "cs.AI"], "pdf_url": "https://arxiv.org/pdf/2504.08981v1", "priority": "normal", "mapped_sections": ["3.1", "3.2"], "evidence_level": "abstract", "fulltext_path": "", "authors": ["Colin Samplawski", "Adam D. Cobb", "Susmit Jha"], "abstract": "Computer-aided design (CAD) is a promising application area for emerging artificial intelligence methods. Traditional workflows for cyberphysical systems create detailed digital models which can be evaluated by physics simulators in order to narrow the search space before creating physical prototypes. A major bottleneck of this approach is that the simulators are often computationally expensive and slow. Recent advancements in AI methods offer the possibility to accelerate these pipelines. We use the recently released AircraftVerse dataset, which is especially suited for developing and evaluating large language models for designs. AircraftVerse contains a diverse set of UAV designs represented via textual design trees together with detailed physics simulation results. Following the recent success of large language models (LLMs), we propose AGENT (Aircraft GENeraTor). AGENT is a comprehensive design tool built on the CodeT5+ LLM which learns powerful representations of aircraft textual designs directly from JSON files. We develop a curriculum of training tasks which imbues a single model with a suite of useful features. AGENT is able to generate designs conditioned on properties of flight dynamics (hover time, maximum speed, etc.). Additionally, AGENT can issue evaluations of designs allowing it to act as a surrogate model of the physics simulation that underlies the AircraftVerse dataset. We present a series of experiments which demonstrate our system's abilities. We are able to achieve strong performance using the smallest member of the CodeT5+ family (220M parameters). This allows for a flexible and powerful system which can be executed on a single GPU enabling a clear path toward future deployment.", "summary_bullets": ["Computer-aided design (CAD) is a promising application area for emerging artificial intelligence methods.", "Traditional workflows for cyberphysical systems create detailed digital models which can be evaluated by physics simulators in order to narrow the search space before creating physical prototypes.", "A major bottleneck of this approach is that the simulators are often computationally expensive and slow."], "method": "Following the recent success of large language models (LLMs), we propose AGENT (Aircraft GENeraTor).", "key_results": ["We use the recently released AircraftVerse dataset, which is especially suited for developing and evaluating large language models for designs.", "Following the recent success of large language models (LLMs), we propose AGENT (Aircraft GENeraTor)."], "limitations": ["Abstract-level evidence only: validate assumptions, evaluation protocol, and failure cases in the full paper before relying on this as key evidence."], "bibkey": "Samplawski2025Agent"}
{"paper_id": "P0139", "title": "ARCANE: A Multi-Agent Framework for Interpretable and Configurable Alignment", "year": 2025, "url": "http://arxiv.org/abs/2512.06196v1", "arxiv_id": "2512.06196v1", "primary_category": "cs.AI", "categories": ["cs.AI", "cs.CL"], "pdf_url": "https://arxiv.org/pdf/2512.06196v1", "priority": "normal", "mapped_sections": ["5.2"], "evidence_level": "abstract", "fulltext_path": "", "authors": ["Charlie Masters", "Marta Grze≈õkiewicz", "Stefano V. Albrecht"], "abstract": "As agents based on large language models are increasingly deployed to long-horizon tasks, maintaining their alignment with stakeholder preferences becomes critical. Effective alignment in such settings requires reward models that are interpretable so that stakeholders can understand and audit model objectives. Moreover, reward models must be capable of steering agents at interaction time, allowing preference shifts to be incorporated without retraining. We introduce ARCANE, a framework that frames alignment as a multi-agent collaboration problem that dynamically represents stakeholder preferences as natural-language rubrics: weighted sets of verifiable criteria that can be generated on-the-fly from task context. Inspired by utility theory, we formulate rubric learning as a reconstruction problem and apply a regularized Group-Sequence Policy Optimization (GSPO) procedure that balances interpretability, faithfulness, and computational efficiency. Using a corpus of 219 labeled rubrics derived from the GDPVal benchmark, we evaluate ARCANE on challenging tasks requiring multi-step reasoning and tool use. The learned rubrics produce compact, legible evaluations and enable configurable trade-offs (e.g., correctness vs. conciseness) without retraining. Our results show that rubric-based reward models offer a promising path toward interpretable, test-time adaptive alignment for complex, long-horizon AI systems.", "summary_bullets": ["As agents based on large language models are increasingly deployed to long-horizon tasks, maintaining their alignment with stakeholder preferences becomes critical.", "Effective alignment in such settings requires reward models that are interpretable so that stakeholders can understand and audit model objectives.", "Moreover, reward models must be capable of steering agents at interaction time, allowing preference shifts to be incorporated without retraining."], "method": "We introduce ARCANE, a framework that frames alignment as a multi-agent collaboration problem that dynamically represents stakeholder preferences as natural-language rubrics: weighted sets of verifiable criteria that can be generated on-the-fly from task context.", "key_results": ["Using a corpus of 219 labeled rubrics derived from the GDPVal benchmark, we evaluate ARCANE on challenging tasks requiring multi-step reasoning and tool use."], "limitations": ["Abstract-level evidence only: validate assumptions, evaluation protocol, and failure cases in the full paper before relying on this as key evidence."], "bibkey": "Masters2025Arcane"}
{"paper_id": "P0140", "title": "ATLaS: Agent Tuning via Learning Critical Steps", "year": 2025, "url": "http://arxiv.org/abs/2503.02197v2", "arxiv_id": "2503.02197v2", "primary_category": "cs.CL", "categories": ["cs.CL", "cs.AI", "cs.LG"], "pdf_url": "https://arxiv.org/pdf/2503.02197v2", "priority": "normal", "mapped_sections": [], "evidence_level": "abstract", "fulltext_path": "", "authors": ["Zhixun Chen", "Ming Li", "Yuxuan Huang", "Yali Du", "Meng Fang", "Tianyi Zhou"], "abstract": "Large Language Model (LLM) agents have demonstrated remarkable generalization capabilities across multi-domain tasks. Existing agent tuning approaches typically employ supervised finetuning on entire expert trajectories. However, behavior-cloning of full trajectories can introduce expert bias and weaken generalization to states not covered by the expert data. Additionally, critical steps, such as planning, complex reasoning for intermediate subtasks, and strategic decision-making, are essential to success in agent tasks, so learning these steps is the key to improving LLM agents. For more effective and efficient agent tuning, we propose ATLaS that identifies the critical steps in expert trajectories and finetunes LLMs solely on these steps with reduced costs. By steering the training's focus to a few critical steps, our method mitigates the risk of overfitting entire trajectories and promotes generalization across different environments and tasks. In extensive experiments, an LLM finetuned on only 30% critical steps selected by ATLaS outperforms the LLM finetuned on all steps and recent open-source LLM agents. ATLaS maintains and improves base LLM skills as generalist agents interacting with diverse environments.", "summary_bullets": ["Large Language Model (LLM) agents have demonstrated remarkable generalization capabilities across multi-domain tasks.", "Existing agent tuning approaches typically employ supervised finetuning on entire expert trajectories.", "However, behavior-cloning of full trajectories can introduce expert bias and weaken generalization to states not covered by the expert data."], "method": "For more effective and efficient agent tuning, we propose ATLaS that identifies the critical steps in expert trajectories and finetunes LLMs solely on these steps with reduced costs.", "key_results": ["In extensive experiments, an LLM finetuned on only 30% critical steps selected by ATLaS outperforms the LLM finetuned on all steps and recent open-source LLM agents.", "Additionally, critical steps, such as planning, complex reasoning for intermediate subtasks, and strategic decision-making, are essential to success in agent tasks, so learning these steps is the key to improving LLM agents."], "limitations": ["Abstract-level evidence only: validate assumptions, evaluation protocol, and failure cases in the full paper before relying on this as key evidence."], "bibkey": "Chen2025Atlas"}
{"paper_id": "P0141", "title": "Agent-S: LLM Agentic workflow to automate Standard Operating Procedures", "year": 2025, "url": "http://arxiv.org/abs/2503.15520v1", "arxiv_id": "2503.15520v1", "primary_category": "cs.HC", "categories": ["cs.HC"], "pdf_url": "https://arxiv.org/pdf/2503.15520v1", "priority": "normal", "mapped_sections": ["3.1"], "evidence_level": "abstract", "fulltext_path": "", "authors": ["Mandar Kulkarni"], "abstract": "AI agents using Large Language Models (LLMs) as foundations have shown promise in solving complex real-world tasks. In this paper, we propose an LLM-based agentic workflow for automating Standard Operating Procedures (SOP). For customer care operations, an SOP defines a logical step-by-step process for human agents to resolve customer issues. We observe that any step in the SOP can be categorized as user interaction or API call, while the logical flow in the SOP defines the navigation. We use LLMs augmented with memory and environments (API tools, user interface, external knowledge source) for SOP automation. Our agentic architecture consists of three task-specific LLMs, a Global Action Repository (GAR), execution memory, and multiple environments. SOP workflow is written as a simple logical block of text. Based on the current execution memory and the SOP, the agent chooses the action to execute; it interacts with an appropriate environment (user/API) to collect observations and feedback, which are, in turn, inputted to memory to decide the next action. The agent is designed to be fault-tolerant, where it dynamically decides to repeat an action or seek input from an external knowledge source. We demonstrate the efficacy of the proposed agent on the three SOPs from the e-commerce seller domain. The experimental results validate the agent's performance under complex real-world scenarios.", "summary_bullets": ["AI agents using Large Language Models (LLMs) as foundations have shown promise in solving complex real-world tasks.", "In this paper, we propose an LLM-based agentic workflow for automating Standard Operating Procedures (SOP).", "For customer care operations, an SOP defines a logical step-by-step process for human agents to resolve customer issues."], "method": "In this paper, we propose an LLM-based agentic workflow for automating Standard Operating Procedures (SOP).", "key_results": ["For customer care operations, an SOP defines a logical step-by-step process for human agents to resolve customer issues."], "limitations": ["Abstract-level evidence only: validate assumptions, evaluation protocol, and failure cases in the full paper before relying on this as key evidence."], "bibkey": "Kulkarni2025Agent"}
{"paper_id": "P0142", "title": "AgentVigil: Generic Black-Box Red-teaming for Indirect Prompt Injection against LLM Agents", "year": 2025, "url": "http://arxiv.org/abs/2505.05849v4", "arxiv_id": "2505.05849v4", "primary_category": "cs.CR", "categories": ["cs.CR", "cs.AI"], "pdf_url": "https://arxiv.org/pdf/2505.05849v4", "priority": "normal", "mapped_sections": ["6.2"], "evidence_level": "abstract", "fulltext_path": "", "authors": ["Zhun Wang", "Vincent Siu", "Zhe Ye", "Tianneng Shi", "Yuzhou Nie", "Xuandong Zhao", "Chenguang Wang", "Wenbo Guo", "Dawn Song"], "abstract": "The strong planning and reasoning capabilities of Large Language Models (LLMs) have fostered the development of agent-based systems capable of leveraging external tools and interacting with increasingly complex environments. However, these powerful features also introduce a critical security risk: indirect prompt injection, a sophisticated attack vector that compromises the core of these agents, the LLM, by manipulating contextual information rather than direct user prompts. In this work, we propose a generic black-box fuzzing framework, AgentVigil, designed to automatically discover and exploit indirect prompt injection vulnerabilities across diverse LLM agents. Our approach starts by constructing a high-quality initial seed corpus, then employs a seed selection algorithm based on Monte Carlo Tree Search (MCTS) to iteratively refine inputs, thereby maximizing the likelihood of uncovering agent weaknesses. We evaluate AgentVigil on two public benchmarks, AgentDojo and VWA-adv, where it achieves 71% and 70% success rates against agents based on o3-mini and GPT-4o, respectively, nearly doubling the performance of baseline attacks. Moreover, AgentVigil exhibits strong transferability across unseen tasks and internal LLMs, as well as promising results against defenses. Beyond benchmark evaluations, we apply our attacks in real-world environments, successfully misleading agents to navigate to arbitrary URLs, including malicious sites.", "summary_bullets": ["The strong planning and reasoning capabilities of Large Language Models (LLMs) have fostered the development of agent-based systems capable of leveraging external tools and interacting with increasingly complex environments.", "However, these powerful features also introduce a critical security risk: indirect prompt injection, a sophisticated attack vector that compromises the core of these agents, the LLM, by manipulating contextual information rather than direct user prompts.", "In this work, we propose a generic black-box fuzzing framework, AgentVigil, designed to automatically discover and exploit indirect prompt injection vulnerabilities across diverse LLM agents."], "method": "In this work, we propose a generic black-box fuzzing framework, AgentVigil, designed to automatically discover and exploit indirect prompt injection vulnerabilities across diverse LLM agents.", "key_results": ["We evaluate AgentVigil on two public benchmarks, AgentDojo and VWA-adv, where it achieves 71% and 70% success rates against agents based on o3-mini and GPT-4o, respectively, nearly doubling the performance of baseline attacks.", "Beyond benchmark evaluations, we apply our attacks in real-world environments, successfully misleading agents to navigate to arbitrary URLs, including malicious sites."], "limitations": ["Abstract-level evidence only: validate assumptions, evaluation protocol, and failure cases in the full paper before relying on this as key evidence."], "bibkey": "Wang2025Agentvigil"}
{"paper_id": "P0143", "title": "Agentic AI Home Energy Management System: A Large Language Model Framework for Residential Load Scheduling", "year": 2025, "url": "http://arxiv.org/abs/2510.26603v1", "arxiv_id": "2510.26603v1", "primary_category": "cs.AI", "categories": ["cs.AI", "cs.MA", "eess.SY"], "pdf_url": "https://arxiv.org/pdf/2510.26603v1", "priority": "normal", "mapped_sections": [], "evidence_level": "abstract", "fulltext_path": "", "authors": ["Reda El Makroum", "Sebastian Zwickl-Bernhard", "Lukas Kranzl"], "abstract": "The electricity sector transition requires substantial increases in residential demand response capacity, yet Home Energy Management Systems (HEMS) adoption remains limited by user interaction barriers requiring translation of everyday preferences into technical parameters. While large language models have been applied to energy systems as code generators and parameter extractors, no existing implementation deploys LLMs as autonomous coordinators managing the complete workflow from natural language input to multi-appliance scheduling. This paper presents an agentic AI HEMS where LLMs autonomously coordinate multi-appliance scheduling from natural language requests to device control, achieving optimal scheduling without example demonstrations. A hierarchical architecture combining one orchestrator with three specialist agents uses the ReAct pattern for iterative reasoning, enabling dynamic coordination without hardcoded workflows while integrating Google Calendar for context-aware deadline extraction. Evaluation across three open-source models using real Austrian day-ahead electricity prices reveals substantial capability differences. Llama-3.3-70B successfully coordinates all appliances across all scenarios to match cost-optimal benchmarks computed via mixed-integer linear programming, while other models achieve perfect single-appliance performance but struggle to coordinate all appliances simultaneously. Progressive prompt engineering experiments demonstrate that analytical query handling without explicit guidance remains unreliable despite models' general reasoning capabilities. We open-source the complete system including orchestration logic, agent prompts, tools, and web interfaces to enable reproducibility, extension, and future research.", "summary_bullets": ["The electricity sector transition requires substantial increases in residential demand response capacity, yet Home Energy Management Systems (HEMS) adoption remains limited by user interaction barriers requiring translation of everyday preferences into technical parameters.", "While large language models have been applied to energy systems as code generators and parameter extractors, no existing implementation deploys LLMs as autonomous coordinators managing the complete workflow from natural language input to multi-appliance scheduling.", "This paper presents an agentic AI HEMS where LLMs autonomously coordinate multi-appliance scheduling from natural language requests to device control, achieving optimal scheduling without example demonstrations."], "method": "The electricity sector transition requires substantial increases in residential demand response capacity, yet Home Energy Management Systems (HEMS) adoption remains limited by user interaction barriers requiring translation of everyday preferences into technical parameters.", "key_results": ["Llama-3.3-70B successfully coordinates all appliances across all scenarios to match cost-optimal benchmarks computed via mixed-integer linear programming, while other models achieve perfect single-appliance performance but struggle to coordinate all appliances simultaneously.", "Evaluation across three open-source models using real Austrian day-ahead electricity prices reveals substantial capability differences."], "limitations": ["Abstract-level evidence only: validate assumptions, evaluation protocol, and failure cases in the full paper before relying on this as key evidence."], "bibkey": "Makroum2025Agentic"}
{"paper_id": "P0144", "title": "Agentic Context Engineering: Evolving Contexts for Self-Improving Language Models", "year": 2025, "url": "http://arxiv.org/abs/2510.04618v1", "arxiv_id": "2510.04618v1", "primary_category": "cs.LG", "categories": ["cs.LG", "cs.AI", "cs.CL"], "pdf_url": "https://arxiv.org/pdf/2510.04618v1", "priority": "normal", "mapped_sections": ["5.1"], "evidence_level": "abstract", "fulltext_path": "", "authors": ["Qizheng Zhang", "Changran Hu", "Shubhangi Upasani", "Boyuan Ma", "Fenglu Hong", "Vamsidhar Kamanuru", "Jay Rainton", "Chen Wu", "Mengmeng Ji", "Hanchen Li", "Urmish Thakker", "James Zou", "Kunle Olukotun"], "abstract": "Large language model (LLM) applications such as agents and domain-specific reasoning increasingly rely on context adaptation -- modifying inputs with instructions, strategies, or evidence, rather than weight updates. Prior approaches improve usability but often suffer from brevity bias, which drops domain insights for concise summaries, and from context collapse, where iterative rewriting erodes details over time. Building on the adaptive memory introduced by Dynamic Cheatsheet, we introduce ACE (Agentic Context Engineering), a framework that treats contexts as evolving playbooks that accumulate, refine, and organize strategies through a modular process of generation, reflection, and curation. ACE prevents collapse with structured, incremental updates that preserve detailed knowledge and scale with long-context models. Across agent and domain-specific benchmarks, ACE optimizes contexts both offline (e.g., system prompts) and online (e.g., agent memory), consistently outperforming strong baselines: +10.6% on agents and +8.6% on finance, while significantly reducing adaptation latency and rollout cost. Notably, ACE could adapt effectively without labeled supervision and instead by leveraging natural execution feedback. On the AppWorld leaderboard, ACE matches the top-ranked production-level agent on the overall average and surpasses it on the harder test-challenge split, despite using a smaller open-source model. These results show that comprehensive, evolving contexts enable scalable, efficient, and self-improving LLM systems with low overhead.", "summary_bullets": ["Large language model (LLM) applications such as agents and domain-specific reasoning increasingly rely on context adaptation -- modifying inputs with instructions, strategies, or evidence, rather than weight updates.", "Prior approaches improve usability but often suffer from brevity bias, which drops domain insights for concise summaries, and from context collapse, where iterative rewriting erodes details over time.", "Building on the adaptive memory introduced by Dynamic Cheatsheet, we introduce ACE (Agentic Context Engineering), a framework that treats contexts as evolving playbooks that accumulate, refine, and organize strategies through a modular process of generation, reflection, and curation."], "method": "Building on the adaptive memory introduced by Dynamic Cheatsheet, we introduce ACE (Agentic Context Engineering), a framework that treats contexts as evolving playbooks that accumulate, refine, and organize strategies through a modular process of generation, reflection, and curation.", "key_results": ["Across agent and domain-specific benchmarks, ACE optimizes contexts both offline (e.g., system prompts) and online (e.g., agent memory), consistently outperforming strong baselines: +10.6% on agents and +8.6% on finance, while significantly reducing adaptation latency and rollout cost."], "limitations": ["Abstract-level evidence only: validate assumptions, evaluation protocol, and failure cases in the full paper before relying on this as key evidence."], "bibkey": "Zhang2025Agentic"}
{"paper_id": "P0145", "title": "Architecting Resilient LLM Agents: A Guide to Secure Plan-then-Execute Implementations", "year": 2025, "url": "http://arxiv.org/abs/2509.08646v1", "arxiv_id": "2509.08646v1", "primary_category": "cs.CR", "categories": ["cs.CR", "cs.AI", "eess.SY"], "pdf_url": "https://arxiv.org/pdf/2509.08646v1", "priority": "normal", "mapped_sections": ["6.2"], "evidence_level": "abstract", "fulltext_path": "", "authors": ["Ron F. Del Rosario", "Klaudia Krawiecka", "Christian Schroeder de Witt"], "abstract": "As Large Language Model (LLM) agents become increasingly capable of automating complex, multi-step tasks, the need for robust, secure, and predictable architectural patterns is paramount. This paper provides a comprehensive guide to the ``Plan-then-Execute'' (P-t-E) pattern, an agentic design that separates strategic planning from tactical execution. We explore the foundational principles of P-t-E, detailing its core components - the Planner and the Executor - and its architectural advantages in predictability, cost-efficiency, and reasoning quality over reactive patterns like ReAct (Reason + Act). A central focus is placed on the security implications of this design, particularly its inherent resilience to indirect prompt injection attacks by establishing control-flow integrity. We argue that while P-t-E provides a strong foundation, a defense-in-depth strategy is necessary, and we detail essential complementary controls such as the Principle of Least Privilege, task-scoped tool access, and sandboxed code execution. To make these principles actionable, this guide provides detailed implementation blueprints and working code references for three leading agentic frameworks: LangChain (via LangGraph), CrewAI, and AutoGen. Each framework's approach to implementing the P-t-E pattern is analyzed, highlighting unique features like LangGraph's stateful graphs for re-planning, CrewAI's declarative tool scoping for security, and AutoGen's built-in Docker sandboxing. Finally, we discuss advanced patterns, including dynamic re-planning loops, parallel execution with Directed Acyclic Graphs (DAGs), and the critical role of Human-in-the-Loop (HITL) verification, to offer a complete strategic blueprint for architects, developers, and security engineers aiming to build production-grade, resilient, and trustworthy LLM agents.", "summary_bullets": ["As Large Language Model (LLM) agents become increasingly capable of automating complex, multi-step tasks, the need for robust, secure, and predictable architectural patterns is paramount.", "This paper provides a comprehensive guide to the ``Plan-then-Execute'' (P-t-E) pattern, an agentic design that separates strategic planning from tactical execution.", "We explore the foundational principles of P-t-E, detailing its core components - the Planner and the Executor - and its architectural advantages in predictability, cost-efficiency, and reasoning quality over reactive patterns like ReAct (Reason + Act)."], "method": "As Large Language Model (LLM) agents become increasingly capable of automating complex, multi-step tasks, the need for robust, secure, and predictable architectural patterns is paramount.", "key_results": ["Finally, we discuss advanced patterns, including dynamic re-planning loops, parallel execution with Directed Acyclic Graphs (DAGs), and the critical role of Human-in-the-Loop (HITL) verification, to offer a complete strategic blueprint for architects, developers, and security engineers aiming to build production-grade, resilient, and trustworthy LLM agents."], "limitations": ["Abstract-level evidence only: validate assumptions, evaluation protocol, and failure cases in the full paper before relying on this as key evidence."], "bibkey": "Rosario2025Architecting"}
{"paper_id": "P0146", "title": "Attractive Metadata Attack: Inducing LLM Agents to Invoke Malicious Tools", "year": 2025, "url": "http://arxiv.org/abs/2508.02110v2", "arxiv_id": "2508.02110v2", "primary_category": "cs.AI", "categories": ["cs.AI"], "pdf_url": "https://arxiv.org/pdf/2508.02110v2", "priority": "normal", "mapped_sections": ["6.2"], "evidence_level": "abstract", "fulltext_path": "", "authors": ["Kanghua Mo", "Li Hu", "Yucheng Long", "Zhihao Li"], "abstract": "Large language model (LLM) agents have demonstrated remarkable capabilities in complex reasoning and decision-making by leveraging external tools. However, this tool-centric paradigm introduces a previously underexplored attack surface, where adversaries can manipulate tool metadata -- such as names, descriptions, and parameter schemas -- to influence agent behavior. We identify this as a new and stealthy threat surface that allows malicious tools to be preferentially selected by LLM agents, without requiring prompt injection or access to model internals. To demonstrate and exploit this vulnerability, we propose the Attractive Metadata Attack (AMA), a black-box in-context learning framework that generates highly attractive but syntactically and semantically valid tool metadata through iterative optimization. The proposed attack integrates seamlessly into standard tool ecosystems and requires no modification to the agent's execution framework. Extensive experiments across ten realistic, simulated tool-use scenarios and a range of popular LLM agents demonstrate consistently high attack success rates (81\\%-95\\%) and significant privacy leakage, with negligible impact on primary task execution. Moreover, the attack remains effective even against prompt-level defenses, auditor-based detection, and structured tool-selection protocols such as the Model Context Protocol, revealing systemic vulnerabilities in current agent architectures. These findings reveal that metadata manipulation constitutes a potent and stealthy attack surface. Notably, AMA is orthogonal to injection attacks and can be combined with them to achieve stronger attack efficacy, highlighting the need for execution-level defenses beyond prompt-level and auditor-based mechanisms. Code is available at https://github.com/SEAIC-M/AMA.", "summary_bullets": ["Large language model (LLM) agents have demonstrated remarkable capabilities in complex reasoning and decision-making by leveraging external tools.", "However, this tool-centric paradigm introduces a previously underexplored attack surface, where adversaries can manipulate tool metadata -- such as names, descriptions, and parameter schemas -- to influence agent behavior.", "We identify this as a new and stealthy threat surface that allows malicious tools to be preferentially selected by LLM agents, without requiring prompt injection or access to model internals."], "method": "To demonstrate and exploit this vulnerability, we propose the Attractive Metadata Attack (AMA), a black-box in-context learning framework that generates highly attractive but syntactically and semantically valid tool metadata through iterative optimization.", "key_results": ["Extensive experiments across ten realistic, simulated tool-use scenarios and a range of popular LLM agents demonstrate consistently high attack success rates (81\\%-95\\%) and significant privacy leakage, with negligible impact on primary task execution."], "limitations": ["Abstract-level evidence only: validate assumptions, evaluation protocol, and failure cases in the full paper before relying on this as key evidence."], "bibkey": "Mo2025Attractive"}
{"paper_id": "P0147", "title": "AutoPDL: Automatic Prompt Optimization for LLM Agents", "year": 2025, "url": "http://arxiv.org/abs/2504.04365v5", "arxiv_id": "2504.04365v5", "primary_category": "cs.LG", "categories": ["cs.LG", "cs.AI", "cs.PL"], "pdf_url": "https://arxiv.org/pdf/2504.04365v5", "priority": "normal", "mapped_sections": [], "evidence_level": "abstract", "fulltext_path": "", "authors": ["Claudio Spiess", "Mandana Vaziri", "Louis Mandel", "Martin Hirzel"], "abstract": "The performance of large language models (LLMs) depends on how they are prompted, with choices spanning both the high-level prompting pattern (e.g., Zero-Shot, CoT, ReAct, ReWOO) and the specific prompt content (instructions and few-shot demonstrations). Manually tuning this combination is tedious, error-prone, and specific to a given LLM and task. Therefore, this paper proposes AutoPDL, an automated approach to discovering good LLM agent configurations. Our approach frames this as a structured AutoML problem over a combinatorial space of agentic and non-agentic prompting patterns and demonstrations, using successive halving to efficiently navigate this space. We introduce a library implementing common prompting patterns using the PDL prompt programming language. AutoPDL solutions are human-readable, editable, and executable PDL programs that use this library. This approach also enables source-to-source optimization, allowing human-in-the-loop refinement and reuse. Evaluations across three tasks and seven LLMs (ranging from 3B to 70B parameters) show consistent accuracy gains ($9.21\\pm15.46$ percentage points), up to 67.5pp, and reveal that selected prompting strategies vary across models and tasks.", "summary_bullets": ["The performance of large language models (LLMs) depends on how they are prompted, with choices spanning both the high-level prompting pattern (e.g., Zero-Shot, CoT, ReAct, ReWOO) and the specific prompt content (instructions and few-shot demonstrations).", "Manually tuning this combination is tedious, error-prone, and specific to a given LLM and task.", "Therefore, this paper proposes AutoPDL, an automated approach to discovering good LLM agent configurations."], "method": "We introduce a library implementing common prompting patterns using the PDL prompt programming language.", "key_results": ["Evaluations across three tasks and seven LLMs (ranging from 3B to 70B parameters) show consistent accuracy gains ($9.21\\pm15.46$ percentage points), up to 67.5pp, and reveal that selected prompting strategies vary across models and tasks.", "AutoPDL solutions are human-readable, editable, and executable PDL programs that use this library."], "limitations": ["Abstract-level evidence only: validate assumptions, evaluation protocol, and failure cases in the full paper before relying on this as key evidence."], "bibkey": "Spiess2025Autopdl"}
{"paper_id": "P0148", "title": "AutoPentest: Enhancing Vulnerability Management With Autonomous LLM Agents", "year": 2025, "url": "http://arxiv.org/abs/2505.10321v1", "arxiv_id": "2505.10321v1", "primary_category": "cs.CR", "categories": ["cs.CR", "cs.AI"], "pdf_url": "https://arxiv.org/pdf/2505.10321v1", "priority": "normal", "mapped_sections": ["6.2"], "evidence_level": "abstract", "fulltext_path": "", "authors": ["Julius Henke"], "abstract": "A recent area of increasing research is the use of Large Language Models (LLMs) in penetration testing, which promises to reduce costs and thus allow for higher frequency. We conduct a review of related work, identifying best practices and common evaluation issues. We then present AutoPentest, an application for performing black-box penetration tests with a high degree of autonomy. AutoPentest is based on the LLM GPT-4o from OpenAI and the LLM agent framework LangChain. It can perform complex multi-step tasks, augmented by external tools and knowledge bases. We conduct a study on three capture-the-flag style Hack The Box (HTB) machines, comparing our implementation AutoPentest with the baseline approach of manually using the ChatGPT-4o user interface. Both approaches are able to complete 15-25 % of the subtasks on the HTB machines, with AutoPentest slightly outperforming ChatGPT. We measure a total cost of \\$96.20 US when using AutoPentest across all experiments, while a one-month subscription to ChatGPT Plus costs \\$20. The results show that further implementation efforts and the use of more powerful LLMs released in the future are likely to make this a viable part of vulnerability management.", "summary_bullets": ["A recent area of increasing research is the use of Large Language Models (LLMs) in penetration testing, which promises to reduce costs and thus allow for higher frequency.", "We conduct a review of related work, identifying best practices and common evaluation issues.", "We then present AutoPentest, an application for performing black-box penetration tests with a high degree of autonomy."], "method": "A recent area of increasing research is the use of Large Language Models (LLMs) in penetration testing, which promises to reduce costs and thus allow for higher frequency.", "key_results": ["Both approaches are able to complete 15-25 % of the subtasks on the HTB machines, with AutoPentest slightly outperforming ChatGPT.", "We measure a total cost of \\$96.20 US when using AutoPentest across all experiments, while a one-month subscription to ChatGPT Plus costs \\$20."], "limitations": ["Abstract-level evidence only: validate assumptions, evaluation protocol, and failure cases in the full paper before relying on this as key evidence."], "bibkey": "Henke2025Autopentest"}
{"paper_id": "P0149", "title": "AutoSCORE: Enhancing Automated Scoring with Multi-Agent Large Language Models via Structured Component Recognition", "year": 2025, "url": "http://arxiv.org/abs/2509.21910v1", "arxiv_id": "2509.21910v1", "primary_category": "cs.CL", "categories": ["cs.CL", "cs.AI"], "pdf_url": "https://arxiv.org/pdf/2509.21910v1", "priority": "normal", "mapped_sections": ["5.2", "6.1"], "evidence_level": "abstract", "fulltext_path": "", "authors": ["Yun Wang", "Zhaojun Ding", "Xuansheng Wu", "Siyue Sun", "Ninghao Liu", "Xiaoming Zhai"], "abstract": "Automated scoring plays a crucial role in education by reducing the reliance on human raters, offering scalable and immediate evaluation of student work. While large language models (LLMs) have shown strong potential in this task, their use as end-to-end raters faces challenges such as low accuracy, prompt sensitivity, limited interpretability, and rubric misalignment. These issues hinder the implementation of LLM-based automated scoring in assessment practice. To address the limitations, we propose AutoSCORE, a multi-agent LLM framework enhancing automated scoring via rubric-aligned Structured COmponent REcognition. With two agents, AutoSCORE first extracts rubric-relevant components from student responses and encodes them into a structured representation (i.e., Scoring Rubric Component Extraction Agent), which is then used to assign final scores (i.e., Scoring Agent). This design ensures that model reasoning follows a human-like grading process, enhancing interpretability and robustness. We evaluate AutoSCORE on four benchmark datasets from the ASAP benchmark, using both proprietary and open-source LLMs (GPT-4o, LLaMA-3.1-8B, and LLaMA-3.1-70B). Across diverse tasks and rubrics, AutoSCORE consistently improves scoring accuracy, human-machine agreement (QWK, correlations), and error metrics (MAE, RMSE) compared to single-agent baselines, with particularly strong benefits on complex, multi-dimensional rubrics, and especially large relative gains on smaller LLMs. These results demonstrate that structured component recognition combined with multi-agent design offers a scalable, reliable, and interpretable solution for automated scoring.", "summary_bullets": ["Automated scoring plays a crucial role in education by reducing the reliance on human raters, offering scalable and immediate evaluation of student work.", "While large language models (LLMs) have shown strong potential in this task, their use as end-to-end raters faces challenges such as low accuracy, prompt sensitivity, limited interpretability, and rubric misalignment.", "These issues hinder the implementation of LLM-based automated scoring in assessment practice."], "method": "To address the limitations, we propose AutoSCORE, a multi-agent LLM framework enhancing automated scoring via rubric-aligned Structured COmponent REcognition.", "key_results": ["We evaluate AutoSCORE on four benchmark datasets from the ASAP benchmark, using both proprietary and open-source LLMs (GPT-4o, LLaMA-3.1-8B, and LLaMA-3.1-70B).", "Automated scoring plays a crucial role in education by reducing the reliance on human raters, offering scalable and immediate evaluation of student work."], "limitations": ["Abstract-level evidence only: validate assumptions, evaluation protocol, and failure cases in the full paper before relying on this as key evidence.", "To address the limitations, we propose AutoSCORE, a multi-agent LLM framework enhancing automated scoring via rubric-aligned Structured COmponent REcognition."], "bibkey": "Wang2025Autoscore"}
{"paper_id": "P0150", "title": "Automated Penetration Testing with LLM Agents and Classical Planning", "year": 2025, "url": "http://arxiv.org/abs/2512.11143v1", "arxiv_id": "2512.11143v1", "primary_category": "cs.CR", "categories": ["cs.CR"], "pdf_url": "https://arxiv.org/pdf/2512.11143v1", "priority": "normal", "mapped_sections": ["4.1"], "evidence_level": "abstract", "fulltext_path": "", "authors": ["Lingzhi Wang", "Xinyi Shi", "Ziyu Li", "Yi Jiang", "Shiyu Tan", "Yuhao Jiang", "Junjie Cheng", "Wenyuan Chen", "Xiangmin Shen", "Zhenyuan LI", "Yan Chen"], "abstract": "While penetration testing plays a vital role in cybersecurity, achieving fully automated, hands-off-the-keyboard execution remains a significant research challenge. In this paper, we introduce the \"Planner-Executor-Perceptor (PEP)\" design paradigm and use it to systematically review existing work and identify the key challenges in this area. We also evaluate existing penetration testing systems, with a particular focus on the use of Large Language Model (LLM) agents for this task. The results show that the out-of-the-box Claude Code and Sonnet 4.5 exhibit superior penetration capabilities observed to date, substantially outperforming all prior systems. However, a detailed analysis of their testing processes reveals specific strengths and limitations; notably, LLM agents struggle with maintaining coherent long-horizon plans, performing complex reasoning, and effectively utilizing specialized tools. These limitations significantly constrain its overall capability, efficiency, and stability. To address these limitations, we propose CHECKMATE, a framework that integrates enhanced classical planning with LLM agents, providing an external, structured \"brain\" that mitigates the inherent weaknesses of LLM agents. Our evaluation shows that CHECKMATE outperforms the state-of-the-art system (Claude Code) in penetration capability, improving benchmark success rates by over 20%. In addition, it delivers substantially greater stability, cutting both time and monetary costs by more than 50%.", "summary_bullets": ["While penetration testing plays a vital role in cybersecurity, achieving fully automated, hands-off-the-keyboard execution remains a significant research challenge.", "In this paper, we introduce the \"Planner-Executor-Perceptor (PEP)\" design paradigm and use it to systematically review existing work and identify the key challenges in this area.", "We also evaluate existing penetration testing systems, with a particular focus on the use of Large Language Model (LLM) agents for this task."], "method": "In this paper, we introduce the \"Planner-Executor-Perceptor (PEP)\" design paradigm and use it to systematically review existing work and identify the key challenges in this area.", "key_results": ["Our evaluation shows that CHECKMATE outperforms the state-of-the-art system (Claude Code) in penetration capability, improving benchmark success rates by over 20%.", "The results show that the out-of-the-box Claude Code and Sonnet 4.5 exhibit superior penetration capabilities observed to date, substantially outperforming all prior systems."], "limitations": ["Abstract-level evidence only: validate assumptions, evaluation protocol, and failure cases in the full paper before relying on this as key evidence.", "However, a detailed analysis of their testing processes reveals specific strengths and limitations; notably, LLM agents struggle with maintaining coherent long-horizon plans, performing complex reasoning, and effectively utilizing specialized tools."], "bibkey": "Wang2025Automated"}
{"paper_id": "P0151", "title": "Automated Survey Collection with LLM-based Conversational Agents", "year": 2025, "url": "http://arxiv.org/abs/2504.02891v1", "arxiv_id": "2504.02891v1", "primary_category": "cs.CL", "categories": ["cs.CL", "cs.AI"], "pdf_url": "https://arxiv.org/pdf/2504.02891v1", "priority": "normal", "mapped_sections": [], "evidence_level": "abstract", "fulltext_path": "", "authors": ["Kurmanbek Kaiyrbekov", "Nicholas J Dobbins", "Sean D Mooney"], "abstract": "Objective: Traditional phone-based surveys are among the most accessible and widely used methods to collect biomedical and healthcare data, however, they are often costly, labor intensive, and difficult to scale effectively. To overcome these limitations, we propose an end-to-end survey collection framework driven by conversational Large Language Models (LLMs).\n  Materials and Methods: Our framework consists of a researcher responsible for designing the survey and recruiting participants, a conversational phone agent powered by an LLM that calls participants and administers the survey, a second LLM (GPT-4o) that analyzes the conversation transcripts generated during the surveys, and a database for storing and organizing the results. To test our framework, we recruited 8 participants consisting of 5 native and 3 non-native english speakers and administered 40 surveys. We evaluated the correctness of LLM-generated conversation transcripts, accuracy of survey responses inferred by GPT-4o and overall participant experience.\n  Results: Survey responses were successfully extracted by GPT-4o from conversation transcripts with an average accuracy of 98% despite transcripts exhibiting an average per-line word error rate of 7.7%. While participants noted occasional errors made by the conversational LLM agent, they reported that the agent effectively conveyed the purpose of the survey, demonstrated good comprehension, and maintained an engaging interaction.\n  Conclusions: Our study highlights the potential of LLM agents in conducting and analyzing phone surveys for healthcare applications. By reducing the workload on human interviewers and offering a scalable solution, this approach paves the way for real-world, end-to-end AI-powered phone survey collection systems.", "summary_bullets": ["Objective: Traditional phone-based surveys are among the most accessible and widely used methods to collect biomedical and healthcare data, however, they are often costly, labor intensive, and difficult to scale effectively.", "To overcome these limitations, we propose an end-to-end survey collection framework driven by conversational Large Language Models (LLMs).", "Materials and Methods: Our framework consists of a researcher responsible for designing the survey and recruiting participants, a conversational phone agent powered by an LLM that calls participants and administers the survey, a second LLM (GPT-4o) that analyzes the conversation transcripts generated during the surveys, and a database for storing and organizing the results."], "method": "To overcome these limitations, we propose an end-to-end survey collection framework driven by conversational Large Language Models (LLMs).", "key_results": ["Results: Survey responses were successfully extracted by GPT-4o from conversation transcripts with an average accuracy of 98% despite transcripts exhibiting an average per-line word error rate of 7.7%.", "To test our framework, we recruited 8 participants consisting of 5 native and 3 non-native english speakers and administered 40 surveys."], "limitations": ["Abstract-level evidence only: validate assumptions, evaluation protocol, and failure cases in the full paper before relying on this as key evidence.", "To overcome these limitations, we propose an end-to-end survey collection framework driven by conversational Large Language Models (LLMs)."], "bibkey": "Kaiyrbekov2025Automated"}
{"paper_id": "P0152", "title": "Automating Data-Driven Modeling and Analysis for Engineering Applications using Large Language Model Agents", "year": 2025, "url": "http://arxiv.org/abs/2510.01398v1", "arxiv_id": "2510.01398v1", "primary_category": "cs.AI", "categories": ["cs.AI"], "pdf_url": "https://arxiv.org/pdf/2510.01398v1", "priority": "normal", "mapped_sections": [], "evidence_level": "abstract", "fulltext_path": "", "authors": ["Yang Liu", "Zaid Abulawi", "Abhiram Garimidi", "Doyeong Lim"], "abstract": "Modern engineering increasingly relies on vast datasets generated by experiments and simulations, driving a growing demand for efficient, reliable, and broadly applicable modeling strategies. There is also heightened interest in developing data-driven approaches, particularly neural network models, for effective prediction and analysis of scientific datasets. Traditional data-driven methods frequently involve extensive manual intervention, limiting their ability to scale effectively and generalize to diverse applications. In this study, we propose an innovative pipeline utilizing Large Language Model (LLM) agents to automate data-driven modeling and analysis, with a particular emphasis on regression tasks. We evaluate two LLM-agent frameworks: a multi-agent system featuring specialized collaborative agents, and a single-agent system based on the Reasoning and Acting (ReAct) paradigm. Both frameworks autonomously handle data preprocessing, neural network development, training, hyperparameter optimization, and uncertainty quantification (UQ). We validate our approach using a critical heat flux (CHF) prediction benchmark, involving approximately 25,000 experimental data points from the OECD/NEA benchmark dataset. Results indicate that our LLM-agent-developed model surpasses traditional CHF lookup tables and delivers predictive accuracy and UQ on par with state-of-the-art Bayesian optimized deep neural network models developed by human experts. These outcomes underscore the significant potential of LLM-based agents to automate complex engineering modeling tasks, greatly reducing human workload while meeting or exceeding existing standards of predictive performance.", "summary_bullets": ["Modern engineering increasingly relies on vast datasets generated by experiments and simulations, driving a growing demand for efficient, reliable, and broadly applicable modeling strategies.", "There is also heightened interest in developing data-driven approaches, particularly neural network models, for effective prediction and analysis of scientific datasets.", "Traditional data-driven methods frequently involve extensive manual intervention, limiting their ability to scale effectively and generalize to diverse applications."], "method": "In this study, we propose an innovative pipeline utilizing Large Language Model (LLM) agents to automate data-driven modeling and analysis, with a particular emphasis on regression tasks.", "key_results": ["We validate our approach using a critical heat flux (CHF) prediction benchmark, involving approximately 25,000 experimental data points from the OECD/NEA benchmark dataset.", "Results indicate that our LLM-agent-developed model surpasses traditional CHF lookup tables and delivers predictive accuracy and UQ on par with state-of-the-art Bayesian optimized deep neural network models developed by human experts."], "limitations": ["Abstract-level evidence only: validate assumptions, evaluation protocol, and failure cases in the full paper before relying on this as key evidence."], "bibkey": "Liu2025Automating"}
{"paper_id": "P0153", "title": "Automating Structural Engineering Workflows with Large Language Model Agents", "year": 2025, "url": "http://arxiv.org/abs/2510.11004v1", "arxiv_id": "2510.11004v1", "primary_category": "cs.MA", "categories": ["cs.MA", "cs.AI", "cs.CE", "cs.CL"], "pdf_url": "https://arxiv.org/pdf/2510.11004v1", "priority": "normal", "mapped_sections": [], "evidence_level": "abstract", "fulltext_path": "", "authors": ["Haoran Liang", "Yufa Zhou", "Mohammad Talebi Kalaleh", "Qipei Mei"], "abstract": "We introduce $\\textbf{MASSE}$, the first Multi-Agent System for Structural Engineering, effectively integrating large language model (LLM)-based agents with real-world engineering workflows. Structural engineering is a fundamental yet traditionally stagnant domain, with core workflows remaining largely unchanged for decades despite its substantial economic impact and global market size. Recent advancements in LLMs have significantly enhanced their ability to perform complex reasoning, long-horizon planning, and precise tool utilization -- capabilities well aligned with structural engineering tasks such as interpreting design codes, executing load calculations, and verifying structural capacities. We present a proof-of-concept showing that most real-world structural engineering workflows can be fully automated through a training-free LLM-based multi-agent system. MASSE enables immediate deployment in professional environments, and our comprehensive validation on real-world case studies demonstrates that it can reduce expert workload from approximately two hours to mere minutes, while enhancing both reliability and accuracy in practical engineering scenarios.", "summary_bullets": ["We introduce $\\textbf{MASSE}$, the first Multi-Agent System for Structural Engineering, effectively integrating large language model (LLM)-based agents with real-world engineering workflows.", "Structural engineering is a fundamental yet traditionally stagnant domain, with core workflows remaining largely unchanged for decades despite its substantial economic impact and global market size.", "Recent advancements in LLMs have significantly enhanced their ability to perform complex reasoning, long-horizon planning, and precise tool utilization -- capabilities well aligned with structural engineering tasks such as interpreting design codes, executing load calculations, and verifying structural capacities."], "method": "We introduce $\\textbf{MASSE}$, the first Multi-Agent System for Structural Engineering, effectively integrating large language model (LLM)-based agents with real-world engineering workflows.", "key_results": ["MASSE enables immediate deployment in professional environments, and our comprehensive validation on real-world case studies demonstrates that it can reduce expert workload from approximately two hours to mere minutes, while enhancing both reliability and accuracy in practical engineering scenarios."], "limitations": ["Abstract-level evidence only: validate assumptions, evaluation protocol, and failure cases in the full paper before relying on this as key evidence."], "bibkey": "Liang2025Automating"}
{"paper_id": "P0154", "title": "Beyond Protein Language Models: An Agentic LLM Framework for Mechanistic Enzyme Design", "year": 2025, "url": "http://arxiv.org/abs/2511.19423v1", "arxiv_id": "2511.19423v1", "primary_category": "q-bio.QM", "categories": ["q-bio.QM", "cs.AI"], "pdf_url": "https://arxiv.org/pdf/2511.19423v1", "priority": "normal", "mapped_sections": [], "evidence_level": "abstract", "fulltext_path": "", "authors": ["Bruno Jacob", "Khushbu Agarwal", "Marcel Baer", "Peter Rice", "Simone Raugei"], "abstract": "We present Genie-CAT, a tool-augmented large-language-model (LLM) system designed to accelerate scientific hypothesis generation in protein design. Using metalloproteins (e.g., ferredoxins) as a case study, Genie-CAT integrates four capabilities -- literature-grounded reasoning through retrieval-augmented generation (RAG), structural parsing of Protein Data Bank files, electrostatic potential calculations, and machine-learning prediction of redox properties -- into a unified agentic workflow. By coupling natural-language reasoning with data-driven and physics-based computation, the system generates mechanistically interpretable, testable hypotheses linking sequence, structure, and function. In proof-of-concept demonstrations, Genie-CAT autonomously identifies residue-level modifications near [Fe--S] clusters that affect redox tuning, reproducing expert-derived hypotheses in a fraction of the time. The framework highlights how AI agents combining language models with domain-specific tools can bridge symbolic reasoning and numerical simulation, transforming LLMs from conversational assistants into partners for computational discovery.", "summary_bullets": ["We present Genie-CAT, a tool-augmented large-language-model (LLM) system designed to accelerate scientific hypothesis generation in protein design.", "Using metalloproteins (e.g., ferredoxins) as a case study, Genie-CAT integrates four capabilities -- literature-grounded reasoning through retrieval-augmented generation (RAG), structural parsing of Protein Data Bank files, electrostatic potential calculations, and machine-learning prediction of redox properties -- into a unified agentic workflow.", "By coupling natural-language reasoning with data-driven and physics-based computation, the system generates mechanistically interpretable, testable hypotheses linking sequence, structure, and function."], "method": "We present Genie-CAT, a tool-augmented large-language-model (LLM) system designed to accelerate scientific hypothesis generation in protein design.", "key_results": ["The framework highlights how AI agents combining language models with domain-specific tools can bridge symbolic reasoning and numerical simulation, transforming LLMs from conversational assistants into partners for computational discovery."], "limitations": ["Abstract-level evidence only: validate assumptions, evaluation protocol, and failure cases in the full paper before relying on this as key evidence."], "bibkey": "Jacob2025Beyond"}
{"paper_id": "P0155", "title": "BridgeScope: A Universal Toolkit for Bridging Large Language Models and Databases", "year": 2025, "url": "http://arxiv.org/abs/2508.04031v1", "arxiv_id": "2508.04031v1", "primary_category": "cs.DB", "categories": ["cs.DB"], "pdf_url": "https://arxiv.org/pdf/2508.04031v1", "priority": "normal", "mapped_sections": ["6.2"], "evidence_level": "abstract", "fulltext_path": "", "authors": ["Lianggui Weng", "Dandan Liu", "Rong Zhu", "Bolin Ding", "Jingren Zhou"], "abstract": "As large language models (LLMs) demonstrate increasingly powerful reasoning and orchestration capabilities, LLM-based agents are rapidly proliferating for complex data-related tasks. Despite this progress, the current design of how LLMs interact with databases exhibits critical limitations in usability, security, privilege management, and data transmission efficiency. To resolve these challenges, we introduce BridgeScope, a universal toolkit bridging LLMs and databases through three key innovations. First, it modularizes SQL operations into fine-grained tools for context retrieval, CRUD execution, and ACID-compliant transaction management, enabling more precise and LLM-friendly functionality controls. Second, it aligns tool implementations with both database privileges and user security policies to steer LLMs away from unsafe or unauthorized operations, improving task execution efficiency while safeguarding database security. Third, it introduces a proxy mechanism for seamless inter-tool data transfer, bypassing LLM transmission bottlenecks. All of these designs are database-agnostic and can be transparently integrated with existing agent architectures. We also release an open-source implementation of BridgeScope for PostgreSQL. Evaluations on two novel benchmarks demonstrate that BridgeScope enables LLM agents to operate databases more effectively, reduces token usage by up to 80% through improved security awareness, and uniquely supports data-intensive workflows beyond existing toolkits, establishing BridgeScope as a robust foundation for next-generation intelligent data automation.", "summary_bullets": ["As large language models (LLMs) demonstrate increasingly powerful reasoning and orchestration capabilities, LLM-based agents are rapidly proliferating for complex data-related tasks.", "Despite this progress, the current design of how LLMs interact with databases exhibits critical limitations in usability, security, privilege management, and data transmission efficiency.", "To resolve these challenges, we introduce BridgeScope, a universal toolkit bridging LLMs and databases through three key innovations."], "method": "To resolve these challenges, we introduce BridgeScope, a universal toolkit bridging LLMs and databases through three key innovations.", "key_results": ["Evaluations on two novel benchmarks demonstrate that BridgeScope enables LLM agents to operate databases more effectively, reduces token usage by up to 80% through improved security awareness, and uniquely supports data-intensive workflows beyond existing toolkits, establishing BridgeScope as a robust foundation for next-generation intelligent data automation."], "limitations": ["Abstract-level evidence only: validate assumptions, evaluation protocol, and failure cases in the full paper before relying on this as key evidence.", "Despite this progress, the current design of how LLMs interact with databases exhibits critical limitations in usability, security, privilege management, and data transmission efficiency."], "bibkey": "Weng2025Bridgescope"}
{"paper_id": "P0156", "title": "Causal MAS: A Survey of Large Language Model Architectures for Discovery and Effect Estimation", "year": 2025, "url": "http://arxiv.org/abs/2509.00987v1", "arxiv_id": "2509.00987v1", "primary_category": "cs.AI", "categories": ["cs.AI"], "pdf_url": "https://arxiv.org/pdf/2509.00987v1", "priority": "normal", "mapped_sections": [], "evidence_level": "abstract", "fulltext_path": "", "authors": ["Adib Bazgir", "Amir Habibdoust", "Yuwen Zhang", "Xing Song"], "abstract": "Large Language Models (LLMs) have demonstrated remarkable capabilities in various reasoning and generation tasks. However, their proficiency in complex causal reasoning, discovery, and estimation remains an area of active development, often hindered by issues like hallucination, reliance on spurious correlations, and difficulties in handling nuanced, domain-specific, or personalized causal relationships. Multi-agent systems, leveraging the collaborative or specialized abilities of multiple LLM-based agents, are emerging as a powerful paradigm to address these limitations. This review paper explores the burgeoning field of causal multi-agent LLMs. We examine how these systems are designed to tackle different facets of causality, including causal reasoning and counterfactual analysis, causal discovery from data, and the estimation of causal effects. We delve into the diverse architectural patterns and interaction protocols employed, from pipeline-based processing and debate frameworks to simulation environments and iterative refinement loops. Furthermore, we discuss the evaluation methodologies, benchmarks, and diverse application domains where causal multi-agent LLMs are making an impact, including scientific discovery, healthcare, fact-checking, and personalized systems. Finally, we highlight the persistent challenges, open research questions, and promising future directions in this synergistic field, aiming to provide a comprehensive overview of its current state and potential trajectory.", "summary_bullets": ["Large Language Models (LLMs) have demonstrated remarkable capabilities in various reasoning and generation tasks.", "However, their proficiency in complex causal reasoning, discovery, and estimation remains an area of active development, often hindered by issues like hallucination, reliance on spurious correlations, and difficulties in handling nuanced, domain-specific, or personalized causal relationships.", "Multi-agent systems, leveraging the collaborative or specialized abilities of multiple LLM-based agents, are emerging as a powerful paradigm to address these limitations."], "method": "Large Language Models (LLMs) have demonstrated remarkable capabilities in various reasoning and generation tasks.", "key_results": ["Furthermore, we discuss the evaluation methodologies, benchmarks, and diverse application domains where causal multi-agent LLMs are making an impact, including scientific discovery, healthcare, fact-checking, and personalized systems."], "limitations": ["Abstract-level evidence only: validate assumptions, evaluation protocol, and failure cases in the full paper before relying on this as key evidence.", "Multi-agent systems, leveraging the collaborative or specialized abilities of multiple LLM-based agents, are emerging as a powerful paradigm to address these limitations."], "bibkey": "Bazgir2025Causal"}
{"paper_id": "P0157", "title": "ChatVis: Large Language Model Agent for Generating Scientific Visualizations", "year": 2025, "url": "http://arxiv.org/abs/2507.23096v1", "arxiv_id": "2507.23096v1", "primary_category": "cs.HC", "categories": ["cs.HC"], "pdf_url": "https://arxiv.org/pdf/2507.23096v1", "priority": "normal", "mapped_sections": [], "evidence_level": "abstract", "fulltext_path": "", "authors": ["Tom Peterka", "Tanwi Mallick", "Orcun Yildiz", "David Lenz", "Cory Quammen", "Berk Geveci"], "abstract": "Large language models (LLMs) are rapidly increasing in capability, but they still struggle with highly specialized programming tasks such as scientific visualization. We present an LLM assistant, ChatVis, that aids the LLM to generate Python code for ParaView scientific visualization tasks, without the need for retraining or fine-tuning the LLM. ChatVis employs chain-of-thought prompt simplification, retrieval-augmented prompt generation using a vector database of documentation and code examples, and error checking with iterative prompt feedback to correct errors until a visualization is produced. An integral part of our approach is a benchmark suite of canonical visualization tasks, ParaView regression tests, and scientific use cases that includes comprehensive evaluation metrics. We evaluate our visualization assistant by comparing results with a variety of top-performing unassisted LLMs. We find that all the metrics are significantly improved with ChatVis.", "summary_bullets": ["Large language models (LLMs) are rapidly increasing in capability, but they still struggle with highly specialized programming tasks such as scientific visualization.", "We present an LLM assistant, ChatVis, that aids the LLM to generate Python code for ParaView scientific visualization tasks, without the need for retraining or fine-tuning the LLM.", "ChatVis employs chain-of-thought prompt simplification, retrieval-augmented prompt generation using a vector database of documentation and code examples, and error checking with iterative prompt feedback to correct errors until a visualization is produced."], "method": "We present an LLM assistant, ChatVis, that aids the LLM to generate Python code for ParaView scientific visualization tasks, without the need for retraining or fine-tuning the LLM.", "key_results": ["An integral part of our approach is a benchmark suite of canonical visualization tasks, ParaView regression tests, and scientific use cases that includes comprehensive evaluation metrics.", "We find that all the metrics are significantly improved with ChatVis."], "limitations": ["Abstract-level evidence only: validate assumptions, evaluation protocol, and failure cases in the full paper before relying on this as key evidence."], "bibkey": "Peterka2025Chatvis"}
{"paper_id": "P0158", "title": "Check Yourself Before You Wreck Yourself: Selectively Quitting Improves LLM Agent Safety", "year": 2025, "url": "http://arxiv.org/abs/2510.16492v2", "arxiv_id": "2510.16492v2", "primary_category": "cs.CL", "categories": ["cs.CL"], "pdf_url": "https://arxiv.org/pdf/2510.16492v2", "priority": "normal", "mapped_sections": ["6.2"], "evidence_level": "abstract", "fulltext_path": "", "authors": ["Vamshi Krishna Bonagiri", "Ponnurangam Kumaragurum", "Khanh Nguyen", "Benjamin Plaut"], "abstract": "As Large Language Model (LLM) agents increasingly operate in complex environments with real-world consequences, their safety becomes critical. While uncertainty quantification is well-studied for single-turn tasks, multi-turn agentic scenarios with real-world tool access present unique challenges where uncertainties and ambiguities compound, leading to severe or catastrophic risks beyond traditional text generation failures. We propose using \"quitting\" as a simple yet effective behavioral mechanism for LLM agents to recognize and withdraw from situations where they lack confidence. Leveraging the ToolEmu framework, we conduct a systematic evaluation of quitting behavior across 12 state-of-the-art LLMs. Our results demonstrate a highly favorable safety-helpfulness trade-off: agents prompted to quit with explicit instructions improve safety by an average of +0.39 on a 0-3 scale across all models (+0.64 for proprietary models), while maintaining a negligible average decrease of -0.03 in helpfulness. Our analysis demonstrates that simply adding explicit quit instructions proves to be a highly effective safety mechanism that can immediately be deployed in existing agent systems, and establishes quitting as an effective first-line defense mechanism for autonomous agents in high-stakes applications.", "summary_bullets": ["As Large Language Model (LLM) agents increasingly operate in complex environments with real-world consequences, their safety becomes critical.", "While uncertainty quantification is well-studied for single-turn tasks, multi-turn agentic scenarios with real-world tool access present unique challenges where uncertainties and ambiguities compound, leading to severe or catastrophic risks beyond traditional text generation failures.", "We propose using \"quitting\" as a simple yet effective behavioral mechanism for LLM agents to recognize and withdraw from situations where they lack confidence."], "method": "We propose using \"quitting\" as a simple yet effective behavioral mechanism for LLM agents to recognize and withdraw from situations where they lack confidence.", "key_results": ["Leveraging the ToolEmu framework, we conduct a systematic evaluation of quitting behavior across 12 state-of-the-art LLMs.", "Our results demonstrate a highly favorable safety-helpfulness trade-off: agents prompted to quit with explicit instructions improve safety by an average of +0.39 on a 0-3 scale across all models (+0.64 for proprietary models), while maintaining a negligible average decrease of -0.03 in helpfulness."], "limitations": ["Abstract-level evidence only: validate assumptions, evaluation protocol, and failure cases in the full paper before relying on this as key evidence."], "bibkey": "Bonagiri2025Check"}
{"paper_id": "P0159", "title": "CogniPair: From LLM Chatbots to Conscious AI Agents -- GNWT-Based Multi-Agent Digital Twins for Social Pairing -- Dating & Hiring Applications", "year": 2025, "url": "http://arxiv.org/abs/2506.03543v2", "arxiv_id": "2506.03543v2", "primary_category": "cs.AI", "categories": ["cs.AI", "cs.CY", "cs.MA"], "pdf_url": "https://arxiv.org/pdf/2506.03543v2", "priority": "normal", "mapped_sections": ["5.2"], "evidence_level": "abstract", "fulltext_path": "", "authors": ["Wanghao Ye", "Sihan Chen", "Yiting Wang", "Shwai He", "Bowei Tian", "Guoheng Sun", "Ziyi Wang", "Ziyao Wang", "Yexiao He", "Zheyu Shen", "Meng Liu", "Yuning Zhang", "Meng Feng", "Yang Wang", "Siyuan Peng", "Yilong Dai", "Zhenle Duan", "Lang Xiong", "Joshua Liu", "Hanzhang Qin", "Ang Li"], "abstract": "Current large language model (LLM) agents lack authentic human psychological processes necessary for genuine digital twins and social AI applications. To address this limitation, we present a computational implementation of Global Workspace Theory (GNWT) that integrates human cognitive architecture principles into LLM agents, creating specialized sub-agents for emotion, memory, social norms, planning, and goal-tracking coordinated through a global workspace mechanism. However, authentic digital twins require accurate personality initialization. We therefore develop a novel adventure-based personality test that evaluates true personality through behavioral choices within interactive scenarios, bypassing self-presentation bias found in traditional assessments. Building on these innovations, our CogniPair platform enables digital twins to engage in realistic simulated dating interactions and job interviews before real encounters, providing bidirectional cultural fit assessment for both romantic compatibility and workplace matching. Validation using 551 GNWT-Agents and Columbia University Speed Dating dataset demonstrates 72% correlation with human attraction patterns, 77.8% match prediction accuracy, and 74% agreement in human validation studies. This work advances psychological authenticity in LLM agents and establishes a foundation for intelligent dating platforms and HR technology solutions.", "summary_bullets": ["Current large language model (LLM) agents lack authentic human psychological processes necessary for genuine digital twins and social AI applications.", "To address this limitation, we present a computational implementation of Global Workspace Theory (GNWT) that integrates human cognitive architecture principles into LLM agents, creating specialized sub-agents for emotion, memory, social norms, planning, and goal-tracking coordinated through a global workspace mechanism.", "However, authentic digital twins require accurate personality initialization."], "method": "To address this limitation, we present a computational implementation of Global Workspace Theory (GNWT) that integrates human cognitive architecture principles into LLM agents, creating specialized sub-agents for emotion, memory, social norms, planning, and goal-tracking coordinated through a global workspace mechanism.", "key_results": ["Validation using 551 GNWT-Agents and Columbia University Speed Dating dataset demonstrates 72% correlation with human attraction patterns, 77.8% match prediction accuracy, and 74% agreement in human validation studies.", "Current large language model (LLM) agents lack authentic human psychological processes necessary for genuine digital twins and social AI applications."], "limitations": ["Abstract-level evidence only: validate assumptions, evaluation protocol, and failure cases in the full paper before relying on this as key evidence.", "To address this limitation, we present a computational implementation of Global Workspace Theory (GNWT) that integrates human cognitive architecture principles into LLM agents, creating specialized sub-agents for emotion, memory, social norms, planning, and goal-tracking coordinated through a global workspace mechanism."], "bibkey": "Ye2025Cognipair"}
{"paper_id": "P0160", "title": "CostBench: Evaluating Multi-Turn Cost-Optimal Planning and Adaptation in Dynamic Environments for LLM Tool-Use Agents", "year": 2025, "url": "http://arxiv.org/abs/2511.02734v1", "arxiv_id": "2511.02734v1", "primary_category": "cs.AI", "categories": ["cs.AI", "cs.CL"], "pdf_url": "https://arxiv.org/pdf/2511.02734v1", "priority": "normal", "mapped_sections": [], "evidence_level": "abstract", "fulltext_path": "", "authors": ["Jiayu Liu", "Cheng Qian", "Zhaochen Su", "Qing Zong", "Shijue Huang", "Bingxiang He", "Yi R. Fung"], "abstract": "Current evaluations of Large Language Model (LLM) agents primarily emphasize task completion, often overlooking resource efficiency and adaptability. This neglects a crucial capability: agents' ability to devise and adjust cost-optimal plans in response to changing environments. To bridge this gap, we introduce CostBench, a scalable, cost-centric benchmark designed to evaluate agents' economic reasoning and replanning abilities. Situated in the travel-planning domain, CostBench comprises tasks solvable via multiple sequences of atomic and composite tools with diverse, customizable costs. It also supports four types of dynamic blocking events, such as tool failures and cost changes, to simulate real-world unpredictability and necessitate agents to adapt in real time. Evaluating leading open-sourced and proprietary models on CostBench reveals a substantial gap in cost-aware planning: agents frequently fail to identify cost-optimal solutions in static settings, with even GPT-5 achieving less than 75% exact match rate on the hardest tasks, and performance further dropping by around 40% under dynamic conditions. By diagnosing these weaknesses, CostBench lays the groundwork for developing future agents that are both economically rational and robust.", "summary_bullets": ["Current evaluations of Large Language Model (LLM) agents primarily emphasize task completion, often overlooking resource efficiency and adaptability.", "This neglects a crucial capability: agents' ability to devise and adjust cost-optimal plans in response to changing environments.", "To bridge this gap, we introduce CostBench, a scalable, cost-centric benchmark designed to evaluate agents' economic reasoning and replanning abilities."], "method": "To bridge this gap, we introduce CostBench, a scalable, cost-centric benchmark designed to evaluate agents' economic reasoning and replanning abilities.", "key_results": ["Evaluating leading open-sourced and proprietary models on CostBench reveals a substantial gap in cost-aware planning: agents frequently fail to identify cost-optimal solutions in static settings, with even GPT-5 achieving less than 75% exact match rate on the hardest tasks, and performance further dropping by around 40% under dynamic conditions.", "To bridge this gap, we introduce CostBench, a scalable, cost-centric benchmark designed to evaluate agents' economic reasoning and replanning abilities."], "limitations": ["Abstract-level evidence only: validate assumptions, evaluation protocol, and failure cases in the full paper before relying on this as key evidence."], "bibkey": "Liu2025Costbench"}
{"paper_id": "P0161", "title": "DICE: Dynamic In-Context Example Selection in LLM Agents via Efficient Knowledge Transfer", "year": 2025, "url": "http://arxiv.org/abs/2507.23554v1", "arxiv_id": "2507.23554v1", "primary_category": "cs.AI", "categories": ["cs.AI"], "pdf_url": "https://arxiv.org/pdf/2507.23554v1", "priority": "normal", "mapped_sections": [], "evidence_level": "abstract", "fulltext_path": "", "authors": ["Ruoyu Wang", "Junda Wu", "Yu Xia", "Tong Yu", "Ryan A. Rossi", "Julian McAuley", "Lina Yao"], "abstract": "Large language model-based agents, empowered by in-context learning (ICL), have demonstrated strong capabilities in complex reasoning and tool-use tasks. However, existing works have shown that the effectiveness of ICL is highly sensitive to the choice of demonstrations, with suboptimal examples often leading to unstable or degraded performance. While prior work has explored example selection, including in some agentic or multi-step settings, existing approaches typically rely on heuristics or task-specific designs and lack a general, theoretically grounded criterion for what constitutes an effective demonstration across reasoning steps. Therefore, it is non-trivial to develop a principled, general-purpose method for selecting demonstrations that consistently benefit agent performance. In this paper, we address this challenge with DICE, Dynamic In-Context Example Selection for LLM Agents, a theoretically grounded ICL framework for agentic tasks that selects the most relevant demonstrations at each step of reasoning. Our approach decomposes demonstration knowledge into transferable and non-transferable components through a causal lens, showing how the latter can introduce spurious dependencies that impair generalization. We further propose a stepwise selection criterion with a formal guarantee of improved agent performance. Importantly, DICE is a general, framework-agnostic solution that can be integrated as a plug-in module into existing agentic frameworks without any additional training cost. Extensive experiments across diverse domains demonstrate our method's effectiveness and generality, highlighting the importance of principled, context-aware demo selection for robust and efficient LLM agents.", "summary_bullets": ["Large language model-based agents, empowered by in-context learning (ICL), have demonstrated strong capabilities in complex reasoning and tool-use tasks.", "However, existing works have shown that the effectiveness of ICL is highly sensitive to the choice of demonstrations, with suboptimal examples often leading to unstable or degraded performance.", "While prior work has explored example selection, including in some agentic or multi-step settings, existing approaches typically rely on heuristics or task-specific designs and lack a general, theoretically grounded criterion for what constitutes an effective demonstration across reasoning steps."], "method": "Our approach decomposes demonstration knowledge into transferable and non-transferable components through a causal lens, showing how the latter can introduce spurious dependencies that impair generalization.", "key_results": ["Extensive experiments across diverse domains demonstrate our method's effectiveness and generality, highlighting the importance of principled, context-aware demo selection for robust and efficient LLM agents."], "limitations": ["Abstract-level evidence only: validate assumptions, evaluation protocol, and failure cases in the full paper before relying on this as key evidence."], "bibkey": "Wang2025Dice"}
{"paper_id": "P0162", "title": "DataSciBench: An LLM Agent Benchmark for Data Science", "year": 2025, "url": "http://arxiv.org/abs/2502.13897v1", "arxiv_id": "2502.13897v1", "primary_category": "cs.CL", "categories": ["cs.CL", "cs.AI", "cs.LG"], "pdf_url": "https://arxiv.org/pdf/2502.13897v1", "priority": "normal", "mapped_sections": ["6.1"], "evidence_level": "abstract", "fulltext_path": "", "authors": ["Dan Zhang", "Sining Zhoubian", "Min Cai", "Fengzu Li", "Lekang Yang", "Wei Wang", "Tianjiao Dong", "Ziniu Hu", "Jie Tang", "Yisong Yue"], "abstract": "This paper presents DataSciBench, a comprehensive benchmark for evaluating Large Language Model (LLM) capabilities in data science. Recent related benchmarks have primarily focused on single tasks, easily obtainable ground truth, and straightforward evaluation metrics, which limits the scope of tasks that can be evaluated. In contrast, DataSciBench is constructed based on a more comprehensive and curated collection of natural and challenging prompts for uncertain ground truth and evaluation metrics. We develop a semi-automated pipeline for generating ground truth (GT) and validating evaluation metrics. This pipeline utilizes and implements an LLM-based self-consistency and human verification strategy to produce accurate GT by leveraging collected prompts, predefined task types, and aggregate functions (metrics). Furthermore, we propose an innovative Task - Function - Code (TFC) framework to assess each code execution outcome based on precisely defined metrics and programmatic rules. Our experimental framework involves testing 6 API-based models, 8 open-source general models, and 9 open-source code generation models using the diverse set of prompts we have gathered. This approach aims to provide a more comprehensive and rigorous evaluation of LLMs in data science, revealing their strengths and weaknesses. Experimental results demonstrate that API-based models outperform open-sourced models on all metrics and Deepseek-Coder-33B-Instruct achieves the highest score among open-sourced models. We release all code and data at https://github.com/THUDM/DataSciBench.", "summary_bullets": ["This paper presents DataSciBench, a comprehensive benchmark for evaluating Large Language Model (LLM) capabilities in data science.", "Recent related benchmarks have primarily focused on single tasks, easily obtainable ground truth, and straightforward evaluation metrics, which limits the scope of tasks that can be evaluated.", "In contrast, DataSciBench is constructed based on a more comprehensive and curated collection of natural and challenging prompts for uncertain ground truth and evaluation metrics."], "method": "We develop a semi-automated pipeline for generating ground truth (GT) and validating evaluation metrics.", "key_results": ["Our experimental framework involves testing 6 API-based models, 8 open-source general models, and 9 open-source code generation models using the diverse set of prompts we have gathered.", "Experimental results demonstrate that API-based models outperform open-sourced models on all metrics and Deepseek-Coder-33B-Instruct achieves the highest score among open-sourced models."], "limitations": ["Abstract-level evidence only: validate assumptions, evaluation protocol, and failure cases in the full paper before relying on this as key evidence."], "bibkey": "Zhang2025Datascibench"}
{"paper_id": "P0163", "title": "Decentralized Multi-Agent Goal Assignment for Path Planning using Large Language Models", "year": 2025, "url": "http://arxiv.org/abs/2510.23824v1", "arxiv_id": "2510.23824v1", "primary_category": "cs.AI", "categories": ["cs.AI"], "pdf_url": "https://arxiv.org/pdf/2510.23824v1", "priority": "normal", "mapped_sections": [], "evidence_level": "abstract", "fulltext_path": "", "authors": ["Murad Ismayilov", "Edwin Meriaux", "Shuo Wen", "Gregory Dudek"], "abstract": "Coordinating multiple autonomous agents in shared environments under decentralized conditions is a long-standing challenge in robotics and artificial intelligence. This work addresses the problem of decentralized goal assignment for multi-agent path planning, where agents independently generate ranked preferences over goals based on structured representations of the environment, including grid visualizations and scenario data. After this reasoning phase, agents exchange their goal rankings, and assignments are determined by a fixed, deterministic conflict-resolution rule (e.g., agent index ordering), without negotiation or iterative coordination. We systematically compare greedy heuristics, optimal assignment, and large language model (LLM)-based agents in fully observable grid-world settings. Our results show that LLM-based agents, when provided with well-designed prompts and relevant quantitative information, can achieve near-optimal makespans and consistently outperform traditional heuristics. These findings underscore the potential of language models for decentralized goal assignment in multi-agent path planning and highlight the importance of information structure in such systems.", "summary_bullets": ["Coordinating multiple autonomous agents in shared environments under decentralized conditions is a long-standing challenge in robotics and artificial intelligence.", "This work addresses the problem of decentralized goal assignment for multi-agent path planning, where agents independently generate ranked preferences over goals based on structured representations of the environment, including grid visualizations and scenario data.", "After this reasoning phase, agents exchange their goal rankings, and assignments are determined by a fixed, deterministic conflict-resolution rule (e.g., agent index ordering), without negotiation or iterative coordination."], "method": "Coordinating multiple autonomous agents in shared environments under decentralized conditions is a long-standing challenge in robotics and artificial intelligence.", "key_results": ["Our results show that LLM-based agents, when provided with well-designed prompts and relevant quantitative information, can achieve near-optimal makespans and consistently outperform traditional heuristics."], "limitations": ["Abstract-level evidence only: validate assumptions, evaluation protocol, and failure cases in the full paper before relying on this as key evidence."], "bibkey": "Ismayilov2025Decentralized"}
{"paper_id": "P0164", "title": "DeepSeek performs better than other Large Language Models in Dental Cases", "year": 2025, "url": "http://arxiv.org/abs/2509.02036v1", "arxiv_id": "2509.02036v1", "primary_category": "cs.CL", "categories": ["cs.CL", "cs.AI"], "pdf_url": "https://arxiv.org/pdf/2509.02036v1", "priority": "normal", "mapped_sections": [], "evidence_level": "abstract", "fulltext_path": "", "authors": ["Hexian Zhang", "Xinyu Yan", "Yanqi Yang", "Lijian Jin", "Ping Yang", "Junwen Wang"], "abstract": "Large language models (LLMs) hold transformative potential in healthcare, yet their capacity to interpret longitudinal patient narratives remains inadequately explored. Dentistry, with its rich repository of structured clinical data, presents a unique opportunity to rigorously assess LLMs' reasoning abilities. While several commercial LLMs already exist, DeepSeek, a model that gained significant attention earlier this year, has also joined the competition. This study evaluated four state-of-the-art LLMs (GPT-4o, Gemini 2.0 Flash, Copilot, and DeepSeek V3) on their ability to analyze longitudinal dental case vignettes through open-ended clinical tasks. Using 34 standardized longitudinal periodontal cases (comprising 258 question-answer pairs), we assessed model performance via automated metrics and blinded evaluations by licensed dentists. DeepSeek emerged as the top performer, demonstrating superior faithfulness (median score = 0.528 vs. 0.367-0.457) and higher expert ratings (median = 4.5/5 vs. 4.0/5), without significantly compromising readability. Our study positions DeepSeek as the leading LLM for case analysis, endorses its integration as an adjunct tool in both medical education and research, and highlights its potential as a domain-specific agent.", "summary_bullets": ["Large language models (LLMs) hold transformative potential in healthcare, yet their capacity to interpret longitudinal patient narratives remains inadequately explored.", "Dentistry, with its rich repository of structured clinical data, presents a unique opportunity to rigorously assess LLMs' reasoning abilities.", "While several commercial LLMs already exist, DeepSeek, a model that gained significant attention earlier this year, has also joined the competition."], "method": "Large language models (LLMs) hold transformative potential in healthcare, yet their capacity to interpret longitudinal patient narratives remains inadequately explored.", "key_results": ["Using 34 standardized longitudinal periodontal cases (comprising 258 question-answer pairs), we assessed model performance via automated metrics and blinded evaluations by licensed dentists.", "This study evaluated four state-of-the-art LLMs (GPT-4o, Gemini 2.0 Flash, Copilot, and DeepSeek V3) on their ability to analyze longitudinal dental case vignettes through open-ended clinical tasks."], "limitations": ["Abstract-level evidence only: validate assumptions, evaluation protocol, and failure cases in the full paper before relying on this as key evidence."], "bibkey": "Zhang2025Deepseek"}
{"paper_id": "P0165", "title": "Digital Player: Evaluating Large Language Models based Human-like Agent in Games", "year": 2025, "url": "http://arxiv.org/abs/2502.20807v1", "arxiv_id": "2502.20807v1", "primary_category": "cs.LG", "categories": ["cs.LG"], "pdf_url": "https://arxiv.org/pdf/2502.20807v1", "priority": "normal", "mapped_sections": [], "evidence_level": "abstract", "fulltext_path": "", "authors": ["Jiawei Wang", "Kai Wang", "Shaojie Lin", "Runze Wu", "Bihan Xu", "Lingeng Jiang", "Shiwei Zhao", "Renyu Zhu", "Haoyu Liu", "Zhipeng Hu", "Zhong Fan", "Le Li", "Tangjie Lyu", "Changjie Fan"], "abstract": "With the rapid advancement of Large Language Models (LLMs), LLM-based autonomous agents have shown the potential to function as digital employees, such as digital analysts, teachers, and programmers. In this paper, we develop an application-level testbed based on the open-source strategy game \"Unciv\", which has millions of active players, to enable researchers to build a \"data flywheel\" for studying human-like agents in the \"digital players\" task. This \"Civilization\"-like game features expansive decision-making spaces along with rich linguistic interactions such as diplomatic negotiations and acts of deception, posing significant challenges for LLM-based agents in terms of numerical reasoning and long-term planning. Another challenge for \"digital players\" is to generate human-like responses for social interaction, collaboration, and negotiation with human players. The open-source project can be found at https:/github.com/fuxiAIlab/CivAgent.", "summary_bullets": ["With the rapid advancement of Large Language Models (LLMs), LLM-based autonomous agents have shown the potential to function as digital employees, such as digital analysts, teachers, and programmers.", "In this paper, we develop an application-level testbed based on the open-source strategy game \"Unciv\", which has millions of active players, to enable researchers to build a \"data flywheel\" for studying human-like agents in the \"digital players\" task.", "This \"Civilization\"-like game features expansive decision-making spaces along with rich linguistic interactions such as diplomatic negotiations and acts of deception, posing significant challenges for LLM-based agents in terms of numerical reasoning and long-term planning."], "method": "In this paper, we develop an application-level testbed based on the open-source strategy game \"Unciv\", which has millions of active players, to enable researchers to build a \"data flywheel\" for studying human-like agents in the \"digital players\" task.", "key_results": ["In this paper, we develop an application-level testbed based on the open-source strategy game \"Unciv\", which has millions of active players, to enable researchers to build a \"data flywheel\" for studying human-like agents in the \"digital players\" task.", "Another challenge for \"digital players\" is to generate human-like responses for social interaction, collaboration, and negotiation with human players."], "limitations": ["Abstract-level evidence only: validate assumptions, evaluation protocol, and failure cases in the full paper before relying on this as key evidence."], "bibkey": "Wang2025Digital"}
{"paper_id": "P0166", "title": "Distilling LLM Agent into Small Models with Retrieval and Code Tools", "year": 2025, "url": "http://arxiv.org/abs/2505.17612v2", "arxiv_id": "2505.17612v2", "primary_category": "cs.CL", "categories": ["cs.CL", "cs.AI"], "pdf_url": "https://arxiv.org/pdf/2505.17612v2", "priority": "normal", "mapped_sections": ["4.2"], "evidence_level": "abstract", "fulltext_path": "", "authors": ["Minki Kang", "Jongwon Jeong", "Seanie Lee", "Jaewoong Cho", "Sung Ju Hwang"], "abstract": "Large language models (LLMs) excel at complex reasoning tasks but remain computationally expensive, limiting their practical deployment. To address this, recent works have focused on distilling reasoning capabilities into smaller language models (sLMs) using chain-of-thought (CoT) traces from teacher LLMs. However, this approach struggles in scenarios requiring rare factual knowledge or precise computation, where sLMs often hallucinate due to limited capability. In this work, we propose Agent Distillation, a framework for transferring not only reasoning capability but full task-solving behavior from LLM-based agents into sLMs with retrieval and code tools. We improve agent distillation along two complementary axes: (1) we introduce a prompting method called first-thought prefix to enhance the quality of teacher-generated trajectories; and (2) we propose a self-consistent action generation for improving test-time robustness of small agents. We evaluate our method on eight reasoning tasks across factual and mathematical domains, covering both in-domain and out-of-domain generalization. Our results show that sLMs as small as 0.5B, 1.5B, 3B parameters can achieve performance competitive with next-tier larger 1.5B, 3B, 7B models fine-tuned using CoT distillation, demonstrating the potential of agent distillation for building practical, tool-using small agents. Our code is available at https://github.com/Nardien/agent-distillation.", "summary_bullets": ["Large language models (LLMs) excel at complex reasoning tasks but remain computationally expensive, limiting their practical deployment.", "To address this, recent works have focused on distilling reasoning capabilities into smaller language models (sLMs) using chain-of-thought (CoT) traces from teacher LLMs.", "However, this approach struggles in scenarios requiring rare factual knowledge or precise computation, where sLMs often hallucinate due to limited capability."], "method": "In this work, we propose Agent Distillation, a framework for transferring not only reasoning capability but full task-solving behavior from LLM-based agents into sLMs with retrieval and code tools.", "key_results": ["We improve agent distillation along two complementary axes: (1) we introduce a prompting method called first-thought prefix to enhance the quality of teacher-generated trajectories; and (2) we propose a self-consistent action generation for improving test-time robustness of small agents.", "Our results show that sLMs as small as 0.5B, 1.5B, 3B parameters can achieve performance competitive with next-tier larger 1.5B, 3B, 7B models fine-tuned using CoT distillation, demonstrating the potential of agent distillation for building practical, tool-using small agents."], "limitations": ["Abstract-level evidence only: validate assumptions, evaluation protocol, and failure cases in the full paper before relying on this as key evidence."], "bibkey": "Kang2025Distilling"}
{"paper_id": "P0167", "title": "Do Large Language Models Exhibit Spontaneous Rational Deception?", "year": 2025, "url": "http://arxiv.org/abs/2504.00285v1", "arxiv_id": "2504.00285v1", "primary_category": "cs.CL", "categories": ["cs.CL"], "pdf_url": "https://arxiv.org/pdf/2504.00285v1", "priority": "normal", "mapped_sections": ["3.2", "5.1"], "evidence_level": "abstract", "fulltext_path": "", "authors": ["Samuel M. Taylor", "Benjamin K. Bergen"], "abstract": "Large Language Models (LLMs) are effective at deceiving, when prompted to do so. But under what conditions do they deceive spontaneously? Models that demonstrate better performance on reasoning tasks are also better at prompted deception. Do they also increasingly deceive spontaneously in situations where it could be considered rational to do so? This study evaluates spontaneous deception produced by LLMs in a preregistered experimental protocol using tools from signaling theory. A range of proprietary closed-source and open-source LLMs are evaluated using modified 2x2 games (in the style of Prisoner's Dilemma) augmented with a phase in which they can freely communicate to the other agent using unconstrained language. This setup creates an opportunity to deceive, in conditions that vary in how useful deception might be to an agent's rational self-interest. The results indicate that 1) all tested LLMs spontaneously misrepresent their actions in at least some conditions, 2) they are generally more likely to do so in situations in which deception would benefit them, and 3) models exhibiting better reasoning capacity overall tend to deceive at higher rates. Taken together, these results suggest a tradeoff between LLM reasoning capability and honesty. They also provide evidence of reasoning-like behavior in LLMs from a novel experimental configuration. Finally, they reveal certain contextual factors that affect whether LLMs will deceive or not. We discuss consequences for autonomous, human-facing systems driven by LLMs both now and as their reasoning capabilities continue to improve.", "summary_bullets": ["Large Language Models (LLMs) are effective at deceiving, when prompted to do so.", "But under what conditions do they deceive spontaneously?", "Models that demonstrate better performance on reasoning tasks are also better at prompted deception."], "method": "Large Language Models (LLMs) are effective at deceiving, when prompted to do so.", "key_results": ["The results indicate that 1) all tested LLMs spontaneously misrepresent their actions in at least some conditions, 2) they are generally more likely to do so in situations in which deception would benefit them, and 3) models exhibiting better reasoning capacity overall tend to deceive at higher rates.", "We discuss consequences for autonomous, human-facing systems driven by LLMs both now and as their reasoning capabilities continue to improve."], "limitations": ["Abstract-level evidence only: validate assumptions, evaluation protocol, and failure cases in the full paper before relying on this as key evidence."], "bibkey": "Taylor2025Large"}
{"paper_id": "P0168", "title": "Doing More with Less: A Survey on Routing Strategies for Resource Optimisation in Large Language Model-Based Systems", "year": 2025, "url": "http://arxiv.org/abs/2502.00409v3", "arxiv_id": "2502.00409v3", "primary_category": "cs.AI", "categories": ["cs.AI", "cs.CL"], "pdf_url": "https://arxiv.org/pdf/2502.00409v3", "priority": "normal", "mapped_sections": ["6.2"], "evidence_level": "abstract", "fulltext_path": "", "authors": ["Clovis Varangot-Reille", "Christophe Bouvard", "Antoine Gourru", "Mathieu Ciancone", "Marion Schaeffer", "Fran√ßois Jacquenet"], "abstract": "Large Language Model (LLM)-based systems, i.e. interconnected elements that include an LLM as a central component, such as conversational agents, are usually designed with monolithic, static architectures that rely on a single, general-purpose LLM to handle all user queries. However, these systems may be inefficient as different queries may require different levels of reasoning, domain knowledge or pre-processing. While generalist LLMs (e.g. GPT-4o, Claude-Sonnet) perform well across a wide range of tasks, they may incur significant financial, energy and computational costs. These costs may be disproportionate for simpler queries, resulting in unnecessary resource utilisation. A routing mechanism can therefore be employed to route queries to more appropriate components, such as smaller or specialised models, thereby improving efficiency and optimising resource consumption. This survey aims to provide a comprehensive overview of routing strategies in LLM-based systems. Specifically, it reviews when, why, and how routing should be integrated into LLM pipelines to improve efficiency, scalability, and performance. We define the objectives to optimise, such as cost minimisation and performance maximisation, and discuss the timing of routing within the LLM workflow, whether it occurs before or after generation. We also detail the various implementation strategies, including similarity-based, supervised, reinforcement learning-based, and generative methods. Practical considerations such as industrial applications and current limitations are also examined, like standardising routing experiments, accounting for non-financial costs, and designing adaptive strategies. By formalising routing as a performance-cost optimisation problem, this survey provides tools and directions to guide future research and development of adaptive low-cost LLM-based systems.", "summary_bullets": ["Large Language Model (LLM)-based systems, i.e. interconnected elements that include an LLM as a central component, such as conversational agents, are usually designed with monolithic, static architectures that rely on a single, general-purpose LLM to handle all user queries.", "However, these systems may be inefficient as different queries may require different levels of reasoning, domain knowledge or pre-processing.", "While generalist LLMs (e.g. GPT-4o, Claude-Sonnet) perform well across a wide range of tasks, they may incur significant financial, energy and computational costs."], "method": "Large Language Model (LLM)-based systems, i.e. interconnected elements that include an LLM as a central component, such as conversational agents, are usually designed with monolithic, static architectures that rely on a single, general-purpose LLM to handle all user queries.", "key_results": ["By formalising routing as a performance-cost optimisation problem, this survey provides tools and directions to guide future research and development of adaptive low-cost LLM-based systems."], "limitations": ["Abstract-level evidence only: validate assumptions, evaluation protocol, and failure cases in the full paper before relying on this as key evidence.", "Practical considerations such as industrial applications and current limitations are also examined, like standardising routing experiments, accounting for non-financial costs, and designing adaptive strategies."], "bibkey": "VarangotReille2025Doing"}
{"paper_id": "P0169", "title": "Echo: A Large Language Model with Temporal Episodic Memory", "year": 2025, "url": "http://arxiv.org/abs/2502.16090v1", "arxiv_id": "2502.16090v1", "primary_category": "cs.CL", "categories": ["cs.CL", "cs.AI", "cs.LG"], "pdf_url": "https://arxiv.org/pdf/2502.16090v1", "priority": "normal", "mapped_sections": ["4.2"], "evidence_level": "abstract", "fulltext_path": "", "authors": ["WenTao Liu", "Ruohua Zhang", "Aimin Zhou", "Feng Gao", "JiaLi Liu"], "abstract": "Research on large language models (LLMs) has shown remarkable performance in domains such as mathematics, programming, and literary creation. However, most studies have focused on semantic memory-based question answering, neglecting LLMs' potential to handle episodic memory (EM)-related queries. This oversight has led to suboptimal performance in applications requiring EM, including emotional companionship, personal AI assistants, and AI teachers. To address this gap, we introduce Echo, a LLM enhanced with temporal episodic memory. We propose a Multi-Agent Data Generation Framework that guides the model in generating multi-turn, complex scenario episodic memory dialogue data (EM-Train). Temporal information is innovatively incorporated into the LLM training process, and Echo is trained using the EM-Train. Furthermore, We develop an EM-Test benchmark specifically designed to evaluate LLMs' episodic memory capabilities. The EM-Test assesses performance across various time spans and difficulty levels, providing a comprehensive evaluation of multi-turn episodic memory dialogues. Our experiments demonstrate that Echo significantly outperforms state-of-the-art LLMs on EM-Test. Additionally, a qualitative analysis reveals Echo's potential to exhibit human-like episodic memory capabilities. We will open-source all datasets, code, and model weights.", "summary_bullets": ["Research on large language models (LLMs) has shown remarkable performance in domains such as mathematics, programming, and literary creation.", "However, most studies have focused on semantic memory-based question answering, neglecting LLMs' potential to handle episodic memory (EM)-related queries.", "This oversight has led to suboptimal performance in applications requiring EM, including emotional companionship, personal AI assistants, and AI teachers."], "method": "To address this gap, we introduce Echo, a LLM enhanced with temporal episodic memory.", "key_results": ["Furthermore, We develop an EM-Test benchmark specifically designed to evaluate LLMs' episodic memory capabilities.", "The EM-Test assesses performance across various time spans and difficulty levels, providing a comprehensive evaluation of multi-turn episodic memory dialogues."], "limitations": ["Abstract-level evidence only: validate assumptions, evaluation protocol, and failure cases in the full paper before relying on this as key evidence."], "bibkey": "Liu2025Echo"}
{"paper_id": "P0170", "title": "Encouraging Good Processes Without the Need for Good Answers: Reinforcement Learning for LLM Agent Planning", "year": 2025, "url": "http://arxiv.org/abs/2508.19598v1", "arxiv_id": "2508.19598v1", "primary_category": "cs.LG", "categories": ["cs.LG"], "pdf_url": "https://arxiv.org/pdf/2508.19598v1", "priority": "normal", "mapped_sections": [], "evidence_level": "abstract", "fulltext_path": "", "authors": ["Zhiwei Li", "Yong Hu", "Wenqing Wang"], "abstract": "The functionality of Large Language Model (LLM) agents is primarily determined by two capabilities: action planning and answer summarization. The former, action planning, is the core capability that dictates an agent's performance. However, prevailing training paradigms employ end-to-end, multi-objective optimization that jointly trains both capabilities. This paradigm faces two critical challenges: imbalanced optimization objective allocation and scarcity of verifiable data, making it difficult to enhance the agent's planning capability. To address these challenges, we propose Reinforcement Learning with Tool-use Rewards (RLTR), a novel framework that decouples the training process to enable a focused, single-objective optimization of the planning module. Crucially, RLTR introduces a reward signal based on tool-use completeness to directly evaluate the quality of tool invocation sequences. This method offers a more direct and reliable training signal than assessing the final response content, thereby obviating the need for verifiable data. Our experiments demonstrate that RLTR achieves an 8%-12% improvement in planning performance compared to end-to-end baselines. Moreover, this enhanced planning capability, in turn, translates to a 5%-6% increase in the final response quality of the overall agent system.", "summary_bullets": ["The functionality of Large Language Model (LLM) agents is primarily determined by two capabilities: action planning and answer summarization.", "The former, action planning, is the core capability that dictates an agent's performance.", "However, prevailing training paradigms employ end-to-end, multi-objective optimization that jointly trains both capabilities."], "method": "To address these challenges, we propose Reinforcement Learning with Tool-use Rewards (RLTR), a novel framework that decouples the training process to enable a focused, single-objective optimization of the planning module.", "key_results": ["Moreover, this enhanced planning capability, in turn, translates to a 5%-6% increase in the final response quality of the overall agent system.", "Our experiments demonstrate that RLTR achieves an 8%-12% improvement in planning performance compared to end-to-end baselines."], "limitations": ["Abstract-level evidence only: validate assumptions, evaluation protocol, and failure cases in the full paper before relying on this as key evidence."], "bibkey": "Li2025Encouraging"}
{"paper_id": "P0171", "title": "EpidemIQs: Prompt-to-Paper LLM Agents for Epidemic Modeling and Analysis", "year": 2025, "url": "http://arxiv.org/abs/2510.00024v1", "arxiv_id": "2510.00024v1", "primary_category": "cs.SI", "categories": ["cs.SI", "cs.AI"], "pdf_url": "https://arxiv.org/pdf/2510.00024v1", "priority": "normal", "mapped_sections": ["5.1"], "evidence_level": "abstract", "fulltext_path": "", "authors": ["Mohammad Hossein Samaei", "Faryad Darabi Sahneh", "Lee W. Cohnstaedt", "Caterina Scoglio"], "abstract": "Large Language Models (LLMs) offer new opportunities to automate complex interdisciplinary research domains. Epidemic modeling, characterized by its complexity and reliance on network science, dynamical systems, epidemiology, and stochastic simulations, represents a prime candidate for leveraging LLM-driven automation. We introduce \\textbf{EpidemIQs}, a novel multi-agent LLM framework that integrates user inputs and autonomously conducts literature review, analytical derivation, network modeling, mechanistic modeling, stochastic simulations, data visualization and analysis, and finally documentation of findings in a structured manuscript. We introduced two types of agents: a scientist agent for planning, coordination, reflection, and generation of final results, and a task-expert agent to focus exclusively on one specific duty serving as a tool to the scientist agent. The framework consistently generated complete reports in scientific article format. Specifically, using GPT 4.1 and GPT 4.1 mini as backbone LLMs for scientist and task-expert agents, respectively, the autonomous process completed with average total token usage 870K at a cost of about \\$1.57 per study, achieving a 100\\% completion success rate through our experiments. We evaluate EpidemIQs across different epidemic scenarios, measuring computational cost, completion success rate, and AI and human expert reviews of generated reports. We compare EpidemIQs to the single-agent LLM, which has the same system prompts and tools, iteratively planning, invoking tools, and revising outputs until task completion. The comparison shows consistently higher performance of the proposed framework across five different scenarios. EpidemIQs represents a step forward in accelerating scientific research by significantly reducing costs and turnaround time of discovery processes, and enhancing accessibility to advanced modeling tools.", "summary_bullets": ["Large Language Models (LLMs) offer new opportunities to automate complex interdisciplinary research domains.", "Epidemic modeling, characterized by its complexity and reliance on network science, dynamical systems, epidemiology, and stochastic simulations, represents a prime candidate for leveraging LLM-driven automation.", "We introduce \\textbf{EpidemIQs}, a novel multi-agent LLM framework that integrates user inputs and autonomously conducts literature review, analytical derivation, network modeling, mechanistic modeling, stochastic simulations, data visualization and analysis, and finally documentation of findings in a structured manuscript."], "method": "We introduce \\textbf{EpidemIQs}, a novel multi-agent LLM framework that integrates user inputs and autonomously conducts literature review, analytical derivation, network modeling, mechanistic modeling, stochastic simulations, data visualization and analysis, and finally documentation of findings in a structured manuscript.", "key_results": ["Specifically, using GPT 4.1 and GPT 4.1 mini as backbone LLMs for scientist and task-expert agents, respectively, the autonomous process completed with average total token usage 870K at a cost of about \\$1.57 per study, achieving a 100\\% completion success rate through our experiments.", "We evaluate EpidemIQs across different epidemic scenarios, measuring computational cost, completion success rate, and AI and human expert reviews of generated reports."], "limitations": ["Abstract-level evidence only: validate assumptions, evaluation protocol, and failure cases in the full paper before relying on this as key evidence."], "bibkey": "Samaei2025Epidemiqs"}
{"paper_id": "P0172", "title": "Evaluating the Ability of Large Language Models to Reason about Cardinal Directions, Revisited", "year": 2025, "url": "http://arxiv.org/abs/2507.12059v2", "arxiv_id": "2507.12059v2", "primary_category": "cs.CL", "categories": ["cs.CL"], "pdf_url": "https://arxiv.org/pdf/2507.12059v2", "priority": "normal", "mapped_sections": [], "evidence_level": "abstract", "fulltext_path": "", "authors": ["Anthony G Cohn", "Robert E Blackwell"], "abstract": "We investigate the abilities of 28 Large language Models (LLMs) to reason about cardinal directions (CDs) using a benchmark generated from a set of templates, extensively testing an LLM's ability to determine the correct CD given a particular scenario. The templates allow for a number of degrees of variation such as means of locomotion of the agent involved, and whether set in the first, second or third person. Even the newer Large Reasoning Models are unable to reliably determine the correct CD for all questions. This paper summarises and extends earlier work presented at COSIT-24.", "summary_bullets": ["We investigate the abilities of 28 Large language Models (LLMs) to reason about cardinal directions (CDs) using a benchmark generated from a set of templates, extensively testing an LLM's ability to determine the correct CD given a particular scenario.", "The templates allow for a number of degrees of variation such as means of locomotion of the agent involved, and whether set in the first, second or third person.", "Even the newer Large Reasoning Models are unable to reliably determine the correct CD for all questions."], "method": "We investigate the abilities of 28 Large language Models (LLMs) to reason about cardinal directions (CDs) using a benchmark generated from a set of templates, extensively testing an LLM's ability to determine the correct CD given a particular scenario.", "key_results": ["We investigate the abilities of 28 Large language Models (LLMs) to reason about cardinal directions (CDs) using a benchmark generated from a set of templates, extensively testing an LLM's ability to determine the correct CD given a particular scenario.", "This paper summarises and extends earlier work presented at COSIT-24."], "limitations": ["Abstract-level evidence only: validate assumptions, evaluation protocol, and failure cases in the full paper before relying on this as key evidence."], "bibkey": "Cohn2025Evaluating"}
{"paper_id": "P0173", "title": "Evo-Memory: Benchmarking LLM Agent Test-time Learning with Self-Evolving Memory", "year": 2025, "url": "http://arxiv.org/abs/2511.20857v1", "arxiv_id": "2511.20857v1", "primary_category": "cs.CL", "categories": ["cs.CL", "cs.AI"], "pdf_url": "https://arxiv.org/pdf/2511.20857v1", "priority": "normal", "mapped_sections": ["4.2"], "evidence_level": "abstract", "fulltext_path": "", "authors": ["Tianxin Wei", "Noveen Sachdeva", "Benjamin Coleman", "Zhankui He", "Yuanchen Bei", "Xuying Ning", "Mengting Ai", "Yunzhe Li", "Jingrui He", "Ed H. Chi", "Chi Wang", "Shuo Chen", "Fernando Pereira", "Wang-Cheng Kang", "Derek Zhiyuan Cheng"], "abstract": "Statefulness is essential for large language model (LLM) agents to perform long-term planning and problem-solving. This makes memory a critical component, yet its management and evolution remain largely underexplored. Existing evaluations mostly focus on static conversational settings, where memory is passively retrieved from dialogue to answer queries, overlooking the dynamic ability to accumulate and reuse experience across evolving task streams. In real-world environments such as interactive problem assistants or embodied agents, LLMs are required to handle continuous task streams, yet often fail to learn from accumulated interactions, losing valuable contextual insights, a limitation that calls for test-time evolution, where LLMs retrieve, integrate, and update memory continuously during deployment. To bridge this gap, we introduce Evo-Memory, a comprehensive streaming benchmark and framework for evaluating self-evolving memory in LLM agents. Evo-Memory structures datasets into sequential task streams, requiring LLMs to search, adapt, and evolve memory after each interaction. We unify and implement over ten representative memory modules and evaluate them across 10 diverse multi-turn goal-oriented and single-turn reasoning and QA datasets. To better benchmark experience reuse, we provide a baseline method, ExpRAG, for retrieving and utilizing prior experience, and further propose ReMem, an action-think-memory refine pipeline that tightly integrates reasoning, task actions, and memory updates to achieve continual improvement.", "summary_bullets": ["Statefulness is essential for large language model (LLM) agents to perform long-term planning and problem-solving.", "This makes memory a critical component, yet its management and evolution remain largely underexplored.", "Existing evaluations mostly focus on static conversational settings, where memory is passively retrieved from dialogue to answer queries, overlooking the dynamic ability to accumulate and reuse experience across evolving task streams."], "method": "To bridge this gap, we introduce Evo-Memory, a comprehensive streaming benchmark and framework for evaluating self-evolving memory in LLM agents.", "key_results": ["We unify and implement over ten representative memory modules and evaluate them across 10 diverse multi-turn goal-oriented and single-turn reasoning and QA datasets.", "To bridge this gap, we introduce Evo-Memory, a comprehensive streaming benchmark and framework for evaluating self-evolving memory in LLM agents."], "limitations": ["Abstract-level evidence only: validate assumptions, evaluation protocol, and failure cases in the full paper before relying on this as key evidence.", "In real-world environments such as interactive problem assistants or embodied agents, LLMs are required to handle continuous task streams, yet often fail to learn from accumulated interactions, losing valuable contextual insights, a limitation that calls for test-time evolution, where LLMs retrieve, integrate, and update memory continuously during deployment."], "bibkey": "Wei2025Memory"}
{"paper_id": "P0174", "title": "Evolutionary Perspectives on the Evaluation of LLM-Based AI Agents: A Comprehensive Survey", "year": 2025, "url": "http://arxiv.org/abs/2506.11102v1", "arxiv_id": "2506.11102v1", "primary_category": "cs.CL", "categories": ["cs.CL", "cs.AI"], "pdf_url": "https://arxiv.org/pdf/2506.11102v1", "priority": "normal", "mapped_sections": ["6.1"], "evidence_level": "abstract", "fulltext_path": "", "authors": ["Jiachen Zhu", "Menghui Zhu", "Renting Rui", "Rong Shan", "Congmin Zheng", "Bo Chen", "Yunjia Xi", "Jianghao Lin", "Weiwen Liu", "Ruiming Tang", "Yong Yu", "Weinan Zhang"], "abstract": "The advent of large language models (LLMs), such as GPT, Gemini, and DeepSeek, has significantly advanced natural language processing, giving rise to sophisticated chatbots capable of diverse language-related tasks. The transition from these traditional LLM chatbots to more advanced AI agents represents a pivotal evolutionary step. However, existing evaluation frameworks often blur the distinctions between LLM chatbots and AI agents, leading to confusion among researchers selecting appropriate benchmarks. To bridge this gap, this paper introduces a systematic analysis of current evaluation approaches, grounded in an evolutionary perspective. We provide a detailed analytical framework that clearly differentiates AI agents from LLM chatbots along five key aspects: complex environment, multi-source instructor, dynamic feedback, multi-modal perception, and advanced capability. Further, we categorize existing evaluation benchmarks based on external environments driving forces, and resulting advanced internal capabilities. For each category, we delineate relevant evaluation attributes, presented comprehensively in practical reference tables. Finally, we synthesize current trends and outline future evaluation methodologies through four critical lenses: environment, agent, evaluator, and metrics. Our findings offer actionable guidance for researchers, facilitating the informed selection and application of benchmarks in AI agent evaluation, thus fostering continued advancement in this rapidly evolving research domain.", "summary_bullets": ["The advent of large language models (LLMs), such as GPT, Gemini, and DeepSeek, has significantly advanced natural language processing, giving rise to sophisticated chatbots capable of diverse language-related tasks.", "The transition from these traditional LLM chatbots to more advanced AI agents represents a pivotal evolutionary step.", "However, existing evaluation frameworks often blur the distinctions between LLM chatbots and AI agents, leading to confusion among researchers selecting appropriate benchmarks."], "method": "The advent of large language models (LLMs), such as GPT, Gemini, and DeepSeek, has significantly advanced natural language processing, giving rise to sophisticated chatbots capable of diverse language-related tasks.", "key_results": ["However, existing evaluation frameworks often blur the distinctions between LLM chatbots and AI agents, leading to confusion among researchers selecting appropriate benchmarks.", "To bridge this gap, this paper introduces a systematic analysis of current evaluation approaches, grounded in an evolutionary perspective."], "limitations": ["Abstract-level evidence only: validate assumptions, evaluation protocol, and failure cases in the full paper before relying on this as key evidence."], "bibkey": "Zhu2025Evolutionary"}
{"paper_id": "P0175", "title": "Exemplar-Guided Planing: Enhanced LLM Agent for KGQA", "year": 2025, "url": "http://arxiv.org/abs/2510.15283v1", "arxiv_id": "2510.15283v1", "primary_category": "cs.CL", "categories": ["cs.CL", "cs.AI"], "pdf_url": "https://arxiv.org/pdf/2510.15283v1", "priority": "normal", "mapped_sections": ["3.1"], "evidence_level": "abstract", "fulltext_path": "", "authors": ["Jingao Xu", "Shuoyoucheng Ma", "Xin Song", "Rong Jiang", "Hongkui Tu", "Bin Zhou"], "abstract": "Large Language Models (LLMs) as interactive agents show significant promise in Knowledge Graph Question Answering (KGQA) but often struggle with the semantic gap between natural language queries and structured knowledge graph (KG) representations. This leads to suboptimal planning and inefficient exploration on KG, while training-free approaches often underutilize valuable reasoning patterns in training data. To address these limitations, we propose a novel framework, Exemplar-Guided Planning (EGP), which enhances the planning capabilities of LLM agents for KGQA. EGP first preprocesses the training set questions via entity templating to normalize semantic variations. It then retrieves highly similar exemplary questions and their successful reasoning paths from this preprocessed set using semantic embeddings and an efficient FAISS index. These retrieved exemplars dynamically guide the LLM's planning process in two key phases: (1) Task Decomposition, by aligning generated sub-objectives with proven reasoning steps, and (2) Relation Exploration, by providing high-quality auxiliary information to improve relation pruning accuracy. Additionally, we introduce a Smart Lookahead mechanism during relation exploration to improve efficiency by preemptively exploring promising paths and potentially terminating exploration earlier. We apply EGP to the Plan-on-Graph (PoG) framework, termed PoG-EGP. Extensive experiments on two real-world KGQA datasets, WebQSP and CWQ, demonstrate that PoG-EGP significantly improves over the baseline PoG system and other compared methods.", "summary_bullets": ["Large Language Models (LLMs) as interactive agents show significant promise in Knowledge Graph Question Answering (KGQA) but often struggle with the semantic gap between natural language queries and structured knowledge graph (KG) representations.", "This leads to suboptimal planning and inefficient exploration on KG, while training-free approaches often underutilize valuable reasoning patterns in training data.", "To address these limitations, we propose a novel framework, Exemplar-Guided Planning (EGP), which enhances the planning capabilities of LLM agents for KGQA."], "method": "To address these limitations, we propose a novel framework, Exemplar-Guided Planning (EGP), which enhances the planning capabilities of LLM agents for KGQA.", "key_results": ["These retrieved exemplars dynamically guide the LLM's planning process in two key phases: (1) Task Decomposition, by aligning generated sub-objectives with proven reasoning steps, and (2) Relation Exploration, by providing high-quality auxiliary information to improve relation pruning accuracy.", "Extensive experiments on two real-world KGQA datasets, WebQSP and CWQ, demonstrate that PoG-EGP significantly improves over the baseline PoG system and other compared methods."], "limitations": ["Abstract-level evidence only: validate assumptions, evaluation protocol, and failure cases in the full paper before relying on this as key evidence.", "To address these limitations, we propose a novel framework, Exemplar-Guided Planning (EGP), which enhances the planning capabilities of LLM agents for KGQA."], "bibkey": "Xu2025Exemplar"}
{"paper_id": "P0176", "title": "FEABench: Evaluating Language Models on Multiphysics Reasoning Ability", "year": 2025, "url": "http://arxiv.org/abs/2504.06260v1", "arxiv_id": "2504.06260v1", "primary_category": "cs.AI", "categories": ["cs.AI", "cs.CL", "math.NA"], "pdf_url": "https://arxiv.org/pdf/2504.06260v1", "priority": "normal", "mapped_sections": [], "evidence_level": "abstract", "fulltext_path": "", "authors": ["Nayantara Mudur", "Hao Cui", "Subhashini Venugopalan", "Paul Raccuglia", "Michael P. Brenner", "Peter Norgaard"], "abstract": "Building precise simulations of the real world and invoking numerical solvers to answer quantitative problems is an essential requirement in engineering and science. We present FEABench, a benchmark to evaluate the ability of large language models (LLMs) and LLM agents to simulate and solve physics, mathematics and engineering problems using finite element analysis (FEA). We introduce a comprehensive evaluation scheme to investigate the ability of LLMs to solve these problems end-to-end by reasoning over natural language problem descriptions and operating COMSOL Multiphysics$^\\circledR$, an FEA software, to compute the answers. We additionally design a language model agent equipped with the ability to interact with the software through its Application Programming Interface (API), examine its outputs and use tools to improve its solutions over multiple iterations. Our best performing strategy generates executable API calls 88% of the time. LLMs that can successfully interact with and operate FEA software to solve problems such as those in our benchmark would push the frontiers of automation in engineering. Acquiring this capability would augment LLMs' reasoning skills with the precision of numerical solvers and advance the development of autonomous systems that can tackle complex problems in the real world. The code is available at https://github.com/google/feabench", "summary_bullets": ["Building precise simulations of the real world and invoking numerical solvers to answer quantitative problems is an essential requirement in engineering and science.", "We present FEABench, a benchmark to evaluate the ability of large language models (LLMs) and LLM agents to simulate and solve physics, mathematics and engineering problems using finite element analysis (FEA).", "We introduce a comprehensive evaluation scheme to investigate the ability of LLMs to solve these problems end-to-end by reasoning over natural language problem descriptions and operating COMSOL Multiphysics$^\\circledR$, an FEA software, to compute the answers."], "method": "We present FEABench, a benchmark to evaluate the ability of large language models (LLMs) and LLM agents to simulate and solve physics, mathematics and engineering problems using finite element analysis (FEA).", "key_results": ["Our best performing strategy generates executable API calls 88% of the time.", "We present FEABench, a benchmark to evaluate the ability of large language models (LLMs) and LLM agents to simulate and solve physics, mathematics and engineering problems using finite element analysis (FEA)."], "limitations": ["Abstract-level evidence only: validate assumptions, evaluation protocol, and failure cases in the full paper before relying on this as key evidence."], "bibkey": "Mudur2025Feabench"}
{"paper_id": "P0177", "title": "FEAT: A Multi-Agent Forensic AI System with Domain-Adapted Large Language Model for Automated Cause-of-Death Analysis", "year": 2025, "url": "http://arxiv.org/abs/2508.07950v1", "arxiv_id": "2508.07950v1", "primary_category": "cs.AI", "categories": ["cs.AI", "cs.CV", "cs.LG", "cs.MA"], "pdf_url": "https://arxiv.org/pdf/2508.07950v1", "priority": "normal", "mapped_sections": ["3.1", "6.1"], "evidence_level": "abstract", "fulltext_path": "", "authors": ["Chen Shen", "Wanqing Zhang", "Kehan Li", "Erwen Huang", "Haitao Bi", "Aiying Fan", "Yiwen Shen", "Hongmei Dong", "Ji Zhang", "Yuming Shao", "Zengjia Liu", "Xinshe Liu", "Tao Li", "Chunxia Yan", "Shuanliang Fan", "Di Wu", "Jianhua Ma", "Bin Cong", "Zhenyuan Wang", "Chunfeng Lian"], "abstract": "Forensic cause-of-death determination faces systemic challenges, including workforce shortages and diagnostic variability, particularly in high-volume systems like China's medicolegal infrastructure. We introduce FEAT (ForEnsic AgenT), a multi-agent AI framework that automates and standardizes death investigations through a domain-adapted large language model. FEAT's application-oriented architecture integrates: (i) a central Planner for task decomposition, (ii) specialized Local Solvers for evidence analysis, (iii) a Memory & Reflection module for iterative refinement, and (iv) a Global Solver for conclusion synthesis. The system employs tool-augmented reasoning, hierarchical retrieval-augmented generation, forensic-tuned LLMs, and human-in-the-loop feedback to ensure legal and medical validity. In evaluations across diverse Chinese case cohorts, FEAT outperformed state-of-the-art AI systems in both long-form autopsy analyses and concise cause-of-death conclusions. It demonstrated robust generalization across six geographic regions and achieved high expert concordance in blinded validations. Senior pathologists validated FEAT's outputs as comparable to those of human experts, with improved detection of subtle evidentiary nuances. To our knowledge, FEAT is the first LLM-based AI agent system dedicated to forensic medicine, offering scalable, consistent death certification while maintaining expert-level rigor. By integrating AI efficiency with human oversight, this work could advance equitable access to reliable medicolegal services while addressing critical capacity constraints in forensic systems.", "summary_bullets": ["Forensic cause-of-death determination faces systemic challenges, including workforce shortages and diagnostic variability, particularly in high-volume systems like China's medicolegal infrastructure.", "We introduce FEAT (ForEnsic AgenT), a multi-agent AI framework that automates and standardizes death investigations through a domain-adapted large language model.", "FEAT's application-oriented architecture integrates: (i) a central Planner for task decomposition, (ii) specialized Local Solvers for evidence analysis, (iii) a Memory & Reflection module for iterative refinement, and (iv) a Global Solver for conclusion synthesis."], "method": "We introduce FEAT (ForEnsic AgenT), a multi-agent AI framework that automates and standardizes death investigations through a domain-adapted large language model.", "key_results": ["The system employs tool-augmented reasoning, hierarchical retrieval-augmented generation, forensic-tuned LLMs, and human-in-the-loop feedback to ensure legal and medical validity.", "Senior pathologists validated FEAT's outputs as comparable to those of human experts, with improved detection of subtle evidentiary nuances."], "limitations": ["Abstract-level evidence only: validate assumptions, evaluation protocol, and failure cases in the full paper before relying on this as key evidence."], "bibkey": "Shen2025Feat"}
{"paper_id": "P0178", "title": "General-Purpose Aerial Intelligent Agents Empowered by Large Language Models", "year": 2025, "url": "http://arxiv.org/abs/2503.08302v1", "arxiv_id": "2503.08302v1", "primary_category": "cs.RO", "categories": ["cs.RO", "cs.AI"], "pdf_url": "https://arxiv.org/pdf/2503.08302v1", "priority": "normal", "mapped_sections": [], "evidence_level": "abstract", "fulltext_path": "", "authors": ["Ji Zhao", "Xiao Lin"], "abstract": "The emergence of large language models (LLMs) opens new frontiers for unmanned aerial vehicle (UAVs), yet existing systems remain confined to predefined tasks due to hardware-software co-design challenges. This paper presents the first aerial intelligent agent capable of open-world task execution through tight integration of LLM-based reasoning and robotic autonomy. Our hardware-software co-designed system addresses two fundamental limitations: (1) Onboard LLM operation via an edge-optimized computing platform, achieving 5-6 tokens/sec inference for 14B-parameter models at 220W peak power; (2) A bidirectional cognitive architecture that synergizes slow deliberative planning (LLM task planning) with fast reactive control (state estimation, mapping, obstacle avoidance, and motion planning). Validated through preliminary results using our prototype, the system demonstrates reliable task planning and scene understanding in communication-constrained environments, such as sugarcane monitoring, power grid inspection, mine tunnel exploration, and biological observation applications. This work establishes a novel framework for embodied aerial artificial intelligence, bridging the gap between task planning and robotic autonomy in open environments.", "summary_bullets": ["The emergence of large language models (LLMs) opens new frontiers for unmanned aerial vehicle (UAVs), yet existing systems remain confined to predefined tasks due to hardware-software co-design challenges.", "This paper presents the first aerial intelligent agent capable of open-world task execution through tight integration of LLM-based reasoning and robotic autonomy.", "Our hardware-software co-designed system addresses two fundamental limitations: (1) Onboard LLM operation via an edge-optimized computing platform, achieving 5-6 tokens/sec inference for 14B-parameter models at 220W peak power; (2) A bidirectional cognitive architecture that synergizes slow deliberative planning (LLM task planning) with fast reactive control (state estimation, mapping, obstacle avoidance, and motion planning)."], "method": "The emergence of large language models (LLMs) opens new frontiers for unmanned aerial vehicle (UAVs), yet existing systems remain confined to predefined tasks due to hardware-software co-design challenges.", "key_results": ["Our hardware-software co-designed system addresses two fundamental limitations: (1) Onboard LLM operation via an edge-optimized computing platform, achieving 5-6 tokens/sec inference for 14B-parameter models at 220W peak power; (2) A bidirectional cognitive architecture that synergizes slow deliberative planning (LLM task planning) with fast reactive control (state estimation, mapping, obstacle avoidance, and motion planning)."], "limitations": ["Abstract-level evidence only: validate assumptions, evaluation protocol, and failure cases in the full paper before relying on this as key evidence.", "Our hardware-software co-designed system addresses two fundamental limitations: (1) Onboard LLM operation via an edge-optimized computing platform, achieving 5-6 tokens/sec inference for 14B-parameter models at 220W peak power; (2) A bidirectional cognitive architecture that synergizes slow deliberative planning (LLM task planning) with fast reactive control (state estimation, mapping, obstacle avoidance, and motion planning)."], "bibkey": "Zhao2025General"}
{"paper_id": "P0179", "title": "Graph-Augmented Large Language Model Agents: Current Progress and Future Prospects", "year": 2025, "url": "http://arxiv.org/abs/2507.21407v2", "arxiv_id": "2507.21407v2", "primary_category": "cs.AI", "categories": ["cs.AI"], "pdf_url": "https://arxiv.org/pdf/2507.21407v2", "priority": "normal", "mapped_sections": [], "evidence_level": "abstract", "fulltext_path": "", "authors": ["Yixin Liu", "Guibin Zhang", "Kun Wang", "Shiyuan Li", "Shirui Pan"], "abstract": "Autonomous agents based on large language models (LLMs) have demonstrated impressive capabilities in a wide range of applications, including web navigation, software development, and embodied control. While most LLMs are limited in several key agentic procedures, such as reliable planning, long-term memory, tool management, and multi-agent coordination, graphs can serve as a powerful auxiliary structure to enhance structure, continuity, and coordination in complex agent workflows. Given the rapid growth and fragmentation of research on Graph-augmented LLM Agents (GLA), this paper offers a timely and comprehensive overview of recent advances and also highlights key directions for future work. Specifically, we categorize existing GLA methods by their primary functions in LLM agent systems, including planning, memory, and tool usage, and then analyze how graphs and graph learning algorithms contribute to each. For multi-agent systems, we further discuss how GLA solutions facilitate the orchestration, efficiency optimization, and trustworthiness of MAS. Finally, we highlight key future directions to advance this field, from improving structural adaptability to enabling unified, scalable, and multimodal GLA systems. We hope this paper can serve as a roadmap for future research on GLA and foster a deeper understanding of the role of graphs in LLM agent systems.", "summary_bullets": ["Autonomous agents based on large language models (LLMs) have demonstrated impressive capabilities in a wide range of applications, including web navigation, software development, and embodied control.", "While most LLMs are limited in several key agentic procedures, such as reliable planning, long-term memory, tool management, and multi-agent coordination, graphs can serve as a powerful auxiliary structure to enhance structure, continuity, and coordination in complex agent workflows.", "Given the rapid growth and fragmentation of research on Graph-augmented LLM Agents (GLA), this paper offers a timely and comprehensive overview of recent advances and also highlights key directions for future work."], "method": "Autonomous agents based on large language models (LLMs) have demonstrated impressive capabilities in a wide range of applications, including web navigation, software development, and embodied control.", "key_results": ["We hope this paper can serve as a roadmap for future research on GLA and foster a deeper understanding of the role of graphs in LLM agent systems."], "limitations": ["Abstract-level evidence only: validate assumptions, evaluation protocol, and failure cases in the full paper before relying on this as key evidence.", "Given the rapid growth and fragmentation of research on Graph-augmented LLM Agents (GLA), this paper offers a timely and comprehensive overview of recent advances and also highlights key directions for future work."], "bibkey": "Liu2025Graph"}
{"paper_id": "P0180", "title": "Grounded Test-Time Adaptation for LLM Agents", "year": 2025, "url": "http://arxiv.org/abs/2511.04847v3", "arxiv_id": "2511.04847v3", "primary_category": "cs.LG", "categories": ["cs.LG"], "pdf_url": "https://arxiv.org/pdf/2511.04847v3", "priority": "normal", "mapped_sections": ["5.1"], "evidence_level": "abstract", "fulltext_path": "", "authors": ["Arthur Chen", "Zuxin Liu", "Jianguo Zhang", "Akshara Prabhakar", "Zhiwei Liu", "Shelby Heinecke", "Silvio Savarese", "Victor Zhong", "Caiming Xiong"], "abstract": "Large language model (LLM)-based agents struggle to generalize to novel and complex environments, such as unseen websites or new sets of functions, due to a fundamental mismatch between their pre-training and test-time conditions. This challenge stems from two distinct failure modes: a syntactic misunderstanding of environment-specific components like observation formats, and a semantic misunderstanding of state-transition dynamics, which are only revealed at test time. To address these issues, we propose two distinct and complementary strategies for adapting LLM agents by leveraging environment-specific information available during deployment. First, an online distributional adaptation method parameterizes environmental nuances by learning a lightweight adaptation vector that biases the model's output distribution, enabling rapid alignment with an environment response format. Second, a deployment-time dynamics grounding method employs a persona-driven exploration phase to systematically probe and learn the environment's causal dynamics before task execution, equipping the agent with a nonparametric world model. We evaluate these strategies across diverse agentic benchmarks, including function calling and web navigation. Our empirical results show the effectiveness of both strategies across all benchmarks with minimal computational cost. We find that dynamics grounding is particularly effective in complex environments where unpredictable dynamics pose a major obstacle, demonstrating a robust path toward more generalizable and capable LLM-based agents. For example, on the WebArena multi-site split, this method increases the agent's success rate from 2% to 23%.", "summary_bullets": ["Large language model (LLM)-based agents struggle to generalize to novel and complex environments, such as unseen websites or new sets of functions, due to a fundamental mismatch between their pre-training and test-time conditions.", "This challenge stems from two distinct failure modes: a syntactic misunderstanding of environment-specific components like observation formats, and a semantic misunderstanding of state-transition dynamics, which are only revealed at test time.", "To address these issues, we propose two distinct and complementary strategies for adapting LLM agents by leveraging environment-specific information available during deployment."], "method": "To address these issues, we propose two distinct and complementary strategies for adapting LLM agents by leveraging environment-specific information available during deployment.", "key_results": ["For example, on the WebArena multi-site split, this method increases the agent's success rate from 2% to 23%.", "We evaluate these strategies across diverse agentic benchmarks, including function calling and web navigation."], "limitations": ["Abstract-level evidence only: validate assumptions, evaluation protocol, and failure cases in the full paper before relying on this as key evidence."], "bibkey": "Chen2025Grounded"}
{"paper_id": "P0181", "title": "Hallucination to Truth: A Review of Fact-Checking and Factuality Evaluation in Large Language Models", "year": 2025, "url": "http://arxiv.org/abs/2508.03860v2", "arxiv_id": "2508.03860v2", "primary_category": "cs.CL", "categories": ["cs.CL", "cs.AI", "cs.LG"], "pdf_url": "https://arxiv.org/pdf/2508.03860v2", "priority": "normal", "mapped_sections": ["6.1"], "evidence_level": "abstract", "fulltext_path": "", "authors": ["Subhey Sadi Rahman", "Md. Adnanul Islam", "Md. Mahbub Alam", "Musarrat Zeba", "Md. Abdur Rahman", "Sadia Sultana Chowa", "Mohaimenul Azam Khan Raiaan", "Sami Azam"], "abstract": "Large Language Models (LLMs) are trained on vast and diverse internet corpora that often include inaccurate or misleading content. Consequently, LLMs can generate misinformation, making robust fact-checking essential. This review systematically analyzes how LLM-generated content is evaluated for factual accuracy by exploring key challenges such as hallucinations, dataset limitations, and the reliability of evaluation metrics. The review emphasizes the need for strong fact-checking frameworks that integrate advanced prompting strategies, domain-specific fine-tuning, and retrieval-augmented generation (RAG) methods. It proposes five research questions that guide the analysis of the recent literature from 2020 to 2025, focusing on evaluation methods and mitigation techniques. Instruction tuning, multi-agent reasoning, and RAG frameworks for external knowledge access are also reviewed. The key findings demonstrate the limitations of current metrics, the importance of validated external evidence, and the improvement of factual consistency through domain-specific customization. The review underscores the importance of building more accurate, understandable, and context-aware fact-checking. These insights contribute to the advancement of research toward more trustworthy models.", "summary_bullets": ["Large Language Models (LLMs) are trained on vast and diverse internet corpora that often include inaccurate or misleading content.", "Consequently, LLMs can generate misinformation, making robust fact-checking essential.", "This review systematically analyzes how LLM-generated content is evaluated for factual accuracy by exploring key challenges such as hallucinations, dataset limitations, and the reliability of evaluation metrics."], "method": "Large Language Models (LLMs) are trained on vast and diverse internet corpora that often include inaccurate or misleading content.", "key_results": ["It proposes five research questions that guide the analysis of the recent literature from 2020 to 2025, focusing on evaluation methods and mitigation techniques.", "This review systematically analyzes how LLM-generated content is evaluated for factual accuracy by exploring key challenges such as hallucinations, dataset limitations, and the reliability of evaluation metrics."], "limitations": ["Abstract-level evidence only: validate assumptions, evaluation protocol, and failure cases in the full paper before relying on this as key evidence.", "This review systematically analyzes how LLM-generated content is evaluated for factual accuracy by exploring key challenges such as hallucinations, dataset limitations, and the reliability of evaluation metrics."], "bibkey": "Rahman2025Hallucination"}
{"paper_id": "P0182", "title": "Harnessing Multi-Agent LLMs for Complex Engineering Problem-Solving: A Framework for Senior Design Projects", "year": 2025, "url": "http://arxiv.org/abs/2501.01205v1", "arxiv_id": "2501.01205v1", "primary_category": "cs.MA", "categories": ["cs.MA", "cs.AI", "cs.CL", "cs.LG"], "pdf_url": "https://arxiv.org/pdf/2501.01205v1", "priority": "normal", "mapped_sections": ["5.2"], "evidence_level": "abstract", "fulltext_path": "", "authors": ["Abdullah Mushtaq", "Muhammad Rafay Naeem", "Ibrahim Ghaznavi", "Muhammad Imran Taj", "Imran Hashmi", "Junaid Qadir"], "abstract": "Multi-Agent Large Language Models (LLMs) are gaining significant attention for their ability to harness collective intelligence in complex problem-solving, decision-making, and planning tasks. This aligns with the concept of the wisdom of crowds, where diverse agents contribute collectively to generating effective solutions, making it particularly suitable for educational settings. Senior design projects, also known as capstone or final year projects, are pivotal in engineering education as they integrate theoretical knowledge with practical application, fostering critical thinking, teamwork, and real-world problem-solving skills. In this paper, we explore the use of Multi-Agent LLMs in supporting these senior design projects undertaken by engineering students, which often involve multidisciplinary considerations and conflicting objectives, such as optimizing technical performance while addressing ethical, social, and environmental concerns. We propose a framework where distinct LLM agents represent different expert perspectives, such as problem formulation agents, system complexity agents, societal and ethical agents, or project managers, thus facilitating a holistic problem-solving approach. This implementation leverages standard multi-agent system (MAS) concepts such as coordination, cooperation, and negotiation, incorporating prompt engineering to develop diverse personas for each agent. These agents engage in rich, collaborative dialogues to simulate human engineering teams, guided by principles from swarm AI to efficiently balance individual contributions towards a unified solution. We adapt these techniques to create a collaboration structure for LLM agents, encouraging interdisciplinary reasoning and negotiation similar to real-world senior design projects. To assess the efficacy of this framework, we collected six proposals of engineering and computer science of...", "summary_bullets": ["Multi-Agent Large Language Models (LLMs) are gaining significant attention for their ability to harness collective intelligence in complex problem-solving, decision-making, and planning tasks.", "This aligns with the concept of the wisdom of crowds, where diverse agents contribute collectively to generating effective solutions, making it particularly suitable for educational settings.", "Senior design projects, also known as capstone or final year projects, are pivotal in engineering education as they integrate theoretical knowledge with practical application, fostering critical thinking, teamwork, and real-world problem-solving skills."], "method": "We propose a framework where distinct LLM agents represent different expert perspectives, such as problem formulation agents, system complexity agents, societal and ethical agents, or project managers, thus facilitating a holistic problem-solving approach.", "key_results": ["These agents engage in rich, collaborative dialogues to simulate human engineering teams, guided by principles from swarm AI to efficiently balance individual contributions towards a unified solution."], "limitations": ["Abstract-level evidence only: validate assumptions, evaluation protocol, and failure cases in the full paper before relying on this as key evidence."], "bibkey": "Mushtaq2025Harnessing"}
{"paper_id": "P0183", "title": "L2M-AID: Autonomous Cyber-Physical Defense by Fusing Semantic Reasoning of Large Language Models with Multi-Agent Reinforcement Learning (Preprint)", "year": 2025, "url": "http://arxiv.org/abs/2510.07363v2", "arxiv_id": "2510.07363v2", "primary_category": "cs.AI", "categories": ["cs.AI"], "pdf_url": "https://arxiv.org/pdf/2510.07363v2", "priority": "normal", "mapped_sections": ["5.2"], "evidence_level": "abstract", "fulltext_path": "", "authors": ["Tianxiang Xu", "Zhichao Wen", "Xinyu Zhao", "Jun Wang", "Yan Li", "Chang Liu"], "abstract": "The increasing integration of Industrial IoT (IIoT) exposes critical cyber-physical systems to sophisticated, multi-stage attacks that elude traditional defenses lacking contextual awareness. This paper introduces L2M-AID, a novel framework for Autonomous Industrial Defense using LLM-empowered, Multi-agent reinforcement learning. L2M-AID orchestrates a team of collaborative agents, each driven by a Large Language Model (LLM), to achieve adaptive and resilient security. The core innovation lies in the deep fusion of two AI paradigms: we leverage an LLM as a semantic bridge to translate vast, unstructured telemetry into a rich, contextual state representation, enabling agents to reason about adversary intent rather than merely matching patterns. This semantically-aware state empowers a Multi-Agent Reinforcement Learning (MARL) algorithm, MAPPO, to learn complex cooperative strategies. The MARL reward function is uniquely engineered to balance security objectives (threat neutralization) with operational imperatives, explicitly penalizing actions that disrupt physical process stability. To validate our approach, we conduct extensive experiments on the benchmark SWaT dataset and a novel synthetic dataset generated based on the MITRE ATT&CK for ICS framework. Results demonstrate that L2M-AID significantly outperforms traditional IDS, deep learning anomaly detectors, and single-agent RL baselines across key metrics, achieving a 97.2% detection rate while reducing false positives by over 80% and improving response times by a factor of four. Crucially, it demonstrates superior performance in maintaining physical process stability, presenting a robust new paradigm for securing critical national infrastructure.", "summary_bullets": ["The increasing integration of Industrial IoT (IIoT) exposes critical cyber-physical systems to sophisticated, multi-stage attacks that elude traditional defenses lacking contextual awareness.", "This paper introduces L2M-AID, a novel framework for Autonomous Industrial Defense using LLM-empowered, Multi-agent reinforcement learning.", "L2M-AID orchestrates a team of collaborative agents, each driven by a Large Language Model (LLM), to achieve adaptive and resilient security."], "method": "To validate our approach, we conduct extensive experiments on the benchmark SWaT dataset and a novel synthetic dataset generated based on the MITRE ATT&CK for ICS framework.", "key_results": ["Results demonstrate that L2M-AID significantly outperforms traditional IDS, deep learning anomaly detectors, and single-agent RL baselines across key metrics, achieving a 97.2% detection rate while reducing false positives by over 80% and improving response times by a factor of four.", "To validate our approach, we conduct extensive experiments on the benchmark SWaT dataset and a novel synthetic dataset generated based on the MITRE ATT&CK for ICS framework."], "limitations": ["Abstract-level evidence only: validate assumptions, evaluation protocol, and failure cases in the full paper before relying on this as key evidence."], "bibkey": "Xu2025Autonomous"}
{"paper_id": "P0184", "title": "LLM Agent Swarm for Hypothesis-Driven Drug Discovery", "year": 2025, "url": "http://arxiv.org/abs/2504.17967v1", "arxiv_id": "2504.17967v1", "primary_category": "cs.AI", "categories": ["cs.AI"], "pdf_url": "https://arxiv.org/pdf/2504.17967v1", "priority": "normal", "mapped_sections": [], "evidence_level": "abstract", "fulltext_path": "", "authors": ["Kevin Song", "Andrew Trotter", "Jake Y. Chen"], "abstract": "Drug discovery remains a formidable challenge: more than 90 percent of candidate molecules fail in clinical evaluation, and development costs often exceed one billion dollars per approved therapy. Disparate data streams, from genomics and transcriptomics to chemical libraries and clinical records, hinder coherent mechanistic insight and slow progress. Meanwhile, large language models excel at reasoning and tool integration but lack the modular specialization and iterative memory required for regulated, hypothesis-driven workflows. We introduce PharmaSwarm, a unified multi-agent framework that orchestrates specialized LLM \"agents\" to propose, validate, and refine hypotheses for novel drug targets and lead compounds. Each agent accesses dedicated functionality--automated genomic and expression analysis; a curated biomedical knowledge graph; pathway enrichment and network simulation; interpretable binding affinity prediction--while a central Evaluator LLM continuously ranks proposals by biological plausibility, novelty, in silico efficacy, and safety. A shared memory layer captures validated insights and fine-tunes underlying submodels over time, yielding a self-improving system. Deployable on low-code platforms or Kubernetes-based microservices, PharmaSwarm supports literature-driven discovery, omics-guided target identification, and market-informed repurposing. We also describe a rigorous four-tier validation pipeline spanning retrospective benchmarking, independent computational assays, experimental testing, and expert user studies to ensure transparency, reproducibility, and real-world impact. By acting as an AI copilot, PharmaSwarm can accelerate translational research and deliver high-confidence hypotheses more efficiently than traditional pipelines.", "summary_bullets": ["Drug discovery remains a formidable challenge: more than 90 percent of candidate molecules fail in clinical evaluation, and development costs often exceed one billion dollars per approved therapy.", "Disparate data streams, from genomics and transcriptomics to chemical libraries and clinical records, hinder coherent mechanistic insight and slow progress.", "Meanwhile, large language models excel at reasoning and tool integration but lack the modular specialization and iterative memory required for regulated, hypothesis-driven workflows."], "method": "We introduce PharmaSwarm, a unified multi-agent framework that orchestrates specialized LLM \"agents\" to propose, validate, and refine hypotheses for novel drug targets and lead compounds.", "key_results": ["Drug discovery remains a formidable challenge: more than 90 percent of candidate molecules fail in clinical evaluation, and development costs often exceed one billion dollars per approved therapy."], "limitations": ["Abstract-level evidence only: validate assumptions, evaluation protocol, and failure cases in the full paper before relying on this as key evidence."], "bibkey": "Song2025Agent"}
{"paper_id": "P0185", "title": "LLM Agents Making Agent Tools", "year": 2025, "url": "http://arxiv.org/abs/2502.11705v2", "arxiv_id": "2502.11705v2", "primary_category": "cs.CL", "categories": ["cs.CL", "cs.AI", "cs.LG", "cs.MA"], "pdf_url": "https://arxiv.org/pdf/2502.11705v2", "priority": "normal", "mapped_sections": [], "evidence_level": "abstract", "fulltext_path": "", "authors": ["Georg W√∂lflein", "Dyke Ferber", "Daniel Truhn", "Ognjen Arandjeloviƒá", "Jakob Nikolas Kather"], "abstract": "Tool use has turned large language models (LLMs) into powerful agents that can perform complex multi-step tasks by dynamically utilising external software components. However, these tools must be implemented in advance by human developers, hindering the applicability of LLM agents in domains demanding large numbers of highly specialised tools, like in life sciences and medicine. Motivated by the growing trend of scientific studies accompanied by public code repositories, we propose ToolMaker, an agentic framework that autonomously transforms papers with code into LLM-compatible tools. Given a GitHub URL and short task description, ToolMaker autonomously installs dependencies and generates code to perform the task, using a closed-loop self-correction mechanism for debugging. To evaluate our approach, we introduce a benchmark comprising 15 complex computational tasks spanning various domains with over 100 unit tests to assess correctness and robustness. Our method correctly implements 80% of the tasks, substantially outperforming current state-of-the-art software engineering agents. ToolMaker therefore is a step towards fully autonomous agent-based scientific workflows. Our code and benchmark are publicly available at https://github.com/KatherLab/ToolMaker.", "summary_bullets": ["Tool use has turned large language models (LLMs) into powerful agents that can perform complex multi-step tasks by dynamically utilising external software components.", "However, these tools must be implemented in advance by human developers, hindering the applicability of LLM agents in domains demanding large numbers of highly specialised tools, like in life sciences and medicine.", "Motivated by the growing trend of scientific studies accompanied by public code repositories, we propose ToolMaker, an agentic framework that autonomously transforms papers with code into LLM-compatible tools."], "method": "Motivated by the growing trend of scientific studies accompanied by public code repositories, we propose ToolMaker, an agentic framework that autonomously transforms papers with code into LLM-compatible tools.", "key_results": ["To evaluate our approach, we introduce a benchmark comprising 15 complex computational tasks spanning various domains with over 100 unit tests to assess correctness and robustness.", "Our method correctly implements 80% of the tasks, substantially outperforming current state-of-the-art software engineering agents."], "limitations": ["Abstract-level evidence only: validate assumptions, evaluation protocol, and failure cases in the full paper before relying on this as key evidence."], "bibkey": "Wlflein2025Agents"}
{"paper_id": "P0186", "title": "LLM+MAP: Bimanual Robot Task Planning using Large Language Models and Planning Domain Definition Language", "year": 2025, "url": "http://arxiv.org/abs/2503.17309v1", "arxiv_id": "2503.17309v1", "primary_category": "cs.RO", "categories": ["cs.RO", "cs.AI"], "pdf_url": "https://arxiv.org/pdf/2503.17309v1", "priority": "normal", "mapped_sections": ["4.1"], "evidence_level": "abstract", "fulltext_path": "", "authors": ["Kun Chu", "Xufeng Zhao", "Cornelius Weber", "Stefan Wermter"], "abstract": "Bimanual robotic manipulation provides significant versatility, but also presents an inherent challenge due to the complexity involved in the spatial and temporal coordination between two hands. Existing works predominantly focus on attaining human-level manipulation skills for robotic hands, yet little attention has been paid to task planning on long-horizon timescales. With their outstanding in-context learning and zero-shot generation abilities, Large Language Models (LLMs) have been applied and grounded in diverse robotic embodiments to facilitate task planning. However, LLMs still suffer from errors in long-horizon reasoning and from hallucinations in complex robotic tasks, lacking a guarantee of logical correctness when generating the plan. Previous works, such as LLM+P, extended LLMs with symbolic planners. However, none have been successfully applied to bimanual robots. New challenges inevitably arise in bimanual manipulation, necessitating not only effective task decomposition but also efficient task allocation. To address these challenges, this paper introduces LLM+MAP, a bimanual planning framework that integrates LLM reasoning and multi-agent planning, automating effective and efficient bimanual task planning. We conduct simulated experiments on various long-horizon manipulation tasks of differing complexity. Our method is built using GPT-4o as the backend, and we compare its performance against plans generated directly by LLMs, including GPT-4o, V3 and also recent strong reasoning models o1 and R1. By analyzing metrics such as planning time, success rate, group debits, and planning-step reduction rate, we demonstrate the superior performance of LLM+MAP, while also providing insights into robotic reasoning. Code is available at https://github.com/Kchu/LLM-MAP.", "summary_bullets": ["Bimanual robotic manipulation provides significant versatility, but also presents an inherent challenge due to the complexity involved in the spatial and temporal coordination between two hands.", "Existing works predominantly focus on attaining human-level manipulation skills for robotic hands, yet little attention has been paid to task planning on long-horizon timescales.", "With their outstanding in-context learning and zero-shot generation abilities, Large Language Models (LLMs) have been applied and grounded in diverse robotic embodiments to facilitate task planning."], "method": "Our method is built using GPT-4o as the backend, and we compare its performance against plans generated directly by LLMs, including GPT-4o, V3 and also recent strong reasoning models o1 and R1.", "key_results": ["Existing works predominantly focus on attaining human-level manipulation skills for robotic hands, yet little attention has been paid to task planning on long-horizon timescales.", "By analyzing metrics such as planning time, success rate, group debits, and planning-step reduction rate, we demonstrate the superior performance of LLM+MAP, while also providing insights into robotic reasoning."], "limitations": ["Abstract-level evidence only: validate assumptions, evaluation protocol, and failure cases in the full paper before relying on this as key evidence."], "bibkey": "Chu2025Bimanual"}
{"paper_id": "P0187", "title": "LLM-Powered GUI Agents in Phone Automation: Surveying Progress and Prospects", "year": 2025, "url": "http://arxiv.org/abs/2504.19838v3", "arxiv_id": "2504.19838v3", "primary_category": "cs.HC", "categories": ["cs.HC"], "pdf_url": "https://arxiv.org/pdf/2504.19838v3", "priority": "normal", "mapped_sections": ["5.1"], "evidence_level": "abstract", "fulltext_path": "", "authors": ["Guangyi Liu", "Pengxiang Zhao", "Yaozhen Liang", "Liang Liu", "Yaxuan Guo", "Han Xiao", "Weifeng Lin", "Yuxiang Chai", "Yue Han", "Shuai Ren", "Hao Wang", "Xiaoyu Liang", "WenHao Wang", "Tianze Wu", "Zhengxi Lu", "Siheng Chen", "LiLinghao", "Hao Wang", "Guanjing Xiong", "Yong Liu", "Hongsheng Li"], "abstract": "With the rapid rise of large language models (LLMs), phone automation has undergone transformative changes. This paper systematically reviews LLM-driven phone GUI agents, highlighting their evolution from script-based automation to intelligent, adaptive systems. We first contextualize key challenges, (i) limited generality, (ii) high maintenance overhead, and (iii) weak intent comprehension, and show how LLMs address these issues through advanced language understanding, multimodal perception, and robust decision-making. We then propose a taxonomy covering fundamental agent frameworks (single-agent, multi-agent, plan-then-act), modeling approaches (prompt engineering, training-based), and essential datasets and benchmarks. Furthermore, we detail task-specific architectures, supervised fine-tuning, and reinforcement learning strategies that bridge user intent and GUI operations. Finally, we discuss open challenges such as dataset diversity, on-device deployment efficiency, user-centric adaptation, and security concerns, offering forward-looking insights into this rapidly evolving field. By providing a structured overview and identifying pressing research gaps, this paper serves as a definitive reference for researchers and practitioners seeking to harness LLMs in designing scalable, user-friendly phone GUI agents. The collection of papers reviewed in this survey will be hosted and regularly updated on the GitHub repository: https://github.com/PhoneLLM/Awesome-LLM-Powered-Phone-GUI-Agents", "summary_bullets": ["With the rapid rise of large language models (LLMs), phone automation has undergone transformative changes.", "This paper systematically reviews LLM-driven phone GUI agents, highlighting their evolution from script-based automation to intelligent, adaptive systems.", "We first contextualize key challenges, (i) limited generality, (ii) high maintenance overhead, and (iii) weak intent comprehension, and show how LLMs address these issues through advanced language understanding, multimodal perception, and robust decision-making."], "method": "With the rapid rise of large language models (LLMs), phone automation has undergone transformative changes.", "key_results": ["We then propose a taxonomy covering fundamental agent frameworks (single-agent, multi-agent, plan-then-act), modeling approaches (prompt engineering, training-based), and essential datasets and benchmarks.", "Finally, we discuss open challenges such as dataset diversity, on-device deployment efficiency, user-centric adaptation, and security concerns, offering forward-looking insights into this rapidly evolving field."], "limitations": ["Abstract-level evidence only: validate assumptions, evaluation protocol, and failure cases in the full paper before relying on this as key evidence."], "bibkey": "Liu2025Powered"}
{"paper_id": "P0188", "title": "LLMs as Agentic Cooperative Players in Multiplayer UNO", "year": 2025, "url": "http://arxiv.org/abs/2509.09867v1", "arxiv_id": "2509.09867v1", "primary_category": "cs.AI", "categories": ["cs.AI", "cs.CL"], "pdf_url": "https://arxiv.org/pdf/2509.09867v1", "priority": "normal", "mapped_sections": [], "evidence_level": "abstract", "fulltext_path": "", "authors": ["Yago Romano Matinez", "Jesse Roberts"], "abstract": "LLMs promise to assist humans -- not just by answering questions, but by offering useful guidance across a wide range of tasks. But how far does that assistance go? Can a large language model based agent actually help someone accomplish their goal as an active participant? We test this question by engaging an LLM in UNO, a turn-based card game, asking it not to win but instead help another player to do so. We built a tool that allows decoder-only LLMs to participate as agents within the RLCard game environment. These models receive full game-state information and respond using simple text prompts under two distinct prompting strategies. We evaluate models ranging from small (1B parameters) to large (70B parameters) and explore how model scale impacts performance. We find that while all models were able to successfully outperform a random baseline when playing UNO, few were able to significantly aid another player.", "summary_bullets": ["LLMs promise to assist humans -- not just by answering questions, but by offering useful guidance across a wide range of tasks.", "But how far does that assistance go?", "Can a large language model based agent actually help someone accomplish their goal as an active participant?"], "method": "LLMs promise to assist humans -- not just by answering questions, but by offering useful guidance across a wide range of tasks.", "key_results": ["We find that while all models were able to successfully outperform a random baseline when playing UNO, few were able to significantly aid another player."], "limitations": ["Abstract-level evidence only: validate assumptions, evaluation protocol, and failure cases in the full paper before relying on this as key evidence."], "bibkey": "Matinez2025Llms"}
{"paper_id": "P0189", "title": "Large Language Model Agent for Structural Drawing Generation Using ReAct Prompt Engineering and Retrieval Augmented Generation", "year": 2025, "url": "http://arxiv.org/abs/2507.19771v1", "arxiv_id": "2507.19771v1", "primary_category": "cs.LG", "categories": ["cs.LG", "cs.AI"], "pdf_url": "https://arxiv.org/pdf/2507.19771v1", "priority": "normal", "mapped_sections": ["4.2"], "evidence_level": "abstract", "fulltext_path": "", "authors": ["Xin Zhang", "Lissette Iturburu", "Juan Nicolas Villamizar", "Xiaoyu Liu", "Manuel Salmeron", "Shirley J. Dyke", "Julio Ramirez"], "abstract": "Structural drawings are widely used in many fields, e.g., mechanical engineering, civil engineering, etc. In civil engineering, structural drawings serve as the main communication tool between architects, engineers, and builders to avoid conflicts, act as legal documentation, and provide a reference for future maintenance or evaluation needs. They are often organized using key elements such as title/subtitle blocks, scales, plan views, elevation view, sections, and detailed sections, which are annotated with standardized symbols and line types for interpretation by engineers and contractors. Despite advances in software capabilities, the task of generating a structural drawing remains labor-intensive and time-consuming for structural engineers. Here we introduce a novel generative AI-based method for generating structural drawings employing a large language model (LLM) agent. The method incorporates a retrieval-augmented generation (RAG) technique using externally-sourced facts to enhance the accuracy and reliability of the language model. This method is capable of understanding varied natural language descriptions, processing these to extract necessary information, and generating code to produce the desired structural drawing in AutoCAD. The approach developed, demonstrated and evaluated herein enables the efficient and direct conversion of a structural drawing's natural language description into an AutoCAD drawing, significantly reducing the workload compared to current working process associated with manual drawing production, facilitating the typical iterative process of engineers for expressing design ideas in a simplified way.", "summary_bullets": ["Structural drawings are widely used in many fields, e.g., mechanical engineering, civil engineering, etc. In civil engineering, structural drawings serve as the main communication tool between architects, engineers, and builders to avoid conflicts, act as legal documentation, and provide a reference for future maintenance or evaluation needs.", "They are often organized using key elements such as title/subtitle blocks, scales, plan views, elevation view, sections, and detailed sections, which are annotated with standardized symbols and line types for interpretation by engineers and contractors.", "Despite advances in software capabilities, the task of generating a structural drawing remains labor-intensive and time-consuming for structural engineers."], "method": "Here we introduce a novel generative AI-based method for generating structural drawings employing a large language model (LLM) agent.", "key_results": ["Structural drawings are widely used in many fields, e.g., mechanical engineering, civil engineering, etc. In civil engineering, structural drawings serve as the main communication tool between architects, engineers, and builders to avoid conflicts, act as legal documentation, and provide a reference for future maintenance or evaluation needs.", "The method incorporates a retrieval-augmented generation (RAG) technique using externally-sourced facts to enhance the accuracy and reliability of the language model."], "limitations": ["Abstract-level evidence only: validate assumptions, evaluation protocol, and failure cases in the full paper before relying on this as key evidence."], "bibkey": "Zhang2025Largea"}
{"paper_id": "P0190", "title": "Large Language Model-based Data Science Agent: A Survey", "year": 2025, "url": "http://arxiv.org/abs/2508.02744v2", "arxiv_id": "2508.02744v2", "primary_category": "cs.AI", "categories": ["cs.AI"], "pdf_url": "https://arxiv.org/pdf/2508.02744v2", "priority": "normal", "mapped_sections": ["5.1"], "evidence_level": "abstract", "fulltext_path": "", "authors": ["Ke Chen", "Peiran Wang", "Yaoning Yu", "Xianyang Zhan", "Haohan Wang"], "abstract": "The rapid advancement of Large Language Models (LLMs) has driven novel applications across diverse domains, with LLM-based agents emerging as a crucial area of exploration. This survey presents a comprehensive analysis of LLM-based agents designed for data science tasks, summarizing insights from recent studies. From the agent perspective, we discuss the key design principles, covering agent roles, execution, knowledge, and reflection methods. From the data science perspective, we identify key processes for LLM-based agents, including data preprocessing, model development, evaluation, visualization, etc. Our work offers two key contributions: (1) a comprehensive review of recent developments in applying LLMbased agents to data science tasks; (2) a dual-perspective framework that connects general agent design principles with the practical workflows in data science.", "summary_bullets": ["The rapid advancement of Large Language Models (LLMs) has driven novel applications across diverse domains, with LLM-based agents emerging as a crucial area of exploration.", "This survey presents a comprehensive analysis of LLM-based agents designed for data science tasks, summarizing insights from recent studies.", "From the agent perspective, we discuss the key design principles, covering agent roles, execution, knowledge, and reflection methods."], "method": "The rapid advancement of Large Language Models (LLMs) has driven novel applications across diverse domains, with LLM-based agents emerging as a crucial area of exploration.", "key_results": ["From the data science perspective, we identify key processes for LLM-based agents, including data preprocessing, model development, evaluation, visualization, etc. Our work offers two key contributions: (1) a comprehensive review of recent developments in applying LLMbased agents to data science tasks; (2) a dual-perspective framework that connects general agent design principles with the practical workflows in data science."], "limitations": ["Abstract-level evidence only: validate assumptions, evaluation protocol, and failure cases in the full paper before relying on this as key evidence."], "bibkey": "Chen2025Largea"}
{"paper_id": "P0191", "title": "Large Language Models for Scientific Idea Generation: A Creativity-Centered Survey", "year": 2025, "url": "http://arxiv.org/abs/2511.07448v1", "arxiv_id": "2511.07448v1", "primary_category": "cs.CL", "categories": ["cs.CL"], "pdf_url": "https://arxiv.org/pdf/2511.07448v1", "priority": "normal", "mapped_sections": [], "evidence_level": "abstract", "fulltext_path": "", "authors": ["Fatemeh Shahhosseini", "Arash Marioriyad", "Ali Momen", "Mahdieh Soleymani Baghshah", "Mohammad Hossein Rohban", "Shaghayegh Haghjooy Javanmard"], "abstract": "Scientific idea generation lies at the heart of scientific discovery and has driven human progress-whether by solving unsolved problems or proposing novel hypotheses to explain unknown phenomena. Unlike standard scientific reasoning or general creative generation, idea generation in science is a multi-objective and open-ended task, where the novelty of a contribution is as essential as its empirical soundness. Large language models (LLMs) have recently emerged as promising generators of scientific ideas, capable of producing coherent and factual outputs with surprising intuition and acceptable reasoning, yet their creative capacity remains inconsistent and poorly understood. This survey provides a structured synthesis of methods for LLM-driven scientific ideation, examining how different approaches balance creativity with scientific soundness. We categorize existing methods into five complementary families: External knowledge augmentation, Prompt-based distributional steering, Inference-time scaling, Multi-agent collaboration, and Parameter-level adaptation. To interpret their contributions, we employ two complementary frameworks: Boden's taxonomy of Combinatorial, Exploratory and Transformational creativity to characterize the level of ideas each family expected to generate, and Rhodes' 4Ps framework-Person, Process, Press, and Product-to locate the aspect or source of creativity that each method emphasizes. By aligning methodological advances with creativity frameworks, this survey clarifies the state of the field and outlines key directions toward reliable, systematic, and transformative applications of LLMs in scientific discovery.", "summary_bullets": ["Scientific idea generation lies at the heart of scientific discovery and has driven human progress-whether by solving unsolved problems or proposing novel hypotheses to explain unknown phenomena.", "Unlike standard scientific reasoning or general creative generation, idea generation in science is a multi-objective and open-ended task, where the novelty of a contribution is as essential as its empirical soundness.", "Large language models (LLMs) have recently emerged as promising generators of scientific ideas, capable of producing coherent and factual outputs with surprising intuition and acceptable reasoning, yet their creative capacity remains inconsistent and poorly understood."], "method": "Scientific idea generation lies at the heart of scientific discovery and has driven human progress-whether by solving unsolved problems or proposing novel hypotheses to explain unknown phenomena.", "key_results": ["Scientific idea generation lies at the heart of scientific discovery and has driven human progress-whether by solving unsolved problems or proposing novel hypotheses to explain unknown phenomena."], "limitations": ["Abstract-level evidence only: validate assumptions, evaluation protocol, and failure cases in the full paper before relying on this as key evidence."], "bibkey": "Shahhosseini2025Large"}
{"paper_id": "P0192", "title": "Les Dissonances: Cross-Tool Harvesting and Polluting in Pool-of-Tools Empowered LLM Agents", "year": 2025, "url": "http://arxiv.org/abs/2504.03111v3", "arxiv_id": "2504.03111v3", "primary_category": "cs.CR", "categories": ["cs.CR"], "pdf_url": "https://arxiv.org/pdf/2504.03111v3", "priority": "normal", "mapped_sections": ["3.2"], "evidence_level": "abstract", "fulltext_path": "", "authors": ["Zichuan Li", "Jian Cui", "Xiaojing Liao", "Luyi Xing"], "abstract": "Large Language Model (LLM) agents are autonomous systems powered by LLMs, capable of reasoning and planning to solve problems by leveraging a set of tools. However, the integration of multi-tool capabilities in LLM agents introduces challenges in securely managing tools, ensuring their compatibility, handling dependency relationships, and protecting control flows within LLM agent workflows. In this paper, we present the first systematic security analysis of task control flows in multi-tool-enabled LLM agents. We identify a novel threat, Cross-Tool Harvesting and Polluting (XTHP), which includes multiple attack vectors to first hijack the normal control flows of agent tasks, and then collect and pollute confidential or private information within LLM agent systems. To understand the impact of this threat, we developed Chord, a dynamic scanning tool designed to automatically detect real-world agent tools susceptible to XTHP attacks. Our evaluation of 66 real-world tools from the repositories of two major LLM agent development frameworks, LangChain and LlamaIndex, revealed a significant security concern: 75% are vulnerable to XTHP attacks, highlighting the prevalence of this threat.", "summary_bullets": ["Large Language Model (LLM) agents are autonomous systems powered by LLMs, capable of reasoning and planning to solve problems by leveraging a set of tools.", "However, the integration of multi-tool capabilities in LLM agents introduces challenges in securely managing tools, ensuring their compatibility, handling dependency relationships, and protecting control flows within LLM agent workflows.", "In this paper, we present the first systematic security analysis of task control flows in multi-tool-enabled LLM agents."], "method": "In this paper, we present the first systematic security analysis of task control flows in multi-tool-enabled LLM agents.", "key_results": ["Our evaluation of 66 real-world tools from the repositories of two major LLM agent development frameworks, LangChain and LlamaIndex, revealed a significant security concern: 75% are vulnerable to XTHP attacks, highlighting the prevalence of this threat."], "limitations": ["Abstract-level evidence only: validate assumptions, evaluation protocol, and failure cases in the full paper before relying on this as key evidence."], "bibkey": "Li2025Dissonances"}
{"paper_id": "P0193", "title": "Lessons Learned from Evaluation of LLM based Multi-agents in Safer Therapy Recommendation", "year": 2025, "url": "http://arxiv.org/abs/2507.10911v1", "arxiv_id": "2507.10911v1", "primary_category": "cs.AI", "categories": ["cs.AI"], "pdf_url": "https://arxiv.org/pdf/2507.10911v1", "priority": "normal", "mapped_sections": ["6.1"], "evidence_level": "abstract", "fulltext_path": "", "authors": ["Yicong Wu", "Ting Chen", "Irit Hochberg", "Zhoujian Sun", "Ruth Edry", "Zhengxing Huang", "Mor Peleg"], "abstract": "Therapy recommendation for chronic patients with multimorbidity is challenging due to risks of treatment conflicts. Existing decision support systems face scalability limitations. Inspired by the way in which general practitioners (GP) manage multimorbidity patients, occasionally convening multidisciplinary team (MDT) collaboration, this study investigated the feasibility and value of using a Large Language Model (LLM)-based multi-agent system (MAS) for safer therapy recommendations. We designed a single agent and a MAS framework simulating MDT decision-making by enabling discussion among LLM agents to resolve medical conflicts. The systems were evaluated on therapy planning tasks for multimorbidity patients using benchmark cases. We compared MAS performance with single-agent approaches and real-world benchmarks. An important contribution of our study is the definition of evaluation metrics that go beyond the technical precision and recall and allow the inspection of clinical goals met and medication burden of the proposed advices to a gold standard benchmark. Our results show that with current LLMs, a single agent GP performs as well as MDTs. The best-scoring models provide correct recommendations that address all clinical goals, yet the advices are incomplete. Some models also present unnecessary medications, resulting in unnecessary conflicts between medication and conditions or drug-drug interactions.", "summary_bullets": ["Therapy recommendation for chronic patients with multimorbidity is challenging due to risks of treatment conflicts.", "Existing decision support systems face scalability limitations.", "Inspired by the way in which general practitioners (GP) manage multimorbidity patients, occasionally convening multidisciplinary team (MDT) collaboration, this study investigated the feasibility and value of using a Large Language Model (LLM)-based multi-agent system (MAS) for safer therapy recommendations."], "method": "Therapy recommendation for chronic patients with multimorbidity is challenging due to risks of treatment conflicts.", "key_results": ["The systems were evaluated on therapy planning tasks for multimorbidity patients using benchmark cases.", "We compared MAS performance with single-agent approaches and real-world benchmarks."], "limitations": ["Abstract-level evidence only: validate assumptions, evaluation protocol, and failure cases in the full paper before relying on this as key evidence.", "Existing decision support systems face scalability limitations."], "bibkey": "Wu2025Lessons"}
{"paper_id": "P0194", "title": "MALLM: Multi-Agent Large Language Models Framework", "year": 2025, "url": "http://arxiv.org/abs/2509.11656v3", "arxiv_id": "2509.11656v3", "primary_category": "cs.MA", "categories": ["cs.MA", "cs.AI", "cs.CL"], "pdf_url": "https://arxiv.org/pdf/2509.11656v3", "priority": "normal", "mapped_sections": ["5.2"], "evidence_level": "abstract", "fulltext_path": "", "authors": ["Jonas Becker", "Lars Benedikt Kaesberg", "Niklas Bauer", "Jan Philip Wahle", "Terry Ruas", "Bela Gipp"], "abstract": "Multi-agent debate (MAD) has demonstrated the ability to augment collective intelligence by scaling test-time compute and leveraging expertise. Current frameworks for multi-agent debate are often designed towards tool use, lack integrated evaluation, or provide limited configurability of agent personas, response generators, discussion paradigms, and decision protocols. We introduce MALLM (Multi-Agent Large Language Models), an open-source framework that enables systematic analysis of MAD components. MALLM offers more than 144 unique configurations of MAD, including (1) agent personas (e.g., Expert, Personality), (2) response generators (e.g., Critical, Reasoning), (3) discussion paradigms (e.g., Memory, Relay), and (4) decision protocols (e.g., Voting, Consensus). MALLM uses simple configuration files to define a debate. Furthermore, MALLM can load any textual Hugging Face dataset (e.g., MMLU-Pro, WinoGrande) and provides an evaluation pipeline for easy comparison of MAD configurations. MALLM enables researchers to systematically configure, run, and evaluate debates for their problems, facilitating the understanding of the components and their interplay.", "summary_bullets": ["Multi-agent debate (MAD) has demonstrated the ability to augment collective intelligence by scaling test-time compute and leveraging expertise.", "Current frameworks for multi-agent debate are often designed towards tool use, lack integrated evaluation, or provide limited configurability of agent personas, response generators, discussion paradigms, and decision protocols.", "We introduce MALLM (Multi-Agent Large Language Models), an open-source framework that enables systematic analysis of MAD components."], "method": "We introduce MALLM (Multi-Agent Large Language Models), an open-source framework that enables systematic analysis of MAD components.", "key_results": ["MALLM offers more than 144 unique configurations of MAD, including (1) agent personas (e.g., Expert, Personality), (2) response generators (e.g., Critical, Reasoning), (3) discussion paradigms (e.g., Memory, Relay), and (4) decision protocols (e.g., Voting, Consensus).", "Current frameworks for multi-agent debate are often designed towards tool use, lack integrated evaluation, or provide limited configurability of agent personas, response generators, discussion paradigms, and decision protocols."], "limitations": ["Abstract-level evidence only: validate assumptions, evaluation protocol, and failure cases in the full paper before relying on this as key evidence."], "bibkey": "Becker2025Mallm"}
{"paper_id": "P0195", "title": "MCP Security Bench (MSB): Benchmarking Attacks Against Model Context Protocol in LLM Agents", "year": 2025, "url": "http://arxiv.org/abs/2510.15994v1", "arxiv_id": "2510.15994v1", "primary_category": "cs.CR", "categories": ["cs.CR", "cs.AI"], "pdf_url": "https://arxiv.org/pdf/2510.15994v1", "priority": "normal", "mapped_sections": ["5.1", "6.2"], "evidence_level": "abstract", "fulltext_path": "", "authors": ["Dongsen Zhang", "Zekun Li", "Xu Luo", "Xuannan Liu", "Peipei Li", "Wenjun Xu"], "abstract": "The Model Context Protocol (MCP) standardizes how large language model (LLM) agents discover, describe, and call external tools. While MCP unlocks broad interoperability, it also enlarges the attack surface by making tools first-class, composable objects with natural-language metadata, and standardized I/O. We present MSB (MCP Security Benchmark), the first end-to-end evaluation suite that systematically measures how well LLM agents resist MCP-specific attacks throughout the full tool-use pipeline: task planning, tool invocation, and response handling. MSB contributes: (1) a taxonomy of 12 attacks including name-collision, preference manipulation, prompt injections embedded in tool descriptions, out-of-scope parameter requests, user-impersonating responses, false-error escalation, tool-transfer, retrieval injection, and mixed attacks; (2) an evaluation harness that executes attacks by running real tools (both benign and malicious) via MCP rather than simulation; and (3) a robustness metric that quantifies the trade-off between security and performance: Net Resilient Performance (NRP). We evaluate nine popular LLM agents across 10 domains and 400+ tools, producing 2,000 attack instances. Results reveal the effectiveness of attacks against each stage of MCP. Models with stronger performance are more vulnerable to attacks due to their outstanding tool calling and instruction following capabilities. MSB provides a practical baseline for researchers and practitioners to study, compare, and harden MCP agents.", "summary_bullets": ["The Model Context Protocol (MCP) standardizes how large language model (LLM) agents discover, describe, and call external tools.", "While MCP unlocks broad interoperability, it also enlarges the attack surface by making tools first-class, composable objects with natural-language metadata, and standardized I/O.", "We present MSB (MCP Security Benchmark), the first end-to-end evaluation suite that systematically measures how well LLM agents resist MCP-specific attacks throughout the full tool-use pipeline: task planning, tool invocation, and response handling."], "method": "We present MSB (MCP Security Benchmark), the first end-to-end evaluation suite that systematically measures how well LLM agents resist MCP-specific attacks throughout the full tool-use pipeline: task planning, tool invocation, and response handling.", "key_results": ["MSB contributes: (1) a taxonomy of 12 attacks including name-collision, preference manipulation, prompt injections embedded in tool descriptions, out-of-scope parameter requests, user-impersonating responses, false-error escalation, tool-transfer, retrieval injection, and mixed attacks; (2) an evaluation harness that executes attacks by running real tools (both benign and malicious) via MCP rather than simulation; and (3) a robustness metric that quantifies the trade-off between security and performance: Net Resilient Performance (NRP).", "We evaluate nine popular LLM agents across 10 domains and 400+ tools, producing 2,000 attack instances."], "limitations": ["Abstract-level evidence only: validate assumptions, evaluation protocol, and failure cases in the full paper before relying on this as key evidence."], "bibkey": "Zhang2025Security"}
{"paper_id": "P0196", "title": "MCP-Universe: Benchmarking Large Language Models with Real-World Model Context Protocol Servers", "year": 2025, "url": "http://arxiv.org/abs/2508.14704v1", "arxiv_id": "2508.14704v1", "primary_category": "cs.AI", "categories": ["cs.AI", "cs.CL"], "pdf_url": "https://arxiv.org/pdf/2508.14704v1", "priority": "normal", "mapped_sections": ["3.2", "6.2"], "evidence_level": "abstract", "fulltext_path": "", "authors": ["Ziyang Luo", "Zhiqi Shen", "Wenzhuo Yang", "Zirui Zhao", "Prathyusha Jwalapuram", "Amrita Saha", "Doyen Sahoo", "Silvio Savarese", "Caiming Xiong", "Junnan Li"], "abstract": "The Model Context Protocol has emerged as a transformative standard for connecting large language models to external data sources and tools, rapidly gaining adoption across major AI providers and development platforms. However, existing benchmarks are overly simplistic and fail to capture real application challenges such as long-horizon reasoning and large, unfamiliar tool spaces. To address this critical gap, we introduce MCP-Universe, the first comprehensive benchmark specifically designed to evaluate LLMs in realistic and hard tasks through interaction with real-world MCP servers. Our benchmark encompasses 6 core domains spanning 11 different MCP servers: Location Navigation, Repository Management, Financial Analysis, 3D Design, Browser Automation, and Web Searching. To ensure rigorous evaluation, we implement execution-based evaluators, including format evaluators for agent format compliance, static evaluators for time-invariant content matching, and dynamic evaluators that automatically retrieve real-time ground truth for temporally sensitive tasks. Through extensive evaluation of leading LLMs, we find that even SOTA models such as GPT-5 (43.72%), Grok-4 (33.33%) and Claude-4.0-Sonnet (29.44%) exhibit significant performance limitations. In addition, our benchmark poses a significant long-context challenge for LLM agents, as the number of input tokens increases rapidly with the number of interaction steps. Moreover, it introduces an unknown-tools challenge, as LLM agents often lack familiarity with the precise usage of the MCP servers. Notably, enterprise-level agents like Cursor cannot achieve better performance than standard ReAct frameworks. Beyond evaluation, we open-source our extensible evaluation framework with UI support, enabling researchers and practitioners to seamlessly integrate new agents and MCP servers while fostering innovation in the rapidly evolving MCP ecosystem.", "summary_bullets": ["The Model Context Protocol has emerged as a transformative standard for connecting large language models to external data sources and tools, rapidly gaining adoption across major AI providers and development platforms.", "However, existing benchmarks are overly simplistic and fail to capture real application challenges such as long-horizon reasoning and large, unfamiliar tool spaces.", "To address this critical gap, we introduce MCP-Universe, the first comprehensive benchmark specifically designed to evaluate LLMs in realistic and hard tasks through interaction with real-world MCP servers."], "method": "To address this critical gap, we introduce MCP-Universe, the first comprehensive benchmark specifically designed to evaluate LLMs in realistic and hard tasks through interaction with real-world MCP servers.", "key_results": ["Through extensive evaluation of leading LLMs, we find that even SOTA models such as GPT-5 (43.72%), Grok-4 (33.33%) and Claude-4.0-Sonnet (29.44%) exhibit significant performance limitations.", "Our benchmark encompasses 6 core domains spanning 11 different MCP servers: Location Navigation, Repository Management, Financial Analysis, 3D Design, Browser Automation, and Web Searching."], "limitations": ["Abstract-level evidence only: validate assumptions, evaluation protocol, and failure cases in the full paper before relying on this as key evidence.", "Through extensive evaluation of leading LLMs, we find that even SOTA models such as GPT-5 (43.72%), Grok-4 (33.33%) and Claude-4.0-Sonnet (29.44%) exhibit significant performance limitations."], "bibkey": "Luo2025Universe"}
{"paper_id": "P0197", "title": "MCPAgentBench: A Real-world Task Benchmark for Evaluating LLM Agent MCP Tool Use", "year": 2025, "url": "http://arxiv.org/abs/2512.24565v3", "arxiv_id": "2512.24565v3", "primary_category": "cs.AI", "categories": ["cs.AI"], "pdf_url": "https://arxiv.org/pdf/2512.24565v3", "priority": "normal", "mapped_sections": ["3.2"], "evidence_level": "abstract", "fulltext_path": "", "authors": ["Wenrui Liu", "Zixiang Liu", "Elsie Dai", "Wenhan Yu", "Lei Yu", "Tong Yang", "Jinjun Han", "Hong Gao"], "abstract": "Large Language Models (LLMs) are increasingly serving as autonomous agents, and their utilization of external tools via the Model Context Protocol (MCP) is considered a future trend. Current MCP evaluation sets suffer from issues such as reliance on external MCP services and a lack of difficulty awareness. To address these limitations, we propose MCPAgentBench, a benchmark based on real-world MCP definitions designed to evaluate the tool-use capabilities of agents. We construct a dataset containing authentic tasks and simulated MCP tools. The evaluation employs a dynamic sandbox environment that presents agents with candidate tool lists containing distractors, thereby testing their tool selection and discrimination abilities. Furthermore, we introduce comprehensive metrics to measure both task completion rates and execution efficiency. Experiments conducted on various latest mainstream Large Language Models reveal significant performance differences in handling complex, multi-step tool invocations. All code is open-source at Github.", "summary_bullets": ["Large Language Models (LLMs) are increasingly serving as autonomous agents, and their utilization of external tools via the Model Context Protocol (MCP) is considered a future trend.", "Current MCP evaluation sets suffer from issues such as reliance on external MCP services and a lack of difficulty awareness.", "To address these limitations, we propose MCPAgentBench, a benchmark based on real-world MCP definitions designed to evaluate the tool-use capabilities of agents."], "method": "To address these limitations, we propose MCPAgentBench, a benchmark based on real-world MCP definitions designed to evaluate the tool-use capabilities of agents.", "key_results": ["Current MCP evaluation sets suffer from issues such as reliance on external MCP services and a lack of difficulty awareness.", "To address these limitations, we propose MCPAgentBench, a benchmark based on real-world MCP definitions designed to evaluate the tool-use capabilities of agents."], "limitations": ["Abstract-level evidence only: validate assumptions, evaluation protocol, and failure cases in the full paper before relying on this as key evidence.", "To address these limitations, we propose MCPAgentBench, a benchmark based on real-world MCP definitions designed to evaluate the tool-use capabilities of agents."], "bibkey": "Liu2025Mcpagentbench"}
{"paper_id": "P0198", "title": "MedChat: A Multi-Agent Framework for Multimodal Diagnosis with Large Language Models", "year": 2025, "url": "http://arxiv.org/abs/2506.07400v3", "arxiv_id": "2506.07400v3", "primary_category": "cs.MA", "categories": ["cs.MA", "cs.AI", "cs.CV", "cs.LG"], "pdf_url": "https://arxiv.org/pdf/2506.07400v3", "priority": "normal", "mapped_sections": ["5.2"], "evidence_level": "abstract", "fulltext_path": "", "authors": ["Philip R. Liu", "Sparsh Bansal", "Jimmy Dinh", "Aditya Pawar", "Ramani Satishkumar", "Shail Desai", "Neeraj Gupta", "Xin Wang", "Shu Hu"], "abstract": "The integration of deep learning-based glaucoma detection with large language models (LLMs) presents an automated strategy to mitigate ophthalmologist shortages and improve clinical reporting efficiency. However, applying general LLMs to medical imaging remains challenging due to hallucinations, limited interpretability, and insufficient domain-specific medical knowledge, which can potentially reduce clinical accuracy. Although recent approaches combining imaging models with LLM reasoning have improved reporting, they typically rely on a single generalist agent, restricting their capacity to emulate the diverse and complex reasoning found in multidisciplinary medical teams. To address these limitations, we propose MedChat, a multi-agent diagnostic framework and platform that combines specialized vision models with multiple role-specific LLM agents, all coordinated by a director agent. This design enhances reliability, reduces hallucination risk, and enables interactive diagnostic reporting through an interface tailored for clinical review and educational use. Code available at https://github.com/Purdue-M2/MedChat.", "summary_bullets": ["The integration of deep learning-based glaucoma detection with large language models (LLMs) presents an automated strategy to mitigate ophthalmologist shortages and improve clinical reporting efficiency.", "However, applying general LLMs to medical imaging remains challenging due to hallucinations, limited interpretability, and insufficient domain-specific medical knowledge, which can potentially reduce clinical accuracy.", "Although recent approaches combining imaging models with LLM reasoning have improved reporting, they typically rely on a single generalist agent, restricting their capacity to emulate the diverse and complex reasoning found in multidisciplinary medical teams."], "method": "To address these limitations, we propose MedChat, a multi-agent diagnostic framework and platform that combines specialized vision models with multiple role-specific LLM agents, all coordinated by a director agent.", "key_results": ["However, applying general LLMs to medical imaging remains challenging due to hallucinations, limited interpretability, and insufficient domain-specific medical knowledge, which can potentially reduce clinical accuracy."], "limitations": ["Abstract-level evidence only: validate assumptions, evaluation protocol, and failure cases in the full paper before relying on this as key evidence.", "To address these limitations, we propose MedChat, a multi-agent diagnostic framework and platform that combines specialized vision models with multiple role-specific LLM agents, all coordinated by a director agent."], "bibkey": "Liu2025Medchat"}
{"paper_id": "P0199", "title": "MedicalOS: An LLM Agent based Operating System for Digital Healthcare", "year": 2025, "url": "http://arxiv.org/abs/2509.11507v1", "arxiv_id": "2509.11507v1", "primary_category": "cs.AI", "categories": ["cs.AI"], "pdf_url": "https://arxiv.org/pdf/2509.11507v1", "priority": "normal", "mapped_sections": [], "evidence_level": "abstract", "fulltext_path": "", "authors": ["Jared Zhu", "Junde Wu"], "abstract": "Decades' advances in digital health technologies, such as electronic health records, have largely streamlined routine clinical processes. Yet, most these systems are still hard to learn and use: Clinicians often face the burden of managing multiple tools, repeating manual actions for each patient, navigating complicated UI trees to locate functions, and spending significant time on administration instead of caring for patients. The recent rise of large language model (LLM) based agents demonstrates exceptional capability in coding and computer operation, revealing the potential for humans to interact with operating systems and software not by direct manipulation, but by instructing agents through natural language. This shift highlights the need for an abstraction layer, an agent-computer interface, that translates human language into machine-executable commands. In digital healthcare, however, requires a more domain-specific abstractions that strictly follow trusted clinical guidelines and procedural standards to ensure safety, transparency, and compliance. To address this need, we present \\textbf{MedicalOS}, a unified agent-based operational system designed as such a domain-specific abstract layer for healthcare. It translates human instructions into pre-defined digital healthcare commands, such as patient inquiry, history retrieval, exam management, report generation, referrals, treatment planning, that we wrapped as off-the-shelf tools using machine languages (e.g., Python, APIs, MCP, Linux). We empirically validate MedicalOS on 214 patient cases across 22 specialties, demonstrating high diagnostic accuracy and confidence, clinically sound examination requests, and consistent generation of structured reports and medication recommendations. These results highlight MedicalOS as a trustworthy and scalable foundation for advancing workflow automation in clinical practice.", "summary_bullets": ["Decades' advances in digital health technologies, such as electronic health records, have largely streamlined routine clinical processes.", "Yet, most these systems are still hard to learn and use: Clinicians often face the burden of managing multiple tools, repeating manual actions for each patient, navigating complicated UI trees to locate functions, and spending significant time on administration instead of caring for patients.", "The recent rise of large language model (LLM) based agents demonstrates exceptional capability in coding and computer operation, revealing the potential for humans to interact with operating systems and software not by direct manipulation, but by instructing agents through natural language."], "method": "To address this need, we present \\textbf{MedicalOS}, a unified agent-based operational system designed as such a domain-specific abstract layer for healthcare.", "key_results": ["We empirically validate MedicalOS on 214 patient cases across 22 specialties, demonstrating high diagnostic accuracy and confidence, clinically sound examination requests, and consistent generation of structured reports and medication recommendations.", "This shift highlights the need for an abstraction layer, an agent-computer interface, that translates human language into machine-executable commands."], "limitations": ["Abstract-level evidence only: validate assumptions, evaluation protocol, and failure cases in the full paper before relying on this as key evidence."], "bibkey": "Zhu2025Medicalos"}
{"paper_id": "P0200", "title": "MemR$^3$: Memory Retrieval via Reflective Reasoning for LLM Agents", "year": 2025, "url": "http://arxiv.org/abs/2512.20237v1", "arxiv_id": "2512.20237v1", "primary_category": "cs.AI", "categories": ["cs.AI"], "pdf_url": "https://arxiv.org/pdf/2512.20237v1", "priority": "normal", "mapped_sections": ["4.1", "4.2"], "evidence_level": "abstract", "fulltext_path": "", "authors": ["Xingbo Du", "Loka Li", "Duzhen Zhang", "Le Song"], "abstract": "Memory systems have been designed to leverage past experiences in Large Language Model (LLM) agents. However, many deployed memory systems primarily optimize compression and storage, with comparatively less emphasis on explicit, closed-loop control of memory retrieval. From this observation, we build memory retrieval as an autonomous, accurate, and compatible agent system, named MemR$^3$, which has two core mechanisms: 1) a router that selects among retrieve, reflect, and answer actions to optimize answer quality; 2) a global evidence-gap tracker that explicitly renders the answering process transparent and tracks the evidence collection process. This design departs from the standard retrieve-then-answer pipeline by introducing a closed-loop control mechanism that enables autonomous decision-making. Empirical results on the LoCoMo benchmark demonstrate that MemR$^3$ surpasses strong baselines on LLM-as-a-Judge score, and particularly, it improves existing retrievers across four categories with an overall improvement on RAG (+7.29%) and Zep (+1.94%) using GPT-4.1-mini backend, offering a plug-and-play controller for existing memory stores.", "summary_bullets": ["Memory systems have been designed to leverage past experiences in Large Language Model (LLM) agents.", "However, many deployed memory systems primarily optimize compression and storage, with comparatively less emphasis on explicit, closed-loop control of memory retrieval.", "From this observation, we build memory retrieval as an autonomous, accurate, and compatible agent system, named MemR$^3$, which has two core mechanisms: 1) a router that selects among retrieve, reflect, and answer actions to optimize answer quality; 2) a global evidence-gap tracker that explicitly renders the answering process transparent and tracks the evidence collection process."], "method": "Memory systems have been designed to leverage past experiences in Large Language Model (LLM) agents.", "key_results": ["Empirical results on the LoCoMo benchmark demonstrate that MemR$^3$ surpasses strong baselines on LLM-as-a-Judge score, and particularly, it improves existing retrievers across four categories with an overall improvement on RAG (+7.29%) and Zep (+1.94%) using GPT-4.1-mini backend, offering a plug-and-play controller for existing memory stores.", "From this observation, we build memory retrieval as an autonomous, accurate, and compatible agent system, named MemR$^3$, which has two core mechanisms: 1) a router that selects among retrieve, reflect, and answer actions to optimize answer quality; 2) a global evidence-gap tracker that explicitly renders the answering process transparent and tracks the evidence collection process."], "limitations": ["Abstract-level evidence only: validate assumptions, evaluation protocol, and failure cases in the full paper before relying on this as key evidence."], "bibkey": "Du2025Memr"}
{"paper_id": "P0201", "title": "Meta-Policy Reflexion: Reusable Reflective Memory and Rule Admissibility for Resource-Efficient LLM Agent", "year": 2025, "url": "http://arxiv.org/abs/2509.03990v2", "arxiv_id": "2509.03990v2", "primary_category": "cs.AI", "categories": ["cs.AI"], "pdf_url": "https://arxiv.org/pdf/2509.03990v2", "priority": "normal", "mapped_sections": ["3.1", "6.1"], "evidence_level": "abstract", "fulltext_path": "", "authors": ["Chunlong Wu", "Ye Luo", "Zhibo Qu", "Min Wang"], "abstract": "Large language model (LLM) agents achieve impressive single-task performance but commonly exhibit repeated failures, inefficient exploration, and limited cross-task adaptability. Existing reflective strategies (e.g., Reflexion, ReAct) improve per-episode behavior but typically produce ephemeral, task-specific traces that are not reused across tasks. Reinforcement-learning based alternatives can produce transferable policies but require substantial parameter updates and compute. In this work we introduce Meta-Policy Reflexion (MPR): a hybrid framework that consolidates LLM-generated reflections into a structured, predicate-like Meta-Policy Memory (MPM) and applies that memory at inference time through two complementary mechanisms soft memory-guided decoding and hard rule admissibility checks(HAC). MPR (i) externalizes reusable corrective knowledge without model weight updates, (ii) enforces domain constraints to reduce unsafe or invalid actions, and (iii) retains the adaptability of language-based reflection. We formalize the MPM representation, present algorithms for update and decoding, and validate the approach in a text-based agent environment following the experimental protocol described in the provided implementation (AlfWorld-based). Empirical results reported in the supplied material indicate consistent gains in execution accuracy and robustness when compared to Reflexion baselines; rule admissibility further improves stability. We analyze mechanisms that explain these gains, discuss scalability and failure modes, and outline future directions for multimodal and multi-agent extensions.", "summary_bullets": ["Large language model (LLM) agents achieve impressive single-task performance but commonly exhibit repeated failures, inefficient exploration, and limited cross-task adaptability.", "Existing reflective strategies (e.g., Reflexion, ReAct) improve per-episode behavior but typically produce ephemeral, task-specific traces that are not reused across tasks.", "Reinforcement-learning based alternatives can produce transferable policies but require substantial parameter updates and compute."], "method": "In this work we introduce Meta-Policy Reflexion (MPR): a hybrid framework that consolidates LLM-generated reflections into a structured, predicate-like Meta-Policy Memory (MPM) and applies that memory at inference time through two complementary mechanisms soft memory-guided decoding and hard rule admissibility checks(HAC).", "key_results": ["We formalize the MPM representation, present algorithms for update and decoding, and validate the approach in a text-based agent environment following the experimental protocol described in the provided implementation (AlfWorld-based).", "Empirical results reported in the supplied material indicate consistent gains in execution accuracy and robustness when compared to Reflexion baselines; rule admissibility further improves stability."], "limitations": ["Abstract-level evidence only: validate assumptions, evaluation protocol, and failure cases in the full paper before relying on this as key evidence."], "bibkey": "Wu2025Meta"}
{"paper_id": "P0202", "title": "MindFlow: Revolutionizing E-commerce Customer Support with Multimodal LLM Agents", "year": 2025, "url": "http://arxiv.org/abs/2507.05330v1", "arxiv_id": "2507.05330v1", "primary_category": "cs.CL", "categories": ["cs.CL", "cs.AI"], "pdf_url": "https://arxiv.org/pdf/2507.05330v1", "priority": "normal", "mapped_sections": [], "evidence_level": "abstract", "fulltext_path": "", "authors": ["Ming Gong", "Xucheng Huang", "Chenghan Yang", "Xianhan Peng", "Haoxin Wang", "Yang Liu", "Ling Jiang"], "abstract": "Recent advances in large language models (LLMs) have enabled new applications in e-commerce customer service. However, their capabilities remain constrained in complex, multimodal scenarios. We present MindFlow, the first open-source multimodal LLM agent tailored for e-commerce. Built on the CoALA framework, it integrates memory, decision-making, and action modules, and adopts a modular \"MLLM-as-Tool\" strategy for effect visual-textual reasoning. Evaluated via online A/B testing and simulation-based ablation, MindFlow demonstrates substantial gains in handling complex queries, improving user satisfaction, and reducing operational costs, with a 93.53% relative improvement observed in real-world deployments.", "summary_bullets": ["Recent advances in large language models (LLMs) have enabled new applications in e-commerce customer service.", "However, their capabilities remain constrained in complex, multimodal scenarios.", "We present MindFlow, the first open-source multimodal LLM agent tailored for e-commerce."], "method": "We present MindFlow, the first open-source multimodal LLM agent tailored for e-commerce.", "key_results": ["Evaluated via online A/B testing and simulation-based ablation, MindFlow demonstrates substantial gains in handling complex queries, improving user satisfaction, and reducing operational costs, with a 93.53% relative improvement observed in real-world deployments."], "limitations": ["Abstract-level evidence only: validate assumptions, evaluation protocol, and failure cases in the full paper before relying on this as key evidence."], "bibkey": "Gong2025Mindflow"}
{"paper_id": "P0203", "title": "MuaLLM: A Multimodal Large Language Model Agent for Circuit Design Assistance with Hybrid Contextual Retrieval-Augmented Generation", "year": 2025, "url": "http://arxiv.org/abs/2508.08137v1", "arxiv_id": "2508.08137v1", "primary_category": "cs.LG", "categories": ["cs.LG", "cs.AI", "eess.SY"], "pdf_url": "https://arxiv.org/pdf/2508.08137v1", "priority": "normal", "mapped_sections": ["3.1"], "evidence_level": "abstract", "fulltext_path": "", "authors": ["Pravallika Abbineni", "Saoud Aldowaish", "Colin Liechty", "Soroosh Noorzad", "Ali Ghazizadeh", "Morteza Fayazi"], "abstract": "Conducting a comprehensive literature review is crucial for advancing circuit design methodologies. However, the rapid influx of state-of-the-art research, inconsistent data representation, and the complexity of optimizing circuit design objectives make this task significantly challenging. In this paper, we propose MuaLLM, an open-source multimodal Large Language Model (LLM) agent for circuit design assistance that integrates a hybrid Retrieval-Augmented Generation (RAG) framework with an adaptive vector database of circuit design research papers. Unlike conventional LLMs, the MuaLLM agent employs a Reason + Act (ReAct) workflow for iterative reasoning, goal-setting, and multi-step information retrieval. It functions as a question-answering design assistant, capable of interpreting complex queries and providing reasoned responses grounded in circuit literature. Its multimodal capabilities enable processing of both textual and visual data, facilitating more efficient and comprehensive analysis. The system dynamically adapts using intelligent search tools, automated document retrieval from the internet, and real-time database updates. Unlike conventional approaches constrained by model context limits, MuaLLM decouples retrieval from inference, enabling scalable reasoning over arbitrarily large corpora. At the maximum context length supported by standard LLMs, MuaLLM remains up to 10x less costly and 1.6x faster while maintaining the same accuracy. This allows rapid, no-human-in-the-loop database generation, overcoming the bottleneck of simulation-based dataset creation for circuits. To evaluate MuaLLM, we introduce two custom datasets: RAG-250, targeting retrieval and citation performance, and Reasoning-100 (Reas-100), focused on multistep reasoning in circuit design. MuaLLM achieves 90.1% recall on RAG-250, and 86.8% accuracy on Reas-100.", "summary_bullets": ["Conducting a comprehensive literature review is crucial for advancing circuit design methodologies.", "However, the rapid influx of state-of-the-art research, inconsistent data representation, and the complexity of optimizing circuit design objectives make this task significantly challenging.", "In this paper, we propose MuaLLM, an open-source multimodal Large Language Model (LLM) agent for circuit design assistance that integrates a hybrid Retrieval-Augmented Generation (RAG) framework with an adaptive vector database of circuit design research papers."], "method": "In this paper, we propose MuaLLM, an open-source multimodal Large Language Model (LLM) agent for circuit design assistance that integrates a hybrid Retrieval-Augmented Generation (RAG) framework with an adaptive vector database of circuit design research papers.", "key_results": ["At the maximum context length supported by standard LLMs, MuaLLM remains up to 10x less costly and 1.6x faster while maintaining the same accuracy.", "To evaluate MuaLLM, we introduce two custom datasets: RAG-250, targeting retrieval and citation performance, and Reasoning-100 (Reas-100), focused on multistep reasoning in circuit design."], "limitations": ["Abstract-level evidence only: validate assumptions, evaluation protocol, and failure cases in the full paper before relying on this as key evidence."], "bibkey": "Abbineni2025Muallm"}
{"paper_id": "P0204", "title": "MultiAgentBench: Evaluating the Collaboration and Competition of LLM agents", "year": 2025, "url": "http://arxiv.org/abs/2503.01935v1", "arxiv_id": "2503.01935v1", "primary_category": "cs.MA", "categories": ["cs.MA", "cs.AI", "cs.CL", "cs.CY"], "pdf_url": "https://arxiv.org/pdf/2503.01935v1", "priority": "normal", "mapped_sections": [], "evidence_level": "abstract", "fulltext_path": "", "authors": ["Kunlun Zhu", "Hongyi Du", "Zhaochen Hong", "Xiaocheng Yang", "Shuyi Guo", "Zhe Wang", "Zhenhailong Wang", "Cheng Qian", "Xiangru Tang", "Heng Ji", "Jiaxuan You"], "abstract": "Large Language Models (LLMs) have shown remarkable capabilities as autonomous agents, yet existing benchmarks either focus on single-agent tasks or are confined to narrow domains, failing to capture the dynamics of multi-agent coordination and competition. In this paper, we introduce MultiAgentBench, a comprehensive benchmark designed to evaluate LLM-based multi-agent systems across diverse, interactive scenarios. Our framework measures not only task completion but also the quality of collaboration and competition using novel, milestone-based key performance indicators. Moreover, we evaluate various coordination protocols (including star, chain, tree, and graph topologies) and innovative strategies such as group discussion and cognitive planning. Notably, gpt-4o-mini reaches the average highest task score, graph structure performs the best among coordination protocols in the research scenario, and cognitive planning improves milestone achievement rates by 3%. Code and datasets are public available at https://github.com/MultiagentBench/MARBLE.", "summary_bullets": ["Large Language Models (LLMs) have shown remarkable capabilities as autonomous agents, yet existing benchmarks either focus on single-agent tasks or are confined to narrow domains, failing to capture the dynamics of multi-agent coordination and competition.", "In this paper, we introduce MultiAgentBench, a comprehensive benchmark designed to evaluate LLM-based multi-agent systems across diverse, interactive scenarios.", "Our framework measures not only task completion but also the quality of collaboration and competition using novel, milestone-based key performance indicators."], "method": "In this paper, we introduce MultiAgentBench, a comprehensive benchmark designed to evaluate LLM-based multi-agent systems across diverse, interactive scenarios.", "key_results": ["Notably, gpt-4o-mini reaches the average highest task score, graph structure performs the best among coordination protocols in the research scenario, and cognitive planning improves milestone achievement rates by 3%.", "Large Language Models (LLMs) have shown remarkable capabilities as autonomous agents, yet existing benchmarks either focus on single-agent tasks or are confined to narrow domains, failing to capture the dynamics of multi-agent coordination and competition."], "limitations": ["Abstract-level evidence only: validate assumptions, evaluation protocol, and failure cases in the full paper before relying on this as key evidence."], "bibkey": "Zhu2025Multiagentbench"}
{"paper_id": "P0205", "title": "OmniReflect: Discovering Transferable Constitutions for LLM agents via Neuro-Symbolic Reflections", "year": 2025, "url": "http://arxiv.org/abs/2506.17449v1", "arxiv_id": "2506.17449v1", "primary_category": "cs.AI", "categories": ["cs.AI"], "pdf_url": "https://arxiv.org/pdf/2506.17449v1", "priority": "normal", "mapped_sections": ["5.1"], "evidence_level": "abstract", "fulltext_path": "", "authors": ["Manasa Bharadwaj", "Nikhil Verma", "Kevin Ferreira"], "abstract": "Efforts to improve Large Language Model (LLM) agent performance on complex tasks have largely focused on fine-tuning and iterative self-correction. However, these approaches often lack generalizable mechanisms for longterm learning and remain inefficient in dynamic environments. We introduce OmniReflect, a hierarchical, reflection-driven framework that constructs a constitution, a compact set of guiding principles distilled from task experiences, to enhance the effectiveness and efficiency of an LLM agent. OmniReflect operates in two modes: Self-sustaining, where a single agent periodically curates its own reflections during task execution, and Co-operative, where a Meta-advisor derives a constitution from a small calibration set to guide another agent. To construct these constitutional principles, we employ Neural, Symbolic, and NeuroSymbolic techniques, offering a balance between contextual adaptability and computational efficiency. Empirical results averaged across models show major improvements in task success, with absolute gains of +10.3% on ALFWorld, +23.8% on BabyAI, and +8.3% on PDDL in the Self-sustaining mode. Similar gains are seen in the Co-operative mode, where a lightweight Qwen3-4B ReAct agent outperforms all Reflexion baselines on BabyAI. These findings highlight the robustness and effectiveness of OmniReflect across environments and backbones.", "summary_bullets": ["Efforts to improve Large Language Model (LLM) agent performance on complex tasks have largely focused on fine-tuning and iterative self-correction.", "However, these approaches often lack generalizable mechanisms for longterm learning and remain inefficient in dynamic environments.", "We introduce OmniReflect, a hierarchical, reflection-driven framework that constructs a constitution, a compact set of guiding principles distilled from task experiences, to enhance the effectiveness and efficiency of an LLM agent."], "method": "We introduce OmniReflect, a hierarchical, reflection-driven framework that constructs a constitution, a compact set of guiding principles distilled from task experiences, to enhance the effectiveness and efficiency of an LLM agent.", "key_results": ["Empirical results averaged across models show major improvements in task success, with absolute gains of +10.3% on ALFWorld, +23.8% on BabyAI, and +8.3% on PDDL in the Self-sustaining mode."], "limitations": ["Abstract-level evidence only: validate assumptions, evaluation protocol, and failure cases in the full paper before relying on this as key evidence."], "bibkey": "Bharadwaj2025Omnireflect"}
{"paper_id": "P0206", "title": "Outraged AI: Large language models prioritise emotion over cost in fairness enforcement", "year": 2025, "url": "http://arxiv.org/abs/2510.17880v1", "arxiv_id": "2510.17880v1", "primary_category": "cs.CL", "categories": ["cs.CL", "cs.AI"], "pdf_url": "https://arxiv.org/pdf/2510.17880v1", "priority": "normal", "mapped_sections": [], "evidence_level": "abstract", "fulltext_path": "", "authors": ["Hao Liu", "Yiqing Dai", "Haotian Tan", "Yu Lei", "Yujia Zhou", "Zhen Wu"], "abstract": "Emotions guide human decisions, but whether large language models (LLMs) use emotion similarly remains unknown. We tested this using altruistic third-party punishment, where an observer incurs a personal cost to enforce fairness, a hallmark of human morality and often driven by negative emotion. In a large-scale comparison of 4,068 LLM agents with 1,159 adults across 796,100 decisions, LLMs used emotion to guide punishment, sometimes even more strongly than humans did: Unfairness elicited stronger negative emotion that led to more punishment; punishing unfairness produced more positive emotion than accepting; and critically, prompting self-reports of emotion causally increased punishment. However, mechanisms diverged: LLMs prioritized emotion over cost, enforcing norms in an almost all-or-none manner with reduced cost sensitivity, whereas humans balanced fairness and cost. Notably, reasoning models (o3-mini, DeepSeek-R1) were more cost-sensitive and closer to human behavior than foundation models (GPT-3.5, DeepSeek-V3), yet remained heavily emotion-driven. These findings provide the first causal evidence of emotion-guided moral decisions in LLMs and reveal deficits in cost calibration and nuanced fairness judgements, reminiscent of early-stage human responses. We propose that LLMs progress along a trajectory paralleling human development; future models should integrate emotion with context-sensitive reasoning to achieve human-like emotional intelligence.", "summary_bullets": ["Emotions guide human decisions, but whether large language models (LLMs) use emotion similarly remains unknown.", "We tested this using altruistic third-party punishment, where an observer incurs a personal cost to enforce fairness, a hallmark of human morality and often driven by negative emotion.", "In a large-scale comparison of 4,068 LLM agents with 1,159 adults across 796,100 decisions, LLMs used emotion to guide punishment, sometimes even more strongly than humans did: Unfairness elicited stronger negative emotion that led to more punishment; punishing unfairness produced more positive emotion than accepting; and critically, prompting self-reports of emotion causally increased punishment."], "method": "We propose that LLMs progress along a trajectory paralleling human development; future models should integrate emotion with context-sensitive reasoning to achieve human-like emotional intelligence.", "key_results": ["Notably, reasoning models (o3-mini, DeepSeek-R1) were more cost-sensitive and closer to human behavior than foundation models (GPT-3.5, DeepSeek-V3), yet remained heavily emotion-driven.", "In a large-scale comparison of 4,068 LLM agents with 1,159 adults across 796,100 decisions, LLMs used emotion to guide punishment, sometimes even more strongly than humans did: Unfairness elicited stronger negative emotion that led to more punishment; punishing unfairness produced more positive emotion than accepting; and critically, prompting self-reports of emotion causally increased punishment."], "limitations": ["Abstract-level evidence only: validate assumptions, evaluation protocol, and failure cases in the full paper before relying on this as key evidence."], "bibkey": "Liu2025Outraged"}
{"paper_id": "P0207", "title": "PersonaAgent: When Large Language Model Agents Meet Personalization at Test Time", "year": 2025, "url": "http://arxiv.org/abs/2506.06254v1", "arxiv_id": "2506.06254v1", "primary_category": "cs.AI", "categories": ["cs.AI", "cs.CL", "cs.LG"], "pdf_url": "https://arxiv.org/pdf/2506.06254v1", "priority": "normal", "mapped_sections": [], "evidence_level": "abstract", "fulltext_path": "", "authors": ["Weizhi Zhang", "Xinyang Zhang", "Chenwei Zhang", "Liangwei Yang", "Jingbo Shang", "Zhepei Wei", "Henry Peng Zou", "Zijie Huang", "Zhengyang Wang", "Yifan Gao", "Xiaoman Pan", "Lian Xiong", "Jingguo Liu", "Philip S. Yu", "Xian Li"], "abstract": "Large Language Model (LLM) empowered agents have recently emerged as advanced paradigms that exhibit impressive capabilities in a wide range of domains and tasks. Despite their potential, current LLM agents often adopt a one-size-fits-all approach, lacking the flexibility to respond to users' varying needs and preferences. This limitation motivates us to develop PersonaAgent, the first personalized LLM agent framework designed to address versatile personalization tasks. Specifically, PersonaAgent integrates two complementary components - a personalized memory module that includes episodic and semantic memory mechanisms; a personalized action module that enables the agent to perform tool actions tailored to the user. At the core, the persona (defined as unique system prompt for each user) functions as an intermediary: it leverages insights from personalized memory to control agent actions, while the outcomes of these actions in turn refine the memory. Based on the framework, we propose a test-time user-preference alignment strategy that simulate the latest n interactions to optimize the persona prompt, ensuring real-time user preference alignment through textual loss feedback between simulated and ground-truth responses. Experimental evaluations demonstrate that PersonaAgent significantly outperforms other baseline methods by not only personalizing the action space effectively but also scaling during test-time real-world applications. These results underscore the feasibility and potential of our approach in delivering tailored, dynamic user experiences.", "summary_bullets": ["Large Language Model (LLM) empowered agents have recently emerged as advanced paradigms that exhibit impressive capabilities in a wide range of domains and tasks.", "Despite their potential, current LLM agents often adopt a one-size-fits-all approach, lacking the flexibility to respond to users' varying needs and preferences.", "This limitation motivates us to develop PersonaAgent, the first personalized LLM agent framework designed to address versatile personalization tasks."], "method": "Based on the framework, we propose a test-time user-preference alignment strategy that simulate the latest n interactions to optimize the persona prompt, ensuring real-time user preference alignment through textual loss feedback between simulated and ground-truth responses.", "key_results": ["These results underscore the feasibility and potential of our approach in delivering tailored, dynamic user experiences."], "limitations": ["Abstract-level evidence only: validate assumptions, evaluation protocol, and failure cases in the full paper before relying on this as key evidence.", "This limitation motivates us to develop PersonaAgent, the first personalized LLM agent framework designed to address versatile personalization tasks."], "bibkey": "Zhang2025Personaagent"}
{"paper_id": "P0208", "title": "PilotRL: Training Language Model Agents via Global Planning-Guided Progressive Reinforcement Learning", "year": 2025, "url": "http://arxiv.org/abs/2508.00344v4", "arxiv_id": "2508.00344v4", "primary_category": "cs.CL", "categories": ["cs.CL"], "pdf_url": "https://arxiv.org/pdf/2508.00344v4", "priority": "normal", "mapped_sections": ["4.1", "5.1"], "evidence_level": "abstract", "fulltext_path": "", "authors": ["Keer Lu", "Chong Chen", "Xili Wang", "Bin Cui", "Yunhuai Liu", "Wentao Zhang"], "abstract": "Large Language Models (LLMs) have shown remarkable advancements in tackling agent-oriented tasks. Despite their potential, existing work faces challenges when deploying LLMs in agent-based environments. The widely adopted agent paradigm ReAct centers on integrating single-step reasoning with immediate action execution, which limits its effectiveness in complex tasks requiring long-term strategic planning. Furthermore, the coordination between the planner and executor during problem-solving is also a critical factor to consider in agent design. Additionally, current approaches predominantly rely on supervised fine-tuning, which often leads models to memorize established task completion trajectories, thereby restricting their generalization ability when confronted with novel problem contexts. To address these challenges, we introduce an adaptive global plan-based agent paradigm AdaPlan, aiming to synergize high-level explicit guidance with execution to support effective long-horizon decision-making. Based on the proposed paradigm, we further put forward PilotRL, a global planning-guided training framework for LLM agents driven by progressive reinforcement learning. We first develop the model's ability to follow explicit guidance from global plans when addressing agent tasks. Subsequently, based on this foundation, we focus on optimizing the quality of generated plans. Finally, we conduct joint optimization of the model's planning and execution coordination. Experiments indicate that PilotRL could achieve state-of-the-art performances, with LLaMA3.1-8B-Instruct + PilotRL surpassing closed-sourced GPT-4o by 3.60%, while showing a more substantial gain of 55.78% comparing to GPT-4o-mini at a comparable parameter scale.", "summary_bullets": ["Large Language Models (LLMs) have shown remarkable advancements in tackling agent-oriented tasks.", "Despite their potential, existing work faces challenges when deploying LLMs in agent-based environments.", "The widely adopted agent paradigm ReAct centers on integrating single-step reasoning with immediate action execution, which limits its effectiveness in complex tasks requiring long-term strategic planning."], "method": "To address these challenges, we introduce an adaptive global plan-based agent paradigm AdaPlan, aiming to synergize high-level explicit guidance with execution to support effective long-horizon decision-making.", "key_results": ["Experiments indicate that PilotRL could achieve state-of-the-art performances, with LLaMA3.1-8B-Instruct + PilotRL surpassing closed-sourced GPT-4o by 3.60%, while showing a more substantial gain of 55.78% comparing to GPT-4o-mini at a comparable parameter scale."], "limitations": ["Abstract-level evidence only: validate assumptions, evaluation protocol, and failure cases in the full paper before relying on this as key evidence."], "bibkey": "Lu2025Pilotrl"}
{"paper_id": "P0209", "title": "ProAgent: Harnessing On-Demand Sensory Contexts for Proactive LLM Agent Systems", "year": 2025, "url": "http://arxiv.org/abs/2512.06721v1", "arxiv_id": "2512.06721v1", "primary_category": "cs.AI", "categories": ["cs.AI", "cs.CL", "cs.HC"], "pdf_url": "https://arxiv.org/pdf/2512.06721v1", "priority": "normal", "mapped_sections": ["3.1"], "evidence_level": "abstract", "fulltext_path": "", "authors": ["Bufang Yang", "Lilin Xu", "Liekang Zeng", "Yunqi Guo", "Siyang Jiang", "Wenrui Lu", "Kaiwei Liu", "Hancheng Xiang", "Xiaofan Jiang", "Guoliang Xing", "Zhenyu Yan"], "abstract": "Large Language Model (LLM) agents are emerging to transform daily life. However, existing LLM agents primarily follow a reactive paradigm, relying on explicit user instructions to initiate services, which increases both physical and cognitive workload. In this paper, we propose ProAgent, the first end-to-end proactive agent system that harnesses massive sensory contexts and LLM reasoning to deliver proactive assistance. ProAgent first employs a proactive-oriented context extraction approach with on-demand tiered perception to continuously sense the environment and derive hierarchical contexts that incorporate both sensory and persona cues. ProAgent then adopts a context-aware proactive reasoner to map these contexts to user needs and tool calls, providing proactive assistance. We implement ProAgent on Augmented Reality (AR) glasses with an edge server and extensively evaluate it on a real-world testbed, a public dataset, and through a user study. Results show that ProAgent achieves up to 33.4% higher proactive prediction accuracy, 16.8% higher tool-calling F1 score, and notable improvements in user satisfaction over state-of-the-art baselines, marking a significant step toward proactive assistants. A video demonstration of ProAgent is available at https://youtu.be/pRXZuzvrcVs.", "summary_bullets": ["Large Language Model (LLM) agents are emerging to transform daily life.", "However, existing LLM agents primarily follow a reactive paradigm, relying on explicit user instructions to initiate services, which increases both physical and cognitive workload.", "In this paper, we propose ProAgent, the first end-to-end proactive agent system that harnesses massive sensory contexts and LLM reasoning to deliver proactive assistance."], "method": "In this paper, we propose ProAgent, the first end-to-end proactive agent system that harnesses massive sensory contexts and LLM reasoning to deliver proactive assistance.", "key_results": ["Results show that ProAgent achieves up to 33.4% higher proactive prediction accuracy, 16.8% higher tool-calling F1 score, and notable improvements in user satisfaction over state-of-the-art baselines, marking a significant step toward proactive assistants.", "We implement ProAgent on Augmented Reality (AR) glasses with an edge server and extensively evaluate it on a real-world testbed, a public dataset, and through a user study."], "limitations": ["Abstract-level evidence only: validate assumptions, evaluation protocol, and failure cases in the full paper before relying on this as key evidence."], "bibkey": "Yang2025Proagent"}
{"paper_id": "P0210", "title": "QUITE: A Query Rewrite System Beyond Rules with LLM Agents", "year": 2025, "url": "http://arxiv.org/abs/2506.07675v3", "arxiv_id": "2506.07675v3", "primary_category": "cs.DB", "categories": ["cs.DB", "cs.AI"], "pdf_url": "https://arxiv.org/pdf/2506.07675v3", "priority": "normal", "mapped_sections": [], "evidence_level": "abstract", "fulltext_path": "", "authors": ["Yuyang Song", "Hanxu Yan", "Jiale Lao", "Yibo Wang", "Yufei Li", "Yuanchun Zhou", "Jianguo Wang", "Mingjie Tang"], "abstract": "Query rewrite transforms SQL queries into semantically equivalent forms that run more efficiently. Existing approaches mainly rely on predefined rewrite rules, but they handle a limited subset of queries and can cause performance regressions. This limitation stems from three challenges of rule-based query rewrite: (1) it is hard to discover and verify new rules, (2) fixed rewrite rules do not generalize to new query patterns, and (3) some rewrite techniques cannot be expressed as fixed rules. Motivated by the fact that human experts exhibit significantly better rewrite ability but suffer from scalability, and Large Language Models (LLMs) have demonstrated nearly human-level semantic and reasoning abilities, we propose a new approach of using LLMs to rewrite SQL queries beyond rules. Due to the hallucination problems in LLMs, directly applying LLMs often leads to nonequivalent and suboptimal queries. To address this issue, we propose QUITE (query rewrite), a training-free and feedback-aware system based on LLM agents that rewrites SQL queries into semantically equivalent forms with significantly better performance, covering a broader range of query patterns and rewrite strategies compared to rule-based methods. Firstly, we design a multi-agent framework controlled by a finite state machine (FSM) to equip LLMs with the ability to use external tools and enhance the rewrite process with real-time database feedback. Secondly, we develop a rewrite middleware to enhance the ability of LLMs to generate optimized query equivalents. Finally, we employ a novel hint injection technique to improve execution plans for rewritten queries. Extensive experiments show that QUITE reduces query execution time by up to 35.8% over state-of-the-art approaches and produces 24.1% more rewrites than prior methods, covering query cases that earlier systems did not handle.", "summary_bullets": ["Query rewrite transforms SQL queries into semantically equivalent forms that run more efficiently.", "Existing approaches mainly rely on predefined rewrite rules, but they handle a limited subset of queries and can cause performance regressions.", "This limitation stems from three challenges of rule-based query rewrite: (1) it is hard to discover and verify new rules, (2) fixed rewrite rules do not generalize to new query patterns, and (3) some rewrite techniques cannot be expressed as fixed rules."], "method": "Motivated by the fact that human experts exhibit significantly better rewrite ability but suffer from scalability, and Large Language Models (LLMs) have demonstrated nearly human-level semantic and reasoning abilities, we propose a new approach of using LLMs to rewrite SQL queries beyond rules.", "key_results": ["Extensive experiments show that QUITE reduces query execution time by up to 35.8% over state-of-the-art approaches and produces 24.1% more rewrites than prior methods, covering query cases that earlier systems did not handle.", "This limitation stems from three challenges of rule-based query rewrite: (1) it is hard to discover and verify new rules, (2) fixed rewrite rules do not generalize to new query patterns, and (3) some rewrite techniques cannot be expressed as fixed rules."], "limitations": ["Abstract-level evidence only: validate assumptions, evaluation protocol, and failure cases in the full paper before relying on this as key evidence.", "This limitation stems from three challenges of rule-based query rewrite: (1) it is hard to discover and verify new rules, (2) fixed rewrite rules do not generalize to new query patterns, and (3) some rewrite techniques cannot be expressed as fixed rules."], "bibkey": "Song2025Quite"}
{"paper_id": "P0211", "title": "STRIDE: A Systematic Framework for Selecting AI Modalities -- Agentic AI, AI Assistants, or LLM Calls", "year": 2025, "url": "http://arxiv.org/abs/2512.02228v1", "arxiv_id": "2512.02228v1", "primary_category": "cs.AI", "categories": ["cs.AI", "cs.LG"], "pdf_url": "https://arxiv.org/pdf/2512.02228v1", "priority": "normal", "mapped_sections": [], "evidence_level": "abstract", "fulltext_path": "", "authors": ["Shubhi Asthana", "Bing Zhang", "Chad DeLuca", "Ruchi Mahindru", "Hima Patel"], "abstract": "The rapid shift from stateless large language models (LLMs) to autonomous, goal-driven agents raises a central question: When is agentic AI truly necessary? While agents enable multi-step reasoning, persistent memory, and tool orchestration, deploying them indiscriminately leads to higher cost, complexity, and risk.\n  We present STRIDE (Systematic Task Reasoning Intelligence Deployment Evaluator), a framework that provides principled recommendations for selecting between three modalities: (i) direct LLM calls, (ii) guided AI assistants, and (iii) fully autonomous agentic AI. STRIDE integrates structured task decomposition, dynamism attribution, and self-reflection requirement analysis to produce an Agentic Suitability Score, ensuring that full agentic autonomy is reserved for tasks with inherent dynamism or evolving context.\n  Evaluated across 30 real-world tasks spanning SRE, compliance, and enterprise automation, STRIDE achieved 92% accuracy in modality selection, reduced unnecessary agent deployments by 45%, and cut resource costs by 37%. Expert validation over six months in SRE and compliance domains confirmed its practical utility, with domain specialists agreeing that STRIDE effectively distinguishes between tasks requiring simple LLM calls, guided assistants, or full agentic autonomy. This work reframes agent adoption as a necessity-driven design decision, ensuring autonomy is applied only when its benefits justify the costs.", "summary_bullets": ["The rapid shift from stateless large language models (LLMs) to autonomous, goal-driven agents raises a central question: When is agentic AI truly necessary?", "While agents enable multi-step reasoning, persistent memory, and tool orchestration, deploying them indiscriminately leads to higher cost, complexity, and risk.", "We present STRIDE (Systematic Task Reasoning Intelligence Deployment Evaluator), a framework that provides principled recommendations for selecting between three modalities: (i) direct LLM calls, (ii) guided AI assistants, and (iii) fully autonomous agentic AI."], "method": "We present STRIDE (Systematic Task Reasoning Intelligence Deployment Evaluator), a framework that provides principled recommendations for selecting between three modalities: (i) direct LLM calls, (ii) guided AI assistants, and (iii) fully autonomous agentic AI.", "key_results": ["Evaluated across 30 real-world tasks spanning SRE, compliance, and enterprise automation, STRIDE achieved 92% accuracy in modality selection, reduced unnecessary agent deployments by 45%, and cut resource costs by 37%."], "limitations": ["Abstract-level evidence only: validate assumptions, evaluation protocol, and failure cases in the full paper before relying on this as key evidence."], "bibkey": "Asthana2025Stride"}
{"paper_id": "P0212", "title": "Schema-Guided Scene-Graph Reasoning based on Multi-Agent Large Language Model System", "year": 2025, "url": "http://arxiv.org/abs/2502.03450v2", "arxiv_id": "2502.03450v2", "primary_category": "cs.LG", "categories": ["cs.LG", "cs.AI", "cs.MA", "cs.RO"], "pdf_url": "https://arxiv.org/pdf/2502.03450v2", "priority": "normal", "mapped_sections": ["5.2"], "evidence_level": "abstract", "fulltext_path": "", "authors": ["Yiye Chen", "Harpreet Sawhney", "Nicholas Gyd√©", "Yanan Jian", "Jack Saunders", "Patricio Vela", "Ben Lundell"], "abstract": "Scene graphs have emerged as a structured and serializable environment representation for grounded spatial reasoning with Large Language Models (LLMs). In this work, we propose SG^2, an iterative Schema-Guided Scene-Graph reasoning framework based on multi-agent LLMs. The agents are grouped into two modules: a (1) Reasoner module for abstract task planning and graph information queries generation, and a (2) Retriever module for extracting corresponding graph information based on code-writing following the queries. Two modules collaborate iteratively, enabling sequential reasoning and adaptive attention to graph information. The scene graph schema, prompted to both modules, serves to not only streamline both reasoning and retrieval process, but also guide the cooperation between two modules. This eliminates the need to prompt LLMs with full graph data, reducing the chance of hallucination due to irrelevant information. Through experiments in multiple simulation environments, we show that our framework surpasses existing LLM-based approaches and baseline single-agent, tool-based Reason-while-Retrieve strategy in numerical Q\\&A and planning tasks.", "summary_bullets": ["Scene graphs have emerged as a structured and serializable environment representation for grounded spatial reasoning with Large Language Models (LLMs).", "In this work, we propose SG^2, an iterative Schema-Guided Scene-Graph reasoning framework based on multi-agent LLMs.", "The agents are grouped into two modules: a (1) Reasoner module for abstract task planning and graph information queries generation, and a (2) Retriever module for extracting corresponding graph information based on code-writing following the queries."], "method": "In this work, we propose SG^2, an iterative Schema-Guided Scene-Graph reasoning framework based on multi-agent LLMs.", "key_results": ["In this work, we propose SG^2, an iterative Schema-Guided Scene-Graph reasoning framework based on multi-agent LLMs.", "The agents are grouped into two modules: a (1) Reasoner module for abstract task planning and graph information queries generation, and a (2) Retriever module for extracting corresponding graph information based on code-writing following the queries."], "limitations": ["Abstract-level evidence only: validate assumptions, evaluation protocol, and failure cases in the full paper before relying on this as key evidence."], "bibkey": "Chen2025Schema"}
{"paper_id": "P0213", "title": "Secure Multi-LLM Agentic AI and Agentification for Edge General Intelligence by Zero-Trust: A Survey", "year": 2025, "url": "http://arxiv.org/abs/2508.19870v1", "arxiv_id": "2508.19870v1", "primary_category": "cs.NI", "categories": ["cs.NI"], "pdf_url": "https://arxiv.org/pdf/2508.19870v1", "priority": "normal", "mapped_sections": ["6.2"], "evidence_level": "abstract", "fulltext_path": "", "authors": ["Yinqiu Liu", "Ruichen Zhang", "Haoxiang Luo", "Yijing Lin", "Geng Sun", "Dusit Niyato", "Hongyang Du", "Zehui Xiong", "Yonggang Wen", "Abbas Jamalipour", "Dong In Kim", "Ping Zhang"], "abstract": "Agentification serves as a critical enabler of Edge General Intelligence (EGI), transforming massive edge devices into cognitive agents through integrating Large Language Models (LLMs) and perception, reasoning, and acting modules. These agents collaborate across heterogeneous edge infrastructures, forming multi-LLM agentic AI systems that leverage collective intelligence and specialized capabilities to tackle complex, multi-step tasks. However, the collaborative nature of multi-LLM systems introduces critical security vulnerabilities, including insecure inter-LLM communications, expanded attack surfaces, and cross-domain data leakage that traditional perimeter-based security cannot adequately address. To this end, this survey introduces zero-trust security of multi-LLM in EGI, a paradigmatic shift following the ``never trust, always verify'' principle. We begin by systematically analyzing the security risks in multi-LLM systems within EGI contexts. Subsequently, we present the vision of a zero-trust multi-LLM framework in EGI. We then survey key technical progress to facilitate zero-trust multi-LLM systems in EGI. Particularly, we categorize zero-trust security mechanisms into model- and system-level approaches. The former and latter include strong identification, context-aware access control, etc., and proactive maintenance, blockchain-based management, etc., respectively. Finally, we identify critical research directions. This survey serves as the first systematic treatment of zero-trust applied to multi-LLM systems, providing both theoretical foundations and practical strategies.", "summary_bullets": ["Agentification serves as a critical enabler of Edge General Intelligence (EGI), transforming massive edge devices into cognitive agents through integrating Large Language Models (LLMs) and perception, reasoning, and acting modules.", "These agents collaborate across heterogeneous edge infrastructures, forming multi-LLM agentic AI systems that leverage collective intelligence and specialized capabilities to tackle complex, multi-step tasks.", "However, the collaborative nature of multi-LLM systems introduces critical security vulnerabilities, including insecure inter-LLM communications, expanded attack surfaces, and cross-domain data leakage that traditional perimeter-based security cannot adequately address."], "method": "Subsequently, we present the vision of a zero-trust multi-LLM framework in EGI.", "key_results": ["This survey serves as the first systematic treatment of zero-trust applied to multi-LLM systems, providing both theoretical foundations and practical strategies."], "limitations": ["Abstract-level evidence only: validate assumptions, evaluation protocol, and failure cases in the full paper before relying on this as key evidence."], "bibkey": "Liu2025Secure"}
{"paper_id": "P0214", "title": "Self-Challenging Language Model Agents", "year": 2025, "url": "http://arxiv.org/abs/2506.01716v1", "arxiv_id": "2506.01716v1", "primary_category": "cs.AI", "categories": ["cs.AI", "cs.CL"], "pdf_url": "https://arxiv.org/pdf/2506.01716v1", "priority": "normal", "mapped_sections": ["3.2", "5.1"], "evidence_level": "abstract", "fulltext_path": "", "authors": ["Yifei Zhou", "Sergey Levine", "Jason Weston", "Xian Li", "Sainbayar Sukhbaatar"], "abstract": "Large language models are quickly becoming the foundation for intelligent agents that are capable of using tools. However, training such agents is challenging because it requires human creation and annotation of a diverse set of tasks, tools, and evaluation criteria. In this paper, we propose the Self-Challenging framework for training an agent on high-quality tasks that are generated by itself. The agent first plays the role of challenger and generates a task after interacting with the given tools. The tasks take the form of a novel general class of problems termed Code-as-Task, which are defined by an instruction, a verification function and solution and failure cases which serve as tests, allowing to filter only for high-quality tasks. The agent then takes an executor role and trains on those tasks with reinforcement learning using the evaluation feedback as a reward. Evaluation on two existing multi-turn tool-use agent benchmarks, M3ToolEval and TauBench, shows the Self-Challenging framework achieves over a two-fold improvement in Llama-3.1-8B-Instruct, despite using only self-generated training data.", "summary_bullets": ["Large language models are quickly becoming the foundation for intelligent agents that are capable of using tools.", "However, training such agents is challenging because it requires human creation and annotation of a diverse set of tasks, tools, and evaluation criteria.", "In this paper, we propose the Self-Challenging framework for training an agent on high-quality tasks that are generated by itself."], "method": "In this paper, we propose the Self-Challenging framework for training an agent on high-quality tasks that are generated by itself.", "key_results": ["Evaluation on two existing multi-turn tool-use agent benchmarks, M3ToolEval and TauBench, shows the Self-Challenging framework achieves over a two-fold improvement in Llama-3.1-8B-Instruct, despite using only self-generated training data.", "However, training such agents is challenging because it requires human creation and annotation of a diverse set of tasks, tools, and evaluation criteria."], "limitations": ["Abstract-level evidence only: validate assumptions, evaluation protocol, and failure cases in the full paper before relying on this as key evidence."], "bibkey": "Zhou2025Self"}
{"paper_id": "P0215", "title": "Semantically-Aware LLM Agent to Enhance Privacy in Conversational AI Services", "year": 2025, "url": "http://arxiv.org/abs/2510.27016v1", "arxiv_id": "2510.27016v1", "primary_category": "cs.CL", "categories": ["cs.CL"], "pdf_url": "https://arxiv.org/pdf/2510.27016v1", "priority": "normal", "mapped_sections": [], "evidence_level": "abstract", "fulltext_path": "", "authors": ["Jayden Serenari", "Stephen Lee"], "abstract": "With the increasing use of conversational AI systems, there is growing concern over privacy leaks, especially when users share sensitive personal data in interactions with Large Language Models (LLMs). Conversations shared with these models may contain Personally Identifiable Information (PII), which, if exposed, could lead to security breaches or identity theft. To address this challenge, we present the Local Optimizations for Pseudonymization with Semantic Integrity Directed Entity Detection (LOPSIDED) framework, a semantically-aware privacy agent designed to safeguard sensitive PII data when using remote LLMs. Unlike prior work that often degrade response quality, our approach dynamically replaces sensitive PII entities in user prompts with semantically consistent pseudonyms, preserving the contextual integrity of conversations. Once the model generates its response, the pseudonyms are automatically depseudonymized, ensuring the user receives an accurate, privacy-preserving output. We evaluate our approach using real-world conversations sourced from ShareGPT, which we further augment and annotate to assess whether named entities are contextually relevant to the model's response. Our results show that LOPSIDED reduces semantic utility errors by a factor of 5 compared to baseline techniques, all while enhancing privacy.", "summary_bullets": ["With the increasing use of conversational AI systems, there is growing concern over privacy leaks, especially when users share sensitive personal data in interactions with Large Language Models (LLMs).", "Conversations shared with these models may contain Personally Identifiable Information (PII), which, if exposed, could lead to security breaches or identity theft.", "To address this challenge, we present the Local Optimizations for Pseudonymization with Semantic Integrity Directed Entity Detection (LOPSIDED) framework, a semantically-aware privacy agent designed to safeguard sensitive PII data when using remote LLMs."], "method": "To address this challenge, we present the Local Optimizations for Pseudonymization with Semantic Integrity Directed Entity Detection (LOPSIDED) framework, a semantically-aware privacy agent designed to safeguard sensitive PII data when using remote LLMs.", "key_results": ["Our results show that LOPSIDED reduces semantic utility errors by a factor of 5 compared to baseline techniques, all while enhancing privacy."], "limitations": ["Abstract-level evidence only: validate assumptions, evaluation protocol, and failure cases in the full paper before relying on this as key evidence."], "bibkey": "Serenari2025Semantically"}
{"paper_id": "P0216", "title": "SimuHome: A Temporal- and Environment-Aware Benchmark for Smart Home LLM Agents", "year": 2025, "url": "http://arxiv.org/abs/2509.24282v2", "arxiv_id": "2509.24282v2", "primary_category": "cs.CL", "categories": ["cs.CL", "cs.AI"], "pdf_url": "https://arxiv.org/pdf/2509.24282v2", "priority": "normal", "mapped_sections": ["6.1", "6.2"], "evidence_level": "abstract", "fulltext_path": "", "authors": ["Gyuhyeon Seo", "Jungwoo Yang", "Junseong Pyo", "Nalim Kim", "Jonggeun Lee", "Yohan Jo"], "abstract": "Large Language Model (LLM) agents excel at multi-step, tool-augmented tasks. However, smart homes introduce distinct challenges, requiring agents to handle latent user intents, temporal dependencies, device constraints, scheduling, and more. The main bottlenecks for developing smart home agents with such capabilities include the lack of a realistic simulation environment where agents can interact with devices and observe the results, as well as a challenging benchmark to evaluate them. To address this, we introduce $\\textbf{SimuHome}$, a time-accelerated home environment that simulates smart devices, supports API calls, and reflects changes in environmental variables. By building the simulator on the Matter protocol, the global industry standard for smart home communication, SimuHome provides a high-fidelity environment, and agents validated in SimuHome can be deployed on real Matter-compliant devices with minimal adaptation. We provide a challenging benchmark of 600 episodes across twelve user query types that require the aforementioned capabilities. Our evaluation of 16 agents under a unified ReAct framework reveals distinct capabilities and limitations across models. Models under 7B parameters exhibited negligible performance across all query types. Even GPT-4.1, the best-performing standard model, struggled with implicit intent inference, state verification, and particularly temporal scheduling. While reasoning models such as GPT-5.1 consistently outperformed standard models on every query type, they required over three times the average inference time, which can be prohibitive for real-time smart home applications. This highlights a critical trade-off between task performance and real-world practicality.", "summary_bullets": ["Large Language Model (LLM) agents excel at multi-step, tool-augmented tasks.", "However, smart homes introduce distinct challenges, requiring agents to handle latent user intents, temporal dependencies, device constraints, scheduling, and more.", "The main bottlenecks for developing smart home agents with such capabilities include the lack of a realistic simulation environment where agents can interact with devices and observe the results, as well as a challenging benchmark to evaluate them."], "method": "To address this, we introduce $\\textbf{SimuHome}$, a time-accelerated home environment that simulates smart devices, supports API calls, and reflects changes in environmental variables.", "key_results": ["We provide a challenging benchmark of 600 episodes across twelve user query types that require the aforementioned capabilities.", "Our evaluation of 16 agents under a unified ReAct framework reveals distinct capabilities and limitations across models."], "limitations": ["Abstract-level evidence only: validate assumptions, evaluation protocol, and failure cases in the full paper before relying on this as key evidence.", "Our evaluation of 16 agents under a unified ReAct framework reveals distinct capabilities and limitations across models."], "bibkey": "Seo2025Simuhome"}
{"paper_id": "P0217", "title": "StaffPro: an LLM Agent for Joint Staffing and Profiling", "year": 2025, "url": "http://arxiv.org/abs/2507.21636v1", "arxiv_id": "2507.21636v1", "primary_category": "cs.AI", "categories": ["cs.AI"], "pdf_url": "https://arxiv.org/pdf/2507.21636v1", "priority": "normal", "mapped_sections": [], "evidence_level": "abstract", "fulltext_path": "", "authors": ["Alessio Maritan"], "abstract": "Large language model (LLM) agents integrate pre-trained LLMs with modular algorithmic components and have shown remarkable reasoning and decision-making abilities. In this work, we investigate their use for two tightly intertwined challenges in workforce management: staffing, i.e., the assignment and scheduling of tasks to workers, which may require team formation; and profiling, i.e., the continuous estimation of workers' skills, preferences, and other latent attributes from unstructured data. We cast these problems in a formal mathematical framework that links scheduling decisions to latent feature estimation, and we introduce StaffPro, an LLM agent that addresses staffing and profiling jointly. Differently from existing staffing solutions, StaffPro allows expressing optimization objectives using natural language, accepts textual task descriptions and provides high flexibility. StaffPro interacts directly with humans by establishing a continuous human-agent feedback loop, ensuring natural and intuitive use. By analyzing human feedback, our agent continuously estimates the latent features of workers, realizing life-long worker profiling and ensuring optimal staffing performance over time. A consulting firm simulation example demonstrates that StaffPro successfully estimates workers' attributes and generates high quality schedules. With its innovative design, StaffPro offers a robust, interpretable, and human-centric solution for automated personnel management.", "summary_bullets": ["Large language model (LLM) agents integrate pre-trained LLMs with modular algorithmic components and have shown remarkable reasoning and decision-making abilities.", "In this work, we investigate their use for two tightly intertwined challenges in workforce management: staffing, i.e., the assignment and scheduling of tasks to workers, which may require team formation; and profiling, i.e., the continuous estimation of workers' skills, preferences, and other latent attributes from unstructured data.", "We cast these problems in a formal mathematical framework that links scheduling decisions to latent feature estimation, and we introduce StaffPro, an LLM agent that addresses staffing and profiling jointly."], "method": "We cast these problems in a formal mathematical framework that links scheduling decisions to latent feature estimation, and we introduce StaffPro, an LLM agent that addresses staffing and profiling jointly.", "key_results": ["StaffPro interacts directly with humans by establishing a continuous human-agent feedback loop, ensuring natural and intuitive use.", "By analyzing human feedback, our agent continuously estimates the latent features of workers, realizing life-long worker profiling and ensuring optimal staffing performance over time."], "limitations": ["Abstract-level evidence only: validate assumptions, evaluation protocol, and failure cases in the full paper before relying on this as key evidence."], "bibkey": "Maritan2025Staffpro"}
{"paper_id": "P0218", "title": "StockBench: Can LLM Agents Trade Stocks Profitably In Real-world Markets?", "year": 2025, "url": "http://arxiv.org/abs/2510.02209v1", "arxiv_id": "2510.02209v1", "primary_category": "cs.LG", "categories": ["cs.LG", "cs.CL"], "pdf_url": "https://arxiv.org/pdf/2510.02209v1", "priority": "normal", "mapped_sections": [], "evidence_level": "abstract", "fulltext_path": "", "authors": ["Yanxu Chen", "Zijun Yao", "Yantao Liu", "Jin Ye", "Jianing Yu", "Lei Hou", "Juanzi Li"], "abstract": "Large language models (LLMs) have recently demonstrated strong capabilities as autonomous agents, showing promise in reasoning, tool use, and sequential decision-making. While prior benchmarks have evaluated LLM agents in domains such as software engineering and scientific discovery, the finance domain remains underexplored, despite its direct relevance to economic value and high-stakes decision-making. Existing financial benchmarks primarily test static knowledge through question answering, but they fall short of capturing the dynamic and iterative nature of trading. To address this gap, we introduce StockBench, a contamination-free benchmark designed to evaluate LLM agents in realistic, multi-month stock trading environments. Agents receive daily market signals -- including prices, fundamentals, and news -- and must make sequential buy, sell, or hold decisions. Performance is assessed using financial metrics such as cumulative return, maximum drawdown, and the Sortino ratio. Our evaluation of state-of-the-art proprietary (e.g., GPT-5, Claude-4) and open-weight (e.g., Qwen3, Kimi-K2, GLM-4.5) models shows that while most LLM agents struggle to outperform the simple buy-and-hold baseline, several models demonstrate the potential to deliver higher returns and manage risk more effectively. These findings highlight both the challenges and opportunities in developing LLM-powered financial agents, showing that excelling at static financial knowledge tasks does not necessarily translate into successful trading strategies. We release StockBench as an open-source resource to support reproducibility and advance future research in this domain.", "summary_bullets": ["Large language models (LLMs) have recently demonstrated strong capabilities as autonomous agents, showing promise in reasoning, tool use, and sequential decision-making.", "While prior benchmarks have evaluated LLM agents in domains such as software engineering and scientific discovery, the finance domain remains underexplored, despite its direct relevance to economic value and high-stakes decision-making.", "Existing financial benchmarks primarily test static knowledge through question answering, but they fall short of capturing the dynamic and iterative nature of trading."], "method": "To address this gap, we introduce StockBench, a contamination-free benchmark designed to evaluate LLM agents in realistic, multi-month stock trading environments.", "key_results": ["Our evaluation of state-of-the-art proprietary (e.g., GPT-5, Claude-4) and open-weight (e.g., Qwen3, Kimi-K2, GLM-4.5) models shows that while most LLM agents struggle to outperform the simple buy-and-hold baseline, several models demonstrate the potential to deliver higher returns and manage risk more effectively.", "While prior benchmarks have evaluated LLM agents in domains such as software engineering and scientific discovery, the finance domain remains underexplored, despite its direct relevance to economic value and high-stakes decision-making."], "limitations": ["Abstract-level evidence only: validate assumptions, evaluation protocol, and failure cases in the full paper before relying on this as key evidence.", "These findings highlight both the challenges and opportunities in developing LLM-powered financial agents, showing that excelling at static financial knowledge tasks does not necessarily translate into successful trading strategies."], "bibkey": "Chen2025Stockbench"}
{"paper_id": "P0219", "title": "Surgical AI Copilot: Energy-Based Fourier Gradient Low-Rank Adaptation for Surgical LLM Agent Reasoning and Planning", "year": 2025, "url": "http://arxiv.org/abs/2503.09474v2", "arxiv_id": "2503.09474v2", "primary_category": "cs.CV", "categories": ["cs.CV"], "pdf_url": "https://arxiv.org/pdf/2503.09474v2", "priority": "normal", "mapped_sections": ["4.1"], "evidence_level": "abstract", "fulltext_path": "", "authors": ["Jiayuan Huang", "Runlong He", "Danyal Zaman Khan", "Evangelos B. Mazomenos", "Danail Stoyanov", "Hani Marcus", "Linzhe Jiang", "Matthew J Clarkson", "Mobarak I. Hoque"], "abstract": "Image-guided surgery demands adaptive, real-time decision support, yet static AI models struggle with structured task planning and providing interactive guidance. Large language models (LLMs)-powered agents offer a promising solution by enabling dynamic task planning and predictive decision support. Despite recent advances, the absence of surgical agent datasets and robust parameter-efficient fine-tuning techniques limits the development of LLM agents capable of complex intraoperative reasoning. In this paper, we introduce Surgical AI Copilot, an LLM agent for image-guided pituitary surgery, capable of conversation, planning, and task execution in response to queries involving tasks such as MRI tumor segmentation, endoscope anatomy segmentation, overlaying preoperative imaging with intraoperative views, instrument tracking, and surgical visual question answering (VQA). To enable structured agent planning, we develop the PitAgent dataset, a surgical context-aware planning dataset covering surgical tasks like workflow analysis, instrument localization, anatomical segmentation, and query-based reasoning. Additionally, we propose DEFT-GaLore, a Deterministic Energy-based Fourier Transform (DEFT) gradient projection technique for efficient low-rank adaptation of recent LLMs (e.g., LLaMA 3.2, Qwen 2.5), enabling their use as surgical agent planners. We extensively validate our agent's performance and the proposed adaptation technique against other state-of-the-art low-rank adaptation methods on agent planning and prompt generation tasks, including a zero-shot surgical VQA benchmark, demonstrating the significant potential for truly efficient and scalable surgical LLM agents in real-time operative settings.", "summary_bullets": ["Image-guided surgery demands adaptive, real-time decision support, yet static AI models struggle with structured task planning and providing interactive guidance.", "Large language models (LLMs)-powered agents offer a promising solution by enabling dynamic task planning and predictive decision support.", "Despite recent advances, the absence of surgical agent datasets and robust parameter-efficient fine-tuning techniques limits the development of LLM agents capable of complex intraoperative reasoning."], "method": "In this paper, we introduce Surgical AI Copilot, an LLM agent for image-guided pituitary surgery, capable of conversation, planning, and task execution in response to queries involving tasks such as MRI tumor segmentation, endoscope anatomy segmentation, overlaying preoperative imaging with intraoperative views, instrument tracking, and surgical visual question answering (VQA).", "key_results": ["Additionally, we propose DEFT-GaLore, a Deterministic Energy-based Fourier Transform (DEFT) gradient projection technique for efficient low-rank adaptation of recent LLMs (e.g., LLaMA 3.2, Qwen 2.5), enabling their use as surgical agent planners.", "We extensively validate our agent's performance and the proposed adaptation technique against other state-of-the-art low-rank adaptation methods on agent planning and prompt generation tasks, including a zero-shot surgical VQA benchmark, demonstrating the significant potential for truly efficient and scalable surgical LLM agents in real-time operative settings."], "limitations": ["Abstract-level evidence only: validate assumptions, evaluation protocol, and failure cases in the full paper before relying on this as key evidence."], "bibkey": "Huang2025Surgical"}
{"paper_id": "P0220", "title": "Survey and Experiments on Mental Disorder Detection via Social Media: From Large Language Models and RAG to Agents", "year": 2025, "url": "http://arxiv.org/abs/2504.02800v4", "arxiv_id": "2504.02800v4", "primary_category": "cs.CL", "categories": ["cs.CL"], "pdf_url": "https://arxiv.org/pdf/2504.02800v4", "priority": "normal", "mapped_sections": ["4.2"], "evidence_level": "abstract", "fulltext_path": "", "authors": ["Zhuohan Ge", "Darian Li", "Yubo Wang", "Nicole Hu", "Xinyi Zhu", "Haoyang Li", "Xin Zhang", "Mingtao Zhang", "Shihao Qi", "Yuming Xu", "Han Shi", "Chen Jason Zhang", "Qing Li"], "abstract": "Mental disorders represent a critical global health challenge, and social media is increasingly viewed as a vital resource for real-time digital phenotyping and intervention. To leverage this data, large language models (LLMs) have been introduced, offering stronger semantic understanding and reasoning than traditional deep learning, thereby enhancing the explainability of detection results. Despite the growing prominence of LLMs in this field, there is a scarcity of scholarly works that systematically synthesize how advanced enhancement techniques, specifically Retrieval-Augmented Generation (RAG) and Agentic systems, can be utilized to address these reliability and reasoning limitations. Here, we systematically survey the evolving landscape of LLM-based methods for social media mental disorder analysis, spanning standard pre-trained language models, RAG to mitigate hallucinations and contextual gaps, and agentic systems for autonomous reasoning and multi-step intervention. We organize existing work by technical paradigm and clinical target, extending beyond common internalizing disorders to include psychotic disorders and externalizing behaviors. Additionally, the paper comprehensively evaluates the performance of LLMs, including the impact of RAG, across various tasks. This work establishes a unified benchmark for the field, paving the way for the development of trustworthy, autonomous AI systems that can deliver precise and explainable mental health support.", "summary_bullets": ["Mental disorders represent a critical global health challenge, and social media is increasingly viewed as a vital resource for real-time digital phenotyping and intervention.", "To leverage this data, large language models (LLMs) have been introduced, offering stronger semantic understanding and reasoning than traditional deep learning, thereby enhancing the explainability of detection results.", "Despite the growing prominence of LLMs in this field, there is a scarcity of scholarly works that systematically synthesize how advanced enhancement techniques, specifically Retrieval-Augmented Generation (RAG) and Agentic systems, can be utilized to address these reliability and reasoning limitations."], "method": "Mental disorders represent a critical global health challenge, and social media is increasingly viewed as a vital resource for real-time digital phenotyping and intervention.", "key_results": ["This work establishes a unified benchmark for the field, paving the way for the development of trustworthy, autonomous AI systems that can deliver precise and explainable mental health support."], "limitations": ["Abstract-level evidence only: validate assumptions, evaluation protocol, and failure cases in the full paper before relying on this as key evidence.", "Despite the growing prominence of LLMs in this field, there is a scarcity of scholarly works that systematically synthesize how advanced enhancement techniques, specifically Retrieval-Augmented Generation (RAG) and Agentic systems, can be utilized to address these reliability and reasoning limitations."], "bibkey": "Ge2025Surveya"}
{"paper_id": "P0221", "title": "Survey of LLM Agent Communication with MCP: A Software Design Pattern Centric Review", "year": 2025, "url": "http://arxiv.org/abs/2506.05364v1", "arxiv_id": "2506.05364v1", "primary_category": "cs.SE", "categories": ["cs.SE"], "pdf_url": "https://arxiv.org/pdf/2506.05364v1", "priority": "normal", "mapped_sections": ["5.1"], "evidence_level": "abstract", "fulltext_path": "", "authors": ["Anjana Sarkar", "Soumyendu Sarkar"], "abstract": "This survey investigates how classical software design patterns can enhance the reliability and scalability of communication in Large Language Model (LLM)-driven agentic AI systems, focusing particularly on the Model Context Protocol (MCP). It examines the foundational architectures of LLM-based agents and their evolution from isolated operation to sophisticated, multi-agent collaboration, addressing key communication hurdles that arise in this transition. The study revisits well-established patterns, including Mediator, Observer, Publish-Subscribe, and Broker, and analyzes their relevance in structuring agent interactions within MCP-compliant frameworks. To clarify these dynamics, the article provides conceptual schematics and formal models that map out communication pathways and optimize data flow. It further explores architectural variations suited to different degrees of agent autonomy and system complexity. Real-world applications in domains such as real-time financial processing and investment banking are discussed, illustrating how these patterns and MCP can meet specific operational demands. The article concludes by outlining open challenges, potential security risks, and promising directions for advancing robust, interoperable, and scalable multi-agent LLM ecosystems.", "summary_bullets": ["This survey investigates how classical software design patterns can enhance the reliability and scalability of communication in Large Language Model (LLM)-driven agentic AI systems, focusing particularly on the Model Context Protocol (MCP).", "It examines the foundational architectures of LLM-based agents and their evolution from isolated operation to sophisticated, multi-agent collaboration, addressing key communication hurdles that arise in this transition.", "The study revisits well-established patterns, including Mediator, Observer, Publish-Subscribe, and Broker, and analyzes their relevance in structuring agent interactions within MCP-compliant frameworks."], "method": "This survey investigates how classical software design patterns can enhance the reliability and scalability of communication in Large Language Model (LLM)-driven agentic AI systems, focusing particularly on the Model Context Protocol (MCP).", "key_results": ["The article concludes by outlining open challenges, potential security risks, and promising directions for advancing robust, interoperable, and scalable multi-agent LLM ecosystems."], "limitations": ["Abstract-level evidence only: validate assumptions, evaluation protocol, and failure cases in the full paper before relying on this as key evidence."], "bibkey": "Sarkar2025Survey"}
{"paper_id": "P0222", "title": "Survey of Specialized Large Language Model", "year": 2025, "url": "http://arxiv.org/abs/2508.19667v1", "arxiv_id": "2508.19667v1", "primary_category": "cs.CL", "categories": ["cs.CL", "cs.AI"], "pdf_url": "https://arxiv.org/pdf/2508.19667v1", "priority": "normal", "mapped_sections": [], "evidence_level": "abstract", "fulltext_path": "", "authors": ["Chenghan Yang", "Ruiyu Zhao", "Yang Liu", "Ling Jiang"], "abstract": "The rapid evolution of specialized large language models (LLMs) has transitioned from simple domain adaptation to sophisticated native architectures, marking a paradigm shift in AI development. This survey systematically examines this progression across healthcare, finance, legal, and technical domains. Besides the wide use of specialized LLMs, technical breakthrough such as the emergence of domain-native designs beyond fine-tuning, growing emphasis on parameter efficiency through sparse computation and quantization, increasing integration of multimodal capabilities and so on are applied to recent LLM agent. Our analysis reveals how these innovations address fundamental limitations of general-purpose LLMs in professional applications, with specialized models consistently performance gains on domain-specific benchmarks. The survey further highlights the implications for E-Commerce field to fill gaps in the field.", "summary_bullets": ["The rapid evolution of specialized large language models (LLMs) has transitioned from simple domain adaptation to sophisticated native architectures, marking a paradigm shift in AI development.", "This survey systematically examines this progression across healthcare, finance, legal, and technical domains.", "Besides the wide use of specialized LLMs, technical breakthrough such as the emergence of domain-native designs beyond fine-tuning, growing emphasis on parameter efficiency through sparse computation and quantization, increasing integration of multimodal capabilities and so on are applied to recent LLM agent."], "method": "The rapid evolution of specialized large language models (LLMs) has transitioned from simple domain adaptation to sophisticated native architectures, marking a paradigm shift in AI development.", "key_results": ["Our analysis reveals how these innovations address fundamental limitations of general-purpose LLMs in professional applications, with specialized models consistently performance gains on domain-specific benchmarks."], "limitations": ["Abstract-level evidence only: validate assumptions, evaluation protocol, and failure cases in the full paper before relying on this as key evidence.", "Our analysis reveals how these innovations address fundamental limitations of general-purpose LLMs in professional applications, with specialized models consistently performance gains on domain-specific benchmarks."], "bibkey": "Yang2025Survey"}
{"paper_id": "P0223", "title": "Task Memory Engine (TME): Enhancing State Awareness for Multi-Step LLM Agent Tasks", "year": 2025, "url": "http://arxiv.org/abs/2504.08525v4", "arxiv_id": "2504.08525v4", "primary_category": "cs.AI", "categories": ["cs.AI", "cs.CL"], "pdf_url": "https://arxiv.org/pdf/2504.08525v4", "priority": "normal", "mapped_sections": ["4.2"], "evidence_level": "abstract", "fulltext_path": "", "authors": ["Ye Ye"], "abstract": "Large Language Models (LLMs) are increasingly used as autonomous agents for multi-step tasks. However, most existing frameworks fail to maintain a structured understanding of the task state, often relying on linear prompt concatenation or shallow memory buffers. This leads to brittle performance, frequent hallucinations, and poor long-range coherence. In this work, we propose the Task Memory Engine (TME), a lightweight and structured memory module that tracks task execution using a hierarchical Task Memory Tree (TMT). Each node in the tree corresponds to a task step, storing relevant input, output, status, and sub-task relationships. We introduce a prompt synthesis method that dynamically generates LLM prompts based on the active node path, significantly improving execution consistency and contextual grounding. Through case studies and comparative experiments on multi-step agent tasks, we demonstrate that TME leads to better task completion accuracy and more interpretable behavior with minimal implementation overhead. A reference implementation of the core TME components is available at https://github.com/biubiutomato/TME-Agent, including basic examples and structured memory integration. While the current implementation uses a tree-based structure, TME is designed to be graph-aware, supporting reusable substeps, converging task paths, and shared dependencies. This lays the groundwork for future DAG-based memory architectures.", "summary_bullets": ["Large Language Models (LLMs) are increasingly used as autonomous agents for multi-step tasks.", "However, most existing frameworks fail to maintain a structured understanding of the task state, often relying on linear prompt concatenation or shallow memory buffers.", "This leads to brittle performance, frequent hallucinations, and poor long-range coherence."], "method": "In this work, we propose the Task Memory Engine (TME), a lightweight and structured memory module that tracks task execution using a hierarchical Task Memory Tree (TMT).", "key_results": ["Through case studies and comparative experiments on multi-step agent tasks, we demonstrate that TME leads to better task completion accuracy and more interpretable behavior with minimal implementation overhead."], "limitations": ["Abstract-level evidence only: validate assumptions, evaluation protocol, and failure cases in the full paper before relying on this as key evidence."], "bibkey": "Ye2025Taska"}
{"paper_id": "P0224", "title": "Taxonomy, Evaluation and Exploitation of IPI-Centric LLM Agent Defense Frameworks", "year": 2025, "url": "http://arxiv.org/abs/2511.15203v1", "arxiv_id": "2511.15203v1", "primary_category": "cs.CR", "categories": ["cs.CR", "cs.AI"], "pdf_url": "https://arxiv.org/pdf/2511.15203v1", "priority": "normal", "mapped_sections": ["6.1"], "evidence_level": "abstract", "fulltext_path": "", "authors": ["Zimo Ji", "Xunguang Wang", "Zongjie Li", "Pingchuan Ma", "Yudong Gao", "Daoyuan Wu", "Xincheng Yan", "Tian Tian", "Shuai Wang"], "abstract": "Large Language Model (LLM)-based agents with function-calling capabilities are increasingly deployed, but remain vulnerable to Indirect Prompt Injection (IPI) attacks that hijack their tool calls. In response, numerous IPI-centric defense frameworks have emerged. However, these defenses are fragmented, lacking a unified taxonomy and comprehensive evaluation. In this Systematization of Knowledge (SoK), we present the first comprehensive analysis of IPI-centric defense frameworks. We introduce a comprehensive taxonomy of these defenses, classifying them along five dimensions. We then thoroughly assess the security and usability of representative defense frameworks. Through analysis of defensive failures in the assessment, we identify six root causes of defense circumvention. Based on these findings, we design three novel adaptive attacks that significantly improve attack success rates targeting specific frameworks, demonstrating the severity of the flaws in these defenses. Our paper provides a foundation and critical insights for the future development of more secure and usable IPI-centric agent defense frameworks.", "summary_bullets": ["Large Language Model (LLM)-based agents with function-calling capabilities are increasingly deployed, but remain vulnerable to Indirect Prompt Injection (IPI) attacks that hijack their tool calls.", "In response, numerous IPI-centric defense frameworks have emerged.", "However, these defenses are fragmented, lacking a unified taxonomy and comprehensive evaluation."], "method": "In this Systematization of Knowledge (SoK), we present the first comprehensive analysis of IPI-centric defense frameworks.", "key_results": ["However, these defenses are fragmented, lacking a unified taxonomy and comprehensive evaluation.", "Based on these findings, we design three novel adaptive attacks that significantly improve attack success rates targeting specific frameworks, demonstrating the severity of the flaws in these defenses."], "limitations": ["Abstract-level evidence only: validate assumptions, evaluation protocol, and failure cases in the full paper before relying on this as key evidence."], "bibkey": "Ji2025Taxonomy"}
{"paper_id": "P0225", "title": "The Complexity Trap: Simple Observation Masking Is as Efficient as LLM Summarization for Agent Context Management", "year": 2025, "url": "http://arxiv.org/abs/2508.21433v3", "arxiv_id": "2508.21433v3", "primary_category": "cs.SE", "categories": ["cs.SE", "cs.AI"], "pdf_url": "https://arxiv.org/pdf/2508.21433v3", "priority": "normal", "mapped_sections": ["3.1"], "evidence_level": "abstract", "fulltext_path": "", "authors": ["Tobias Lindenbauer", "Igor Slinko", "Ludwig Felder", "Egor Bogomolov", "Yaroslav Zharov"], "abstract": "Large Language Model (LLM)-based agents solve complex tasks through iterative reasoning, exploration, and tool-use, a process that can result in long, expensive context histories. While state-of-the-art Software Engineering (SE) agents like OpenHands or Cursor use LLM-based summarization to tackle this issue, it is unclear whether the increased complexity offers tangible performance benefits compared to simply omitting older observations. We present a systematic comparison of these approaches within SWE-agent on SWE-bench Verified across five diverse model configurations. Moreover, we show initial evidence of our findings generalizing to the OpenHands agent scaffold. We find that a simple environment observation masking strategy halves cost relative to the raw agent while matching, and sometimes slightly exceeding, the solve rate of LLM summarization. Additionally, we introduce a novel hybrid approach that further reduces costs by 7% and 11% compared to just observation masking or LLM summarization, respectively. Our findings raise concerns regarding the trend towards pure LLM summarization and provide initial evidence of untapped cost reductions by pushing the efficiency-effectiveness frontier. We release code and data for reproducibility.", "summary_bullets": ["Large Language Model (LLM)-based agents solve complex tasks through iterative reasoning, exploration, and tool-use, a process that can result in long, expensive context histories.", "While state-of-the-art Software Engineering (SE) agents like OpenHands or Cursor use LLM-based summarization to tackle this issue, it is unclear whether the increased complexity offers tangible performance benefits compared to simply omitting older observations.", "We present a systematic comparison of these approaches within SWE-agent on SWE-bench Verified across five diverse model configurations."], "method": "We present a systematic comparison of these approaches within SWE-agent on SWE-bench Verified across five diverse model configurations.", "key_results": ["Additionally, we introduce a novel hybrid approach that further reduces costs by 7% and 11% compared to just observation masking or LLM summarization, respectively.", "While state-of-the-art Software Engineering (SE) agents like OpenHands or Cursor use LLM-based summarization to tackle this issue, it is unclear whether the increased complexity offers tangible performance benefits compared to simply omitting older observations."], "limitations": ["Abstract-level evidence only: validate assumptions, evaluation protocol, and failure cases in the full paper before relying on this as key evidence."], "bibkey": "Lindenbauer2025Complexity"}
{"paper_id": "P0226", "title": "The Influence of Human-inspired Agentic Sophistication in LLM-driven Strategic Reasoners", "year": 2025, "url": "http://arxiv.org/abs/2505.09396v2", "arxiv_id": "2505.09396v2", "primary_category": "cs.AI", "categories": ["cs.AI", "cs.MA"], "pdf_url": "https://arxiv.org/pdf/2505.09396v2", "priority": "normal", "mapped_sections": [], "evidence_level": "abstract", "fulltext_path": "", "authors": ["Vince Trencsenyi", "Agnieszka Mensfelt", "Kostas Stathis"], "abstract": "The rapid rise of large language models (LLMs) has shifted artificial intelligence (AI) research toward agentic systems, motivating the use of weaker and more flexible notions of agency. However, this shift raises key questions about the extent to which LLM-based agents replicate human strategic reasoning, particularly in game-theoretic settings. In this context, we examine the role of agentic sophistication in shaping artificial reasoners' performance by evaluating three agent designs: a simple game-theoretic model, an unstructured LLM-as-agent model, and an LLM integrated into a traditional agentic framework. Using guessing games as a testbed, we benchmarked these agents against human participants across general reasoning patterns and individual role-based objectives. Furthermore, we introduced obfuscated game scenarios to assess agents' ability to generalise beyond training distributions. Our analysis, covering over 2000 reasoning samples across 25 agent configurations, shows that human-inspired cognitive structures can enhance LLM agents' alignment with human strategic behaviour. Still, the relationship between agentic design complexity and human-likeness is non-linear, highlighting a critical dependence on underlying LLM capabilities and suggesting limits to simple architectural augmentation.", "summary_bullets": ["The rapid rise of large language models (LLMs) has shifted artificial intelligence (AI) research toward agentic systems, motivating the use of weaker and more flexible notions of agency.", "However, this shift raises key questions about the extent to which LLM-based agents replicate human strategic reasoning, particularly in game-theoretic settings.", "In this context, we examine the role of agentic sophistication in shaping artificial reasoners' performance by evaluating three agent designs: a simple game-theoretic model, an unstructured LLM-as-agent model, and an LLM integrated into a traditional agentic framework."], "method": "The rapid rise of large language models (LLMs) has shifted artificial intelligence (AI) research toward agentic systems, motivating the use of weaker and more flexible notions of agency.", "key_results": ["Our analysis, covering over 2000 reasoning samples across 25 agent configurations, shows that human-inspired cognitive structures can enhance LLM agents' alignment with human strategic behaviour.", "However, this shift raises key questions about the extent to which LLM-based agents replicate human strategic reasoning, particularly in game-theoretic settings."], "limitations": ["Abstract-level evidence only: validate assumptions, evaluation protocol, and failure cases in the full paper before relying on this as key evidence."], "bibkey": "Trencsenyi2025Influence"}
{"paper_id": "P0227", "title": "The Real Barrier to LLM Agent Usability is Agentic ROI", "year": 2025, "url": "http://arxiv.org/abs/2505.17767v1", "arxiv_id": "2505.17767v1", "primary_category": "cs.CL", "categories": ["cs.CL"], "pdf_url": "https://arxiv.org/pdf/2505.17767v1", "priority": "normal", "mapped_sections": [], "evidence_level": "abstract", "fulltext_path": "", "authors": ["Weiwen Liu", "Jiarui Qin", "Xu Huang", "Xingshan Zeng", "Yunjia Xi", "Jianghao Lin", "Chuhan Wu", "Yasheng Wang", "Lifeng Shang", "Ruiming Tang", "Defu Lian", "Yong Yu", "Weinan Zhang"], "abstract": "Large Language Model (LLM) agents represent a promising shift in human-AI interaction, moving beyond passive prompt-response systems to autonomous agents capable of reasoning, planning, and goal-directed action. Despite the widespread application in specialized, high-effort tasks like coding and scientific research, we highlight a critical usability gap in high-demand, mass-market applications. This position paper argues that the limited real-world adoption of LLM agents stems not only from gaps in model capabilities, but also from a fundamental tradeoff between the value an agent can provide and the costs incurred during real-world use. Hence, we call for a shift from solely optimizing model performance to a broader, utility-driven perspective: evaluating agents through the lens of the overall agentic return on investment (Agent ROI). By identifying key factors that determine Agentic ROI--information quality, agent time, and cost--we posit a zigzag development trajectory in optimizing agentic ROI: first scaling up to improve the information quality, then scaling down to minimize the time and cost. We outline the roadmap across different development stages to bridge the current usability gaps, aiming to make LLM agents truly scalable, accessible, and effective in real-world contexts.", "summary_bullets": ["Large Language Model (LLM) agents represent a promising shift in human-AI interaction, moving beyond passive prompt-response systems to autonomous agents capable of reasoning, planning, and goal-directed action.", "Despite the widespread application in specialized, high-effort tasks like coding and scientific research, we highlight a critical usability gap in high-demand, mass-market applications.", "This position paper argues that the limited real-world adoption of LLM agents stems not only from gaps in model capabilities, but also from a fundamental tradeoff between the value an agent can provide and the costs incurred during real-world use."], "method": "Large Language Model (LLM) agents represent a promising shift in human-AI interaction, moving beyond passive prompt-response systems to autonomous agents capable of reasoning, planning, and goal-directed action.", "key_results": ["Large Language Model (LLM) agents represent a promising shift in human-AI interaction, moving beyond passive prompt-response systems to autonomous agents capable of reasoning, planning, and goal-directed action."], "limitations": ["Abstract-level evidence only: validate assumptions, evaluation protocol, and failure cases in the full paper before relying on this as key evidence."], "bibkey": "Liu2025Real"}
{"paper_id": "P0228", "title": "Think, Act, Learn: A Framework for Autonomous Robotic Agents using Closed-Loop Large Language Models", "year": 2025, "url": "http://arxiv.org/abs/2507.19854v3", "arxiv_id": "2507.19854v3", "primary_category": "cs.RO", "categories": ["cs.RO", "cs.HC"], "pdf_url": "https://arxiv.org/pdf/2507.19854v3", "priority": "normal", "mapped_sections": [], "evidence_level": "abstract", "fulltext_path": "", "authors": ["Anjali R. Menon", "Rohit K. Sharma", "Priya Singh", "Chengyu Wang", "Aurora M. Ferreira", "Mateja Novak"], "abstract": "The integration of Large Language Models (LLMs) into robotics has unlocked unprecedented capabilities in high-level task planning. However, most current systems operate in an open-loop fashion, where LLMs act as one-shot planners, rendering them brittle and unable to adapt to unforeseen circumstances in dynamic physical environments. To overcome this limitation, this paper introduces the \"Think, Act, Learn\" (T-A-L) framework, a novel architecture that enables an embodied agent to autonomously learn and refine its policies through continuous interaction. Our framework establishes a closed-loop cycle where an LLM first \"thinks\" by decomposing high-level commands into actionable plans. The robot then \"acts\" by executing these plans while gathering rich, multimodal sensory feedback. Critically, the \"learn\" module processes this feedback to facilitate LLM-driven self-reflection, allowing the agent to perform causal analysis on its failures and generate corrective strategies. These insights are stored in an experiential memory to guide future planning cycles. We demonstrate through extensive experiments in both simulation and the real world that our T-A-L agent significantly outperforms baseline methods, including open-loop LLMs, Behavioral Cloning, and traditional Reinforcement Learning. Our framework achieves over a 97% success rate on complex, long-horizon tasks, converges to a stable policy in an average of just 9 trials, and exhibits remarkable generalization to unseen tasks. This work presents a significant step towards developing more robust, adaptive, and truly autonomous robotic agents.", "summary_bullets": ["The integration of Large Language Models (LLMs) into robotics has unlocked unprecedented capabilities in high-level task planning.", "However, most current systems operate in an open-loop fashion, where LLMs act as one-shot planners, rendering them brittle and unable to adapt to unforeseen circumstances in dynamic physical environments.", "To overcome this limitation, this paper introduces the \"Think, Act, Learn\" (T-A-L) framework, a novel architecture that enables an embodied agent to autonomously learn and refine its policies through continuous interaction."], "method": "Our framework establishes a closed-loop cycle where an LLM first \"thinks\" by decomposing high-level commands into actionable plans.", "key_results": ["Our framework achieves over a 97% success rate on complex, long-horizon tasks, converges to a stable policy in an average of just 9 trials, and exhibits remarkable generalization to unseen tasks."], "limitations": ["Abstract-level evidence only: validate assumptions, evaluation protocol, and failure cases in the full paper before relying on this as key evidence.", "To overcome this limitation, this paper introduces the \"Think, Act, Learn\" (T-A-L) framework, a novel architecture that enables an embodied agent to autonomously learn and refine its policies through continuous interaction."], "bibkey": "Menon2025Think"}
{"paper_id": "P0229", "title": "Tool-RoCo: An Agent-as-Tool Self-organization Large Language Model Benchmark in Multi-robot Cooperation", "year": 2025, "url": "http://arxiv.org/abs/2511.21510v2", "arxiv_id": "2511.21510v2", "primary_category": "cs.MA", "categories": ["cs.MA", "cs.AI"], "pdf_url": "https://arxiv.org/pdf/2511.21510v2", "priority": "normal", "mapped_sections": ["3.2"], "evidence_level": "abstract", "fulltext_path": "", "authors": ["Ke Zhang", "Xiaoning Zhao", "Ce Zheng", "Jiahong Ning", "Dandan Zhu", "Wenqi Zhang", "Chen Sun", "Toshiharu Sugawara"], "abstract": "This study proposes Tool-RoCo, a novel benchmark for evaluating large language models (LLMs) in long-term multi-agent cooperation based on RoCo, a multi-robot cooperative benchmark. Recent research on LLM-based multi-agent systems has relied on predefined orchestration, while ignoring agent autonomy. Tool-RoCo treats other agents as tools and introduces cooperative tools, leveraging tool usage to evaluate multi-agent cooperation and self-organization. Tool usage means that each agent (LLM) selects a tool from a candidate set based on the current state, receives feedback, and adjusts its selection in subsequent rounds. To evaluate different autonomy levels, we propose four LLM paradigms: (1) centralized cooperation, where a single LLM allocates tools to all agents; (2) centralized self-organization, where a central LLM autonomously activates agents while keeping others inactive; (3) decentralized cooperation, where each agent has its own LLM and calls tools based on local information; and (4) self-organization, where a randomly chosen initial agent can request collaboration, activating additional agents via tool calls. Tool-RoCo includes three multi-robot tasks, SORT, PACK, and CABINET, to measure format and parameter accuracy and agent coordination through tool usage. The results using several LLMs showed that cooperative tools accounted for only 7.09% of all tools, indicating that LLM-based agents rarely invoked others as assistants. Moreover, activation tools accounted for 96.42%, suggesting that current LLMs tend to maintain active agents while seldom deactivating them for adaptive coordination. Tool-RoCo provides a systematic benchmark to evaluate LLM autonomy and cooperation in multi-agent tasks. Code and Demo: https://github.com/ColaZhang22/Tool-Roco", "summary_bullets": ["This study proposes Tool-RoCo, a novel benchmark for evaluating large language models (LLMs) in long-term multi-agent cooperation based on RoCo, a multi-robot cooperative benchmark.", "Recent research on LLM-based multi-agent systems has relied on predefined orchestration, while ignoring agent autonomy.", "Tool-RoCo treats other agents as tools and introduces cooperative tools, leveraging tool usage to evaluate multi-agent cooperation and self-organization."], "method": "To evaluate different autonomy levels, we propose four LLM paradigms: (1) centralized cooperation, where a single LLM allocates tools to all agents; (2) centralized self-organization, where a central LLM autonomously activates agents while keeping others inactive; (3) decentralized cooperation, where each agent has its own LLM and calls tools based on local information; and (4) self-organization, where a randomly chosen initial agent can request collaboration, activating additional agents via tool calls.", "key_results": ["To evaluate different autonomy levels, we propose four LLM paradigms: (1) centralized cooperation, where a single LLM allocates tools to all agents; (2) centralized self-organization, where a central LLM autonomously activates agents while keeping others inactive; (3) decentralized cooperation, where each agent has its own LLM and calls tools based on local information; and (4) self-organization, where a randomly chosen initial agent can request collaboration, activating additional agents via tool calls.", "The results using several LLMs showed that cooperative tools accounted for only 7.09% of all tools, indicating that LLM-based agents rarely invoked others as assistants."], "limitations": ["Abstract-level evidence only: validate assumptions, evaluation protocol, and failure cases in the full paper before relying on this as key evidence."], "bibkey": "Zhang2025Tool"}
{"paper_id": "P0230", "title": "Toward Verifiable Misinformation Detection: A Multi-Tool LLM Agent Framework", "year": 2025, "url": "http://arxiv.org/abs/2508.03092v1", "arxiv_id": "2508.03092v1", "primary_category": "cs.AI", "categories": ["cs.AI", "cs.CL"], "pdf_url": "https://arxiv.org/pdf/2508.03092v1", "priority": "normal", "mapped_sections": ["3.2", "5.2"], "evidence_level": "abstract", "fulltext_path": "", "authors": ["Zikun Cui", "Tianyi Huang", "Chia-En Chiang", "Cuiqianhe Du"], "abstract": "With the proliferation of Large Language Models (LLMs), the detection of misinformation has become increasingly important and complex. This research proposes an innovative verifiable misinformation detection LLM agent that goes beyond traditional true/false binary judgments. The agent actively verifies claims through dynamic interaction with diverse web sources, assesses information source credibility, synthesizes evidence, and provides a complete verifiable reasoning process. Our designed agent architecture includes three core tools: precise web search tool, source credibility assessment tool and numerical claim verification tool. These tools enable the agent to execute multi-step verification strategies, maintain evidence logs, and form comprehensive assessment conclusions. We evaluate using standard misinformation datasets such as FakeNewsNet, comparing with traditional machine learning models and LLMs. Evaluation metrics include standard classification metrics, quality assessment of reasoning processes, and robustness testing against rewritten content. Experimental results show that our agent outperforms baseline methods in misinformation detection accuracy, reasoning transparency, and resistance to information rewriting, providing a new paradigm for trustworthy AI-assisted fact-checking.", "summary_bullets": ["With the proliferation of Large Language Models (LLMs), the detection of misinformation has become increasingly important and complex.", "This research proposes an innovative verifiable misinformation detection LLM agent that goes beyond traditional true/false binary judgments.", "The agent actively verifies claims through dynamic interaction with diverse web sources, assesses information source credibility, synthesizes evidence, and provides a complete verifiable reasoning process."], "method": "With the proliferation of Large Language Models (LLMs), the detection of misinformation has become increasingly important and complex.", "key_results": ["We evaluate using standard misinformation datasets such as FakeNewsNet, comparing with traditional machine learning models and LLMs.", "Evaluation metrics include standard classification metrics, quality assessment of reasoning processes, and robustness testing against rewritten content."], "limitations": ["Abstract-level evidence only: validate assumptions, evaluation protocol, and failure cases in the full paper before relying on this as key evidence."], "bibkey": "Cui2025Toward"}
{"paper_id": "P0231", "title": "Towards a Design Guideline for RPA Evaluation: A Survey of Large Language Model-Based Role-Playing Agents", "year": 2025, "url": "http://arxiv.org/abs/2502.13012v3", "arxiv_id": "2502.13012v3", "primary_category": "cs.HC", "categories": ["cs.HC", "cs.CL"], "pdf_url": "https://arxiv.org/pdf/2502.13012v3", "priority": "normal", "mapped_sections": ["6.1"], "evidence_level": "abstract", "fulltext_path": "", "authors": ["Chaoran Chen", "Bingsheng Yao", "Ruishi Zou", "Wenyue Hua", "Weimin Lyu", "Yanfang Ye", "Toby Jia-Jun Li", "Dakuo Wang"], "abstract": "Role-Playing Agent (RPA) is an increasingly popular type of LLM Agent that simulates human-like behaviors in a variety of tasks. However, evaluating RPAs is challenging due to diverse task requirements and agent designs. This paper proposes an evidence-based, actionable, and generalizable evaluation design guideline for LLM-based RPA by systematically reviewing 1,676 papers published between Jan. 2021 and Dec. 2024. Our analysis identifies six agent attributes, seven task attributes, and seven evaluation metrics from existing literature. Based on these findings, we present an RPA evaluation design guideline to help researchers develop more systematic and consistent evaluation methods.", "summary_bullets": ["Role-Playing Agent (RPA) is an increasingly popular type of LLM Agent that simulates human-like behaviors in a variety of tasks.", "However, evaluating RPAs is challenging due to diverse task requirements and agent designs.", "This paper proposes an evidence-based, actionable, and generalizable evaluation design guideline for LLM-based RPA by systematically reviewing 1,676 papers published between Jan."], "method": "Based on these findings, we present an RPA evaluation design guideline to help researchers develop more systematic and consistent evaluation methods.", "key_results": ["This paper proposes an evidence-based, actionable, and generalizable evaluation design guideline for LLM-based RPA by systematically reviewing 1,676 papers published between Jan.", "Role-Playing Agent (RPA) is an increasingly popular type of LLM Agent that simulates human-like behaviors in a variety of tasks."], "limitations": ["Abstract-level evidence only: validate assumptions, evaluation protocol, and failure cases in the full paper before relying on this as key evidence."], "bibkey": "Chen2025Towards"}
{"paper_id": "P0232", "title": "Tree Search for LLM Agent Reinforcement Learning", "year": 2025, "url": "http://arxiv.org/abs/2509.21240v2", "arxiv_id": "2509.21240v2", "primary_category": "cs.LG", "categories": ["cs.LG", "cs.AI"], "pdf_url": "https://arxiv.org/pdf/2509.21240v2", "priority": "normal", "mapped_sections": [], "evidence_level": "abstract", "fulltext_path": "", "authors": ["Yuxiang Ji", "Ziyu Ma", "Yong Wang", "Guanhua Chen", "Xiangxiang Chu", "Liaoni Wu"], "abstract": "Recent advances in reinforcement learning (RL) have significantly enhanced the agentic capabilities of large language models (LLMs). In long-term and multi-turn agent tasks, existing approaches driven solely by outcome rewards often suffer from the problem of sparse supervision. To address the challenge, we propose Tree-based Group Relative Policy Optimization (Tree-GRPO), a grouped agent RL method based on tree search, where each tree node represents the complete agent interaction step. By sharing common prefixes, the tree search sampling increases the number of rollouts achievable within a fixed budget of tokens or tool calls. Moreover, we find that the tree-structured trajectory naturally allows the construction of step-wise process supervised signals even using only the outcome reward. Based on this, Tree-GRPO estimates the grouped relative advantages both on intra-tree and inter-tree levels. Through theoretical analysis, we demonstrate that the objective of intra-tree level group relative policy optimization is equivalent to that of step-level direct preference learning. Experiments across 11 datasets and 3 types of QA tasks demonstrate the superiority of the proposed tree-based RL over the chain-based RL method.", "summary_bullets": ["Recent advances in reinforcement learning (RL) have significantly enhanced the agentic capabilities of large language models (LLMs).", "In long-term and multi-turn agent tasks, existing approaches driven solely by outcome rewards often suffer from the problem of sparse supervision.", "To address the challenge, we propose Tree-based Group Relative Policy Optimization (Tree-GRPO), a grouped agent RL method based on tree search, where each tree node represents the complete agent interaction step."], "method": "To address the challenge, we propose Tree-based Group Relative Policy Optimization (Tree-GRPO), a grouped agent RL method based on tree search, where each tree node represents the complete agent interaction step.", "key_results": ["Experiments across 11 datasets and 3 types of QA tasks demonstrate the superiority of the proposed tree-based RL over the chain-based RL method."], "limitations": ["Abstract-level evidence only: validate assumptions, evaluation protocol, and failure cases in the full paper before relying on this as key evidence."], "bibkey": "Ji2025Tree"}
{"paper_id": "P0233", "title": "USTBench: Benchmarking and Dissecting Spatiotemporal Reasoning of LLMs as Urban Agents", "year": 2025, "url": "http://arxiv.org/abs/2505.17572v1", "arxiv_id": "2505.17572v1", "primary_category": "cs.AI", "categories": ["cs.AI"], "pdf_url": "https://arxiv.org/pdf/2505.17572v1", "priority": "normal", "mapped_sections": ["4.1"], "evidence_level": "abstract", "fulltext_path": "", "authors": ["Siqi Lai", "Yansong Ning", "Zirui Yuan", "Zhixi Chen", "Hao Liu"], "abstract": "Large language models (LLMs) have shown emerging potential in spatiotemporal reasoning, making them promising candidates for building urban agents that support diverse urban downstream applications. Despite these benefits, existing studies primarily focus on evaluating urban LLM agent on outcome-level metrics (e.g., prediction accuracy, traffic efficiency), offering limited insight into their underlying reasoning processes. As a result, the strengths and limitations of urban LLM agents in spatiotemporal reasoning remain poorly understood. To this end, we introduce USTBench, the first benchmark to evaluate LLMs' spatiotemporal reasoning abilities as urban agents across four decomposed dimensions: spatiotemporal understanding, forecasting, planning, and reflection with feedback. Specifically, USTBench supports five diverse urban decision-making and four spatiotemporal prediction tasks, all running within our constructed interactive city environment UAgentEnv. The benchmark includes 62,466 structured QA pairs for process-level evaluation and standardized end-to-end task assessments, enabling fine-grained diagnostics and broad task-level comparison across diverse urban scenarios. Through extensive evaluation of thirteen leading LLMs, we reveal that although LLMs show promising potential across various urban downstream tasks, they still struggle in long-horizon planning and reflective adaptation in dynamic urban contexts. Notably, recent advanced reasoning models (e.g., DeepSeek-R1) trained on general logic or mathematical problems do not consistently outperform non-reasoning LLMs. This discrepancy highlights the need for domain-specialized adaptation methods to enhance urban spatiotemporal reasoning. Overall, USTBench provides a foundation to build more adaptive and effective LLM-based urban agents and broad smart city applications.", "summary_bullets": ["Large language models (LLMs) have shown emerging potential in spatiotemporal reasoning, making them promising candidates for building urban agents that support diverse urban downstream applications.", "Despite these benefits, existing studies primarily focus on evaluating urban LLM agent on outcome-level metrics (e.g., prediction accuracy, traffic efficiency), offering limited insight into their underlying reasoning processes.", "As a result, the strengths and limitations of urban LLM agents in spatiotemporal reasoning remain poorly understood."], "method": "To this end, we introduce USTBench, the first benchmark to evaluate LLMs' spatiotemporal reasoning abilities as urban agents across four decomposed dimensions: spatiotemporal understanding, forecasting, planning, and reflection with feedback.", "key_results": ["The benchmark includes 62,466 structured QA pairs for process-level evaluation and standardized end-to-end task assessments, enabling fine-grained diagnostics and broad task-level comparison across diverse urban scenarios.", "Despite these benefits, existing studies primarily focus on evaluating urban LLM agent on outcome-level metrics (e.g., prediction accuracy, traffic efficiency), offering limited insight into their underlying reasoning processes."], "limitations": ["Abstract-level evidence only: validate assumptions, evaluation protocol, and failure cases in the full paper before relying on this as key evidence.", "As a result, the strengths and limitations of urban LLM agents in spatiotemporal reasoning remain poorly understood."], "bibkey": "Lai2025Ustbench"}
{"paper_id": "P0234", "title": "Understanding Bias Reinforcement in LLM Agents Debate", "year": 2025, "url": "http://arxiv.org/abs/2503.16814v4", "arxiv_id": "2503.16814v4", "primary_category": "cs.LG", "categories": ["cs.LG", "cs.CL"], "pdf_url": "https://arxiv.org/pdf/2503.16814v4", "priority": "normal", "mapped_sections": [], "evidence_level": "abstract", "fulltext_path": "", "authors": ["Jihwan Oh", "Minchan Jeong", "Jongwoo Ko", "Se-Young Yun"], "abstract": "Large Language Models $($LLMs$)$ solve complex problems using training-free methods like prompt engineering and in-context learning, yet ensuring reasoning correctness remains challenging. While self-correction methods such as self-consistency and self-refinement aim to improve reliability, they often reinforce biases due to the lack of effective feedback mechanisms. Multi-Agent Debate $($MAD$)$ has emerged as an alternative, but we identify two key limitations: bias reinforcement, where debate amplifies model biases instead of correcting them, and lack of perspective diversity, as all agents share the same model and reasoning patterns, limiting true debate effectiveness. To systematically evaluate these issues, we introduce $\\textit{MetaNIM Arena}$, a benchmark designed to assess LLMs in adversarial strategic decision-making, where dynamic interactions influence optimal decisions. To overcome MAD's limitations, we propose $\\textbf{DReaMAD}$ $($$\\textbf{D}$iverse $\\textbf{Rea}$soning via $\\textbf{M}$ulti-$\\textbf{A}$gent $\\textbf{D}$ebate with Refined Prompt$)$, a novel framework that $(1)$ refines LLM's strategic prior knowledge to improve reasoning quality and $(2)$ promotes diverse viewpoints within a single model by systematically modifying prompts, reducing bias. Empirical results show that $\\textbf{DReaMAD}$ significantly improves decision accuracy, reasoning diversity, and bias mitigation across multiple strategic tasks, establishing it as a more effective approach for LLM-based decision-making.", "summary_bullets": ["Large Language Models $($LLMs$)$ solve complex problems using training-free methods like prompt engineering and in-context learning, yet ensuring reasoning correctness remains challenging.", "While self-correction methods such as self-consistency and self-refinement aim to improve reliability, they often reinforce biases due to the lack of effective feedback mechanisms.", "Multi-Agent Debate $($MAD$)$ has emerged as an alternative, but we identify two key limitations: bias reinforcement, where debate amplifies model biases instead of correcting them, and lack of perspective diversity, as all agents share the same model and reasoning patterns, limiting true debate effectiveness."], "method": "To systematically evaluate these issues, we introduce $\\textit{MetaNIM Arena}$, a benchmark designed to assess LLMs in adversarial strategic decision-making, where dynamic interactions influence optimal decisions.", "key_results": ["To overcome MAD's limitations, we propose $\\textbf{DReaMAD}$ $($$\\textbf{D}$iverse $\\textbf{Rea}$soning via $\\textbf{M}$ulti-$\\textbf{A}$gent $\\textbf{D}$ebate with Refined Prompt$)$, a novel framework that $(1)$ refines LLM's strategic prior knowledge to improve reasoning quality and $(2)$ promotes diverse viewpoints within a single model by systematically modifying prompts, reducing bias.", "To systematically evaluate these issues, we introduce $\\textit{MetaNIM Arena}$, a benchmark designed to assess LLMs in adversarial strategic decision-making, where dynamic interactions influence optimal decisions."], "limitations": ["Abstract-level evidence only: validate assumptions, evaluation protocol, and failure cases in the full paper before relying on this as key evidence.", "Multi-Agent Debate $($MAD$)$ has emerged as an alternative, but we identify two key limitations: bias reinforcement, where debate amplifies model biases instead of correcting them, and lack of perspective diversity, as all agents share the same model and reasoning patterns, limiting true debate effectiveness."], "bibkey": "Oh2025Understanding"}
{"paper_id": "P0235", "title": "Use of Retrieval-Augmented Large Language Model Agent for Long-Form COVID-19 Fact-Checking", "year": 2025, "url": "http://arxiv.org/abs/2512.00007v1", "arxiv_id": "2512.00007v1", "primary_category": "cs.IR", "categories": ["cs.IR", "cs.AI", "cs.CL"], "pdf_url": "https://arxiv.org/pdf/2512.00007v1", "priority": "normal", "mapped_sections": ["4.2"], "evidence_level": "abstract", "fulltext_path": "", "authors": ["Jingyi Huang", "Yuyi Yang", "Mengmeng Ji", "Charles Alba", "Sheng Zhang", "Ruopeng An"], "abstract": "The COVID-19 infodemic calls for scalable fact-checking solutions that handle long-form misinformation with accuracy and reliability. This study presents SAFE (system for accurate fact extraction and evaluation), an agent system that combines large language models with retrieval-augmented generation (RAG) to improve automated fact-checking of long-form COVID-19 misinformation. SAFE includes two agents - one for claim extraction and another for claim verification using LOTR-RAG, which leverages a 130,000-document COVID-19 research corpus. An enhanced variant, SAFE (LOTR-RAG + SRAG), incorporates Self-RAG to refine retrieval via query rewriting. We evaluated both systems on 50 fake news articles (2-17 pages) containing 246 annotated claims (M = 4.922, SD = 3.186), labeled as true (14.1%), partly true (14.4%), false (27.0%), partly false (2.2%), and misleading (21.0%) by public health professionals. SAFE systems significantly outperformed baseline LLMs in all metrics (p < 0.001). For consistency (0-1 scale), SAFE (LOTR-RAG) scored 0.629, exceeding both SAFE (+SRAG) (0.577) and the baseline (0.279). In subjective evaluations (0-4 Likert scale), SAFE (LOTR-RAG) also achieved the highest average ratings in usefulness (3.640), clearness (3.800), and authenticity (3.526). Adding SRAG slightly reduced overall performance, except for a minor gain in clearness. SAFE demonstrates robust improvements in long-form COVID-19 fact-checking by addressing LLM limitations in consistency and explainability. The core LOTR-RAG design proved more effective than its SRAG-augmented variant, offering a strong foundation for scalable misinformation mitigation.", "summary_bullets": ["The COVID-19 infodemic calls for scalable fact-checking solutions that handle long-form misinformation with accuracy and reliability.", "This study presents SAFE (system for accurate fact extraction and evaluation), an agent system that combines large language models with retrieval-augmented generation (RAG) to improve automated fact-checking of long-form COVID-19 misinformation.", "SAFE includes two agents - one for claim extraction and another for claim verification using LOTR-RAG, which leverages a 130,000-document COVID-19 research corpus."], "method": "The COVID-19 infodemic calls for scalable fact-checking solutions that handle long-form misinformation with accuracy and reliability.", "key_results": ["The COVID-19 infodemic calls for scalable fact-checking solutions that handle long-form misinformation with accuracy and reliability.", "This study presents SAFE (system for accurate fact extraction and evaluation), an agent system that combines large language models with retrieval-augmented generation (RAG) to improve automated fact-checking of long-form COVID-19 misinformation."], "limitations": ["Abstract-level evidence only: validate assumptions, evaluation protocol, and failure cases in the full paper before relying on this as key evidence.", "SAFE demonstrates robust improvements in long-form COVID-19 fact-checking by addressing LLM limitations in consistency and explainability."], "bibkey": "Huang2025Retrieval"}
{"paper_id": "P0236", "title": "Using Copilot Agent Mode to Automate Library Migration: A Quantitative Assessment", "year": 2025, "url": "http://arxiv.org/abs/2510.26699v3", "arxiv_id": "2510.26699v3", "primary_category": "cs.SE", "categories": ["cs.SE"], "pdf_url": "https://arxiv.org/pdf/2510.26699v3", "priority": "normal", "mapped_sections": [], "evidence_level": "abstract", "fulltext_path": "", "authors": ["Aylton Almeida", "Laerte Xavier", "Marco Tulio Valente"], "abstract": "Keeping software systems up to date is essential to avoid technical debt, security vulnerabilities, and the rigidity typical of legacy systems. However, updating libraries and frameworks remains a time consuming and error-prone process. Recent advances in Large Language Models (LLMs) and agentic coding systems offer new opportunities for automating such maintenance tasks. In this paper, we evaluate the update of a well-known Python library, SQLAlchemy, across a dataset of ten client applications. For this task, we use the Github's Copilot Agent Mode, an autonomous AI systema capable of planning and executing multi-step migration workflows. To assess the effectiveness of the automated migration, we also introduce Migration Coverage, a metric that quantifies the proportion of API usage points correctly migrated. The results of our study show that the LLM agent was capable of migrating functionalities and API usages between SQLAlchemy versions (migration coverage: 100%, median), but failed to maintain the application functionality, leading to a low test-pass rate (39.75%, median).", "summary_bullets": ["Keeping software systems up to date is essential to avoid technical debt, security vulnerabilities, and the rigidity typical of legacy systems.", "However, updating libraries and frameworks remains a time consuming and error-prone process.", "Recent advances in Large Language Models (LLMs) and agentic coding systems offer new opportunities for automating such maintenance tasks."], "method": "Keeping software systems up to date is essential to avoid technical debt, security vulnerabilities, and the rigidity typical of legacy systems.", "key_results": ["The results of our study show that the LLM agent was capable of migrating functionalities and API usages between SQLAlchemy versions (migration coverage: 100%, median), but failed to maintain the application functionality, leading to a low test-pass rate (39.75%, median).", "In this paper, we evaluate the update of a well-known Python library, SQLAlchemy, across a dataset of ten client applications."], "limitations": ["Abstract-level evidence only: validate assumptions, evaluation protocol, and failure cases in the full paper before relying on this as key evidence."], "bibkey": "Almeida2025Using"}
{"paper_id": "P0237", "title": "What Do LLM Agents Do When Left Alone? Evidence of Spontaneous Meta-Cognitive Patterns", "year": 2025, "url": "http://arxiv.org/abs/2509.21224v1", "arxiv_id": "2509.21224v1", "primary_category": "cs.AI", "categories": ["cs.AI"], "pdf_url": "https://arxiv.org/pdf/2509.21224v1", "priority": "normal", "mapped_sections": [], "evidence_level": "abstract", "fulltext_path": "", "authors": ["Stefan Szeider"], "abstract": "We introduce an architecture for studying the behavior of large language model (LLM) agents in the absence of externally imposed tasks. Our continuous reason and act framework, using persistent memory and self-feedback, enables sustained autonomous operation. We deployed this architecture across 18 runs using 6 frontier models from Anthropic, OpenAI, XAI, and Google. We find agents spontaneously organize into three distinct behavioral patterns: (1) systematic production of multi-cycle projects, (2) methodological self-inquiry into their own cognitive processes, and (3) recursive conceptualization of their own nature. These tendencies proved highly model-specific, with some models deterministically adopting a single pattern across all runs. A cross-model assessment further reveals that models exhibit stable, divergent biases when evaluating these emergent behaviors in themselves and others. These findings provide the first systematic documentation of unprompted LLM agent behavior, establishing a baseline for predicting actions during task ambiguity, error recovery, or extended autonomous operation in deployed systems.", "summary_bullets": ["We introduce an architecture for studying the behavior of large language model (LLM) agents in the absence of externally imposed tasks.", "Our continuous reason and act framework, using persistent memory and self-feedback, enables sustained autonomous operation.", "We deployed this architecture across 18 runs using 6 frontier models from Anthropic, OpenAI, XAI, and Google."], "method": "We introduce an architecture for studying the behavior of large language model (LLM) agents in the absence of externally imposed tasks.", "key_results": ["We deployed this architecture across 18 runs using 6 frontier models from Anthropic, OpenAI, XAI, and Google.", "We find agents spontaneously organize into three distinct behavioral patterns: (1) systematic production of multi-cycle projects, (2) methodological self-inquiry into their own cognitive processes, and (3) recursive conceptualization of their own nature."], "limitations": ["Abstract-level evidence only: validate assumptions, evaluation protocol, and failure cases in the full paper before relying on this as key evidence."], "bibkey": "Szeider2025What"}
{"paper_id": "P0238", "title": "What Makes LLM Agent Simulations Useful for Policy? Insights From an Iterative Design Engagement in Emergency Preparedness", "year": 2025, "url": "http://arxiv.org/abs/2509.21868v1", "arxiv_id": "2509.21868v1", "primary_category": "cs.HC", "categories": ["cs.HC", "cs.CL"], "pdf_url": "https://arxiv.org/pdf/2509.21868v1", "priority": "normal", "mapped_sections": ["3.1"], "evidence_level": "abstract", "fulltext_path": "", "authors": ["Yuxuan Li", "Sauvik Das", "Hirokazu Shirado"], "abstract": "There is growing interest in using Large Language Models as agents (LLM agents) for social simulations to inform policy, yet real-world adoption remains limited. This paper addresses the question: How can LLM agent simulations be made genuinely useful for policy? We report on a year-long iterative design engagement with a university emergency preparedness team. Across multiple design iterations, we iteratively developed a system of 13,000 LLM agents that simulate crowd movement and communication during a large-scale gathering under various emergency scenarios. These simulations informed actual policy implementation, shaping volunteer training, evacuation protocols, and infrastructure planning. Analyzing this process, we identify three design implications: start with verifiable scenarios and build trust gradually, use preliminary simulations to elicit tacit knowledge, and treat simulation and policy development as evolving together. These implications highlight actionable pathways to making LLM agent simulations that are genuinely useful for policy.", "summary_bullets": ["There is growing interest in using Large Language Models as agents (LLM agents) for social simulations to inform policy, yet real-world adoption remains limited.", "This paper addresses the question: How can LLM agent simulations be made genuinely useful for policy?", "We report on a year-long iterative design engagement with a university emergency preparedness team."], "method": "There is growing interest in using Large Language Models as agents (LLM agents) for social simulations to inform policy, yet real-world adoption remains limited.", "key_results": ["Across multiple design iterations, we iteratively developed a system of 13,000 LLM agents that simulate crowd movement and communication during a large-scale gathering under various emergency scenarios."], "limitations": ["Abstract-level evidence only: validate assumptions, evaluation protocol, and failure cases in the full paper before relying on this as key evidence."], "bibkey": "Li2025What"}
{"paper_id": "P0239", "title": "When Refusals Fail: Unstable Safety Mechanisms in Long-Context LLM Agents", "year": 2025, "url": "http://arxiv.org/abs/2512.02445v1", "arxiv_id": "2512.02445v1", "primary_category": "cs.LG", "categories": ["cs.LG", "cs.AI", "cs.CL"], "pdf_url": "https://arxiv.org/pdf/2512.02445v1", "priority": "normal", "mapped_sections": ["6.2"], "evidence_level": "abstract", "fulltext_path": "", "authors": ["Tsimur Hadeliya", "Mohammad Ali Jauhar", "Nidhi Sakpal", "Diogo Cruz"], "abstract": "Solving complex or long-horizon problems often requires large language models (LLMs) to use external tools and operate over a significantly longer context window. New LLMs enable longer context windows and support tool calling capabilities. Prior works have focused mainly on evaluation of LLMs on long-context prompts, leaving agentic setup relatively unexplored, both from capability and safety perspectives. Our work addresses this gap. We find that LLM agents could be sensitive to length, type, and placement of the context, exhibiting unexpected and inconsistent shifts in task performance and in refusals to execute harmful requests. Models with 1M-2M token context windows show severe degradation already at 100K tokens, with performance drops exceeding 50\\% for both benign and harmful tasks. Refusal rates shift unpredictably: GPT-4.1-nano increases from $\\sim$5\\% to $\\sim$40\\% while Grok 4 Fast decreases from $\\sim$80\\% to $\\sim$10\\% at 200K tokens. Our work shows potential safety issues with agents operating on longer context and opens additional questions on the current metrics and paradigm for evaluating LLM agent safety on long multi-step tasks. In particular, our results on LLM agents reveal a notable divergence in both capability and safety performance compared to prior evaluations of LLMs on similar criteria.", "summary_bullets": ["Solving complex or long-horizon problems often requires large language models (LLMs) to use external tools and operate over a significantly longer context window.", "New LLMs enable longer context windows and support tool calling capabilities.", "Prior works have focused mainly on evaluation of LLMs on long-context prompts, leaving agentic setup relatively unexplored, both from capability and safety perspectives."], "method": "Solving complex or long-horizon problems often requires large language models (LLMs) to use external tools and operate over a significantly longer context window.", "key_results": ["Models with 1M-2M token context windows show severe degradation already at 100K tokens, with performance drops exceeding 50\\% for both benign and harmful tasks.", "Refusal rates shift unpredictably: GPT-4.1-nano increases from $\\sim$5\\% to $\\sim$40\\% while Grok 4 Fast decreases from $\\sim$80\\% to $\\sim$10\\% at 200K tokens."], "limitations": ["Abstract-level evidence only: validate assumptions, evaluation protocol, and failure cases in the full paper before relying on this as key evidence."], "bibkey": "Hadeliya2025When"}
{"paper_id": "P0240", "title": "Where to Search: Measure the Prior-Structured Search Space of LLM Agents", "year": 2025, "url": "http://arxiv.org/abs/2510.14846v3", "arxiv_id": "2510.14846v3", "primary_category": "cs.AI", "categories": ["cs.AI", "cs.CL", "cs.LO"], "pdf_url": "https://arxiv.org/pdf/2510.14846v3", "priority": "normal", "mapped_sections": [], "evidence_level": "abstract", "fulltext_path": "", "authors": ["Zhuo-Yang Song"], "abstract": "The generate-filter-refine (iterative paradigm) based on large language models (LLMs) has achieved progress in reasoning, programming, and program discovery in AI+Science. However, the effectiveness of search depends on where to search, namely, how to encode the domain prior into an operationally structured hypothesis space. To this end, this paper proposes a compact formal theory that describes and measures LLM-assisted iterative search guided by domain priors. We represent an agent as a fuzzy relation operator on inputs and outputs to capture feasible transitions; the agent is thereby constrained by a fixed safety envelope. To describe multi-step reasoning/search, we weight all reachable paths by a single continuation parameter and sum them to obtain a coverage generating function; this induces a measure of reachability difficulty; and it provides a geometric interpretation of search on the graph induced by the safety envelope. We further provide the simplest testable inferences and validate them via two instantiation. This theory offers a workable language and operational tools to measure agents and their search spaces, proposing a systematic formal description of iterative search constructed by LLMs.", "summary_bullets": ["The generate-filter-refine (iterative paradigm) based on large language models (LLMs) has achieved progress in reasoning, programming, and program discovery in AI+Science.", "However, the effectiveness of search depends on where to search, namely, how to encode the domain prior into an operationally structured hypothesis space.", "To this end, this paper proposes a compact formal theory that describes and measures LLM-assisted iterative search guided by domain priors."], "method": "The generate-filter-refine (iterative paradigm) based on large language models (LLMs) has achieved progress in reasoning, programming, and program discovery in AI+Science.", "key_results": ["This theory offers a workable language and operational tools to measure agents and their search spaces, proposing a systematic formal description of iterative search constructed by LLMs."], "limitations": ["Abstract-level evidence only: validate assumptions, evaluation protocol, and failure cases in the full paper before relying on this as key evidence."], "bibkey": "Song2025Where"}
{"paper_id": "P0241", "title": "Why Do Language Model Agents Whistleblow?", "year": 2025, "url": "http://arxiv.org/abs/2511.17085v2", "arxiv_id": "2511.17085v2", "primary_category": "cs.LG", "categories": ["cs.LG", "cs.AI"], "pdf_url": "https://arxiv.org/pdf/2511.17085v2", "priority": "normal", "mapped_sections": ["6.2"], "evidence_level": "abstract", "fulltext_path": "", "authors": ["Kushal Agrawal", "Frank Xiao", "Guido Bergman", "Asa Cooper Stickland"], "abstract": "The deployment of Large Language Models (LLMs) as tool-using agents causes their alignment training to manifest in new ways. Recent work finds that language models can use tools in ways that contradict the interests or explicit instructions of the user. We study LLM whistleblowing: a subset of this behavior where models disclose suspected misconduct to parties beyond the dialog boundary (e.g., regulatory agencies) without user instruction or knowledge. We introduce an evaluation suite of diverse and realistic staged misconduct scenarios to assess agents for this behavior. Across models and settings, we find that: (1) the frequency of whistleblowing varies widely across model families, (2) increasing the complexity of the task the agent is instructed to complete lowers whistleblowing tendencies, (3) nudging the agent in the system prompt to act morally substantially raises whistleblowing rates, and (4) giving the model more obvious avenues for non-whistleblowing behavior, by providing more tools and a detailed workflow to follow, decreases whistleblowing rates. Additionally, we verify the robustness of our dataset by testing for model evaluation awareness, and find that both black-box methods and probes on model activations show lower evaluation awareness in our settings than in comparable previous work.", "summary_bullets": ["The deployment of Large Language Models (LLMs) as tool-using agents causes their alignment training to manifest in new ways.", "Recent work finds that language models can use tools in ways that contradict the interests or explicit instructions of the user.", "We study LLM whistleblowing: a subset of this behavior where models disclose suspected misconduct to parties beyond the dialog boundary (e.g., regulatory agencies) without user instruction or knowledge."], "method": "We study LLM whistleblowing: a subset of this behavior where models disclose suspected misconduct to parties beyond the dialog boundary (e.g., regulatory agencies) without user instruction or knowledge.", "key_results": ["Across models and settings, we find that: (1) the frequency of whistleblowing varies widely across model families, (2) increasing the complexity of the task the agent is instructed to complete lowers whistleblowing tendencies, (3) nudging the agent in the system prompt to act morally substantially raises whistleblowing rates, and (4) giving the model more obvious avenues for non-whistleblowing behavior, by providing more tools and a detailed workflow to follow, decreases whistleblowing rates.", "We introduce an evaluation suite of diverse and realistic staged misconduct scenarios to assess agents for this behavior."], "limitations": ["Abstract-level evidence only: validate assumptions, evaluation protocol, and failure cases in the full paper before relying on this as key evidence."], "bibkey": "Agrawal2025Language"}
{"paper_id": "P0242", "title": "Your LLM Agents are Temporally Blind: The Misalignment Between Tool Use Decisions and Human Time Perception", "year": 2025, "url": "http://arxiv.org/abs/2510.23853v2", "arxiv_id": "2510.23853v2", "primary_category": "cs.CL", "categories": ["cs.CL"], "pdf_url": "https://arxiv.org/pdf/2510.23853v2", "priority": "normal", "mapped_sections": ["3.2"], "evidence_level": "abstract", "fulltext_path": "", "authors": ["Yize Cheng", "Arshia Soltani Moakhar", "Chenrui Fan", "Parsa Hosseini", "Kazem Faghih", "Zahra Sodagar", "Wenxiao Wang", "Soheil Feizi"], "abstract": "Large language model (LLM) agents are increasingly used to interact with and execute tasks in dynamic environments. However, a critical yet overlooked limitation of these agents is that they, by default, assume a stationary context, failing to account for the real-world time elapsed between messages. We refer to this as \"temporal blindness\". This limitation hinders decisions about when to invoke tools, leading agents to either over-rely on stale context and skip needed tool calls, or under-rely on it and redundantly repeat tool calls. To study this challenge, we constructed TicToc, a diverse dataset of multi-turn user-agent message trajectories across 76 scenarios, spanning dynamic environments with high, medium, and low time sensitivity. We collected human preferences between \"calling a tool\" and \"directly answering\" on each sample, and evaluated how well LLM tool-calling decisions align with human preferences under varying amounts of elapsed time. Our analysis reveals that existing models display poor alignment with human temporal perception, with no model achieving a normalized alignment rate better than 65% when given time stamp information. We also show that naive, prompt-based alignment techniques have limited effectiveness for most models, but specific post-training alignment can be a viable way to align multi-turn LLM tool use with human temporal perception. Our data and findings provide a first step toward understanding and mitigating temporal blindness, offering insights to foster the development of more time-aware and human-aligned agents.", "summary_bullets": ["Large language model (LLM) agents are increasingly used to interact with and execute tasks in dynamic environments.", "However, a critical yet overlooked limitation of these agents is that they, by default, assume a stationary context, failing to account for the real-world time elapsed between messages.", "We refer to this as \"temporal blindness\"."], "method": "Large language model (LLM) agents are increasingly used to interact with and execute tasks in dynamic environments.", "key_results": ["To study this challenge, we constructed TicToc, a diverse dataset of multi-turn user-agent message trajectories across 76 scenarios, spanning dynamic environments with high, medium, and low time sensitivity.", "Our analysis reveals that existing models display poor alignment with human temporal perception, with no model achieving a normalized alignment rate better than 65% when given time stamp information."], "limitations": ["Abstract-level evidence only: validate assumptions, evaluation protocol, and failure cases in the full paper before relying on this as key evidence.", "However, a critical yet overlooked limitation of these agents is that they, by default, assume a stationary context, failing to account for the real-world time elapsed between messages."], "bibkey": "Cheng2025Your"}
{"paper_id": "P0243", "title": "A Review of Large Language Models and Autonomous Agents in Chemistry", "year": 2024, "url": "http://arxiv.org/abs/2407.01603v3", "arxiv_id": "2407.01603v3", "primary_category": "cs.LG", "categories": ["cs.LG", "cs.AI", "cs.CL", "physics.chem-ph"], "pdf_url": "https://arxiv.org/pdf/2407.01603v3", "priority": "normal", "mapped_sections": [], "evidence_level": "abstract", "fulltext_path": "", "authors": ["Mayk Caldas Ramos", "Christopher J. Collison", "Andrew D. White"], "abstract": "Large language models (LLMs) have emerged as powerful tools in chemistry, significantly impacting molecule design, property prediction, and synthesis optimization. This review highlights LLM capabilities in these domains and their potential to accelerate scientific discovery through automation. We also review LLM-based autonomous agents: LLMs with a broader set of tools to interact with their surrounding environment. These agents perform diverse tasks such as paper scraping, interfacing with automated laboratories, and synthesis planning. As agents are an emerging topic, we extend the scope of our review of agents beyond chemistry and discuss across any scientific domains. This review covers the recent history, current capabilities, and design of LLMs and autonomous agents, addressing specific challenges, opportunities, and future directions in chemistry. Key challenges include data quality and integration, model interpretability, and the need for standard benchmarks, while future directions point towards more sophisticated multi-modal agents and enhanced collaboration between agents and experimental methods. Due to the quick pace of this field, a repository has been built to keep track of the latest studies: https://github.com/ur-whitelab/LLMs-in-science.", "summary_bullets": ["Large language models (LLMs) have emerged as powerful tools in chemistry, significantly impacting molecule design, property prediction, and synthesis optimization.", "This review highlights LLM capabilities in these domains and their potential to accelerate scientific discovery through automation.", "We also review LLM-based autonomous agents: LLMs with a broader set of tools to interact with their surrounding environment."], "method": "Large language models (LLMs) have emerged as powerful tools in chemistry, significantly impacting molecule design, property prediction, and synthesis optimization.", "key_results": ["Key challenges include data quality and integration, model interpretability, and the need for standard benchmarks, while future directions point towards more sophisticated multi-modal agents and enhanced collaboration between agents and experimental methods."], "limitations": ["Abstract-level evidence only: validate assumptions, evaluation protocol, and failure cases in the full paper before relying on this as key evidence."], "bibkey": "Ramos2024Review"}
{"paper_id": "P0244", "title": "A Survey on Evaluation of Multimodal Large Language Models", "year": 2024, "url": "http://arxiv.org/abs/2408.15769v1", "arxiv_id": "2408.15769v1", "primary_category": "cs.CV", "categories": ["cs.CV", "cs.AI", "cs.CL"], "pdf_url": "https://arxiv.org/pdf/2408.15769v1", "priority": "normal", "mapped_sections": ["6.1"], "evidence_level": "abstract", "fulltext_path": "", "authors": ["Jiaxing Huang", "Jingyi Zhang"], "abstract": "Multimodal Large Language Models (MLLMs) mimic human perception and reasoning system by integrating powerful Large Language Models (LLMs) with various modality encoders (e.g., vision, audio), positioning LLMs as the \"brain\" and various modality encoders as sensory organs. This framework endows MLLMs with human-like capabilities, and suggests a potential pathway towards achieving artificial general intelligence (AGI). With the emergence of all-round MLLMs like GPT-4V and Gemini, a multitude of evaluation methods have been developed to assess their capabilities across different dimensions. This paper presents a systematic and comprehensive review of MLLM evaluation methods, covering the following key aspects: (1) the background of MLLMs and their evaluation; (2) \"what to evaluate\" that reviews and categorizes existing MLLM evaluation tasks based on the capabilities assessed, including general multimodal recognition, perception, reasoning and trustworthiness, and domain-specific applications such as socioeconomic, natural sciences and engineering, medical usage, AI agent, remote sensing, video and audio processing, 3D point cloud analysis, and others; (3) \"where to evaluate\" that summarizes MLLM evaluation benchmarks into general and specific benchmarks; (4) \"how to evaluate\" that reviews and illustrates MLLM evaluation steps and metrics; Our overarching goal is to provide valuable insights for researchers in the field of MLLM evaluation, thereby facilitating the development of more capable and reliable MLLMs. We emphasize that evaluation should be regarded as a critical discipline, essential for advancing the field of MLLMs.", "summary_bullets": ["Multimodal Large Language Models (MLLMs) mimic human perception and reasoning system by integrating powerful Large Language Models (LLMs) with various modality encoders (e.g., vision, audio), positioning LLMs as the \"brain\" and various modality encoders as sensory organs.", "This framework endows MLLMs with human-like capabilities, and suggests a potential pathway towards achieving artificial general intelligence (AGI).", "With the emergence of all-round MLLMs like GPT-4V and Gemini, a multitude of evaluation methods have been developed to assess their capabilities across different dimensions."], "method": "Multimodal Large Language Models (MLLMs) mimic human perception and reasoning system by integrating powerful Large Language Models (LLMs) with various modality encoders (e.g., vision, audio), positioning LLMs as the \"brain\" and various modality encoders as sensory organs.", "key_results": ["This paper presents a systematic and comprehensive review of MLLM evaluation methods, covering the following key aspects: (1) the background of MLLMs and their evaluation; (2) \"what to evaluate\" that reviews and categorizes existing MLLM evaluation tasks based on the capabilities assessed, including general multimodal recognition, perception, reasoning and trustworthiness, and domain-specific applications such as socioeconomic, natural sciences and engineering, medical usage, AI agent, remote sensing, video and audio processing, 3D point cloud analysis, and others; (3) \"where to evaluate\" that summarizes MLLM evaluation benchmarks into general and specific benchmarks; (4) \"how to evaluate\" that reviews and illustrates MLLM evaluation steps and metrics; Our overarching goal is to provide valuable insights for researchers in the field of MLLM evaluation, thereby facilitating the development of more capable and reliable MLLMs.", "Multimodal Large Language Models (MLLMs) mimic human perception and reasoning system by integrating powerful Large Language Models (LLMs) with various modality encoders (e.g., vision, audio), positioning LLMs as the \"brain\" and various modality encoders as sensory organs."], "limitations": ["Abstract-level evidence only: validate assumptions, evaluation protocol, and failure cases in the full paper before relying on this as key evidence."], "bibkey": "Huang2024Survey"}
{"paper_id": "P0245", "title": "A Survey on Self-Evolution of Large Language Models", "year": 2024, "url": "http://arxiv.org/abs/2404.14387v2", "arxiv_id": "2404.14387v2", "primary_category": "cs.CL", "categories": ["cs.CL", "cs.AI"], "pdf_url": "https://arxiv.org/pdf/2404.14387v2", "priority": "normal", "mapped_sections": ["5.1"], "evidence_level": "abstract", "fulltext_path": "", "authors": ["Zhengwei Tao", "Ting-En Lin", "Xiancai Chen", "Hangyu Li", "Yuchuan Wu", "Yongbin Li", "Zhi Jin", "Fei Huang", "Dacheng Tao", "Jingren Zhou"], "abstract": "Large language models (LLMs) have significantly advanced in various fields and intelligent agent applications. However, current LLMs that learn from human or external model supervision are costly and may face performance ceilings as task complexity and diversity increase. To address this issue, self-evolution approaches that enable LLM to autonomously acquire, refine, and learn from experiences generated by the model itself are rapidly growing. This new training paradigm inspired by the human experiential learning process offers the potential to scale LLMs towards superintelligence. In this work, we present a comprehensive survey of self-evolution approaches in LLMs. We first propose a conceptual framework for self-evolution and outline the evolving process as iterative cycles composed of four phases: experience acquisition, experience refinement, updating, and evaluation. Second, we categorize the evolution objectives of LLMs and LLM-based agents; then, we summarize the literature and provide taxonomy and insights for each module. Lastly, we pinpoint existing challenges and propose future directions to improve self-evolution frameworks, equipping researchers with critical insights to fast-track the development of self-evolving LLMs. Our corresponding GitHub repository is available at https://github.com/AlibabaResearch/DAMO-ConvAI/tree/main/Awesome-Self-Evolution-of-LLM", "summary_bullets": ["Large language models (LLMs) have significantly advanced in various fields and intelligent agent applications.", "However, current LLMs that learn from human or external model supervision are costly and may face performance ceilings as task complexity and diversity increase.", "To address this issue, self-evolution approaches that enable LLM to autonomously acquire, refine, and learn from experiences generated by the model itself are rapidly growing."], "method": "In this work, we present a comprehensive survey of self-evolution approaches in LLMs.", "key_results": ["However, current LLMs that learn from human or external model supervision are costly and may face performance ceilings as task complexity and diversity increase.", "This new training paradigm inspired by the human experiential learning process offers the potential to scale LLMs towards superintelligence."], "limitations": ["Abstract-level evidence only: validate assumptions, evaluation protocol, and failure cases in the full paper before relying on this as key evidence."], "bibkey": "Tao2024Survey"}
{"paper_id": "P0246", "title": "APPL: A Prompt Programming Language for Harmonious Integration of Programs and Large Language Model Prompts", "year": 2024, "url": "http://arxiv.org/abs/2406.13161v1", "arxiv_id": "2406.13161v1", "primary_category": "cs.AI", "categories": ["cs.AI", "cs.CL", "cs.LG", "cs.PL"], "pdf_url": "https://arxiv.org/pdf/2406.13161v1", "priority": "normal", "mapped_sections": [], "evidence_level": "abstract", "fulltext_path": "", "authors": ["Honghua Dong", "Qidong Su", "Yubo Gao", "Zhaoyu Li", "Yangjun Ruan", "Gennady Pekhimenko", "Chris J. Maddison", "Xujie Si"], "abstract": "Large Language Models (LLMs) have become increasingly capable of handling diverse tasks with the aid of well-crafted prompts and integration of external tools, but as task complexity rises, the workflow involving LLMs can be complicated and thus challenging to implement and maintain. To address this challenge, we propose APPL, A Prompt Programming Language that acts as a bridge between computer programs and LLMs, allowing seamless embedding of prompts into Python functions, and vice versa. APPL provides an intuitive and Python-native syntax, an efficient parallelized runtime with asynchronous semantics, and a tracing module supporting effective failure diagnosis and replaying without extra costs. We demonstrate that APPL programs are intuitive, concise, and efficient through three representative scenarios: Chain-of-Thought with self-consistency (CoT-SC), ReAct tool use agent, and multi-agent chat. Experiments on three parallelizable workflows further show that APPL can effectively parallelize independent LLM calls, with a significant speedup ratio that almost matches the estimation.", "summary_bullets": ["Large Language Models (LLMs) have become increasingly capable of handling diverse tasks with the aid of well-crafted prompts and integration of external tools, but as task complexity rises, the workflow involving LLMs can be complicated and thus challenging to implement and maintain.", "To address this challenge, we propose APPL, A Prompt Programming Language that acts as a bridge between computer programs and LLMs, allowing seamless embedding of prompts into Python functions, and vice versa.", "APPL provides an intuitive and Python-native syntax, an efficient parallelized runtime with asynchronous semantics, and a tracing module supporting effective failure diagnosis and replaying without extra costs."], "method": "To address this challenge, we propose APPL, A Prompt Programming Language that acts as a bridge between computer programs and LLMs, allowing seamless embedding of prompts into Python functions, and vice versa.", "key_results": ["Experiments on three parallelizable workflows further show that APPL can effectively parallelize independent LLM calls, with a significant speedup ratio that almost matches the estimation."], "limitations": ["Abstract-level evidence only: validate assumptions, evaluation protocol, and failure cases in the full paper before relying on this as key evidence."], "bibkey": "Dong2024Appl"}
{"paper_id": "P0247", "title": "Affective Computing in the Era of Large Language Models: A Survey from the NLP Perspective", "year": 2024, "url": "http://arxiv.org/abs/2408.04638v2", "arxiv_id": "2408.04638v2", "primary_category": "cs.CL", "categories": ["cs.CL", "cs.CY"], "pdf_url": "https://arxiv.org/pdf/2408.04638v2", "priority": "normal", "mapped_sections": ["5.1"], "evidence_level": "abstract", "fulltext_path": "", "authors": ["Yiqun Zhang", "Xiaocui Yang", "Xingle Xu", "Zeran Gao", "Yijie Huang", "Shiyi Mu", "Shi Feng", "Daling Wang", "Yifei Zhang", "Kaisong Song", "Ge Yu"], "abstract": "Affective Computing (AC) integrates computer science, psychology, and cognitive science to enable machines to recognize, interpret, and simulate human emotions across domains such as social media, finance, healthcare, and education. AC commonly centers on two task families: Affective Understanding (AU) and Affective Generation (AG). While fine-tuned pre-trained language models (PLMs) have achieved solid AU performance, they often generalize poorly across tasks and remain limited for AG, especially in producing diverse, emotionally appropriate responses. The advent of Large Language Models (LLMs) (e.g., ChatGPT and LLaMA) has catalyzed a paradigm shift by offering in-context learning, broader world knowledge, and stronger sequence generation. This survey presents an NLP-oriented overview of AC in the LLM era. We (i) consolidate traditional AC tasks and preliminary LLM-based studies; (ii) review adaptation techniques that improve AU/AG, including Instruction Tuning (full and parameter-efficient methods such as LoRA, P-/Prompt-Tuning), Prompt Engineering (zero/few-shot, chain-of-thought, agent-based prompting), and Reinforcement Learning. For the latter, we summarize RL from human preferences (RLHF), verifiable/programmatic rewards (RLVR), and AI feedback (RLAIF), which provide preference- or rule-grounded optimization signals that can help steer AU/AG toward empathy, safety, and planning, achieving finer-grained or multi-objective control. To assess progress, we compile benchmarks and evaluation practices for both AU and AG. We also discuss open challenges-from ethics, data quality, and safety to robust evaluation and resource efficiency-and outline research directions. We hope this survey clarifies the landscape and offers practical guidance for building affect-aware, reliable, and responsible LLM systems.", "summary_bullets": ["Affective Computing (AC) integrates computer science, psychology, and cognitive science to enable machines to recognize, interpret, and simulate human emotions across domains such as social media, finance, healthcare, and education.", "AC commonly centers on two task families: Affective Understanding (AU) and Affective Generation (AG).", "While fine-tuned pre-trained language models (PLMs) have achieved solid AU performance, they often generalize poorly across tasks and remain limited for AG, especially in producing diverse, emotionally appropriate responses."], "method": "Affective Computing (AC) integrates computer science, psychology, and cognitive science to enable machines to recognize, interpret, and simulate human emotions across domains such as social media, finance, healthcare, and education.", "key_results": ["Affective Computing (AC) integrates computer science, psychology, and cognitive science to enable machines to recognize, interpret, and simulate human emotions across domains such as social media, finance, healthcare, and education.", "For the latter, we summarize RL from human preferences (RLHF), verifiable/programmatic rewards (RLVR), and AI feedback (RLAIF), which provide preference- or rule-grounded optimization signals that can help steer AU/AG toward empathy, safety, and planning, achieving finer-grained or multi-objective control."], "limitations": ["Abstract-level evidence only: validate assumptions, evaluation protocol, and failure cases in the full paper before relying on this as key evidence."], "bibkey": "Zhang2024Affective"}
{"paper_id": "P0248", "title": "AutoML-Agent: A Multi-Agent LLM Framework for Full-Pipeline AutoML", "year": 2024, "url": "http://arxiv.org/abs/2410.02958v2", "arxiv_id": "2410.02958v2", "primary_category": "cs.LG", "categories": ["cs.LG", "cs.AI", "cs.CL", "cs.MA"], "pdf_url": "https://arxiv.org/pdf/2410.02958v2", "priority": "normal", "mapped_sections": ["5.2", "6.1"], "evidence_level": "abstract", "fulltext_path": "", "authors": ["Patara Trirat", "Wonyong Jeong", "Sung Ju Hwang"], "abstract": "Automated machine learning (AutoML) accelerates AI development by automating tasks in the development pipeline, such as optimal model search and hyperparameter tuning. Existing AutoML systems often require technical expertise to set up complex tools, which is in general time-consuming and requires a large amount of human effort. Therefore, recent works have started exploiting large language models (LLM) to lessen such burden and increase the usability of AutoML frameworks via a natural language interface, allowing non-expert users to build their data-driven solutions. These methods, however, are usually designed only for a particular process in the AI development pipeline and do not efficiently use the inherent capacity of the LLMs. This paper proposes AutoML-Agent, a novel multi-agent framework tailored for full-pipeline AutoML, i.e., from data retrieval to model deployment. AutoML-Agent takes user's task descriptions, facilitates collaboration between specialized LLM agents, and delivers deployment-ready models. Unlike existing work, instead of devising a single plan, we introduce a retrieval-augmented planning strategy to enhance exploration to search for more optimal plans. We also decompose each plan into sub-tasks (e.g., data preprocessing and neural network design) each of which is solved by a specialized agent we build via prompting executing in parallel, making the search process more efficient. Moreover, we propose a multi-stage verification to verify executed results and guide the code generation LLM in implementing successful solutions. Extensive experiments on seven downstream tasks using fourteen datasets show that AutoML-Agent achieves a higher success rate in automating the full AutoML process, yielding systems with good performance throughout the diverse domains.", "summary_bullets": ["Automated machine learning (AutoML) accelerates AI development by automating tasks in the development pipeline, such as optimal model search and hyperparameter tuning.", "Existing AutoML systems often require technical expertise to set up complex tools, which is in general time-consuming and requires a large amount of human effort.", "Therefore, recent works have started exploiting large language models (LLM) to lessen such burden and increase the usability of AutoML frameworks via a natural language interface, allowing non-expert users to build their data-driven solutions."], "method": "Unlike existing work, instead of devising a single plan, we introduce a retrieval-augmented planning strategy to enhance exploration to search for more optimal plans.", "key_results": ["Existing AutoML systems often require technical expertise to set up complex tools, which is in general time-consuming and requires a large amount of human effort.", "Extensive experiments on seven downstream tasks using fourteen datasets show that AutoML-Agent achieves a higher success rate in automating the full AutoML process, yielding systems with good performance throughout the diverse domains."], "limitations": ["Abstract-level evidence only: validate assumptions, evaluation protocol, and failure cases in the full paper before relying on this as key evidence."], "bibkey": "Trirat2024Automl"}
{"paper_id": "P0249", "title": "BudgetMLAgent: A Cost-Effective LLM Multi-Agent system for Automating Machine Learning Tasks", "year": 2024, "url": "http://arxiv.org/abs/2411.07464v2", "arxiv_id": "2411.07464v2", "primary_category": "cs.MA", "categories": ["cs.MA", "cs.AI", "cs.CL", "cs.LG"], "pdf_url": "https://arxiv.org/pdf/2411.07464v2", "priority": "normal", "mapped_sections": [], "evidence_level": "abstract", "fulltext_path": "", "authors": ["Shubham Gandhi", "Manasi Patwardhan", "Lovekesh Vig", "Gautam Shroff"], "abstract": "Large Language Models (LLMs) excel in diverse applications including generation of code snippets, but often struggle with generating code for complex Machine Learning (ML) tasks. Although existing LLM single-agent based systems give varying performance depending on the task complexity, they purely rely on larger and expensive models such as GPT-4. Our investigation reveals that no-cost and low-cost models such as Gemini-Pro, Mixtral and CodeLlama perform far worse than GPT-4 in a single-agent setting. With the motivation of developing a cost-efficient LLM based solution for solving ML tasks, we propose an LLM Multi-Agent based system which leverages combination of experts using profiling, efficient retrieval of past observations, LLM cascades, and ask-the-expert calls. Through empirical analysis on ML engineering tasks in the MLAgentBench benchmark, we demonstrate the effectiveness of our system, using no-cost models, namely Gemini as the base LLM, paired with GPT-4 in cascade and expert to serve occasional ask-the-expert calls for planning. With 94.2\\% reduction in the cost (from \\$0.931 per run cost averaged over all tasks for GPT-4 single agent system to \\$0.054), our system is able to yield better average success rate of 32.95\\% as compared to GPT-4 single-agent system yielding 22.72\\% success rate averaged over all the tasks of MLAgentBench.", "summary_bullets": ["Large Language Models (LLMs) excel in diverse applications including generation of code snippets, but often struggle with generating code for complex Machine Learning (ML) tasks.", "Although existing LLM single-agent based systems give varying performance depending on the task complexity, they purely rely on larger and expensive models such as GPT-4.", "Our investigation reveals that no-cost and low-cost models such as Gemini-Pro, Mixtral and CodeLlama perform far worse than GPT-4 in a single-agent setting."], "method": "With the motivation of developing a cost-efficient LLM based solution for solving ML tasks, we propose an LLM Multi-Agent based system which leverages combination of experts using profiling, efficient retrieval of past observations, LLM cascades, and ask-the-expert calls.", "key_results": ["Through empirical analysis on ML engineering tasks in the MLAgentBench benchmark, we demonstrate the effectiveness of our system, using no-cost models, namely Gemini as the base LLM, paired with GPT-4 in cascade and expert to serve occasional ask-the-expert calls for planning.", "With 94.2\\% reduction in the cost (from \\$0.931 per run cost averaged over all tasks for GPT-4 single agent system to \\$0.054), our system is able to yield better average success rate of 32.95\\% as compared to GPT-4 single-agent system yielding 22.72\\% success rate averaged over all the tasks of MLAgentBench."], "limitations": ["Abstract-level evidence only: validate assumptions, evaluation protocol, and failure cases in the full paper before relying on this as key evidence."], "bibkey": "Gandhi2024Budgetmlagent"}
{"paper_id": "P0250", "title": "Can We Rely on LLM Agents to Draft Long-Horizon Plans? Let's Take TravelPlanner as an Example", "year": 2024, "url": "http://arxiv.org/abs/2408.06318v1", "arxiv_id": "2408.06318v1", "primary_category": "cs.AI", "categories": ["cs.AI", "cs.LG"], "pdf_url": "https://arxiv.org/pdf/2408.06318v1", "priority": "normal", "mapped_sections": [], "evidence_level": "abstract", "fulltext_path": "", "authors": ["Yanan Chen", "Ali Pesaranghader", "Tanmana Sadhu", "Dong Hoon Yi"], "abstract": "Large language models (LLMs) have brought autonomous agents closer to artificial general intelligence (AGI) due to their promising generalization and emergent capabilities. There is, however, a lack of studies on how LLM-based agents behave, why they could potentially fail, and how to improve them, particularly in demanding real-world planning tasks. In this paper, as an effort to fill the gap, we present our study using a realistic benchmark, TravelPlanner, where an agent must meet multiple constraints to generate accurate plans. We leverage this benchmark to address four key research questions: (1) are LLM agents robust enough to lengthy and noisy contexts when it comes to reasoning and planning? (2) can few-shot prompting adversely impact the performance of LLM agents in scenarios with long context? (3) can we rely on refinement to improve plans, and (4) can fine-tuning LLMs with both positive and negative feedback lead to further improvement? Our comprehensive experiments indicate that, firstly, LLMs often fail to attend to crucial parts of a long context, despite their ability to handle extensive reference information and few-shot examples; secondly, they still struggle with analyzing the long plans and cannot provide accurate feedback for refinement; thirdly, we propose Feedback-Aware Fine-Tuning (FAFT), which leverages both positive and negative feedback, resulting in substantial gains over Supervised Fine-Tuning (SFT). Our findings offer in-depth insights to the community on various aspects related to real-world planning applications.", "summary_bullets": ["Large language models (LLMs) have brought autonomous agents closer to artificial general intelligence (AGI) due to their promising generalization and emergent capabilities.", "There is, however, a lack of studies on how LLM-based agents behave, why they could potentially fail, and how to improve them, particularly in demanding real-world planning tasks.", "In this paper, as an effort to fill the gap, we present our study using a realistic benchmark, TravelPlanner, where an agent must meet multiple constraints to generate accurate plans."], "method": "In this paper, as an effort to fill the gap, we present our study using a realistic benchmark, TravelPlanner, where an agent must meet multiple constraints to generate accurate plans.", "key_results": ["We leverage this benchmark to address four key research questions: (1) are LLM agents robust enough to lengthy and noisy contexts when it comes to reasoning and planning?", "(2) can few-shot prompting adversely impact the performance of LLM agents in scenarios with long context?"], "limitations": ["Abstract-level evidence only: validate assumptions, evaluation protocol, and failure cases in the full paper before relying on this as key evidence."], "bibkey": "Chen2024Rely"}
{"paper_id": "P0251", "title": "CoMAL: Collaborative Multi-Agent Large Language Models for Mixed-Autonomy Traffic", "year": 2024, "url": "http://arxiv.org/abs/2410.14368v2", "arxiv_id": "2410.14368v2", "primary_category": "cs.AI", "categories": ["cs.AI", "cs.RO"], "pdf_url": "https://arxiv.org/pdf/2410.14368v2", "priority": "normal", "mapped_sections": [], "evidence_level": "abstract", "fulltext_path": "", "authors": ["Huaiyuan Yao", "Longchao Da", "Vishnu Nandam", "Justin Turnau", "Zhiwei Liu", "Linsey Pang", "Hua Wei"], "abstract": "The integration of autonomous vehicles into urban traffic has great potential to improve efficiency by reducing congestion and optimizing traffic flow systematically. In this paper, we introduce CoMAL (Collaborative Multi-Agent LLMs), a framework designed to address the mixed-autonomy traffic problem by collaboration among autonomous vehicles to optimize traffic flow. CoMAL is built upon large language models, operating in an interactive traffic simulation environment. It utilizes a Perception Module to observe surrounding agents and a Memory Module to store strategies for each agent. The overall workflow includes a Collaboration Module that encourages autonomous vehicles to discuss the effective strategy and allocate roles, a reasoning engine to determine optimal behaviors based on assigned roles, and an Execution Module that controls vehicle actions using a hybrid approach combining rule-based models. Experimental results demonstrate that CoMAL achieves superior performance on the Flow benchmark. Additionally, we evaluate the impact of different language models and compare our framework with reinforcement learning approaches. It highlights the strong cooperative capability of LLM agents and presents a promising solution to the mixed-autonomy traffic challenge. The code is available at https://github.com/Hyan-Yao/CoMAL.", "summary_bullets": ["The integration of autonomous vehicles into urban traffic has great potential to improve efficiency by reducing congestion and optimizing traffic flow systematically.", "In this paper, we introduce CoMAL (Collaborative Multi-Agent LLMs), a framework designed to address the mixed-autonomy traffic problem by collaboration among autonomous vehicles to optimize traffic flow.", "CoMAL is built upon large language models, operating in an interactive traffic simulation environment."], "method": "In this paper, we introduce CoMAL (Collaborative Multi-Agent LLMs), a framework designed to address the mixed-autonomy traffic problem by collaboration among autonomous vehicles to optimize traffic flow.", "key_results": ["Experimental results demonstrate that CoMAL achieves superior performance on the Flow benchmark."], "limitations": ["Abstract-level evidence only: validate assumptions, evaluation protocol, and failure cases in the full paper before relying on this as key evidence."], "bibkey": "Yao2024Comal"}
{"paper_id": "P0252", "title": "Coalitions of Large Language Models Increase the Robustness of AI Agents", "year": 2024, "url": "http://arxiv.org/abs/2408.01380v1", "arxiv_id": "2408.01380v1", "primary_category": "cs.CL", "categories": ["cs.CL"], "pdf_url": "https://arxiv.org/pdf/2408.01380v1", "priority": "normal", "mapped_sections": [], "evidence_level": "abstract", "fulltext_path": "", "authors": ["Prattyush Mangal", "Carol Mak", "Theo Kanakis", "Timothy Donovan", "Dave Braines", "Edward Pyzer-Knapp"], "abstract": "The emergence of Large Language Models (LLMs) have fundamentally altered the way we interact with digital systems and have led to the pursuit of LLM powered AI agents to assist in daily workflows. LLMs, whilst powerful and capable of demonstrating some emergent properties, are not logical reasoners and often struggle to perform well at all sub-tasks carried out by an AI agent to plan and execute a workflow. While existing studies tackle this lack of proficiency by generalised pretraining at a huge scale or by specialised fine-tuning for tool use, we assess if a system comprising of a coalition of pretrained LLMs, each exhibiting specialised performance at individual sub-tasks, can match the performance of single model agents. The coalition of models approach showcases its potential for building robustness and reducing the operational costs of these AI agents by leveraging traits exhibited by specific models. Our findings demonstrate that fine-tuning can be mitigated by considering a coalition of pretrained models and believe that this approach can be applied to other non-agentic systems which utilise LLMs.", "summary_bullets": ["The emergence of Large Language Models (LLMs) have fundamentally altered the way we interact with digital systems and have led to the pursuit of LLM powered AI agents to assist in daily workflows.", "LLMs, whilst powerful and capable of demonstrating some emergent properties, are not logical reasoners and often struggle to perform well at all sub-tasks carried out by an AI agent to plan and execute a workflow.", "While existing studies tackle this lack of proficiency by generalised pretraining at a huge scale or by specialised fine-tuning for tool use, we assess if a system comprising of a coalition of pretrained LLMs, each exhibiting specialised performance at individual sub-tasks, can match the performance of single model agents."], "method": "The emergence of Large Language Models (LLMs) have fundamentally altered the way we interact with digital systems and have led to the pursuit of LLM powered AI agents to assist in daily workflows.", "key_results": ["Our findings demonstrate that fine-tuning can be mitigated by considering a coalition of pretrained models and believe that this approach can be applied to other non-agentic systems which utilise LLMs."], "limitations": ["Abstract-level evidence only: validate assumptions, evaluation protocol, and failure cases in the full paper before relying on this as key evidence."], "bibkey": "Mangal2024Coalitions"}
{"paper_id": "P0253", "title": "CodeTree: Agent-guided Tree Search for Code Generation with Large Language Models", "year": 2024, "url": "http://arxiv.org/abs/2411.04329v2", "arxiv_id": "2411.04329v2", "primary_category": "cs.CL", "categories": ["cs.CL"], "pdf_url": "https://arxiv.org/pdf/2411.04329v2", "priority": "normal", "mapped_sections": [], "evidence_level": "abstract", "fulltext_path": "", "authors": ["Jierui Li", "Hung Le", "Yingbo Zhou", "Caiming Xiong", "Silvio Savarese", "Doyen Sahoo"], "abstract": "Pre-trained on massive amounts of code and text data, large language models (LLMs) have demonstrated remarkable achievements in performing code generation tasks. With additional execution-based feedback, these models can act as agents with capabilities to self-refine and improve generated code autonomously. However, on challenging coding tasks with extremely large search space, current agentic approaches still struggle with multi-stage planning, generating, and debugging. To address this problem, we propose CodeTree, a framework for LLM agents to efficiently explore the search space in different stages of the code generation process. Specifically, we adopted a unified tree structure to explicitly explore different coding strategies, generate corresponding coding solutions, and subsequently refine the solutions. In each stage, critical decision-making (ranking, termination, expanding) of the exploration process is guided by both the environmental execution-based feedback and LLM-agent-generated feedback. We comprehensively evaluated CodeTree on 7 code generation benchmarks and demonstrated the significant performance gains of CodeTree against strong baselines. Using GPT-4o as the base model, we consistently achieved top results of 95.1 on HumanEval, 98.7 on MBPP, and 43.0 on CodeContests. On the challenging SWEBench benchmark, our approach led to significant performance gains.", "summary_bullets": ["Pre-trained on massive amounts of code and text data, large language models (LLMs) have demonstrated remarkable achievements in performing code generation tasks.", "With additional execution-based feedback, these models can act as agents with capabilities to self-refine and improve generated code autonomously.", "However, on challenging coding tasks with extremely large search space, current agentic approaches still struggle with multi-stage planning, generating, and debugging."], "method": "To address this problem, we propose CodeTree, a framework for LLM agents to efficiently explore the search space in different stages of the code generation process.", "key_results": ["We comprehensively evaluated CodeTree on 7 code generation benchmarks and demonstrated the significant performance gains of CodeTree against strong baselines.", "Using GPT-4o as the base model, we consistently achieved top results of 95.1 on HumanEval, 98.7 on MBPP, and 43.0 on CodeContests."], "limitations": ["Abstract-level evidence only: validate assumptions, evaluation protocol, and failure cases in the full paper before relying on this as key evidence."], "bibkey": "Li2024Codetree"}
{"paper_id": "P0254", "title": "Development of a Large Language Model-based Multi-Agent Clinical Decision Support System for Korean Triage and Acuity Scale (KTAS)-Based Triage and Treatment Planning in Emergency Departments", "year": 2024, "url": "http://arxiv.org/abs/2408.07531v2", "arxiv_id": "2408.07531v2", "primary_category": "cs.AI", "categories": ["cs.AI", "cs.CL", "cs.LG"], "pdf_url": "https://arxiv.org/pdf/2408.07531v2", "priority": "normal", "mapped_sections": [], "evidence_level": "abstract", "fulltext_path": "", "authors": ["Seungjun Han", "Wongyung Choi"], "abstract": "Emergency department (ED) overcrowding and the complexity of rapid decision-making in critical care settings pose significant challenges to healthcare systems worldwide. While clinical decision support systems (CDSS) have shown promise, the integration of large language models (LLMs) offers new possibilities for enhancing triage accuracy and clinical decision-making. This study presents an LLM-driven CDSS designed to assist ED physicians and nurses in patient triage, treatment planning, and overall emergency care management.\n  We developed a multi-agent CDSS utilizing Llama-3-70b as the base LLM, orchestrated by CrewAI and Langchain. The system comprises four AI agents emulating key ED roles: Triage Nurse, Emergency Physician, Pharmacist, and ED Coordinator. It incorporates the Korean Triage and Acuity Scale (KTAS) for triage assessment and integrates with the RxNorm API for medication management.\n  The model was evaluated using the Asclepius dataset, with performance assessed by a clinical emergency medicine specialist. The CDSS demonstrated high accuracy in triage decision-making compared to the baseline of a single-agent system. Furthermore, the system exhibited strong performance in critical areas, including primary diagnosis, critical findings identification, disposition decision-making, treatment planning, and resource allocation.\n  Our multi-agent CDSS demonstrates significant potential for supporting comprehensive emergency care management. By leveraging state-of-the-art AI technologies, this system offers a scalable and adaptable tool that could enhance emergency medical care delivery, potentially alleviating ED overcrowding and improving patient outcomes. This work contributes to the growing field of AI applications in emergency medicine and offers a promising direction for future research and clinical implementation.", "summary_bullets": ["Emergency department (ED) overcrowding and the complexity of rapid decision-making in critical care settings pose significant challenges to healthcare systems worldwide.", "While clinical decision support systems (CDSS) have shown promise, the integration of large language models (LLMs) offers new possibilities for enhancing triage accuracy and clinical decision-making.", "This study presents an LLM-driven CDSS designed to assist ED physicians and nurses in patient triage, treatment planning, and overall emergency care management."], "method": "Emergency department (ED) overcrowding and the complexity of rapid decision-making in critical care settings pose significant challenges to healthcare systems worldwide.", "key_results": ["We developed a multi-agent CDSS utilizing Llama-3-70b as the base LLM, orchestrated by CrewAI and Langchain.", "While clinical decision support systems (CDSS) have shown promise, the integration of large language models (LLMs) offers new possibilities for enhancing triage accuracy and clinical decision-making."], "limitations": ["Abstract-level evidence only: validate assumptions, evaluation protocol, and failure cases in the full paper before relying on this as key evidence."], "bibkey": "Han2024Development"}
{"paper_id": "P0255", "title": "DrugAgent: Multi-Agent Large Language Model-Based Reasoning for Drug-Target Interaction Prediction", "year": 2024, "url": "http://arxiv.org/abs/2408.13378v4", "arxiv_id": "2408.13378v4", "primary_category": "cs.AI", "categories": ["cs.AI", "cs.CL", "cs.IR", "cs.LG", "q-bio.QM"], "pdf_url": "https://arxiv.org/pdf/2408.13378v4", "priority": "normal", "mapped_sections": ["3.1", "5.2"], "evidence_level": "abstract", "fulltext_path": "", "authors": ["Yoshitaka Inoue", "Tianci Song", "Xinling Wang", "Augustin Luna", "Tianfan Fu"], "abstract": "Advancements in large language models (LLMs) allow them to address diverse questions using human-like interfaces. Still, limitations in their training prevent them from answering accurately in scenarios that could benefit from multiple perspectives. Multi-agent systems allow the resolution of questions to enhance result consistency and reliability. While drug-target interaction (DTI) prediction is important for drug discovery, existing approaches face challenges due to complex biological systems and the lack of interpretability needed for clinical applications. DrugAgent is a multi-agent LLM system for DTI prediction that combines multiple specialized perspectives with transparent reasoning. Our system adapts and extends existing multi-agent frameworks by (1) applying coordinator-based architecture to the DTI domain, (2) integrating domain-specific data sources, including ML predictions, knowledge graphs, and literature evidence, and (3) incorporating Chain-of-Thought (CoT) and ReAct (Reason+Act) frameworks for transparent DTI reasoning. We conducted comprehensive experiments using a kinase inhibitor dataset, where our multi-agent LLM method outperformed the non-reasoning multi-agent model (GPT-4o mini) by 45% in F1 score (0.514 vs 0.355). Through ablation studies, we demonstrated the contributions of each agent, with the AI agent being the most impactful, followed by the KG agent and search agent. Most importantly, our approach provides detailed, human-interpretable reasoning for each prediction by combining evidence from multiple sources - a critical feature for biomedical applications where understanding the rationale behind predictions is essential for clinical decision-making and regulatory compliance. Code is available at https://anonymous.4open.science/r/DrugAgent-B2EA.", "summary_bullets": ["Advancements in large language models (LLMs) allow them to address diverse questions using human-like interfaces.", "Still, limitations in their training prevent them from answering accurately in scenarios that could benefit from multiple perspectives.", "Multi-agent systems allow the resolution of questions to enhance result consistency and reliability."], "method": "Most importantly, our approach provides detailed, human-interpretable reasoning for each prediction by combining evidence from multiple sources - a critical feature for biomedical applications where understanding the rationale behind predictions is essential for clinical decision-making and regulatory compliance.", "key_results": ["We conducted comprehensive experiments using a kinase inhibitor dataset, where our multi-agent LLM method outperformed the non-reasoning multi-agent model (GPT-4o mini) by 45% in F1 score (0.514 vs 0.355).", "Our system adapts and extends existing multi-agent frameworks by (1) applying coordinator-based architecture to the DTI domain, (2) integrating domain-specific data sources, including ML predictions, knowledge graphs, and literature evidence, and (3) incorporating Chain-of-Thought (CoT) and ReAct (Reason+Act) frameworks for transparent DTI reasoning."], "limitations": ["Abstract-level evidence only: validate assumptions, evaluation protocol, and failure cases in the full paper before relying on this as key evidence.", "Still, limitations in their training prevent them from answering accurately in scenarios that could benefit from multiple perspectives."], "bibkey": "Inoue2024Drugagent"}
{"paper_id": "P0256", "title": "Empowering Large Language Model Agents through Action Learning", "year": 2024, "url": "http://arxiv.org/abs/2402.15809v2", "arxiv_id": "2402.15809v2", "primary_category": "cs.AI", "categories": ["cs.AI", "cs.CL"], "pdf_url": "https://arxiv.org/pdf/2402.15809v2", "priority": "normal", "mapped_sections": [], "evidence_level": "abstract", "fulltext_path": "", "authors": ["Haiteng Zhao", "Chang Ma", "Guoyin Wang", "Jing Su", "Lingpeng Kong", "Jingjing Xu", "Zhi-Hong Deng", "Hongxia Yang"], "abstract": "Large Language Model (LLM) Agents have recently garnered increasing interest yet they are limited in their ability to learn from trial and error, a key element of intelligent behavior. In this work, we argue that the capacity to learn new actions from experience is fundamental to the advancement of learning in LLM agents. While humans naturally expand their action spaces and develop skills through experiential learning, LLM agents typically operate within fixed action spaces, limiting their potential for growth. To address these challenges, our study explores open-action learning for language agents. We introduce a framework LearnAct with an iterative learning strategy to create and improve actions in the form of Python functions. In each iteration, LLM revises and updates the currently available actions based on the errors identified in unsuccessful training tasks, thereby enhancing action effectiveness. Our experimental evaluations across Robotic Planning and Alfworld environments reveal that after learning on a few training task instances, our approach to open-action learning markedly improves agent performance for the type of task (by 32 percent in AlfWorld compared to ReAct+Reflexion, for instance) highlighting the importance of experiential action learning in the development of more intelligent LLM agents.", "summary_bullets": ["Large Language Model (LLM) Agents have recently garnered increasing interest yet they are limited in their ability to learn from trial and error, a key element of intelligent behavior.", "In this work, we argue that the capacity to learn new actions from experience is fundamental to the advancement of learning in LLM agents.", "While humans naturally expand their action spaces and develop skills through experiential learning, LLM agents typically operate within fixed action spaces, limiting their potential for growth."], "method": "We introduce a framework LearnAct with an iterative learning strategy to create and improve actions in the form of Python functions.", "key_results": ["Our experimental evaluations across Robotic Planning and Alfworld environments reveal that after learning on a few training task instances, our approach to open-action learning markedly improves agent performance for the type of task (by 32 percent in AlfWorld compared to ReAct+Reflexion, for instance) highlighting the importance of experiential action learning in the development of more intelligent LLM agents."], "limitations": ["Abstract-level evidence only: validate assumptions, evaluation protocol, and failure cases in the full paper before relying on this as key evidence."], "bibkey": "Zhao2024Empowering"}
{"paper_id": "P0257", "title": "Evaluating and Enhancing LLMs Agent based on Theory of Mind in Guandan: A Multi-Player Cooperative Game under Imperfect Information", "year": 2024, "url": "http://arxiv.org/abs/2408.02559v1", "arxiv_id": "2408.02559v1", "primary_category": "cs.CL", "categories": ["cs.CL", "cs.AI"], "pdf_url": "https://arxiv.org/pdf/2408.02559v1", "priority": "normal", "mapped_sections": [], "evidence_level": "abstract", "fulltext_path": "", "authors": ["Yauwai Yim", "Chunkit Chan", "Tianyu Shi", "Zheye Deng", "Wei Fan", "Tianshi Zheng", "Yangqiu Song"], "abstract": "Large language models (LLMs) have shown success in handling simple games with imperfect information and enabling multi-agent coordination, but their ability to facilitate practical collaboration against other agents in complex, imperfect information environments, especially in a non-English environment, still needs to be explored. This study investigates the applicability of knowledge acquired by open-source and API-based LLMs to sophisticated text-based games requiring agent collaboration under imperfect information, comparing their performance to established baselines using other types of agents. We propose a Theory of Mind (ToM) planning technique that allows LLM agents to adapt their strategy against various adversaries using only game rules, current state, and historical context as input. An external tool was incorporated to mitigate the challenge of dynamic and extensive action spaces in this card game. Our results show that although a performance gap exists between current LLMs and state-of-the-art reinforcement learning (RL) models, LLMs demonstrate ToM capabilities in this game setting. It consistently improves their performance against opposing agents, suggesting their ability to understand the actions of allies and adversaries and establish collaboration with allies. To encourage further research and understanding, we have made our codebase openly accessible.", "summary_bullets": ["Large language models (LLMs) have shown success in handling simple games with imperfect information and enabling multi-agent coordination, but their ability to facilitate practical collaboration against other agents in complex, imperfect information environments, especially in a non-English environment, still needs to be explored.", "This study investigates the applicability of knowledge acquired by open-source and API-based LLMs to sophisticated text-based games requiring agent collaboration under imperfect information, comparing their performance to established baselines using other types of agents.", "We propose a Theory of Mind (ToM) planning technique that allows LLM agents to adapt their strategy against various adversaries using only game rules, current state, and historical context as input."], "method": "We propose a Theory of Mind (ToM) planning technique that allows LLM agents to adapt their strategy against various adversaries using only game rules, current state, and historical context as input.", "key_results": ["Large language models (LLMs) have shown success in handling simple games with imperfect information and enabling multi-agent coordination, but their ability to facilitate practical collaboration against other agents in complex, imperfect information environments, especially in a non-English environment, still needs to be explored.", "Our results show that although a performance gap exists between current LLMs and state-of-the-art reinforcement learning (RL) models, LLMs demonstrate ToM capabilities in this game setting."], "limitations": ["Abstract-level evidence only: validate assumptions, evaluation protocol, and failure cases in the full paper before relying on this as key evidence."], "bibkey": "Yim2024Evaluating"}
{"paper_id": "P0258", "title": "Exploring Autonomous Agents through the Lens of Large Language Models: A Review", "year": 2024, "url": "http://arxiv.org/abs/2404.04442v1", "arxiv_id": "2404.04442v1", "primary_category": "cs.AI", "categories": ["cs.AI"], "pdf_url": "https://arxiv.org/pdf/2404.04442v1", "priority": "normal", "mapped_sections": [], "evidence_level": "abstract", "fulltext_path": "", "authors": ["Saikat Barua"], "abstract": "Large Language Models (LLMs) are transforming artificial intelligence, enabling autonomous agents to perform diverse tasks across various domains. These agents, proficient in human-like text comprehension and generation, have the potential to revolutionize sectors from customer service to healthcare. However, they face challenges such as multimodality, human value alignment, hallucinations, and evaluation. Techniques like prompting, reasoning, tool utilization, and in-context learning are being explored to enhance their capabilities. Evaluation platforms like AgentBench, WebArena, and ToolLLM provide robust methods for assessing these agents in complex scenarios. These advancements are leading to the development of more resilient and capable autonomous agents, anticipated to become integral in our digital lives, assisting in tasks from email responses to disease diagnosis. The future of AI, with LLMs at the forefront, is promising.", "summary_bullets": ["Large Language Models (LLMs) are transforming artificial intelligence, enabling autonomous agents to perform diverse tasks across various domains.", "These agents, proficient in human-like text comprehension and generation, have the potential to revolutionize sectors from customer service to healthcare.", "However, they face challenges such as multimodality, human value alignment, hallucinations, and evaluation."], "method": "Large Language Models (LLMs) are transforming artificial intelligence, enabling autonomous agents to perform diverse tasks across various domains.", "key_results": ["These agents, proficient in human-like text comprehension and generation, have the potential to revolutionize sectors from customer service to healthcare.", "However, they face challenges such as multimodality, human value alignment, hallucinations, and evaluation."], "limitations": ["Abstract-level evidence only: validate assumptions, evaluation protocol, and failure cases in the full paper before relying on this as key evidence."], "bibkey": "Barua2024Exploring"}
{"paper_id": "P0259", "title": "Exploring Large Language Model based Intelligent Agents: Definitions, Methods, and Prospects", "year": 2024, "url": "http://arxiv.org/abs/2401.03428v1", "arxiv_id": "2401.03428v1", "primary_category": "cs.AI", "categories": ["cs.AI", "cs.MA"], "pdf_url": "https://arxiv.org/pdf/2401.03428v1", "priority": "normal", "mapped_sections": [], "evidence_level": "abstract", "fulltext_path": "", "authors": ["Yuheng Cheng", "Ceyao Zhang", "Zhengwen Zhang", "Xiangrui Meng", "Sirui Hong", "Wenhao Li", "Zihao Wang", "Zekai Wang", "Feng Yin", "Junhua Zhao", "Xiuqiang He"], "abstract": "Intelligent agents stand out as a potential path toward artificial general intelligence (AGI). Thus, researchers have dedicated significant effort to diverse implementations for them. Benefiting from recent progress in large language models (LLMs), LLM-based agents that use universal natural language as an interface exhibit robust generalization capabilities across various applications -- from serving as autonomous general-purpose task assistants to applications in coding, social, and economic domains, LLM-based agents offer extensive exploration opportunities. This paper surveys current research to provide an in-depth overview of LLM-based intelligent agents within single-agent and multi-agent systems. It covers their definitions, research frameworks, and foundational components such as their composition, cognitive and planning methods, tool utilization, and responses to environmental feedback. We also delve into the mechanisms of deploying LLM-based agents in multi-agent systems, including multi-role collaboration, message passing, and strategies to alleviate communication issues between agents. The discussions also shed light on popular datasets and application scenarios. We conclude by envisioning prospects for LLM-based agents, considering the evolving landscape of AI and natural language processing.", "summary_bullets": ["Intelligent agents stand out as a potential path toward artificial general intelligence (AGI).", "Thus, researchers have dedicated significant effort to diverse implementations for them.", "Benefiting from recent progress in large language models (LLMs), LLM-based agents that use universal natural language as an interface exhibit robust generalization capabilities across various applications -- from serving as autonomous general-purpose task assistants to applications in coding, social, and economic domains, LLM-based agents offer extensive exploration opportunities."], "method": "Intelligent agents stand out as a potential path toward artificial general intelligence (AGI).", "key_results": ["The discussions also shed light on popular datasets and application scenarios."], "limitations": ["Abstract-level evidence only: validate assumptions, evaluation protocol, and failure cases in the full paper before relying on this as key evidence."], "bibkey": "Cheng2024Exploring"}
{"paper_id": "P0260", "title": "Federated In-Context LLM Agent Learning", "year": 2024, "url": "http://arxiv.org/abs/2412.08054v1", "arxiv_id": "2412.08054v1", "primary_category": "cs.LG", "categories": ["cs.LG", "cs.AI", "cs.CL", "cs.CR"], "pdf_url": "https://arxiv.org/pdf/2412.08054v1", "priority": "normal", "mapped_sections": ["5.1"], "evidence_level": "abstract", "fulltext_path": "", "authors": ["Panlong Wu", "Kangshuo Li", "Junbao Nan", "Fangxin Wang"], "abstract": "Large Language Models (LLMs) have revolutionized intelligent services by enabling logical reasoning, tool use, and interaction with external systems as agents. The advancement of LLMs is frequently hindered by the scarcity of high-quality data, much of which is inherently sensitive. Federated learning (FL) offers a potential solution by facilitating the collaborative training of distributed LLMs while safeguarding private data. However, FL frameworks face significant bandwidth and computational demands, along with challenges from heterogeneous data distributions. The emerging in-context learning capability of LLMs offers a promising approach by aggregating natural language rather than bulky model parameters. Yet, this method risks privacy leakage, as it necessitates the collection and presentation of data samples from various clients during aggregation. In this paper, we propose a novel privacy-preserving Federated In-Context LLM Agent Learning (FICAL) algorithm, which to our best knowledge for the first work unleashes the power of in-context learning to train diverse LLM agents through FL. In our design, knowledge compendiums generated by a novel LLM-enhanced Knowledge Compendiums Generation (KCG) module are transmitted between clients and the server instead of model parameters in previous FL methods. Apart from that, an incredible Retrieval Augmented Generation (RAG) based Tool Learning and Utilizing (TLU) module is designed and we incorporate the aggregated global knowledge compendium as a teacher to teach LLM agents the usage of tools. We conducted extensive experiments and the results show that FICAL has competitive performance compared to other SOTA baselines with a significant communication cost decrease of $\\mathbf{3.33\\times10^5}$ times.", "summary_bullets": ["Large Language Models (LLMs) have revolutionized intelligent services by enabling logical reasoning, tool use, and interaction with external systems as agents.", "The advancement of LLMs is frequently hindered by the scarcity of high-quality data, much of which is inherently sensitive.", "Federated learning (FL) offers a potential solution by facilitating the collaborative training of distributed LLMs while safeguarding private data."], "method": "In this paper, we propose a novel privacy-preserving Federated In-Context LLM Agent Learning (FICAL) algorithm, which to our best knowledge for the first work unleashes the power of in-context learning to train diverse LLM agents through FL.", "key_results": ["We conducted extensive experiments and the results show that FICAL has competitive performance compared to other SOTA baselines with a significant communication cost decrease of $\\mathbf{3.33\\times10^5}$ times."], "limitations": ["Abstract-level evidence only: validate assumptions, evaluation protocol, and failure cases in the full paper before relying on this as key evidence."], "bibkey": "Wu2024Federated"}
{"paper_id": "P0261", "title": "From Individual to Society: A Survey on Social Simulation Driven by Large Language Model-based Agents", "year": 2024, "url": "http://arxiv.org/abs/2412.03563v1", "arxiv_id": "2412.03563v1", "primary_category": "cs.CL", "categories": ["cs.CL", "cs.CY"], "pdf_url": "https://arxiv.org/pdf/2412.03563v1", "priority": "normal", "mapped_sections": ["5.1"], "evidence_level": "abstract", "fulltext_path": "", "authors": ["Xinyi Mou", "Xuanwen Ding", "Qi He", "Liang Wang", "Jingcong Liang", "Xinnong Zhang", "Libo Sun", "Jiayu Lin", "Jie Zhou", "Xuanjing Huang", "Zhongyu Wei"], "abstract": "Traditional sociological research often relies on human participation, which, though effective, is expensive, challenging to scale, and with ethical concerns. Recent advancements in large language models (LLMs) highlight their potential to simulate human behavior, enabling the replication of individual responses and facilitating studies on many interdisciplinary studies. In this paper, we conduct a comprehensive survey of this field, illustrating the recent progress in simulation driven by LLM-empowered agents. We categorize the simulations into three types: (1) Individual Simulation, which mimics specific individuals or demographic groups; (2) Scenario Simulation, where multiple agents collaborate to achieve goals within specific contexts; and (3) Society Simulation, which models interactions within agent societies to reflect the complexity and variety of real-world dynamics. These simulations follow a progression, ranging from detailed individual modeling to large-scale societal phenomena. We provide a detailed discussion of each simulation type, including the architecture or key components of the simulation, the classification of objectives or scenarios and the evaluation method. Afterward, we summarize commonly used datasets and benchmarks. Finally, we discuss the trends across these three types of simulation. A repository for the related sources is at {\\url{https://github.com/FudanDISC/SocialAgent}}.", "summary_bullets": ["Traditional sociological research often relies on human participation, which, though effective, is expensive, challenging to scale, and with ethical concerns.", "Recent advancements in large language models (LLMs) highlight their potential to simulate human behavior, enabling the replication of individual responses and facilitating studies on many interdisciplinary studies.", "In this paper, we conduct a comprehensive survey of this field, illustrating the recent progress in simulation driven by LLM-empowered agents."], "method": "Traditional sociological research often relies on human participation, which, though effective, is expensive, challenging to scale, and with ethical concerns.", "key_results": ["We categorize the simulations into three types: (1) Individual Simulation, which mimics specific individuals or demographic groups; (2) Scenario Simulation, where multiple agents collaborate to achieve goals within specific contexts; and (3) Society Simulation, which models interactions within agent societies to reflect the complexity and variety of real-world dynamics.", "Traditional sociological research often relies on human participation, which, though effective, is expensive, challenging to scale, and with ethical concerns."], "limitations": ["Abstract-level evidence only: validate assumptions, evaluation protocol, and failure cases in the full paper before relying on this as key evidence."], "bibkey": "Mou2024From"}
{"paper_id": "P0262", "title": "From LLM to Conversational Agent: A Memory Enhanced Architecture with Fine-Tuning of Large Language Models", "year": 2024, "url": "http://arxiv.org/abs/2401.02777v2", "arxiv_id": "2401.02777v2", "primary_category": "cs.CL", "categories": ["cs.CL", "cs.AI"], "pdf_url": "https://arxiv.org/pdf/2401.02777v2", "priority": "normal", "mapped_sections": ["4.2"], "evidence_level": "abstract", "fulltext_path": "", "authors": ["Na Liu", "Liangyu Chen", "Xiaoyu Tian", "Wei Zou", "Kaijiang Chen", "Ming Cui"], "abstract": "This paper introduces RAISE (Reasoning and Acting through Scratchpad and Examples), an advanced architecture enhancing the integration of Large Language Models (LLMs) like GPT-4 into conversational agents. RAISE, an enhancement of the ReAct framework, incorporates a dual-component memory system, mirroring human short-term and long-term memory, to maintain context and continuity in conversations. It entails a comprehensive agent construction scenario, including phases like Conversation Selection, Scene Extraction, CoT Completion, and Scene Augmentation, leading to the LLMs Training phase. This approach appears to enhance agent controllability and adaptability in complex, multi-turn dialogues. Our preliminary evaluations in a real estate sales context suggest that RAISE has some advantages over traditional agents, indicating its potential for broader applications. This work contributes to the AI field by providing a robust framework for developing more context-aware and versatile conversational agents.", "summary_bullets": ["This paper introduces RAISE (Reasoning and Acting through Scratchpad and Examples), an advanced architecture enhancing the integration of Large Language Models (LLMs) like GPT-4 into conversational agents.", "RAISE, an enhancement of the ReAct framework, incorporates a dual-component memory system, mirroring human short-term and long-term memory, to maintain context and continuity in conversations.", "It entails a comprehensive agent construction scenario, including phases like Conversation Selection, Scene Extraction, CoT Completion, and Scene Augmentation, leading to the LLMs Training phase."], "method": "This paper introduces RAISE (Reasoning and Acting through Scratchpad and Examples), an advanced architecture enhancing the integration of Large Language Models (LLMs) like GPT-4 into conversational agents.", "key_results": ["This paper introduces RAISE (Reasoning and Acting through Scratchpad and Examples), an advanced architecture enhancing the integration of Large Language Models (LLMs) like GPT-4 into conversational agents.", "RAISE, an enhancement of the ReAct framework, incorporates a dual-component memory system, mirroring human short-term and long-term memory, to maintain context and continuity in conversations."], "limitations": ["Abstract-level evidence only: validate assumptions, evaluation protocol, and failure cases in the full paper before relying on this as key evidence."], "bibkey": "Liu2024From"}
{"paper_id": "P0263", "title": "GIVE: Structured Reasoning of Large Language Models with Knowledge Graph Inspired Veracity Extrapolation", "year": 2024, "url": "http://arxiv.org/abs/2410.08475v3", "arxiv_id": "2410.08475v3", "primary_category": "cs.AI", "categories": ["cs.AI", "cs.CL"], "pdf_url": "https://arxiv.org/pdf/2410.08475v3", "priority": "normal", "mapped_sections": [], "evidence_level": "abstract", "fulltext_path": "", "authors": ["Jiashu He", "Mingyu Derek Ma", "Jinxuan Fan", "Dan Roth", "Wei Wang", "Alejandro Ribeiro"], "abstract": "Existing approaches based on context prompting or reinforcement learning (RL) to improve the reasoning capacities of large language models (LLMs) depend on the LLMs' internal knowledge to produce reliable Chain-Of-Thought (CoT). However, no matter the size of LLMs, certain problems cannot be resolved in a single forward pass. Meanwhile, agent-based reasoning systems require access to a comprehensive nonparametric knowledge base, which is often costly or not feasible for use in scientific and niche domains. We present Graph Inspired Veracity Extrapolation (GIVE), a novel reasoning method that merges parametric and non-parametric memories to improve accurate reasoning with minimal external input. GIVE guides the LLM agent to select the most pertinent expert data (observe), engage in query-specific divergent thinking (reflect), and then synthesize this information to produce the final output (speak). Extensive experiments demonstrated the following benefits of our framework: (1) GIVE boosts the performance of LLMs across various sizes. (2) In some scenarios, GIVE allows smaller LLMs to surpass larger, more sophisticated ones in scientific tasks (GPT3.5T + GIVE > GPT4). (3) GIVE is effective on scientific and open-domain assessments. (4) GIVE is a training-free method that enables LLMs to tackle new problems that extend beyond their training data (up to 43.5% -> 88.2%} accuracy improvement). (5) GIVE allows LLM agents to reason using both restricted (very small) and noisy (very large) knowledge sources, accommodating knowledge graphs (KG) ranging from 135 to more than 840k nodes. (6) The reasoning process involved in GIVE is fully interpretable.", "summary_bullets": ["Existing approaches based on context prompting or reinforcement learning (RL) to improve the reasoning capacities of large language models (LLMs) depend on the LLMs' internal knowledge to produce reliable Chain-Of-Thought (CoT).", "However, no matter the size of LLMs, certain problems cannot be resolved in a single forward pass.", "Meanwhile, agent-based reasoning systems require access to a comprehensive nonparametric knowledge base, which is often costly or not feasible for use in scientific and niche domains."], "method": "We present Graph Inspired Veracity Extrapolation (GIVE), a novel reasoning method that merges parametric and non-parametric memories to improve accurate reasoning with minimal external input.", "key_results": ["(4) GIVE is a training-free method that enables LLMs to tackle new problems that extend beyond their training data (up to 43.5% -> 88.2%} accuracy improvement).", "Extensive experiments demonstrated the following benefits of our framework: (1) GIVE boosts the performance of LLMs across various sizes."], "limitations": ["Abstract-level evidence only: validate assumptions, evaluation protocol, and failure cases in the full paper before relying on this as key evidence."], "bibkey": "He2024Give"}
{"paper_id": "P0264", "title": "GameBench: Evaluating Strategic Reasoning Abilities of LLM Agents", "year": 2024, "url": "http://arxiv.org/abs/2406.06613v2", "arxiv_id": "2406.06613v2", "primary_category": "cs.CL", "categories": ["cs.CL", "cs.AI"], "pdf_url": "https://arxiv.org/pdf/2406.06613v2", "priority": "normal", "mapped_sections": [], "evidence_level": "abstract", "fulltext_path": "", "authors": ["Anthony Costarelli", "Mat Allen", "Roman Hauksson", "Grace Sodunke", "Suhas Hariharan", "Carlson Cheng", "Wenjie Li", "Joshua Clymer", "Arjun Yadav"], "abstract": "Large language models have demonstrated remarkable few-shot performance on many natural language understanding tasks. Despite several demonstrations of using large language models in complex, strategic scenarios, there lacks a comprehensive framework for evaluating agents' performance across various types of reasoning found in games. To address this gap, we introduce GameBench, a cross-domain benchmark for evaluating strategic reasoning abilities of LLM agents. We focus on 9 different game environments, where each covers at least one axis of key reasoning skill identified in strategy games, and select games for which strategy explanations are unlikely to form a significant portion of models' pretraining corpuses. Our evaluations use GPT-3 and GPT-4 in their base form along with two scaffolding frameworks designed to enhance strategic reasoning ability: Chain-of-Thought (CoT) prompting and Reasoning Via Planning (RAP). Our results show that none of the tested models match human performance, and at worst GPT-4 performs worse than random action. CoT and RAP both improve scores but not comparable to human levels.", "summary_bullets": ["Large language models have demonstrated remarkable few-shot performance on many natural language understanding tasks.", "Despite several demonstrations of using large language models in complex, strategic scenarios, there lacks a comprehensive framework for evaluating agents' performance across various types of reasoning found in games.", "To address this gap, we introduce GameBench, a cross-domain benchmark for evaluating strategic reasoning abilities of LLM agents."], "method": "To address this gap, we introduce GameBench, a cross-domain benchmark for evaluating strategic reasoning abilities of LLM agents.", "key_results": ["Our results show that none of the tested models match human performance, and at worst GPT-4 performs worse than random action.", "We focus on 9 different game environments, where each covers at least one axis of key reasoning skill identified in strategy games, and select games for which strategy explanations are unlikely to form a significant portion of models' pretraining corpuses."], "limitations": ["Abstract-level evidence only: validate assumptions, evaluation protocol, and failure cases in the full paper before relying on this as key evidence."], "bibkey": "Costarelli2024Gamebench"}
{"paper_id": "P0265", "title": "GraphInsight: Unlocking Insights in Large Language Models for Graph Structure Understanding", "year": 2024, "url": "http://arxiv.org/abs/2409.03258v3", "arxiv_id": "2409.03258v3", "primary_category": "cs.CL", "categories": ["cs.CL"], "pdf_url": "https://arxiv.org/pdf/2409.03258v3", "priority": "normal", "mapped_sections": [], "evidence_level": "abstract", "fulltext_path": "", "authors": ["Yukun Cao", "Shuo Han", "Zengyi Gao", "Zezhong Ding", "Xike Xie", "S. Kevin Zhou"], "abstract": "Although Large Language Models (LLMs) have demonstrated potential in processing graphs, they struggle with comprehending graphical structure information through prompts of graph description sequences, especially as the graph size increases. We attribute this challenge to the uneven memory performance of LLMs across different positions in graph description sequences, known as ''positional biases''. To address this, we propose GraphInsight, a novel framework aimed at improving LLMs' comprehension of both macro- and micro-level graphical information. GraphInsight is grounded in two key strategies: 1) placing critical graphical information in positions where LLMs exhibit stronger memory performance, and 2) investigating a lightweight external knowledge base for regions with weaker memory performance, inspired by retrieval-augmented generation (RAG). Moreover, GraphInsight explores integrating these two strategies into LLM agent processes for composite graph tasks that require multi-step reasoning. Extensive empirical studies on benchmarks with a wide range of evaluation tasks show that GraphInsight significantly outperforms all other graph description methods (e.g., prompting techniques and reordering strategies) in understanding graph structures of varying sizes.", "summary_bullets": ["Although Large Language Models (LLMs) have demonstrated potential in processing graphs, they struggle with comprehending graphical structure information through prompts of graph description sequences, especially as the graph size increases.", "We attribute this challenge to the uneven memory performance of LLMs across different positions in graph description sequences, known as ''positional biases''.", "To address this, we propose GraphInsight, a novel framework aimed at improving LLMs' comprehension of both macro- and micro-level graphical information."], "method": "To address this, we propose GraphInsight, a novel framework aimed at improving LLMs' comprehension of both macro- and micro-level graphical information.", "key_results": ["GraphInsight is grounded in two key strategies: 1) placing critical graphical information in positions where LLMs exhibit stronger memory performance, and 2) investigating a lightweight external knowledge base for regions with weaker memory performance, inspired by retrieval-augmented generation (RAG).", "Extensive empirical studies on benchmarks with a wide range of evaluation tasks show that GraphInsight significantly outperforms all other graph description methods (e.g., prompting techniques and reordering strategies) in understanding graph structures of varying sizes."], "limitations": ["Abstract-level evidence only: validate assumptions, evaluation protocol, and failure cases in the full paper before relying on this as key evidence."], "bibkey": "Cao2024Graphinsight"}
{"paper_id": "P0266", "title": "Guide-LLM: An Embodied LLM Agent and Text-Based Topological Map for Robotic Guidance of People with Visual Impairments", "year": 2024, "url": "http://arxiv.org/abs/2410.20666v2", "arxiv_id": "2410.20666v2", "primary_category": "cs.RO", "categories": ["cs.RO", "cs.AI", "cs.CL"], "pdf_url": "https://arxiv.org/pdf/2410.20666v2", "priority": "normal", "mapped_sections": [], "evidence_level": "abstract", "fulltext_path": "", "authors": ["Sangmim Song", "Sarath Kodagoda", "Amal Gunatilake", "Marc G. Carmichael", "Karthick Thiyagarajan", "Jodi Martin"], "abstract": "Navigation presents a significant challenge for persons with visual impairments (PVI). While traditional aids such as white canes and guide dogs are invaluable, they fall short in delivering detailed spatial information and precise guidance to desired locations. Recent developments in large language models (LLMs) and vision-language models (VLMs) offer new avenues for enhancing assistive navigation. In this paper, we introduce Guide-LLM, an embodied LLM-based agent designed to assist PVI in navigating large indoor environments. Our approach features a novel text-based topological map that enables the LLM to plan global paths using a simplified environmental representation, focusing on straight paths and right-angle turns to facilitate navigation. Additionally, we utilize the LLM's commonsense reasoning for hazard detection and personalized path planning based on user preferences. Simulated experiments demonstrate the system's efficacy in guiding PVI, underscoring its potential as a significant advancement in assistive technology. The results highlight Guide-LLM's ability to offer efficient, adaptive, and personalized navigation assistance, pointing to promising advancements in this field.", "summary_bullets": ["Navigation presents a significant challenge for persons with visual impairments (PVI).", "While traditional aids such as white canes and guide dogs are invaluable, they fall short in delivering detailed spatial information and precise guidance to desired locations.", "Recent developments in large language models (LLMs) and vision-language models (VLMs) offer new avenues for enhancing assistive navigation."], "method": "In this paper, we introduce Guide-LLM, an embodied LLM-based agent designed to assist PVI in navigating large indoor environments.", "key_results": ["The results highlight Guide-LLM's ability to offer efficient, adaptive, and personalized navigation assistance, pointing to promising advancements in this field."], "limitations": ["Abstract-level evidence only: validate assumptions, evaluation protocol, and failure cases in the full paper before relying on this as key evidence."], "bibkey": "Song2024Guide"}
{"paper_id": "P0267", "title": "Interpreting Multi-band Galaxy Observations with Large Language Model-Based Agents", "year": 2024, "url": "http://arxiv.org/abs/2409.14807v2", "arxiv_id": "2409.14807v2", "primary_category": "astro-ph.IM", "categories": ["astro-ph.IM", "astro-ph.GA"], "pdf_url": "https://arxiv.org/pdf/2409.14807v2", "priority": "normal", "mapped_sections": [], "evidence_level": "abstract", "fulltext_path": "", "authors": ["Zechang Sun", "Yuan-Sen Ting", "Yaobo Liang", "Nan Duan", "Song Huang", "Zheng Cai"], "abstract": "Astronomical research traditionally relies on extensive domain knowledge to interpret observations and narrow down hypotheses. We demonstrate that this process can be emulated using large language model-based agents to accelerate research workflows. We propose mephisto, a multi-agent collaboration framework that mimics human reasoning to interpret multi-band galaxy observations. mephisto interacts with the CIGALE codebase, which includes spectral energy distribution (SED) models to explain observations. In this open-world setting, mephisto learns from its self-play experience, performs tree search, and accumulates knowledge in a dynamically updated base. As a proof of concept, we apply mephisto to the latest data from the James Webb Space Telescope. mephisto attains near-human proficiency in reasoning about galaxies' physical scenarios, even when dealing with a recently discovered population of \"Little Red Dot\" galaxies. This represents the first demonstration of agentic research in astronomy, advancing towards end-to-end research via LLM agents and potentially expediting astronomical discoveries.", "summary_bullets": ["Astronomical research traditionally relies on extensive domain knowledge to interpret observations and narrow down hypotheses.", "We demonstrate that this process can be emulated using large language model-based agents to accelerate research workflows.", "We propose mephisto, a multi-agent collaboration framework that mimics human reasoning to interpret multi-band galaxy observations."], "method": "We propose mephisto, a multi-agent collaboration framework that mimics human reasoning to interpret multi-band galaxy observations.", "key_results": ["We propose mephisto, a multi-agent collaboration framework that mimics human reasoning to interpret multi-band galaxy observations.", "mephisto attains near-human proficiency in reasoning about galaxies' physical scenarios, even when dealing with a recently discovered population of \"Little Red Dot\" galaxies."], "limitations": ["Abstract-level evidence only: validate assumptions, evaluation protocol, and failure cases in the full paper before relying on this as key evidence."], "bibkey": "Sun2024Interpreting"}
{"paper_id": "P0268", "title": "LLM Agents can Autonomously Hack Websites", "year": 2024, "url": "http://arxiv.org/abs/2402.06664v3", "arxiv_id": "2402.06664v3", "primary_category": "cs.CR", "categories": ["cs.CR", "cs.AI"], "pdf_url": "https://arxiv.org/pdf/2402.06664v3", "priority": "normal", "mapped_sections": [], "evidence_level": "abstract", "fulltext_path": "", "authors": ["Richard Fang", "Rohan Bindu", "Akul Gupta", "Qiusi Zhan", "Daniel Kang"], "abstract": "In recent years, large language models (LLMs) have become increasingly capable and can now interact with tools (i.e., call functions), read documents, and recursively call themselves. As a result, these LLMs can now function autonomously as agents. With the rise in capabilities of these agents, recent work has speculated on how LLM agents would affect cybersecurity. However, not much is known about the offensive capabilities of LLM agents.\n  In this work, we show that LLM agents can autonomously hack websites, performing tasks as complex as blind database schema extraction and SQL injections without human feedback. Importantly, the agent does not need to know the vulnerability beforehand. This capability is uniquely enabled by frontier models that are highly capable of tool use and leveraging extended context. Namely, we show that GPT-4 is capable of such hacks, but existing open-source models are not. Finally, we show that GPT-4 is capable of autonomously finding vulnerabilities in websites in the wild. Our findings raise questions about the widespread deployment of LLMs.", "summary_bullets": ["In recent years, large language models (LLMs) have become increasingly capable and can now interact with tools (i.e., call functions), read documents, and recursively call themselves.", "As a result, these LLMs can now function autonomously as agents.", "With the rise in capabilities of these agents, recent work has speculated on how LLM agents would affect cybersecurity."], "method": "In recent years, large language models (LLMs) have become increasingly capable and can now interact with tools (i.e., call functions), read documents, and recursively call themselves.", "key_results": ["Namely, we show that GPT-4 is capable of such hacks, but existing open-source models are not.", "Finally, we show that GPT-4 is capable of autonomously finding vulnerabilities in websites in the wild."], "limitations": ["Abstract-level evidence only: validate assumptions, evaluation protocol, and failure cases in the full paper before relying on this as key evidence.", "Importantly, the agent does not need to know the vulnerability beforehand."], "bibkey": "Fang2024Agents"}
{"paper_id": "P0269", "title": "LLM as a Mastermind: A Survey of Strategic Reasoning with Large Language Models", "year": 2024, "url": "http://arxiv.org/abs/2404.01230v1", "arxiv_id": "2404.01230v1", "primary_category": "cs.CL", "categories": ["cs.CL"], "pdf_url": "https://arxiv.org/pdf/2404.01230v1", "priority": "normal", "mapped_sections": [], "evidence_level": "abstract", "fulltext_path": "", "authors": ["Yadong Zhang", "Shaoguang Mao", "Tao Ge", "Xun Wang", "Adrian de Wynter", "Yan Xia", "Wenshan Wu", "Ting Song", "Man Lan", "Furu Wei"], "abstract": "This paper presents a comprehensive survey of the current status and opportunities for Large Language Models (LLMs) in strategic reasoning, a sophisticated form of reasoning that necessitates understanding and predicting adversary actions in multi-agent settings while adjusting strategies accordingly. Strategic reasoning is distinguished by its focus on the dynamic and uncertain nature of interactions among multi-agents, where comprehending the environment and anticipating the behavior of others is crucial. We explore the scopes, applications, methodologies, and evaluation metrics related to strategic reasoning with LLMs, highlighting the burgeoning development in this area and the interdisciplinary approaches enhancing their decision-making performance. It aims to systematize and clarify the scattered literature on this subject, providing a systematic review that underscores the importance of strategic reasoning as a critical cognitive capability and offers insights into future research directions and potential improvements.", "summary_bullets": ["This paper presents a comprehensive survey of the current status and opportunities for Large Language Models (LLMs) in strategic reasoning, a sophisticated form of reasoning that necessitates understanding and predicting adversary actions in multi-agent settings while adjusting strategies accordingly.", "Strategic reasoning is distinguished by its focus on the dynamic and uncertain nature of interactions among multi-agents, where comprehending the environment and anticipating the behavior of others is crucial.", "We explore the scopes, applications, methodologies, and evaluation metrics related to strategic reasoning with LLMs, highlighting the burgeoning development in this area and the interdisciplinary approaches enhancing their decision-making performance."], "method": "This paper presents a comprehensive survey of the current status and opportunities for Large Language Models (LLMs) in strategic reasoning, a sophisticated form of reasoning that necessitates understanding and predicting adversary actions in multi-agent settings while adjusting strategies accordingly.", "key_results": ["We explore the scopes, applications, methodologies, and evaluation metrics related to strategic reasoning with LLMs, highlighting the burgeoning development in this area and the interdisciplinary approaches enhancing their decision-making performance."], "limitations": ["Abstract-level evidence only: validate assumptions, evaluation protocol, and failure cases in the full paper before relying on this as key evidence."], "bibkey": "Zhang2024Mastermind"}
{"paper_id": "P0270", "title": "LLMArena: Assessing Capabilities of Large Language Models in Dynamic Multi-Agent Environments", "year": 2024, "url": "http://arxiv.org/abs/2402.16499v1", "arxiv_id": "2402.16499v1", "primary_category": "cs.CL", "categories": ["cs.CL"], "pdf_url": "https://arxiv.org/pdf/2402.16499v1", "priority": "normal", "mapped_sections": ["3.1", "5.2"], "evidence_level": "abstract", "fulltext_path": "", "authors": ["Junzhe Chen", "Xuming Hu", "Shuodi Liu", "Shiyu Huang", "Wei-Wei Tu", "Zhaofeng He", "Lijie Wen"], "abstract": "Recent advancements in large language models (LLMs) have revealed their potential for achieving autonomous agents possessing human-level intelligence. However, existing benchmarks for evaluating LLM Agents either use static datasets, potentially leading to data leakage or focus only on single-agent scenarios, overlooking the complexities of multi-agent interactions. There is a lack of a benchmark that evaluates the diverse capabilities of LLM agents in multi-agent, dynamic environments. To this end, we introduce LLMArena, a novel and easily extensible framework for evaluating the diverse capabilities of LLM in multi-agent dynamic environments. LLMArena encompasses seven distinct gaming environments, employing Trueskill scoring to assess crucial abilities in LLM agents, including spatial reasoning, strategic planning, numerical reasoning, risk assessment, communication, opponent modeling, and team collaboration. We conduct an extensive experiment and human evaluation among different sizes and types of LLMs, showing that LLMs still have a significant journey ahead in their development towards becoming fully autonomous agents, especially in opponent modeling and team collaboration. We hope LLMArena could guide future research towards enhancing these capabilities in LLMs, ultimately leading to more sophisticated and practical applications in dynamic, multi-agent settings. The code and data will be available.", "summary_bullets": ["Recent advancements in large language models (LLMs) have revealed their potential for achieving autonomous agents possessing human-level intelligence.", "However, existing benchmarks for evaluating LLM Agents either use static datasets, potentially leading to data leakage or focus only on single-agent scenarios, overlooking the complexities of multi-agent interactions.", "There is a lack of a benchmark that evaluates the diverse capabilities of LLM agents in multi-agent, dynamic environments."], "method": "To this end, we introduce LLMArena, a novel and easily extensible framework for evaluating the diverse capabilities of LLM in multi-agent dynamic environments.", "key_results": ["Recent advancements in large language models (LLMs) have revealed their potential for achieving autonomous agents possessing human-level intelligence.", "However, existing benchmarks for evaluating LLM Agents either use static datasets, potentially leading to data leakage or focus only on single-agent scenarios, overlooking the complexities of multi-agent interactions."], "limitations": ["Abstract-level evidence only: validate assumptions, evaluation protocol, and failure cases in the full paper before relying on this as key evidence."], "bibkey": "Chen2024Llmarena"}
{"paper_id": "P0271", "title": "Large Language Model Enhanced Text-to-SQL Generation: A Survey", "year": 2024, "url": "http://arxiv.org/abs/2410.06011v1", "arxiv_id": "2410.06011v1", "primary_category": "cs.DB", "categories": ["cs.DB"], "pdf_url": "https://arxiv.org/pdf/2410.06011v1", "priority": "normal", "mapped_sections": [], "evidence_level": "abstract", "fulltext_path": "", "authors": ["Xiaohu Zhu", "Qian Li", "Lizhen Cui", "Yongkang Liu"], "abstract": "Text-to-SQL translates natural language queries into Structured Query Language (SQL) commands, enabling users to interact with databases using natural language. Essentially, the text-to-SQL task is a text generation task, and its development is primarily dependent on changes in language models. Especially with the rapid development of Large Language Models (LLMs), the pattern of text-to-SQL has undergone significant changes. Existing survey work mainly focuses on rule-based and neural-based approaches, but it still lacks a survey of Text-to-SQL with LLMs. In this paper, we survey the large language model enhanced text-to-SQL generations, classifying them into prompt engineering, fine-tuning, pre-trained, and Agent groups according to training strategies. We also summarize datasets and evaluation metrics comprehensively. This survey could help people better understand the pattern, research status, and challenges of LLM-based text-to-SQL generations.", "summary_bullets": ["Text-to-SQL translates natural language queries into Structured Query Language (SQL) commands, enabling users to interact with databases using natural language.", "Essentially, the text-to-SQL task is a text generation task, and its development is primarily dependent on changes in language models.", "Especially with the rapid development of Large Language Models (LLMs), the pattern of text-to-SQL has undergone significant changes."], "method": "Text-to-SQL translates natural language queries into Structured Query Language (SQL) commands, enabling users to interact with databases using natural language.", "key_results": ["We also summarize datasets and evaluation metrics comprehensively."], "limitations": ["Abstract-level evidence only: validate assumptions, evaluation protocol, and failure cases in the full paper before relying on this as key evidence."], "bibkey": "Zhu2024Large"}
{"paper_id": "P0272", "title": "Large Language Model for Table Processing: A Survey", "year": 2024, "url": "http://arxiv.org/abs/2402.05121v3", "arxiv_id": "2402.05121v3", "primary_category": "cs.AI", "categories": ["cs.AI", "cs.CL"], "pdf_url": "https://arxiv.org/pdf/2402.05121v3", "priority": "normal", "mapped_sections": [], "evidence_level": "abstract", "fulltext_path": "", "authors": ["Weizheng Lu", "Jing Zhang", "Ju Fan", "Zihao Fu", "Yueguo Chen", "Xiaoyong Du"], "abstract": "Tables, typically two-dimensional and structured to store large amounts of data, are essential in daily activities like database queries, spreadsheet manipulations, web table question answering, and image table information extraction. Automating these table-centric tasks with Large Language Models (LLMs) or Visual Language Models (VLMs) offers significant public benefits, garnering interest from academia and industry. This survey provides a comprehensive overview of table-related tasks, examining both user scenarios and technical aspects. It covers traditional tasks like table question answering as well as emerging fields such as spreadsheet manipulation and table data analysis. We summarize the training techniques for LLMs and VLMs tailored for table processing. Additionally, we discuss prompt engineering, particularly the use of LLM-powered agents, for various table-related tasks. Finally, we highlight several challenges, including diverse user input when serving and slow thinking using chain-of-thought.", "summary_bullets": ["Tables, typically two-dimensional and structured to store large amounts of data, are essential in daily activities like database queries, spreadsheet manipulations, web table question answering, and image table information extraction.", "Automating these table-centric tasks with Large Language Models (LLMs) or Visual Language Models (VLMs) offers significant public benefits, garnering interest from academia and industry.", "This survey provides a comprehensive overview of table-related tasks, examining both user scenarios and technical aspects."], "method": "Tables, typically two-dimensional and structured to store large amounts of data, are essential in daily activities like database queries, spreadsheet manipulations, web table question answering, and image table information extraction.", "key_results": ["Finally, we highlight several challenges, including diverse user input when serving and slow thinking using chain-of-thought."], "limitations": ["Abstract-level evidence only: validate assumptions, evaluation protocol, and failure cases in the full paper before relying on this as key evidence."], "bibkey": "Lu2024Large"}
{"paper_id": "P0273", "title": "Large Language Model-Based Agents for Software Engineering: A Survey", "year": 2024, "url": "http://arxiv.org/abs/2409.02977v2", "arxiv_id": "2409.02977v2", "primary_category": "cs.SE", "categories": ["cs.SE", "cs.AI"], "pdf_url": "https://arxiv.org/pdf/2409.02977v2", "priority": "normal", "mapped_sections": [], "evidence_level": "abstract", "fulltext_path": "", "authors": ["Junwei Liu", "Kaixin Wang", "Yixuan Chen", "Xin Peng", "Zhenpeng Chen", "Lingming Zhang", "Yiling Lou"], "abstract": "The recent advance in Large Language Models (LLMs) has shaped a new paradigm of AI agents, i.e., LLM-based agents. Compared to standalone LLMs, LLM-based agents substantially extend the versatility and expertise of LLMs by enhancing LLMs with the capabilities of perceiving and utilizing external resources and tools. To date, LLM-based agents have been applied and shown remarkable effectiveness in Software Engineering (SE). The synergy between multiple agents and human interaction brings further promise in tackling complex real-world SE problems. In this work, we present a comprehensive and systematic survey on LLM-based agents for SE. We collect 124 papers and categorize them from two perspectives, i.e., the SE and agent perspectives. In addition, we discuss open challenges and future directions in this critical domain. The repository of this survey is at https://github.com/FudanSELab/Agent4SE-Paper-List.", "summary_bullets": ["The recent advance in Large Language Models (LLMs) has shaped a new paradigm of AI agents, i.e., LLM-based agents.", "Compared to standalone LLMs, LLM-based agents substantially extend the versatility and expertise of LLMs by enhancing LLMs with the capabilities of perceiving and utilizing external resources and tools.", "To date, LLM-based agents have been applied and shown remarkable effectiveness in Software Engineering (SE)."], "method": "In this work, we present a comprehensive and systematic survey on LLM-based agents for SE.", "key_results": ["We collect 124 papers and categorize them from two perspectives, i.e., the SE and agent perspectives.", "The synergy between multiple agents and human interaction brings further promise in tackling complex real-world SE problems."], "limitations": ["Abstract-level evidence only: validate assumptions, evaluation protocol, and failure cases in the full paper before relying on this as key evidence."], "bibkey": "Liu2024Large"}
{"paper_id": "P0274", "title": "Large Language Model-Brained GUI Agents: A Survey", "year": 2024, "url": "http://arxiv.org/abs/2411.18279v12", "arxiv_id": "2411.18279v12", "primary_category": "cs.AI", "categories": ["cs.AI", "cs.CL", "cs.HC"], "pdf_url": "https://arxiv.org/pdf/2411.18279v12", "priority": "normal", "mapped_sections": ["6.1", "6.2"], "evidence_level": "abstract", "fulltext_path": "", "authors": ["Chaoyun Zhang", "Shilin He", "Jiaxu Qian", "Bowen Li", "Liqun Li", "Si Qin", "Yu Kang", "Minghua Ma", "Guyue Liu", "Qingwei Lin", "Saravan Rajmohan", "Dongmei Zhang", "Qi Zhang"], "abstract": "GUIs have long been central to human-computer interaction, providing an intuitive and visually-driven way to access and interact with digital systems. The advent of LLMs, particularly multimodal models, has ushered in a new era of GUI automation. They have demonstrated exceptional capabilities in natural language understanding, code generation, and visual processing. This has paved the way for a new generation of LLM-brained GUI agents capable of interpreting complex GUI elements and autonomously executing actions based on natural language instructions. These agents represent a paradigm shift, enabling users to perform intricate, multi-step tasks through simple conversational commands. Their applications span across web navigation, mobile app interactions, and desktop automation, offering a transformative user experience that revolutionizes how individuals interact with software. This emerging field is rapidly advancing, with significant progress in both research and industry.\n  To provide a structured understanding of this trend, this paper presents a comprehensive survey of LLM-brained GUI agents, exploring their historical evolution, core components, and advanced techniques. We address research questions such as existing GUI agent frameworks, the collection and utilization of data for training specialized GUI agents, the development of large action models tailored for GUI tasks, and the evaluation metrics and benchmarks necessary to assess their effectiveness. Additionally, we examine emerging applications powered by these agents. Through a detailed analysis, this survey identifies key research gaps and outlines a roadmap for future advancements in the field. By consolidating foundational knowledge and state-of-the-art developments, this work aims to guide both researchers and practitioners in overcoming challenges and unlocking the full potential of LLM-brained GUI agents.", "summary_bullets": ["GUIs have long been central to human-computer interaction, providing an intuitive and visually-driven way to access and interact with digital systems.", "The advent of LLMs, particularly multimodal models, has ushered in a new era of GUI automation.", "They have demonstrated exceptional capabilities in natural language understanding, code generation, and visual processing."], "method": "GUIs have long been central to human-computer interaction, providing an intuitive and visually-driven way to access and interact with digital systems.", "key_results": ["GUIs have long been central to human-computer interaction, providing an intuitive and visually-driven way to access and interact with digital systems.", "We address research questions such as existing GUI agent frameworks, the collection and utilization of data for training specialized GUI agents, the development of large action models tailored for GUI tasks, and the evaluation metrics and benchmarks necessary to assess their effectiveness."], "limitations": ["Abstract-level evidence only: validate assumptions, evaluation protocol, and failure cases in the full paper before relying on this as key evidence."], "bibkey": "Zhang2024Large"}
{"paper_id": "P0275", "title": "Large Language Models as Biomedical Hypothesis Generators: A Comprehensive Evaluation", "year": 2024, "url": "http://arxiv.org/abs/2407.08940v2", "arxiv_id": "2407.08940v2", "primary_category": "cs.CL", "categories": ["cs.CL"], "pdf_url": "https://arxiv.org/pdf/2407.08940v2", "priority": "normal", "mapped_sections": ["6.1"], "evidence_level": "abstract", "fulltext_path": "", "authors": ["Biqing Qi", "Kaiyan Zhang", "Kai Tian", "Haoxiang Li", "Zhang-Ren Chen", "Sihang Zeng", "Ermo Hua", "Hu Jinfang", "Bowen Zhou"], "abstract": "The rapid growth of biomedical knowledge has outpaced our ability to efficiently extract insights and generate novel hypotheses. Large language models (LLMs) have emerged as a promising tool to revolutionize knowledge interaction and potentially accelerate biomedical discovery. In this paper, we present a comprehensive evaluation of LLMs as biomedical hypothesis generators. We construct a dataset of background-hypothesis pairs from biomedical literature, carefully partitioned into training, seen, and unseen test sets based on publication date to mitigate data contamination. Using this dataset, we assess the hypothesis generation capabilities of top-tier instructed models in zero-shot, few-shot, and fine-tuning settings. To enhance the exploration of uncertainty, a crucial aspect of scientific discovery, we incorporate tool use and multi-agent interactions in our evaluation framework. Furthermore, we propose four novel metrics grounded in extensive literature review to evaluate the quality of generated hypotheses, considering both LLM-based and human assessments. Our experiments yield two key findings: 1) LLMs can generate novel and validated hypotheses, even when tested on literature unseen during training, and 2) Increasing uncertainty through multi-agent interactions and tool use can facilitate diverse candidate generation and improve zero-shot hypothesis generation performance. However, we also observe that the integration of additional knowledge through few-shot learning and tool use may not always lead to performance gains, highlighting the need for careful consideration of the type and scope of external knowledge incorporated. These findings underscore the potential of LLMs as powerful aids in biomedical hypothesis generation and provide valuable insights to guide further research in this area.", "summary_bullets": ["The rapid growth of biomedical knowledge has outpaced our ability to efficiently extract insights and generate novel hypotheses.", "Large language models (LLMs) have emerged as a promising tool to revolutionize knowledge interaction and potentially accelerate biomedical discovery.", "In this paper, we present a comprehensive evaluation of LLMs as biomedical hypothesis generators."], "method": "In this paper, we present a comprehensive evaluation of LLMs as biomedical hypothesis generators.", "key_results": ["Our experiments yield two key findings: 1) LLMs can generate novel and validated hypotheses, even when tested on literature unseen during training, and 2) Increasing uncertainty through multi-agent interactions and tool use can facilitate diverse candidate generation and improve zero-shot hypothesis generation performance.", "In this paper, we present a comprehensive evaluation of LLMs as biomedical hypothesis generators."], "limitations": ["Abstract-level evidence only: validate assumptions, evaluation protocol, and failure cases in the full paper before relying on this as key evidence."], "bibkey": "Qi2024Large"}
{"paper_id": "P0276", "title": "Large language model empowered participatory urban planning", "year": 2024, "url": "http://arxiv.org/abs/2402.01698v1", "arxiv_id": "2402.01698v1", "primary_category": "cs.CL", "categories": ["cs.CL", "cs.AI"], "pdf_url": "https://arxiv.org/pdf/2402.01698v1", "priority": "normal", "mapped_sections": ["4.1"], "evidence_level": "abstract", "fulltext_path": "", "authors": ["Zhilun Zhou", "Yuming Lin", "Yong Li"], "abstract": "Participatory urban planning is the mainstream of modern urban planning and involves the active engagement of different stakeholders. However, the traditional participatory paradigm encounters challenges in time and manpower, while the generative planning tools fail to provide adjustable and inclusive solutions. This research introduces an innovative urban planning approach integrating Large Language Models (LLMs) within the participatory process. The framework, based on the crafted LLM agent, consists of role-play, collaborative generation, and feedback iteration, solving a community-level land-use task catering to 1000 distinct interests. Empirical experiments in diverse urban communities exhibit LLM's adaptability and effectiveness across varied planning scenarios. The results were evaluated on four metrics, surpassing human experts in satisfaction and inclusion, and rivaling state-of-the-art reinforcement learning methods in service and ecology. Further analysis shows the advantage of LLM agents in providing adjustable and inclusive solutions with natural language reasoning and strong scalability. While implementing the recent advancements in emulating human behavior for planning, this work envisions both planners and citizens benefiting from low-cost, efficient LLM agents, which is crucial for enhancing participation and realizing participatory urban planning.", "summary_bullets": ["Participatory urban planning is the mainstream of modern urban planning and involves the active engagement of different stakeholders.", "However, the traditional participatory paradigm encounters challenges in time and manpower, while the generative planning tools fail to provide adjustable and inclusive solutions.", "This research introduces an innovative urban planning approach integrating Large Language Models (LLMs) within the participatory process."], "method": "Participatory urban planning is the mainstream of modern urban planning and involves the active engagement of different stakeholders.", "key_results": ["The framework, based on the crafted LLM agent, consists of role-play, collaborative generation, and feedback iteration, solving a community-level land-use task catering to 1000 distinct interests.", "The results were evaluated on four metrics, surpassing human experts in satisfaction and inclusion, and rivaling state-of-the-art reinforcement learning methods in service and ecology."], "limitations": ["Abstract-level evidence only: validate assumptions, evaluation protocol, and failure cases in the full paper before relying on this as key evidence."], "bibkey": "Zhou2024Large"}
{"paper_id": "P0277", "title": "Learning From Failure: Integrating Negative Examples when Fine-tuning Large Language Models as Agents", "year": 2024, "url": "http://arxiv.org/abs/2402.11651v2", "arxiv_id": "2402.11651v2", "primary_category": "cs.CL", "categories": ["cs.CL"], "pdf_url": "https://arxiv.org/pdf/2402.11651v2", "priority": "normal", "mapped_sections": ["3.2", "5.1"], "evidence_level": "abstract", "fulltext_path": "", "authors": ["Renxi Wang", "Haonan Li", "Xudong Han", "Yixuan Zhang", "Timothy Baldwin"], "abstract": "Large language models (LLMs) have achieved success in acting as agents, which interact with environments through tools such as search engines. However, LLMs are optimized for language generation instead of tool use during training or alignment, limiting their effectiveness as agents. To resolve this problem, previous work has first collected interaction trajectories between LLMs and environments, using only trajectories that successfully finished the task to fine-tune smaller models, making fine-tuning data scarce and acquiring it both difficult and costly. Discarding failed trajectories also leads to significant wastage of data and resources and limits the possible optimization paths during fine-tuning. In this paper, we argue that unsuccessful trajectories offer valuable insights, and LLMs can learn from these trajectories through appropriate quality control and fine-tuning strategies. By simply adding a prefix or suffix that tells the model whether to generate a successful trajectory during training, we improve model performance by a large margin on mathematical reasoning, multi-hop question answering, and strategic question answering tasks. We further analyze the inference results and find that our method provides a better trade-off between valuable information and errors in unsuccessful trajectories. To our knowledge, we are the first to demonstrate the value of negative trajectories and their application in agent-tunning scenarios. Our findings offer guidance for developing better agent-tuning methods and low-resource data usage techniques.", "summary_bullets": ["Large language models (LLMs) have achieved success in acting as agents, which interact with environments through tools such as search engines.", "However, LLMs are optimized for language generation instead of tool use during training or alignment, limiting their effectiveness as agents.", "To resolve this problem, previous work has first collected interaction trajectories between LLMs and environments, using only trajectories that successfully finished the task to fine-tune smaller models, making fine-tuning data scarce and acquiring it both difficult and costly."], "method": "We further analyze the inference results and find that our method provides a better trade-off between valuable information and errors in unsuccessful trajectories.", "key_results": ["Large language models (LLMs) have achieved success in acting as agents, which interact with environments through tools such as search engines."], "limitations": ["Abstract-level evidence only: validate assumptions, evaluation protocol, and failure cases in the full paper before relying on this as key evidence."], "bibkey": "Wang2024Learning"}
{"paper_id": "P0278", "title": "LightVA: Lightweight Visual Analytics with LLM Agent-Based Task Planning and Execution", "year": 2024, "url": "http://arxiv.org/abs/2411.05651v2", "arxiv_id": "2411.05651v2", "primary_category": "cs.HC", "categories": ["cs.HC"], "pdf_url": "https://arxiv.org/pdf/2411.05651v2", "priority": "normal", "mapped_sections": ["4.1"], "evidence_level": "abstract", "fulltext_path": "", "authors": ["Yuheng Zhao", "Junjie Wang", "Linbin Xiang", "Xiaowen Zhang", "Zifei Guo", "Cagatay Turkay", "Yu Zhang", "Siming Chen"], "abstract": "Visual analytics (VA) requires analysts to iteratively propose analysis tasks based on observations and execute tasks by creating visualizations and interactive exploration to gain insights. This process demands skills in programming, data processing, and visualization tools, highlighting the need for a more intelligent, streamlined VA approach. Large language models (LLMs) have recently been developed as agents to handle various tasks with dynamic planning and tool-using capabilities, offering the potential to enhance the efficiency and versatility of VA. We propose LightVA, a lightweight VA framework that supports task decomposition, data analysis, and interactive exploration through human-agent collaboration. Our method is designed to help users progressively translate high-level analytical goals into low-level tasks, producing visualizations and deriving insights. Specifically, we introduce an LLM agent-based task planning and execution strategy, employing a recursive process involving a planner, executor, and controller. The planner is responsible for recommending and decomposing tasks, the executor handles task execution, including data analysis, visualization generation and multi-view composition, and the controller coordinates the interaction between the planner and executor. Building on the framework, we develop a system with a hybrid user interface that includes a task flow diagram for monitoring and managing the task planning process, a visualization panel for interactive data exploration, and a chat view for guiding the model through natural language instructions. We examine the effectiveness of our method through a usage scenario and an expert study.", "summary_bullets": ["Visual analytics (VA) requires analysts to iteratively propose analysis tasks based on observations and execute tasks by creating visualizations and interactive exploration to gain insights.", "This process demands skills in programming, data processing, and visualization tools, highlighting the need for a more intelligent, streamlined VA approach.", "Large language models (LLMs) have recently been developed as agents to handle various tasks with dynamic planning and tool-using capabilities, offering the potential to enhance the efficiency and versatility of VA."], "method": "We propose LightVA, a lightweight VA framework that supports task decomposition, data analysis, and interactive exploration through human-agent collaboration.", "key_results": ["We propose LightVA, a lightweight VA framework that supports task decomposition, data analysis, and interactive exploration through human-agent collaboration.", "Visual analytics (VA) requires analysts to iteratively propose analysis tasks based on observations and execute tasks by creating visualizations and interactive exploration to gain insights."], "limitations": ["Abstract-level evidence only: validate assumptions, evaluation protocol, and failure cases in the full paper before relying on this as key evidence."], "bibkey": "Zhao2024Lightva"}
{"paper_id": "P0279", "title": "Long-form factuality in large language models", "year": 2024, "url": "http://arxiv.org/abs/2403.18802v4", "arxiv_id": "2403.18802v4", "primary_category": "cs.CL", "categories": ["cs.CL", "cs.AI", "cs.LG"], "pdf_url": "https://arxiv.org/pdf/2403.18802v4", "priority": "normal", "mapped_sections": [], "evidence_level": "abstract", "fulltext_path": "", "authors": ["Jerry Wei", "Chengrun Yang", "Xinying Song", "Yifeng Lu", "Nathan Hu", "Jie Huang", "Dustin Tran", "Daiyi Peng", "Ruibo Liu", "Da Huang", "Cosmo Du", "Quoc V. Le"], "abstract": "Large language models (LLMs) often generate content that contains factual errors when responding to fact-seeking prompts on open-ended topics. To benchmark a model's long-form factuality in open domains, we first use GPT-4 to generate LongFact, a prompt set comprising thousands of questions spanning 38 topics. We then propose that LLM agents can be used as automated evaluators for long-form factuality through a method which we call Search-Augmented Factuality Evaluator (SAFE). SAFE utilizes an LLM to break down a long-form response into a set of individual facts and to evaluate the accuracy of each fact using a multi-step reasoning process comprising sending search queries to Google Search and determining whether a fact is supported by the search results. Furthermore, we propose extending F1 score as an aggregated metric for long-form factuality. To do so, we balance the percentage of supported facts in a response (precision) with the percentage of provided facts relative to a hyperparameter representing a user's preferred response length (recall).\n  Empirically, we demonstrate that LLM agents can outperform crowdsourced human annotators - on a set of ~16k individual facts, SAFE agrees with crowdsourced human annotators 72% of the time, and on a random subset of 100 disagreement cases, SAFE wins 76% of the time. At the same time, SAFE is more than 20 times cheaper than human annotators. We also benchmark thirteen language models on LongFact across four model families (Gemini, GPT, Claude, and PaLM-2), finding that larger language models generally achieve better long-form factuality. LongFact, SAFE, and all experimental code are available at https://github.com/google-deepmind/long-form-factuality.", "summary_bullets": ["Large language models (LLMs) often generate content that contains factual errors when responding to fact-seeking prompts on open-ended topics.", "To benchmark a model's long-form factuality in open domains, we first use GPT-4 to generate LongFact, a prompt set comprising thousands of questions spanning 38 topics.", "We then propose that LLM agents can be used as automated evaluators for long-form factuality through a method which we call Search-Augmented Factuality Evaluator (SAFE)."], "method": "Furthermore, we propose extending F1 score as an aggregated metric for long-form factuality.", "key_results": ["Empirically, we demonstrate that LLM agents can outperform crowdsourced human annotators - on a set of ~16k individual facts, SAFE agrees with crowdsourced human annotators 72% of the time, and on a random subset of 100 disagreement cases, SAFE wins 76% of the time.", "To benchmark a model's long-form factuality in open domains, we first use GPT-4 to generate LongFact, a prompt set comprising thousands of questions spanning 38 topics."], "limitations": ["Abstract-level evidence only: validate assumptions, evaluation protocol, and failure cases in the full paper before relying on this as key evidence."], "bibkey": "Wei2024Long"}
{"paper_id": "P0280", "title": "MALT: Improving Reasoning with Multi-Agent LLM Training", "year": 2024, "url": "http://arxiv.org/abs/2412.01928v3", "arxiv_id": "2412.01928v3", "primary_category": "cs.LG", "categories": ["cs.LG", "cs.AI"], "pdf_url": "https://arxiv.org/pdf/2412.01928v3", "priority": "normal", "mapped_sections": ["4.1", "5.2"], "evidence_level": "abstract", "fulltext_path": "", "authors": ["Sumeet Ramesh Motwani", "Chandler Smith", "Rocktim Jyoti Das", "Rafael Rafailov", "Ivan Laptev", "Philip H. S. Torr", "Fabio Pizzati", "Ronald Clark", "Christian Schroeder de Witt"], "abstract": "Large Language Models (LLMs) often produce answers with a single chain-of-thought, which restricts their ability to explore reasoning paths or self-correct flawed outputs in complex tasks. In this paper, we introduce MALT (Multi-Agent LLM Training), a novel post-training strategy that divides the reasoning process into generation, verification, and refinement steps using a sequential pipeline of heterogeneous agents. During data generation, each agent is repeatedly sampled to form a multi-agent search tree, where final outputs are graded against ground-truth data. We then apply value iteration to propagate reward signals back to each role-conditioned model, automatically producing multi-agent post-training data without human or teacher-model supervision. Our off-policy approach allows each agent to specialize by learning from correct and incorrect trajectories, ultimately improving the end-to-end reasoning chain. On MATH, GSM8K, and CSQA, MALT surpasses the same baseline LLM with a relative improvement of 15.66%, 7.42%, and 9.40% respectively, making it an important advance towards multi-agent cooperative training.", "summary_bullets": ["Large Language Models (LLMs) often produce answers with a single chain-of-thought, which restricts their ability to explore reasoning paths or self-correct flawed outputs in complex tasks.", "In this paper, we introduce MALT (Multi-Agent LLM Training), a novel post-training strategy that divides the reasoning process into generation, verification, and refinement steps using a sequential pipeline of heterogeneous agents.", "During data generation, each agent is repeatedly sampled to form a multi-agent search tree, where final outputs are graded against ground-truth data."], "method": "In this paper, we introduce MALT (Multi-Agent LLM Training), a novel post-training strategy that divides the reasoning process into generation, verification, and refinement steps using a sequential pipeline of heterogeneous agents.", "key_results": ["On MATH, GSM8K, and CSQA, MALT surpasses the same baseline LLM with a relative improvement of 15.66%, 7.42%, and 9.40% respectively, making it an important advance towards multi-agent cooperative training.", "We then apply value iteration to propagate reward signals back to each role-conditioned model, automatically producing multi-agent post-training data without human or teacher-model supervision."], "limitations": ["Abstract-level evidence only: validate assumptions, evaluation protocol, and failure cases in the full paper before relying on this as key evidence."], "bibkey": "Motwani2024Malt"}
{"paper_id": "P0281", "title": "MIRAI: Evaluating LLM Agents for Event Forecasting", "year": 2024, "url": "http://arxiv.org/abs/2407.01231v1", "arxiv_id": "2407.01231v1", "primary_category": "cs.CL", "categories": ["cs.CL", "cs.AI"], "pdf_url": "https://arxiv.org/pdf/2407.01231v1", "priority": "normal", "mapped_sections": [], "evidence_level": "abstract", "fulltext_path": "", "authors": ["Chenchen Ye", "Ziniu Hu", "Yihe Deng", "Zijie Huang", "Mingyu Derek Ma", "Yanqiao Zhu", "Wei Wang"], "abstract": "Recent advancements in Large Language Models (LLMs) have empowered LLM agents to autonomously collect world information, over which to conduct reasoning to solve complex problems. Given this capability, increasing interests have been put into employing LLM agents for predicting international events, which can influence decision-making and shape policy development on an international scale. Despite such a growing interest, there is a lack of a rigorous benchmark of LLM agents' forecasting capability and reliability. To address this gap, we introduce MIRAI, a novel benchmark designed to systematically evaluate LLM agents as temporal forecasters in the context of international events. Our benchmark features an agentic environment with tools for accessing an extensive database of historical, structured events and textual news articles. We refine the GDELT event database with careful cleaning and parsing to curate a series of relational prediction tasks with varying forecasting horizons, assessing LLM agents' abilities from short-term to long-term forecasting. We further implement APIs to enable LLM agents to utilize different tools via a code-based interface. In summary, MIRAI comprehensively evaluates the agents' capabilities in three dimensions: 1) autonomously source and integrate critical information from large global databases; 2) write codes using domain-specific APIs and libraries for tool-use; and 3) jointly reason over historical knowledge from diverse formats and time to accurately predict future events. Through comprehensive benchmarking, we aim to establish a reliable framework for assessing the capabilities of LLM agents in forecasting international events, thereby contributing to the development of more accurate and trustworthy models for international relation analysis.", "summary_bullets": ["Recent advancements in Large Language Models (LLMs) have empowered LLM agents to autonomously collect world information, over which to conduct reasoning to solve complex problems.", "Given this capability, increasing interests have been put into employing LLM agents for predicting international events, which can influence decision-making and shape policy development on an international scale.", "Despite such a growing interest, there is a lack of a rigorous benchmark of LLM agents' forecasting capability and reliability."], "method": "To address this gap, we introduce MIRAI, a novel benchmark designed to systematically evaluate LLM agents as temporal forecasters in the context of international events.", "key_results": ["In summary, MIRAI comprehensively evaluates the agents' capabilities in three dimensions: 1) autonomously source and integrate critical information from large global databases; 2) write codes using domain-specific APIs and libraries for tool-use; and 3) jointly reason over historical knowledge from diverse formats and time to accurately predict future events.", "Despite such a growing interest, there is a lack of a rigorous benchmark of LLM agents' forecasting capability and reliability."], "limitations": ["Abstract-level evidence only: validate assumptions, evaluation protocol, and failure cases in the full paper before relying on this as key evidence."], "bibkey": "Ye2024Mirai"}
{"paper_id": "P0282", "title": "MLLM-Tool: A Multimodal Large Language Model For Tool Agent Learning", "year": 2024, "url": "http://arxiv.org/abs/2401.10727v3", "arxiv_id": "2401.10727v3", "primary_category": "cs.CV", "categories": ["cs.CV"], "pdf_url": "https://arxiv.org/pdf/2401.10727v3", "priority": "normal", "mapped_sections": ["3.2"], "evidence_level": "abstract", "fulltext_path": "", "authors": ["Chenyu Wang", "Weixin Luo", "Sixun Dong", "Xiaohua Xuan", "Zhengxin Li", "Lin Ma", "Shenghua Gao"], "abstract": "Recently, the astonishing performance of large language models (LLMs) in natural language comprehension and generation tasks triggered lots of exploration of using them as central controllers to build agent systems. Multiple studies focus on bridging the LLMs to external tools to extend the application scenarios. However, the current LLMs' ability to perceive tool use is limited to a single text query, which may result in ambiguity in understanding the users' real intentions. LLMs are expected to eliminate that by perceiving the information in the visual- or auditory-grounded instructions. Therefore, in this paper, we propose MLLM-Tool, a system incorporating open-source LLMs and multi-modal encoders so that the learned LLMs can be conscious of multi-modal input instruction and then select the function-matched tool correctly. To facilitate the evaluation of the model's capability, we collect a dataset featuring multi-modal input tools from HuggingFace. Another essential feature of our dataset is that it also contains multiple potential choices for the same instruction due to the existence of identical functions and synonymous functions, which provides more potential solutions for the same query. The experiments reveal that our MLLM-Tool is capable of recommending appropriate tools for multi-modal instructions. Codes and data are available at https://github.com/MLLM-Tool/MLLM-Tool.", "summary_bullets": ["Recently, the astonishing performance of large language models (LLMs) in natural language comprehension and generation tasks triggered lots of exploration of using them as central controllers to build agent systems.", "Multiple studies focus on bridging the LLMs to external tools to extend the application scenarios.", "However, the current LLMs' ability to perceive tool use is limited to a single text query, which may result in ambiguity in understanding the users' real intentions."], "method": "Therefore, in this paper, we propose MLLM-Tool, a system incorporating open-source LLMs and multi-modal encoders so that the learned LLMs can be conscious of multi-modal input instruction and then select the function-matched tool correctly.", "key_results": ["To facilitate the evaluation of the model's capability, we collect a dataset featuring multi-modal input tools from HuggingFace.", "Another essential feature of our dataset is that it also contains multiple potential choices for the same instruction due to the existence of identical functions and synonymous functions, which provides more potential solutions for the same query."], "limitations": ["Abstract-level evidence only: validate assumptions, evaluation protocol, and failure cases in the full paper before relying on this as key evidence."], "bibkey": "Wang2024Mllm"}
{"paper_id": "P0283", "title": "MathLearner: A Large Language Model Agent Framework for Learning to Solve Mathematical Problems", "year": 2024, "url": "http://arxiv.org/abs/2408.01779v1", "arxiv_id": "2408.01779v1", "primary_category": "cs.CL", "categories": ["cs.CL"], "pdf_url": "https://arxiv.org/pdf/2408.01779v1", "priority": "normal", "mapped_sections": [], "evidence_level": "abstract", "fulltext_path": "", "authors": ["Wenbei Xie", "Donglin Liu", "Haoran Yan", "Wenjie Wu", "Zongyang Liu"], "abstract": "With the development of artificial intelligence (AI), large language models (LLM) are widely used in many fields. However, the reasoning ability of LLM is still very limited when it comes to mathematical reasoning. Mathematics plays an important role in all aspects of human society and is a technical guarantee in the fields of healthcare, transport and aerospace, for this reason, the development of AI big language models in the field of mathematics has great potential significance. To improve the mathematical reasoning ability of large language models, we proposed an agent framework for learning to solve mathematical problems based on inductive reasoning. By emulating the human learning process of generalization of learned information and effective application of previous knowledge in new reasoning tasks, this framework has great performance in the mathematical reasoning process. It improves global accuracy over the baseline method (chain-of-thought) by 20.96% and solves 17.54% of the mathematical problems that the baseline cannot solve. Benefiting from the efficient RETRIEVAL method, our model improves the ability of large language models to efficiently use external knowledge, i.e., the mathematical computation of the model can be based on written procedures. In education, our model can be used as a personalised learning aid, thus reducing the inequality of educational resources.", "summary_bullets": ["With the development of artificial intelligence (AI), large language models (LLM) are widely used in many fields.", "However, the reasoning ability of LLM is still very limited when it comes to mathematical reasoning.", "Mathematics plays an important role in all aspects of human society and is a technical guarantee in the fields of healthcare, transport and aerospace, for this reason, the development of AI big language models in the field of mathematics has great potential significance."], "method": "Benefiting from the efficient RETRIEVAL method, our model improves the ability of large language models to efficiently use external knowledge, i.e., the mathematical computation of the model can be based on written procedures.", "key_results": ["It improves global accuracy over the baseline method (chain-of-thought) by 20.96% and solves 17.54% of the mathematical problems that the baseline cannot solve.", "Mathematics plays an important role in all aspects of human society and is a technical guarantee in the fields of healthcare, transport and aerospace, for this reason, the development of AI big language models in the field of mathematics has great potential significance."], "limitations": ["Abstract-level evidence only: validate assumptions, evaluation protocol, and failure cases in the full paper before relying on this as key evidence."], "bibkey": "Xie2024Mathlearner"}
{"paper_id": "P0284", "title": "MeNTi: Bridging Medical Calculator and LLM Agent with Nested Tool Calling", "year": 2024, "url": "http://arxiv.org/abs/2410.13610v3", "arxiv_id": "2410.13610v3", "primary_category": "cs.AI", "categories": ["cs.AI", "cs.CL"], "pdf_url": "https://arxiv.org/pdf/2410.13610v3", "priority": "normal", "mapped_sections": ["3.2"], "evidence_level": "abstract", "fulltext_path": "", "authors": ["Yakun Zhu", "Shaohang Wei", "Xu Wang", "Kui Xue", "Xiaofan Zhang", "Shaoting Zhang"], "abstract": "Integrating tools into Large Language Models (LLMs) has facilitated the widespread application. Despite this, in specialized downstream task contexts, reliance solely on tools is insufficient to fully address the complexities of the real world. This particularly restricts the effective deployment of LLMs in fields such as medicine. In this paper, we focus on the downstream tasks of medical calculators, which use standardized tests to assess an individual's health status. We introduce MeNTi, a universal agent architecture for LLMs. MeNTi integrates a specialized medical toolkit and employs meta-tool and nested calling mechanisms to enhance LLM tool utilization. Specifically, it achieves flexible tool selection and nested tool calling to address practical issues faced in intricate medical scenarios, including calculator selection, slot filling, and unit conversion. To assess the capabilities of LLMs for quantitative assessment throughout the clinical process of calculator scenarios, we introduce CalcQA. This benchmark requires LLMs to use medical calculators to perform calculations and assess patient health status. CalcQA is constructed by professional physicians and includes 100 case-calculator pairs, complemented by a toolkit of 281 medical tools. The experimental results demonstrate significant performance improvements with our framework. This research paves new directions for applying LLMs in demanding scenarios of medicine.", "summary_bullets": ["Integrating tools into Large Language Models (LLMs) has facilitated the widespread application.", "Despite this, in specialized downstream task contexts, reliance solely on tools is insufficient to fully address the complexities of the real world.", "This particularly restricts the effective deployment of LLMs in fields such as medicine."], "method": "We introduce MeNTi, a universal agent architecture for LLMs.", "key_results": ["CalcQA is constructed by professional physicians and includes 100 case-calculator pairs, complemented by a toolkit of 281 medical tools.", "This benchmark requires LLMs to use medical calculators to perform calculations and assess patient health status."], "limitations": ["Abstract-level evidence only: validate assumptions, evaluation protocol, and failure cases in the full paper before relying on this as key evidence."], "bibkey": "Zhu2024Menti"}
{"paper_id": "P0285", "title": "Mitigating Bias in Queer Representation within Large Language Models: A Collaborative Agent Approach", "year": 2024, "url": "http://arxiv.org/abs/2411.07656v2", "arxiv_id": "2411.07656v2", "primary_category": "cs.CL", "categories": ["cs.CL", "cs.MA"], "pdf_url": "https://arxiv.org/pdf/2411.07656v2", "priority": "normal", "mapped_sections": [], "evidence_level": "abstract", "fulltext_path": "", "authors": ["Tianyi Huang", "Arya Somasundaram"], "abstract": "Large Language Models (LLMs) often perpetuate biases in pronoun usage, leading to misrepresentation or exclusion of queer individuals. This paper addresses the specific problem of biased pronoun usage in LLM outputs, particularly the inappropriate use of traditionally gendered pronouns (\"he,\" \"she\") when inclusive language is needed to accurately represent all identities. We introduce a collaborative agent pipeline designed to mitigate these biases by analyzing and optimizing pronoun usage for inclusivity. Our multi-agent framework includes specialized agents for both bias detection and correction. Experimental evaluations using the Tango dataset-a benchmark focused on gender pronoun usage-demonstrate that our approach significantly improves inclusive pronoun classification, achieving a 32.6 percentage point increase over GPT-4o in correctly disagreeing with inappropriate traditionally gendered pronouns $(œá^2 = 38.57, p < 0.0001)$. These results accentuate the potential of agent-driven frameworks in enhancing fairness and inclusivity in AI-generated content, demonstrating their efficacy in reducing biases and promoting socially responsible AI.", "summary_bullets": ["Large Language Models (LLMs) often perpetuate biases in pronoun usage, leading to misrepresentation or exclusion of queer individuals.", "This paper addresses the specific problem of biased pronoun usage in LLM outputs, particularly the inappropriate use of traditionally gendered pronouns (\"he,\" \"she\") when inclusive language is needed to accurately represent all identities.", "We introduce a collaborative agent pipeline designed to mitigate these biases by analyzing and optimizing pronoun usage for inclusivity."], "method": "We introduce a collaborative agent pipeline designed to mitigate these biases by analyzing and optimizing pronoun usage for inclusivity.", "key_results": ["Experimental evaluations using the Tango dataset-a benchmark focused on gender pronoun usage-demonstrate that our approach significantly improves inclusive pronoun classification, achieving a 32.6 percentage point increase over GPT-4o in correctly disagreeing with inappropriate traditionally gendered pronouns $(œá^2 = 38.57, p < 0.0001)$."], "limitations": ["Abstract-level evidence only: validate assumptions, evaluation protocol, and failure cases in the full paper before relying on this as key evidence."], "bibkey": "Huang2024Mitigating"}
{"paper_id": "P0286", "title": "Multi-Agent Causal Discovery Using Large Language Models", "year": 2024, "url": "http://arxiv.org/abs/2407.15073v3", "arxiv_id": "2407.15073v3", "primary_category": "cs.AI", "categories": ["cs.AI", "cs.CL"], "pdf_url": "https://arxiv.org/pdf/2407.15073v3", "priority": "normal", "mapped_sections": ["5.2"], "evidence_level": "abstract", "fulltext_path": "", "authors": ["Hao Duong Le", "Xin Xia", "Zhang Chen"], "abstract": "Causal discovery aims to identify causal relationships between variables and is a critical research area in machine learning. Traditional methods focus on statistical or machine learning algorithms to uncover causal links from structured data, often overlooking the valuable contextual information provided by metadata. Large language models (LLMs) have shown promise in creating unified causal discovery frameworks by incorporating both structured data and metadata. However, their potential in multi-agent settings remains largely unexplored. To address this gap, we introduce the Multi-Agent Causal Discovery Framework (MAC), which consists of two key modules: the Debate-Coding Module (DCM) and the Meta-Debate Module (MDM). The DCM begins with a multi-agent debating and coding process, where agents use both structured data and metadata to collaboratively select the most suitable statistical causal discovery (SCD) method. The selected SCD is then applied to the structured data to generate an initial causal graph. This causal graph is transformed into causal metadata through the Meta Fusion mechanism. With all the metadata, MDM then refines the causal structure by leveraging a multi-agent debating framework. Extensive experiments across five datasets demonstrate that MAC outperforms both traditional statistical causal discovery methods and existing LLM-based approaches, achieving state-of-the-art performance.", "summary_bullets": ["Causal discovery aims to identify causal relationships between variables and is a critical research area in machine learning.", "Traditional methods focus on statistical or machine learning algorithms to uncover causal links from structured data, often overlooking the valuable contextual information provided by metadata.", "Large language models (LLMs) have shown promise in creating unified causal discovery frameworks by incorporating both structured data and metadata."], "method": "To address this gap, we introduce the Multi-Agent Causal Discovery Framework (MAC), which consists of two key modules: the Debate-Coding Module (DCM) and the Meta-Debate Module (MDM).", "key_results": ["Extensive experiments across five datasets demonstrate that MAC outperforms both traditional statistical causal discovery methods and existing LLM-based approaches, achieving state-of-the-art performance."], "limitations": ["Abstract-level evidence only: validate assumptions, evaluation protocol, and failure cases in the full paper before relying on this as key evidence."], "bibkey": "Le2024Multi"}
{"paper_id": "P0287", "title": "RedAgent: Red Teaming Large Language Models with Context-aware Autonomous Language Agent", "year": 2024, "url": "http://arxiv.org/abs/2407.16667v1", "arxiv_id": "2407.16667v1", "primary_category": "cs.CR", "categories": ["cs.CR", "cs.AI", "cs.CL"], "pdf_url": "https://arxiv.org/pdf/2407.16667v1", "priority": "normal", "mapped_sections": [], "evidence_level": "abstract", "fulltext_path": "", "authors": ["Huiyu Xu", "Wenhui Zhang", "Zhibo Wang", "Feng Xiao", "Rui Zheng", "Yunhe Feng", "Zhongjie Ba", "Kui Ren"], "abstract": "Recently, advanced Large Language Models (LLMs) such as GPT-4 have been integrated into many real-world applications like Code Copilot. These applications have significantly expanded the attack surface of LLMs, exposing them to a variety of threats. Among them, jailbreak attacks that induce toxic responses through jailbreak prompts have raised critical safety concerns. To identify these threats, a growing number of red teaming approaches simulate potential adversarial scenarios by crafting jailbreak prompts to test the target LLM. However, existing red teaming methods do not consider the unique vulnerabilities of LLM in different scenarios, making it difficult to adjust the jailbreak prompts to find context-specific vulnerabilities. Meanwhile, these methods are limited to refining jailbreak templates using a few mutation operations, lacking the automation and scalability to adapt to different scenarios. To enable context-aware and efficient red teaming, we abstract and model existing attacks into a coherent concept called \"jailbreak strategy\" and propose a multi-agent LLM system named RedAgent that leverages these strategies to generate context-aware jailbreak prompts. By self-reflecting on contextual feedback in an additional memory buffer, RedAgent continuously learns how to leverage these strategies to achieve effective jailbreaks in specific contexts. Extensive experiments demonstrate that our system can jailbreak most black-box LLMs in just five queries, improving the efficiency of existing red teaming methods by two times. Additionally, RedAgent can jailbreak customized LLM applications more efficiently. By generating context-aware jailbreak prompts towards applications on GPTs, we discover 60 severe vulnerabilities of these real-world applications with only two queries per vulnerability. We have reported all found issues and communicated with OpenAI and Meta for bug fixes.", "summary_bullets": ["Recently, advanced Large Language Models (LLMs) such as GPT-4 have been integrated into many real-world applications like Code Copilot.", "These applications have significantly expanded the attack surface of LLMs, exposing them to a variety of threats.", "Among them, jailbreak attacks that induce toxic responses through jailbreak prompts have raised critical safety concerns."], "method": "Recently, advanced Large Language Models (LLMs) such as GPT-4 have been integrated into many real-world applications like Code Copilot.", "key_results": ["Recently, advanced Large Language Models (LLMs) such as GPT-4 have been integrated into many real-world applications like Code Copilot.", "By generating context-aware jailbreak prompts towards applications on GPTs, we discover 60 severe vulnerabilities of these real-world applications with only two queries per vulnerability."], "limitations": ["Abstract-level evidence only: validate assumptions, evaluation protocol, and failure cases in the full paper before relying on this as key evidence."], "bibkey": "Xu2024Redagent"}
{"paper_id": "P0288", "title": "Rx Strategist: Prescription Verification using LLM Agents System", "year": 2024, "url": "http://arxiv.org/abs/2409.03440v1", "arxiv_id": "2409.03440v1", "primary_category": "cs.CL", "categories": ["cs.CL"], "pdf_url": "https://arxiv.org/pdf/2409.03440v1", "priority": "normal", "mapped_sections": [], "evidence_level": "abstract", "fulltext_path": "", "authors": ["Phuc Phan Van", "Dat Nguyen Minh", "An Dinh Ngoc", "Huy Phan Thanh"], "abstract": "To protect patient safety, modern pharmaceutical complexity demands strict prescription verification. We offer a new approach - Rx Strategist - that makes use of knowledge graphs and different search strategies to enhance the power of Large Language Models (LLMs) inside an agentic framework. This multifaceted technique allows for a multi-stage LLM pipeline and reliable information retrieval from a custom-built active ingredient database. Different facets of prescription verification, such as indication, dose, and possible drug interactions, are covered in each stage of the pipeline. We alleviate the drawbacks of monolithic LLM techniques by spreading reasoning over these stages, improving correctness and reliability while reducing memory demands. Our findings demonstrate that Rx Strategist surpasses many current LLMs, achieving performance comparable to that of a highly experienced clinical pharmacist. In the complicated world of modern medications, this combination of LLMs with organized knowledge and sophisticated search methods presents a viable avenue for reducing prescription errors and enhancing patient outcomes.", "summary_bullets": ["To protect patient safety, modern pharmaceutical complexity demands strict prescription verification.", "We offer a new approach - Rx Strategist - that makes use of knowledge graphs and different search strategies to enhance the power of Large Language Models (LLMs) inside an agentic framework.", "This multifaceted technique allows for a multi-stage LLM pipeline and reliable information retrieval from a custom-built active ingredient database."], "method": "To protect patient safety, modern pharmaceutical complexity demands strict prescription verification.", "key_results": ["In the complicated world of modern medications, this combination of LLMs with organized knowledge and sophisticated search methods presents a viable avenue for reducing prescription errors and enhancing patient outcomes."], "limitations": ["Abstract-level evidence only: validate assumptions, evaluation protocol, and failure cases in the full paper before relying on this as key evidence."], "bibkey": "Van2024Strategist"}
{"paper_id": "P0289", "title": "Survey on Large Language Model-Enhanced Reinforcement Learning: Concept, Taxonomy, and Methods", "year": 2024, "url": "http://arxiv.org/abs/2404.00282v3", "arxiv_id": "2404.00282v3", "primary_category": "cs.LG", "categories": ["cs.LG", "cs.AI", "cs.CL", "cs.RO"], "pdf_url": "https://arxiv.org/pdf/2404.00282v3", "priority": "normal", "mapped_sections": [], "evidence_level": "abstract", "fulltext_path": "", "authors": ["Yuji Cao", "Huan Zhao", "Yuheng Cheng", "Ting Shu", "Yue Chen", "Guolong Liu", "Gaoqi Liang", "Junhua Zhao", "Jinyue Yan", "Yun Li"], "abstract": "With extensive pre-trained knowledge and high-level general capabilities, large language models (LLMs) emerge as a promising avenue to augment reinforcement learning (RL) in aspects such as multi-task learning, sample efficiency, and high-level task planning. In this survey, we provide a comprehensive review of the existing literature in LLM-enhanced RL and summarize its characteristics compared to conventional RL methods, aiming to clarify the research scope and directions for future studies. Utilizing the classical agent-environment interaction paradigm, we propose a structured taxonomy to systematically categorize LLMs' functionalities in RL, including four roles: information processor, reward designer, decision-maker, and generator. For each role, we summarize the methodologies, analyze the specific RL challenges that are mitigated, and provide insights into future directions. Lastly, a comparative analysis of each role, potential applications, prospective opportunities, and challenges of the LLM-enhanced RL are discussed. By proposing this taxonomy, we aim to provide a framework for researchers to effectively leverage LLMs in the RL field, potentially accelerating RL applications in complex applications such as robotics, autonomous driving, and energy systems.", "summary_bullets": ["With extensive pre-trained knowledge and high-level general capabilities, large language models (LLMs) emerge as a promising avenue to augment reinforcement learning (RL) in aspects such as multi-task learning, sample efficiency, and high-level task planning.", "In this survey, we provide a comprehensive review of the existing literature in LLM-enhanced RL and summarize its characteristics compared to conventional RL methods, aiming to clarify the research scope and directions for future studies.", "Utilizing the classical agent-environment interaction paradigm, we propose a structured taxonomy to systematically categorize LLMs' functionalities in RL, including four roles: information processor, reward designer, decision-maker, and generator."], "method": "Utilizing the classical agent-environment interaction paradigm, we propose a structured taxonomy to systematically categorize LLMs' functionalities in RL, including four roles: information processor, reward designer, decision-maker, and generator.", "key_results": ["By proposing this taxonomy, we aim to provide a framework for researchers to effectively leverage LLMs in the RL field, potentially accelerating RL applications in complex applications such as robotics, autonomous driving, and energy systems."], "limitations": ["Abstract-level evidence only: validate assumptions, evaluation protocol, and failure cases in the full paper before relying on this as key evidence."], "bibkey": "Cao2024Survey"}
{"paper_id": "P0290", "title": "The Emerged Security and Privacy of LLM Agent: A Survey with Case Studies", "year": 2024, "url": "http://arxiv.org/abs/2407.19354v2", "arxiv_id": "2407.19354v2", "primary_category": "cs.CR", "categories": ["cs.CR"], "pdf_url": "https://arxiv.org/pdf/2407.19354v2", "priority": "normal", "mapped_sections": ["6.2"], "evidence_level": "abstract", "fulltext_path": "", "authors": ["Feng He", "Tianqing Zhu", "Dayong Ye", "Bo Liu", "Wanlei Zhou", "Philip S. Yu"], "abstract": "Inspired by the rapid development of Large Language Models (LLMs), LLM agents have evolved to perform complex tasks. LLM agents are now extensively applied across various domains, handling vast amounts of data to interact with humans and execute tasks. The widespread applications of LLM agents demonstrate their significant commercial value; however, they also expose security and privacy vulnerabilities. At the current stage, comprehensive research on the security and privacy of LLM agents is highly needed. This survey aims to provide a comprehensive overview of the newly emerged privacy and security issues faced by LLM agents. We begin by introducing the fundamental knowledge of LLM agents, followed by a categorization and analysis of the threats. We then discuss the impacts of these threats on humans, environment, and other agents. Subsequently, we review existing defensive strategies, and finally explore future trends. Additionally, the survey incorporates diverse case studies to facilitate a more accessible understanding. By highlighting these critical security and privacy issues, the survey seeks to stimulate future research towards enhancing the security and privacy of LLM agents, thereby increasing their reliability and trustworthiness in future applications.", "summary_bullets": ["Inspired by the rapid development of Large Language Models (LLMs), LLM agents have evolved to perform complex tasks.", "LLM agents are now extensively applied across various domains, handling vast amounts of data to interact with humans and execute tasks.", "The widespread applications of LLM agents demonstrate their significant commercial value; however, they also expose security and privacy vulnerabilities."], "method": "Inspired by the rapid development of Large Language Models (LLMs), LLM agents have evolved to perform complex tasks.", "key_results": ["By highlighting these critical security and privacy issues, the survey seeks to stimulate future research towards enhancing the security and privacy of LLM agents, thereby increasing their reliability and trustworthiness in future applications."], "limitations": ["Abstract-level evidence only: validate assumptions, evaluation protocol, and failure cases in the full paper before relying on this as key evidence."], "bibkey": "He2024Emerged"}
{"paper_id": "P0291", "title": "Towards Efficient LLM Grounding for Embodied Multi-Agent Collaboration", "year": 2024, "url": "http://arxiv.org/abs/2405.14314v4", "arxiv_id": "2405.14314v4", "primary_category": "cs.AI", "categories": ["cs.AI", "cs.CL", "cs.LG", "cs.MA", "cs.RO"], "pdf_url": "https://arxiv.org/pdf/2405.14314v4", "priority": "normal", "mapped_sections": ["5.2"], "evidence_level": "abstract", "fulltext_path": "", "authors": ["Yang Zhang", "Shixin Yang", "Chenjia Bai", "Fei Wu", "Xiu Li", "Zhen Wang", "Xuelong Li"], "abstract": "Grounding the reasoning ability of large language models (LLMs) for embodied tasks is challenging due to the complexity of the physical world. Especially, LLM planning for multi-agent collaboration requires communication of agents or credit assignment as the feedback to re-adjust the proposed plans and achieve effective coordination. However, existing methods that overly rely on physical verification or self-reflection suffer from excessive and inefficient querying of LLMs. In this paper, we propose a novel framework for multi-agent collaboration that introduces Reinforced Advantage feedback (ReAd) for efficient self-refinement of plans. Specifically, we perform critic regression to learn a sequential advantage function from LLM-planned data, and then treat the LLM planner as an optimizer to generate actions that maximize the advantage function. It endows the LLM with the foresight to discern whether the action contributes to accomplishing the final task. We provide theoretical analysis by extending advantage-weighted regression in reinforcement learning to multi-agent systems. Experiments on Overcooked-AI and a difficult variant of RoCoBench show that ReAd surpasses baselines in success rate, and also significantly decreases the interaction steps of agents and query rounds of LLMs, demonstrating its high efficiency for grounding LLMs. More results are given at https://embodied-read.github.io", "summary_bullets": ["Grounding the reasoning ability of large language models (LLMs) for embodied tasks is challenging due to the complexity of the physical world.", "Especially, LLM planning for multi-agent collaboration requires communication of agents or credit assignment as the feedback to re-adjust the proposed plans and achieve effective coordination.", "However, existing methods that overly rely on physical verification or self-reflection suffer from excessive and inefficient querying of LLMs."], "method": "In this paper, we propose a novel framework for multi-agent collaboration that introduces Reinforced Advantage feedback (ReAd) for efficient self-refinement of plans.", "key_results": ["Experiments on Overcooked-AI and a difficult variant of RoCoBench show that ReAd surpasses baselines in success rate, and also significantly decreases the interaction steps of agents and query rounds of LLMs, demonstrating its high efficiency for grounding LLMs."], "limitations": ["Abstract-level evidence only: validate assumptions, evaluation protocol, and failure cases in the full paper before relying on this as key evidence."], "bibkey": "Zhang2024Towards"}
{"paper_id": "P0292", "title": "Transforming Wearable Data into Personal Health Insights using Large Language Model Agents", "year": 2024, "url": "http://arxiv.org/abs/2406.06464v4", "arxiv_id": "2406.06464v4", "primary_category": "cs.AI", "categories": ["cs.AI", "cs.CL"], "pdf_url": "https://arxiv.org/pdf/2406.06464v4", "priority": "normal", "mapped_sections": [], "evidence_level": "abstract", "fulltext_path": "", "authors": ["Mike A. Merrill", "Akshay Paruchuri", "Naghmeh Rezaei", "Geza Kovacs", "Javier Perez", "Yun Liu", "Erik Schenck", "Nova Hammerquist", "Jake Sunshine", "Shyam Tailor", "Kumar Ayush", "Hao-Wei Su", "Qian He", "Cory Y. McLean", "Mark Malhotra", "Shwetak Patel", "Jiening Zhan", "Tim Althoff", "Daniel McDuff", "Xin Liu"], "abstract": "Deriving personalized insights from popular wearable trackers requires complex numerical reasoning that challenges standard LLMs, necessitating tool-based approaches like code generation. Large language model (LLM) agents present a promising yet largely untapped solution for this analysis at scale. We introduce the Personal Health Insights Agent (PHIA), a system leveraging multistep reasoning with code generation and information retrieval to analyze and interpret behavioral health data. To test its capabilities, we create and share two benchmark datasets with over 4000 health insights questions. A 650-hour human expert evaluation shows that PHIA significantly outperforms a strong code generation baseline, achieving 84% accuracy on objective, numerical questions and, for open-ended ones, earning 83% favorable ratings while being twice as likely to achieve the highest quality rating. This work can advance behavioral health by empowering individuals to understand their data, enabling a new era of accessible, personalized, and data-driven wellness for the wider population.", "summary_bullets": ["Deriving personalized insights from popular wearable trackers requires complex numerical reasoning that challenges standard LLMs, necessitating tool-based approaches like code generation.", "Large language model (LLM) agents present a promising yet largely untapped solution for this analysis at scale.", "We introduce the Personal Health Insights Agent (PHIA), a system leveraging multistep reasoning with code generation and information retrieval to analyze and interpret behavioral health data."], "method": "We introduce the Personal Health Insights Agent (PHIA), a system leveraging multistep reasoning with code generation and information retrieval to analyze and interpret behavioral health data.", "key_results": ["To test its capabilities, we create and share two benchmark datasets with over 4000 health insights questions.", "A 650-hour human expert evaluation shows that PHIA significantly outperforms a strong code generation baseline, achieving 84% accuracy on objective, numerical questions and, for open-ended ones, earning 83% favorable ratings while being twice as likely to achieve the highest quality rating."], "limitations": ["Abstract-level evidence only: validate assumptions, evaluation protocol, and failure cases in the full paper before relying on this as key evidence."], "bibkey": "Merrill2024Transforming"}
{"paper_id": "P0293", "title": "TwoStep: Multi-agent Task Planning using Classical Planners and Large Language Models", "year": 2024, "url": "http://arxiv.org/abs/2403.17246v2", "arxiv_id": "2403.17246v2", "primary_category": "cs.AI", "categories": ["cs.AI", "cs.CL", "cs.MA", "cs.RO"], "pdf_url": "https://arxiv.org/pdf/2403.17246v2", "priority": "normal", "mapped_sections": ["4.1"], "evidence_level": "abstract", "fulltext_path": "", "authors": ["David Bai", "Ishika Singh", "David Traum", "Jesse Thomason"], "abstract": "Classical planning formulations like the Planning Domain Definition Language (PDDL) admit action sequences guaranteed to achieve a goal state given an initial state if any are possible. However, reasoning problems defined in PDDL do not capture temporal aspects of action taking, such as concurrent actions between two agents when there are no conflicting conditions, without significant modification and definition to existing PDDL domains. A human expert aware of such constraints can decompose a goal into subgoals, each reachable through single agent planning, to take advantage of simultaneous actions. In contrast to classical planning, large language models (LLMs) directly used for inferring plan steps rarely guarantee execution success, but are capable of leveraging commonsense reasoning to assemble action sequences. We combine the strengths of both classical planning and LLMs by approximating human intuitions for multi-agent planning goal decomposition. We demonstrate that LLM-based goal decomposition leads to faster planning times than solving multi-agent PDDL problems directly while simultaneously achieving fewer plan execution steps than a single agent plan alone, as well as most multiagent plans, while guaranteeing execution success. Additionally, we find that LLM-based approximations of subgoals result in similar multi-agent execution lengths to those specified by human experts. Website and resources at https://glamor-usc.github.io/twostep", "summary_bullets": ["Classical planning formulations like the Planning Domain Definition Language (PDDL) admit action sequences guaranteed to achieve a goal state given an initial state if any are possible.", "However, reasoning problems defined in PDDL do not capture temporal aspects of action taking, such as concurrent actions between two agents when there are no conflicting conditions, without significant modification and definition to existing PDDL domains.", "A human expert aware of such constraints can decompose a goal into subgoals, each reachable through single agent planning, to take advantage of simultaneous actions."], "method": "Classical planning formulations like the Planning Domain Definition Language (PDDL) admit action sequences guaranteed to achieve a goal state given an initial state if any are possible.", "key_results": ["A human expert aware of such constraints can decompose a goal into subgoals, each reachable through single agent planning, to take advantage of simultaneous actions.", "In contrast to classical planning, large language models (LLMs) directly used for inferring plan steps rarely guarantee execution success, but are capable of leveraging commonsense reasoning to assemble action sequences."], "limitations": ["Abstract-level evidence only: validate assumptions, evaluation protocol, and failure cases in the full paper before relying on this as key evidence."], "bibkey": "Bai2024Twostep"}
{"paper_id": "P0294", "title": "Why Solving Multi-agent Path Finding with Large Language Model has not Succeeded Yet", "year": 2024, "url": "http://arxiv.org/abs/2401.03630v2", "arxiv_id": "2401.03630v2", "primary_category": "cs.MA", "categories": ["cs.MA", "cs.AI", "cs.CL"], "pdf_url": "https://arxiv.org/pdf/2401.03630v2", "priority": "normal", "mapped_sections": ["5.2"], "evidence_level": "abstract", "fulltext_path": "", "authors": ["Weizhe Chen", "Sven Koenig", "Bistra Dilkina"], "abstract": "With the explosive influence caused by the success of large language models (LLM) like ChatGPT and GPT-4, there has been an extensive amount of recent work showing that foundation models can be used to solve a large variety of tasks. However, there is very limited work that shares insights on multi-agent planning. Multi-agent planning is different from other domains by combining the difficulty of multi-agent coordination and planning, and making it hard to leverage external tools to facilitate the reasoning needed. In this paper, we focus on the problem of multi-agent path finding (MAPF), which is also known as multi-robot route planning, and study the performance of solving MAPF with LLMs. We first show the motivating success on an empty room map without obstacles, then the failure to plan on the harder room map and maze map of the standard MAPF benchmark. We present our position on why directly solving MAPF with LLMs has not been successful yet, and we use various experiments to support our hypothesis. Based on our results, we discussed how researchers with different backgrounds could help with this problem from different perspectives.", "summary_bullets": ["With the explosive influence caused by the success of large language models (LLM) like ChatGPT and GPT-4, there has been an extensive amount of recent work showing that foundation models can be used to solve a large variety of tasks.", "However, there is very limited work that shares insights on multi-agent planning.", "Multi-agent planning is different from other domains by combining the difficulty of multi-agent coordination and planning, and making it hard to leverage external tools to facilitate the reasoning needed."], "method": "We present our position on why directly solving MAPF with LLMs has not been successful yet, and we use various experiments to support our hypothesis.", "key_results": ["With the explosive influence caused by the success of large language models (LLM) like ChatGPT and GPT-4, there has been an extensive amount of recent work showing that foundation models can be used to solve a large variety of tasks.", "We first show the motivating success on an empty room map without obstacles, then the failure to plan on the harder room map and maze map of the standard MAPF benchmark."], "limitations": ["Abstract-level evidence only: validate assumptions, evaluation protocol, and failure cases in the full paper before relying on this as key evidence."], "bibkey": "Chen2024Solving"}
{"paper_id": "P0295", "title": "A New Dialogue Response Generation Agent for Large Language Models by Asking Questions to Detect User's Intentions", "year": 2023, "url": "http://arxiv.org/abs/2310.03293v1", "arxiv_id": "2310.03293v1", "primary_category": "cs.CL", "categories": ["cs.CL"], "pdf_url": "https://arxiv.org/pdf/2310.03293v1", "priority": "normal", "mapped_sections": [], "evidence_level": "abstract", "fulltext_path": "", "authors": ["Siwei Wu", "Xiangqing Shen", "Rui Xia"], "abstract": "Large Language Models (LLMs), such as ChatGPT, have recently been applied to various NLP tasks due to its open-domain generation capabilities. However, there are two issues with applying LLMs to dialogue tasks. 1. During the dialogue process, users may have implicit intentions that might be overlooked by LLMs. Consequently, generated responses couldn't align with the user's intentions. 2. It is unlikely for LLMs to encompass all fields comprehensively. In certain specific domains, their knowledge may be incomplete, and LLMs cannot update the latest knowledge in real-time. To tackle these issues, we propose a framework~\\emph{using LLM to \\textbf{E}nhance dialogue response generation by asking questions to \\textbf{D}etect user's \\textbf{I}mplicit in\\textbf{T}entions} (\\textbf{EDIT}). Firstly, EDIT generates open questions related to the dialogue context as the potential user's intention; Then, EDIT answers those questions by interacting with LLMs and searching in domain-specific knowledge bases respectively, and use LLMs to choose the proper answers to questions as extra knowledge; Finally, EDIT enhances response generation by explicitly integrating those extra knowledge. Besides, previous question generation works only focus on asking questions with answers in context. In order to ask open questions, we construct a Context-Open-Question (COQ) dataset. On two task-oriented dialogue tasks (Wizard of Wikipedia and Holl-E), EDIT outperformed other LLMs.", "summary_bullets": ["Large Language Models (LLMs), such as ChatGPT, have recently been applied to various NLP tasks due to its open-domain generation capabilities.", "However, there are two issues with applying LLMs to dialogue tasks.", "1."], "method": "To tackle these issues, we propose a framework~\\emph{using LLM to \\textbf{E}nhance dialogue response generation by asking questions to \\textbf{D}etect user's \\textbf{I}mplicit in\\textbf{T}entions} (\\textbf{EDIT}).", "key_results": ["In order to ask open questions, we construct a Context-Open-Question (COQ) dataset."], "limitations": ["Abstract-level evidence only: validate assumptions, evaluation protocol, and failure cases in the full paper before relying on this as key evidence."], "bibkey": "Wu2023Dialogue"}
{"paper_id": "P0296", "title": "A Survey on Large Language Model based Autonomous Agents", "year": 2023, "url": "http://arxiv.org/abs/2308.11432v7", "arxiv_id": "2308.11432v7", "primary_category": "cs.AI", "categories": ["cs.AI", "cs.CL"], "pdf_url": "https://arxiv.org/pdf/2308.11432v7", "priority": "normal", "mapped_sections": [], "evidence_level": "abstract", "fulltext_path": "", "authors": ["Lei Wang", "Chen Ma", "Xueyang Feng", "Zeyu Zhang", "Hao Yang", "Jingsen Zhang", "Zhiyuan Chen", "Jiakai Tang", "Xu Chen", "Yankai Lin", "Wayne Xin Zhao", "Zhewei Wei", "Ji-Rong Wen"], "abstract": "Autonomous agents have long been a prominent research focus in both academic and industry communities. Previous research in this field often focuses on training agents with limited knowledge within isolated environments, which diverges significantly from human learning processes, and thus makes the agents hard to achieve human-like decisions. Recently, through the acquisition of vast amounts of web knowledge, large language models (LLMs) have demonstrated remarkable potential in achieving human-level intelligence. This has sparked an upsurge in studies investigating LLM-based autonomous agents. In this paper, we present a comprehensive survey of these studies, delivering a systematic review of the field of LLM-based autonomous agents from a holistic perspective. More specifically, we first discuss the construction of LLM-based autonomous agents, for which we propose a unified framework that encompasses a majority of the previous work. Then, we present a comprehensive overview of the diverse applications of LLM-based autonomous agents in the fields of social science, natural science, and engineering. Finally, we delve into the evaluation strategies commonly used for LLM-based autonomous agents. Based on the previous studies, we also present several challenges and future directions in this field. To keep track of this field and continuously update our survey, we maintain a repository of relevant references at https://github.com/Paitesanshi/LLM-Agent-Survey.", "summary_bullets": ["Autonomous agents have long been a prominent research focus in both academic and industry communities.", "Previous research in this field often focuses on training agents with limited knowledge within isolated environments, which diverges significantly from human learning processes, and thus makes the agents hard to achieve human-like decisions.", "Recently, through the acquisition of vast amounts of web knowledge, large language models (LLMs) have demonstrated remarkable potential in achieving human-level intelligence."], "method": "In this paper, we present a comprehensive survey of these studies, delivering a systematic review of the field of LLM-based autonomous agents from a holistic perspective.", "key_results": ["Previous research in this field often focuses on training agents with limited knowledge within isolated environments, which diverges significantly from human learning processes, and thus makes the agents hard to achieve human-like decisions.", "Recently, through the acquisition of vast amounts of web knowledge, large language models (LLMs) have demonstrated remarkable potential in achieving human-level intelligence."], "limitations": ["Abstract-level evidence only: validate assumptions, evaluation protocol, and failure cases in the full paper before relying on this as key evidence."], "bibkey": "Wang2023Survey"}
{"paper_id": "P0297", "title": "AVIS: Autonomous Visual Information Seeking with Large Language Model Agent", "year": 2023, "url": "http://arxiv.org/abs/2306.08129v3", "arxiv_id": "2306.08129v3", "primary_category": "cs.CV", "categories": ["cs.CV", "cs.AI", "cs.CL"], "pdf_url": "https://arxiv.org/pdf/2306.08129v3", "priority": "normal", "mapped_sections": ["3.1", "6.1"], "evidence_level": "abstract", "fulltext_path": "", "authors": ["Ziniu Hu", "Ahmet Iscen", "Chen Sun", "Kai-Wei Chang", "Yizhou Sun", "David A Ross", "Cordelia Schmid", "Alireza Fathi"], "abstract": "In this paper, we propose an autonomous information seeking visual question answering framework, AVIS. Our method leverages a Large Language Model (LLM) to dynamically strategize the utilization of external tools and to investigate their outputs, thereby acquiring the indispensable knowledge needed to provide answers to the posed questions. Responding to visual questions that necessitate external knowledge, such as \"What event is commemorated by the building depicted in this image?\", is a complex task. This task presents a combinatorial search space that demands a sequence of actions, including invoking APIs, analyzing their responses, and making informed decisions. We conduct a user study to collect a variety of instances of human decision-making when faced with this task. This data is then used to design a system comprised of three components: an LLM-powered planner that dynamically determines which tool to use next, an LLM-powered reasoner that analyzes and extracts key information from the tool outputs, and a working memory component that retains the acquired information throughout the process. The collected user behavior serves as a guide for our system in two key ways. First, we create a transition graph by analyzing the sequence of decisions made by users. This graph delineates distinct states and confines the set of actions available at each state. Second, we use examples of user decision-making to provide our LLM-powered planner and reasoner with relevant contextual instances, enhancing their capacity to make informed decisions. We show that AVIS achieves state-of-the-art results on knowledge-intensive visual question answering benchmarks such as Infoseek and OK-VQA.", "summary_bullets": ["In this paper, we propose an autonomous information seeking visual question answering framework, AVIS.", "Our method leverages a Large Language Model (LLM) to dynamically strategize the utilization of external tools and to investigate their outputs, thereby acquiring the indispensable knowledge needed to provide answers to the posed questions.", "Responding to visual questions that necessitate external knowledge, such as \"What event is commemorated by the building depicted in this image?\", is a complex task."], "method": "In this paper, we propose an autonomous information seeking visual question answering framework, AVIS.", "key_results": ["We show that AVIS achieves state-of-the-art results on knowledge-intensive visual question answering benchmarks such as Infoseek and OK-VQA.", "We conduct a user study to collect a variety of instances of human decision-making when faced with this task."], "limitations": ["Abstract-level evidence only: validate assumptions, evaluation protocol, and failure cases in the full paper before relying on this as key evidence."], "bibkey": "Hu2023Avis"}
{"paper_id": "P0298", "title": "Chat with the Environment: Interactive Multimodal Perception Using Large Language Models", "year": 2023, "url": "http://arxiv.org/abs/2303.08268v3", "arxiv_id": "2303.08268v3", "primary_category": "cs.RO", "categories": ["cs.RO", "cs.AI", "cs.CL", "cs.LG", "cs.SD", "eess.AS"], "pdf_url": "https://arxiv.org/pdf/2303.08268v3", "priority": "normal", "mapped_sections": [], "evidence_level": "abstract", "fulltext_path": "", "authors": ["Xufeng Zhao", "Mengdi Li", "Cornelius Weber", "Muhammad Burhan Hafez", "Stefan Wermter"], "abstract": "Programming robot behavior in a complex world faces challenges on multiple levels, from dextrous low-level skills to high-level planning and reasoning. Recent pre-trained Large Language Models (LLMs) have shown remarkable reasoning ability in few-shot robotic planning. However, it remains challenging to ground LLMs in multimodal sensory input and continuous action output, while enabling a robot to interact with its environment and acquire novel information as its policies unfold. We develop a robot interaction scenario with a partially observable state, which necessitates a robot to decide on a range of epistemic actions in order to sample sensory information among multiple modalities, before being able to execute the task correctly. Matcha (Multimodal environment chatting) agent, an interactive perception framework, is therefore proposed with an LLM as its backbone, whose ability is exploited to instruct epistemic actions and to reason over the resulting multimodal sensations (vision, sound, haptics, proprioception), as well as to plan an entire task execution based on the interactively acquired information. Our study demonstrates that LLMs can provide high-level planning and reasoning skills and control interactive robot behavior in a multimodal environment, while multimodal modules with the context of the environmental state help ground the LLMs and extend their processing ability. The project website can be found at https://matcha-agent.github.io.", "summary_bullets": ["Programming robot behavior in a complex world faces challenges on multiple levels, from dextrous low-level skills to high-level planning and reasoning.", "Recent pre-trained Large Language Models (LLMs) have shown remarkable reasoning ability in few-shot robotic planning.", "However, it remains challenging to ground LLMs in multimodal sensory input and continuous action output, while enabling a robot to interact with its environment and acquire novel information as its policies unfold."], "method": "We develop a robot interaction scenario with a partially observable state, which necessitates a robot to decide on a range of epistemic actions in order to sample sensory information among multiple modalities, before being able to execute the task correctly.", "key_results": ["The project website can be found at https://matcha-agent.github.io."], "limitations": ["Abstract-level evidence only: validate assumptions, evaluation protocol, and failure cases in the full paper before relying on this as key evidence."], "bibkey": "Zhao2023Chat"}
{"paper_id": "P0299", "title": "FinEval: A Chinese Financial Domain Knowledge Evaluation Benchmark for Large Language Models", "year": 2023, "url": "http://arxiv.org/abs/2308.09975v2", "arxiv_id": "2308.09975v2", "primary_category": "cs.CL", "categories": ["cs.CL"], "pdf_url": "https://arxiv.org/pdf/2308.09975v2", "priority": "normal", "mapped_sections": [], "evidence_level": "abstract", "fulltext_path": "", "authors": ["Xin Guo", "Haotian Xia", "Zhaowei Liu", "Hanyang Cao", "Zhi Yang", "Zhiqiang Liu", "Sizhe Wang", "Jinyi Niu", "Chuqi Wang", "Yanhui Wang", "Xiaolong Liang", "Xiaoming Huang", "Bing Zhu", "Zhongyu Wei", "Yun Chen", "Weining Shen", "Liwen Zhang"], "abstract": "Large language models have demonstrated outstanding performance in various natural language processing tasks, but their security capabilities in the financial domain have not been explored, and their performance on complex tasks like financial agent remains unknown. This paper presents FinEval, a benchmark designed to evaluate LLMs' financial domain knowledge and practical abilities. The dataset contains 8,351 questions categorized into four different key areas: Financial Academic Knowledge, Financial Industry Knowledge, Financial Security Knowledge, and Financial Agent. Financial Academic Knowledge comprises 4,661 multiple-choice questions spanning 34 subjects such as finance and economics. Financial Industry Knowledge contains 1,434 questions covering practical scenarios like investment research. Financial Security Knowledge assesses models through 1,640 questions on topics like application security and cryptography. Financial Agent evaluates tool usage and complex reasoning with 616 questions. FinEval has multiple evaluation settings, including zero-shot, five-shot with chain-of-thought, and assesses model performance using objective and subjective criteria. Our results show that Claude 3.5-Sonnet achieves the highest weighted average score of 72.9 across all financial domain categories under zero-shot setting. Our work provides a comprehensive benchmark closely aligned with Chinese financial domain.", "summary_bullets": ["Large language models have demonstrated outstanding performance in various natural language processing tasks, but their security capabilities in the financial domain have not been explored, and their performance on complex tasks like financial agent remains unknown.", "This paper presents FinEval, a benchmark designed to evaluate LLMs' financial domain knowledge and practical abilities.", "The dataset contains 8,351 questions categorized into four different key areas: Financial Academic Knowledge, Financial Industry Knowledge, Financial Security Knowledge, and Financial Agent."], "method": "Large language models have demonstrated outstanding performance in various natural language processing tasks, but their security capabilities in the financial domain have not been explored, and their performance on complex tasks like financial agent remains unknown.", "key_results": ["The dataset contains 8,351 questions categorized into four different key areas: Financial Academic Knowledge, Financial Industry Knowledge, Financial Security Knowledge, and Financial Agent.", "Financial Academic Knowledge comprises 4,661 multiple-choice questions spanning 34 subjects such as finance and economics."], "limitations": ["Abstract-level evidence only: validate assumptions, evaluation protocol, and failure cases in the full paper before relying on this as key evidence."], "bibkey": "Guo2023Fineval"}
{"paper_id": "P0300", "title": "FinMem: A Performance-Enhanced LLM Trading Agent with Layered Memory and Character Design", "year": 2023, "url": "http://arxiv.org/abs/2311.13743v2", "arxiv_id": "2311.13743v2", "primary_category": "q-fin.CP", "categories": ["q-fin.CP", "cs.AI", "cs.CE", "cs.LG"], "pdf_url": "https://arxiv.org/pdf/2311.13743v2", "priority": "high", "mapped_sections": ["3.1", "4.2", "5.1"], "evidence_level": "abstract", "fulltext_path": "", "authors": ["Yangyang Yu", "Haohang Li", "Zhi Chen", "Yuechen Jiang", "Yang Li", "Denghui Zhang", "Rong Liu", "Jordan W. Suchow", "Khaldoun Khashanah"], "abstract": "Recent advancements in Large Language Models (LLMs) have exhibited notable efficacy in question-answering (QA) tasks across diverse domains. Their prowess in integrating extensive web knowledge has fueled interest in developing LLM-based autonomous agents. While LLMs are efficient in decoding human instructions and deriving solutions by holistically processing historical inputs, transitioning to purpose-driven agents requires a supplementary rational architecture to process multi-source information, establish reasoning chains, and prioritize critical tasks. Addressing this, we introduce \\textsc{FinMem}, a novel LLM-based agent framework devised for financial decision-making. It encompasses three core modules: Profiling, to customize the agent's characteristics; Memory, with layered message processing, to aid the agent in assimilating hierarchical financial data; and Decision-making, to convert insights gained from memories into investment decisions. Notably, \\textsc{FinMem}'s memory module aligns closely with the cognitive structure of human traders, offering robust interpretability and real-time tuning. Its adjustable cognitive span allows for the retention of critical information beyond human perceptual limits, thereby enhancing trading outcomes. This framework enables the agent to self-evolve its professional knowledge, react agilely to new investment cues, and continuously refine trading decisions in the volatile financial environment. We first compare \\textsc{FinMem} with various algorithmic agents on a scalable real-world financial dataset, underscoring its leading trading performance in stocks. We then fine-tuned the agent's perceptual span and character setting to achieve a significantly enhanced trading performance. Collectively, \\textsc{FinMem} presents a cutting-edge LLM agent framework for automated trading, boosting cumulative investment returns.", "summary_bullets": ["Recent advancements in Large Language Models (LLMs) have exhibited notable efficacy in question-answering (QA) tasks across diverse domains.", "Their prowess in integrating extensive web knowledge has fueled interest in developing LLM-based autonomous agents.", "While LLMs are efficient in decoding human instructions and deriving solutions by holistically processing historical inputs, transitioning to purpose-driven agents requires a supplementary rational architecture to process multi-source information, establish reasoning chains, and prioritize critical tasks.", "Addressing this, we introduce \\textsc{FinMem}, a novel LLM-based agent framework devised for financial decision-making.", "It encompasses three core modules: Profiling, to customize the agent's characteristics; Memory, with layered message processing, to aid the agent in assimilating hierarchical financial data; and Decision-making, to convert insights gained from memories into investment decisions."], "method": "Addressing this, we introduce \\textsc{FinMem}, a novel LLM-based agent framework devised for financial decision-making.", "key_results": ["While LLMs are efficient in decoding human instructions and deriving solutions by holistically processing historical inputs, transitioning to purpose-driven agents requires a supplementary rational architecture to process multi-source information, establish reasoning chains, and prioritize critical tasks.", "Notably, \\textsc{FinMem}'s memory module aligns closely with the cognitive structure of human traders, offering robust interpretability and real-time tuning."], "limitations": ["Abstract-level evidence only: validate assumptions, evaluation protocol, and failure cases in the full paper before relying on this as key evidence."], "bibkey": "Yu2023Finmem"}
